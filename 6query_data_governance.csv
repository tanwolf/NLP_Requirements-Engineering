QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"75961897","How to sync in realtime a legacy system with a new one?","<p>I have a legacy grails based system with a MySQL database. I rewrite incrementally the system in a modern Spring Boot 3 and MongoDB Atlas database. I can't just shutdown the legacy system and replace it and I want to rewrite only the write operations (create, update and delete) in the legacy system. So, I write a synchronization mechanism between the legacy system and the new system. To do this, I write a sync project and I use MongoDB Atlas triggers + functions.</p>
<p>For now, the process is as follow:</p>
<ol>
<li>I call the modern Spring project from the legacy project controller with a REST API call for all write operation (create, update and delete).</li>
<li>I save the new entity in the new project database based on MongoDB Atlas.</li>
<li>A trigger is executed on the entity write and execute a MongoDB Atlas function. This function sends the write event to a sync project.</li>
<li>The sync project receives the write event, transform the new entity into MySQL insert, update or delete executed directly in the legacy database to update it. The sync project keeps also a Link between the legacy entity and the new entity for further calls.</li>
</ol>
<p>With this mechanism I don't need to update any read operations in the legacy system because the legacy database is updated by the sync project. All the system works as expected but I must do a Thread.sleep in the legacy controller in order to wait the MongoDB Atlas trigger + function execution and the legacy database update.</p>
<p>This waiting is not really acceptable but I don't see what I can do to resolve the problem.</p>
<p>Do you have any idea?</p>
","<mongodb><spring-boot><data-synchronization>","2023-04-07 20:58:30","19","0","1","75994127","<p>I have resolved my problem with AOP instead of MongoDB Atlas function.</p>
"
"75916054","Insufficient CPU quota when running data quality task in GCP dataplex","<p>I try to follow the below guide from GCP to create a data quality task. <a href=""https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin"" rel=""nofollow noreferrer"">https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin</a> when I try to run the task I get the error</p>
<p><a href=""https://i.stack.imgur.com/OquEI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OquEI.png"" alt=""Error"" /></a></p>
<ul>
<li>As per my understanding Dataplex is calling dataproc internally for running the spark jobs due to which we get the error message. But we dont have the option to adjust the number of CPUs that can be used.</li>
<li>I tried to increase the CPU quota but GCP does not allow me( My account is enabled for billing for a long time)
Any suggestion to correct this error will be appreciated.</li>
</ul>
","<google-cloud-platform><google-cloud-dataproc><google-dataplex>","2023-04-03 04:33:41","76","1","2","75919323","<p>It won't help you, but even if you have activated the billing, as personal user you have a very limited number of allowed CPU (here my CPU quotas for all region; and I'm a GDE! Check it in your project.)
<a href=""https://i.stack.imgur.com/gxtVe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gxtVe.png"" alt=""enter image description here"" /></a></p>
<p>And here the quotas with my corporate sandbox project
<a href=""https://i.stack.imgur.com/rj5O0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rj5O0.png"" alt=""enter image description here"" /></a></p>
<p>If you have the opportunity to test it in a corporate account, or if you have a Google Cloud contact, you could ask for a quota increase.</p>
"
"75916054","Insufficient CPU quota when running data quality task in GCP dataplex","<p>I try to follow the below guide from GCP to create a data quality task. <a href=""https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin"" rel=""nofollow noreferrer"">https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin</a> when I try to run the task I get the error</p>
<p><a href=""https://i.stack.imgur.com/OquEI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OquEI.png"" alt=""Error"" /></a></p>
<ul>
<li>As per my understanding Dataplex is calling dataproc internally for running the spark jobs due to which we get the error message. But we dont have the option to adjust the number of CPUs that can be used.</li>
<li>I tried to increase the CPU quota but GCP does not allow me( My account is enabled for billing for a long time)
Any suggestion to correct this error will be appreciated.</li>
</ul>
","<google-cloud-platform><google-cloud-dataproc><google-dataplex>","2023-04-03 04:33:41","76","1","2","75920854","<p>As mentioned by @guillaume, there is set a limit on the number of CPUs you can use for a region (and all regions) in GCP. It is different for individuals and organizations. You can <a href=""https://cloud.google.com/docs/quota#api_specific_quota"" rel=""nofollow noreferrer"">check</a> this under IAM -&gt; Quotas-&gt; Compute Engine API CPU quota.</p>
<p>Solutions to your error:</p>
<ul>
<li><p>You can request a <a href=""https://cloud.google.com/docs/quota#requesting_higher_quota"" rel=""nofollow noreferrer"">quota
increase</a>
on the number of CPUs for a region/all regions for your project.</p>
</li>
<li><p>Try changing the configuration of your Dataproc Cluster by decreasing
the number of secondary workers. <a href=""https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#using_secondary_workers"" rel=""nofollow noreferrer"">Reference</a></p>
</li>
</ul>
"
"75695419","Using great expectations with databricks autolaoder","<p>I have implemented a data pipeline using autoloader bronze --&gt; silver --&gt; gold.</p>
<p>now while I do this I want to perform some data quality checks, and for that I'm using great expectations library.</p>
<p>However I'm stuck with below error when trying to validate the data</p>
<pre><code>validator.expect_column_values_to_not_be_null(column=&quot;col1&quot;)â€‹
validator.expect_column_values_to_be_in_set(
   column=&quot;col2&quot;,
   value_set=[1,6]
)
</code></pre>
<blockquote>
<p>MetricResolutionError: Queries with streaming sources must be executed with writeStream.start();</p>
</blockquote>
<p>looks like great expectations can work with only static/batch data.</p>
<p>How can I get it working for streaming data?</p>
<p>I followed below in my databricks notebook to get started with great_expectations</p>
<p><a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_databricks/"" rel=""nofollow noreferrer"">https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_databricks/</a></p>
<pre><code>from pyspark.sql.functions import col, to_date, date_format
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType
import time
 
# autoloader table and checkpoint paths
basepath = &quot;/mnt/autoloaderdemodl/datagenerator/&quot;
bronzeTable = basepath + &quot;bronze/&quot;
bronzeCheckpoint = basepath + &quot;checkpoint/bronze/&quot;
bronzeSchema = basepath + &quot;schema/bronze/&quot;
silverTable = basepath + &quot;silver/&quot;
silverCheckpoint = basepath + &quot;checkpoint/silver/&quot;
landingZoneLocation = &quot;/mnt/autoloaderdemodl/datageneratorraw/customerdata_csv&quot;
 
# Load data from the CSV file using Auto Loader to bronze table using rescue as schema evolution option
raw_df = spark.readStream.format(&quot;cloudFiles&quot;) \
            .option(&quot;cloudFiles.format&quot;, &quot;csv&quot;) \
            .option(&quot;cloudFiles.schemaEvolutionMode&quot;, &quot;rescue&quot;) \
            .option(&quot;Header&quot;, True) \
            .option(&quot;cloudFiles.schemaLocation&quot;, bronzeSchema) \
            .option(&quot;cloudFiles.inferSchema&quot;, &quot;true&quot;) \
            .option(&quot;cloudFiles.inferColumnTypes&quot;, True) \
        .load(landingZoneLocation)
 
        # Write raw data to the bronze layer
bronze_df = raw_df.writeStream.format(&quot;delta&quot;) \
            .trigger(once=True) \
            .queryName(&quot;bronzeLoader&quot;) \
            .option(&quot;checkpointLocation&quot;, bronzeCheckpoint) \
            .option(&quot;mergeSchema&quot;, &quot;true&quot;) \
            .outputMode(&quot;append&quot;) \
            .start(bronzeTable)
# Wait for the bronze stream to finish
bronze_df.awaitTermination()
bronze = spark.read.format(&quot;delta&quot;).load(bronzeTable)
bronze_count = bronze.count()
display(bronze)
print(&quot;Number of rows in bronze table: {}&quot;.format(bronze_count))
 
 
bronze_df = spark.readStream.format(&quot;delta&quot;).load(bronzeTable)
 
# Apply date format transformations to the DataFrame
# Transform the date columns
silver_df = bronze_df.withColumn(&quot;date1&quot;, to_date(col(&quot;date1&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date2&quot;, to_date(col(&quot;date2&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date3&quot;, to_date(col(&quot;date3&quot;), &quot;MMddyy&quot;))
 
# Write the transformed DataFrame to the Silver layer
silver_stream  = silver_df.writeStream \
    .format(&quot;delta&quot;) \
    .outputMode(&quot;append&quot;) \
    .option(&quot;mergeSchema&quot;, &quot;true&quot;) \
    .option(&quot;checkpointLocation&quot;, silverCheckpoint) \
    .trigger(once=True) \
    .start(silverTable)
 

# Wait for the write stream to complete
silver_stream.awaitTermination()
# Count the number of rows in the Silver table
silver = spark.read.format(&quot;delta&quot;).load(silverTable)
display(silver)
silver_count = silver.count()
print(&quot;Number of rows in silver table: {}&quot;.format(silver_count))
</code></pre>
<p>PS - customer doesn't want to use DLT yet.</p>
<p>code to include expectation validation</p>
<pre><code>import great_expectations as ge
from great_expectations.datasource.types import BatchKwargs

bronze_df = spark.readStream.format(&quot;delta&quot;).load(bronzeTable)

# Apply date format transformations to the DataFrame
# Transform the date columns
silver_df = bronze_df.withColumn(&quot;date1&quot;, to_date(col(&quot;date1&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date2&quot;, to_date(col(&quot;date2&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date3&quot;, to_date(col(&quot;date3&quot;), &quot;MMddyy&quot;))
def validate_micro_batch(batch_df, epoch):
    print(&quot;inside function&quot;)
    # Use Great Expectations to validate the batch DataFrame
    clean_df = batch_df
    clean_df.expect_column_values_to_not_be_null(column=&quot;col1&quot;)
    clean_df.expect_column_values_to_be_between(
        column=&quot;col2&quot;, min_value=0, max_value=1000
    )
    clean_df.write.format(&quot;delta&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;).mode(&quot;append&quot;).saveAsTable(silverTable)
    # Print the validation results for the batch
    validation_results = clean_df.validate()
    print(&quot;Validation results for batch {}:&quot;.format(batch_id))
    print(validation_results)
    
# Write the transformed DataFrame to the Silver layer if it passes all expectations
silver_stream = silver_df.writeStream \
    .format(&quot;delta&quot;) \
    .outputMode(&quot;append&quot;) \
    .foreachBatch(validate_micro_batch) \
    .option(&quot;checkpointLocation&quot;, silverCheckpoint) \
    .trigger(once=True) \
    .start()
# Wait for the write stream to complete
silver_stream.awaitTermination()
# Count the number of rows in the Silver table
silver = spark.read.format(&quot;delta&quot;).load(silverTable)
display(silver)
silver_count = silver.count()
print(&quot;Number of rows in silver table: {}&quot;.format(silver_count))
</code></pre>
","<azure-databricks><spark-structured-streaming><great-expectations><data-quality><databricks-autoloader>","2023-03-10 11:12:13","136","1","1","75713053","<p>Great Expectations is designed to work with batches of the data, so if you want to use it with Spark structured streaming then you will need to implement your checks inside a function that will be passed to <code>foreachBatch</code> argument of <code>writeStream</code> (<a href=""https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch"" rel=""nofollow noreferrer"">doc</a>).</p>
<p>It will look something like this:</p>
<pre class=""lang-py prettyprint-override""><code>def foreach_batch_func(df, epoch):
  # apply GE expectations to df and get clean dataframe
  clean_df = df....
  clean_df.write.format(&quot;delta&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;) \
    .mode(&quot;append&quot;).saveAsTable(silverTable)

silver_stream  = silver_df.writeStream \
    .format(&quot;delta&quot;) \
    .outputMode(&quot;append&quot;) \
    .foreachBatch(foreach_batch_func) \
    .option(&quot;checkpointLocation&quot;, silverCheckpoint) \
    .trigger(once=True) \
    .start()
</code></pre>
<p>But really, for this kind of the checks, Great Expectations would be overkill. And really, you need to discuss about adoption of Delta Live Tables for this.</p>
<p>P.S. You may need to <a href=""https://docs.databricks.com/structured-streaming/delta-lake.html#idempotent-table-writes-in-foreachbatch"" rel=""nofollow noreferrer"">add options for idempotent writes to Delta</a>.</p>
"
"75665949","How to select all rows that have the same value in each column using tidyverse?","<p>I'm working on data quality analysis for a questionnaire where respondents were asked to check mark every bit of food that they ate. Some respondents left the form blank so I'm trying to figure out a way to select or count all rows where each column is blank.</p>
<p>In this particular dataset, the value of &quot;No&quot; indicates that the box was not checked on the form. A value of &quot;Yes&quot; means that the box was checked. They are currently character variables in the dataset. It's easy to count the number of yes and no responses in a particular column, but I'm interested in counting rows where every single response is &quot;NO.&quot;</p>
<p>To make things simple, let's say that there are 5 columns representing different foods on the chart: Apple, Banana, Sandwich, Yogurt, Strawberry (the actual dataset has over 40 different food items). How would I select every row where &quot;No&quot; is the response for each food item?</p>
<p>This is what I've tried so far and it isn't working:</p>
<pre><code>food_history &lt;- food %&gt;%
     filter(Apple:Strawberry==&quot;No&quot;)
</code></pre>
","<r><filter><tidyverse><data-quality>","2023-03-07 18:36:28","42","1","1","75665986","<p>We can use <code>if_all</code></p>
<pre><code>library(dplyr)
food %&gt;%
   filter(if_all(Apple:Strawberry, ~ .x == &quot;No&quot;))
</code></pre>
"
"75657343","Check the data quality in Google Sheets (asking for suggestions)","<p>I'm trying to create a sheet to check the data quality from a survey in Google Sheets the document have this format:</p>
<p><a href=""https://i.stack.imgur.com/iajHg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iajHg.png"" alt=""enter image description here"" /></a></p>
<p>So basically I was using this formula <code>=COUNTIF(B2:F2,&quot;Don't know&quot;)</code> to count <code>Don't know</code>, empty spaces, 0's and numbers &gt; than 9, if the percentage from the counts is bigger than 0.31 or 31% the data quality is not good, as an example I'll take <code>row 2</code>, it has responses from Column B-F (5 cells) and this is the total number of responses, the count of bad data is 2, so in percentage will be represented the quality as <code>2/5 = 40%</code>, but I want to do the same for the all survey questions in a survey with 1600 questions, <a href=""https://docs.google.com/spreadsheets/d/1SylcKsNjuA9WJ6aCiD1YuORWHfJidlAl3umthylGwuE/edit?usp=sharing"" rel=""nofollow noreferrer"">this document</a>,the document attached contains a small piece of data from the original survey. So I would like to ask for a better solution that counting all the columns and the rows, I'm asking for a recomendation or how I should check the data quality, basically in the document I have all the formulas that I need but I would like to have all the formulas in just one. Also in the document in the desirable output column there is the final result that I would like to have. Hoping my explanaiton was good about the desired output.</p>
","<google-sheets><google-sheets-formula><data-quality>","2023-03-07 01:44:44","124","0","2","75680753","<p>First of all clear everything in ranges <code>G2:J21</code> and <code>L2:M21</code></p>
<p>You can now try the following 2 formulas in cells <code>G2</code> and <code>L2</code> respectively.<br />
<em>As for column <code>K2</code>, it becomes obsolete since it is a fixed number (number 5) that is easily incorporated within the 2nd formula</em></p>
<p>in cell <code>G2</code> try</p>
<pre><code>=INDEX(IF(A2:A21=&quot;&quot;,,
          {BYROW(B2:F21,LAMBDA(zz,{COUNTIF(zz,&quot;Don't know&quot;),COUNTBLANK(zz)})),
           BYROW(F2:F21,LAMBDA(xx,{COUNTIF(xx, 0),COUNTIF(xx, &quot;&gt;9&quot;)}))}))
</code></pre>
<p>in cell <code>L2</code> try</p>
<pre><code>={INDEX(IF(A2:A21=&quot;&quot;,,
            LET(vv,BYROW(G2:J21,LAMBDA(ww,SUM(ww)/5)),
               {vv,INDEX(IF((vv&lt;= 0.3),&quot;Good Quality&quot;,IF(vv&gt;0.31,&quot;Bad Quality&quot;,)))})))}
</code></pre>
<p><a href=""https://i.stack.imgur.com/lHyyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lHyyX.png"" alt=""enter image description here"" /></a></p>
<p><em>(<strong>Do</strong> adjust the formulae according to your ranges and locale)</em></p>
"
"75657343","Check the data quality in Google Sheets (asking for suggestions)","<p>I'm trying to create a sheet to check the data quality from a survey in Google Sheets the document have this format:</p>
<p><a href=""https://i.stack.imgur.com/iajHg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iajHg.png"" alt=""enter image description here"" /></a></p>
<p>So basically I was using this formula <code>=COUNTIF(B2:F2,&quot;Don't know&quot;)</code> to count <code>Don't know</code>, empty spaces, 0's and numbers &gt; than 9, if the percentage from the counts is bigger than 0.31 or 31% the data quality is not good, as an example I'll take <code>row 2</code>, it has responses from Column B-F (5 cells) and this is the total number of responses, the count of bad data is 2, so in percentage will be represented the quality as <code>2/5 = 40%</code>, but I want to do the same for the all survey questions in a survey with 1600 questions, <a href=""https://docs.google.com/spreadsheets/d/1SylcKsNjuA9WJ6aCiD1YuORWHfJidlAl3umthylGwuE/edit?usp=sharing"" rel=""nofollow noreferrer"">this document</a>,the document attached contains a small piece of data from the original survey. So I would like to ask for a better solution that counting all the columns and the rows, I'm asking for a recomendation or how I should check the data quality, basically in the document I have all the formulas that I need but I would like to have all the formulas in just one. Also in the document in the desirable output column there is the final result that I would like to have. Hoping my explanaiton was good about the desired output.</p>
","<google-sheets><google-sheets-formula><data-quality>","2023-03-07 01:44:44","124","0","2","75684319","<p>You may try this:</p>
<pre><code>=byrow(B2:F,lambda(z,if(offset(index(z,,1),0,-1)=&quot;&quot;,,
      let(x,(countif(z,&quot;Don't Know&quot;)+countblank(z)+--(index(z,,5)=0)+--(index(z,,5)&gt;9))/columns(z),
      if(x&gt;0.3,&quot;Bad&quot;,&quot;Good&quot;)&amp;&quot; Quality&quot;))))
</code></pre>
<p><a href=""https://i.stack.imgur.com/vjZ37.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjZ37.png"" alt=""enter image description here"" /></a></p>
<p>if you wish to see the score alongside the quality status then use:</p>
<pre><code>=byrow(B2:F,lambda(z,if(offset(index(z,,1),0,-1)=&quot;&quot;,,let(x,(countif(z,&quot;Don't Know&quot;)+countblank(z)+--(index(z,,5)=0)+--(index(z,,5)&gt;9))/columns(z),{to_percent(x),if(x&gt;0.3,&quot;Bad&quot;,&quot;Good&quot;)&amp;&quot; Quality&quot;}))))
</code></pre>
"
"75606832","Is there a local Graph DB (e.g. Neo4j) for Flutter that optionally syncs with the cloud?","<p>do you have a smart approach to tackle the following challenge? A flutter app wants to model its data in a graph structure. This graph structure includes meta information about objects, text based information and links to files (e.g. images, videos, audios, documents). These files are stored in a seperate file storage (to keep the graph light). This solution should work locally and optionally sync (as fast as possible) with the cloud. So I suppose there had to be some sort of local server that hosts both a graph database and file storage as well as some mechanism that can sync (ideally in the background?) these data with an instance in a cloud (to enable syncronization across devices). How would you go about developing a solution in flutter for this? Please let me know if you need more information.</p>
<p>Thank you and kind regards,
Niklas</p>
<p>I already did my own research on this but don't find a graph db integration for flutter where I can run a local instance that syncs with the cloud. I would expect that my problem is not that unique and there's some frameworks for it. I'd like to understand this better by hearing how you would tackle the problem. Thank you.</p>
","<flutter><graph><cloud><local><data-synchronization>","2023-03-01 17:02:13","51","1","1","75745723","<p>It's a little unclear to me based on your question what problem you are actually trying to solve here, but for my answer I'm going to assume you are looking for some kind of realtime sync that stores locally while offline, and then automatically detects a network connection and syncs again.</p>
<p>There really isn't a native neo4j solution here currently, nor, honestly, do I expect there ever will be.</p>
<p>There are, however similar tools for other types of databases, so I suppose it may happen one day.
<a href=""https://www.mongodb.com/docs/realm/sdk/flutter/"" rel=""nofollow noreferrer"">https://www.mongodb.com/docs/realm/sdk/flutter/</a>
<a href=""https://firebase.google.com/docs/database/flutter/offline-capabilities"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/database/flutter/offline-capabilities</a></p>
<p>My team isn't exactly solving for this exact problem, because we aren't really at a point where we're thinking about offline functionality, so I apologize if this question isn't a perfect match. But what we do is use GraphQL to interface with our Neo4j database. By doing so, we have an opportunity to model our local data however we need to on the client and easily convert to JSON objects to interface with our API in a graph-like structure.</p>
<p>Unfortunately, given the complexities of syncing offline/online in realtime, and especially because neo4j/graphql doesn't really have good subscription functionality yet, you may find yourself having to either roll out some very challenging custom solutions, lean on something like the libraries referenced above, or making compromises on your functionality.</p>
<p>Alternatively, if realtime sync isn't the point, I would argue that your client database really doesn't need to use the same tools as your server db  (that's what apis are for) and that you should consider using sqlite, one of the above db tools, or one of the many other mobile client databases available. You can find many of them here: <a href=""https://pub.dev/packages?q=database"" rel=""nofollow noreferrer"">https://pub.dev/packages?q=database</a></p>
<p>updating to add: Depending on your goals and experience level you may also want to consider the question of if you actually even need a database on the client at all.</p>
"
"75590411","Multiple Users editing the same (DT) table at the same time","<p>Is it possible to have multiple users modifying the same DT table simultaneously (like a google doc / excel ...) ? We have an app with a table where users that are connected to the app need to add comments within the table. But in the case of multiple users that have loaded the page before anyone has added a comment, the last one to add one deletes all previously entered comments. I've done this kind of thing with a MongoDB but for this app we would like to stay inside shiny. Would you know how to circumvent this issue ? If Shiny package exists, even if it's not DT related ?
Thanks a lot, let me know if I can clarify the question !!</p>
","<r><shiny><dt><data-synchronization>","2023-02-28 09:56:56","54","1","1","75592442","<p>This can be done using a global <code>reactiveVal</code> (defined outside of the <code>server</code> function), which is shared among sessions.</p>
<p>The following is based on the example given <a href=""https://github.com/rstudio/DT/pull/480#issue-289146755"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code>library(shiny)
library(DT)

DF &lt;- reactiveVal(iris) # cross session reactiveVal

ui &lt;- fluidPage(DTOutput('myTable'))

server &lt;- function(input, output, session) {
  observeEvent(input$myTable_cell_edit, {
    info = input$myTable_cell_edit
    str(info)
    i = info$row
    j = info$col + 1  # column index offset by 1
    v = info$value
    tmpDF &lt;- DF()
    tmpDF[i, j] &lt;- DT::coerceValue(v, DF()[i, j])
    DF(tmpDF)
  })
  output$myTable &lt;- renderDT({
    DF()
  }, selection = 'none', rownames = FALSE, editable = TRUE)
}

shinyApp(ui, server)
</code></pre>
<p><em>DT edit in the first session appears in the second and vice versa:</em>
<a href=""https://i.stack.imgur.com/c81MH.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c81MH.gif"" alt=""result"" /></a></p>
"
"75229394","Can we use Microsoft Purview and Unity Catalog together","<p><code>Unity Catalog</code> is the Azure Databricks data governance solution for the Lakehouse. Whereas, <code>Microsoft Purview</code> provides a unified data governance solution to help manage and govern your on-premises, multicloud, and software as a service (SaaS) data.</p>
<p><strong>Question</strong>: In our <code>same</code> Azure Cloud project, can we use <code>Unity Catalog</code> for the Azure Databricks Lakehouse, and use Microsoft Purview for the rest of our Azure project?</p>
<p><strong>Update</strong>: In our current Azure subscription, we have divided workload as follows:</p>
<ol>
<li><strong>SQL related workload</strong>: we are doing all our SQL database work using Databricks <code>only</code> (no Azure SQL databases are involved). That is, we are using Databricks Lakehouse, Delta Lake, Deatricks SQL etc. to perform <code>ETL</code> and all <code>Data Analytics work</code>.</li>
<li><strong>All Non-SQL workload</strong>: All other assets (Excel files, csv files, pdf, media files etc.) are stored in various Azure storage accounts.</li>
</ol>
<p>MS Purview is doing a good job in scanning assets in scenario 2 above, and it easily creates a holistic, up-to-date map of our data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage. It also enables our data consumers to access valuable, trustworthy data management.</p>
<p>However, our almost 50% of the work (SQL, ETL, Data Analytics etc.) is done in Azure Databricks where we have significant challenges with Purview. We were wondering if it's possible to keep Purview and Unity Catalog separate as follows: Purview does its Data Governance work for scenario 1 only and Unity Catalog does its Data Governance work for scenario 2 only.</p>
<p>This recently released update may resolve our issue of making Purview work better with Azure Databricks but we have not tried it yet: <a href=""https://learn.microsoft.com/en-us/azure/purview/register-scan-azure-databricks"" rel=""nofollow noreferrer"">Connect to and manage Azure Databricks in Microsoft Purview (Preview)</a></p>
","<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>","2023-01-25 03:16:02","1692","2","3","75231419","<p>As of right now there is no official integration between Unity Catalog and Purview yet, but it may come in the future. You may join <a href=""https://www.databricks.com/p/webinar/productroadmapwebinar"" rel=""nofollow noreferrer"">Azure Databricks roadmap webinar</a> that will be tomorrow to get more information.</p>
<p>Regarding the actual question - imho, nothing prevents you from using UC &amp; Purview in the same Azure project.</p>
<p>P.S. You can get metadata &amp; lineage information into Purview by loading data from <a href=""https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-information-schema"" rel=""nofollow noreferrer"">information schema tables</a> and using Purview APIs to store it in Purview.</p>
"
"75229394","Can we use Microsoft Purview and Unity Catalog together","<p><code>Unity Catalog</code> is the Azure Databricks data governance solution for the Lakehouse. Whereas, <code>Microsoft Purview</code> provides a unified data governance solution to help manage and govern your on-premises, multicloud, and software as a service (SaaS) data.</p>
<p><strong>Question</strong>: In our <code>same</code> Azure Cloud project, can we use <code>Unity Catalog</code> for the Azure Databricks Lakehouse, and use Microsoft Purview for the rest of our Azure project?</p>
<p><strong>Update</strong>: In our current Azure subscription, we have divided workload as follows:</p>
<ol>
<li><strong>SQL related workload</strong>: we are doing all our SQL database work using Databricks <code>only</code> (no Azure SQL databases are involved). That is, we are using Databricks Lakehouse, Delta Lake, Deatricks SQL etc. to perform <code>ETL</code> and all <code>Data Analytics work</code>.</li>
<li><strong>All Non-SQL workload</strong>: All other assets (Excel files, csv files, pdf, media files etc.) are stored in various Azure storage accounts.</li>
</ol>
<p>MS Purview is doing a good job in scanning assets in scenario 2 above, and it easily creates a holistic, up-to-date map of our data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage. It also enables our data consumers to access valuable, trustworthy data management.</p>
<p>However, our almost 50% of the work (SQL, ETL, Data Analytics etc.) is done in Azure Databricks where we have significant challenges with Purview. We were wondering if it's possible to keep Purview and Unity Catalog separate as follows: Purview does its Data Governance work for scenario 1 only and Unity Catalog does its Data Governance work for scenario 2 only.</p>
<p>This recently released update may resolve our issue of making Purview work better with Azure Databricks but we have not tried it yet: <a href=""https://learn.microsoft.com/en-us/azure/purview/register-scan-azure-databricks"" rel=""nofollow noreferrer"">Connect to and manage Azure Databricks in Microsoft Purview (Preview)</a></p>
","<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>","2023-01-25 03:16:02","1692","2","3","75536926","<p>The integration between Purview and UC is in private preview.</p>
"
"75229394","Can we use Microsoft Purview and Unity Catalog together","<p><code>Unity Catalog</code> is the Azure Databricks data governance solution for the Lakehouse. Whereas, <code>Microsoft Purview</code> provides a unified data governance solution to help manage and govern your on-premises, multicloud, and software as a service (SaaS) data.</p>
<p><strong>Question</strong>: In our <code>same</code> Azure Cloud project, can we use <code>Unity Catalog</code> for the Azure Databricks Lakehouse, and use Microsoft Purview for the rest of our Azure project?</p>
<p><strong>Update</strong>: In our current Azure subscription, we have divided workload as follows:</p>
<ol>
<li><strong>SQL related workload</strong>: we are doing all our SQL database work using Databricks <code>only</code> (no Azure SQL databases are involved). That is, we are using Databricks Lakehouse, Delta Lake, Deatricks SQL etc. to perform <code>ETL</code> and all <code>Data Analytics work</code>.</li>
<li><strong>All Non-SQL workload</strong>: All other assets (Excel files, csv files, pdf, media files etc.) are stored in various Azure storage accounts.</li>
</ol>
<p>MS Purview is doing a good job in scanning assets in scenario 2 above, and it easily creates a holistic, up-to-date map of our data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage. It also enables our data consumers to access valuable, trustworthy data management.</p>
<p>However, our almost 50% of the work (SQL, ETL, Data Analytics etc.) is done in Azure Databricks where we have significant challenges with Purview. We were wondering if it's possible to keep Purview and Unity Catalog separate as follows: Purview does its Data Governance work for scenario 1 only and Unity Catalog does its Data Governance work for scenario 2 only.</p>
<p>This recently released update may resolve our issue of making Purview work better with Azure Databricks but we have not tried it yet: <a href=""https://learn.microsoft.com/en-us/azure/purview/register-scan-azure-databricks"" rel=""nofollow noreferrer"">Connect to and manage Azure Databricks in Microsoft Purview (Preview)</a></p>
","<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>","2023-01-25 03:16:02","1692","2","3","75899684","<p>Purview currently doesn't support scanning catalogs with a metastore attached.
I tried to set this up too, but I only get tables from the standard hive_metastore directory.</p>
<p>There is an Azure Databricks to Purview Lineage Connector.
You can check it out <a href=""https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator"" rel=""nofollow noreferrer"">here</a>.</p>
"
"75131262","great expectation with delta table","<p>I am trying to run a great expectation suite on a delta table in Databricks. But I would want to run this on part of the table with a query. Though the validation is running fine, it's running on full table data.</p>
<p>I know that I can load a Dataframe and pass it to Batch Request but I would like to load the data directly with query.</p>
<pre><code>batch_request = RuntimeBatchRequest(
    datasource_name=&quot;datasource&quot;,
    data_connector_name=&quot;data_quality_run&quot;,
    data_asset_name=&quot;Input Data&quot;,
    runtime_parameters={&quot;path&quot;: &quot;/delta table path&quot;},
    batch_identifiers={&quot;data_quality_check&quot;: f&quot;data_quality_check_{datetime.date.today().strftime('%Y%m%d')}&quot;},
    batch_spec_passthrough={&quot;reader_method&quot;: &quot;delta&quot;, &quot;reader_options&quot;: {&quot;header&quot;: True}, &quot;query&quot; : {&quot;name&quot;:&quot;John&quot;}},
)

</code></pre>
<p>Above batch request loading the data ignoring the query option. Is there any way to pass the query for delta table in the batch request</p>
","<databricks><delta-lake><great-expectations><data-quality>","2023-01-16 07:27:39","149","0","1","75259725","<p>You can try to put <code>query</code> inside of <code>runtime_parameters</code>.</p>
<p>This works for me when I am querying data in SQL Server:</p>
<pre class=""lang-py prettyprint-override""><code>batch_request = RuntimeBatchRequest(
    datasource_name=&quot;my_mssql_datasource&quot;,
    data_connector_name=&quot;default_runtime_data_connector_name&quot;,
    data_asset_name=&quot;default_name&quot;,
    runtime_parameters={
        &quot;query&quot;: &quot;SELECT * from dbo.MyTable WHERE Created = GETDATE()&quot;
    },
    batch_identifiers={&quot;default_identifier_name&quot;: &quot;default_identifier&quot;},
)
</code></pre>
"
"74694421","data cleansing - 2 columns change the data in one column if criteria is met","<p>I have columns with vehicle data, for vehicles greater than 1 year old with mileage less than 100 I want to replace mileage less than 100 with 1000.</p>
<p>my attempts -</p>
<pre><code>mileage_corr = vehicle_data_all.loc[(vehicle_data_all[&quot;mileage&quot;] &lt; 100) &amp; (vehicle_data_all[&quot;year&quot;] &lt; 2020)], 1000

Error - AttributeError: 'tuple' object has no attribute
</code></pre>
<p>and</p>
<pre><code>mileage_corr = vehicle_data_all.loc[(vehicle_data_all[&quot;mileage&quot;] &lt; 100) &amp; (vehicle_data_all[&quot;year&quot;] &lt; 2020)]
mileage_corr['mileage'].where(mileage_corr['mileage'] &lt;= 100, 1000, inplace=True)

error -
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  return self._where(
</code></pre>
","<python><tuples><where-clause>","2022-12-05 21:10:48","17","0","1","74694968","<p>Without complete information, assuming your <code>vehicle_data_all</code> DataFrame looks something like this,</p>
<pre><code>    years   mileage
0   2019    192
1   2014    78
2   2010    38
3   2018    119
4   2019    4
5   2012    122
6   2005    50
7   2015    69
8   2004    56
9   2003    194
</code></pre>
<p>Pandas has a way of assigning based on a filter result. This is referred to as <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer"">setting values</a>.</p>
<pre class=""lang-py prettyprint-override""><code>df.loc[condition, &quot;field_to_change&quot;] = desired_change
</code></pre>
<p>Applied to your dataframe would look something like this,</p>
<pre class=""lang-py prettyprint-override""><code>vehicle_data_all.loc[((vehicle_data_all[&quot;mileage&quot;] &lt; 100) &amp; (vehicle_data_all[&quot;year&quot;] &lt; 2020)), &quot;mileage&quot;] = 1000
</code></pre>
<p>This was my result,</p>
<pre><code>    years   mileage
0   2019    192
1   2014    1000
2   2010    1000
3   2018    119
4   2019    1000
5   2012    122
6   2005    1000
7   2015    1000
8   2004    1000
9   2003    194
</code></pre>
"
"74685196","How to synchronize python and c sharp program running in windows?","<p>I am currently working in a project where I need to sychronise data between python and c sharp.I need to label data from c sharp using python machine learning program. To label the data, I am using timestamp from both the application and based on the common timestamp, I am labelling the data.</p>
<p>Python program is running every 0.5 to 1.5 sec and C sharp program is running 10 times every 1 sec. Since the two process are running differently, I know there is some time lag. So labelling the data using the timestamp is not much accurate. I want to analyse the time lag properly. For this I am looking for options of real time synchronization between the two programs. I have looked into sockets but I think there is a better way using IPC. I donot know much about this.</p>
<p>I am thinking to create a shared variable between python and c#. Since python is slower, I will update that variable using python and read that variable from c# program. so same variable instance on both the program would tell us that they are synchronized perfectly. So I can look the value of this variable instead of timestamp for labelling the data. I am thinking that this might solve the issue. Please let me know what would be the best optimal solution to minimize the time difference lag between the two program.</p>
<p>since these are complex projects, I cannot implement them in a single program. I need to find a way to synchronize these two programs.</p>
<p>Any suggestions would be appreciated. Thank you.</p>
<p>I tried working with socket programming but they were not that good and a bit complex. So I am now thinking about IPC but still not sure which is the best way.</p>
","<python><c#><synchronization><data-synchronization>","2022-12-05 08:22:23","35","0","1","75196651","<p>First of all, I implemented a socket in C# program so that I get data from the socket. Then I implemented multiprocessing in python. One process will request to the socket and another process will work for ML model. I was able to achieve the synchronization using the multiprocessing module. I used multiprocessing.Event() to wait for the event from another process. You can also look into shared variables in python using multiprocessing.Value, multiprocessing.Array, multiprocessing.Event.</p>
"
"74493186","Using Pydequu on Jupyter Notebook and having this ""An error occurred while calling o70.run.'","<p>I'm trying to use Pydequu on Jupyter Notebook when i try to use <code>ConstraintSuggestionRunner</code> and show this error:</p>
<pre><code>Py4JJavaError: An error occurred while calling o70.run.
: java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction.toAggregateExpression(boolean)'
</code></pre>
<p>I'm using this setup for the test:</p>
<ul>
<li>SDKMAN</li>
<li>sdk install <code>java 8.0.292.hs-adpt</code></li>
<li>SPARK 3.0.0</li>
</ul>
<p><em>I got this configs from <a href=""https://github.com/awslabs/python-deequ"" rel=""nofollow noreferrer"">awslabs/python-dequu</a> on README.md file.</em></p>
<pre class=""lang-py prettyprint-override""><code>import os
from pyspark.sql import SparkSession, Row
import pydeequ

os.environ[&quot;SPARK_VERSION&quot;] = &quot;3.0.0&quot;
</code></pre>
<p>The <code>error</code> it's from below code:</p>
<pre class=""lang-py prettyprint-override""><code>spark = (SparkSession
    .builder
    .config(&quot;spark.jars.packages&quot;, pydeequ.deequ_maven_coord)
    .config(&quot;spark.jars.excludes&quot;, pydeequ.f2j_maven_coord)
    .config(&quot;org.apache.spark.sql.catalyst&quot;, &quot;spark-catalyst_2.12-3.1.2-amzn-0.jar&quot;)
    .getOrCreate())

df = spark.sparkContext.parallelize([
            Row(a=&quot;foo&quot;, b=1, c=5),
            Row(a=&quot;bar&quot;, b=2, c=6),
            Row(a=&quot;baz&quot;, b=3, c=None)]).toDF()
</code></pre>
<p>Complete <code>error</code>:</p>
<pre><code>---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
Input In [3], in &lt;cell line: 3&gt;()
      1 from pydeequ.suggestions import *
      3 suggestionResult = ConstraintSuggestionRunner(spark) \
      4              .onData(df) \
      5              .addConstraintRule(DEFAULT()) \
----&gt; 6              .run()
      8 # Constraint Suggestions in JSON format
      9 print(suggestionResult)

File /opt/conda/lib/python3.10/site-packages/pydeequ/suggestions.py:81, in ConstraintSuggestionRunBuilder.run(self)
     74 def run(self):
     75     &quot;&quot;&quot;
     76     A method that runs the desired ConstraintSuggestionRunBuilder functions on the data to obtain a constraint
     77             suggestion result. The result is then translated to python.
     78 
     79     :return: A constraint suggestion result
     80     &quot;&quot;&quot;
---&gt; 81     result = self._ConstraintSuggestionRunBuilder.run()
     83     jvmSuggestionResult = self._jvm.com.amazon.deequ.suggestions.ConstraintSuggestionResult
     84     result_json = json.loads(jvmSuggestionResult.getConstraintSuggestionsAsJson(result))

File /usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args)
   1315 command = proto.CALL_COMMAND_NAME +\
   1316     self.command_header +\
   1317     args_command +\
   1318     proto.END_COMMAND_PART
   1320 answer = self.gateway_client.send_command(command)
-&gt; 1321 return_value = get_return_value(
   1322     answer, self.gateway_client, self.target_id, self.name)
   1324 for temp_arg in temp_args:
   1325     temp_arg._detach()

File /usr/local/spark/python/pyspark/sql/utils.py:190, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)
    188 def deco(*a: Any, **kw: Any) -&gt; Any:
    189     try:
--&gt; 190         return f(*a, **kw)
    191     except Py4JJavaError as e:
    192         converted = convert_exception(e.java_exception)

File /usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)
    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325 if answer[1] == REFERENCE_TYPE:
--&gt; 326     raise Py4JJavaError(
    327         &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    328         format(target_id, &quot;.&quot;, name), value)
    329 else:
    330     raise Py4JError(
    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\n{3}\n&quot;.
    332         format(target_id, &quot;.&quot;, name, value))

Py4JJavaError: An error occurred while calling o70.run.
: java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction.toAggregateExpression(boolean)'
    at org.apache.spark.sql.DeequFunctions$.withAggregateFunction(DeequFunctions.scala:31)
    at org.apache.spark.sql.DeequFunctions$.stateful_approx_count_distinct(DeequFunctions.scala:60)
    at com.amazon.deequ.analyzers.ApproxCountDistinct.aggregationFunctions(ApproxCountDistinct.scala:52)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.$anonfun$runScanningAnalyzers$3(AnalysisRunner.scala:319)
    at scala.collection.immutable.List.flatMap(List.scala:366)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.liftedTree1$1(AnalysisRunner.scala:319)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.runScanningAnalyzers(AnalysisRunner.scala:318)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.doAnalysisRun(AnalysisRunner.scala:167)
    at com.amazon.deequ.analyzers.runners.AnalysisRunBuilder.run(AnalysisRunBuilder.scala:110)
    at com.amazon.deequ.profiles.ColumnProfiler$.profile(ColumnProfiler.scala:141)
    at com.amazon.deequ.profiles.ColumnProfilerRunner.run(ColumnProfilerRunner.scala:72)
    at com.amazon.deequ.profiles.ColumnProfilerRunBuilder.run(ColumnProfilerRunBuilder.scala:185)
    at com.amazon.deequ.suggestions.ConstraintSuggestionRunner.profileAndSuggest(ConstraintSuggestionRunner.scala:203)
    at com.amazon.deequ.suggestions.ConstraintSuggestionRunner.run(ConstraintSuggestionRunner.scala:102)
    at com.amazon.deequ.suggestions.ConstraintSuggestionRunBuilder.run(ConstraintSuggestionRunBuilder.scala:226)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
    at java.base/java.lang.Thread.run(Thread.java:833)
</code></pre>
","<python><data-quality><data-profiling>","2022-11-18 17:12:13","139","2","1","75732362","<p>I had the same issue by using almost the same environment as you (meaning Jupyter notebook and Spark 3.1.1).
I solved the problem by following the steps:</p>
<ol>
<li><p>Download the deequ-2.0.0-spark-3.1.jar from maven repository <a href=""https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.0-spark-3.1/deequ-2.0.0-spark-3.1.jar"" rel=""nofollow noreferrer"">https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.0-spark-3.1/deequ-2.0.0-spark-3.1.jar</a></p>
</li>
<li><p>Upload deequ-2.0.0-spark-3.1.jar into the a Jupyter folder /home/jovyan/work/java-libs/</p>
</li>
<li><p>In the notebook add the following line:</p>
<p>os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///home/jovyan/work/java-libs/deequ-2.0.0-spark-3.1.jar pyspark-shell'</p>
</li>
<li><p>Use the initialization code</p>
<pre><code>from pyspark.sql import SparkSession, Row
</code></pre>
<p>import pydeequ</p>
<p>spark=(SparkSession
.builder
.getOrCreate())</p>
</li>
</ol>
<p>By following the above steps I could get rid off the error that you've mentioned.
However, I bumped into a new one:</p>
<pre><code>  py4j.Py4JException: Constructor com.amazon.deequ.suggestions.rules.CategoricalRangeRule([]) does not exist
</code></pre>
<p>that I've solved with a code like this:</p>
<pre><code>suggestionResult = (
    ConstraintSuggestionRunner(spark)
    .onData(df)
    .addConstraintRule(CompleteIfCompleteRule())
    .addConstraintRule(NonNegativeNumbersRule())
    .addConstraintRule(RetainCompletenessRule())
    .addConstraintRule(RetainTypeRule())
    .addConstraintRule(UniqueIfApproximatelyUniqueRule())
    .run()
)

print(json.dumps(suggestionResult))
</code></pre>
"
"74420549","Delta live tables data quality checks -Retain failed records","<p>There are 3 types of quality checks in Delta live tables:</p>
<ul>
<li><code>expect</code> (retain invalid records)</li>
<li><code>expect_or_drop</code> (drop invalid records)</li>
<li><code>expect_or_fail</code> (fail on invalid records)</li>
</ul>
<p>I want to retain invalid records, but I also want to keep track of them. So, by using <code>expect</code>, can I query the invalid records, or is it just for keeping stats like &quot;n records were invalid&quot;?</p>
","<databricks><azure-databricks><data-ingestion><delta-live-tables>","2022-11-13 11:34:13","273","1","1","74422470","<p><code>expect</code> just record that you had some problems so you have some statistics about you data quality in the pipeline.  But it's not very useful in practice.</p>
<p>Native quarantine functionality is still not available, that's why there is the <a href=""https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cookbook.html#quarantine-invalid-data"" rel=""nofollow noreferrer"">recipe in the cookbook</a>.  Although it's not exactly what you need, you can still build on top of it, especially if you take into account the second part of the recipe that explicitly adds a <code>Quarantine</code> column - we can combine it with <code>expect</code> to get statistics into UI:</p>
<pre><code>import dlt
from pyspark.sql.functions import expr

rules = {}
quarantine_rules = {}

...
quarantine_rules = &quot;NOT({0})&quot;.format(&quot; AND &quot;.join(rules.values()))

@dlt.table(
  name=&quot;partitioned_farmers_market&quot;,
  partition_cols = [ 'Quarantine' ]
)
@dlt.expect_all(rules)
def get_partitioned_farmers_market():
  return (
    dlt.read(&quot;raw_farmers_market&quot;)
      .withColumn(&quot;Quarantine&quot;, expr(quarantine_rules))
      .select(&quot;MarketName&quot;, &quot;Website&quot;, &quot;Location&quot;, &quot;State&quot;,
              &quot;Facebook&quot;, &quot;Twitter&quot;, &quot;Youtube&quot;, &quot;Organic&quot;, &quot;updateTime&quot;,
              &quot;Quarantine&quot;)
  )
</code></pre>
<p>Another approach would be to use first part of the recipe (that uses <code>expect_all_or_drop</code>), and just union both tables (it's better to mark the valid/invalid tables with <code>temporary = True</code> marker)</p>
"
"74214041","How to maintain order of data frame when making pandas pivot table","<p>trying to make a heat map out of a pivot table but am having trouble keeping the order of how I sorted my original data frame. Below is a sample code of what my data looks exactly like and how I made my pivot table.</p>
<pre><code>simple_df = pd.DataFrame({'Skill 1': ['Python','Python','Python','Communication','Communication','Communication','Data Governance','Data Governance','Data Governance'], 'Skill 2': ['Python','Communication','Data Governance','Python','Communication','Data Governance','Python','Communication','Data Governance'],'Score':[1,0.9,0.4,0.9,1,0.4,0.4,0.4,1],'Skill 1 Type':['Programming','Programming','Programming','Written','Written','Written','Cyber','Cyber','Cyber']})
simple_df=simple_df.sort_values(by = ['Skill 1 Type'], ascending = [True], na_position = 'first')
test=simple_df.groupby(['Skill 1','Skill 2'], sort=True)['Score'].sum().unstack('Skill 2')
</code></pre>
<p>Since I sorted on &quot;Skill 1 Type&quot;, I would like to have the pivot table's y-axis labels keep the same order of how &quot;Skill 1&quot; appears in my sorted dataframe. So ideally, I would have (Data Governance, Python, Communication) rather than (Communication, Data Governance, Python) on my y axis. What are some ways I can do this? Thank you!</p>
","<python><pandas><sorting><pivot><data-governance>","2022-10-26 20:45:54","99","2","1","74214380","<p>You can save the original order and then use reindex to swap the columns and rows.</p>
<pre><code>original_order = simple_df[&quot;Skill 1&quot;].unique()
(simple_df.pivot(index=&quot;Skill 1&quot;, columns=&quot;Skill 2&quot;, values=&quot;Score&quot;)
 .reindex(index=original_order, columns=original_order))


Skill 2          Data Governance  Python  Communication
Skill 1                                                
Data Governance              1.0     0.4            0.4
Python                       0.4     1.0            0.9
Communication                0.4     0.9            1.0
</code></pre>
"
"74151965","ZooKeeper zxid is used up due to data synchronization between the active and standby HBase clusters","<p>During data synchronization between the active and standby HBase clusters, the active cluster frequently performs the setdata operation on ZooKeeper. Most znodes are in /hbase/replication/xxx format. As a result, the ZooKeeper zxid is exhausted and the ZooKeeper service is abnormal. Can the setdata operation be reduced?</p>
","<hbase>","2022-10-21 10:01:44","35","0","1","74176026","<p>The HLog size is being increased to reduce the number of registrations with the ZooKeeper during active/standby synchronization.</p>
"
"74016176","Detecting similar columns across multiple files based on statistical profile","<p>I'm attempting to clean up a set of old files that contain sensor data measurements. Many of the files don't have headers, and the format (column ordering, etc.) is inconsistent.  I'm thinking the best that I can do in these cases is to match statistical profiles of the columns to data from files that do have good headers.  This seems like it should be simple using something like <a href=""https://github.com/ydataai/pandas-profiling"" rel=""nofollow noreferrer"">Pandas Profiling</a>, but I haven't found any examples. I'm looking for something that would calculate a score for the similarity between each column in the header-less file and each &quot;known&quot; column for which I already have headers.</p>
<p>Example Data with Headers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Large Value Column</th>
<th>Small Value Column</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>5</td>
</tr>
<tr>
<td>102</td>
<td>12</td>
</tr>
<tr>
<td>110</td>
<td>8</td>
</tr>
<tr>
<td>98</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>Example Data with only column numbers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>99</td>
</tr>
<tr>
<td>9</td>
<td>105</td>
</tr>
<tr>
<td>11</td>
<td>101</td>
</tr>
<tr>
<td>14</td>
<td>100</td>
</tr>
</tbody>
</table>
</div>
<p>For the above example, I would like to automatically determine that column 1 should be added to &quot;Large Value Column&quot; and Column 0 to &quot;Small Value Column&quot;.</p>
","<python><pandas><pandas-profiling><data-quality><data-profiling>","2022-10-10 14:05:55","56","0","2","74016463","<p>That's an interesting question.</p>
<p>The solution that will work heavily depends on the data you're working with. But here's my thoughts, assuming that your data is similar to the little exampled above, just longer.</p>
<p>The first step would be to create a <strong>descriptor</strong> that takes as input a row and returns a low-dimension vector that describes this row. For instance: the mean, the standard deviation, the median, ... This should be designed according to what <em>actually</em> describes your data. Then, for each column of your dataset, you're going to have a <strong>descriptor vector</strong> and (eventually) a <strong>label</strong> (ie. the column's header), that may or not be known.</p>
<p>The next step can either be <strong>classification</strong> or <strong>clustering</strong>, depending on how much data you have and the quality of your descriptors. Look up <a href=""https://scikit-learn.org/stable/"" rel=""nofollow noreferrer"">Scikit</a> documentation about how to do so. Their API is very easy to use and comes with many examples. If you know the number of real headers, you could try to use <a href=""https://scikit-learn.org/stable/modules/clustering.html?highlight=k%20mean#k-means"" rel=""nofollow noreferrer""><strong>K-means</strong></a> and I think it would work really well. Maybe <a href=""https://scikit-learn.org/stable/modules/neighbors.html?highlight=knn"" rel=""nofollow noreferrer"">KNN</a> can also work. You probably want to look for a sorting algorithm quite 'simple' at first (it's easier to tune), and if nothing simple works then you could try more fancy approaches.</p>
<p>I hope this helps you.</p>
"
"74016176","Detecting similar columns across multiple files based on statistical profile","<p>I'm attempting to clean up a set of old files that contain sensor data measurements. Many of the files don't have headers, and the format (column ordering, etc.) is inconsistent.  I'm thinking the best that I can do in these cases is to match statistical profiles of the columns to data from files that do have good headers.  This seems like it should be simple using something like <a href=""https://github.com/ydataai/pandas-profiling"" rel=""nofollow noreferrer"">Pandas Profiling</a>, but I haven't found any examples. I'm looking for something that would calculate a score for the similarity between each column in the header-less file and each &quot;known&quot; column for which I already have headers.</p>
<p>Example Data with Headers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Large Value Column</th>
<th>Small Value Column</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>5</td>
</tr>
<tr>
<td>102</td>
<td>12</td>
</tr>
<tr>
<td>110</td>
<td>8</td>
</tr>
<tr>
<td>98</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>Example Data with only column numbers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>99</td>
</tr>
<tr>
<td>9</td>
<td>105</td>
</tr>
<tr>
<td>11</td>
<td>101</td>
</tr>
<tr>
<td>14</td>
<td>100</td>
</tr>
</tbody>
</table>
</div>
<p>For the above example, I would like to automatically determine that column 1 should be added to &quot;Large Value Column&quot; and Column 0 to &quot;Small Value Column&quot;.</p>
","<python><pandas><pandas-profiling><data-quality><data-profiling>","2022-10-10 14:05:55","56","0","2","74017731","<p>As stated by <a href=""https://stackoverflow.com/users/13219173/arthur-bricq"">@Arthur Bricq</a>, the best solution depends a lot on the shape of your data. Unfortunately, here's no &quot;one solution fits all approach&quot;. Having said that, the following code was able to correctly rename the columns from your example, using the <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer""><code>fuzzywuzzy</code></a> package:</p>
<blockquote>
<p>Note: You need to pip install <code>fuzzywuzzy</code> before running the code below. To do so, run the following command:</p>
<p><code>pip install fuzzywuzzy</code></p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>
from typing import Tuple, List
from fuzzywuzzy import fuzz
import pandas as pd


def find_similar(
    *dfs: pd.DataFrame, known_df: pd.DataFrame
) -&gt; Tuple[List[pd.DataFrame], pd.DataFrame]:
    &quot;&quot;&quot;
    Name columns in a list of dataframes based on the columns from known dataframe.

    Each of the dataframes in the list will be renamed based on the columns from the
    known dataframe. To avoid cases where two different unnamed columns match the
    same column from the known dataframe, the function matches each of the known columns
    to the unnamed column that contains the highest similarity score and then removes
    the matched columns from the list of possible columns. For example:

    &gt;&gt;&gt; x = pd.DataFrame(
    ...     {'Large Value Column' :[100, 102, 110, 98,]
    ...      'Small Value Column': [5, 12, 8, 10,]}
    ... )
    &gt;&gt;&gt; y = pd.DataFrame(
    ...     {0: [6, 9, 11, 14],
    ...      1: [20, 25, 55, 65]}
    ... )
    &gt;&gt;&gt; pd.DataFrame(
    ...     [
    ...         [
    ...             fuzz.ratio(
    ...                 list(map(str, y[col].values)), list(map(str, x[_col].values))
    ...             ) for _col in x.columns
    ...         ] for col in y.columns
    ...     ],
    ...     index=y.columns,
    ...     columns=x.columns,
    ... )
       Large Value Column  Small Value Column
    0                  73                  77
    1                  71                  74

    In the above example, &quot;Small Value Column&quot; has the highest similarity score for
    both columns &quot;0&quot; and &quot;1&quot;. However, since &quot;0&quot; has the highest similarity score
    then, &quot;1&quot; will be renamed to &quot;Large Value Column&quot;.

    Parameters
    ----------
    dfs : pd.DataFrame
        List of dataframes to rename.
    known_df : pd.DataFrame
        Pandas dataframe with the correct column names.

    Returns
    -------
    Tuple[List[pd.DataFrame], pd.DataFrame]
        List of dataframes with renamed columns, and the known dataframe.
    &quot;&quot;&quot;
    # Store the results in a list
    _dfs = []
    # Loop over the dataframes
    for df in dfs:
        # Create a dataframe that contains the similarities between the columns
        # in the known dataframe and the columns in the current dataframe.
        # The columns represent the known dataframe columns, and the index
        # represents the current dataframe columns.
        similarity_df = pd.DataFrame(
            [
                [
                    fuzz.ratio(
                        list(map(str, df[col].values)),
                        list(map(str, known_df[_col].values)),
                    )
                    for _col in known_df.columns
                ]
                for col in df.columns
            ],
            index=df.columns,
            columns=known_df.columns,
        )
        # Dictionary to map old and new column names
        rename_dict = {}

        # Keep track of the columns that have already been matched
        # to avoid naming two columns using the same name.
        used_names = []
        for col in similarity_df.max().sort_values(ascending=False).keys():
            old_name = similarity_df[~similarity_df.index.isin(used_names)][
                col
            ].idxmax()
            rename_dict[old_name] = col
            used_names.append(old_name)
        # Rename the columns in the current dataframe and append it to the list
        _dfs.append(df.rename(columns=rename_dict, errors=&quot;ignore&quot;))
    return _dfs, known_df

</code></pre>
<h2>Example</h2>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd


x = pd.DataFrame(
    {
        &quot;Large Value Column&quot;: [100, 102, 110, 98],
        &quot;Small Value Column&quot;: [5, 12, 8, 10],
    }
)
y = pd.DataFrame({0: [6, 9, 11, 14], 1: [20, 40, 55, 65]})
z = pd.DataFrame({0: [95, 80, 72, 100], 1: [0, 20, 14, 10]})

pd.concat(find_similar(y, z, known_df=x)[0])

# Returns:
#
#    Small Value Column  Large Value Column
# 0                   6                  20
# 1                   9                  40
# 2                  11                  55
# 3                  14                  65
# 0                   0                  95
# 1                  20                  80
# 2                  14                  72
# 3                  10                 100
</code></pre>
<p><a href=""https://i.stack.imgur.com/GyQGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GyQGA.png"" alt=""enter image description here"" /></a></p>
"
"73984122","python great expectation compatible with pyspark","<p>I am implementing data quality checks using Great expectation library. does this library compatible with Pyspark does this run on multiple cores?</p>
","<python><data-quality><great-expectations>","2022-10-07 08:00:47","71","1","1","74014429","<p>Yes it is compatible with Pyspark. Here is the example.</p>
<p>datasource creation.</p>
<pre><code>datasources:
  spark_ds:
    class_name: Datasource
    execution_engine:
      module_name: great_expectations.execution_engine
      class_name: SparkDFExecutionEngine
      force_reuse_spark_context: true
    module_name: great_expectations.datasource
    data_connectors:
      spark_ds_connector:
        class_name: RuntimeDataConnector
        module_name: great_expectations.datasource.data_connector
        batch_identifiers:
          - batch_id
</code></pre>
<p>Create runtime batch request</p>
<pre><code>df=#Create your dataframe
request=RuntimeBatchRequest(
            datasource_name=&quot;spark_ds&quot;,
            data_connector_name=&quot;spark_ds_connector&quot;,
            data_asset_name=&quot;any_asset_name&quot;,  

            runtime_parameters={&quot;batch_data&quot;: df},  

            batch_identifiers={&quot;batch_id&quot;: &quot;batch_id&quot;},
        )
    
    
ge_context.run_checkpoint(checkpoint_name=&quot;checkpoint&quot;, validations=[{&quot;batch_request&quot;: request, &quot;expectation_suite_name&quot;: &quot;suite_name&quot;}])
</code></pre>
"
"73869461","How to filter rows that violates constraints deequ","<p>In order to do some unit test on my data I am using PyDeequ. Is there a way to filter out the rows which violate the defined constraints? I was not able to find anything online. Here is my code:</p>
<pre><code>df1 = (spark
       .read
       .format(&quot;csv&quot;)
       .option(&quot;header&quot;, &quot;true&quot;)
       .option(&quot;encoding&quot;, &quot;ISO-8859-1&quot;)
       .load(&quot;addresses.csv&quot;, sep = ','))

check = Check(spark, CheckLevel.Warning, &quot;Review Check&quot;)

checkResult = (VerificationSuite(spark)
    .onData(df1)
    .addCheck(
        check
        .isComplete(&quot;Nome&quot;)
        .isComplete(&quot;Citta&quot;)
        .isUnique(&quot;CAP&quot;)
        .isUnique(&quot;Number&quot;)
        .isContainedIn(&quot;Number&quot;, (&quot;11&quot;,&quot;12&quot;,&quot;13&quot;,&quot;14&quot;,&quot;15&quot;,&quot;16&quot;))
    )
    .run())

checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)
checkResult_df.show()
</code></pre>
","<python><data-quality><amazon-deequ>","2022-09-27 14:39:27","162","0","1","75538870","<p>Filtering for where <code>constraint_status</code> in <code>checkResult_df</code> is equal to <code>Failure</code> should be what you're looking for.</p>
<p>Building off the above example:</p>
<pre><code>from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes
from pydeequ.verification import VerificationResult, VerificationSuite
from pyspark.sql import functions as F

df1 = (spark
       .read
       .format(&quot;csv&quot;)
       .option(&quot;header&quot;, &quot;true&quot;)
       .option(&quot;encoding&quot;, &quot;ISO-8859-1&quot;)
       .load(&quot;addresses.csv&quot;, sep = ','))

check = Check(spark, CheckLevel.Warning, &quot;Review Check&quot;)

checkResult = (VerificationSuite(spark)
    .onData(df1)
    .addCheck(
        check
        .isComplete(&quot;Nome&quot;)
        .isComplete(&quot;Citta&quot;)
        .isUnique(&quot;CAP&quot;)
        .isUnique(&quot;Number&quot;)
        .isContainedIn(&quot;Number&quot;, (&quot;11&quot;,&quot;12&quot;,&quot;13&quot;,&quot;14&quot;,&quot;15&quot;,&quot;16&quot;))
    )
    .run())

checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)

# Added this snippet
# Filtering for any failed data quality constraints
df_checked_constraints_failures = \
    (checkResult_df
     .filter(F.col(&quot;constraint_status&quot;) == &quot;Failure&quot;))
</code></pre>
<p>It also might be helpful to alert or log these failures:</p>
<pre><code>import logging

logger = logging.getLogger(__name__)

# If any data quality check fails, log/raise exception/alert Slack
if df_checked_constraints_failures.count() &gt; 0:
    logger.info(
      df_checked_constraints_failures.show(n=df_checked_constraints_failures.count(),
                                           truncate=False)
    )
   # maybe raise exception here
   # maybe send POST message to Slack webhook for channel that monitors applications
</code></pre>
"
"73812304","Delta live tables data quality checks","<p>I'm using delta live tables from Databricks and I was trying to implement a complex data quality check (so-called expectations) by following <a href=""https://i.stack.imgur.com/zDR5e.png"" rel=""nofollow noreferrer"">this guide</a>. After I tested my implementation, I realized that even though the expectation is failing, the tables dependent downstream on the source table are still loaded.</p>
<p>To illustrate what I mean, here is an image describing the situation.</p>
<p><a href=""https://i.stack.imgur.com/zDR5e.png"" rel=""nofollow noreferrer"">Image of the pipeline lineage and the incorrect behaviour</a></p>
<p>I would assume that if the report_table fails due to the expectation not being met (in my case, it was validating for correct primary keys), then the Customer_s table would not be loaded. However, as can be seen in the photo, this is not quite what happened.</p>
<p>Do you have any idea on how to achieve the desired result? How can I define a complex validation with SQL that would cause the future nodes to not be loaded (or it would make the pipeline fail)?</p>
","<databricks><delta-live-tables>","2022-09-22 09:28:42","580","1","1","73923489","<p>The default behavior when expectation violation occurs in Delta Live Tables is to load the data but track the data quality metrics (retain invalid records). The other options are : ON VIOLATION DROP ROW and ON VIOLATION FAIL UPDATE. Choose &quot;ON VIOLATION DROP ROW&quot; if that is the behavior you want in your pipeline.
<a href=""https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html#drop-invalid-records"" rel=""nofollow noreferrer"">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html#drop-invalid-records</a></p>
"
"73646830","What inference can be made out of Baseline drift distance in data quality monitoring sagemaker?","<p><a href=""https://i.stack.imgur.com/GU4v0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GU4v0.png"" alt=""enter image description here"" /></a></p>
<p>How is the above distance calculated and can I set my own threshold?</p>
","<amazon-web-services><amazon-sagemaker>","2022-09-08 09:35:36","120","0","1","73666111","<p>You can find information on how the distributions are compared here (see <code>distribution_constraints</code> in the table):</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html</a></p>
<p>You can change the threshold in the constraint file to what you would like.</p>
<p>The baseline computes baseline schema constraints and statistics for each feature using Deequ. If youwould like more details you can take a look at the implementation here:</p>
<p><a href=""https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/Distance.scala"" rel=""nofollow noreferrer"">https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/Distance.scala</a></p>
"
"73580807","Data governance Snowflake unload/copy to export data","<p>I perform data unload form Snowflake to s3 or by using Snowql localy.
I'd like to know if there's any kind of data tracing (for data governance) to always record or tag and save somewhere in Snowflake that a data was unloaded.
Thanks</p>
","<snowflake-cloud-data-platform><data-governance>","2022-09-02 09:55:26","35","1","1","73583724","<p><code>COPY INTO &lt;location&gt;</code> used for data unloading will leave trace in
<a href=""https://docs.snowflake.com/en/sql-reference/account-usage/access_history.html"" rel=""nofollow noreferrer"">ACCESS_HISTORY</a>:</p>
<blockquote>
<p>This Account Usage view can be used to query the access history of Snowflake objects (e.g. table, view, column) within the last 365 days (1 year).</p>
<p>This view supports write operations of the following type:</p>
<p>Data unloading statements:</p>
<ul>
<li>COPY INTO internalStage FROM TABLE</li>
<li>COPY INTO externalStage FROM TABLE</li>
<li>COPY INTO externalLocation FROM TABLE</li>
</ul>
</blockquote>
<p>and <a href=""https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html"" rel=""nofollow noreferrer"">QUERY_HISTORY</a></p>
<hr />
<p>For data metering perspective <a href=""https://docs.snowflake.com/en/sql-reference/account-usage/data_transfer_history.html"" rel=""nofollow noreferrer"">DATA_TRANSFER_HISTORY</a>:</p>
<blockquote>
<p>This Account Usage view can be used to query the history of data transferred from Snowflake tables into a different cloud storage providerâ€™s network (i.e. from Snowflake on AWS, Google Cloud Platform, or Microsoft Azure into the other cloud providerâ€™s network) and/or geographical region within the last 365 days (1 year).</p>
</blockquote>
"
"73570260","How Can I Attach Policy Tags to columns using Python API","<p>As a part of data governance, we have created Taxonomies, Policy Tags Using &quot;Python API&quot;. And I am trying to Assign Policy Tags to Columns [Name, Age] for a table <code>Project.Dataset.TMP_TBL</code>.
Looked across the GCP Documentation but couldn't find any code snippets of Python to do this.
Please Help me out with and Example code Snippet to do so.</p>
","<google-cloud-platform><google-bigquery><google-api-python-client><google-data-catalog><data-governance>","2022-09-01 13:33:46","64","1","1","73576768","<p>You can use <a href=""https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html#google.cloud.bigquery.client.Client.update_table"" rel=""nofollow noreferrer"">update_table()</a> to assign policy tags to columns. When updating a policy tag for a specific column see code below:</p>
<pre class=""lang-py prettyprint-override""><code>from google.cloud import bigquery
from google.cloud.bigquery.schema import SchemaField,PolicyTagList
client = bigquery.Client()

table_id = 'project_id.dataset_id.table_id'

policy_tags = PolicyTagList(names=[&quot;projects/&lt;my_project_id&gt;/locations/us/taxonomies/&lt;taxonomy_id&gt;/policyTags/&lt;policy_tag_id&gt;&quot;])
table = client.get_table(table_id)

table.schema = [
        SchemaField(
            name=&quot;name&quot;,
            field_type=&quot;STRING&quot;,
            policy_tags=policy_tags
            ),
        SchemaField(
            name=&quot;age&quot;,
            field_type=&quot;INT64&quot;
            )
        ]

table = client.update_table(table=table,fields=[&quot;schema&quot;])
print(table.schema)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/qvvpu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qvvpu.png"" alt=""enter image description here"" /></a></p>
<p>NOTE: For example you have a table with 2 columns (<code>name</code>, <code>age</code>). If you want to update the <code>policy_tag</code> for <code>name</code> only you can do so as shown with the code above. But you also need to redefine <code>age</code> so it won't error out.</p>
"
"73443328","How to UPIVOT all columns in a table and aggregate into Data Quality/ Validation Metrics? SQL SNOWFLAKE","<p>I have a table with 60+ columns in it that I would like to UNPIVOT so that each column becomes a row and then find the fill rate, min value and max value of each entry.</p>
<p>For Example</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>START_DATE</th>
<th>END_DATE</th>
<th>EVENT_ID</th>
<th>PROVIDER_CODE</th>
</tr>
</thead>
<tbody>
<tr>
<td>01</td>
<td>01/23/21</td>
<td>03/14/21</td>
<td>0023401</td>
<td>0012323</td>
</tr>
<tr>
<td>02</td>
<td>06/04/21</td>
<td>09/20/21</td>
<td>0025906</td>
<td>0023454</td>
</tr>
<tr>
<td>03</td>
<td>07/20/21</td>
<td>12/02/21</td>
<td>0027093</td>
<td>0034983</td>
</tr>
</tbody>
</table>
</div>
<p>And I want the output to look like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column_Name</th>
<th>Fill_Rate</th>
<th>Min</th>
<th>Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID</td>
<td>0.7934</td>
<td>01</td>
<td>03</td>
</tr>
<tr>
<td>Start_Date</td>
<td>0.6990</td>
<td>01/23/21</td>
<td>07/20/21</td>
</tr>
<tr>
<td>End_Date</td>
<td>0.9089</td>
<td>03/14/21</td>
<td>12/02/21</td>
</tr>
<tr>
<td>Event_ID</td>
<td>1.0000</td>
<td>0023401</td>
<td>0027093</td>
</tr>
</tbody>
</table>
</div>
<p>Struggling to get the desired output, especially because of different data types in the different columns</p>
<p>i tried doing the following, but it doesn't allow taking the agg functions within the unpivot</p>
<pre><code>select *
from &quot;DSVC_MERCKPAN_PROD&quot;.&quot;COHORTS_LATEST&quot;.&quot;MEDICAL_HEADERS&quot;
UNPIVOT (
max(code) as max_value,
min(code) as min_value,
avg(code) as fill_rate,
code as column_name
)
</code></pre>
<p>For fill rate, I was trying to use this logic as ID is always populated so it has the total number of rows, however the other columns can be null</p>
<pre><code> (COUNT_IF(start_date is not null))/(COUNT_IF(ID is not null))) as FILL_RATE,
</code></pre>
","<sql><validation><snowflake-cloud-data-platform><unpivot><data-quality>","2022-08-22 10:00:53","175","0","1","73469976","<p>I have 2 ideas to implement the report.</p>
<p>The first way is casting all values to <code>VARCHAR</code> and then using <code>UNPIVOT</code>:</p>
<pre class=""lang-sql prettyprint-override""><code>-- Generate dummy data
create or replace table t1 (c1 int, c2 int, c3 int, c4 int, c5 int, c6 int, c7 int, c8 int, c9 int, c10 int) as
select
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null)
from table(generator(rowcount =&gt; 1000000000))
;

-- Query
with
cols as (
    select column_name, ordinal_position
    from information_schema.columns
    where table_catalog = current_database()
    and table_schema = current_schema()
    and table_name = 'T1'
),
stringified as (
    select
        c1::varchar c1, c2::varchar c2, c3::varchar c3, c4::varchar c4, c5::varchar c5,
        c6::varchar c6, c7::varchar c7, c8::varchar c8, c9::varchar c9, c10::varchar c10
    from t1
),
data as (
    select column_name, column_value
    from stringified
    unpivot(column_value for column_name in (c1, c2, c3, c4, c5, c6, c7, c8, c9, c10))
)
select
    c.column_name,
    count(d.column_value)/(select count(*) from t1) fill_rate,
    min(d.column_value) min,
    max(d.column_value) max
from cols c
left join data d using (column_name)
group by c.column_name, c.ordinal_position
order by c.ordinal_position
;
/*
COLUMN_NAME FILL_RATE   MIN MAX
C1  0.500000    -1000000069270747870    999999972962694409
C2  0.499980    -1000000027928146782    999999946877079818
C3  0.499996    -1000000012155323098    999999942281548701
C4  0.500017    -1000000056353213091    999999946421698482
C5  0.500015    -1000000015608859996    999999993977648967
C6  0.500003    -1000000007081089270    999999998851014730
C7  0.499987    -100000008605944993 999999968272328033
C8  0.499992    -1000000042470913027    999999977402822725
C9  0.500011    -1000000058928465662    999999969060696774
C10 0.500029    -1000000011306371004    99999996061390938
*/
</code></pre>
<p>It's a straightforward way, but it still needs to list up all column names twice and it's a bit tough in the case the number of columns is very massive (but I believe it's much better than a huge UNION ALL query).</p>
<hr />
<p>Another solution is a bit tricky, but you can unpivot a table by using <code>OBJECT_CONSTRUCT(*)</code> aggregation if the row length doesn't exceed a VARIANT value limit (16 MiB):</p>
<pre class=""lang-sql prettyprint-override""><code>with
cols as (
    select column_name, ordinal_position
    from information_schema.columns
    where table_catalog = current_database()
    and table_schema = current_schema()
    and table_name = 'T1'
),
data as (
    select f.key column_name, f.value::varchar column_value
    from (select object_construct(*) rec from t1) up,
    lateral flatten(up.rec) f
)
select
    c.column_name,
    count(d.column_value)/(select count(*) from t1) fill_rate,
    min(d.column_value) min,
    max(d.column_value) max
from cols c
left join data d using (column_name)
group by c.column_name, c.ordinal_position
order by c.ordinal_position
;

/*
COLUMN_NAME FILL_RATE   MIN MAX
C1  0.500000    -1000000069270747870    999999972962694409
C2  0.499980    -1000000027928146782    999999946877079818
C3  0.499996    -1000000012155323098    999999942281548701
C4  0.500017    -1000000056353213091    999999946421698482
C5  0.500015    -1000000015608859996    999999993977648967
C6  0.500003    -1000000007081089270    999999998851014730
C7  0.499987    -100000008605944993 999999968272328033
C8  0.499992    -1000000042470913027    999999977402822725
C9  0.500011    -1000000058928465662    999999969060696774
C10 0.500029    -1000000011306371004    99999996061390938
*/
</code></pre>
<p><code>OBJECT_CONSTRUCT(*)</code> aggregation is a special usage of the <code>OBJECT_CONSTRUCT</code> function that extracts column names as a key of each JSON object. As far as I know, this is the only way to extract column names from a table along with values in a programmatic way.</p>
<p>Since <code>OBJECT_CONSTRUCT</code> is relatively a heavy operation, it usually takes a longer time than the first solution, but you don't need to write all column names with this trick.</p>
"
"73280033","Provide aws credentials to Airflow GreatExpectationsOperator","<p>I would like to use GreatExpectationsOperator to perform data quality validations.</p>
<p>The validation results data should be stored in S3.
I don't see an option to send an airflow connection name to the GE operator, and the AWS credentials in my organization are stored in an airflow connection.</p>
<p>How can great expectations retrieve s3 credentials from airflow connection? and not from the default aws credentials in .aws dir?</p>
<p>Thanks!</p>
","<airflow><data-quality><great-expectations>","2022-08-08 15:03:11","178","0","1","73410609","<p>We ended up creating a new oprator that inherit from GE operator and the operator get the connection as part of its ecxeute method.</p>
"
"73207861","How to change the way Talend formulates SQL queries in a JDBC connection?","<p>In Talend Data Quality, I have configured a JDBC connection to an OpenEdge database and it's working fine.</p>
<p>I can pull the list of tables and select columns to analyse, but when executing analysis, I get this :</p>
<p><strong>Table &quot;DBGSS.SGSSGSS&quot; cannot be found.</strong></p>
<p>This is because it does not specify a schema, only the database name - <strong>DBGSS</strong>.</p>
<p>How can I make it specify database, <strong>schema</strong> and then the table name ? Or just the table name, its would work too.</p>
<p>Thanks !</p>
","<java><jdbc><talend><data-quality>","2022-08-02 12:50:17","51","0","1","73218274","<p>You can use a <code>tDBConnection </code> component that give you the right to specify a schÃ©ma</p>
<p>Then , use it with the option of <code>Use Existing connection</code></p>
<p>See below documentation , <a href=""https://help.talend.com/r/en-US/7.3/db-generic/tdbconnection"" rel=""nofollow noreferrer"">https://help.talend.com/r/en-US/7.3/db-generic/tdbconnection</a></p>
"
"73124632","Oracle DB trigger-based synchronization error in history store table type 0?","<p>We are doing a <em>trigger-based</em> synch with DB Convert studio tool from Oracle as a source. This method creates triggers on each table and a <code>history_store</code> table.</p>
<p>But, the software that uses the source oracle DB gives the following error and refuses to work:</p>
<blockquote>
<p>Unknown type (table: history_store, column: pk_date_dest, type: 0)</p>
</blockquote>
<p>The <code>pk_date_dest</code> column is initialized as <code>&quot;pk_date_dest&quot; NVARCHAR2(400) NOT NULL</code>. What might be the reason for this <em>type 0</em>?</p>
","<oracle><types><triggers><data-synchronization>","2022-07-26 13:51:02","40","0","1","73138360","<p>I changed manually the <code>NVARCHAR2</code> data type of the columns to <code>VARCHAR2</code> and the error stopped appearing. Quite strange to be honest, as there are <code>NVARCHAR2</code> data types in other tables that are native to the database, but... at least that fixed it.</p>
<pre><code>ALTER TABLE tablename MODIFY (&quot;pk_date_dest&quot; VARCHAR2(400));
</code></pre>
"
"72913371","Catalogs in Databricks","<p>I have started reading about the Unity Catalog that Databricks has introduced. I understand the basic issue that it is trying to solve, but I do not understand what exactly a Catalog is.</p>
<p>This was available in the Databricks documentation,</p>
<blockquote>
<p>A catalog contains schemas (databases), and a schema contains tables and views.</p>
</blockquote>
<p><a href=""https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html</a></p>
<p>How does this added layer (on top of schemas) help? I am guessing it has something to do with governance?</p>
<p>I would really appreciate an example, if possible.</p>
","<databricks><catalog><databricks-unity-catalog><data-governance>","2022-07-08 15:02:39","1035","4","1","72915616","<p>Really, Catalog is an another data management layer inside the bigger objects - Unity Catalog Metastore.  Closest analogy of the Catalog is a single Hive Metastore - it's also contains databases (schemas) that contain tables and views. Catalogs could be used to isolate objects of some entity (business unit/project/environments (dev,stagin,prod)/...) from objects of other entities.  You can give manage permissions of the catalogs to respective admins of the business units, projects, ..., and they can then assign permissions on individual schemas and tables/views.</p>
"
"72773732","Is ObjectBox pragmatic for multi-vendor saas application in terms of data sync and storage","<p>I am exploring the options to start working on a hotel management app that many hotel owners can download, create an account and use on their front desk. Offline support of ObjectBox makes it an attractive option. But I have this question:</p>
<blockquote>
<p>Lets say a Hotel A lists their rooms in my app, and uses the app to book reservations, track inventory, and generate reports. Let's say Hotel B and C do the same on using their own accounts.</p>
</blockquote>
<p>Does data syncing mean that all the data from all accounts (from all the hotels) is automatically synced to when internet is available, does that not mean that all connected devices will download data (by way of syncing) from other users? How does OB decide how much of the data is to be synced?</p>
","<synchronization><data-synchronization><objectbox>","2022-06-27 14:25:15","40","0","1","72786249","<p>When using ObjectBox Sync all data is currently shared. Sync will make sure each client has all of the data the server has (and changes of a client are distributed to all other clients).</p>
<p>To handle your use case would require to run a separate Sync server instance for each customer.</p>
<p>Note: there is still <a href=""https://sync.objectbox.io/sync-client#authentication-options"" rel=""nofollow noreferrer"">authentication</a> (via shared secret or Google auth).</p>
<p>For more technical details see the <a href=""https://sync.objectbox.io/"" rel=""nofollow noreferrer"">docs</a>. There is also <a href=""https://objectbox.io/sync/"" rel=""nofollow noreferrer"">a website</a> and you can email us to talk about details.</p>
"
"72588558","Prevent WaitForSingleObject from hanging on a named semaphore when other process is terminated?","<p>When using a globally named mutex to synchronize across two processes, and one of the two processes are killed (say in Task Manager, or due to a fault), the other process returns from <code>WaitForSingleObject()</code> with the appropriate error code and can continue.</p>
<p>When using a globally name semaphore, it does not release the waiting process if the other process is killed / terminated.  <code>WaitForSingleObject()</code> will wait until it times out (which may be <code>INFINITE</code> or hours).</p>
<p>How do I stop <code>WaitForSingleObject()</code> from waiting when the other process is killed or terminated?</p>
<p>In this case, there is a single count on the semaphore used to control read/write requests of a shared buffer.  The Requester signals the Provider to provide certain data, the Provider updates the buffer and signals back to the Requester that it can now read the buffer.</p>
","<winapi><synchronization><semaphore><data-synchronization>","2022-06-11 23:34:01","65","0","2","72588928","<p>I suggest that you switch to using WaitForMultipleObjects and wait for the handle of the process that might get terminated (or thread if you want to do this within a single process) in addition to your semaphore handle. That way you can continue to use INFINITE timeouts. Just have to check the return value to see which object was signalled.</p>
<p>Also, I would consider a process terminating while holding a semaphore somewhat of a bug, particularly a semaphore used for actual inter-process communication.</p>
"
"72588558","Prevent WaitForSingleObject from hanging on a named semaphore when other process is terminated?","<p>When using a globally named mutex to synchronize across two processes, and one of the two processes are killed (say in Task Manager, or due to a fault), the other process returns from <code>WaitForSingleObject()</code> with the appropriate error code and can continue.</p>
<p>When using a globally name semaphore, it does not release the waiting process if the other process is killed / terminated.  <code>WaitForSingleObject()</code> will wait until it times out (which may be <code>INFINITE</code> or hours).</p>
<p>How do I stop <code>WaitForSingleObject()</code> from waiting when the other process is killed or terminated?</p>
<p>In this case, there is a single count on the semaphore used to control read/write requests of a shared buffer.  The Requester signals the Provider to provide certain data, the Provider updates the buffer and signals back to the Requester that it can now read the buffer.</p>
","<winapi><synchronization><semaphore><data-synchronization>","2022-06-11 23:34:01","65","0","2","72589494","<p>Adding to the accepted answer.</p>
<p>I added logic if the <code>waitms</code> was going to be longer than some value <code>maxwaitms</code> then the requester/provider exchange the providers process id (<code>GetCurrentProcessId()</code>) before the long process.  The requester opens a handle (<code>OpenHandle()</code>) to the provider process and waits on both the semaphore and the process handle to know when writing is done (or process terminated).</p>
"
"72558498","Is it sensible to use symfony as native app api?","<p>Good day,</p>
<p>I have a question for the experienced developers:</p>
<p>At the moment I work a lot with the PHP framework Symfony.</p>
<p>Out of interest, I would now like to delve into the topic of native app development
using React.
As part of a practice project, I want to transfer/sync data between a SQL DB on a server and the app.
My question is, is it a good way to write a symfony application for this,
which only acts as an API for the database?</p>
<ul>
<li>Does this make sense from a performance and effort point of view?</li>
<li>What alternative ways are there?</li>
<li>Which ways of storing data on servers are used most frequently in the productive environment?</li>
</ul>
<p>I am happy about suggestions, links and informations in every direction.</p>
<p>Thanks and Greetings</p>
","<reactjs><database><api><symfony><data-synchronization>","2022-06-09 10:10:26","74","-1","1","72560181","<p>My answers (based on my experiences and my collegues answers) :</p>
<ul>
<li>As you can see on the documentation <a href=""https://symfony.com/doc/current/the-fast-track/en/26-api.html"" rel=""nofollow noreferrer"">here</a> it's very easy to made your own api with Symfony tkants to API Platform. For the performance it's very acceptable especially if you use Symfony (&gt;= 5.4) because a lot of cleaning has been done in the kernel and PHP 8 for its performance improvements especially at compile time (JIT compiletor). More info <a href=""https://developers.ibexa.co/blog/benchmarks-php-7.4-8.0-jit-opcache-preloading-symfony"" rel=""nofollow noreferrer"">here</a> if you need it.</li>
<li>Alternatively you can create your API rest with NodeJS but it doesn't bring much especially if your application is already made in PHP. Adding a layer can sometimes make things heavier instead of lighter.</li>
<li>It's depend of your need, the size, the number of users ... You have to determine the target to choose the better solution. If you have it already I can help you.</li>
</ul>
"
"72457676","Great Expectations list total unique values","<p>I have run Great Expectation check expect_column_values_to_be_unique check on one of the column. It produced the following result as below.Total There are 62 Duplicates but in the output list it is returning only 20 elements. How to retrieve all duplicate records in that column.
<code>df.expect_column_values_to_be_unique('A')</code></p>
<pre><code>  &quot;exception_info&quot;: null,
  &quot;expectation_config&quot;: {
    &quot;expectation_type&quot;: &quot;expect_column_values_to_be_unique&quot;,
    &quot;kwargs&quot;: {
      &quot;column&quot;: &quot;A&quot;,
      &quot;result_format&quot;: &quot;BASIC&quot;
    },
    &quot;meta&quot;: {}
  },
  &quot;meta&quot;: {},
  &quot;success&quot;: false,
  &quot;result&quot;: {
    &quot;element_count&quot;: 100,
    &quot;missing_count&quot;: 0,
    &quot;missing_percent&quot;: 0.0,
    &quot;unexpected_count&quot;: 62,
    &quot;unexpected_percent&quot;: 62.0,
    &quot;unexpected_percent_nonmissing&quot;: 62.0,
    &quot;partial_unexpected_list&quot;: [
      37,
      62,
      72,
      53,
      22,
      61,
      95,
      21,
      64,
      59,
      77,
      53,
      0,
      22,
      24,
      46,
      0,
      16,
      78,
      60
    ]
  }
}
</code></pre>
","<python><pyspark><apache-spark-sql><data-quality><great-expectations>","2022-06-01 07:11:03","703","0","2","72462496","<p>I think you are using &quot;<a href=""https://sparkbyexamples.com/spark/spark-show-full-column-content-dataframe/"" rel=""nofollow noreferrer"">show</a>&quot; without parameters.  By default this only shows the first 20 rows.  If you wish to see more you need to pass in how many rows you want to see: (This will show you 200 rows, and not truncate the length of the column)</p>
<pre><code>df.select( col(&quot;*&quot;) ).show(200,false)
</code></pre>
"
"72457676","Great Expectations list total unique values","<p>I have run Great Expectation check expect_column_values_to_be_unique check on one of the column. It produced the following result as below.Total There are 62 Duplicates but in the output list it is returning only 20 elements. How to retrieve all duplicate records in that column.
<code>df.expect_column_values_to_be_unique('A')</code></p>
<pre><code>  &quot;exception_info&quot;: null,
  &quot;expectation_config&quot;: {
    &quot;expectation_type&quot;: &quot;expect_column_values_to_be_unique&quot;,
    &quot;kwargs&quot;: {
      &quot;column&quot;: &quot;A&quot;,
      &quot;result_format&quot;: &quot;BASIC&quot;
    },
    &quot;meta&quot;: {}
  },
  &quot;meta&quot;: {},
  &quot;success&quot;: false,
  &quot;result&quot;: {
    &quot;element_count&quot;: 100,
    &quot;missing_count&quot;: 0,
    &quot;missing_percent&quot;: 0.0,
    &quot;unexpected_count&quot;: 62,
    &quot;unexpected_percent&quot;: 62.0,
    &quot;unexpected_percent_nonmissing&quot;: 62.0,
    &quot;partial_unexpected_list&quot;: [
      37,
      62,
      72,
      53,
      22,
      61,
      95,
      21,
      64,
      59,
      77,
      53,
      0,
      22,
      24,
      46,
      0,
      16,
      78,
      60
    ]
  }
}
</code></pre>
","<python><pyspark><apache-spark-sql><data-quality><great-expectations>","2022-06-01 07:11:03","703","0","2","72465801","<p>You're currently passing <code>result_format</code> as <code>BASIC</code>. To get the level of detail you're looking for, you'll want to instead pass <code>result_format</code> for this Expectation as <code>COMPLETE</code> to get the full list of unexpected values. For example:</p>
<pre><code>df.expect_column_values_to_be_unique(column=&quot;A&quot;, result_format=&quot;COMPLETE&quot;)
</code></pre>
<p>See <a href=""https://docs.greatexpectations.io/docs/reference/expectations/result_format"" rel=""nofollow noreferrer"">this documentation</a> for more on <code>result_format</code>.</p>
"
"72404368","Getting data quality in Delta Live Table (bronze, gold, silver..)","<p>How to check if Delta Live Table is in bronze, gold or silver layer(zone) with python? I have notebook for creating Delta Live Table pipeline, and I need to know what is quality of data(silver, bronze, gold). How to get that information with python?</p>
<p>In SQL exist something like <code>TBLPROPERTIES('BRONZE')</code>, is there anything like that for python?</p>
","<python><databricks><azure-databricks><delta-live-tables>","2022-05-27 11:01:21","533","1","2","72405660","<p>If you have a sql solution run it directly with</p>
<pre class=""lang-py prettyprint-override""><code>spark.sql('''SHOW TBLPROPERTIES table_name'')
</code></pre>
"
"72404368","Getting data quality in Delta Live Table (bronze, gold, silver..)","<p>How to check if Delta Live Table is in bronze, gold or silver layer(zone) with python? I have notebook for creating Delta Live Table pipeline, and I need to know what is quality of data(silver, bronze, gold). How to get that information with python?</p>
<p>In SQL exist something like <code>TBLPROPERTIES('BRONZE')</code>, is there anything like that for python?</p>
","<python><databricks><azure-databricks><delta-live-tables>","2022-05-27 11:01:21","533","1","2","72953171","<p>You can do something like this:</p>
<pre><code>@dlt.table(
  comment=&quot;Bronze live streaming table for Test data&quot;,
  name=&quot;bronze_test_table&quot;,
  table_properties={
    &quot;quality&quot;: &quot;bronze&quot;
  }
)
</code></pre>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72319352","<p>Well, you've got readers, and you've got writers, and you need a lock, so.... how about <a href=""https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock"" rel=""nofollow noreferrer"">a readers/writer lock</a>?</p>
<p>The reason I mention that up-front is because (a) you might not be aware of it, but more importantly (b) there's no standard RW lock in C++ (EDIT: my mistake, one was added in C++14), so your thinking about this is perhaps being done in the context of std::mutex. Once you've decided to go with a RW lock, you can benefit from other people's thinking about those locks.</p>
<p>In particular, there's a number of different options for prioritizing threads contending over RW locks. With one option, a thread acquiring a write lock waits until all current reader threads drop the lock, but readers who start waiting after the writer don't get the lock until the writer's done with it.</p>
<p>With that strategy, as long as the writer thread releases and reacquires the lock after each transaction, and as long as the writer completes each transaction within your 1 ms target, readers don't starve.</p>
<p>And if your writer <em>can't</em> promise that, then there is zero alternative but to redesign the writer: either doing more processing <em>before</em> acquiring the lock, or splitting a transaction into multiple pieces where it's safe to drop the lock between each.</p>
<p>If, on the other hand, your writer's transactions take <em>much less</em> than 1 ms, then you might <em>consider</em> skipping the release/reacquire between each one if less than 1 ms has elapsed (purely to reduce the processing overhead of doing so).... but I wouldn't advise it. Adding complexity and special cases and (shudder) <em>wall clock time</em> to your implementation is rarely the most practical way to maximize performance, and rapidly increases the risk of bugs. A simple multithreading system is a reliable multithreading system.</p>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72322472","<p>Normally in such a scenario you use a <a href=""https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock"" rel=""nofollow noreferrer"">reader-writer-lock</a>. This allows either a read by all readers in parallel or a write by a single writer.</p>
<p>But that does nothing to stop a writer from holding the lock for minutes if it so desires. Forcing the writer out of the lock is probably also not a good idea. The object is probably in some inconsistent state mid changed.</p>
<p>There is another synchronization method called <a href=""https://en.wikipedia.org/wiki/Read-copy-update"" rel=""nofollow noreferrer"">read-copy-update</a> that might help. This allows writers to modify element without being blocked by readers. The drawback is that you might get some readers still reading the old data and others reading the new data for some time.</p>
<p>It also might be problematic with multiple writers if they try to change the same member. The slower writer might have computed all the needed updates only to notice some other thread changes the object. It then has to start over wasting all the time it already spend.</p>
<p>Note: copying the element can be done in constant time, certainly under 1ms. So you can guarantee readers are never blocked for long. By releasing the write lock first you guarantee readers to read between any 2 writes, assuming the RW lock is designed with the same principle.</p>
<hr />
<p>So I would suggest another solution I call write-intent-locking:</p>
<p>You start with a RW lock but add a lock to handle write-intent. Any writer can acquire the write-intent lock at any time, but only one of them, it's exclusive. Once a write holds the write-intent lock it copies the element
and starts modifying the copy. It can take as long as it wants to do that as it's not blocking any readers. It does block other writers though.</p>
<p>When all the modifications are done the writer acquires the write lock and then quickly copies, moves or replaces the element with the prepared copy. It then releases the write and write-intent lock, unblocking both the readers and writers that want to access the same element.</p>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72322687","<p>The way I would approach this is to have two identical copies of the dataset; call them copy A and copy B.</p>
<p>Readers always read from copy B, being careful to lock a reader/writer lock in read-only mode before accessing it.</p>
<p>When a writer-thread wants to update the dataset, it locks copy A (using a regular mutex) and updates it.  The writer-thread can take as long as it likes to do this, because no readers are using copy A.</p>
<p>When the writer-thread is done updating copy A, it locks the reader/writer lock (in exclusive/writer-lock mode) and swaps dataset A with dataset B.  (This swap should be done by exchanging pointers, and is therefore O(1) fast).</p>
<p>The writer-thread then unlocks the reader/writer-lock (so that any waiting reader-threads can now access the updated data-set), and then updates the other data-set the same way it updated the first data-set.  This can also take as long as the writer-thread likes, since no reader-threads are waiting on this dataset anymore.</p>
<p>Finally the writer-thread unlocks the regular mutex, and we're done.</p>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72324685","<p>If model allows writing to be interrupted, then it also allows buffering. Use a fifo queue and start reading only when there are 50 elements written already. Use (smart)pointers to swap data in fifo queue. Swapping 8 bytes of pointer takes nanoseconds. Since there is buffering, writing will be on a different element than readers are working with so there wont be lock contention as long as producer can keep the pace with producers.</p>
<hr />
<p>Why doesn't the reader produce its own consumer data? If you can have n producers and n consumers, each consumer can produce its own data too, without any producer. But this will have different multithread scaling. Maybe your algorithm is not applicable here but if it is, it would be more like independent multi-processing instead of multi-threading.</p>
<hr />
<p>Writer work can be converted to multiple smaller jobs? Progress within writer can be reported to an atomic counter. When a reader has a waiting budget, it checks atomic value and if it looks slow, it can use same atomic value to instantly push it to 100% progress and writer sees it and early-quits lock.</p>
"
"72298345","Xamarin IOS/Android DOTMIM SYNC compatible with Azure Sql","<p>I have an Xamarin cross platform application.  I am currently storing my data in a Sqlite Database.  However we want to have bi-directional syncing with an Azure Sql Database.  After much reading I decided to give Dotmim Sync a try.  The initial sync worked, however when adding a column to a table and attempting to migrate the data (following the tutorial), I got an error stating that the tracking table was missing.  I redid everything again and realized that the entity tracking table was never created and I am not sure why.  However Sql created a tracking table but it was not the entity tracking table that the error stated was missing.</p>
<p>I am curious if anyone with Xamarin has been able to successfully create bi-directional syncing with Sqlite and Azure Sql using Dotmim Sync.  I have yet to find anything else that will work.  Other than hand jamming it in this tutorial:  <a href=""https://www.xamarinhelp.com/mobile-database-bi-directional-synchronization-rest-api/"" rel=""nofollow noreferrer"">https://www.xamarinhelp.com/mobile-database-bi-directional-synchronization-rest-api/</a></p>
<p>I am not against that, just seems like a lot of room for error.  I am hoping someone out there has had success with what I am trying to do.</p>
","<database><sqlite><xamarin.forms><azure-sql-database><data-synchronization>","2022-05-19 03:36:16","215","0","1","72950103","<p>Hello I'm using Dotmim sync to synchronize a Sql Server database with a Sqlite database hotsted in a xamarin forms application through an API.
I think that I had the same problem as yours.
The problem is, as far as I understood, that the tracking tables are created only on the first sync.
If you change the schema of those tables you will need to call the Deprovision method.
This method will re-create the stored procedures and the triggers with the new database schema.
I'll leave you the link to the docs:
<a href=""https://dotmimsync.readthedocs.io/Provision.html#provision-deprovision"" rel=""nofollow noreferrer"">https://dotmimsync.readthedocs.io/Provision.html#provision-deprovision</a>.</p>
"
"72198976","Azure Purview Data Lineage with Databricks","<p>I am using Azure Purview for Data Governance, and Data Lineage. We use Databricks in our Data Architecture, but there isn't any native support for capturing Data Lineage with Databricks.</p>
<p>I found the following links that will allow you to create custom processes in Azure Purview.</p>
<p><a href=""https://stackoverflow.com/questions/69365553/databricks-notebooks-lineage-in-azure-purview"">Databricks notebooks lineage in Azure Purview</a></p>
<p>Can someone let me know if there is any recent methods of achieving Data Lineage in Azure Purview with Databricks?</p>
","<azure-databricks><azure-purview><data-governance>","2022-05-11 09:54:02","1301","0","1","72273266","<p>Data integration and ETL tools can push lineage into Microsoft Purview at execution time. Tools such as Data Factory, Data Share, Synapse, Azure Databricks, and so on, belong to this category of data processing systems. The data processing systems reference datasets as source from different databases and storage solutions to create target datasets. The list of data processing systems currently integrated with Microsoft Purview for lineage are listed in below table.</p>
<p><a href=""https://i.stack.imgur.com/FSaJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FSaJd.png"" alt=""enter image description here"" /></a></p>
<p>Refer - <a href=""https://learn.microsoft.com/en-us/azure/purview/catalog-lineage-user-guide#data-processing-systems"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/purview/catalog-lineage-user-guide#data-processing-systems</a></p>
<hr />
<p><strong>EDIT:</strong> July 2022 - Since this question was answered, the Microsoft Purview team released an open source solution accelerator to extract lineage from Databricks and ingest it into Microsoft Purview: <a href=""https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator"" rel=""nofollow noreferrer"">A connector to ingest Azure Databricks lineage into Microsoft Purview</a> (github.com)</p>
<p>This solution accelerator, together with the <a href=""https://openlineage.io/"" rel=""nofollow noreferrer"">OpenLineage</a> project, provides a connector that will transfer lineage metadata from Spark operations in Azure Databricks to Microsoft Purview, allowing you to see a table-level lineage graph. It supports Delta, Azure SQL, Data Lake Gen 2, and more.</p>
"
"72110716","Spark Compatible Data Quality Framework for Narrow Data","<p>I'm trying to find an appropriate data quality framework for very large amounts of time series data in a <a href=""https://en.wikipedia.org/wiki/Wide_and_narrow_data"" rel=""nofollow noreferrer"">narrow</a> format.</p>
<p>Image billions of rows of data that look kinda like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Sensor</th>
<th>Timestamp</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>12251</td>
<td>12</td>
</tr>
<tr>
<td>B</td>
<td>12262</td>
<td>&quot;A&quot;</td>
</tr>
<tr>
<td>A</td>
<td>12261</td>
<td>13</td>
</tr>
<tr>
<td>A</td>
<td>12271</td>
<td>13</td>
</tr>
<tr>
<td>C</td>
<td>12273</td>
<td>5.4545</td>
</tr>
</tbody>
</table>
</div>
<p>There are hundreds of thousands of sensors, but for each timestamp only a very small percentage send values.</p>
<p>I'm building  Data Quality Monitoring for this data that checks some expectations about the values (e.g. whether the value falls within the expected range for a given sensor, there are tens of thousands of different expectations). Due to the size of the data and existing infrastructure the solution has to be run on Spark.  I would like to build this solution on an (ideally open source) data quality framework, but cannot find anything appropriate.</p>
<p>I've looked into Great Expectations and Deequ, but these fundamentally seem to be build for &quot;wide data&quot; where the expectations are defined for columns. I could theoretically reshape (pivot) my data to this format, but it would be a very expensive operation and result in an extremly sparse table that is awkward to work with (or require sampling on the time and in this way a loss of information).</p>
<p>Does anyone know of an existing (spark compatible) framework for such time series data in narrow format? Or can point me to best practices how to apply Deequ/Great Expectations in such a setting?</p>
","<apache-spark><data-quality><great-expectations><amazon-deequ>","2022-05-04 09:29:37","177","0","1","74200703","<p>Have you tried <code>github.com/canimus/cuallee</code>
It is an open-source framework, that supports the Observation API to make testing on billions of records, super-fast, and less resource greedy as pydeequ. Is intuitive, and easy to use.</p>
"
"72048628","How to ensure data synchronization with OpenMP?","<p>When I try to do the math expression from the following code the matrix values are not consistent, how can I fix that?</p>
<pre><code>#pragma omp parallel num_threads (NUM_THREADS)
    {
    #pragma omp for
        for(int i = 1; i &lt; qtdPassos; i++)
        {   
            #pragma omp critical
            matriz[i][0] = matriz[i-1][0]; /
            for (int j = 1; j &lt; qtdElementos-1; j++)
            {
                matriz[i][j] = (matriz[i-1][j-1] + (2 * matriz[i-1][j]) + matriz[i-1][j+1]) / 4; // Xi(t+1) = [Xi-1 ^ (t) + 2 * Xi ^ (t)+ Xi+1 ^ (t)] / 4
            }
            matriz[i][qtdElementos-1] = matriz[i-1][qtdElementos-1]; 
        }
    }
</code></pre>
<p><a href=""https://i.stack.imgur.com/UwrzA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UwrzA.png"" alt=""Matrix execution"" /></a></p>
","<c><parallel-processing><openmp>","2022-04-28 18:27:29","69","0","1","72049114","<p>The problem comes from a <strong>race condition</strong> which is due to a <strong>loop carried dependency</strong>. The encompassing loop cannot be parallelised (nor the inner loop) since loop iterations <code>matriz</code> read/write the current and previous row. The same applies for the column.</p>
<p>Note that OpenMP does not check if the loop can be parallelized (in fact, it theoretically cannot in general). It is <em>your responsibility</em> to check that. Additionally, note that using a critical section for the whole iteration serializes the execution defeating the purpose of a parallel loop (in fact, it will be slower due to the overhead of the critical section). Note also that <code>#pragma omp critical</code> only applies on the next statement. Protecting the line <code>matriz[i][0] = matriz[i-1][0];</code> is not enough to avoid the race condition.</p>
<p>I do not think this current code can be (efficiently) parallelised. That being said, if your goal is to implement a 1D/2D stencil, then you can use a double buffering technique (ie. write in a 2D array that is different from the input array). A similar logic can be applied for 1D stencil repeated multiple times (which is apparently what you want to do). Note that the results will be different in that case. For the 1D stencil case, this double buffering strategy can fix the dependency issue and enable you to parallelize the inner-loop. For the 2D stencil case, the two nested loops can be parallelized.</p>
"
"72033307","How does Firebase and MongoDB Atlas Synchronise Time?","<p>Does anyone know how the following services take their time references from. In other words: <strong>with what source do they sync their time reference?</strong></p>
<ul>
<li>Firebase</li>
<li>MongoDB Atlas</li>
</ul>
<p>Found out that AWS services sync their time with a service called Amazon Time Sync.</p>
<blockquote>
<p>Amazon Time Sync is used by EC2 instances and other AWS services. It
uses a fleet of redundant satellite-connected and atomic clocks in
each Region to deliver time derived from these highly accurate
reference clocks. This service is provided at no additional charge.</p>
</blockquote>
<p>Likewise I need information about Firebase and MongoDB Atlas specifically. Any help/source is appreciated.</p>
","<firebase><time><mongodb-atlas><data-synchronization>","2022-04-27 17:57:04","76","-1","1","72041249","<p>What I found by myself.</p>
<p><strong>AWS Services</strong> - AWS services sync time with Amazon Time Sync. It uses a fleet of redundant satellite-connected and atomic clocks in each Region to deliver time derived from these highly accurate reference clocks.</p>
<p><strong>Google Services</strong> - Google services including Firebase use Google Public NTP. This is a free, global time service that you can use to synchronize to Google's atomic clocks.</p>
<p><strong>MongoDB Atlas</strong> - MongoDB Atlas has been enhanced by a move to a global logical clock. Implemented as a hybrid logical clock and protected by encryption.</p>
<p>Since all the services use highly accurate time services, we can assume that they represent the exact time so that time is synchronized across all the services in one application.</p>
<p><em><strong>References</strong></em></p>
<ol>
<li><a href=""https://aws.amazon.com/about-aw"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aw</a></li>
<li><a href=""https://developers.google.com/time/faq"" rel=""nofollow noreferrer"">https://developers.google.com/time/faq</a></li>
<li><a href=""https://www.mongodb.com/blog/post/transactions-background-part-4-the-global-logical-clock"" rel=""nofollow noreferrer"">https://www.mongodb.com/blog/post/transactions-background-part-4-the-global-logical-clock</a></li>
</ol>
"
"71962062","How to get sum of multiple rows in a table dynamically","<p>I am trying to get the total sum from columns of a specific data type(money) for multiple tables in a database. Currently I am able to get the list of columns from specific tables but I am unable to get the sums from those columns.</p>
<p>This is what I have now</p>
<pre><code>    use database 1 
Select + Column_Name
    From information_schema.columns
    Where TABLE_NAME = 'claimant'
    and data_type = 'money'
</code></pre>
<p>The result looks something like below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">table_name</th>
<th>column_name</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_a</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_b</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_c</td>
</tr>
</tbody>
</table>
</div>
<p>what I would like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">table_name</th>
<th>column_name</th>
<th>total_sum</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_a</td>
<td>66.20</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_b</td>
<td>300.50</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_c</td>
<td>5389.42</td>
</tr>
</tbody>
</table>
</div>
<p>update for @Squirrel Here is the code I have but it's still giving me issues with truncation.</p>
<pre><code>{

declare @sql nvarchar(max);

select  @sql = 'with cte as (' + char(13)
            + 'select' + char(13)
            + string_agg(char(9) + quotename(column_name) + ' = sum(' + quotename(COLUMN_NAME) + ')', ',' + char(13)) + char(13)
            + 'from ' + max(quotename(table_name)) + char(13)
            + ')' + char(13)
            + 'select a.table_name, a.column_name, a.total_sum ' + char(13)
            + 'from   cte ' + char(13)
            + 'cross apply (' + char(13)
            + char(9) + 'values' + char(13)
            + string_agg(char(9) + '(''' + table_name + ''',''' + column_name  + ''',' + quotename(COLUMN_NAME) + ')', ',' + char(13)) + char(13)
            + ') a (table_name, column_name, total_sum)'
from   information_schema.columns AS A
INNER JOIN EDL01.STAGING.TABLE_DETAILS B
ON A.TABLE_NAME = B.DEST_TABLE_NAME
where  A.table_name = B.DEST_TABLE_NAME
and    data_type  = 'money'



print @sql
exec sp_executesql @sql 
}
</code></pre>
<p>below is the create table</p>
<pre><code>CREATE TABLE [staging].[TABLE_DETAILS](
    [SOURCE_TABLE_NAME] [varchar](100) NULL,
    [DEST_TABLE_NAME] [varchar](100) NULL,
    [TYPE] [varchar](10) NULL,
    [PRIORITY] [int] NULL,
    [SOURCE_TABLE_DATABASE] [varchar](50) NULL,
    [SOURCE_TABLE_SCHEMA] [varchar](50) NULL,
    [DEST_TABLE_DATABASE] [varchar](50) NULL,
    [DEST_TABLE_SCHEMA] [varchar](50) NULL
) ON [PRIMARY]
GO
</code></pre>
<p>Below is part of the results</p>
<pre><code>select a.table_name, a.column_name, a.total_sum 
from   cte 
cross apply (
    values
('PAYMENT','BILLEDAMOUNT',[BILLEDAMOUNT]),
    ('PAYMENT','AMOUNT',[AMOUNT]),
    ('SIMS_PAYMENT','CHECKAMOUNT',[CHECKAMOUNT]),
    ('BILLREVIEWHEADER','JURISDICTIONAMOUNT1',[JURISDICTIONAMOUNT1]),
    ('BILLREVIEWHEADER','JURISDICTIONAMOUNT2',[JURISDICTIONAMOUNT2]),
    ('BILLREVIE
</code></pre>
","<sql><database><tsql><data-quality>","2022-04-22 00:04:16","276","0","1","71962417","<p>You need to form the query dynamically and then execute it using <code>sp_executesql</code> or <code>exec()</code></p>
<p>Note : <code>char(9)</code> is tab, <code>char(13)</code> is carriage return. These are added to format the query so that it is readable when you <code>print</code> it out for verification.</p>
<pre><code>declare @sql nvarchar(max);

select @sql = 'with cte as (' + char(13)
            + 'select' + char(13)
            + string_agg(char(9) + quotename(column_name) + ' = sum(' + quotename(column_name) + ')', ',' + char(13)) + char(13)
            + 'from ' + max(quotename(table_name)) + char(13)
            + ')' + char(13)
            + 'select a.table_name, a.column_name, a.total_sum ' + char(13)
            + 'from   cte ' + char(13)
            + 'cross apply (' + char(13)
            + char(9) + 'values' + char(13)
            + string_agg(char(9) + '(''' + table_name + ''', ''' + column_name + ''',' + quotename(column_name) + ')', ',' + char(13)) + char(13)
            + ') a (table_name, column_name, total_sum)'
from   information_schema.columns
where  table_name = 'table_1'
and    data_type  = 'money'

print @sql
exec sp_executesql @sql
</code></pre>
<p>For your sample table, the generated dynamic query is</p>
<pre><code>with cte as (
select
    [column_a] = sum([column_a]),
    [column_b] = sum([column_b]),
    [column_c] = sum([column_c])
from [table_1]
)
select a.table_name, a.column_name, a.total_sum 
from   cte 
cross apply (
    values
    ('table_1', 'column_a',[column_a]),
    ('table_1', 'column_b',[column_b]),
    ('table_1', 'column_c',[column_c])
) a (table_name, column_name, total_sum)
</code></pre>
<p>EDIT
using a loop to iterate each table. Basically it execute above query for each of the table and insert the result into a temp table</p>
<p>see <a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=0e10f3ac35c1b4b5c3258a4fd4ba9d1c"" rel=""nofollow noreferrer"">db&lt;&gt;fiddle demo</a></p>
<p>for earlier SQL Server version without string_agg(), use <code>for xml path</code></p>
<pre><code>select @sql  = 'with cte as (' + char(13)
             + 'select' + char(13)
             + stuff
               (
                  (
                      select ',' + quotename(COLUMN_NAME) + ' = sum(' + quotename(COLUMN_NAME) + ')'
                      from   INFORMATION_SCHEMA.COLUMNS
                      where  TABLE_NAME = @table
                      and    DATA_TYPE  = 'money'
                      for xml path('')
                  ), 
                  1, 1, ''
               ) + char(13)
            + 'from ' + max(quotename(@table)) + char(13)
            + ')' + char(13)
            + 'select a.table_name, a.column_name, a.total_sum ' + char(13)
            + 'from   cte ' + char(13)
            + 'cross apply (' + char(13)
            + char(9) + 'values' + char(13)
            + stuff
            (
              (
                  select    ',' + '(''' + TABLE_NAME + ''', ''' + COLUMN_NAME + ''',' + quotename(COLUMN_NAME) + ')'
                  from   INFORMATION_SCHEMA.COLUMNS
                  where  TABLE_NAME = @table
                  and    DATA_TYPE  = 'money'
                  for xml path('')
              ),
            1, 1, ''
            )
            + ') a (table_name, column_name, total_sum)' + char(13)
</code></pre>
"
"71696251","Does Azure Purview have a data lineage API?","<p>I know there are connectors for Purview that support data lineage data collection.  However, I'm wondering if Purview has any sort of API that allows any data processing (ETL) process to write a lineage record/document to the Purview lineage repository?</p>
","<azure><azure-purview><data-governance>","2022-03-31 16:39:19","127","1","1","71698220","<p>It's built on <a href=""https://medium.datadriveninvestor.com/azure-purview-catalog-is-based-on-apache-atlas-8e340830cabe"" rel=""nofollow noreferrer"">Apache Atlas</a> - Data Governance and Metadata framework for Hadoop.</p>
"
"71456221","How to add multiple column dynamically based on filter condition","<p>I am trying to create multiple columns dynamically based on filter condition after comparing two data frame with below code</p>
<pre><code>source_df
+---+-----+-----+----+
|key|val11|val12|date|
+---+-----+-----+-----+
|abc|  1.1| john|2-3-21
|def|  3.0| dani|2-2-21
+---+-----+-----+------

dest_df
+---+-----+-----+------+
|key|val11|val12|date  |
+---+-----+-----+------
|abc|  2.1| jack|2-3-21|
|def|  3.0| dani|2-2-21|
-----------------------
</code></pre>
<pre><code>columns= source_df.columns[1:]
joined_df=source_df\
    .join(dest_df, 'key', 'full')
for column in columns:
     column_name=&quot;difference_in_&quot;+str(column)
     report = joined_df\
    .filter((source_df[column] != dest_df[column]))\
    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))


</code></pre>
<p>The output I expect is</p>
<pre><code>#Expected
+---+-----------------+------------------+
|key| difference_in_val11| difference_in_val12 |
+---+-----------------+------------------+
|abc|[src:1.1,dst:2.1]|[src:john,dst:jack]|
+---+-----------------+-------------------+
</code></pre>
<p>I get only one column result</p>
<pre><code>#Actual
+---+-----------------+-
|key| difference_in_val12  |
+---+-----------------+-|
|abc|[src:john,dst:jack]|
+---+-----------------+-
</code></pre>
<p>How to generate multiple columns based on filter condition dynamically?</p>
","<dataframe><pyspark><apache-spark-sql><data-quality>","2022-03-13 11:18:03","419","-2","2","71456729","<p>Dataframes are immutable objects. Having said that, you need to create another dataframe using the one that got generated in the 1st iteration. Something like below -</p>
<pre><code>from pyspark.sql import functions as F

columns= source_df.columns[1:]
joined_df=source_df\
    .join(dest_df, 'key', 'full')
for column in columns:
  if column != columns[-1]:
       column_name=&quot;difference_in_&quot;+str(column)
       report = joined_df\
                    .filter((source_df[column] != dest_df[column]))\
                    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))

  else:
    column_name=&quot;difference_in_&quot;+str(column)
    report1 = report.filter((source_df[column] != dest_df[column]))\
                    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))
report1.show()
#report.show()
</code></pre>
<p><strong>Output</strong> -</p>
<pre><code>+---+-----+-----+-----+-----+-------------------+-------------------+
|key|val11|val12|val11|val12|difference_in_val11|difference_in_val12|
+---+-----+-----+-----+-----+-------------------+-------------------+
|abc|  1.1| john|  2.1| jack|  [src:1.1,dst:2.1]|[src:john,dst:jack]|
+---+-----+-----+-----+-----+-------------------+-------------------+
</code></pre>
"
"71456221","How to add multiple column dynamically based on filter condition","<p>I am trying to create multiple columns dynamically based on filter condition after comparing two data frame with below code</p>
<pre><code>source_df
+---+-----+-----+----+
|key|val11|val12|date|
+---+-----+-----+-----+
|abc|  1.1| john|2-3-21
|def|  3.0| dani|2-2-21
+---+-----+-----+------

dest_df
+---+-----+-----+------+
|key|val11|val12|date  |
+---+-----+-----+------
|abc|  2.1| jack|2-3-21|
|def|  3.0| dani|2-2-21|
-----------------------
</code></pre>
<pre><code>columns= source_df.columns[1:]
joined_df=source_df\
    .join(dest_df, 'key', 'full')
for column in columns:
     column_name=&quot;difference_in_&quot;+str(column)
     report = joined_df\
    .filter((source_df[column] != dest_df[column]))\
    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))


</code></pre>
<p>The output I expect is</p>
<pre><code>#Expected
+---+-----------------+------------------+
|key| difference_in_val11| difference_in_val12 |
+---+-----------------+------------------+
|abc|[src:1.1,dst:2.1]|[src:john,dst:jack]|
+---+-----------------+-------------------+
</code></pre>
<p>I get only one column result</p>
<pre><code>#Actual
+---+-----------------+-
|key| difference_in_val12  |
+---+-----------------+-|
|abc|[src:john,dst:jack]|
+---+-----------------+-
</code></pre>
<p>How to generate multiple columns based on filter condition dynamically?</p>
","<dataframe><pyspark><apache-spark-sql><data-quality>","2022-03-13 11:18:03","419","-2","2","71458061","<p>You could also do this with a union of both dataframes and then collect list only if collect_set size is greater than 1 , this can avoid joining the dataframes:</p>
<pre><code>from pyspark.sql import functions as F
cols = source_df.drop(&quot;key&quot;).columns

output = (source_df.withColumn(&quot;ref&quot;,F.lit(&quot;src:&quot;))
          .unionByName(dest_df.withColumn(&quot;ref&quot;,F.lit(&quot;dst:&quot;))).groupBy(&quot;key&quot;)
.agg(*[F.when(F.size(F.collect_set(i))&gt;1,F.collect_list(F.concat(&quot;ref&quot;,i))).alias(i)
       for i in cols]).dropna(subset = cols, how='all')
         )
</code></pre>
<hr />
<pre><code>output.show()

+---+------------------+--------------------+
|key|             val11|               val12|
+---+------------------+--------------------+
|abc|[src:1.1, dst:2.1]|[src:john, dst:jack]|
+---+------------------+--------------------+
</code></pre>
"
"71394862","Pyspark how can identify unmatched row value from two data frame","<p>I have below two data frame from which i am trying to identify the unmatched row value from data frame two. This is the part of migration where i want to see the difference after source data being migrated/moved to different destination.</p>
<pre><code>source_df
+---+-----+-----+
|key|val11|val12|
+---+-----+-----+
|abc|  1.1|  1.2|
|def|  3.0|  3.4|
+---+-----+-----+

dest_df
+---+-----+-----+
|key|val11|val12|
+---+-----+-----+
|abc|  2.1|  2.2|
|def|  3.0|  3.4|
+---+-----+-----+
</code></pre>
<p>i want to see the output something like below</p>
<pre><code>key: abc,

col:          val11                  val12

difference:  [src-1.1,dst:2.1]       [src:1.2,dst:2.2]

</code></pre>
<p>Any solution for this?</p>
","<pyspark><apache-spark-sql><data-quality>","2022-03-08 12:13:07","387","1","1","71395462","<pre><code>source_df  = spark.createDataFrame(
  [
('abc','1.1','1.2'),
('def','3.0','3.4'),
  ], ['key','val11','val12']
)

dest_df  = spark.createDataFrame(
  [
('abc','2.1','2.2'),
('def','3.0','3.4'),
  ], ['key','val11','val12']
)

report = source_df\
    .join(dest_df, 'key', 'full')\
    .filter((source_df.val11 != dest_df.val11) | (source_df.val12 != dest_df.val12))\
    .withColumn('difference_val11', F.concat(F.lit('[src:'), source_df.val11, F.lit(',dst:'),dest_df.val11,F.lit(']')))\
    .withColumn('difference_val12', F.concat(F.lit('[src:'), source_df.val12, F.lit(',dst:'),dest_df.val12,F.lit(']')))\
    .select('key', 'difference_val11', 'difference_val12')

report.show()

+---+-----------------+-----------------+
|key| difference_val11| difference_val12|
+---+-----------------+-----------------+
|abc|[src:1.1,dst:2.1]|[src:1.1,dst:2.1]|
+---+-----------------+-----------------+
</code></pre>
<p>Or, if you want exactally in that format:</p>
<pre><code>for x in report.select('key', 'difference_val11', 'difference_val12').collect():
    print(&quot;key: &quot; + str(x[0]) + &quot;,\n\n&quot; +\
          &quot;col:          val11                 val12\n\n&quot; +\
         &quot;difference:   &quot; + str(x[1]) + &quot;     &quot; + str(x[2]))
</code></pre>
<p>Output:</p>
<pre><code>key: abc,

col:          val11                 val12

difference:   [src:1.1,dst:2.1]     [src:1.2,dst:2.2]
</code></pre>
"
"71203005","Data Quality check with Python Dask","<p>Currently trying to write code to check for data quality of a 7 gb data file. I tried googling exactly but to no avail. Initially, the purpose of the code is to check how many are nulls/NaNs and later on to join it with another datafile and compare the quality between each. We are expecting the second is the more reliable but I would like to later on automate the whole process. I was wondering if there is someone here willing to share their data quality python code using Dask. Thank you</p>
","<python><dask><data-quality>","2022-02-21 08:08:19","107","0","1","71207327","<p>I would suggest the following approach:</p>
<ul>
<li>try to define how you would check quality on small dataset and implement it in Pandas</li>
<li>try to generalize the process in a way that if each &quot;part of file&quot; or partition is of good quality, than whole dataset can be considered of good quality.</li>
<li>use Dask's map_partitions to parralelize this processing over your dataset's partition.</li>
</ul>
"
"71126481","Domain Driven Design, should I use multiple databases or a single source of truth","<p>I'm about to propose some fundamental changes to my employers and would like the opinion of the community (opinion because I know that getting a solid answer to something like this is a bit far-fetched).</p>
<p><strong>Context:</strong></p>
<p>The platform I'm working on was built by a tech consultancy before I joined. While I was being onboarded they explained that they used DDD to build it, they have 2 domains, the client side and the admin side, each has its own database, its own GraphQl server, and its own back-end and front-end frameworks. The data between the tables is being synchronized through an http service that's triggered by the GraphQl server on row insertions, updates, and deletes.</p>
<p><strong>Problem:</strong></p>
<p>All of the data present on the client domain is found in the admin domain, there's no domain specific data there. Synchronization is a mess and is buggy. The team isn't large enough to manage all the resources and keep track of the different schemas.</p>
<p><strong>Proposal:</strong></p>
<p>Remove the client database and GraphQl servers, have a single source of truth database for all the current and potentially future applications. Rethink the schema, split the tables that need to be split, consolidate the ones that should be joined, and create new tables according to the actual current business flow.</p>
<p>Am I justified in my proposal, or was the tech consultancy doing the right thing and I'm sending us backwards?</p>
","<database><devops><domain-driven-design><business-logic>","2022-02-15 12:31:41","393","0","1","71136555","<p>Normally you have a database, or schema, for each separated boundary context. That means, that the initial idea of the consultancy company was correct.</p>
<p>What's not correct is the way that the consistency between the two is managed. You don't do it on tables changes but with services inside one (or both) the domains listening to the events and taking the update actions. It's a lot of work, anyway, because you have to update the event handlers on every change (in the events or tables structure).
This code is what's called <em>anti corruption layer</em>, that's exactly what it does: it avoids any corruption between the copies of the domain in another domain.</p>
<p>Said this, as you pointed out, your team is small and it could be that maintaining such a layer (and hence code) could cost a lot of energies. But, you've also to remember that once you've done, you have just to update it when needed.
Anyway, back to the proposal, you could also take this route. What you should (must, I would say) is that in each domain the external tables should be accessed only by some services, or queries, and this code should never ever modify the content that it access. Never. But I suppose that you already know this.</p>
<p>Nothing is written in the stone, the rules should always be adapted when put in a real context. Two separated databases means more work, but also a much better separation of the domains. It could never happen that someone accidentally modifies the content of the tables of the other domain. On the other side, one database means less work, but also much more care about what the code does.</p>
"
"71093522","How to push data from a on-premises database to tableau crm","<p>We have an on-premises oracle database installed on a server. We have to create some Charts/Dashboards with Tableau CRM on those data on-premises. Note that, tableau CRM is not Tableau Online, it is a Tableau version for the Salesforce ecosystem.</p>
<p>Tableau CRM has APIs, so we can push data to it or can upload CSV programmatically to it.
So, what can be done are,</p>
<ol>
<li>Run a nodeJS app on the on-premise server, pull data from Oracle DB, and then push to Tableau CRM via the TCRM API.</li>
<li>Run a nodeJS app on the on-premise server, pull data from Oracle DB, create CSV, push the CSV via TCRM API</li>
</ol>
<p>I have tested with the 2nd option and it is working fine.</p>
<p>But, you all know, it is not efficient. Because I have to run a cronJob and schedule the process multiple times in a day. I have to query the full table all the time.</p>
<p>I am looking for a better approach. Any other tools/technology you know to have a smooth sync process?</p>
<p>Thanks</p>
","<database><tableau-api><data-synchronization><oracle-cdc>","2022-02-12 16:08:08","348","0","1","71397637","<p>The second method you described in the questions is a good solution. However, you can optimize it a bit.</p>
<blockquote>
<pre><code>I have to query the full table all the time.
</code></pre>
</blockquote>
<p>This is can be avoided. If you take a look at the <a href=""https://developer.salesforce.com/docs/atlas.en-us.bi_dev_guide_ext_data.meta/bi_dev_guide_ext_data/bi_ext_data_object_externaldata.htm"" rel=""nofollow noreferrer"">documentation</a> of SObject <code>InsightsExternalData</code> you can see that it has a field by name <code>Operation</code> which takes one of these values <code>Append, Delete, Overwrite, Upsert</code>
what you will have to do is when you push data to Tableau CRM you can use the <code>Append</code> operator and push the records that don't exist in TCRM. That way you only query the delta records from your database. This reduces the size of the CSV you will have to push and since the size is less it takes less time to get uploaded into TCRM.</p>
<p>However, to implement this solution you need two things on the database side.</p>
<ol>
<li>A unique identifier that uniquely identifies every record in the database</li>
<li>A DateTime field</li>
</ol>
<p>Once you have these two, you have to write a query that sorts all the records in ascending order of the DateTime field and take only the files that fall below the last UniqueId you pushed into TCRM. That way your result set only contains delta records that you don't have on TCRM. After that you can use the same pipeline you built to push data.</p>
"
"70950914","Sagemaker: How to debug Model monitoring(data quality and model quality)?","<p>I have created a Data Quality monitoring from Sagemaker Studio UI and also created using sagemaker SDK <a href=""https://stackoverflow.com/q/69179914/11844406"">code</a>, I referred to create model Data Quality monitoring job.</p>
<p><strong>Errors:</strong></p>
<ol>
<li>when there is no captured data (this is expected)</li>
</ol>
<blockquote>
<p>Monitoring job failure reason:</p>
<p>Job inputs had no data</p>
</blockquote>
<ol start=""2"">
<li>From logs, I can see that it is using <code>Java</code> in background. Not sure how to debug?</li>
</ol>
<blockquote>
<p>org.json4s.package$MappingException: Do not know how to convert
JObject(List(0,JDouble(38.0))) into class java.lang.String.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/3da3i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3da3i.png"" alt=""enter image description here"" /></a></p>
<p>Once we create the DataQuality monitoring job using Sagemaker Studio UI or Sagemkaer python sdk, it is taking a hour to start. I would like to know is there a way to debug monitoring job without waiting for a hour every time we get a error?</p>
","<python><amazon-web-services><amazon-sagemaker>","2022-02-02 05:34:02","190","0","1","71225027","<p>For development, it might be easier to trigger execution of the monitoring job manually. Take a look at this <a href=""https://github.com/aws-samples/amazon-sagemaker-mlops-workshop/blob/main/labs/05_model_monitor/monitoringjob_utils.py"" rel=""nofollow noreferrer"">python code</a></p>
<p>If you want to see how it's used, open the <a href=""https://github.com/aws-samples/amazon-sagemaker-mlops-workshop/blob/main/labs/05_model_monitor/05_model_monitor.ipynb"" rel=""nofollow noreferrer"">lab 5 notebook</a> of the workshop and scroll almost to the end, to the cells right after the &quot;Triggering execution manually&quot; title.</p>
"
"70939067","Test yaml great-expectations with Bigquery","<p>I am having troubles testing the yaml of great-expectation to bigquery.
I followed the official documentation and got to this code</p>
<pre><code>import os 
import great_expectations as ge 

datasource_yaml = &quot;&quot;&quot;
name: my_bigquery_datasource
class_name: Datasource
execution_engine:
  class_name: SqlAlchemyExecutionEngine
  connection_string: bigquery://&lt;GCP_PROJECT_NAME&gt;/&lt;BIGQUERY_DATASET&gt;
data_connectors:
  default_runtime_data_connector_name:
    class_name: RuntimeDataConnector
    batch_identifiers:
      - default_identifier_name
  default_inferred_data_connector_name:
    class_name: InferredAssetSqlDataConnector
    include_schema_name: true
&quot;&quot;&quot;
context = ge.get_context()

context.test_yaml_config(datasource_yaml)
</code></pre>
<p>The code works but it takes soo much time. I did deep debugging and see that the problem is that it wants to retrieve all the datasets of the project in bigquery and all the tables from all datasets. We have over 200 datasets and thousands of tables.
I haven't found a way to filter the only dataset that i need or more specifically the table. I thought the connection_string should do it but doesn't.</p>
<p>In my deep debugging, and got to the <code>inferred_asset_sql_data_connector.py</code> module. I saw that it should filter the schema_name problem is that always comes as None. And don't know how to pass it as the dataset I want.</p>
<p><a href=""https://i.stack.imgur.com/GIOwe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GIOwe.png"" alt=""enter image description here"" /></a></p>
<p>I followed this <a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/how_to_configure_a_dataconnector_to_introspect_and_partition_tables_in_sql/#configuring-introspection-and-tables"" rel=""nofollow noreferrer"">guide</a> as well of introspection but getting other errors.
<a href=""https://i.stack.imgur.com/DuuHW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DuuHW.png"" alt=""enter image description here"" /></a></p>
<p>If I put the SimpleSqlalchemyDatasource as class_name I get the following error. And I dont know how to initalize the engine for bq in sqlalchemy in the context of greatexpectations.</p>
<p><a href=""https://i.stack.imgur.com/NaE0M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NaE0M.png"" alt=""enter image description here"" /></a></p>
","<python><google-bigquery><data-quality><great-expectations>","2022-02-01 10:33:54","519","2","1","72880705","<blockquote>
<p>default_inferred_data_connector_name</p>
</blockquote>
<p>tries to  fetch all dataset and table info from bigquery and it will create assets. You can remove the default_inferred_data_connector_name and use</p>
<blockquote>
<p>RuntimeBatchRequest</p>
</blockquote>
<p>and use query to validate the data.</p>
<p>Regarding authentication issue you can change the</p>
<blockquote>
<p>connection_string: bigquery://&lt;GCP_PROJECT_NAME&gt;/&lt;BIGQUERY_DATASET&gt;</p>
</blockquote>
<p>to</p>
<blockquote>
<p>connection_string: bigquery://&lt;GCP_PROJECT_NAME&gt;/&lt;BIGQUERY_DATASET&gt;?credentials_path=&lt;path_to_credential file &gt;</p>
</blockquote>
<p>More info on sql alchemy configuration can be found at <a href=""https://github.com/googleapis/python-bigquery-sqlalchemy"" rel=""nofollow noreferrer"">https://github.com/googleapis/python-bigquery-sqlalchemy</a></p>
"
"70767609","When will the data quality function be released to the release version","<p>When will the dolphinscheduler data quality function be released to the release version?
Is there a time plan for thisã€‚</p>
","<data-quality><apache-dolphinscheduler>","2022-01-19 08:44:43","70","1","1","71985892","<p>3.0-alpha has supported this feature, you can download the version from the url[1]</p>
<p>1,https://dolphinscheduler.apache.org/en-us/download/download.html</p>
"
"70642143","Recursive method for calculate percentual of repeated values for each column in my df with R","<p>I need to use lapply/sapply or other recursive methods for my real df for calculate how many repeated values have in each column/variable.</p>
<p>Here I used an small example to reproduce my case:</p>
<pre><code>library(dplyr)

df &lt;- data.frame(
var1 = c(1,2,3,4,5,6,7,8,9,10 ),
var2 = c(1,1,2,3,4,5,6,7,9,10 ),
var3 = c(1,1,1,2,3,4,5,6,7,8 ),
var4 = c(2,2,1,1,2,1,1,2,1,2 ),
var5 = c(1,1,1,1,1,4,5,5,6,7 ),
var6 = c(4,4,4,5,5,5,5,5,5,5 )   
)

</code></pre>
<p>I have <code>r nrow(df)</code> in my dataset and now I need to obtain the % of repeated values for each column. Suppose that my real <code>df</code> have a lot of columns, and I need to do it recursively. I tryed to use <code>lapply/sapply</code>, but it didnÂ´t worked...</p>
<pre><code># create function that is used in lapply
perc_repeated &lt;- function(variables){
  
  paste(round((sum(table(df$variables)-1) / nrow(df))*100,2),&quot;%&quot;)
  
}

perce_repeated_values &lt;- lapply(df, perc_repeated) 
perce_repeated_values

</code></pre>
<p><img src=""https://github.com/rhozon/datasets/raw/master/perc_repeated_for_each_col.png"" alt="""" /></p>
<p>How to do this optimally if my dataframe increases in number of columns to something like 700, using some recursive function for each column and getting the results in an orderly way in a dataframe from largest to smallest ? (eg of the variable that has it 100% repeated values for the one that reaches 0%) in something like:</p>
<pre><code>df_repeated

variable      perc_repeated_values
var6                    80%
var4                    80%
var5                    50%
var3                    20%
var2                    20%
var1                     0%

</code></pre>
","<r><dataframe><function><recursion><data-quality>","2022-01-09 14:17:58","56","0","1","70642292","<p>This can easily be done with <code>dplyr::summarize()</code></p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)

df &lt;- data.frame(
  var1 = c(1,2,3,4,5,6,7,8,9,10 ),
  var2 = c(1,1,2,3,4,5,6,7,9,10 ),
  var3 = c(1,1,1,2,3,4,5,6,7,8 ),
  var4 = c(2,2,1,1,2,1,1,2,1,2 ),
  var5 = c(1,1,1,1,1,4,5,5,6,7 ),
  var6 = c(4,4,4,5,5,5,5,5,5,5 )   
)

df %&gt;% 
  summarise(across(everything(),
                   ~100 * (1 - n_distinct(.)/n()))) %&gt;% 
  pivot_longer(everything(), 
               names_to = &quot;var&quot;, 
               values_to = &quot;percent_repeated&quot;) %&gt;% 
  arrange(desc(percent_repeated))
#&gt; # A tibble: 6 x 2
#&gt;   var   percent_repeated
#&gt;   &lt;chr&gt;            &lt;dbl&gt;
#&gt; 1 var4                80
#&gt; 2 var6                80
#&gt; 3 var5                50
#&gt; 4 var3                20
#&gt; 5 var2                10
#&gt; 6 var1                 0
</code></pre>
<p><sup>Created on 2022-01-09 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
"
"70573285","OpenAPI as a single source of truth - limitations","<p>One of the benefits being promoted for API-first design or OpenAPI is that of their use as a single source of truth. To my mind, these schemas only serve as a contract - the actual source of truth for your API lies in your microservices implementation (typically a http endpoint).</p>
<p>How can OpenAPI claim to be a single source of truth when the contract cannot be enforced until the implementation on the API side is complete? I realise there is tooling available to assist with this, such as validation middleware that can be used to match your request and response against your schema, however this is typically only validated at the point that a network request is made, not at compile time.</p>
<p>Of course you could write api tests to validate, but this is very much dependent on good test coverage and not something you get out of the box.</p>
<p>TLDR - OpenAPI markets itself as being a single source of truth for APIs, but this simply isn't true until your API implementation matches the spec. What tools/techniques (if any) can be used to mitigate this?</p>
","<rest><architecture><openapi><openapi-generator><system-design>","2022-01-04 01:48:15","195","-2","1","70586214","<p>Did a bit of additional research into available tooling and found a solution that helps mitigate this issue:</p>
<p><a href=""https://www.npmjs.com/package/openapi-backend"" rel=""nofollow noreferrer"">open-api-backend</a> (and presumably other such libraries) have capabilities to map your api routes/handlers to a specific openAPI operation or operationID. You can then enforce schema validation such that only routes defined in the spec can be implemented, else a fail-fast error is thrown)</p>
"
"70377595","Sync sanity database to salesforce","<p>I have some sites made with sanity. So I want to sync my sanity database to salesforce. If I get a better flow or idea or links it will help</p>
","<salesforce><data-synchronization><sanity>","2021-12-16 10:40:38","75","0","1","70389261","<p>you need to create RestResource's in salesforce and then need to hit resource end point to create/update/delete data into salesforce.
It's pretty easy you can learn it from trailhead <a href=""https://trailhead.salesforce.com/en/content/learn/modules/apex_integration_services/apex_integration_webservices"" rel=""nofollow noreferrer"">https://trailhead.salesforce.com/en/content/learn/modules/apex_integration_services/apex_integration_webservices</a></p>
"
"70269484","Should two identical databases, one acting as the single source of truth, be forced to have the same timestamps?","<p>I have two databases with virtually identical records, one acting as the single source of truth. The single source of truth regularly updates the other database so that they match.</p>
<p>I am wondering if the timestamps of each identical record should be forced to be the <em>same</em> between databases, or if I should allow each database to manage its own timestamps independently?</p>
<p>Are there any long-term pros or cons to identical timestamps, perhaps under the context of running helpful queries to maintain data integrity and uniformity should they get out of sync?</p>
<p>I am new to distributed systems, so thanks in advance for your patience and help.</p>
","<mongodb><postgresql><database-design><architecture><distributed-system>","2021-12-08 03:13:42","89","-3","1","70269610","<p>@Danyolo - I suggest you take a look at MongoDB replicasets. MongoDB has a really neat replication system to ensure high availability and resiliency. Check out this link:</p>
<p><a href=""https://docs.mongodb.com/manual/core/replica-set-architecture-geographically-distributed/"" rel=""nofollow noreferrer"">https://docs.mongodb.com/manual/core/replica-set-architecture-geographically-distributed/</a></p>
"
"70204649","Using viewModel as the single source of truth in jetpack compose","<p>let's say we have a viewModel that has a value called apiKey inside. Contents of this value is received from DataStore in form of a Flow and then, it is exposed as LiveData.
On the other hand we have a Fragment called SettingsFragment, and we are trying to display that apiKey inside a TextField, let the user modify it and save it in DataStore right away.
The solution that I'm currently using is down below, but the issue is that the UI gets very laggy and slow when changes are being made to the text.
My question is that what is the best way to implement this and still have a single source of truth for our apiKey?</p>
<pre><code>class SettingsViewModel() : ViewModel() {

    val apiKey = readOutFromDataStore.asLiveData()

    fun saveApiKey(apiKey: String) {
        viewModelScope.launch(Dispatchers.IO) {
            saveToDataStore(&quot;KEY&quot;, apiKey)
        }
    }
}

/** SettingsFragment **/
...

@Composable
fun ContentView() {
    var text = mViewModel.apiKey.observeAsState().value?.apiKey ?: &quot;&quot;

    Column() {
        OutlinedTextField(
            label = { Text(text = &quot;API Key&quot;) },
            value = text,
            onValueChange = {
                text = it
                mViewModel.saveApiKey(it)
            })
    }
}
</code></pre>
","<android><view><viewmodel><android-jetpack-compose><android-jetpack-datastore>","2021-12-02 18:24:10","1372","1","1","70205099","<p>Don't save the TextField's value in the onValueChange event to the data store on every key press - which is almost certainly slowing you down - especially if you are using the same thread. Use a local state variable and only update the data store when the user either moves the focus elsewhere or they save what's on the screen through some button press. You also need to avoid mixing UI threading with data storage threading which should be on the IO thread. Here is one possible solution:</p>
<pre class=""lang-kotlin prettyprint-override""><code>@Composable
fun ContentViewHandler() {
    ContentView(
        initialText = viewmodel.getApiKey(),
        onTextChange = { text -&gt;
            viewmodel.updateApiKey(text)
        }
    )
}

@Composable
fun ContentView(
    initialText: String,
    onTextChange: (text: String) -&gt; Unit
) {
    var text by remember { mutableStateOf(initialText) }

    Column() {
        OutlinedTextField(
            label = { Text(text = &quot;API Key&quot;) },
            value = text,
            onValueChange = {
                text = it
            },
            modifier = Modifier.onFocusChanged {
                onTextChange(text)
            }
        )

        // Or add a button and save the text when clicked.
    }
}
</code></pre>
"
"70127649","How to have a single source of truth for poetry and pre-commit package version?","<p>I'm looking into <a href=""https://github.com/br3ndonland/template-python"" rel=""noreferrer"">this</a> Python project template. They use <code>poetry</code> to define <a href=""https://github.com/br3ndonland/template-python/blob/main/pyproject.toml"" rel=""noreferrer"">dev dependencies</a></p>
<pre><code>[tool.poetry.dev-dependencies]
black = {version = &quot;*&quot;, allow-prereleases = true}
flake8 = &quot;*&quot;
isort = &quot;^5.6&quot;
mypy = &quot;&gt;0.900,&lt;1&quot;
...
</code></pre>
<p>They use also <a href=""https://github.com/br3ndonland/template-python/blob/main/.pre-commit-config.yaml"" rel=""noreferrer""><code>pre-commit</code></a> to check housekeeping things (e.g., formatting, linting, issorting, ...), both for git and for CI workflows:</p>
<pre><code>minimum_pre_commit_version: 2.8.0
default_stages: [commit, push, manual]
repos:
  - repo: https://github.com/psf/black
    rev: 21.11b1
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8
        args: [--max-line-length=88]
  - repo: https://github.com/pycqa/isort
    rev: 5.10.1
    hooks:
      - id: isort
        args: [--filter-files]
  - ...
</code></pre>
<p>In my case, I definitely want a local version of dev packages managed by poetry for my IDE, and I'd like also to harness the pre-commit framework <em>&quot;as is&quot;</em>, without switching to <a href=""https://stackoverflow.com/a/67796237/4820341""><code>language: system</code></a>. Working this way, I need to manage each package version in two different places.</p>
<p><strong>Is there a non-manual way to keep dev packages versions (i.e,. <code>black</code>, <code>flake8</code>, <code>isort</code>, <code>mypy</code>, ...) aligned to a single source of truth?</strong>  A Coockiecutter template could be an option, but it looks overkilling.</p>
","<python><pre-commit-hook><python-poetry><pre-commit.com>","2021-11-26 16:49:15","5005","18","2","70136571","<p>I would recommend keeping the linter stuff only in the config of <code>pre-commit</code>.</p>
<p><code>pre-commit</code> doesn't necessarily run as a pre-commit hook. You can run the checks every time by <code>pre-commit run --all-files</code> or if you want to run it only on given files with <code>pre-commit run --files path/to/file</code>.</p>
<p>You can even say which which check should run, e.g. <code>pre-commit run black --all-files</code></p>
"
"70127649","How to have a single source of truth for poetry and pre-commit package version?","<p>I'm looking into <a href=""https://github.com/br3ndonland/template-python"" rel=""noreferrer"">this</a> Python project template. They use <code>poetry</code> to define <a href=""https://github.com/br3ndonland/template-python/blob/main/pyproject.toml"" rel=""noreferrer"">dev dependencies</a></p>
<pre><code>[tool.poetry.dev-dependencies]
black = {version = &quot;*&quot;, allow-prereleases = true}
flake8 = &quot;*&quot;
isort = &quot;^5.6&quot;
mypy = &quot;&gt;0.900,&lt;1&quot;
...
</code></pre>
<p>They use also <a href=""https://github.com/br3ndonland/template-python/blob/main/.pre-commit-config.yaml"" rel=""noreferrer""><code>pre-commit</code></a> to check housekeeping things (e.g., formatting, linting, issorting, ...), both for git and for CI workflows:</p>
<pre><code>minimum_pre_commit_version: 2.8.0
default_stages: [commit, push, manual]
repos:
  - repo: https://github.com/psf/black
    rev: 21.11b1
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8
        args: [--max-line-length=88]
  - repo: https://github.com/pycqa/isort
    rev: 5.10.1
    hooks:
      - id: isort
        args: [--filter-files]
  - ...
</code></pre>
<p>In my case, I definitely want a local version of dev packages managed by poetry for my IDE, and I'd like also to harness the pre-commit framework <em>&quot;as is&quot;</em>, without switching to <a href=""https://stackoverflow.com/a/67796237/4820341""><code>language: system</code></a>. Working this way, I need to manage each package version in two different places.</p>
<p><strong>Is there a non-manual way to keep dev packages versions (i.e,. <code>black</code>, <code>flake8</code>, <code>isort</code>, <code>mypy</code>, ...) aligned to a single source of truth?</strong>  A Coockiecutter template could be an option, but it looks overkilling.</p>
","<python><pre-commit-hook><python-poetry><pre-commit.com>","2021-11-26 16:49:15","5005","18","2","70308217","<p>Finally, I created a pre-commit hook to do the job: <a href=""https://github.com/floatingpurr/sync_with_poetry"" rel=""noreferrer"">https://github.com/floatingpurr/sync_with_poetry</a></p>
<blockquote>
<p>Edit: If you use PDM, you can refer to this one: <a href=""https://github.com/floatingpurr/sync_with_pdm"" rel=""noreferrer"">https://github.com/floatingpurr/sync_with_pdm</a></p>
</blockquote>
<p>This hook just keeps in sync the repos <code>rev</code> in <code>.pre-commit-config.yaml</code> with the packages version locked into <code>poetry.lock</code>.</p>
<p>If you use:</p>
<ul>
<li>Poetry for dependency management</li>
<li>local dev packages for your IDE (via Poetry)</li>
<li>pre-commit hooks</li>
</ul>
<p>this meta-hook can be useful to (un-)bump repos rev for you.</p>
"
"69952568","How can i update values stored in map to database after every 1 hour while synchronizing incoming requests on route which updates map continously","<p>I am facing a problem to update the values stored in map to database because the map is updating continously due to incoming requests on the route and at the same time I want to update values in database periodically with the help of map. How to synchronize both the operations?</p>
<pre><code>const url_count = new Map();

async function update_count(){
 
    // update value in mongodb 
    
}
app.get('/teeny/:code', async (req, res) =&gt; {
    try {
        const url = await Link_URL.findOne({
            _id : req.params.code
        })
        if (url) {
          if(url_count.has(req.params.code)){
            const val = url_count.get(req.params.code);
            url_count.set(req.params.code,val+1);
            update_count();
          }
          else{
              url_count.set(req.params.code,1);
          }  
            return res.redirect(url.URL);
        } else {
            return res.status(404).json('No URL Found')
        }
    }
    catch (err) {
        console.error(err)
        res.status(500).json('Server Error')
    }
})
cron.schedule('* * 1 * *', update_count);
</code></pre>
","<javascript><node.js><mongodb><data-synchronization>","2021-11-13 08:09:37","125","0","1","69952683","<p>You could use <a href=""http://bunkat.github.io/later/"" rel=""nofollow noreferrer"">Later</a> for a simple solution... here you would be collecting all those values and then insert them at a specified time that you specified in later..</p>
<p>For something heavier, you could go for <a href=""https://github.com/OptimalBits/bull"" rel=""nofollow noreferrer"">Bull</a> or for <a href=""https://github.com/arobson/rabbot"" rel=""nofollow noreferrer"">Rabbot</a></p>
<p>If you want to kill a lion, go for <a href=""https://www.npmjs.com/package/node-celery"" rel=""nofollow noreferrer"">Node-Celery</a>... but make sure to have python installed.</p>
<p>Basically, all you need is a queuing system to collect[in a queue] all those tasks that you want to save later...</p>
"
"69888507","Dynamic SQL table validation for data quality dimension","<p>I have the following code to test for <code>nulls</code> in a whole table using dynamic sql:</p>
<pre><code>/*Completitud*/
--Housekeeping:
drop table if exists tmp_completitud;
--Declarar variables para el loop:
declare @custom_sql   VARCHAR(max)
declare @tablename as VARCHAR(255) = 'maestrodatoscriticos' --Nombre de tabla a usar.
--Reemplazar '_[dimension]' como &quot;apellido&quot; de la tabla por cada nueva dimension:
set @custom_sql = 'select ''' + @tablename + '_Completitud' + ''' as tabla'
select @custom_sql =
           --Reemplazar query de dimension aqui:
       @custom_sql + ', ' + 'sum(cast(iif(' + c.name + ' is null,0,1) as decimal)) / count(*) as ' + c.name
from sys.columns c
         inner join sys.tables t on c.object_id = t.object_id
where t.name = @tablename
set @custom_sql = @custom_sql + ' into tmp_completitud from ' + @tablename
--print @custom_sql
exec (@custom_sql);
--Poblar tabla de dimensiones con dimension actual:
insert into dimensiones
select *
from tmp_completitud;
</code></pre>
<p>I now want to test for unique values, but I'm having a hard time with aggregate functions inside a subquery. So far I have:</p>
<pre><code>select sum(cast(iif(
            ( select sum(cnt) from ( select count(distinct identificacion) as cnt from maestrodatoscriticos ) as x ) =
            ( select sum(cnt2) from ( select count(identificacion) as cnt2 from maestrodatoscriticos ) as y ), 0,
            1) as decimal)) / count(*)
from maestrodatoscriticos;
</code></pre>
<p>And I would like to somehow integrate the <code>select sum(cast(iif...</code> into the <code>select @custom_sql = ...</code> above. Any ideas?</p>
","<tsql><dynamic-sql><data-quality>","2021-11-08 18:51:35","51","0","1","69889583","<p>I actually resolved this with some help from a co-worker. The code is:</p>
<pre><code>/*Unicidad*/
--Housekeeping:
drop table if exists tmp_unicidad;
--Declarar variables para el loop:
declare @sqluni VARCHAR(max) declare @tableuni as VARCHAR(255) = 'maestrodatoscriticos' --Nombre de tabla a usar.
--Reemplazar '_[dimension]' como &quot;apellido&quot; de la tabla por cada nueva dimension:
set @sqluni = 'select ''' + @tableuni + '_Unicidad' + ''' as tabla'
select @sqluni =
           --Reemplazar query de dimension aqui:
       @sqluni + ', ' + 'count(distinct ' + c.name + ') * 1.00 / count(*) * 1.00 as ' + c.name
from sys.columns c
         inner join sys.tables t on c.object_id = t.object_id
where t.name = @tableuni
set @sqluni = @sqluni + ' into tmp_unicidad from ' + @tableuni
--print @custom_sql
exec (@sqluni);
--Poblar tabla de dimensiones con dimension actual:
insert into dimensiones
select *
from tmp_unicidad;
</code></pre>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69698129","<p>I'd rather see you post (parts of) the original data file and show you how it's done all the way with awk, but here's what you asked for using GNU awk (<code>gensub</code>):</p>
<pre><code>$ gawk '{print gensub(/([0-9]+ )/,&quot;\\1| &quot;,1,$0)}' file
</code></pre>
<p>Output:</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
...
</code></pre>
<p><strong>Edit:</strong> Hmm, too much <code>gensub</code> lately I guess, just use awk:</p>
<pre><code>$ awk '{sub(/([0-9]+ )/,&quot;&amp;| &quot;)}1' file
</code></pre>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69699619","<p>Another approach with any <code>awk</code> is to use <code>match()</code> to locate where the first number and whitespace ends and then use <code>substr()</code> to print up to that point, add a <code>&quot;|&quot;</code> and then use <code>substr()</code> again to print from that point to the end, e.g.</p>
<pre><code>awk '{ 
    match($0,/^[ \t0-9]+/)
    print substr($0,0,RLENGTH-1), &quot;|&quot;, substr($0, RLENGTH+1)
}'
</code></pre>
<p><strong>Example Use/Output</strong></p>
<p>With your sample input in the file name <code>media</code>, you would do:</p>
<pre class=""lang-none prettyprint-override""><code>$ awk '{ match($0,/^[ \t0-9]+/); print substr($0,0,RLENGTH-1), &quot;|&quot;, substr($0, RLENGTH+1) }' media
   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
</code></pre>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69699836","<p>All <a href=""/questions/tagged/bash"" class=""post-tag"" title=""show questions tagged &#39;bash&#39;"" rel=""tag"">bash</a> / <a href=""/questions/tagged/zsh"" class=""post-tag"" title=""show questions tagged &#39;zsh&#39;"" rel=""tag"">zsh</a> answer.</p>
<p>If you are not married to the whitespace around the first field you can do:</p>
<pre><code>$ while read -r x rest; do printf &quot;%s|%s\n&quot; &quot;$x&quot; &quot;$rest&quot;; done &lt;file
2|Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
29|Media Server Enterprise
7|Media Server lite
10|Media server lite 1.0
468|Media server lite 2.0
8|Media server lite 2.3
1|Media server lite 2.4
40|Media server lite 3.0
3|Media server lite 3.3
</code></pre>
<p>That result is a single character <code>&quot;|&quot;</code> delimiter CSV file rather than having a three character <code>&quot; | &quot;</code> for a CSV delimiter (which is more difficult to deal with later...)</p>
<p>If you want the first field to be wider and a column, you can use a Bash regex to separate the first field from the rest:</p>
<pre><code>while IFS= read -r line || [[ -n $line ]]; do 
    if [[ $line =~ ^[[:blank:]]*([[:digit:]]+)[[:blank:]]+(.*) ]]; then
        printf &quot;%4s | %s\n&quot; &quot;${BASH_REMATCH[1]}&quot; &quot;${BASH_REMATCH[2]}&quot;
    fi  
done &lt;file 
   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
<hr />
<p>For an <a href=""/questions/tagged/awk"" class=""post-tag"" title=""show questions tagged &#39;awk&#39;"" rel=""tag"">awk</a> answer, I would do:</p>
<pre><code>awk -v de=&quot; | &quot; '
FNR==NR{length($1)&gt;max ? max=length($1) : max=max; next}
{
    s=&quot;&quot;
    for (i=2;i&lt;=NF;i++) s=s ? s OFS $i : $i
    printf &quot; %*s%s%s\n&quot;, max, $1, de, s
}
' file file
   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69700530","<p>With <code>sed</code>(1)</p>
<pre><code>sed 's/^\([[:space:]]*[[:digit:]]\{1,\}\)/\1 |/' file.txt 
</code></pre>
<hr />
<ul>
<li><p>The <code>^</code> is an anchor which means the start/beginning.</p>
</li>
<li><p>The <code>( )</code> is a capture group, (the <code>(</code> and <code>)</code> needs to be escaped with B.R.E.) what ever pattern is inside it will be in <code>\1</code>. Which is the first capture group.</p>
</li>
<li><p><code>[[:space:]]</code> white space.</p>
</li>
<li><p><code>*</code> is a quantifier that means zero or more.</p>
</li>
<li><p><code>[[:digit:]]</code> is an int.</p>
</li>
<li><p><code>{1,}</code> is a quantifier that means one or more, but the <code>{</code> and <code>}</code> needs to be escape with B.R.E. which is the default regex engine by <code>sed</code>.</p>
</li>
</ul>
<hr />
<p>If the <code>-E</code> flag/option (E.R.E.) is used the escaping can be omitted in the pattern matching.</p>
<pre><code>sed -E 's/^([[:space:]]*[[:digit:]]{1,})/\1 |/' file.txt
</code></pre>
"
"69606534","Offline first app and data sharing between users","<p>I want to develop cross platform app which has three components (client app, server app, database) if possible. I especially want to use it offline.</p>
<p>App is similar to TODO app. I want to make a TODO app to try at the beginning.</p>
<ol>
<li><p>Users use the app without any internet connection. They are make CRUD operations as offline in first use. The user can use it offline indefinitely.</p>
</li>
<li><p>If the user wants to register to app, all of user's data sync to online database and use offline or online. Data can be synchronized at certain times every day.</p>
</li>
<li><p>Users share some data with other users and use offline/online.</p>
</li>
</ol>
<p>How can I handle these situations. Are there any examples/documents/suggestions :)</p>
<p>Thank you.</p>
<p>I researched many platforms.</p>
<p>Maybe pouchdb/couchdb can handle first 2 situations but I think the third is complicated with couchdb per user, and I don't want to connect directly to database, I want to use application server as middleware.</p>
<p>I saw dexiejs, Mendix, logux, Mango realm, ......</p>
<p>I guess what I've seen doesn't quite meet the demands.</p>
<p>I will concentrate on PWA.</p>
","<web-applications><couchdb><offline><pouchdb><data-synchronization>","2021-10-17 16:44:54","603","0","1","69669993","<p>With CouchDB you can let users access databases and documents that belong to a different user right down to the document level.</p>
<p>You need to look at how &quot;users&quot; and &quot;roles&quot; work, and you need to look into &quot;design documents&quot; for setting what users can access and do with documents they have access to in specific databases.</p>
<p>The PouchDB.com website has some great documentation and example code to help you get started with this. Check out their &quot;Plug-ins&quot; page and specifically &quot;pouchdb.authentication.js&quot; and the &quot;CouchDB authentication recipes&quot; <a href=""https://github.com/pouchdb-community/pouchdb-authentication/blob/master/docs/recipes.md"" rel=""nofollow noreferrer"">here</a></p>
"
"69152012","Java: data synchronization issue","<p>My Java program makes a request to client's API and client's API will return a key-value pair on <strong>irregular</strong> time intervals (i.e. not every 5 seconds / 10 seconds, sometimes 1 second or 5 seconds).</p>
<p>And I have inserted my own code which is a <code>HashMap</code>, into the client API code to store all the key-value pairs.</p>
<p>My goal is run the below code marked with &quot;!!!&quot; as soon as the Hashmap conditions are matched.
I am not an expert in Java data synchronization. Note that <code>Thread.Sleep(3000)</code> would not work, as the key-value pairs are updated on irregular time intervals. Moreover, the value of the same key will also change over time.
I tried the run the program below and it immediately ran through the block of code marked with &quot;!!!&quot;, which is not what I want to achieve.</p>
<p>What is the most efficient solution to tackle this problem ?</p>
<pre><code>Class Testing{
    public static void main(String[] args){
        // connect to API...
        ClientAPI clientAPI = new ClientAPI();
        clientAPI.connect();

        // make a request to retrieve an update of a HashMap
        clientAPI.requestHashMapUpdate();

        // !!! execute the following block of code only if hashmap contains a specific key value pair, i.e. key1, 1 
        // if hashmap does not contain key or the key-value pair do not match , then wait until the key-value pair are matched and run the code
        if(clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)){
            if(clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1){
                    System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
            }
        }

    }
}


Class ClientAPI{
    private HashMap&lt;String,Integer&gt; myHashMap;
    
    // Constructor
    public clientAPI(){
            myHashMap = new HashMap&lt;String,Integer&gt;();
    }

    // callback function of API's requestHashMapUpdate method
    public void updateHashMapKeyValuePair(String key, int value){
        System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
        // code i wrote to update hashmap with key value pair returned from API
        myHashMap.put(key,value);
    }

    // my code to retrieve the latest snapshot of myHashMap
    public HashMap&lt;String,Integer&gt; getMyHashMap(){
        return myHashMap;
    }

}
</code></pre>
","<java><volatile><data-synchronization>","2021-09-12 13:48:05","74","0","2","69152421","<p>Override your client API (if it is not closed to extends):</p>
<pre><code>class MyClientAPI extends ClientAPI {
    ...
    @Override
    public void updateHashMapKeyValuePair(String key, int value) {
        super.updateHashMapKeyValuePair(key, value);
        if(&quot;key1&quot;.equals(key) &amp;&amp; 1 == value)
            System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
    }
    ...
}
</code></pre>
<p>and simply check each new value that comes in.</p>
<p>To use it, only change</p>
<pre><code>ClientAPI clientAPI = new MyClientAPI();
</code></pre>
<p>Another way to solve this is to provide your <code>ClientAPI</code> class with the ability to register listeners</p>
<pre><code>interface UpdateHashMapKeyValuePairListener {
    void digestUpdate(String key, int value);
}

class ClientAPI {
    ...
    private List&lt;UpdateHashMapKeyValuePairListener&gt; listeners = new ArrayList();
    ...
    public void registerUpdateListener(UpdateHashMapKeyValuePairListener u) {
        listeners.add(u);
    }
    ...
    public void updateHashMapKeyValuePair(final String key, final int value) {
        listeners.forEach(u -&gt; u.digestUpdate(key, value));
        ...
    }
    ...
}
</code></pre>
<p>then, anyone who wants to know when a new value enters, just implement this interface, e.g.</p>
<pre><code>...
clientAPI.registerUpdateListener((key, value) -&gt; {
    if(&quot;key1&quot;.equals(key) &amp;&amp; 1 == value)
        System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
});
clientAPI.connect();
...
</code></pre>
"
"69152012","Java: data synchronization issue","<p>My Java program makes a request to client's API and client's API will return a key-value pair on <strong>irregular</strong> time intervals (i.e. not every 5 seconds / 10 seconds, sometimes 1 second or 5 seconds).</p>
<p>And I have inserted my own code which is a <code>HashMap</code>, into the client API code to store all the key-value pairs.</p>
<p>My goal is run the below code marked with &quot;!!!&quot; as soon as the Hashmap conditions are matched.
I am not an expert in Java data synchronization. Note that <code>Thread.Sleep(3000)</code> would not work, as the key-value pairs are updated on irregular time intervals. Moreover, the value of the same key will also change over time.
I tried the run the program below and it immediately ran through the block of code marked with &quot;!!!&quot;, which is not what I want to achieve.</p>
<p>What is the most efficient solution to tackle this problem ?</p>
<pre><code>Class Testing{
    public static void main(String[] args){
        // connect to API...
        ClientAPI clientAPI = new ClientAPI();
        clientAPI.connect();

        // make a request to retrieve an update of a HashMap
        clientAPI.requestHashMapUpdate();

        // !!! execute the following block of code only if hashmap contains a specific key value pair, i.e. key1, 1 
        // if hashmap does not contain key or the key-value pair do not match , then wait until the key-value pair are matched and run the code
        if(clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)){
            if(clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1){
                    System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
            }
        }

    }
}


Class ClientAPI{
    private HashMap&lt;String,Integer&gt; myHashMap;
    
    // Constructor
    public clientAPI(){
            myHashMap = new HashMap&lt;String,Integer&gt;();
    }

    // callback function of API's requestHashMapUpdate method
    public void updateHashMapKeyValuePair(String key, int value){
        System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
        // code i wrote to update hashmap with key value pair returned from API
        myHashMap.put(key,value);
    }

    // my code to retrieve the latest snapshot of myHashMap
    public HashMap&lt;String,Integer&gt; getMyHashMap(){
        return myHashMap;
    }

}
</code></pre>
","<java><volatile><data-synchronization>","2021-09-12 13:48:05","74","0","2","69152875","<p>Check the contents of the HashMap every time it is updated. To achieve this, you can register a listener in <code>Testing</code> class to be triggered every time the <code>HashMap</code> is updated in <code>ClientAPI</code> class.</p>
<p>First define the functional interface for the listener:</p>
<pre><code> @FunctionalInterface
  public interface OnUpdateMapListener {
      void onUpdateMap(Map&lt;String, Integer&gt; map);
  }
</code></pre>
<p>then add the listener in the <code>ClientAPI</code></p>
<pre><code> public class ClientAPI {
        private HashMap&lt;String, Integer&gt; myHashMap;
        private OnUpdateMapListener onUpdateMapListener;
</code></pre>
<p>Define <code>onMapUpdate</code> method to pass the listener's body in <code>ClientAPI</code>:</p>
<pre><code>public void onMapUpdate(OnUpdateMapListener onUpdateMapListener) {
        this.onUpdateMapListener = onUpdateMapListener;
}
</code></pre>
<p>and trigger the listener upon HashMap update in <code>updateHashMapKeyValuePair</code></p>
<pre><code>public void updateHashMapKeyValuePair(String key, int value) {
  System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
  // code i wrote to update hashmap with key value pair returned from API
   myHashMap.put(key, value);
   onUpdateMapListener.onUpdateMap(myHashMap);
 }
</code></pre>
<p>In the main method register the listener and check the map contents. This will be triggered every time the <code>ClientAPI</code> receives new Map contents in <code>updateHashMapKeyValuePair</code> method:</p>
<pre><code>clientAPI.onMapUpdate(stringIntegerHashMap -&gt; {
      // !!! the following code is executed every time the hashMap is updated
       if (clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)) {
           if (clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1) {
             System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
           }
      }
});
</code></pre>
<p><code>ClientAPI</code>  class:</p>
<pre><code>public class ClientAPI {
    private HashMap&lt;String, Integer&gt; myHashMap;

    private OnUpdateMapListener onUpdateMapListener;

    // Constructor
    public ClientAPI() {
        myHashMap = new HashMap&lt;String, Integer&gt;();
    }

    // callback function of API's requestHashMapUpdate method
    public void updateHashMapKeyValuePair(String key, int value) {
        System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
        // code i wrote to update hashmap with key value pair returned from API
        myHashMap.put(key, value);
        onUpdateMapListener.onUpdateMap(myHashMap);
    }

    // my code to retrieve the latest snapshot of myHashMap
    public HashMap&lt;String, Integer&gt; getMyHashMap() {
        return myHashMap;
    }

    public void requestHashMapUpdate() {
      //.....
    }

    public void onMapUpdate(OnUpdateMapListener onUpdateMapListener) {
        this.onUpdateMapListener = onUpdateMapListener;
    }
}
</code></pre>
<p><code>Testing</code> class:</p>
<pre><code> public static void main(String[] args) {
      // connect to API...
        ClientAPI clientAPI = new ClientAPI();
        clientAPI.connect();

        // make a request to retrieve an update of a HashMap
       clientAPI.requestHashMapUpdate();

        clientAPI.onMapUpdate(stringIntegerHashMap -&gt; {
            // !!! the following code is executed every time the hashMap is updated
            if (clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)) {
                if (clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1) {
                    System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
                }
            }
        });

        clientAPI.updateHashMapKeyValuePair(&quot;key1&quot;, 1);
        clientAPI.updateHashMapKeyValuePair(&quot;key1&quot;, 2);
    }
</code></pre>
<p>Results:</p>
<pre><code>Key:key1, Value: 1
Print Me only if key and value are matched !!!!!!!!
Key:key1, Value: 2
</code></pre>
"
"68555327","Watson Knowledge Catalogue missing the categories selection","<p>I'm trying to use WKC within my trial IBM Cloud account, I have the instance created but when I look to create the categories I am unable to see the &quot;categories&quot; option under the &quot;governance&quot; area within the menus. Specifically I only see under governance the following 3 options: Data dashboard, policy manager, business gloassary. Anyone know why I'm not able to see the full set?</p>
","<watson-knowledge-catalog><data-governance>","2021-07-28 06:20:42","66","0","1","68579935","<p>CPD menu items are dependent on the roles the current logged in user has.  If your exploring CPD, try using an ADMIN role.  If you are logged in as admin, ensure you have access to the &quot;category&quot; name in question.</p>
"
"68375516","How to synchnorize files with mobile phone running LineageOS?","<p>I have a mobile phone running LineageOS 16.0. I would like to organize the automatic synchronization of particular directory on mobile phone with some remote directory (e.g. exposed via FTP/SMB/SSH) on the server. Ideally would be to have an option either to make one-way (synchronize only missing files) or two-way (source and target are identical after synchronization) modes.</p>
<p>In particular above can be achieved by using the <a href=""https://en.wikipedia.org/wiki/Rsync"" rel=""nofollow noreferrer""><code>rsync</code></a> utility. I see the following options:</p>
<h3 id=""mobile-phone-is-exposing-the-folder-via-e.g.ssh-6hwt"">Mobile phone is exposing the folder via e.g. SSH</h3>
<p>This would require LineageOS running SSH server. In this case <code>rsync</code> could be run on the server (periodically or on some event).</p>
<ul>
<li>Is SSHD server already installed on LineageOS (<a href=""https://binfalse.de/2018/09/06/native-ssh-server-on-linageos/"" rel=""nofollow noreferrer"">Native SSH server on LinageOS</a>)? If not, is it possible to install it from a package?</li>
<li>Will running in the background <code>sshd</code> daemon drain the mobile battery?</li>
<li>How difficult would be to add it to startup (<a href=""https://lisas.de/%7Ealex/?p=237"" rel=""nofollow noreferrer"">SSHD on LineageOS</a>, <a href=""https://h4des.org/blog/index.php?/archives/359-Android-LineageOS-16-Execute-Script-on-Start-Up.html"" rel=""nofollow noreferrer"">Execute Script on Start Up</a>)?</li>
</ul>
<h3 id=""mobile-phone-is-running-rsync-857p"">Mobile phone is running rsync</h3>
<ul>
<li>This requires <a href=""https://github.com/LineageOS/android_external_rsync"" rel=""nofollow noreferrer""><code>rsync</code> installation to LineageOS</a>. Maybe there is ready-to-use package or will it require the compilation?</li>
<li>I would need an icon (or some other easy way) to execute <code>rsync</code> on mobile. How to organize that?</li>
</ul>
<p>If you see other more handy/easy options, please describe how to setup/organize them.</p>
<p>P.S. Dropbox or any other cloud solutions is not an option (out of scope for this question).</p>
<p>Thanks in advance!</p>
","<android><rsync><data-synchronization><lineageos>","2021-07-14 09:31:04","125","0","1","68471640","<p>I use the FolderSync app for this (it supports connecting to an SSH server at scheduled intervals, although I personally use it with Nextcloud). Syncthing is also a good solution I've used in the past, although you'll need to install that on your server also (no cloud required).</p>
<p>Trying to run an rsync server on a phone doesn't sound appealing, but there are a couple of rsync client apps in the play store.</p>
"
"68355695","Is Core Data necessary for across device data synchronization with iCloud?","<p>With <code>NSPersistentCloudKitContainer</code> developers can easily enable data synchronization across devices for their App with Core Data stack.</p>
<p>How about for a new app that requires local data persistent (offline data storage on the client) and across device data synchronization (data available on iPhone, iPad, macOS with the same iCloud account sign-in), is Core Data still required? would CloudKit be used as a replacement for Core Data?</p>
<p>Is Core Data a stack can be skipped with CloutKit?</p>
","<ios><swift><core-data><icloud><cloudkit>","2021-07-13 02:15:43","344","-1","2","68355995","<p>It seems CloudKit is a transport mechanism and it is not meant to be used as local storage or persistent mechanism. If there is local offline storage need Core Data should still be used.</p>
"
"68355695","Is Core Data necessary for across device data synchronization with iCloud?","<p>With <code>NSPersistentCloudKitContainer</code> developers can easily enable data synchronization across devices for their App with Core Data stack.</p>
<p>How about for a new app that requires local data persistent (offline data storage on the client) and across device data synchronization (data available on iPhone, iPad, macOS with the same iCloud account sign-in), is Core Data still required? would CloudKit be used as a replacement for Core Data?</p>
<p>Is Core Data a stack can be skipped with CloutKit?</p>
","<ios><swift><core-data><icloud><cloudkit>","2021-07-13 02:15:43","344","-1","2","68356412","<p>CloudKit is transport only. Core Data is local persistence only. You can use them together, but you can also use them separately. If you want to use CloudKit but not Core Data, you would need to write your own code to handle local persistence. You might, for example, use plain SQLite, with a wrapper like <a href=""https://github.com/groue/GRDB.swift"" rel=""nofollow noreferrer"">GRDB</a> for local persistence. Or some other way. There are lots of ways to store local data on apps. However, <code>NSPersistentCloudKitContainer</code> is part of Core Data (thatâ€™s what makes it work with CloudKit), so you would not use that.</p>
"
"68287830","Python : Changing the original data using a for loop","<p>I have some really big txt files (&gt; 2 gb) where the quality of the data is not good.
In some columns (that should be integer), for values below 1000.00 , '.' is used as the decimal point (e.g. 473.71886) but for values above 1000.00 then the form is like that 7.541,72419. So ',' is used as the decimal point and '.' for the thousands separator.</p>
<p>I have already read the text file using pd.read_csv with the below command</p>
<pre><code>df = pd.read_csv('mseg.txt',delimiter=(&quot;#|#&quot;),nrows=(1000),engine = 'python')
</code></pre>
<p>I tried to build the regular expression to be used but it doesn't work
<code>pattern = &quot;[0-9]+[\.][0-9]+[,][0-9]+&quot;</code></p>
<p>I was thinking of using the below code to correct the above problem but it doesn't work. (in the below code I used as <code>pattern2 = &quot;,&quot;</code> to test the code)</p>
<pre><code>for i in df.iloc[:,-5]:
    df3 = []
    if re.search(pattern2,i):
        k= i.replace(&quot;.&quot;,&quot;&quot;)
        print(k)
        df3.append(k)
    else:
        df3.append(k)
return dfe3
</code></pre>
<p>The <code>print(k)</code> in the loop seems to work fine but when I run df3 then I get the below output</p>
<pre><code>['\x00 \x003\x004\x00\x006\x006\x005\x00,\x002\x001\x007\x006\x000\x00']
</code></pre>
<p>Could anyone help?</p>
","<python><pandas><for-loop><data-quality>","2021-07-07 14:20:53","71","0","2","68288365","<p>You can try this:</p>
<pre><code>&gt;&gt;&gt; df
             0
0    473.71886
1  7.541,72419
</code></pre>
<pre><code>&gt;&gt;&gt; df[0].str.split(r'[^\d]') \
         .apply(lambda x: f&quot;{''.join(x[:-1])}.{x[-1]}&quot;)

0      473.75410
1    71886.72419
dtype: float64
</code></pre>
"
"68287830","Python : Changing the original data using a for loop","<p>I have some really big txt files (&gt; 2 gb) where the quality of the data is not good.
In some columns (that should be integer), for values below 1000.00 , '.' is used as the decimal point (e.g. 473.71886) but for values above 1000.00 then the form is like that 7.541,72419. So ',' is used as the decimal point and '.' for the thousands separator.</p>
<p>I have already read the text file using pd.read_csv with the below command</p>
<pre><code>df = pd.read_csv('mseg.txt',delimiter=(&quot;#|#&quot;),nrows=(1000),engine = 'python')
</code></pre>
<p>I tried to build the regular expression to be used but it doesn't work
<code>pattern = &quot;[0-9]+[\.][0-9]+[,][0-9]+&quot;</code></p>
<p>I was thinking of using the below code to correct the above problem but it doesn't work. (in the below code I used as <code>pattern2 = &quot;,&quot;</code> to test the code)</p>
<pre><code>for i in df.iloc[:,-5]:
    df3 = []
    if re.search(pattern2,i):
        k= i.replace(&quot;.&quot;,&quot;&quot;)
        print(k)
        df3.append(k)
    else:
        df3.append(k)
return dfe3
</code></pre>
<p>The <code>print(k)</code> in the loop seems to work fine but when I run df3 then I get the below output</p>
<pre><code>['\x00 \x003\x004\x00\x006\x006\x005\x00,\x002\x001\x007\x006\x000\x00']
</code></pre>
<p>Could anyone help?</p>
","<python><pandas><for-loop><data-quality>","2021-07-07 14:20:53","71","0","2","68288405","<p>I would suggest to do the following:</p>
<p>If there is a ',' in the number replace it with a '.' but get rid of the ',' before.
So you would change a 1.234,567 to 1234,567 and then to 1234.567.
Then all of your numbers should be in the same format.</p>
<pre><code>df3 = []
for index,i in df.iloc[:,-5]:  
    if ',' in i:
        i= i.replace(&quot;.&quot;,&quot;&quot;).replace(',','.')
    df3[index] = i
</code></pre>
"
"68127250","Using SQLAdapter and SQLCommandBuilder without a Primary Key","<p>I'm exploring the use of <code>SqlCommandBuilder</code> alongside <code>Adapter.Update()</code> to synchronize a DataGridView with an SQL Database table.</p>
<p>I want to auto-generate SQL Update statements using <code>SqlCommandBuilder.GetUpdateCommand()</code>, however, it fails with
&quot;<code>Dynamic SQL generation for the UpdateCommand is not supported against a SelectCommand that does not return any key column information</code>&quot;. This makes sense, because my table <strong>doesn't have a primary key</strong>.</p>
<p>I cannot set the primary key on the source table, but I do have an identity column.
I'd like to specify to the command builder which column to use as the primary key. There is such a feature on the DataTable class, but it seems to have no effect on the <code>SqlCommandBuilder</code>.</p>
<p>I tried the following:</p>
<pre><code>// Add Primary Key to help command builder identify unique rows
Table.PrimaryKey = new DataColumn[] { Table.Columns[&quot;ComponentID&quot;] };
</code></pre>
<p>But it seems that this information does not propagate to the <code>SqlDataAdapter</code> and <code>SqlCommandBuilder</code> because I still get the error.</p>
<p>Here's the order I've tried:</p>
<pre><code>// get data
Adapter.Fill(Table);

// specify primary key column
Table.PrimaryKey = new DataColumn[] { Table.Columns[&quot;ComponentID&quot;] };

cmdBuilder = new SqlCommandBuilder(Adapter);

cmdBuilder.GetUpdateCommand() // &lt;-- Error here
</code></pre>
<p>Are there any solutions here at all, or do I have to specify the update and insert statements?</p>
","<c#><sql><.net><data-synchronization>","2021-06-25 07:39:17","531","2","1","68146181","<p>So, as pointed out by @PanagiotisKanavos, the <code>SqlCommandBuilder</code> does not support tables without primary keys, even if you set it in the <code>DataTable</code> object.</p>
<p>Therefore, I had no choice but write my own Command Builder.</p>
<p>To use it, you need to provide:</p>
<ul>
<li>The SqlConnection to use</li>
<li>The Database (if not provided in the connection)</li>
<li>The SQLAdapter, with the Select Command already set (there's a constructor for that)</li>
</ul>
<p>How you use it:</p>
<pre><code>string selectQuery = &quot;SELECT * FROM [dbCache].[dbo].[Component] ORDER BY [ComponentType] DESC&quot;;

// Initialize the SqlDataAdapter object by specifying a Select command 
// that retrieves data from the table.
Adapter = new SqlDataAdapter(selectQuery, Connection)
{
    FillLoadOption = LoadOption.PreserveChanges,
    MissingSchemaAction = MissingSchemaAction.AddWithKey
};

// build all sql commands
Adapter = SQLCommandBuilder.BuildAll(Adapter, Connection);
</code></pre>
<p>Next, the full class code:</p>
<pre><code>public static class SQLCommandBuilder
{
    public enum CommandType
    {
        Update = 0,
        Insert = 1,
        Delete = 2
    }

    /// &lt;summary&gt;
    /// Build and add the insert, update and delete commands to the given SqlAdapter
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;adapter&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;connection&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;database&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;idColumns&quot;&gt;&lt;/param&gt;
    /// &lt;returns&gt;the modified adapter&lt;/returns&gt;
    public static SqlDataAdapter BuildAll(
        SqlDataAdapter adapter, SqlConnection connection, string database = null, string[] idColumns = null
        )
    {
        DataTable data = new DataTable();
        // fill datatable with select data
        adapter.Fill(data);

        if (database == null)
        {
            if (string.IsNullOrEmpty(connection.Database))
            {
                throw new ArgumentException(
                    &quot;Could not determine database from connection object. Please specify it manually&quot;
                    );
            }
            // get database from connection
            database = connection.Database;
        }
        // get table name
        string table = data.TableName;
        // get all column names
        string[] allColumns = data.Columns.Cast&lt;DataColumn&gt;()
            .Select(col =&gt; col.ColumnName).ToArray();
        
        // only get id columns if the user has not manually specified them
        if (idColumns == null)
        {
            // get id columns from the table. This includes any unique or auto-incrementing column
            idColumns = data.Columns.Cast&lt;DataColumn&gt;()
                .Where(col =&gt; col.AutoIncrement || col.Unique)
                .Select(col =&gt; col.ColumnName)
                .ToArray();

            // if no id columns found
            if (idColumns.Length == 0)
            {
                // throw an error
                throw new Exception(&quot;No ID columns found in the table!&quot;);
            }
        }
        else
        {
            // if the specfified columns don't exist
            if (idColumns.All(id =&gt; allColumns.Contains(id, StringComparer.CurrentCultureIgnoreCase)))
            {
                // throw an error
                throw new ArgumentException(&quot;Provided ID columns do not exist in the table!&quot;);
            }
        }
        

        // generate all commands
        adapter.InsertCommand =
            BuildCommand(CommandType.Insert, connection, database, table, allColumns, idColumns);
        adapter.UpdateCommand =
            BuildCommand(CommandType.Update, connection, database, table, allColumns, idColumns);
        adapter.DeleteCommand =
            BuildCommand(CommandType.Delete, connection, database, table, allColumns, idColumns);

        // return the modified adapter
        return adapter;
    }

    /// &lt;summary&gt;
    /// Build a command of the given type using the provided parameters
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;cmdtype&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;connection&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;database&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;table&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;allColumns&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;idColumns&quot;&gt;&lt;/param&gt;
    /// &lt;returns&gt;&lt;/returns&gt;
    public static SqlCommand BuildCommand(
        CommandType cmdtype, SqlConnection connection, string database, string table, 
        string[] allColumns, string[] idColumns
        )
    {
        if (allColumns == null || allColumns.Length == 0)
        {
            throw new ArgumentNullException(&quot;allColumns&quot;, &quot;allColumns cannot be null or empty!&quot;);
        }
        if (idColumns == null || idColumns.Length == 0)
        {
            throw new ArgumentNullException(&quot;idColumns&quot;, &quot;idColumns cannot be null or empty!&quot;);
        }

        string strCommand = null;

        switch (cmdtype)
        {
            case CommandType.Insert:

                // get columns to set values for. Id columns not included because they should
                // be set by the table
                string[] insertCols = allColumns.Except(idColumns).ToArray();

                strCommand =
                    &quot;INSERT INTO [&quot; + database + &quot;].[dbo].[&quot; + table + &quot;]\n&quot; +
                    &quot;([&quot; + string.Join(&quot;], [&quot;, insertCols) + &quot;])\n&quot; +
                    &quot;VALUES (@&quot; + string.Join(&quot;, @&quot;, insertCols.Select(s =&gt; s.Replace(&quot; &quot;, &quot;&quot;))) + &quot;)&quot;;
                break;
            case CommandType.Update:
                // compare each id column to a paremeterized variable of the same name prefixed with &quot;old&quot;
                string[] idCompsOld = idColumns
                    .Select(col =&gt; &quot;[&quot; + col + &quot;] = @old&quot; + col.Replace(&quot; &quot;, &quot;&quot;))
                    .ToArray();

                // create a setting statement. Don't set id columns, as they should never be modifiable
                string[] setStatement = allColumns.Except(idColumns)
                    .Select(col =&gt; &quot;[&quot; + col + &quot;] = @&quot; + col.Replace(&quot; &quot;, &quot;&quot;))
                    .ToArray();

                strCommand =
                    &quot;UPDATE [&quot; + database + &quot;].[dbo].[&quot; + table + &quot;]\n&quot; +
                    &quot;SET &quot; + string.Join(&quot;, &quot;, setStatement) + &quot;\n&quot; +
                    &quot;WHERE &quot; + string.Join(&quot; AND &quot;, idCompsOld);
                break;
            case CommandType.Delete:
                // compare each id column to a paremeterized variable of the same name
                string[] idComps = idColumns
                    .Select(col =&gt; &quot;[&quot; + col + &quot;] = @&quot; + col.Replace(&quot; &quot;, &quot;&quot;))
                    .ToArray();
                strCommand =
                    &quot;DELETE FROM [&quot; + database + &quot;].[dbo].[&quot; + table + &quot;]\n&quot; +
                    &quot;WHERE &quot; + string.Join(&quot; AND &quot;, idComps);
                break;
        }

        SqlCommand command = new SqlCommand(strCommand, connection);

        // cycle through all columns
        for( int i = 0; i &lt; allColumns.Length; i++)
        {
            string col = allColumns[i];

            // create a parameter for that column
            SqlParameter para = new SqlParameter()
            {
                ParameterName = &quot;@&quot; + col.Replace(&quot; &quot;, &quot;&quot;),
                SourceColumn = col
            };
            // add the paramter to the command
            command.Parameters.Add(para);

            // in the special case of the update statement, extra parameters are needed for the
            // old values
            if (cmdtype == CommandType.Update)
            {
                // create a parameter for that column
                para = new SqlParameter()
                {
                    ParameterName = &quot;@old&quot; + col.Replace(&quot; &quot;, &quot;&quot;),
                    SourceColumn = col,
                    SourceVersion = DataRowVersion.Original
                };
                // add the paramter to the command
                command.Parameters.Add(para);
            }
        }

        return command;
    }
}
</code></pre>
<p>Using the following code to print the commands:</p>
<pre><code>// Display the Update, Insert, and Delete commands that were automatically generated
// by the SQLCommandBuilder.
Console.WriteLine(&quot;Update command : &quot;);
Console.WriteLine(Adapter.UpdateCommand.CommandText);
Console.WriteLine();

Console.WriteLine(&quot;Insert command : &quot;);
Console.WriteLine(Adapter.InsertCommand.CommandText);
Console.WriteLine();

Console.WriteLine(&quot;Delete command : &quot;);
Console.WriteLine(Adapter.DeleteCommand.CommandText);
Console.WriteLine();
</code></pre>
<p>I get:</p>
<pre><code>Update command : 
UPDATE [dbCache].[dbo].[Component]
SET [ComponentType] = @ComponentType, [Drawings] = @Drawings, [StatusNo] = @StatusNo
WHERE [ComponentlD] = @oldComponentlD 

Insert command :
INSERT INTO [dbCache].[dbo].[Component]
([ComponentType], [Drawings], [StatusNo])
VALUES (@ComponentType, @Drawings, @StatusNo) 

Delete command :
DELETE FROM [dbCache].[dbo].[Component]
WHERE [ComponentlD] = @ComponentlD 
</code></pre>
<p>I have tested the insert, update and delete statements and they seem to all work!</p>
"
"67697063","How to Import Google workspace data automatically to Big Query database?","<p>How to daily import Google workspace data automatically to Big Query database?</p>
<p>I'm new to Big Query and i can do it manually but i want to automate this process. Thanks.</p>
","<import><google-bigquery><google-cloud-sql><data-synchronization><database-link>","2021-05-26 00:16:03","68","0","1","67697406","<p>With BigQuery you can create <code>external tables</code>, that enable you to query data that is stored in your Google Drive (CSV, Avro, JSON, or Google Sheets documents).</p>
<p>You can find a nice how-to <a href=""https://cloud.google.com/bigquery/external-data-drive"" rel=""nofollow noreferrer"">here</a>.</p>
"
"67154576","Data Quality Process - defining rules","<p>I am working on a <strong>Data Quality Monitoring</strong> project which is new me.
I started with a <strong>Data Profiling</strong> to analyse my data and have a global view of it.
Next, i thought about defining some data quality rules, but i'm a little bit confused about how to implement these rules.
If u guys can guide me a little bit as i'm totally new to this.</p>
","<python><monitoring><data-quality>","2021-04-19 00:12:42","257","1","1","71782114","<p>This is quite ambiguous question but I try to guess a few tips how to start. Since you are a new to data quality and want already implementation hints, lets start from that.</p>
<p>Purpose: Data quality monitoring system wants to a) recognize error and b) trigger next step how to handle it.</p>
<p>First, build a data quality rule for your data set. The rule can be attribute, record, table or cross-table rule. Lets start with attribute level rule. Implement a rule that recognizes that attribute content does not have '@' in it. Run it to email attributes and create an error record for each row that does not have '@' in email attribute. Error record should have these attributes:</p>
<p>ErrorInstanceID; ErrorName; ErrorCategory; ErrorRule; ErrorLevel; ErrorReaction; ErrorScript; SourceSystem; SourceTable; SourceRecord; SourceAttribute; ErrorDate;</p>
<p>&quot;asd2321sa1&quot;; &quot;Email Format Invalid&quot;; &quot;AttributeError&quot;; &quot;Does not contain @&quot;; &quot;Warning|Alert&quot;; &quot;Request new email at next login&quot;; &quot;ScriptID x&quot;; &quot;Excel1&quot;; &quot;Sheet1&quot;; &quot;RowID=34&quot;; &quot;Column=Email&quot;; &quot;1.1.2022&quot;</p>
<p>MONITORING SYSTEM</p>
<p>You need to make above scripts configurable so that you can change systems, tables and columns as well as rules easily. When ran on top of data sets, they will all populate error records to the same structures resulting in a consistent and historical storage of all errors. You should be able to build reports about existing errors in specific systems, trends of errors appearing or getting fixed and so on.</p>
<p>Next, you need to start building a full-sale data quality metadata repository with a proper data model and design a suitable historical versioning for the above information. You need to store information like which rules were ran and when, which systems and tables they checked, and so on. To detect which systems have bee included in monitoring and also to recognize if systems are not monitored with correct rules. In practice, quality monitoring for data quality monitoring system. You should have statistics which systems are monitored with specific rules, when they were ran last time, aggregates of inspected tables, records and errors.</p>
<p>Typically, its more important to focus on errors that need immediate attention and &quot;alert&quot; an end-user to go fix the issue or triggers a new workflow or flag in source system. For example, invalid emails might be categorized as alerts and be just aggregate statistics. We have 2134223 invalid emails. Nobody cares. However, it might be more important to recognize invalid email of a person who has ordered his bills as digital invoices to his email. Alert. That kind of error (Invalid Email AND Email Invoicing) should trigger an alert and set up a flag in CRM for end users to try get email fixed. There should not be any error records for this error. But this kind of rule should be ran on top of all systems that store customer contact and billind preferences.</p>
<p>For a technical person, I could recommend this book. It's a good book that goes deeper in technical and logical issues of data quality assessment and monitoring systems. There is also a small metadata model for data quality metadata structures. <a href=""https://rads.stackoverflow.com/amzn/click/com/0977140024"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">https://www.amazon.com/Data-Quality-Assessment-Arkady-Maydanchik/dp/0977140024/</a></p>
"
"67039582","How to build a collaborative app using Google Drive as storage?","<p>I am writing an app with JSON as the file format to store the data. I want the users to store those JSON files in their own Google drive so that they can share the files to their friends as they want. And their friends can also open my app at the same time to collaborative real time.</p>
<p>Google Drive API allows an app to create and delete files in users' Google drive. But how can my app support collaboration like Google Docs or Google Sheets, so that 2 or more people can work on the same file and the change will be saved back to the Google drive and updated on the other side in the real time?</p>
","<google-drive-api><collaboration><data-synchronization><google-drive-shared-drive>","2021-04-10 21:50:24","354","1","1","67054942","<h3>Answer:</h3>
<p><em>You need to create a <a href=""https://developers.google.com/drive/api/v3/reference/permissions"" rel=""nofollow noreferrer"">Permission</a> for each user/group you wish to give access to the file.</em></p>
<h3>Example:</h3>
<p>Using the <a href=""https://developers.google.com/drive/api/v3/reference/permissions/create"" rel=""nofollow noreferrer"">Permissions.create</a> API method, you can create a permission and attach it to the file.</p>
<p>The following example shows how to give a user write permissions using the Google API JavaScript client:</p>
<pre class=""lang-js prettyprint-override""><code>// Copyright 2021 Google LLC.
// SPDX-License-Identifier: Apache-2.0

function execute() {
  return gapi.client.drive.permissions.create({
    &quot;fileId&quot;: &quot;-some-do&quot;,
    &quot;resource&quot;: {
      &quot;role&quot;: &quot;writer&quot;,
      &quot;type&quot;: &quot;user&quot;,
      &quot;emailAddress&quot;: &quot;someuser@domain.com&quot;
    }
  })
}
</code></pre>
<h3>References:</h3>
<ul>
<li><a href=""https://developers.google.com/drive/api/v3/reference/permissions"" rel=""nofollow noreferrer"">Permissions | Google Apps Script | Google Developers</a></li>
<li><a href=""https://developers.google.com/drive/api/v3/reference/permissions/create"" rel=""nofollow noreferrer"">Permissions: create | Google Apps Script | Google Developers</a></li>
</ul>
"
"66997667","Docker multi postgres containers with one mount point","<p>I have two Postgres databases and I want to sync data between themes.</p>
<p>So far I have these two containers, exactly the same with different posts and different names.</p>
<pre><code>docker container run --name='p1' -d -p 5435:5432 -v /tmp/dbs/test/:/var/lib/postgresql/data postgres


docker container run --name='p2' -d -p 5436:5432 -v /tmp/dbs/test/:/var/lib/postgresql/data postgres  
</code></pre>
<p>The problem happens when something changed.</p>
<p>If I change something in p1 like insert a row, then I can't see it in p2.</p>
<p>But if I kill, and run containers again, then I can see the inserted data in both of themes.</p>
<p>Why this is happening?<br />
Is there a way to sync data between themes?</p>
","<postgresql><docker><docker-volume><data-synchronization>","2021-04-08 05:08:56","443","2","1","66998188","<p>Running two postmaster processes on the same files is a sure road to data corruption. Don't do that.</p>
<p>You cannot have multi-master replication with standard PostgreSQL, but you can have a read-only standby server.</p>
"
"66966682","Data quality - Missing values (Pandas)","<p>I'm working on a data quality project. I'm trying to generate a data quality report using pandas-profiling profileReport but when i verify the report it says that i have no missing values while i do have empty cells.
Or do you have any other suggestion
<a href=""https://i.stack.imgur.com/w7BxC.png"" rel=""nofollow noreferrer"">Result</a></p>
<pre><code>df = pd.read_excel('D:/SDAD/PFE/bi_bpcustomer.xls')
print(df.dtypes)
reportCl=ProfileReport(df)
reportCl.to_file(output_file='rapportClient.html')
</code></pre>
<p>Here's a part of my xls file showing missing cells:
<a href=""https://i.stack.imgur.com/pcCk0.png"" rel=""nofollow noreferrer"">xls file</a></p>
","<python><pandas><missing-data><data-quality>","2021-04-06 10:11:08","357","1","1","66974533","<p>It's fine, i found a way by adding this to my code:</p>
<pre><code>missing_values = [&quot;&quot;,&quot; &quot;]
client= pd.read_excel('D:/SDAD/PFE/bi_bpcustomer.xls',na_values = 
missing_values)
</code></pre>
"
"66892108","Data quality - check if all values in a character column are numbers in R","<p>I am looking to perform data quality on numerous system generated tables. One of the checks is to see if all values in a character column are only numbers. I am looking to know the number columns where this check is true. Using the following table as an example I would want to identify that  two columns (code and age) are character columns that consist of only numeric values.</p>
<p><strong>Table Structure</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column Name</th>
<th>Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>character</td>
</tr>
<tr>
<td>type</td>
<td>character</td>
</tr>
<tr>
<td>code</td>
<td>character</td>
</tr>
<tr>
<td>member_id</td>
<td>integer</td>
</tr>
<tr>
<td>collection_date</td>
<td>date</td>
</tr>
<tr>
<td>age</td>
<td>character</td>
</tr>
<tr>
<td>height</td>
<td>double</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table Values</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column Name</th>
<th>Column Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>only letters</td>
</tr>
<tr>
<td>type</td>
<td>only letters</td>
</tr>
<tr>
<td>code</td>
<td>only numbers</td>
</tr>
<tr>
<td>member_id</td>
<td>only numbers</td>
</tr>
<tr>
<td>collection_date</td>
<td>only dates</td>
</tr>
<tr>
<td>age</td>
<td>only numbers</td>
</tr>
<tr>
<td>height</td>
<td>only numbers</td>
</tr>
</tbody>
</table>
</div>
<p>I am having issues thinking of the logic that is required to do this. What I have done thus far is:</p>
<p>To select only columns that are character data types</p>
<pre><code>df %&gt;%
  dplyr::select_if(is.character)
</code></pre>
<p>To validate that all values in the column are numeric (or null, which is fine)</p>
<pre><code>sum(varhandle::check.numeric(df$code)) == nrow(df)
</code></pre>
<p>I am hoping to build a function that performs this across all columns and stores the number where the check is true into a column (i.e. dplyr::mutate), but I am not sure how to structure this; is it an across, an apply, or something else. Or is there any existing function/package that would perform this task? Any help is appreciated.</p>
","<r><data-quality>","2021-03-31 16:58:13","910","1","1","66892136","<p>We could construct the condition within <code>select</code> itself</p>
<pre><code>library(dplyr)
iris %&gt;%
   select(where(~ all(varhandle::check.numeric(.)))) 
</code></pre>
<hr />
<p>It is not clear whether <code>numeric</code> columns with mismatched type or columns having some character elements and thus got converted to <code>class</code> <code>character</code>.  If it is the former, then add a <code>type.convert</code> before the <code>select</code> and then get only the numeric columns</p>
<pre><code>df %&gt;%
   type.convert(as.is = TRUE) %&gt;%
   select(where(is.numeric))
</code></pre>
"
"66861075","How to use hasUniqueness check in PyDeequ?","<p>I'm using <a href=""https://github.com/awslabs/python-deequ"" rel=""nofollow noreferrer"">PyDeequ</a> for data quality and I want to check the uniqueness of a set of columns. There is a Check method <code>hasUniqueness</code> but I can't figure how to use it.</p>
<p>I'm trying:</p>
<pre><code>check.hasUniqueness([col1, col2], ????) 
</code></pre>
<p>But what should we use here for the assertion function in place of <code>????</code>?</p>
<p>Has anyone tried the check <code>hasUniqueness</code> for a combination of columns?</p>
","<python><apache-spark><pyspark><data-quality><amazon-deequ>","2021-03-29 20:18:00","1250","1","1","66861855","<p><a href=""https://github.com/awslabs/python-deequ/blob/e358c835172a3a74c33a246550c2783be5df6123/pydeequ/checks.py#L242"" rel=""nofollow noreferrer""><code>hasUniqueness</code></a> takes a function that accepts an in/float parameter and returns a boolean :</p>
<blockquote>
<p>Creates a constraint that asserts any uniqueness in a single or
combined set of key columns. Uniqueness is the fraction of unique
values of a column(s)  values that occur exactly once.</p>
</blockquote>
<p>Here's an example of usage :</p>
<pre><code>df.show()
#+---+---+
#|  a|  b|
#+---+---+
#|foo|  1|
#|bar|  0|
#|baz|  1|
#|bar|  0|
#+---+---+
</code></pre>
<p>In this dataframe, the combination of columns <code>a</code> and <code>b</code> has 2 values that occur exactly once <code>(foo, 1)</code> and <code>(baz, 1)</code> so <code>Uniqueness = 0.5</code> here. Let's verify it using the check constraint :</p>
<pre><code>from pydeequ.checks import CheckLevel, Check
from pydeequ.verification import VerificationResult, VerificationSuite

result = VerificationSuite(spark).onData(df).addCheck(
    Check(spark, CheckLevel.Warning, &quot;test hasUniqueness&quot;)
        .hasUniqueness([&quot;a&quot;, &quot;b&quot;], lambda x: x == 0.5)
).run()

result_df = VerificationResult.checkResultsAsDataFrame(spark, result)

result_df.select(&quot;constraint_status&quot;).show()

#+-----------------+
#|constraint_status|
#+-----------------+
#|          Success|
#+-----------------+
</code></pre>
"
"66702353","Is it possible to get lineage metadata from the pipeline in my Data Fusion Action plugin?","<p>I'm trying to get data lineage metadata like data source/schema and data target/schema in a custom Action plugin which gets executed after the successful run of the other steps in the pipeline.</p>
<p>I have a basic Action plugin that executes but I'm having trouble finding a way to get the metadata I'm after.</p>
<p>The use case I'm working on is pushing data lineage into a third party data governance tool.</p>
<p>I would very much appreciate if someone could point me in the right direction!</p>
","<java><google-cloud-platform><google-cloud-data-fusion><cdap><data-governance>","2021-03-19 04:02:38","430","1","1","66852007","<p>As was suggested in my comment, you might consider to use CDAP <em><a href=""https://cdap.atlassian.net/wiki/spaces/DOCS/pages/480346522/System+Metadata"" rel=""nofollow noreferrer"">system metadata</a></em> inventory to extract the particular property for the desired entity via CDAP existed RESTfull API methods by sending appropriate HTTP request as explained in CDAP <a href=""https://cdap.atlassian.net/wiki/spaces/DOCS/pages/477692187/Metadata+Microservices"" rel=""nofollow noreferrer""><em>Metadata Microservices</em></a> documentation. Said this entity properties can also depict <a href=""https://cdap.atlassian.net/wiki/spaces/DOCS/pages/477692187/Metadata+Microservices#Field-Lineage-Summary"" rel=""nofollow noreferrer"">lineage</a> of dataset fields returning the result in JSON format.</p>
<p>However, adjusting appropriate HTTP method mostly depends on the particular use case, therefore feel free to further contribute and share your further discovering.</p>
"
"66102886","Great Expectations: base_directory must be an absolute path if root_directory is not provided","<p>This is about Great Expectations module in python primarily used for data quality checks (I found their documentation to be inadequate). So I've been trying to set up the data context on my notebook (using a local datasource) - as mentioned in:</p>
<p><a href=""https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_data_contexts/how_to_instantiate_a_data_context_without_a_yml_file.html#how-to-guides-configuring-data-contexts-how-to-instantiate-a-data-context-without-a-yml-file"" rel=""nofollow noreferrer"">https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_data_contexts/how_to_instantiate_a_data_context_without_a_yml_file.html#how-to-guides-configuring-data-contexts-how-to-instantiate-a-data-context-without-a-yml-file</a></p>
<p>Following is my code :</p>
<pre><code>from great_expectations.data_context.types.base import DataContextConfig
from great_expectations.data_context.types.base import DatasourceConfig
from great_expectations.data_context.types.base import FilesystemStoreBackendDefaults
from great_expectations.data_context import BaseDataContext

data_context_config = DataContextConfig(
    datasources={
        &quot;debaprc_test&quot;: DatasourceConfig(
            class_name=&quot;PandasDatasource&quot;,
            batch_kwargs_generators={
                &quot;subdir_reader&quot;: {
                    &quot;class_name&quot;: &quot;SubdirReaderBatchKwargsGenerator&quot;,
                    &quot;base_directory&quot;: &quot;/Users/debaprc/Downloads&quot;              
                }
            },
        )
    },
    store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=&quot;/Users/debaprc/GE_Test/New/&quot;)
)

context = BaseDataContext(project_config=data_context_config)
</code></pre>
<p>And this is the error I get:</p>
<p><code>base_directory must be an absolute path if root_directory is not provided</code></p>
<p>What am I doing wrong?</p>
","<python><data-quality><great-expectations>","2021-02-08 13:45:59","2394","4","1","66105460","<p>Thank you so much for using Great Expectations. That is a known issue with our latest upgrade of the Checkpoints feature, which was fixed on our develop branch. Please install from the develop branch or wait until our next release 0.13.9 coming this week.</p>
"
"66071767","Theoretically, are DATE and TIME two different variables?","<p>Iâ€™m curious to find out if, in terms of tidy data principles, a column containing â€œdate and timeâ€ ( 1/1/21 11:31) would be considered as a single variable or tow separate ones?</p>
","<data-processing><data-quality>","2021-02-05 23:03:00","28","-1","1","66071796","<p>Timestamp:</p>
<p>An important difference is that DATETIME represents a date (as found in a calendar) and a time (as can be observed on a wall clock), while TIMESTAMP represents a well defined point in time.</p>
"
"65949690","Look for best approach replicating oracle table in S3 bucket","<p>My problem:</p>
<p>I need a data pipeline created from my organizationâ€™s Oracle DB (Oracle Cloud Infrastructure) to an AWS S3 bucket. Ideally, I would love for there to be some mechanism for oracle to push new data that has entered the database to be pushed to an S3 bucket as it is added (in whatever format).</p>
<p>Question:</p>
<p>Is this possible with Oracle native, specifically Oracle Cloud Infrastructure?</p>
<p>Or would is there a better solution you have seen?</p>
<p>Note:
I have seen AWS has the Data Sync product, this seems like it could facilitate with this problem, however I am not sure if it is suitable for this specific problem.</p>
","<oracle><amazon-s3><data-synchronization><oracle-cloud-infrastructure>","2021-01-29 06:16:59","791","1","1","65953426","<p>An S3 bucket is object storage; it can only hold complete files. You cannot open and update an existing file like you would in a normal file system, even just to add new rows. You will need to construct your whole file outside of Oracle and then push it to S3 with some other mechanism.</p>
<p>You may want to consider the following steps:</p>
<ul>
<li>Export your data from Oracle Cloud into Oracle Object Storage (similar to S3) using the Oracle Cloud's integration with their object storage. (<a href=""https://blogs.oracle.com/datawarehousing/the-simplest-guide-to-exporting-data-from-autonomous-database-directly-to-object-storage"" rel=""nofollow noreferrer"">https://blogs.oracle.com/datawarehousing/the-simplest-guide-to-exporting-data-from-autonomous-database-directly-to-object-storage</a>)</li>
</ul>
<p>THEN:</p>
<ul>
<li>Let the customer access the Oracle Object Store as they normally would access S3, using Oracle's Amazon S3 Compatibility API. (<a href=""https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/s3compatibleapi.htm"" rel=""nofollow noreferrer"">https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/s3compatibleapi.htm</a>)</li>
</ul>
<p>OR:</p>
<ul>
<li>Use an externally driven script to download the data - either from Oracle Object Store or directly from the database - to a server, then push the file up to Amazon S3. The server could be local, or hosted in either Oracle OCI or in AWS, as long as it has access to both object stores. (<a href=""https://blogs.oracle.com/linux/using-rclone-to-copy-data-in-and-out-of-oracle-cloud-object-storage"" rel=""nofollow noreferrer"">https://blogs.oracle.com/linux/using-rclone-to-copy-data-in-and-out-of-oracle-cloud-object-storage</a>)</li>
</ul>
<p>OR:</p>
<ul>
<li>You may be able to use AWS Data Sync to move data directly from Oracle Object Storage to S3, depending on networking configuration requirements. (<a href=""https://aws.amazon.com/blogs/aws/aws-datasync-adds-support-for-on-premises-object-storage/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/aws/aws-datasync-adds-support-for-on-premises-object-storage/</a>)</li>
</ul>
"
"65171714","Image loading/caching off the main thread","<p>I am writing a custom image fetcher to fetch the images needed for my collection view. Below is my image fetcher logic</p>
<pre><code>class ImageFetcher {

    /// Thread safe cache that stores `UIImage`s against corresponding URL's
    private var cache = Synchronised([URL: UIImage]())

    /// Inflight Requests holder which we can use to cancel the requests if needed
    /// Thread safe
    private var inFlightRequests = Synchronised([UUID: URLSessionDataTask]())
    
    
    func fetchImage(using url: URL, completion: @escaping (Result&lt;UIImage, Error&gt;) -&gt; Void) -&gt; UUID? {
        /// If the image is present in cache return it
        if let image = cache.value[url] {
            completion(.success(image))
        }
        
        let uuid = UUID()
        
        let dataTask = URLSession.shared.dataTask(with: url) { [weak self] data, response, error in
            guard let self = self else { return }
            defer {
                self.inFlightRequests.value.removeValue(forKey:uuid )
            }
            
            if let data = data, let image = UIImage(data: data) {
                self.cache.value[url] = image
                
                DispatchQueue.main.async {
                    completion(.success(image))
                }
                return
            }
            
            guard let error = error else {
                // no error , no data
                // trigger some special error
                return
            }
            
            
            // Task cancelled do not send error code
            guard (error as NSError).code == NSURLErrorCancelled else {
                completion(.failure(error))
                return
            }
        }
        
        dataTask.resume()
        
        self.inFlightRequests.value[uuid] = dataTask
        
        return uuid
    }
    
    func cancelLoad(_ uuid: UUID) {
        self.inFlightRequests.value[uuid]?.cancel()
        self.inFlightRequests.value.removeValue(forKey: uuid)
    }
}

</code></pre>
<p>This is a block of code that provides the thread safety needed to access the cache</p>
<pre><code>/// Use to make a struct thread safe
public class Synchronised&lt;T&gt; {
    private var _value: T
    
    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)
    
    public init(_ value: T) {
        _value = value
    }
    
    public var value: T {
        get {
            return queue.sync { _value }
        }
        set { queue.async(flags: .barrier) { self._value = newValue }}
    }
}

</code></pre>
<p>I am not seeing the desired scroll performance and I anticipate that is because my main thread is getting blocked when I try to access the cache(<code>queue.sync { _value }</code>). I am calling the <code>fetchImage</code> method from the <code>cellForRowAt</code> method of the collectionView and I can't seem to find a way to dispatch it off the main thread because I would need the request's UUID so I would be able to cancel the request if needed. Any suggestions on how to get this off the main thread or are there any suggestions to architect this in a better way?</p>
","<ios><multithreading><grand-central-dispatch><data-synchronization><barrier>","2020-12-06 18:39:58","322","2","1","65186234","<p>I do not believe that your scroll performance is related to <code>fetchImage</code>. While there are modest performance issues in <code>Synchronized</code>, it likely is not enough to explain your issues. That having been said, there are several issue here, but blocking the main queue does not appear to be one of them.</p>
<p>The more likely culprit might be retrieving assets that are larger than the image view (e.g. large asset in small image view requires resizing which can block the main thread) or some mistake in the fetching logic. When you say â€œnot seeing desired scroll performanceâ€, is it stuttering or just slow? The nature of the â€œscroll performanceâ€ problem will dictate the solution.</p>
<hr />
<p>A few unrelated observations:</p>
<ol>
<li><p><code>Synchronised</code>, used with a dictionary, is not thread-safe. Yes, the getter and setter for <code>value</code> is synchronized, but not the subsequent manipulation of that dictionary. It is also very inefficient (though, not likely sufficiently inefficient to explain the problems you are having).</p>
<p>I would suggest not synchronizing the retrieval and setting of the whole dictionary, but rather make a synchronized dictionary type:</p>
<pre><code>public class SynchronisedDictionary&lt;Key: Hashable, Value&gt; {
    private var _value: [Key: Value]

    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)

    public init(_ value: [Key: Value] = [:]) {
        _value = value
    }

    // you don't need/want this
    //
    // public var value: [Key: Value] {
    //     get { queue.sync { _value } }
    //     set { queue.async(flags: .barrier) { self._value = newValue } }
    // }

    subscript(key: Key) -&gt; Value? {
        get { queue.sync { _value[key] } }
        set { queue.async(flags: .barrier) { self._value[key] = newValue } }
    }

    var count: Int { queue.sync { _value.count } }
}
</code></pre>
<p>In my tests, in release build this was about 20 times faster. Plus it is thread-safe.</p>
<p>But, the idea is that you should not expose the underlying dictionary, but rather just expose whatever interface you need for the synchronization type to manage the dictionary. You will likely want to add additional methods to the above (e.g. <code>removeAll</code> or whatever), but the above should be sufficient for your immediate purposes. And you should be able to do things like:</p>
<pre><code>var dictionary = SynchronizedDictionary&lt;String, UIImage&gt;()

dictionary[&quot;foo&quot;] = image
imageView.image = dictionary[&quot;foo&quot;]
print(dictionary.count)
</code></pre>
<p>Alternatively, you could just dispatch all updates to the dictionary to the main queue (see point 4 below), then you don't need this synchronized dictionary type at all.</p>
</li>
<li><p>You might consider using <code>NSCache</code>, instead of your own dictionary, to hold the images. You want to make sure that you respond to memory pressure (emptying the cache) or some fixed total cost limit. Plus, <code>NSCache</code> is already thread-safe.</p>
</li>
<li><p>In <code>fetchImage</code>, you have several paths of execution where you do not call the completion handler. As a matter of convention, you will want to ensure that the completion handler is always called. E.g. what if the caller started a spinner before fetching the image, and stopping it in the completion handler? If you might not call the completion handler, then the spinner might never stop, either.</p>
</li>
<li><p>Similarly, where you do call the completion handler, you do not always dispatch it back to the main queue. I would either always dispatch back to the main queue (relieving the caller from having to do so) or just call the completion handler from the current queue, but only dispatching some of them to the main queue is an invitation for confusion.</p>
</li>
</ol>
<hr />
<p>FWIW, you can create Unit Tests target and demonstrate the difference between the original <code>Synchronised</code> and the <code>SynchronisedDictionary</code>, by testing a massively concurrent modification of the dictionary with <code>concurrentPerform</code>:</p>
<pre><code>// this is not thread-safe if T is mutable

public class Synchronised&lt;T&gt; {
    private var _value: T

    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)

    public init(_ value: T) {
        _value = value
    }

    public var value: T {
        get { queue.sync { _value } }
        set { queue.async(flags: .barrier) { self._value = newValue }}
    }
}

// this is thread-safe dictionary ... assuming `Value` is not mutable reference type

public class SynchronisedDictionary&lt;Key: Hashable, Value&gt; {
    private var _value: [Key: Value]

    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)

    public init(_ value: [Key: Value] = [:]) {
        _value = value
    }

    subscript(key: Key) -&gt; Value? {
        get { queue.sync { _value[key] } }
        set { queue.async(flags: .barrier) { self._value[key] = newValue } }
    }

    var count: Int { queue.sync { _value.count } }
}

class SynchronisedTests: XCTestCase {
    let iterations = 10_000

    func testSynchronised() throws {
        let dictionary = Synchronised([String: Int]())

        DispatchQueue.concurrentPerform(iterations: iterations) { i in
            let key = &quot;\(i)&quot;
            dictionary.value[key] = i
        }

        XCTAssertEqual(iterations, dictionary.value.count)  //  XCTAssertEqual failed: (&quot;10000&quot;) is not equal to (&quot;834&quot;)
    }

    func testSynchronisedDictionary() throws {
        let dictionary = SynchronisedDictionary&lt;String, Int&gt;()

        DispatchQueue.concurrentPerform(iterations: iterations) { i in
            let key = &quot;\(i)&quot;
            dictionary[key] = i
        }

        XCTAssertEqual(iterations, dictionary.count)        // success
    }
}
</code></pre>
"
"65033677","Define Data Quality Rules for Big Data","<p>Is there any way to define Data quality rules that can be applied over Dataframes.
The template to define the rule should be easy enough for any lay man to define and then we can take these rules and convert them to pyspark codes and run them over the data.</p>
<p>I was thinking in line as below.</p>
<pre><code>ID  ProjectID   RuleID  Attribute1  Value1          Condition1  Attribute2  Value2          Condition2  Type    ModifyAttribute ModificationLogic   CustomUDF
1   1           1       SerialNum   6               EQUAL                                               MODIFY  SerialNum   SUBSTR(serialNum,1,6)   
2   1           2       DriverName  ['A','B','C']   VALUEMATCH  Source      ['D','E','F']   IN          REJECT  
</code></pre>
<p>If there is any tools or Domain specific language to define the same it would help.
If there is any template to define rules which can be applied cross attribute and across multiple tables (join, example country lookup) is also helpful.</p>
","<validation><pyspark><data-quality>","2020-11-27 07:47:25","209","1","1","65662430","<p>Surprised no one gave a shot at answering this yet. Typically, for a use case like this, I would use <a href=""https://docs.python.org/3/library/configparser.html"" rel=""nofollow noreferrer"">ConfigParser</a>. Based on what your architecture is, you can define sections and rules which can easily be read and executed. But that's something a developer would find easy to use rather than a normal user.</p>
<p>Now that's out of the way, for your use case, as python is a scripting language with a lot of flexibility, you can simply create an excel in the format you have given which will dictate the flow of your data manipulation. I hope this helps in some way. Let me know if you need more info.</p>
"
"64559045","ready/valid vs 2-way handshaking vs 4-way handshaking","<p>I am confused about whether ready/valid handshaking is functionally equivalent to req/ack (2-way) handshaking? By being functionally equivalent, I mean that we can perform data transfers with ready/valid handshaking in all the cases in which we can do with req/ack (2-way) handshaking and vice versa? Are there any scenarios in which one scheme will work while the other will not?</p>
<p>As an extension to the same question, is req/ack (2-way) functionally equivalent to req/ack (4-way) handshaking? Mostly, I have found the difference to be in terms of hardware required and of course speed. Are there any scenarios in which we are bound to use req/ack (4-way) or req/ack (2-way) for that matter.</p>
<p>In summary, I want to build a connection between the three schemes -- where will one scheme fail and the other scheme will work.</p>
<p><strong>The question is in the context of both synchronous and asynchronous designs</strong></p>
<p><a href=""https://www.slideserve.com/kyna/self-timed-and-asynchronous-design"" rel=""nofollow noreferrer"">A reference to handshaking.</a></p>
","<cpu-architecture><handshake><data-synchronization>","2020-10-27 16:41:59","1447","1","1","67226736","<p>An old question but I'll answer anyway for the sake of future similar queries.</p>
<p><strong>1. When are they used?</strong></p>
<p>The 4-phased or 2-phased <code>req/ack</code> protocols are necessary in the absence of clock, in asynchronous logic. The signals are responses to one another: both are responses in the 4-phased protocol, <code>ack</code> is the only response in the 2-phased protocol.</p>
<p>The <code>valid/ready</code> (or equivalent) protocol doesn't need the return to zero phases thanks to the synchronization on the rising edge of the clock, it can only be used in synchronous logic. Furthermore, only one phase is required since the <code>valid</code> and <code>ready</code> are not a response to each other on the current transfer, they are simply updated on the next cycle.</p>
<p><strong>2. How do they compare?</strong></p>
<p>All 3 protocols controls the transfer of data accurately, they can transmit back pressure too. So <strong>yes, they are functionally equivalent</strong>.</p>
<p><em>4-phased <code>req/ack</code> signals switch twice as much, which is not great from a <strong>performance</strong> and <strong>power consumption</strong> point of view. But synchronous circuits have block buffers that may consume a lot when the fan-out is big, which have other consequences like EMI, the need of local capacitors and/or the use of spread spectrum techniques. It all depends on the scale of the circuit and the technology. In general though, it will be harder to maintain the same throughput with a protocol that needs to switch signals at twice the rate.</em></p>
<p>2-phased <code>ack/req</code> provides the same functionality too but the implementation has its disadvantages, the logic to handle high/low may be more complex vs the natural 4-phased protocol. It requires XOR gates and a reference register to output transitions instead of states (changing polarity instead of indicating &quot;I'm ready&quot;). It requires XOR gates to detect if a change of state should occur.</p>
<p><em>From a <strong>performance</strong> point of view, it takes more resources and increases the critical path but reduces the number of phases. It's not clear whether the outcome will improve the performances, to check but it likely depends on the technology. One justification would be the transmission of the protocol over long or loaded lines of limited bandwidth, where it could be interesting to increase the rate over those lines at the expense of local gate area.</em></p>
<p><strong>3. Are they interchangeable?</strong></p>
<p>It is possible to interface two <code>valid/ready</code> blocks with a <code>req/ack</code> protocol in a synchronous circuit. However, in order to interface two <code>req/ack</code> blocks with a <code>valid/ready</code> protocol in an asynchronous circuit, you'd need a third synchronization signal for the notion of transfer cycle.</p>
<p>The question is moot in most cases because there is a penalty and no advantage. <code>valid/ready</code> or equivalent will be used in synchronous circuits. In asynchronous circuits, you have to choose between 2-phased or 4-phased <code>req/ack</code>.</p>
<p>It is possible to interface 2-phased and 4-phased <code>req/ack</code> blocks with XOR and S/R latches in asynchronous circuits.</p>
<p><strong>In summary</strong></p>
<p>They are functionally equivalent when they are used in their respective synchronous / asynchronous implementations, but cannot always be used in the other domain (<code>req/ack</code> can be used in synchronous domain but underperforms, <code>valid/ready</code> cannot be used as such in asynchronous domain). Each implementation has a different impact on performance, power consumption and resource utilization.</p>
"
"64552905","Is there a way of using two authentication schemes for the same controller?","<p>I would like to use my Open Data (OData) Controllers to access data from a third party grid tool in MVC (using cookie authentication), and I would also like to use the same controllers to synchronize data with a mobile Xamarin app (using token authentication).</p>
<p>Here is an extract from my startup file...</p>
<pre><code>            services.AddAuthentication(options =&gt;
        {
            //options.DefaultScheme = CookieAuthenticationDefaults.AuthenticationScheme;
            //options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;
            //options.DefaultChallengeScheme = CookieAuthenticationDefaults.AuthenticationScheme;
        })
            .AddCookie(options =&gt; options.SlidingExpiration = true)
            .AddJwtBearer(options =&gt;
            {
                options.TokenValidationParameters = new TokenValidationParameters
                {
                    ValidateIssuer = false,
                    ValidateAudience = false,
                    ValidateLifetime = false,
                    RequireExpirationTime = false,
                    ValidIssuer = authOptions.Issuer,
                    ValidAudience = authOptions.Audience,
                    IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(authOptions.SecureKey))
                };
            });
</code></pre>
<p>and the relevant bits of the controllers look like the following...</p>
<pre><code> [Authorize(AuthenticationSchemes = CookieAuthenticationDefaults.AuthenticationScheme), Authorize(AuthenticationSchemes = JwtBearerDefaults.AuthenticationScheme)]
 public class FooODataController : ODataController {...}
</code></pre>
<p>The problem is that both cookies and bearer tokens are always challenged, so that the user is never authenticated.  Does anyone know a way of implementing this so that the user is authenticated if either of the challenges are successful?</p>
","<asp.net-core><authentication><odata><syncfusion><data-synchronization>","2020-10-27 10:54:37","282","0","1","64726167","<p>It's because there is two [Authorize(..), Authorize(..)] attributes, instead you should have one with multiple schemas: (see <a href=""https://learn.microsoft.com/en-us/aspnet/core/security/authorization/limitingidentitybyscheme?view=aspnetcore-3.1"" rel=""nofollow noreferrer"">this</a>)</p>
<pre class=""lang-cs prettyprint-override""><code>[Authorize(AuthenticationSchemes = CookieAuthenticationDefaults.AuthenticationScheme + &quot;,&quot; + JwtBearerDefaults.AuthenticationScheme)]
public class FooODataController : ODataController 

</code></pre>
"
"64384850","unittest - making setUp() the single source of truth for test data and its effect on parallelized testing","<p>As far as I know, using <code>unittest</code> in Python is done something like this -</p>
<pre class=""lang-py prettyprint-override""><code># test.py
import unittest
import math
from my_math import do_math

class test_do(unittest.TestCase):

    def test_first_case(self):
        self.assertEqual(math.sqrt(4), do_math.sqroot(4))
        self.assertEqual(math.sqrt(9), do_math.sqroot(9))

if __name__ == &quot;__main__&quot;:
    unittest.main()

</code></pre>
<p>My concern is specifically with the computation we need to perform to verify the test results. If we use the <code>setUp()</code> method, we could restructure the code as follows</p>
<pre class=""lang-py prettyprint-override""><code># test.py
import unittest
import math
from my_math import do_math

class test_do(unittest.TestCase):

    def setUp(self):
        self.test_1 = 4
        self.test_2 = 9
        self.test_sqroot_1 = math.sqrt(self.test_1)
        self.test_sqroot_2 = math.sqrt(self.test_2)

    def test_first_case(self):
        self.assertEqual(self.test_sqroot_1, do_math.sqroot(self.test_1))
        self.assertEqual(self.test_sqroot_2, do_math.sqroot(self.test_2))

if __name__ == &quot;__main__&quot;:
    unittest.main()

</code></pre>
<p>If I need to refactor the testcases in terms of true or target values, I will need to change just the setUp() function body, whereas I would need to make changes everywhere in the first approach (considering multiple testcases accessing the same data).</p>
<p>My question is this - in terms of parallelized testing, given that I am doing most of the computation needed for verification in the <code>setUp()</code> function and not in the testcase functions themselves, is the second approach adviseable?</p>
","<python><unit-testing><tdd><python-unittest><parallel-testing>","2020-10-16 07:37:28","79","0","1","65273701","<p>Why do you need to calculate the square root of 4 and 9 multiple times? Why not precalculate it, especially in such trivial cases?</p>
<pre><code>self.test_1 = 4
self.test_2 = 9
self.test_sqroot_1 = 2
self.test_sqroot_2 = 3
</code></pre>
<p><strong>Edit</strong> (based on comment):</p>
<p>Even if the calculation is more involved, I would never do it like this. The setup of a test should be as simple and quick as possible. If the calculation would yield the same result for each test, why do it at all. Calculate once and set as constant value.</p>
<p>On the other hand, if the calculation has side-effects that are needed for the tests, e.g. it is actually setting up a database or similar, then call it a setup, not a calculation. In this case it does, in fact, belong in the setup method. Such complicated and time consuming setups should still be generally avoided, except for integration or end-to-end tests which are expected to be slow and should be kept to a minimum.</p>
"
"64259415","How to identify unsynced entities? Is the client the one who generates the Ids?","<p>Let's say I want to synchronize (using HTTP protocol) an entity called <code>Person</code>. So, the persons in client (mobile/desktop/whatever) is a mirror-replicate of persons exist in server's database. Obviously, server owns all persons and client owns only the specific user's persons.</p>
<p>Consider the following case.</p>
<p>Client is offline. While he is offline, he creates a <code>Person</code> and because he can't connect to the  server, he keeps this person to a local storage. Let's say a local database (SQLite or whatever). <strong>The moment this happens, how is/should this person identified?</strong></p>
<p>Before I started implementing the whole thing, I thought the server should be the one that generates the IDs of persons coming to him. However, when I started implementing, I start facing this problem.</p>
<p>In case the server generates the IDs, since the person is never seen by the server, client must give it an ID in order to be able to find the person and obviously use this ID to store the person into his local storage. Now, when client comes online, he will send the person to server. Server, gives it an ID and stores it in his own database. After that, client will request for any kind of person changes that happened after his last time of synced and server will return this specific person.</p>
<p>Lets make an example. Client is offline, creates 100 Persons and stores them to his local storage. Person 1, Person 2, Person 3, etc... Now, he gets connected to the server and he sends all 100 persons. Since the connection happens over HTTP, client makes a post request to <code>post-persons</code> endpoint. Then, server generates IDs (either incremental or UUIDs) and probably change some other properties as well. Now, client access <code>get-persons</code> endpoint and he sees 100 updated persons, each one of them having a new ID that he could not know about. <strong>How does the client know which of these persons correspond to persons that client already has?</strong> Removing the old client's 100 persons, and inserting 100 new with server ID seems unorthodox. With other words, Person 1 known by the client, is stored as Person [uuid] in server, and server returns it as Person [uuid]. How client knows that his Person 1 corresponds to Person [uuid]? A solution might be, to send client's IDs to server, and server will respond like Person [uuid] 1. Now client knows, his 1 is this one. And to me, this seems even more unorthodox.</p>
<p>Second option is to have the clients generate UUIDs either they are offline, either online. This solution seems the &quot;simplest&quot; approach when it comes to implementation by my side. Client creates Person [uuid]. When he comes online, he sends it to server. After that client accesses <code>get-persons</code> and he gets as respond an update Person [uuid]. He easily identifies &amp; stores it in his local storage. The server does not generate any kind of ID for persons.</p>
<p>Is there anything I am missing? Till now, I thought servers are the ones that generate the IDs of syncable entities, but I think the second approach is easier to implement and more comprehensive. But does it introduce any kind of &quot;danger&quot; for later?</p>
<p>There are no explicit requirements when it comes to what kind of ID I will use to client, or the server. However, I am aware of the trade-offs using UUIDs over simple increment numbers.</p>
<p>The stack (even though I consider it irrelevant):</p>
<ul>
<li>Spring boot as server among Hibernate and MySQL</li>
<li>Client with Hibernate and H2 standalone as local storage</li>
<li>Everything Java 8</li>
</ul>
","<java><database><synchronization><primary-key><data-synchronization>","2020-10-08 09:17:00","137","0","1","64260362","<p>There are other possible solutions, and the best one depends on your particular situation - whether you need to maintain local relations, etc.</p>
<hr />
<p><strong>The first solution keeps using IDs with no logic around it on the server.</strong></p>
<p>Don't generate ID at all and store persons without it (if possible), and when synced, the server returns IDs, and you change them locally.</p>
<p>Thus, it's simple to know which persons are already in sync with the server (those with ID) and which are not yet known to the server.</p>
<p>It's also simple to implement INSERT/UPDATE logic.</p>
<p>However, this solution may be a problem if you need to maintain local relations.</p>
<p>When you need local relations, you can generate temporary IDs. Let's say that you want a numeric ID, and so all positive IDs are those that are already synced, and negative IDs are those that are temporary.</p>
<p>When synced with the server, you obtain new IDs from the server (positive ones), and you simply rewrite all IDs in your local database. That's, however, a bit error-prone as you have to be sure to update all relations.</p>
<p>You can also have localId and serverId. Technically, you only need serverId to be globally unique to store persons on your server. You keep serverId empty and fill it once the entity is synchronized with the server (the server returns it). And you can generate localId as you want and use it for local relations. However, this can be a problematic approach if you need to have several clients in sync, and each of them would like to generate its own local IDs.</p>
<hr />
<p><strong>The second approach is to use ID pools. It needs a bit of work on the server and storing assigned pools.</strong></p>
<p>You can allocate a pool of IDs for the given client. The client must be online at least once, and the server sends its unique offset.</p>
<p>Depending on your needs, you can have, for example, an 8-byte identifier (64 bits). The first 36 bits can be used for the offset, so your server can manage 2^36 clients. And the last 28 bits are free to use by the client.</p>
<p>So the client just increments the internal counter whenever it needs a new ID and adds the unique offset =&gt; gets a globally unique identifier.</p>
"
"64218004","Azure Data Sync Error - Member Database is Read Only","<p>I have configured Azure Data Sync between our SQL on-prem and Azure SQL. Sync direction is to Hub (Azure) and conflict resolution is set to Member Win. Sync group has been configured successfully, and I have selected the tables I want to sync in the Hub (I synced schema previously using Data Migration). I have 83 tables that are selected for synchronization.</p>
<p><strong>Problem:</strong> When starting the synchronization, I receive the following error:</p>
<blockquote>
<p>Database provisioning failed with the exception &quot;SqlException Error Code: -2146232060 - SqlError Number:3906, Message: Failed to update database &quot;*****&quot; because the database is read-only. SqlError Number:2759, Message: CREATE SCHEMA failed due to previous errors.</p>
</blockquote>
<p>Why would Data Sync be attempting to <code>CREATE SCHEMA</code> on my read-only on-prem instance, when I have configured it to synchronize in the other direction?</p>
<p>Appreciate any insight you can offer,</p>
<p>Cheers,</p>
","<sql><azure><data-synchronization><azure-data-sync>","2020-10-06 01:04:32","307","2","1","67356253","<p>You need to give dbowner access to role you are using with agent so that it can create a schema initially. It will create schema and tables with datasync.tablename that will have sync logs for respective table. If you sync 10 tables then 10 such tables will be created.</p>
"
"64002859","jQuery AJAX call returning images in wrong order - data synchronization","<p>I use a JQuery AJAX call to return image, name, address, country and score of a tourist attraction in a given city (by latitude). I save the returned data into 5 arrays: for images, names, addresses, country, and score.
Function &quot;tom1&quot; gets all data except images, it triggers function &quot;tom2&quot; that get image name.
Function &quot;tom2&quot; triggers function &quot;callme&quot;.</p>
<p>When function call me appends all 5 values to the page, images often don't match the other info.</p>
<p>Is it related to the AJAX's asynchronous request?</p>
<p>Can you help me solve it? Here is my code:</p>
<pre class=""lang-js prettyprint-override""><code>function tom1(lat, lon) {
  var queryURL = &quot;https://api.tomtom.com/search/2/search/museum.json?key=&quot; + tomAPI + &quot;&amp;lat=&quot; + lat + &quot;&amp;lon=&quot; + lon;

  $.ajax({
    url: queryURL,
    method: &quot;GET&quot;
  }).then(function(response) {

    //only select POIs which possess &quot;dataSources&quot; key = images
    for (var i = 0; i &lt; 20; i++) {
      if (response.results[i].dataSources !== undefined) {
        poiId.push(response.results[i].dataSources.poiDetails[0].id);
        placeName.push(response.results[i].poi.name);
        address.push(response.results[i].address.freeformAddress);
        country.push(response.results[i].address.country);
        rank.push(response.results[i].score.toFixed(1));
      } else {
        console.log(&quot;not lucky today&quot;)
      }
    }
    tom2(poiId);
  });
}

// get POI details and images after you know POI's ID
function tom2(poiId) {

  for (i = 0; i &lt; poiId.length; i++) {
    var queryURL = &quot;https://api.tomtom.com/search/2/poiDetails.json?key=&quot; + tomAPI + &quot;&amp;id=&quot; + poiId[i];

    $.ajax({
      url: queryURL,
      method: &quot;GET&quot;
    }).then(function(response) {
      imgId.push(response.result.photos[0].id);
      callme(imgId);
    });
  }

}
</code></pre>

","<javascript><jquery><ajax><api><asynchronous>","2020-09-22 03:52:12","84","0","1","64003259","<p>Yes, tom2 makes a number of asynchronous calls, and the callback function of those can typically be executed in a different order. So the order of the elements in imgId will be mixed up.</p>
"
"63872347","Is there a pattern for data synchronization in two independent services?","<p>I need the data in the two services to be identical. Changes can be made to each of the services at the same time. I made integration with each service using the REST API. But here's what and how to track and what are the options for resolving data conflicts. For example, Google calendar events and some task Manager, such as Todoist (I know that there is a built-in integration :) )</p>
","<java><synchronization>","2020-09-13 15:14:59","192","-1","1","63872799","<p>When <code>Service1</code> has an update, then <code>Service1</code> publishes a message to a topic.  <code>Service2</code> is subscribed to the topic, receives the message, and <code>Service2</code> updates its data. <code>Service2</code> doesn't know that the message originated from <code>Service1</code>.</p>
<p>Similarly, when <code>Service2</code> has an update, then <code>Service2</code> publishes a message to the same topic.  <code>Service1</code> is subscribed to the topic, receives the message, and <code>Service1</code> updates its data. <code>Service1</code> doesn't know that the message originated from <code>Service2</code>.</p>
<p>This is referred to as a publish-subscribe pattern, and is near real-time, where synchronization depends on the backlog of messages to be processed.  It depends on the app, but a few milliseconds of being out of sync is usually acceptable for a human user.</p>
"
"63856422","Json data synchronization in database","<p>I keep json data from database. I want to sync with the name that comes with the post and get the price of the data I sync. How can I do that?</p>
<pre class=""lang-php prettyprint-override""><code>$video_tkn      = &quot;3D&quot;;
$reklam_json    = json_decode($siteayar-&gt;reklam_json);
$video          = json_decode($reklam_json-&gt;video);

$video_arr      = array();

foreach ($video as $v) {
    if ($v-&gt;video_tur==$video_tkn) {
        $video_arr[] = $v-&gt;fiyat;
    }else{
        $video_arr[] = 0;
    }
}

print_r($video_arr[0]);

//Output : 0

</code></pre>
<pre class=""lang-php prettyprint-override""><code>//$reklam_json-&gt;video
</code></pre>
<pre class=""lang-json prettyprint-override""><code>
[
   {
      &quot;video_tur&quot;:&quot;2D&quot;,
      &quot;fiyat&quot;:&quot;20&quot;
   },
   {
      &quot;video_tur&quot;:&quot;3D&quot;,
      &quot;fiyat&quot;:&quot;80&quot;
   }
]
</code></pre>
","<php><json>","2020-09-12 03:02:07","93","0","1","63856893","<p>You can do something like. The problem with the code that you have written is that in the $video_arr[0] it is always writing 0 as per your else condition because $video_tkn= &quot;3D&quot; value is in second index in the array $video</p>
<pre><code>&lt;?php 
$video_tkn      = &quot;3D&quot;;
$reklam_json    = json_decode($siteayar-&gt;reklam_json);
$video          = json_decode($reklam_json-&gt;video);

$video_arr      = array();

foreach ($video as $v) {
    if ($v-&gt;video_tur==$video_tkn) {
        $video_arr[] = $v-&gt;fiyat;
    }
}

print_r($video_arr[0]);
</code></pre>
"
"63854572","Handling data quality issues on medium csv report. Best Practices","<h3>Need help with a <strong>better pratices</strong> question</h3>
<p>I have an <strong>azure function</strong> that brings data form differents APIs and match them toguether to create a final csv report. I have a poblation of 60k-100k and 30 columns</p>
<p>For the sake of the explanation, I'm going to use a small School example.</p>
<pre class=""lang-cs prettyprint-override""><code>public Student {
    string Grade {get; set;}
    Name   LegName {get; set;}
    string FatherName {get; set;}
    string TeacherId {get; set;}
    string SchoolId {get; set;}
}

public Name {
    string FirstName {get; set;}
    string LastName {get; set;}
}
</code></pre>
<p>Before constructing the report, I create two <code>Dictionary</code> with &lt;Id, Name&gt; from two APIs that expose Schools and Teachers information. And of course, a list of <code>Student</code> that comes from the Student APIs. I have no control of this trhee APIs, design, data quality, nothing.</p>
<p>Now, when I have all the data, I start to create the report.</p>
<pre class=""lang-cs prettyprint-override""><code>string GenerateTXT(Dictionary&lt;string, string&gt; schools, Dictionary&lt;string,  string&gt; teachers, Student students){
    StringBuilder content = new StringBuilder();

    foreach(var student in students){
        content.Append($&quot;{student.Grade}\t&quot;);
        content.Append($&quot;{student.LegName.FirstName}\t&quot;);
        content.Append($&quot;{student.LegName.LastName}\t&quot;);
        content.Append($&quot;{schools.TryGetValue(student.TeacherId)}\t&quot;);
        content.Append($&quot;{teachers.TryGetValue(student.SchoolId)}t&quot;;        
        content.Append($&quot;{student.FatherNme}\t&quot;);
        content.AppendLine();
    }

    return content.ToString();    
}
</code></pre>
<p>Now here comes the problem. I started noticing data quality issues so the function started throwing exceptions. For example, students who do not have a valid school or teacher, or a student who does not have a name. I tried to solve expected scenarios and exception handling.</p>
<pre class=""lang-cs prettyprint-override""><code>string GenerateTXT(Dictionary&lt;string, string&gt; schools, Dictionary&lt;string,  string&gt; teachers, Student students){
    StringBuilder content = new StringBuilder();
    var value = string.Empty;
    foreach(var student in students){
        try {
            content.Append($&quot;{student.Grade}\t&quot;);
            content.Append($&quot;{student.LegName.FirstName}\t&quot;);
            content.Append($&quot;{student.LegName.LastName}\t&quot;);            
            if(teachers.TryGetValue(student.TeacherId))
                content.Append($&quot;{teachers[student.TeacherId]}\t&quot;);
            else
                content.Append($&quot;\t&quot;);
            if(schools.TryGetValue(student.SchoolId))
                content.Append($&quot;{schools[student.SchoolId]}\t&quot;);
            else
                content.Append($&quot;\t&quot;);            
            content.Append($&quot;{student.FatherNme}\t&quot;);
            content.AppendLine();
        }
        catch(Exception ex) {
            log.Error($&quot;Error reading worker {student.FirstName}&quot;);
        }
        
    }
    return content.ToString();
}
</code></pre>
<p>The problem with this is that when an unexpected error happens, I <strong>stop</strong> reading the next columns of data that maybe I have and instead jump to the next worker. Therefore, if a student for some random reason does not have a name, that row in the report will only have the grade, and nothing else, but I actually had the rest of the values. So here comes the question. I could put a <code>try catch</code> on each column, but remember that my real scenario has like 30 columns and could be more... so I think it's a really bad solution. Is there a pattern to solve this in a better way?</p>
<h2>Thanks in advance!</h2>
","<c#><design-patterns><azure-functions>","2020-09-11 21:33:42","61","1","1","63856670","<p>So the first bit of advice I am going to give you is to use <a href=""https://www.nuget.org/packages/CsvHelper/"" rel=""nofollow noreferrer"">CsvHelper</a>. This is a tried and true library as it handles all those edge cases you will never think of. So, saying that, give this a shot:</p>
<pre><code>public class Student
{
    public string Grade { get; set; }
    public Name LegName { get; set; }
    public string FatherName { get; set; }
    public string TeacherId { get; set; }
    public string SchoolId { get; set; }
}

public class Name
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
}

public class NormalizedData
{
    public string Grade { get; set; }
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public string School { get; set; }
    public string Teacher { get; set; }
    public string FatherName { get; set; }
}

static void GenerateCSVData(CsvHelper.CsvWriter csv, Dictionary&lt;string, string&gt; schools,
    Dictionary&lt;string, string&gt; teachers, Student[] students)
{
    var normalizedData = students.Select(x =&gt; new NormalizedData
    {
        Grade = x.Grade,
        FatherName = x.FatherName,
        FirstName = x.LegName?.FirstName, // sanity check incase LegName is null
        LastName = x.LegName?.LastName, // ...
        School = schools.ContainsKey(x.SchoolId ?? string.Empty) ? schools[x.SchoolId] : null,
        Teacher = teachers.ContainsKey(x.TeacherId ?? string.Empty) ? teachers[x.TeacherId] : null
    });
    csv.WriteRecords(normalizedData);
}

private static string GenerateStringCSVData(Dictionary&lt;string, string&gt; schools,
    Dictionary&lt;string, string&gt; teachers, Student[] students)
{
    using(var ms = new MemoryStream())
    {
        using(var sr = new StreamWriter(ms, leaveOpen: true))
        using (var csv = new CsvHelper.CsvWriter(sr,
            new CsvConfiguration(CultureInfo.InvariantCulture)
        {
            Delimiter = &quot;,&quot;, // change this to &quot;\t&quot; if you want to use tabs
            Encoding = Encoding.UTF8
        }))
        {
            GenerateCSVData(csv, schools, teachers, students);
        }
        ms.Position = 0;
        return Encoding.UTF8.GetString(ms.GetBuffer(), 0, (int)ms.Length);
    }
}

private static int Main(string[] args)
{
    var teachers = new Dictionary&lt;string, string&gt;
    {
        { &quot;j123&quot;, &quot;Jimmy Carter&quot; },
        { &quot;r334&quot;, &quot;Ronald Reagan&quot; },
        { &quot;g477&quot;, &quot;George Bush&quot; }
    };
    var schools = new Dictionary&lt;string, string&gt;
    {
        { &quot;s123&quot;, &quot;Jimmy Carter University&quot; },
        { &quot;s334&quot;, &quot;Ronald Reagan University&quot; },
        { &quot;s477&quot;, &quot;George Bush University&quot; }
    };

    var students = new Student[]
    {
        new Student
        {
            FatherName = &quot;Bob Jimmy&quot;,
            SchoolId = &quot;s477&quot;,
            Grade = &quot;5&quot;,
            LegName = new Name{ FirstName = &quot;Apple&quot;, LastName = &quot;Jimmy&quot; },
            TeacherId = &quot;r334&quot;
        },
        new Student
        {
            FatherName = &quot;Jim Bobby&quot;,
            SchoolId = null, // intentional
            Grade = &quot;&quot;, // intentional
            LegName = null, // intentional
            TeacherId = &quot;invalid id&quot; // intentional
        },
        new Student
        {
            FatherName = &quot;Mike Michael&quot;,
            SchoolId = &quot;s123&quot;,
            Grade = &quot;12&quot;,
            LegName = new Name{ FirstName = &quot;Peach&quot;, LastName = &quot;Michael&quot; },
            TeacherId = &quot;g477&quot;
        },
    };

    var stringData = GenerateStringCSVData(schools, teachers, students);

    return 0;
}
</code></pre>
<p>This outputs:</p>
<pre><code>Grade,FirstName,LastName,School,Teacher,FatherName
5,Apple,Jimmy,George Bush University,Ronald Reagan,Bob Jimmy
,,,,,Jim Bobby
12,Peach,Michael,Jimmy Carter University,George Bush,Mike Michael
</code></pre>
<p>So, you can see, one of the students has invalid data in it, but it recovers just fine by placing blank data instead of crashing or throwing exceptions.</p>
<p>Now I haven't seen your original data, so there may be more tweaks you have to make to this to cover all edge cases, but it will be a lot easier to tweak this when using CsvHelper as your writer.</p>
"
"63578813","In CRM Design, should it be mandatory to update a phone number whenever an address change is requested?","<p>should it be mandatory to update a phone number whenever an address change is requested.</p>
<p>Thank you</p>
","<dynamics-crm><crm><application-design><data-quality>","2020-08-25 12:25:11","52","-2","1","63579000","<p>Depends. On a lot of factors.</p>
<p>For eg, in India, if the phone type is a landline and the address changed to a different city, the area code of the phone number will be different. Hence, allowing the user to update the phone number also while updating the address makes a lot of sense.</p>
<p>With mobile phones, there is number portability (in most jurisdictions). However, a user could choose to not port their mobile number (rare, but can happen for a number of reasons such as provider coverage in the new area being poor or unavailable). In this case also, it makes sense to update the phone number while updating the address.</p>
<p>So, there is a business case for simplifying the user experience with one single update screen instead of having to update address and phone separately upon change of address, IMHO.</p>
"
"63514310","Syncs of RDBMS to Redis","<p>I have RDS that serves as the source of truth. A challenge that I have is to have this database partially synced to Redis to make it available for a server app to use. This would be a one way sync always going in one direction, but I can't wrap my head around what tools do I use to make these syncs happen preferably in an optimized way. In other words, rather than loading the entire data set it would be great if deltas are synced only.</p>
<p>I hope someone can give some insight on how this can be done.
Thank you!</p>
","<caching><redis><data-synchronization>","2020-08-20 23:09:55","1612","3","1","63519400","<p>Most RDBMS provide a way to subscribe to the transaction allowing you to put in place a &quot;change data capture&quot; event streams.</p>
<p>In this case you can subscribe to the databases events, and put the change or updated record inside Redis.</p>
<p>You can for example use <a href=""https://debezium.io/"" rel=""noreferrer"">Debezium</a> to capture the event, as you can see Debezium community has connectors for various datasources.</p>
<pre><code>---------          ------------         --------- 
| RDBMS |========&gt;&gt;| Debezium | ======&gt; | Redis |
---------          ------------         ---------

</code></pre>
<p>This <a href=""https://github.com/tgrall/redis-microservices-demo"" rel=""noreferrer"">demonstration</a> (mostly Java) shows this (a little richer since it is using Redis Streams and an intermediate state), the event is capture by this <a href=""https://github.com/tgrall/redis-microservices-demo/blob/18ae54e17adad1e4edbfaa1cdd08df3f69c64301/db-to-streams-service/src/main/java/io/redis/demos/services/db/events/streams/listener/CDCEventListener.java#L87"" rel=""noreferrer"">method</a> so capture Insert/Update/Delete in MySQL and sending the information to Redis.</p>
<p>Another option, that does not match you need but interesting, is to do a &quot;write behind&quot; Cache. In this case you update the Cache in Redis and Redis push the update in RDBMS using <a href=""http://redisgears.io/"" rel=""noreferrer"">Gears</a>.</p>
<p>You can find more information about this &quot;write behind&quot; with Redis Gears in this <a href=""https://github.com/RedisGears/rgsync"" rel=""noreferrer"">GitHub repo</a>.</p>
<pre><code>---------          ---------         --------- 
| Redis |========&gt;&gt;| Gears | ======&gt; | RDBMS |
---------          ---------         ---------

</code></pre>
"
"63478053","Histogram in Anomaly detection Deequ library","<p>Can we use histogram analyzer in anomaly detection?</p>
<p>Let's say, I want to check for the change in the ratio of variables in a specified column. For example
histogram analysis for a column with Male and Female as values is something like (Male - 0.6) and (Female - 0.4).
Now If the value changes from these to some other values it should be an anomaly in the data.
I have tried it but couldn't figure it out.</p>
<p>Can we achieve something like this currently?</p>
","<scala><data-quality><amazon-deequ>","2020-08-18 23:34:11","466","1","1","63774712","<p><a href=""https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/anomalydetection/AnomalyDetectionStrategy.scala#L30"" rel=""nofollow noreferrer"">AnomalyDetectionStrategy</a> needs a series of Doubles.</p>
<p>As long as you can cast your computation as a 1D anomaly detection problem you can use deequ for this. For example, you could use a Compliance analyzer (instead of histogram) to compute the frequency of Males/Females and feed this into the anomaly detector.</p>
"
"63344371","How to run Informatica data quality workflow from SQL Server","<p>I want to run Informatica Data Quality job from SQL Server job. Both SQL Server and Informatica are installed on different Windows VMs.</p>
","<azure><informatica>","2020-08-10 16:40:22","327","0","1","63392327","<p>Create a webservice and use it for invoking jobs from other sources. Here's a
<a href=""https://kb.informatica.com/howto/6/Pages/1/151274.aspx"" rel=""nofollow noreferrer"">HOW TO: Create a Web Service in Informatica Data Quality</a></p>
<p>For calling a webservice from SQL Server please refer <a href=""https://stackoverflow.com/questions/33449/can-you-call-a-webservice-from-tsql-code"">this StacOverflow thread</a>. Do read <a href=""https://stackoverflow.com/a/33884/2834065"">this answer</a> talking about why it's a bad idea.</p>
"
"63210891","best solution to sync 2 mysql databases in a laravel project","<p>I have a laravel project that is supposed to run in a localhost.
but we needed to add the ability to do some modification while user of the app is away from his pc that the app runs on it's host.
I know i can deploy the whole project in an online server but this solution is not an option till now.
we have only a weak online server (it's slower a lot than localhost);
so we can use this weak online server for these situations when the user wants to do some modifications remotely which would happen from time to time almost two or three times a day while the localhost will have the heavy work of the rest of the day which may be more than 3 or 4 hundreds processes a day.
i can't make the whole load on the online server while it's very slow like that and we don't need online benefits a lot, just for those two or three times remote modifications that the app user may or may not need, so i can't trade off localhost speed for online benefits which i need only two or three times a day.
what solution can i do.
i knew about master-slave and master-master replication but it's not an option too.</p>
<p>is there any ideas and thank you in advance.</p>
<p>-------------- about the two environments (local and online)------------
local is windows running xamp stack (apache, mysql, php)
server is linux (don't know actually which distro but any way i can't install any tools there ... just php packages with composer)</p>
","<mysql><database><laravel><data-synchronization>","2020-08-01 23:08:57","1870","0","2","63211049","<p>I had the same problem for uploading my laravel project</p>
<p>just use FileZilla to upload your project, even with the worst internet speed you can do it.
and save yourself the trouble.</p>
"
"63210891","best solution to sync 2 mysql databases in a laravel project","<p>I have a laravel project that is supposed to run in a localhost.
but we needed to add the ability to do some modification while user of the app is away from his pc that the app runs on it's host.
I know i can deploy the whole project in an online server but this solution is not an option till now.
we have only a weak online server (it's slower a lot than localhost);
so we can use this weak online server for these situations when the user wants to do some modifications remotely which would happen from time to time almost two or three times a day while the localhost will have the heavy work of the rest of the day which may be more than 3 or 4 hundreds processes a day.
i can't make the whole load on the online server while it's very slow like that and we don't need online benefits a lot, just for those two or three times remote modifications that the app user may or may not need, so i can't trade off localhost speed for online benefits which i need only two or three times a day.
what solution can i do.
i knew about master-slave and master-master replication but it's not an option too.</p>
<p>is there any ideas and thank you in advance.</p>
<p>-------------- about the two environments (local and online)------------
local is windows running xamp stack (apache, mysql, php)
server is linux (don't know actually which distro but any way i can't install any tools there ... just php packages with composer)</p>
","<mysql><database><laravel><data-synchronization>","2020-08-01 23:08:57","1870","0","2","67109245","<p>To answer your question, If I were you, I will create a <strong>sync_data</strong> table in the application. And the sync data table will have the responsibility to record the changes occurring for various entities.</p>
<p>For example, if you change customer data in the localhost, you will save an entry to <strong>sync_data</strong> like <strong>type=customer, id=10, action=update, synced=No</strong>. And using a cron you can push these updates -fetching the customer record by saved id-  to your online server in regular intervals, which will not make your online server busy. Furthermore, your online users will have the latest data at least.</p>
"
"62838610","How to improve data synchronization using Unity an multicast UDP socket","<p>I'm teaching myself some simple networking using Unity and Sockets and I'm running into problems synchronizing data between a client and server. I'm aware that there are other options using Unity Networking but, before I move on, I want to understand better how to improve my code using the System libraries.</p>
<p>In this example I'm simply trying to stream my mouse position over a multicast UDP socket. I'm encoding a string into a byte array and sending that array once per frame. I'm aware that sending these values as a string is un-optimal but, unless that is likely the bottleneck, I'm assuming It's ok come back to optimize that later.</p>
<p>Im my setup server is sending values at 60 fps, and the client is reading at the same rate. The problem I'm having is that when the client receives values it typically receives many at once. If I log the values I received with a <code>-----</code> between each frame I typically get output like this:</p>
<pre><code>------
------
------
------
------
------
------
119,396
91,396
45,391
18,379
-8,362
-35,342
-59,314
------
------
------
------
------
------
------
</code></pre>
<p>I would expect unsynchronized update cycles to lead to receiving two values per frame, but I'm not sure what might be accounting for the larger discrepancy.</p>
<p>Here's the Server code:</p>
<pre><code>using UnityEngine;
using System.Net;
using System.Net.Sockets;
using System.Text;

public class Server : MonoBehaviour
{
    Socket _socket;

    void OnEnable ()
    {
        var ip = IPAddress.Parse (&quot;224.5.6.7&quot;);
        var ipEndPoint = new IPEndPoint(ip, 4567);

        _socket = new Socket (AddressFamily.InterNetwork, SocketType.Dgram, ProtocolType.Udp);
        _socket.SetSocketOption (SocketOptionLevel.IP, SocketOptionName.AddMembership, new MulticastOption (ip));
        _socket.SetSocketOption (SocketOptionLevel.IP, SocketOptionName.MulticastTimeToLive, 2);
        _socket.Connect(ipEndPoint);
    }

    void OnDisable ()
    {
        if (_socket != null)
        {
            _socket.Close();
            _socket = null;
        }
    }

    public void Send (string message)
    {
        var byteArray = Encoding.ASCII.GetBytes (message);
        _socket.Send (byteArray);
    }
}
</code></pre>
<p>And the client:</p>
<pre><code>using UnityEngine;
using System.Net;
using System.Net.Sockets;
using System.Text;

public class Client : MonoBehaviour
{
    Socket _socket;
    byte[] _byteBuffer = new byte[16];

    public delegate void MessageRecievedEvent (string message);
    public MessageRecievedEvent messageWasRecieved = delegate {};
    

    void OnEnable ()
    {
        var ipEndPoint = new IPEndPoint(IPAddress.Any, 4567);
        var ip = IPAddress.Parse(&quot;224.5.6.7&quot;);

        _socket = new Socket(AddressFamily.InterNetwork, SocketType.Dgram, ProtocolType.Udp);
        _socket.Bind (ipEndPoint);
        _socket.SetSocketOption (SocketOptionLevel.IP, SocketOptionName.AddMembership, new MulticastOption(ip,IPAddress.Any));
    }

    void Update ()
    {
        while (_socket.Available &gt; 0)
        {
            for (int i = 0; i &lt; _byteBuffer.Length; i++) _byteBuffer[i] = 0;
            _socket.Receive (_byteBuffer);
            messageWasRecieved (Encoding.ASCII.GetString (_byteBuffer));
        }
    }
}
</code></pre>
<p>If anybody could shed light on what I can do to improve synchronization that would be a great help.</p>
","<c#><sockets><unity3d><udp><multicastsocket>","2020-07-10 16:31:37","292","2","1","62840547","<p>Network I/O is subject to a large number of external influences, and TCP/IP as a protocol has few requirements. Certainly none that would provide a guarantee of the behavior you seem to want.</p>
<p>Unfortunately, without a good <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">Minimal, Complete, and Verifiable code example</a>, it's not possible to verify that your server is in fact sending data at the interval you claim. It's entirely possible you have a bug that's causing this behavior.</p>
<p>But if we assume that the code itself is perfect, there are still no guarantees when using UDP that datagrams won't be batched up at some point along the way, such that a large number appear in the network buffer all at once. I would expect this to happen with higher frequency when the datagrams are sent through multiple network nodes (e.g. a switch and especially over the Internet), but it could just as easily happen when the server and client are both on the same computer.</p>
<p>Ironically, one option that <em>might</em> force the datagrams to be spread out more is to pad each datagram with extra bytes. The exact number of bytes required would depend on the exact network route; to do this &quot;perfectly&quot; might require writing some calibration logic that tries different padding amounts until the code sees datagrams arriving at the intervals it expects.</p>
<p>But that would significantly increase the complexity of your network I/O code, and yet still would not <em>guarantee</em> the behavior you'd like. And it has some obvious negative side-effects, including the extra overhead on the network (something people using metered network connections certainly won't appreciate), as well as increasing the likelihood of a UDP datagram being dropped altogether.</p>
<p>It's not clear from your question whether your project actually requires multicast UDP, or if that's just something in your code because that's what some tutorial or other example you're following was using. If multicast is not actually a requirement, another thing you definitely should try is to use direct UDP without multicasting.</p>
<p>FWIW: I would not implement this the way you have. Instead, I would use asynchronous receive operations, so that my client receives datagrams the instant they are available, rather than only checking periodically each frame of rendering. I would also include a sequence number in the datagram, and discard (ignore) any datagrams that arrive out of sequence (i.e. where the sequence number isn't strictly greater than the most recent sequence number already received). This approach will improve (perhaps only slightly) responsiveness, but also will handle situations where the datagrams arrive out of order, or are duplicated (two of the three main delivery issues one will experience with UDPâ€¦the third being, of course, failure of delivery).</p>
"
"62766807","Thrift, Avro and ProtoBuf data governance","<p>We have a use case of data streaming from the main transactional system to other downstream such as data analytics and machine learning team.</p>
<p>One of the requirements are to ensure data governance that data source can control who can read which column, and potentially lifecycle of a data to ensure data siting in another domain gets purged should the source data removed it, such as if a user deletes the account, we need to make sure the data in all downstream gets removed.</p>
<p>While we are considering Thrift, Avro and ProtoBuf, what are the common frameworks that we can use for such data governance? Do any of these protocol supports metadata for such data governance around data authorization, lifecycle?</p>
","<protocol-buffers><avro><thrift-protocol><data-security>","2020-07-07 01:33:03","258","0","1","62810976","<p>Let me get this straight:</p>
<p><strong>protobuf is not a security device</strong>; to someone with the right tools it is just as readable as xml or json, with the slight issue that it can be uncertain how to interpret some values;</p>
<p>It's not of a much difference than JSON nor XML. It is just an interface language. Sure, it has <a href=""https://developers.google.com/protocol-buffers/docs/encoding"" rel=""nofollow noreferrer"">encoding</a>, it is a bit different and a lot more customizable, but it does in no way confront security. It is up to you to secure the channel between sender and receiver.</p>
"
"62724853","How to reconcile React's setState with using the URL as the single source of truth?","<p>In React's documentation they recommend generally abstaining from the use of <code>this.forceUpdate</code>, instead recommending to use <code>this.setState</code> whenever possible.</p>
<p>In my application I have a component that contains a slide-out pane. It has nested elements and clicking each item updates the URL in a different way. For example, I could navigate to the following URLs by clicking around in it:</p>
<pre><code>site.com/posts/create
site.com/posts/read
site.com/posts/4895734
site.com/posts/5465462
site.com/messages/43455
site.com/messages/create
</code></pre>
<p>Clicking each of these items should update the component's view and display it differently based on the URL. However, just calling <code>window.history.pushState</code> to update the URL does not cause React to update (you need to call <code>setState</code> or <code>forceUpdate</code> for that).</p>
<p>As a result, I am unsure of how to proceed. I rather not duplicate the &quot;source of truth&quot; and mirror the URL in React's state variables, because then there is conflict over whether or not the URL should be used to determine the state versus the component's state variables. It seems better to have the URL be the single source of truth.</p>
<p>However, if this is the case, then every time I change the URL I would have to do <code>this.forceUpdate</code> or some no-op call like <code>this.setState({ asdf: 5 })</code>. This would certainly work, but React's documentation specifically warns about calling <code>forceUpdate</code> all over the place in the application.</p>
<p>Is there a better way?</p>
","<javascript><reactjs><url><browser-history>","2020-07-04 03:06:57","590","1","1","62726260","<p>I'm not sure what routing configuration you have, but are you using the history package?
<a href=""https://reactrouter.com/core/api/history"" rel=""nofollow noreferrer"">https://reactrouter.com/core/api/history</a></p>
<p><a href=""https://www.npmjs.com/package/history"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/history</a></p>
<p>Example:</p>
<p><a href=""https://codesandbox.io/s/vigilant-dhawan-qf6ke?file=/src/App.js"" rel=""nofollow noreferrer"">https://codesandbox.io/s/vigilant-dhawan-qf6ke?file=/src/App.js</a></p>
<p>That should mount/rerender on url changes, as well as give the ability to pass state through the router.</p>
<p>Edit:</p>
<p>Simplified version</p>
<p><a href=""https://codesandbox.io/s/trusting-cookies-pyhq6?file=/src/Router.js"" rel=""nofollow noreferrer"">https://codesandbox.io/s/trusting-cookies-pyhq6?file=/src/Router.js</a></p>
"
"62426021","CouchDB data synchronization","<p>I'm developing a TODO app for myself. It has to be offline first so data synchronization became a problem soon. As to my searches, CouchDB does data sync (replication) very well but I'm having some confusion here.<br>
First of all, I'm using Flutter and Dart programming language and there isn't any couch client on Dart. For javascript, there is <a href=""https://pouchdb.com/"" rel=""nofollow noreferrer"">PouchDB</a> which apparently makes data sync with remote database automatic (<strong>Am I right?</strong>) as far as I know I should be good to go without any third party library since CouchDB comes with native HTTP API, I can just store user data as a JSON file and just sync it with CouchDB server as a document (<strong>Am I right?</strong>)<br>
Another <a href=""https://stackoverflow.com/questions/46351432/per-user-db-pouchdb-couchdb-shared-data-doable"">confusion</a> <a href=""https://www.joshmorony.com/part-2-creating-a-multiple-user-app-with-ionic-2-pouchdb-couchdb/"" rel=""nofollow noreferrer"">for</a> <a href=""https://docs.couchdb.org/en/stable/config/couch-peruser.html"" rel=""nofollow noreferrer"">me</a> is that should user data be stored as a document or should I create 1 database per user(In other words does couch syncs database or document?) It also raises authorizations. A user which has access to a database has access to all documents. I want to restrict each user to its documents without placing another API between couch and end-user application.</p>
","<synchronization><couchdb><replication><pouchdb>","2020-06-17 09:41:16","2380","3","1","62456887","<p>A primary benefit of pouchdb is having local databases that have APIs like couchdb but are present even while offline, etc. So if you were using pouchdb and want to work offline you would almost certainly want per user dbs on couchdb and to do <strong>pouchdb &lt;-> couchdb</strong> syncing, etc. But you can't use pouchdb without JS and depending on it in a webview, etc, would largely undermine the benefit of compiling to native code instead of a webview based app.</p>

<p>Using HTTP APIs directly and storing a local document, it is probably simpler to start with each user storing a few simple things as additional JSON on their own user document in _users. You can always add per-user dbs later. The _users database is a little bit special in that it gives each user read access to their own document without having to give them access to other user docs and relies on a validation design document to protect important fields while otherwise letting them edit their data.</p>

<p>In such a scenerio, the client keeps a copy of the users own document locally that it needs to sync back to the server when it has networking. There are of course conflict possibilities for a user with multiple devices, and they can be resolved either by the client itself comparing local and server versions or with the help of update design documents on the server.</p>

<p>Starting from each user having their private data in their own _users doc, one can still build dbs for groups of users where all users in the group share read access to all documents but can only update ones they created. A per user db is really only a special case of such a database, where the group has been reduced to only one member.</p>
"
"62285280","How to ensure consistency of data between two execution loops","<p>I have been developing control software based on FreeRTOS operating system. From the timing point of view the software can be divided into two loops.</p>

<p>The first execution loop (fast loop (FL)) is invoked with period 100 us at the end of analogue-digital conversion. The second execution loop (slow loop (SL)) is invoked after each fourth execution of fast loop. So the timing of the control software can be described by following timing diagram </p>

<p><a href=""https://i.stack.imgur.com/Q5EbK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q5EbK.jpg"" alt=""enter image description here""></a></p>

<p>I need to ensure consistency of data shared between fast and slow loop. The slow loop has to use during its whole execution time the data values valid at the end of fourth execution of the fast loop. </p>

<p>I have been thinking about how to ensure the above mentioned consistency of data. My first idea is to use some global variable (letÂ´s say <code>mutex</code>) </p>

<pre><code>typedef enum
{
    SLOW_LOOP_RUNNING,
    SLOW_LOOP_WAITING
}MutexState

MutexState mutex; 
</code></pre>

<p>which will be used in atomic manner. For example in case the slow loop starts its calculation it at first executes</p>

<pre><code>mutex = SLOW_LOOP_RUNNING;
</code></pre>

<p>At the end of slow loop execution it executes</p>

<pre><code>mutex = SLOW_LOOP_WAITING;
</code></pre>

<p>The fast loop monitors always the status of the <code>mutex</code> variable and in case it finds out that the mutex contains <code>SLOW_LOOP_RUNNING</code> value it doesnÂ´t overwrite the shared variables. I think that this could work but I donÂ´t like the global variable. Does anybody have better idea how to resolve that? Thanks in advance for any suggestions.</p>
","<c><embedded><task><data-synchronization>","2020-06-09 14:44:48","97","0","1","62286951","<p>If the data is small, I'd just send/copy it. Especially if you're already using <code>fork()</code>, you can use <a href=""https://man7.org/linux/man-pages/man2/pipe.2.html"" rel=""nofollow noreferrer"">pipes</a> (<a href=""https://stackoverflow.com/a/44115862/6069586"">ex</a>). Where your fast loop, on every 4th cycle, can <a href=""https://man7.org/linux/man-pages/man2/write.2.html"" rel=""nofollow noreferrer""><code>write(buf)</code></a> the data to the pipe fd, and the slow loop then <a href=""https://man7.org/linux/man-pages/man2/read.2.html"" rel=""nofollow noreferrer""><code>read(buf)</code></a> from it. Even better, if you only want to run the slow loop cycle after you've received data from the 4th fast loop cycle, you can <a href=""https://man7.org/linux/man-pages/man2/select.2.html"" rel=""nofollow noreferrer""><code>select()</code></a> to only run the slow loop function when there is data to read from the pipe. This approach, vs mutexes, should be cleaner &amp; less error prone, avoiding the data race entirely; allowing you to treat the slow loop task as a purely functional task.</p>
"
"62202159","React Initializing and synchronizing state with backend","<p>I wonder what is the best way/pattern to initialize state and keep it synced with the server. I've read and tried a lot for the last couple of days but haven't found anything that solves my question.</p>

<p>The example is really simple. There is a state - it's a number in the example for the sake of simplicity, although in real life it would be an object - that I need to retrieve from the server. Once retrieved, I want it to be synchronized with the server. The getValueFromServer is a mock that returns a random value after waiting a random amount of time in a setTimeout.</p>

<p>So, to initialize my state I use a useEffect on an empty array as a dependency, and to keep it synched I use a useEffect with the state as a dependency.</p>

<p>The problem is that it is trying to save an undefined value. The log is as follows. </p>

<p>1 -> Initializing for first time</p>

<p>2 -> API call to save in server value:  undefined</p>

<p>3 -> API call to save in server value:  6.026930847574949</p>

<p>What I get:</p>

<p>1: Runs on mounting as expected.</p>

<p>2: This one I didn't expect. I guess it is triggered because of the ""useState"".</p>

<p>3: Runs once we get the response from the server. Kind of obvious but a pain in the ass, because why on earth would I want to save this.</p>

<p>What would the best approach be here? Using something like a ""isInitialized"" flag in the useEffect with dependency feels kind of hacked and not professional.</p>

<p>Code below and you can find it working here too: <a href=""https://codesandbox.io/s/optimistic-rgb-uce9f"" rel=""nofollow noreferrer"">https://codesandbox.io/s/optimistic-rgb-uce9f</a></p>

<pre><code>import React, { useState, useEffect } from ""react"";
import { getValueFromServer } from ""./api"";

export default function App() {
  const [value, setValue] = useState();

  useEffect(() =&gt; {
    async function initialize() {
      console.log(""Initializing for first time"");
      let serverValue = await getValueFromServer();
      setValue(serverValue);
    }
    initialize();
  }, []);

  useEffect(() =&gt; {
    console.log(""API call to save in server value: "", value);
  }, [value]);

  const handleClick = () =&gt; {
    setValue(value + 1);
  };

  return (
    &lt;div className=""App""&gt;
      &lt;h1&gt;Value: {value}&lt;/h1&gt;
      &lt;button onClick={handleClick}&gt;Add 1 to value&lt;/button&gt;
    &lt;/div&gt;
  );
}
</code></pre>
","<reactjs><react-hooks><data-synchronization>","2020-06-04 18:55:35","907","1","2","62202502","<p>your app does other stuff while it's busy <code>await</code>ing something to happen in your first <code>useEffect</code> hook function.  ""other stuff"" in this case is ""executing your second <code>useEffect</code> hook function"".  <code>serverValue</code>, and therefore <code>value</code>, is not defined at this point, so your <code>console.log</code> prints <code>undefined</code>.  once your <code>await</code> promise resolves to a value and <code>setValue</code> gets called, your second <code>useEffect</code> hook function's dependencies change causing your function to be run a second time (and printing out the value you expect).</p>

<p>if you have to have two <code>useEffect</code> hooks, then just bail out of the second one when <code>value</code> is undefined:</p>

<pre class=""lang-js prettyprint-override""><code>  useEffect(() =&gt; {
    if (value === undefined) {
      return;
    }

    console.log('value is:', value);
  }, [ value ]);
</code></pre>
"
"62202159","React Initializing and synchronizing state with backend","<p>I wonder what is the best way/pattern to initialize state and keep it synced with the server. I've read and tried a lot for the last couple of days but haven't found anything that solves my question.</p>

<p>The example is really simple. There is a state - it's a number in the example for the sake of simplicity, although in real life it would be an object - that I need to retrieve from the server. Once retrieved, I want it to be synchronized with the server. The getValueFromServer is a mock that returns a random value after waiting a random amount of time in a setTimeout.</p>

<p>So, to initialize my state I use a useEffect on an empty array as a dependency, and to keep it synched I use a useEffect with the state as a dependency.</p>

<p>The problem is that it is trying to save an undefined value. The log is as follows. </p>

<p>1 -> Initializing for first time</p>

<p>2 -> API call to save in server value:  undefined</p>

<p>3 -> API call to save in server value:  6.026930847574949</p>

<p>What I get:</p>

<p>1: Runs on mounting as expected.</p>

<p>2: This one I didn't expect. I guess it is triggered because of the ""useState"".</p>

<p>3: Runs once we get the response from the server. Kind of obvious but a pain in the ass, because why on earth would I want to save this.</p>

<p>What would the best approach be here? Using something like a ""isInitialized"" flag in the useEffect with dependency feels kind of hacked and not professional.</p>

<p>Code below and you can find it working here too: <a href=""https://codesandbox.io/s/optimistic-rgb-uce9f"" rel=""nofollow noreferrer"">https://codesandbox.io/s/optimistic-rgb-uce9f</a></p>

<pre><code>import React, { useState, useEffect } from ""react"";
import { getValueFromServer } from ""./api"";

export default function App() {
  const [value, setValue] = useState();

  useEffect(() =&gt; {
    async function initialize() {
      console.log(""Initializing for first time"");
      let serverValue = await getValueFromServer();
      setValue(serverValue);
    }
    initialize();
  }, []);

  useEffect(() =&gt; {
    console.log(""API call to save in server value: "", value);
  }, [value]);

  const handleClick = () =&gt; {
    setValue(value + 1);
  };

  return (
    &lt;div className=""App""&gt;
      &lt;h1&gt;Value: {value}&lt;/h1&gt;
      &lt;button onClick={handleClick}&gt;Add 1 to value&lt;/button&gt;
    &lt;/div&gt;
  );
}
</code></pre>
","<reactjs><react-hooks><data-synchronization>","2020-06-04 18:55:35","907","1","2","62202515","<blockquote>
  <p>What would the best approach be here? Using something like a
  ""isInitialized"" flag in the useEffect with dependency feels kind of
  hacked and not professional.</p>
</blockquote>

<p>You can either use a flag or initialize object with default value in <code>useState</code></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""true"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const { Fragment, useState, useEffect } = React;

const getValueFromServer = () =&gt; new Promise((resolve, reject) =&gt; setTimeout(() =&gt; resolve(Math.random())), 1000)

const App = () =&gt; {
  const [value, setValue] = useState(null);
  const [isLoading, setLoading] = useState(true);

  useEffect(() =&gt; {
    let isUnmounted = false;
  
    getValueFromServer().then(serverValue =&gt; {
      console.log(""Initializing for first time"");
      if(isUnmounted) {
        return;
      }
      setValue(serverValue);
      setLoading(false);
    })
    
    return () =&gt; {
      isUnmounted = true;
    }
  }, []);

  useEffect(() =&gt; {
    if(!value) {
      return () =&gt; {}
    }
  
    console.log(""API call to save in server value: "", value);
    setTimeout(() =&gt; setLoading(false), 50);
  }, [value]);

  const handleClick = () =&gt; {
    setLoading(true);
    setValue(value + 1);
  };

  return &lt;div className=""App""&gt;
    {isLoading ? &lt;Fragment&gt;
      &lt;span&gt;Loading...&lt;/span&gt;
    &lt;/Fragment&gt; : &lt;Fragment&gt;
      &lt;h1&gt;Value: {value}&lt;/h1&gt;
      &lt;button onClick={handleClick}&gt;Add 1 to value&lt;/button&gt;
    &lt;/Fragment&gt;}
  &lt;/div&gt;
}

ReactDOM.render(
    &lt;App /&gt;,
    document.getElementById('root')
  );</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;script src=""https://unpkg.com/react/umd/react.development.js""&gt;&lt;/script&gt;
&lt;script src=""https://unpkg.com/react-dom/umd/react-dom.development.js""&gt;&lt;/script&gt;
&lt;script src=""https://cdnjs.cloudflare.com/ajax/libs/babel-polyfill/7.10.1/polyfill.js""&gt;&lt;/script&gt;
&lt;script src=""https://unpkg.com/babel-standalone@6/babel.min.js""&gt;&lt;/script&gt;
&lt;div id=""root""&gt;&lt;/div&gt;</code></pre>
</div>
</div>
</p>
"
"61852544","Finding clusers or coherent values in a dataset with Python/pandas","<p>i am prettys new to Python and trying to do an event analysis.
I have two datasets: One with events and one with stockdata. Now i need to construct equally weighted portfolios and 'refresh' the portfolio construction every month.
Therfor I need consistent data (i guess). I mean for every date i need stock prices for all stocks which are in this analysis.
No I wanted to filter the data in this way that it shows me the biggest 'cluster' where i have data for all stocks for a certain period of time. 
As alternative show me all stocks and the period of time which have data.
I hope you guys understand my explanation.</p>

<pre><code>import pandas_datareader as pdr
import pandas as pd
import numpy as np
from sklearn import linear_model
import scipy.stats as st
 d = {'Date': ['1.02.2019', '2.02.2019', '3.02.2019', '4.02.2019', '5.02.2019'], 
     'a': [3.6, 3.4, 4.1, 4.2, 4.3], 
     'b': ['',2.4, 2.5, 2.6, 2.5], 
     'c': [2.5, 2.4,'',2.5, 2.5], 
     'd': [2.3, 2.4, 2.4, 2.5, '']}

df = pd.DataFrame(data=d)
df.set_index('Date')
</code></pre>

<p>In this case it should give me either a,b from 2.5.2019 to 5.02.2019 or a,d from 1.05.2019 to 4.5.2019 or something like this.</p>

<p>Is there a forumla for this in pandas?</p>

<p>Thx in advance</p>
","<python><pandas><events><data-quality>","2020-05-17 13:26:14","114","0","1","61854300","<p>You could do something like this, where the date ranges between the nulls are returned:</p>

<pre><code>d = {'Date': ['1.02.2019', '2.02.2019', '3.02.2019', '4.02.2019', '5.02.2019', '6.02.2019', '7.02.2019', '8.02.2019', '9.02.2019', '10.02.2019'], 
     'a': [3.6, 3.4, 4.1, 4.2, 4.3, 3.6, 3.4, 4.1, 4.2, 4.3], 
     'b': ['',2.4, 2.5, 2.6, 2.5, '',2.4, 2.5, 2.6, 2.5], 
     'c': [2.5, 2.4,'',2.5, 2.5, 2.5, 2.4,'',2.5, 2.5], 
     'd': [2.3, 2.4, 2.4, 2.5, '', 2.3, 2.4, 2.4, 2.5, '']}

df = pd.DataFrame(data=d)
df = df.replace('', np.nan)
df['Date'] = pd.to_datetime(df['Date'])
df.sort_values(by=['Date'], inplace=True)
df['Date'] = df['Date'].astype(str)

stocks = ['a', 'b', 'c', 'd']
for stock in stocks:
    print(f'Stock {stock}')
    nan_indexes = df[df[stock].isna()].index
    for i, value in enumerate(nan_indexes):
        if value == df.shape[0]-1:
            print(f'end index = {df.iloc[value-1][""Date""]}')
            continue
        if i==0:
            print(f'start index = {df.iloc[value+1][""Date""]}')
        else:
            print(f'end index = {df.iloc[value-1][""Date""]}')
            print(f'start index = {df.iloc[value+1][""Date""]}')
        if i==len(nan_indexes)-1:
            print(f'end index = {df.iloc[df.shape[0]-1][""Date""]}')
</code></pre>

<p><strong>Output</strong>:</p>

<pre><code>Stock a
Stock b
start index = 2019-02-02
end index = 2019-05-02
start index = 2019-07-02
end index = 2019-10-02
Stock c
start index = 2019-04-02
end index = 2019-07-02
start index = 2019-09-02
end index = 2019-10-02
Stock d
start index = 2019-06-02
end index = 2019-09-02
</code></pre>
"
"61796267","Data Lake: fix corrupted files on Ingestion vs ETL","<h2>Objective</h2>

<p>I'm building datalake, the general flow looks like Nifi -> Storage -> ETL -> Storage -> Data Warehouse.</p>

<p>The general rule for Data Lake sounds like no pre-processing on ingestion stage. All ongoing processing should happen at ETL, so you have provenance over raw &amp; processed data. </p>

<h2>Issue</h2>

<p>Source system sends corrupted CSV files. Means besides header and data, the first too lines are always of free format metadata we'll never use. Only single table is corrupted, the corrupted CSV is used by single Spark job at the moment (lets call it <code>X</code>).</p>

<h2>Question</h2>

<p><em>Is it a good approach to remove those two lines at Nifi layer?</em> See option 3 at ""Workarounds"".</p>

<h2>Workarounds</h2>

<ol>
<li>Handle the corrupted records inside Spark job <code>X</code>. IMHO, this is bad approach, because we gonna use that file at different tools in future (data governance schema crawlers, maybe some Athena/ADLA-like engines over ADLS/S3). Means corrupted records handling logic should be implemented at multiple places.</li>
<li>Fix corrupted files on ETL layer and store them at ""fixed"" layer. All ongoing activities (ETL, data governance, MPP engines) will work only with ""fixed"" layer, instead of ""raw"" layer. This sounds for me as an overhead, to create a new layer for single CSV.</li>
<li>Fix (remove the first two strings from the CSV) at Nifi layer. Means ""raw"" storage layer will always contain readable data. IMHO, this is good because it's simple and the handling logic is implemented at one place.</li>
</ol>
","<architecture><etl><data-ingestion><data-lake><data-governance>","2020-05-14 11:29:52","522","2","2","62008132","<p>First thing, I think that your question is brilliant and in the way you expose the mental process I can say that you have your answer already. </p>

<p>As you mention</p>

<blockquote>
  <p>The general rule for Data Lake sounds like no pre-processing on the ingestion stage. </p>
</blockquote>

<p>This is the philosophical bottom line, and all the hype is growing over this easy to oversimplify idea. </p>

<p>If we check the definition of <a href=""https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/"" rel=""nofollow noreferrer"">AWS of what is a data lake</a>.</p>

<blockquote>
  <p>A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analyticsâ€”from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.</p>
</blockquote>

<p>It is a basic definition, but let's use it as a ""appeal to authority"".  They say clearly that you can store data ""as-is"". </p>

<ol>
<li>My first question is: does ""you can"" mean strictly ""you should""?. Also, they mention that it allows you to ""run different types of analyticsâ€”from dashboards and visualizations to big data processing"", etc.  </li>
<li>My second question is: if the data is knowingly unstable for actually anything...is it legit to anyways dump it there? </li>
</ol>

<p>In the same link, a bit below, the also say </p>

<blockquote>
  <p>The main challenge with a data lake architecture is that raw data is stored with no oversight of the contents. For a data lake to make data usable, it needs to have defined mechanisms to catalog, and secure data. Without these elements, data cannot be found, or trusted resulting in a â€œdata swamp."" Meeting the needs of wider audiences require data lakes to have governance, semantic consistency, and access controls.</p>
</blockquote>

<p>In general my way of looking at it, is that throwing everything there to follow the rule of ""no preprocessing, is a general attempt of being more catholic than the pope, or maybe a general tendency to oversimplify the rules. I believe that the idea of ""as is"", and the power of it goes more in the direction of not doing data filtering or transformation in injection, assuming that we don't really know what are all the possible use cases in the future, so having raw data is good and scalable. But it doesn't mean that having data that we know is corrupted is good, and I believe that quality is a requirement always for data and in all stages should be at least accessible. </p>

<p>This takes me to the next thought: one very repeated idea is that data lake allows schema-on-read (<a href=""https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/"" rel=""nofollow noreferrer"">AWS</a>, <a href=""https://quickbooks-engineering.intuit.com/schema-on-read-curse-of-data-lakes-our-5-antidotes-1386199d262f"" rel=""nofollow noreferrer"">Intuit</a>, <a href=""https://www.ibmbigdatahub.com/blog/charting-data-lake-using-data-models-schema-read-and-schema-write"" rel=""nofollow noreferrer"">IBM</a>, <a href=""https://www.oreilly.com/content/data-governance-and-the-death-of-schema-on-read/"" rel=""nofollow noreferrer"">O'Reilly</a>). Being so, it makes sense to keep as much as possible something with some kind of schema, if we don't want to overcomplicate the life of everyone that will potentially want to use it, otherwise, we could maybe render it in cases useless as the overhead of using it can be discouraging. Actually, the O'Reilly article above, called ""the death of schema on read"" talks about exactly the complexity added by the lack of governance. So I guess removing some chaos will help the success of the data lake. </p>

<p>So far I think my position is very clear for myself -it was not that much when I started writing the response- but I will try to wrap up with the latest reference, that is an article that I read a few time. Published in gartner.com' press room as early as 2014, it is called ""<a href=""http://Gartner%20Says%20Beware%20of%20the%20Data%20Lake%20Fallacy"" rel=""nofollow noreferrer"">Beware of the Data Lake Fallacy</a>"". The whole article is quite interesting, but I will highlight this part</p>

<blockquote>
  <p>Data lakes, therefore, carry substantial risks. The most important is the inability to determine data quality or the lineage of findings by other analysts or users that have found value, previously, in using the same data in the lake. By its definition, a data lake accepts any data, without oversight or governance. Without descriptive metadata and a mechanism to maintain it, the data lake risks turning into a data swamp.</p>
</blockquote>

<p>I agree with that. It is fun at the beginning. Save everything, see you S3 bucket populated, and even run a few queries in Athena or Presto or run some Spark jobs over lots of gzip files and feel that we are in a magic time to live in. But then this small contamination comes, and we accept it, and someday the S3 buckets are not 10 but 100, and the small exceptions are not 2 but 20, and too many things to keep in mind and things get messier and messier. </p>

<p>Eventually this is opinion-based. But I would say usable data will make happier your future self. </p>

<p>Said this, I would go to your options:</p>

<ol>
<li><p>Handle the corrupted records inside Spark job X. You said it. That would be hating yourself and your team, cursing them to do a work that could be avoided. </p></li>
<li><p>Fix corrupted files on ETL layer and store them at ""fixed"" layer. You said it, too much overhead. You will continually tempt to delete the first layer. Actually I forecast you would end up with a lifecycle policy to get rid of old objects automatically to save cost. </p></li>
<li><p>Seems neat and honest. No one can tell you ""that is crazy"". The only thing you need to make sure is that actually the data you will delete is not business-related, and there is not a possible use in the future that you cannot figure now. Even in this case, I would follow some approach to play safe:</p>

<ul>
<li>Remove the first two strings from the CSV at Nifi layer, and save the readable data in the ""raw"" storage layer</li>
<li>To protect yourself from the case of ""we didn't see this coming"" keep a meta-data bucket in which you save simple files with those 2 lines removed, so you can access them in the future if need be, and you can reply to anyone with a different opinion that can say in the future ""you shouldn't have deleted that"".  But I say this because I cannot imagine what those two lines are, maybe this is totally overkilling. </li>
</ul></li>
</ol>

<hr>

<p>Personally, I love data lakes, and I love the philosophy behind every system but I also like to question everything case by case. I have lots of data in flat files, json, csv, and a lot of production workload based on that. But the most beautiful part of my data lake is not really purely unprocessed data, we found extremely powerful to do a first minimal cleanup, and when possible -for data that has fundamentally inserts and not updates-, also transform it to Parquet or ORC and even compress it with snappy. And I can tell you that I really enjoy using that data, even run queries on it directly. Raw data yes, but usable. </p>
"
"61796267","Data Lake: fix corrupted files on Ingestion vs ETL","<h2>Objective</h2>

<p>I'm building datalake, the general flow looks like Nifi -> Storage -> ETL -> Storage -> Data Warehouse.</p>

<p>The general rule for Data Lake sounds like no pre-processing on ingestion stage. All ongoing processing should happen at ETL, so you have provenance over raw &amp; processed data. </p>

<h2>Issue</h2>

<p>Source system sends corrupted CSV files. Means besides header and data, the first too lines are always of free format metadata we'll never use. Only single table is corrupted, the corrupted CSV is used by single Spark job at the moment (lets call it <code>X</code>).</p>

<h2>Question</h2>

<p><em>Is it a good approach to remove those two lines at Nifi layer?</em> See option 3 at ""Workarounds"".</p>

<h2>Workarounds</h2>

<ol>
<li>Handle the corrupted records inside Spark job <code>X</code>. IMHO, this is bad approach, because we gonna use that file at different tools in future (data governance schema crawlers, maybe some Athena/ADLA-like engines over ADLS/S3). Means corrupted records handling logic should be implemented at multiple places.</li>
<li>Fix corrupted files on ETL layer and store them at ""fixed"" layer. All ongoing activities (ETL, data governance, MPP engines) will work only with ""fixed"" layer, instead of ""raw"" layer. This sounds for me as an overhead, to create a new layer for single CSV.</li>
<li>Fix (remove the first two strings from the CSV) at Nifi layer. Means ""raw"" storage layer will always contain readable data. IMHO, this is good because it's simple and the handling logic is implemented at one place.</li>
</ol>
","<architecture><etl><data-ingestion><data-lake><data-governance>","2020-05-14 11:29:52","522","2","2","62110069","<p>I like the philosophy offered in the accepted answer but I'd like to provide a more tactical answer...</p>

<ul>
<li>Use the handle 'bad records' option on the spark read, e.g.:</li>
</ul>

<pre><code>spark.read
  .option(""badRecordsPath"", ""/tmp/badRecordsPath"")
  .format(""csv"")
  .load(""/input/csvFile.csv"")
</code></pre>

<p><a href=""https://docs.databricks.com/spark/latest/spark-sql/handling-bad-records.html"" rel=""nofollow noreferrer"">Reference ""Handling bad records and files""</a></p>

<p><a href=""https://docs.databricks.com/data/data-sources/read-csv.html"" rel=""nofollow noreferrer"">Reference ""CSV files""</a></p>

<p>You can use this with a schema option <code>.schema(customSchema)</code> code to get a level of schema verification too (and better performance) on the read side of your jobs.</p>

<ul>
<li><p>To perform schema checks on write, take a
look at <a href=""https://delta.io"" rel=""nofollow noreferrer"">Delta Lake open source project</a> which has schema on write enforcement and ACID transactions for more reliability.</p></li>
<li><p>Managed Delta Lake will let you bin pack your small files with the <code>OPTIMIZE</code> command <a href=""https://docs.databricks.com/spark/latest/spark-sql/language-manual/optimize.html"" rel=""nofollow noreferrer"">Databricks Delta Lake Optimize command</a></p>

<ul>
<li>Because of ACID transactions and bin packing, Spark Structured Streaming and Delta Lake work really well together to continue the streaming data acquisition Nifi is performing.</li>
</ul></li>
</ul>
"
"61622353","How is it possible to synchronize data between multiple devices?","<p>Let's say you are writing a program that many clients use. One server will only be able to handle the connections to a certain amount. The more connections you need to handle, the more power you need until you get a server farm containing different devices.
If you for example run an application where different clients can store data on your servers how is it possible to synchronize data on each device? Which hardware/software solutions exist? Or how is all the data stored?</p>
","<storage><data-synchronization><server-farm>","2020-05-05 20:21:18","407","0","2","61622897","<p>I can suggest an idea for a manual program creation , using file system only , you can exchange files between clients and server , but the server program will , in a period of time ( for example every 5 minutes ) broadcast the list of all his files to all connected clients and the exchanges will have to wait then ( if we are talking about a big volume of files) if its for small files then a 30 sec or 1 minute can be enough</p>
"
"61622353","How is it possible to synchronize data between multiple devices?","<p>Let's say you are writing a program that many clients use. One server will only be able to handle the connections to a certain amount. The more connections you need to handle, the more power you need until you get a server farm containing different devices.
If you for example run an application where different clients can store data on your servers how is it possible to synchronize data on each device? Which hardware/software solutions exist? Or how is all the data stored?</p>
","<storage><data-synchronization><server-farm>","2020-05-05 20:21:18","407","0","2","69648529","<p>There are many forms to do that...</p>
<p>I think you will need to have a single source of truth, let's say (the server on this case).</p>
<p>Then you can have a incremental number version (id) that contains the latest changes.</p>
<p>So each device can poll that number version only in order to know if is up to date.</p>
<p>If not, the device can ask the server the changes from the version that device has.</p>
<p>Each time a device makes a change, that change is stored on the server, and the version is incremented.</p>
<p>That can be one basic implementation. If you need real time updates, you can add to that implementation some publish-suscribe channel using Sockets... or some service like channels from pusher.com</p>
<p>So, every time a device made a change on the server, you can send a notificaiton on the channel with some information (the change) or the new ID of versiÃ³n and all the devices can update the information via the change (if is only one) or ask the server all the changes if there are many (like the device was disconnected from internet for a while).</p>
"
"61528731","R - Efficient data cleansing with custom functions","<p>I am working on cleaning a dataset composed of 1M names. The cleaning is done by a function that includes around 40 greps such as <code>name=gsub(""Johnmichael"", ""John Michael"",name,ignore.case=TRUE)</code> and <code>name=gsub(""Mihcael"", ""Michael"",name,ignore.case=TRUE)</code></p>

<p>I am currently using the cleaning function straight-up like this:</p>

<pre><code>contacts$first_name=clean_name(contacts$first_name)
</code></pre>

<p>My issue is that my code is very slow since it applies the function to the whole vector one at a time. I am trying to find a way to use the function in parallel for each string, I have tried sapply but I do not seem to find any improvements.
<strong>Any advice?</strong></p>
","<r><data-science>","2020-04-30 16:53:47","28","0","1","61528771","<p>2</p>

<p>Install the OpenBLAS in R in Windows x64</p>

<p>Open the url <a href=""http://sourceforge.net/projects/openblas/files/"" rel=""nofollow noreferrer"">http://sourceforge.net/projects/openblas/files/</a></p>

<p>Open the the latest version folder</p>

<p>download OpenBLAS-v0.2.13-Win64-int32.zip and mingw64_dll.zip</p>

<p>Unpack the ""OpenBLAS-v0.2.13-Win64-int32.zip"" find ""libopenblas.dll"" and rename this file to ""Rblas.dll"",copy the file to the path like this ""\R\R-3.1.2\bin\x64""(Remember to backup) Unpack the ""mingw64_dll.zip"" and copy all the DLL to the same path ""\R\R-3.1.2\bin\x64""</p>
"
"61408836","In a DAG, how can I find the column Primary Key in a table and test if any Null value?","<p>I writing a DataQualityOperator in a DAG. 
It should check if there's data in a Redshift table. To do this, I would like to check if the primary column contains null values. With sql, I found the name of the column Primary key. How do I check if it contains null values?(which means that the table is not good in my case).</p>

<pre><code>class DataQualityOperator(BaseOperator):
        check_template = """"""
                    SELECT a.attname
                    FROM   pg_index i
                    JOIN   pg_attribute a ON a.attrelid = i.indrelid
                                        AND a.attnum = ANY(i.indkey)
                    WHERE  i.indrelid = 'tablename'::regclass
                    AND    i.indisprimary;
        """"""

        def __init__ (self,redshift_conn_id = """", target_table="""", *args, **kwargs):
             super(...)  

        def execute(self, context):
            self.log.info(f'DataQualityOperator processing {target_table}')
            redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)

            check_records = redshift.get_records(check_template.format(self.target_table))


</code></pre>

<p>How to achieve this? 
Thank you for the help.</p>
","<postgresql><airflow><data-quality>","2020-04-24 12:53:37","224","1","1","61433434","<p>Are you looking for the SQL to check for NULLs in a column?  If so </p>

<pre><code>select count(1) from &lt;table&gt; where &lt;column&gt; is NULL; 
</code></pre>

<p>If other, please clarify.  </p>
"
"61365530","Is there a better way to implement single source of truth with rxjava in android","<p>In my app I have database which uses Room and a network service using retrofit. I have a requirement where if there is no data in local database I need to query the network and show a progress bar. If the network returns empty data then I need to show a empty view. One of the problem is that I need to ignore the empty data from the room and only consider empty data from the server so that when the user doesn't have any data he just sees a loading view and after the server returns empty data he will see empty view.</p>

<p>I have implemented this using a publish subject. Lce(loading content error) is wrapper object around data.</p>

<pre><code>val recentPublish = PublishSubject.create&lt;Lce&lt;List&lt;RecentMessage&gt;&gt;&gt;()

fun loadRecentMessages() {
        loadMessageFromDB()
        loadRecentMessageFromServer()
}
private fun loadMessageFromDB() {
    disposable = recentMessageDao.getRecentMessages() // this is a flowable
        .subscribeOn(Schedulers.io())
        .subscribe({
            Timber.d(""recent message from db size ${it.size}"")
            handleMessageFromDB(it)
        }, {
            it.printStackTrace()
            Timber.e(""error on flowable from db!"")
        })
}
protected fun handleMessageFromDB(messages: List&lt;RecentMessage&gt;) {
    // only publish if the data is not empty
    if (messages.isNotEmpty()) {
        recentPublish.onNext(Lce.Content(messages))
    }
}

private fun loadRecentMessageFromServer() {
    recentPublish.onNext(Lce.Loading())
    networkService.getLatestMessage() // this is a single
        .subscribe({
            val parsedMessages =
                DtoConverter.convertRecentPrivateMessageResponse(it, user.id!!)
            handleMessageFromServer(parsedMessages)
        }, {
            it.printStackTrace()
            recentPublish.onNext(Lce.Error(it))
            Timber.w(""failed to load recent message for private chat from server"")
        })
}

private fun handleMessageFromServer(recentMessages: List&lt;RecentMessage&gt;) {
    Timber.i(""recent messages from server ${recentMessages.size}"")
    if (recentMessages.isEmpty()) {
        recentPublish.onNext(Lce.Content(arrayListOf()))
    } else {
        recentMessageDao.saveAll(recentMessages)
    }
}
</code></pre>

<p>In the above code I am only passing the empty data from server and ignoring the empty data from room. This solution works but I wonder if there is some better functional approach to solve this problem. I am a beginner to Rxjava and any help will be appreciated. Thank you.</p>
","<android><rx-java2>","2020-04-22 12:47:03","1106","0","1","61386717","<p>After some research and the comment from @EpicPandaForce, I came up with this approach. I learned quite few things and it just clicked on me, about how to correctly use rxjava. Here is my approach, any comment will be appreciated. </p>

<pre><code>    fun getMessages(): Observable&lt;Lce&lt;List&lt;RecentMessage&gt;&gt;&gt; {
        return Observable.mergeDelayError(getMessagesFromDB(), getMessagesFromNetwork()) // even if network fails, we still want to observe the DB
    }

    private fun getMessagesFromDB(): Observable&lt;Lce.Content&lt;List&lt;RecentMessage&gt;&gt;&gt; {
        return recentMessageDao.getRecentMessages()
            .filter {
                it.isNotEmpty() // only forward the data from db if it's not empty
            }.map {
                Lce.Content(it)
            }
    }

    private fun getMessagesFromNetwork(): Observable&lt;Lce&lt;List&lt;RecentMessage&gt;&gt;&gt; {
        // first show a loading , then request for data
        return Observable.concat(Observable.just(Lce.Loading()), profileService.getLatestMessage()
                .flatMap {
                    processServerResponse(it) // store the data to db
                }.onErrorReturn {
                    Lce.Error(it)
                }.filter {
                    (it as Lce.Content).packet.isEmpty() // only forward data if it's empty
                })
    }


    private fun processServerResponse(response: RecentMessageResponse): Observable&lt;Lce&lt;List&lt;RecentMessage&gt;&gt;&gt; {
        return Observable.create {
            val parsedMessages =
                DtoConverter.convertRecentPrivateMessageResponse(response, user.id!!)
            handleMessageFromServer(parsedMessages)
            it.onComplete() // we use single source of truth so don't return anyting
        }
    }
</code></pre>
"
"61249797","SQL - find all examples of values in all colums with given characteristic","<p>I have a dataset (8.5 mill rows), where all values in all columns must be enclosed in quotation symbols ("" ""). I have discovered that there is a problem - some few records holds values in some columns with the last quotation symbol missing. Now I need to try to get an overview on the issue - which columns have examples of this error (it is due to truncation upstream in the solution). </p>

<p>From the example dummy data inserted below: </p>

<p>How do I write a query, which outputs the columns ""Last name"" and ""Age"" due to the missing end quotation in row 2 and 3 in these columns? To be clear - how do I identify columns with sporadic truncated values?</p>

<p><a href=""https://i.stack.imgur.com/KTe48.png"" rel=""nofollow noreferrer"">Example data with missing quotation symbols</a></p>

<p>Thanks,</p>

<p>knn</p>
","<sql><data-quality>","2020-04-16 12:04:58","29","1","1","61249907","<p>You need to check the last character in the string. It is done using the substring function, passing an argument of -1 means the last character. And you check to see if it is different than the double quote symbol.</p>

<pre><code>SELECT * FROM YourTable
WHERE 
substr(""Last Name"", -1) &lt;&gt; '""' OR substr(Age,-1) &lt;&gt; '""'
</code></pre>

<p>You can play around with it here <a href=""http://sqlfiddle.com/#!4/10a77e/1"" rel=""nofollow noreferrer"">http://sqlfiddle.com/#!4/10a77e/1</a></p>
"
"61145869","How do you use a list of dataframe names to perform data cleansing operations?","<p>I have a list of DataFrame names stored in a list like this:</p>

<pre><code>target_dfs = []
for x in np.arange(1950, 2020) :
    target_dfs.append('df_stat_data_' + str(x))
</code></pre>

<p>This yields a list of strings. But the actual DataFrames with those names do exist.</p>

<p>How do I effectively use each value on the list as a DataFrame and do operations such as dropping the last 3 rows?</p>

<p>I am trying to avoid doing something like this:</p>

<pre><code>df_stats_data_1950 = df_stats_data_1950.iloc[:-3]
...
df_stats_data_2020 = df_stats_data_2020.iloc[:-3]
</code></pre>
","<python><pandas>","2020-04-10 17:54:04","18","0","1","61145997","<p>Make it a dictionary:</p>

<pre class=""lang-py prettyprint-override""><code>target_dfs = {1950: df1, 1951: df2}
</code></pre>

<p>You can now do stuff like:</p>

<pre><code>for x in np.arange(1950, 2020):
    target_dfs[x].iloc[:-3]
</code></pre>
"
"61116079","How to get two SwiftUI sliders to update to and from a third value which represents the â€˜single source of truthâ€™ for their respective values","<p>I am very new to Combine and I am trying to get two SwiftUI sliders to update to and from a third value which represents the â€˜single source of truthâ€™ for their respective values. </p>

<p>Each slider has a range from 0â€¦1 but uses a different function to determine its value. In the example below, if the true value is 0.5, the first slider sets and gets that directly (0.5) while the second slider uses the value squared (0.25). </p>

<p>The example works, but I get the impression that this isnâ€™t really the proper way to do it. I get infinite recursion if the guard statements are removed in the Model. Can anyone show me the correct way to do this? Thanks!</p>

<p>Here is the Model:</p>

<pre><code>import SwiftUI

class Model : ObservableObject{

    @Published var trueValue: CGFloat = 0{
        didSet{
            slider1Value = trueValue
            slider2Value = trueValue * trueValue
        }
    }

    @Published var slider1Value: CGFloat = 0{
        didSet{
            guard oldValue != slider1Value else { return }

            trueValue = slider1Value
        }
    }

    @Published var slider2Value: CGFloat = 0{
        didSet{
            guard oldValue != slider2Value else { return }

            trueValue = slider2Value.squareRoot()
        }
    }
}

</code></pre>

<p>Here is the ContentView:</p>

<pre><code>
struct ContentView: View {
    @EnvironmentObject var model: Model

    var body: some View {
        GeometryReader{ geometry in
            VStack{
                ProportionalSlider(value: self.$model.slider1Value, width: geometry.size.width - 20)
                    .frame(width: geometry.size.width, height: 18)
                ProportionalSlider(value: self.$model.slider2Value, width: geometry.size.width - 20)
                    .frame(width: geometry.size.width, height: 18)
            }
        }
    }
}

</code></pre>

<p>And here is the ProportionalSlider:</p>

<pre><code>
struct ProportionalSlider: View {
    @Binding var value: CGFloat
    var width: CGFloat

    var body: some View{
        let gesture =  DragGesture(minimumDistance: 0)
            .onChanged({ mouse in         
                let clamped = min(self.width, max(0, mouse.location.x))
                self.value = clamped/self.width
            })

        return GeometryReader{ geometry in
            ZStack{
                ZStack(alignment: .leading){
                    Capsule().fill(Color.blue).frame(width: self.width, height: 15)
                       .gesture(gesture)
                    Capsule().fill(Color.red).frame(width: self.value * self.width, height: 15)
                       .gesture(gesture)
                }
            }
        }
    }
}

</code></pre>
","<swiftui><combine>","2020-04-09 07:33:09","264","0","1","61116276","<p>Use one <code>@Published</code> property and one computed property:</p>

<pre><code>class Model: ObservedObject {
    @Published var linearValue: CGFloat = 0

    var quadraticValue: CGFloat {
        get { linearValue * linearValue }
        set { linearValue = sqrt(newValue) }
    }
}
</code></pre>

<p>Bind one slider to <code>$model.linearValue</code> and bind the other to <code>$model.quadraticValue</code>.</p>
"
"61109143","SQL data cleansing","<p>I wanted to see if someone can help me with some data cleansing:</p>

<ol>
<li><p>I need to identify columns that have at least 5 numbers in a row. There is other data in the column</p></li>
<li><p>I need to identify columns that have exactly nine numbers in a row AND have ""("" one space to the right.  There is other data around this combination</p></li>
</ol>

<p>example - XXXXXXXXX (</p>

<p>I will apply this logic to a case expression.</p>
","<sql><sql-server>","2020-04-08 20:05:40","53","-2","2","61109260","<pre><code>  select *
  from your_table 
  where (
    your_column like '%11111$' 
        or your_column like '%22222%' 
        or your_column like '%33333%'
        or your_column like '%44444%'
        or your_column like '%55555%'
        or your_column like '%66666%'
        or your_column like '%77777%'
        or your_column like '%88888%'
        or your_column like '%99999%'
        or your_column like '%00000%'
   or your_column like '[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9] ('
</code></pre>
"
"61109143","SQL data cleansing","<p>I wanted to see if someone can help me with some data cleansing:</p>

<ol>
<li><p>I need to identify columns that have at least 5 numbers in a row. There is other data in the column</p></li>
<li><p>I need to identify columns that have exactly nine numbers in a row AND have ""("" one space to the right.  There is other data around this combination</p></li>
</ol>

<p>example - XXXXXXXXX (</p>

<p>I will apply this logic to a case expression.</p>
","<sql><sql-server>","2020-04-08 20:05:40","53","-2","2","66175129","<p>the problem is that no matter what you do it is a scan of the table</p>
<p>This needs to be a simple one-time pass program and on a backup image of the database</p>
<p>You can also use REPLACE in a wacky way to eliminate letters and put it all into a temp table, then just hammer on it .</p>
<p>I do something different.</p>
<p>I find position markers according to how the string was built, and apply meaningful name variables such as</p>
<p>First_left_paren_pos , third_space_pos, and other things like that, then, if your number is supposed to be between two markers, just do a substring on that column between those markers</p>
<p>In the real case, I will find that &quot; (&quot; at the end</p>
"
"60920728","Angular data synchronization issue","<p>I wanted to retrieve an information from backend if some email address from input already exists. Based on this information I'm calling a function that make a post that inserts user into database. The problem is that user is inserted only after second click on my SignUp button (function registerUser is called on this button).  </p>

<p>Component stuff:</p>

<pre><code> registerUser(form: NgForm) {
    let date: Date = new Date();
    this.newUser.registrationDate = date;
    this.checkEmailStatus();  //IMPLEMENTATION BELOW
      if (this.signupForm.valid === true &amp;&amp; this.emailStatus) {
      this.portfolioAppService.registerUser(this.newUser).subscribe((data) =&gt; {
        this.clearFields();
        this.navigateToLogin();
      },
        error =&gt; console.error(error)
      );
    }
  }

  checkEmailStatus() {
    this.portfolioAppService.checkEmailStatus(this.newUser.email).subscribe((data: string) =&gt; {
      if (data != """") {
        this.emailStatus = true;
      }
      else this.emailStatus = false;
    },
      error =&gt; console.error(error)
    );
  }
</code></pre>

<p>Here is my service:</p>

<pre><code>  checkEmailStatus(email: string): Observable&lt;string&gt; {
    return this.http.get&lt;string&gt;(`/api/Users/CheckEmailStatus_${email}`, this.httpOptions);
  }
</code></pre>

<p>Here is backend:</p>

<pre><code>    [HttpGet]
    [Route(""~/api/Users/CheckEmailStatus_{email}"")]
    public string CheckEmailStatus(string email)
    {
        try
        {
            User user = _context.Users.Where(u =&gt; u.Email == email).FirstOrDefault();
            if (user != null)
            {
                return user.Email;
            }
            else
            {
                return """";
            }

        }
        catch (Exception e)
        {
            throw new Exception(""Error!"");
        }

    }
</code></pre>
","<javascript><c#><angular><typescript><synchronization>","2020-03-29 21:07:11","164","0","1","60921020","<p>Call to <code>this.portfolioAppService.checkEmailStatus()</code> is <a href=""https://stackoverflow.com/a/14220323/6513921"">asynchronous</a>. So when you check <code>if (this.signupForm.valid === true &amp;&amp; this.emailStatus)</code> after the <code>this.checkEmailStatus()</code> call, the variable <code>this.emailStatus</code> is still undefined. To fix it, you could return an observable from the <code>checkEmailStatus()</code> in the component. Try the following</p>

<p>Component </p>

<pre class=""lang-js prettyprint-override""><code>registerUser(form: NgForm) {
  let date: Date = new Date();
  this.newUser.registrationDate = date;
  this.checkEmailStatus().pipe(take(1)).subscribe(status =&gt; {
    if (this.signupForm.valid === true &amp;&amp; status) {  // &lt;-- check the status of email address
      this.portfolioAppService.registerUser(this.newUser).subscribe((data) =&gt; {
        this.clearFields();
        this.navigateToLogin();
      },
        error =&gt; console.error(error)
      );
    }
  });
}

checkEmailStatus() : Observable&lt;boolean&gt; {
  const result = new Subject&lt;boolean&gt;();

  this.portfolioAppService.checkEmailStatus(this.newUser.email).subscribe(
    (data: string) =&gt; {
      if (data !== '') {
        result.next(true);
      }
      else result.next(false);
    },
    error =&gt; { 
      console.error(error);
      result.next(false);
    }
  );

  return result.asObservable();
}
</code></pre>
"
"60739515","Conditional input fields for Google sheets? (to assist data quality)","<p>Im struggling with my users putting data into google sheets incorrect. <strong>I need to know if there is anyway to set conditions to how users input data in google sheets?</strong> ie a watermark in the cells guiding them on how to enter data eg.<code>YYYY-MM-DD</code> </p>

<p><img src=""https://i.stack.imgur.com/5Mc3j.png"" alt=""VIEW EXAMPLE""></p>

<p>Any other ideas on how to guide users to input data correctly into google sheet are also welcome.</p>
","<validation><user-interface><google-sheets><user-input><user-experience>","2020-03-18 12:23:52","520","1","1","60740002","<p>there is no such thing as watermark but there are ways...</p>

<p>add column before date column and use this formula and drag down:</p>

<pre><code>=IFERROR({"""", ""yyyy-mm-dd""})
</code></pre>

<p><a href=""https://i.stack.imgur.com/Terti.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Terti.png"" alt=""enter image description here""></a></p>

<p>then use conditional formatting:</p>

<p><a href=""https://i.stack.imgur.com/W5qS9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W5qS9.png"" alt=""enter image description here""></a></p>

<p>then hide column:</p>

<p><a href=""https://i.stack.imgur.com/AfWRg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AfWRg.png"" alt=""enter image description here""></a></p>

<p>then use data validation:</p>

<p><a href=""https://i.stack.imgur.com/SsTPC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SsTPC.png"" alt=""0""></a></p>

<p>then use internal pre-formatting:</p>

<p><a href=""https://i.stack.imgur.com/vKtbv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vKtbv.png"" alt=""0""></a></p>

<hr>

<p>final result:</p>

<p><a href=""https://i.stack.imgur.com/nmuA3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nmuA3.png"" alt=""0""></a></p>
"
"60553913","Data Sync Between 3 Microservices","<p><a href=""https://i.stack.imgur.com/nbdYJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nbdYJ.png"" alt=""Microservice""></a></p>

<p>From the image given above let's say I have 2 microservices running own each DBs and the customer info is retrieved by another microservice with its own DB only for user transactions like add delete users. I am completly new to microservices and never used tools like <strong>'Messega Brokers'</strong> . After digging around I found data source and sink with <strong>Kafka</strong>. Using customser database as a source and other DBs as sink. Is this approach correct? Or how can I use <strong>Kafka</strong> in a proper way. Can you point me any tutorials?</p>

<p><strong>EDIT:</strong></p>

<p>I learnt that actually the tables would be all the same, I mean 3 different database but 3 same table, so logical replication would be OK . Could  using logical replication violate the microservice architecture? From the image Above I still wonder how can I use customer IDs (primary key) in Order Service API table</p>

<p><strong>EDIT 2:</strong></p>

<p>I found <a href=""https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/"" rel=""nofollow noreferrer"">this</a> article</p>
","<apache-kafka><microservices><data-synchronization>","2020-03-05 21:04:20","1608","2","1","60569960","<p>We're using a microservice prototyping framework which creates database and message broker integration boilerplate code, more here: <a href=""https://medium.com/@krawa76/bootstrap-node-js-microservice-stack-4a348db38e51"" rel=""nofollow noreferrer"">https://medium.com/@krawa76/bootstrap-node-js-microservice-stack-4a348db38e51</a></p>
"
"60494254","Detecting unit differences in data (SAS)","<p>I have two sets of financial data that tend to contain differences due to unit errors e.g. $10000 in one dataset may be $1000 in the other.</p>

<p>I'm trying to code a check for such differences, but the only way I can think of is to divide the two variables and see if the difference is in a table of 0.001, 0.01, 0.1, 10, 100 etc, but it would be hard to catch all of the differences.</p>

<p>Is there a smarter way to do this?</p>
","<sas><difference><data-quality>","2020-03-02 17:59:07","55","0","3","60494378","<p>Use <code>proc compare</code>. Be sure the two datasets are sorted in identical order, either by row or by specific groups. Use the <code>by</code> statement as needed. More info on options can be found in the <a href=""https://go.documentation.sas.com/?docsetId=proc&amp;docsetTarget=p1l5iwaf47ma83n1euxxp10g8xh5.htm&amp;docsetVersion=9.4&amp;locale=en"" rel=""nofollow noreferrer"">documentation</a>.</p>

<p>Example - compare a modified <code>cars</code> dataset with <code>sashelp.cars</code>:</p>

<pre><code>data cars_modified;
    set sashelp.cars;

    if(mod(_N_, 2) = 0) then msrp = msrp - 100;
run;

proc compare base    = sashelp.cars 
             compare = cars_modified 
             out     = out_differences 
             outnoequal 
             outdif
             noprint;
    var msrp;
run;
</code></pre>

<p>Only the observations with differences are output in <code>out_differences</code>:</p>

<pre><code>_TYPE_  _OBS_   MSRP
DIF     2      $-100
DIF     4      $-100
DIF     6      $-100
DIF     8      $-100
DIF     10     $-100
...
</code></pre>
"
"60494254","Detecting unit differences in data (SAS)","<p>I have two sets of financial data that tend to contain differences due to unit errors e.g. $10000 in one dataset may be $1000 in the other.</p>

<p>I'm trying to code a check for such differences, but the only way I can think of is to divide the two variables and see if the difference is in a table of 0.001, 0.01, 0.1, 10, 100 etc, but it would be hard to catch all of the differences.</p>

<p>Is there a smarter way to do this?</p>
","<sas><difference><data-quality>","2020-03-02 17:59:07","55","0","3","60495335","<p>So you appear to be asking to find cases where X/Y is a number that is exactly 1.00Exx where XX is an integer, other than 0.</p>

<pre><code>data _null_;
do x=1,10,100,1000;
  do y=1,2,3,10.1,10 ;
   ratio = x/y;
   power = floor(log10(ratio));
   if power ne 0 and 1.00 = round(ratio/10**power,0.01) then 
     put 'Ratio of ' x 'over ' y 'is 10**' power '.' 
   ;    
  end;
end;
run;
</code></pre>

<p>Results:</p>

<pre><code>Ratio of 1 over 10 is 10**-1 .
Ratio of 10 over 1 is 10**1 .
Ratio of 100 over 1 is 10**2 .
Ratio of 100 over 10 is 10**1 .
Ratio of 1000 over 1 is 10**3 .
Ratio of 1000 over 10 is 10**2 .
</code></pre>
"
"60494254","Detecting unit differences in data (SAS)","<p>I have two sets of financial data that tend to contain differences due to unit errors e.g. $10000 in one dataset may be $1000 in the other.</p>

<p>I'm trying to code a check for such differences, but the only way I can think of is to divide the two variables and see if the difference is in a table of 0.001, 0.01, 0.1, 10, 100 etc, but it would be hard to catch all of the differences.</p>

<p>Is there a smarter way to do this?</p>
","<sas><difference><data-quality>","2020-03-02 17:59:07","55","0","3","60504047","<p>For a numeric value <strong>X</strong> you can compute the nearest the rational expression, <strong>p/q</strong>.</p>

<p>If you calculate ratio</p>

<pre><code>X = amount_for_source_A / amount_from_source_B;
status = math.rational(X,1e5,p,q);
</code></pre>

<p>the ratio will be a multiple of 10 if <code>p=1 or q=1</code></p>

<p>Example:</p>

<pre><code>proc ds2;
  package math / overwrite = yes;
    method rational(double x, double maxden, in_out integer p, in_out integer q) returns double;
      /*
      ** FROM: https://www.ics.uci.edu/~eppstein/numth/frap.c
      ** FROM: https://stackoverflow.com/questions/95727/how-to-convert-floats-to-human-readable-fractions
      **
      ** find rational approximation to given real number
      ** David Eppstein / UC Irvine / 8 Aug 1993
      **
      ** With corrections from Arno Formella, May 2008
      **
      ** Modified for Proc DS2, Richard DeVenezia, Jan 2020.
      **
      ** usage: rational(r,d,p,q)
      **   x is real number to approx
      **   maxden is the maximum denominator allowed
      **   p is return for numerator
      **   q is return for denominator
      **   returns 0 if no problems
      **
      ** based on the theory of continued fractions
      ** if x = a1 + 1/(a2 + 1/(a3 + 1/(a4 + ...)))
      ** then best approximation is found by truncating this series
      ** (with some adjustments in the last term).
      **
      ** Note the fraction can be recovered as the first column of the matrix
      **  ( a1 1 ) ( a2 1 ) ( a3 1 ) ...
      **  ( 1  0 ) ( 1  0 ) ( 1  0 )
      ** Instead of keeping the sequence of continued fraction terms,
      ** we just keep the last partial product of these matrices.
      */

      declare integer m[0:1,0:1];
      declare double startx e1 e2;
      declare integer ai t result p1 q1 p2 q2;

      startx = x;

      /* initialize matrix */
      m[0,0] = 1; m[1,1] = 1;
      m[0,1] = 0; m[1,0] = 0;

      /* loop finding terms until denom gets too big */
      do while (1);

        ai = x;

        if not ( m[1,0] * ai + m[1,1] &lt; maxden ) then leave;

        t = m[0,0] * ai + m[0,1];
        m[0,1] = m[0,0];
        m[0,0] = t;

        t = m[1,0] * ai + m[1,1];
        m[1,1] = m[1,0];
        m[1,0] = t;

        if x = ai then leave;     %* AF: division by zero;

        x = 1 / (x - ai);

        if x &gt; 2147483647 /*x'7FFFFFFF'*/ then leave;  %* AF: representation failure;
      end;

      /* now remaining x is between 0 and 1/ai */
      /* approx as either 0 or 1/m where m is max that will fit in maxden */
      /* first try zero */

      p1 = m[0,0];
      q1 = m[1,0];
      e1 = startx - 1.0 * p1 / q1;

      /* now try other possibility */

      ai = (maxden - m[1,1]) / m[1,0];

      m[0,0] = m[0,0] * ai + m[0,1];
      m[1,0] = m[1,0] * ai + m[1,1];

      p2 = m[0,0];
      q2 = m[1,0];
      e2 = startx - 1.0 * p2 / q2;

      if abs(e1) &lt;= abs(e2) then do;
        p = p1;
        q = q1;
      end;
      else do;
        p = p2;
        q = q2;
      end;

      return 0;
    end;
  endpackage;
run;
quit;
</code></pre>

<p><code>* Example uage;</code></p>

<pre><code>proc ds2;
  data _null_;
    declare package math math();
    declare double x;
    declare int p1 q1 p q;

    method run();
      streaminit(12345);

      x = 0;
      do _n_ = 1 to 20;
        p1 = ceil(rand('uniform',9));
        q1 = ceil(rand('uniform',9));

        x + 1. * p1 / q1;

        math.rational (x, 10000, p, q);

        put 'add' p1 '/' q1 '  ' x=best16. 'is' p '/' q;
      end;
    end;
  enddata;
run;
quit;
</code></pre>

<pre><code>----- LOG -----
add 4 / 1    x=               4 is 4 / 1
add 4 / 2    x=               6 is 6 / 1
add 2 / 7    x=6.28571428571429 is 44 / 7
add 4 / 6    x=6.95238095238095 is 146 / 21
add 5 / 2    x=9.45238095238095 is 397 / 42
add 5 / 2    x= 11.952380952381 is 251 / 21
add 7 / 1    x= 18.952380952381 is 398 / 21
add 8 / 6    x=20.2857142857143 is 142 / 7
add 9 / 3    x=23.2857142857143 is 163 / 7
add 8 / 2    x=27.2857142857143 is 191 / 7
add 3 / 1    x=30.2857142857143 is 212 / 7
add 9 / 3    x=33.2857142857143 is 233 / 7
add 4 / 3    x=34.6190476190476 is 727 / 21
add 4 / 6    x=35.2857142857143 is 247 / 7
add 1 / 9    x=35.3968253968254 is 2230 / 63
add 8 / 3    x=38.0634920634921 is 2398 / 63
add 2 / 4    x=38.5634920634921 is 4859 / 126
add 5 / 1    x=43.5634920634921 is 5489 / 126
add 1 / 2    x=44.0634920634921 is 2776 / 63
add 2 / 7    x=44.3492063492064 is 2794 / 63

</code></pre>

<p><a href=""https://www.devenezia.com/downloads/sas/samples/#mathpackage"" rel=""nofollow noreferrer"">DS2 math package</a></p>
"
"60346233","How to identify different data types within a single column?","<p>Let us say that we have a column with the following values:</p>

<p>Apple,
Mango,
Orange,
123,
987,
Guava,
01/01/2020</p>

<p>python recognizes this column as an ""object"" data type automatically.
I have been given a task to count the number of data types in a single column. 
For a human eye, it is evident that there are 3 data types in the above column values: string, int, date.
However, I am unable to come up with a code which could do this segregation.
Looking forward to the guidance!
Thank you!</p>
","<python><data-analysis><data-cleaning><data-quality>","2020-02-21 21:08:35","427","-1","2","60346279","<p>Could you provide some code on what you have attempted? How are you setting up the data structure? Is it an m * n matrix where each column would have a common j in <code>data[i][j]</code>? In that case, it seems like you could just iterate over the matrix rows for the column and use the <code>type()</code> method.</p>

<p>Not knowing a bit more about your setup makes it difficult to provide a solution. Generally data in columns is of a common type.</p>
"
"60346233","How to identify different data types within a single column?","<p>Let us say that we have a column with the following values:</p>

<p>Apple,
Mango,
Orange,
123,
987,
Guava,
01/01/2020</p>

<p>python recognizes this column as an ""object"" data type automatically.
I have been given a task to count the number of data types in a single column. 
For a human eye, it is evident that there are 3 data types in the above column values: string, int, date.
However, I am unable to come up with a code which could do this segregation.
Looking forward to the guidance!
Thank you!</p>
","<python><data-analysis><data-cleaning><data-quality>","2020-02-21 21:08:35","427","-1","2","60346305","<p>If your data is in the form of a list, you could always do <code>list(map(type, list_name))</code> to return a list of types.</p>
"
"60284286","Hybris Synchronization error for custom attribute","<p>I have added a new attribute OOTB (BundleTemplate) to Product (this attribute is catalog aware) and everytime I  try to synchronize a product I get this error:</p>

<pre><code>ERROR [000001KN::de.hybris.platform.catalog.jalo.synchronization.CatalogVersionSyncJob] (000001KN) [CatalogVersionSyncMaster] sync ended with 4 unfinished items - see last sync media for details
ERROR [000001KN::de.hybris.platform.catalog.jalo.synchronization.CatalogVersionSyncJob] (000001KN) [CatalogVersionSyncJob] Finished synchronization in 0d 00h:00m:00s:999ms. There were errors during the synchronization!
</code></pre>

<p>With this info I just can't find the reason why the sync isn't working , is there any way to debug this to get a better LOG ?</p>
","<sap-commerce-cloud><data-synchronization><backoffice>","2020-02-18 15:26:15","1960","3","1","60300403","<p>de.hybris.platform.catalog.jalo.synchronization.ItemCopyCreator#copy() Line 166</p>

<p>Solved!</p>
"
"60259536","How to transfer or remove data synchronization with Redux-persist?","<p>Made an authorization application. After a successful login, data is written to AsyncStorage.
I need to access data through shared storage. How to transfer or remove data synchronization with Redux-persist?</p>
","<reactjs><react-native>","2020-02-17 09:37:38","47","0","1","60259751","<p>Redux-persist have blacklist and whitelist, if your value in backlist it won't be persisted ( whitelist is opposite).</p>
"
"60245931","Loopback 4 - Mongo and Elastic data synchronization","<p>Is there an easy way to sync MongoDB data with Elastic if we are using a <strong>Loopback 4</strong> application? (In a NodeJS Express application we can easily do this using mongoosastic plugin.)</p>

<p>Currently in loopback a model is bound to one data source which is either a mongo or elastic. My ultimate goal is to write POST, PUT, DELETE methods in mongo (and replicate in elastic) and GET methods to use Elastic as datasource.</p>
","<mongodb><typescript><elasticsearch><synchronization><loopback4>","2020-02-16 06:18:46","130","0","1","60247816","<p>I would highly recommend to decouple the syncoronization part from the core routes of your application. You could leverage the MongoDB Change streams to listen for change and update your elasticsearch. This could be a simple app running on a totally different server which does the syncronization while you carry on serving your requests. This will make the overall process and architecture more durable and fault tolerant. You can read more about this on following link</p>

<p><a href=""https://medium.com/@montumodi/realtime-sync-from-mongodb-to-elasticsearch-using-change-streams-2559ad312011"" rel=""nofollow noreferrer"">Mongodb to elasticsearch</a></p>
"
"60223389","Using React's Hooks useReducer, the state is not ""single source of truth""? How does the App or Sibling get the state of other components?","<p>I tried the <a href=""https://reactjs.org/docs/hooks-reference.html#usereducer"" rel=""nofollow noreferrer"">sample code of useReducer</a>:</p>

<pre><code>const initialState = { count: 0 };

function reducer(state, action) {
  switch (action.type) {
    case ""increment"":
      return { count: state.count + 1 };
    case ""decrement"":
      return { count: state.count - 1 };
    default:
      throw new Error();
  }
}

export default function Counter() {
  const [state, dispatch] = useReducer(reducer, initialState);

  // ...
</code></pre>

<p>and instantiated 3 Counters in the App.
Sample on: <a href=""https://codesandbox.io/s/serene-morse-c017r"" rel=""nofollow noreferrer"">https://codesandbox.io/s/serene-morse-c017r</a></p>

<p>So it seems the state is local to each Counter component, and is not the ""single source of truth"" like on Redux?  If the App wants to get the value of all counters, or one Counter wants to get the value of another Counter, how would that be done?</p>
","<reactjs><react-hooks><use-reducer>","2020-02-14 09:23:31","476","0","1","60223562","<p>I made changes in your codesandbox and used context to make your counter as single source of truth like redux.</p>

<pre><code>import React from ""react"";
const CountStateContext = React.createContext();
const CountDispatchContext = React.createContext();
function countReducer(state, action) {
  switch (action.type) {
    case ""increment"": {
      return { count: state.count + 1 };
    }
    case ""decrement"": {
      return { count: state.count - 1 };
    }
    default: {
      throw new Error(`Unhandled action type: ${action.type}`);
    }
  }
}
function CountProvider({ children }) {
  const [state, dispatch] = React.useReducer(countReducer, { count: 0 });
  return (
    &lt;CountStateContext.Provider value={state}&gt;
      &lt;CountDispatchContext.Provider value={dispatch}&gt;
        {children}
      &lt;/CountDispatchContext.Provider&gt;
    &lt;/CountStateContext.Provider&gt;
  );
}
function useCountState() {
  const context = React.useContext(CountStateContext);
  if (context === undefined) {
    throw new Error(""useCountState must be used within a CountProvider"");
  }
  return context;
}
function useCountDispatch() {
  const context = React.useContext(CountDispatchContext);
  if (context === undefined) {
    throw new Error(""useCountDispatch must be used within a CountProvider"");
  }
  return context;
}
export { CountProvider, useCountState, useCountDispatch };
</code></pre>

<p><a href=""https://codesandbox.io/s/modern-wildflower-ihvjj"" rel=""nofollow noreferrer"">https://codesandbox.io/s/modern-wildflower-ihvjj</a></p>
"
"60220106","Using Kafka for syncing data between two microservices","<p>I am trying to use kakfa to sync data for two microservices A &amp; B.</p>

<p><strong>A</strong> stages some data for a group of employees in a company in a database table. Then end user will trigger an event from UI to <strong>A</strong>'s backend service where it will send kakfa message(s) to a topic <strong>B</strong> is subscribed to.</p>

<p><strong>B</strong> then takes the data either from a message or from a staged table, validates it and persist to its own database table.</p>

<p>Questions I have are..</p>

<ol>
<li><p>Employees can range from 10 to 1000s per company, and there could be multiple companies trying to sync the data at certain time of the year. So performance is a concern. What would be a good way to divide the load? meaning.. should I design the message to be at employee level? This would mean there could be thousands of messages although design wise it would be the simplest. Or should it be at a company level? or group of employees within a company? Microservice is not doing much processing and persisting to the table.. Would it able to handle the load? What would be the limiting factor?</p></li>
<li><p>Data we are handling is JSON stored in DB. Would it be better to have a staging table and look up from <strong>B</strong> using some sort of primary key in the message? or is having all the data within a message be fine? JSON is not that big per employee data, but if aggregated to a group of employee let's say 100s, it may be 10-100 Kilobytes. Are we buying much from looking up data from the table?</p></li>
<li><p>We need to be able to track the status/errors, so that end user is aware of any issues and perform action to correct data and/or try resync. Some approach I thought of was creating a table, call it <strong>BATCH_JOB</strong> and <strong>BATCH_TASK</strong> table to keep track of request at the job level (UI event for a group of employees as mentioned which trigger the resync process) and task (employee level). Or would there be a cleaner approach?</p></li>
</ol>

<p>Any help/tips of design would be appreciated.</p>
","<java><performance><apache-kafka><relational-database><data-synchronization>","2020-02-14 04:43:18","1488","1","1","60227444","<blockquote>
  <p>What would be a good way to divide the load?</p>
</blockquote>

<p>The short answer is using custom partitioning schemes with a reasonably large number of partitions. Say 100. </p>

<p>Or you can create a topic per company, depending on if you are using different record schemas per topic</p>

<blockquote>
  <p>Are we buying much from looking up data from the table?</p>
</blockquote>

<p>Well, you cannot query a topic as easily as a table, so that's the benefit... You could also use a KTable and interactive queries</p>

<blockquote>
  <p>Data we are handling is JSON stored in DB</p>
</blockquote>

<p>I assume you're not just putting one BLOB column into the database (and you've not clarified what database you're using either).</p>

<p>Personally, I'd suggest you use Avro and Kafka Connect to sink topics into databases. That's the recommended solution for such a task within the Kafka APIs without introducing other projects like Spark or writing your own database code</p>

<blockquote>
  <p>We need to be able to track the status/errors, so that end user is aware of any issues and perform action to correct data</p>
</blockquote>

<p>Tables could work, but if you can write records to a table, you can also write events to another Kafka topic and get ""notifications"" from that </p>
"
"60175997","Generalized Data Quality Checks on Datasets","<p>I am pulling in a handful of different datasets daily, performing a few simple data quality checks, and then shooting off emails if a dataset fails the checks.</p>

<p>My checks are as plain as checking for duplicates in the dataset, as well as checking if the number of rows and columns in a dataset haven't changed -- See below.</p>

<pre><code>assert df.shape == (1016545, 8)
assert len(df) - len(df.drop_duplicates()) == 0
</code></pre>

<p>Since these datasets are updated daily and may change the number of rows, is there a better way to check instead of hardcoding the specific number? </p>

<p>For instance, one dataset might have only 400 rows, and another might have 2 million. 
Could I say to check within 'one standard deviation' of the number of rows from yesterday? But in that case, I would need to start collecting previous days counts in a separate table, and that could get ugly.</p>

<p>Right now, for tables that change daily, I'm doing the following rudimentary check:</p>

<pre><code>assert df.shape[0] &lt;= 1016545 + 100
assert df.shape[0] &gt;= 1016545 - 100
</code></pre>

<p>But obviously this is not sustainable.</p>

<p>Any suggestions are much appreciated.</p>
","<python><pandas><airflow><standard-deviation><data-quality>","2020-02-11 19:23:18","959","0","1","60177264","<p>Yes, you would need to store some previous information, but since you don't seem to care about perfectly statistically accurate I think you can cheat a little. If you keep the average number of records based on the previous samples, the previous deviation you calculated, and the number of samples you took you can get reasonably close to what you are looking for by finding the weighted average of the previous deviation with the current deviation.</p>

<p>For example:</p>

<p>If the average count has been 1016545 with a deviation of 85 captured over 10 samples, and today's count is 1016612. If you calculate the difference from the mean <code>(1016612 - 1016545 = 67)</code> then the weighted average of the previous deviation and the current deviation <code>((85*10 + 67)/11 â‰ˆ 83)</code>. </p>

<p>This makes it so you are only storing a handful of variables for each data set instead of all the record counts back in time, but this also means it's not actually standard deviation.</p>

<p>As for storage, you could store your data <a href=""https://www.geeksforgeeks.org/sql-using-python/"" rel=""nofollow noreferrer"">in a database</a> or <a href=""https://stackabuse.com/reading-and-writing-json-to-a-file-in-python/"" rel=""nofollow noreferrer"">a json file</a> or any number of other locations -- I won't go into detail for that since it's not clear what environment you are working in or what resources you have available.</p>

<p>Hope that helps!</p>
"
"60170535","On Athena AWS, last update on table?","<p>I try to monitor the data quality on AWS Athena. 
I would like to know how can i find when data have been loaded in a table? </p>

<p>The table hasn't partition and i can't do the partition on this table. </p>

<p>Thanks for your help!</p>
","<sql><amazon-s3><metadata><amazon-athena><data-quality>","2020-02-11 13:57:54","1026","2","1","60197241","<p>If Partition is not there, its easy as only one file it will be referring.Athena is not basically a table but analytics tool which reads a file in S3. You can browse to file path and get last last_modified. To get location try running </p>

<pre><code>     SHOW CREATE TABLE &lt;table_name&gt;
</code></pre>
"
"60081298","Use Azure data sync with localdb","<p>B""H</p>

<p>How can I use Azure Data Sync with <strong>localdb</strong>?</p>

<p>Please note: I am not asking about using it with a ""Local Database"" as in an on-prem database. I am speaking specifically about the technology (particular Sql Server Edition) known as <strong>localdb</strong>. As in <code>(localdb)\MSSQLLocalDB</code>.</p>

<p>The issue is adding the <code>localdb</code> instance to <code>SQL Data Sync Agent</code>. The agent must be run under an entity (windows login) that has permission to run as a service. <strong>It can not be run using a windows live identity</strong> which is what the users are usually logged in as. Yet it also needs to have permissions to the database in localdb.</p>

<p>Use Case:</p>

<p>I have a central database running in Azure. With multiple users with laptops and limited internet connection, running local apps (some built in Access). Connecting directly from the apps to Azure is too slow or otherwise impractical. Therefore I install the lightweight <code>localdb</code> as a cache for the users data. Which will be merged using Azure Data Sync.</p>

<p>I am currently using a solution built directly on Microsoft Sync <em>Framework</em>. However there is a maintenance overhead on that solution. So I would like to migrate to the Microsoft maintained Azure Data Sync.</p>

<p>Thank you</p>
","<azure><azure-sql-database><localdb><data-synchronization><azure-data-sync>","2020-02-05 17:40:54","261","1","1","60290790","<p>B""H</p>

<p>Ok, it was actually a silly mistake.</p>

<p>You actually <strong>can</strong> run the <code>SQL Data Sync Agent</code> using a windows live identity. You just need to use the local user name. Which is the name of the folder for that user in the <code>Users</code> folder. You must also use the name of the computer in the format <code>MyComputer\localUserName</code>.
Once I was running the <code>SQL Data Sync Agent</code> under the currently logged in user - who also has access to the DB - everything else ran smoothly.</p>
"
"60055229","issue in data scheduling from mysql to mongo db","<p>We are developing a SAAS system in which we have initially used MySQL as DB but as data grows our listings of data get slower so to resolve that we had used Mongo DB in which we had stored the prepared JSON we need to display (with all the join of MySQL) for some time it works well </p>

<p>we have written a scheduler in java which runs in every 2 mins and update the modified records from MySQL to mongo   </p>

<p>Initially, it works well but as time goes and data and its rate increases it fails many times  so we decided to find any alternative for that  which can read from MySQL binlogs and we can merge MySQL tables according to our need on the way and store in Mongo DB </p>

<pre><code>Table 1
  Col11
  Col12
  Col13
  Col14
  Col15
</code></pre>

<p>Table 2
      Col21
      Col22
      Col23
      Col24
      Col25</p>

<p>Mongo Collection</p>

<pre><code>  Col11
  Col12
  Col13
  Col14
  Col15 
  Col21
  Col22
  Col23
  Col24
  Col25
</code></pre>
","<mysql><mongodb><apache-kafka><etl><data-synchronization>","2020-02-04 10:20:02","188","0","1","60055392","<p>One option could be <a href=""https://docs.confluent.io/3.0.0/connect/"" rel=""nofollow noreferrer"">Kafka Connect</a> for moving data from MySQL to Kafka and then from Kafka to your MongoDB. </p>

<p><strong>Step 1: Use <a href=""https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html"" rel=""nofollow noreferrer""><code>JDBCSourceConnector</code></a> to move data from MySQL to Kafka</strong> </p>

<blockquote>
  <p>The Kafka Connect JDBC source connector allows you to import data from
  any relational database with a JDBC driver into Apache KafkaÂ® topics.</p>
</blockquote>

<p><strong>Step 2: Use <a href=""https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html"" rel=""nofollow noreferrer"">MongoDB Connector</a> to move data from Kafka to MongoDB</strong></p>

<blockquote>
  <p>Map and persist events from Kafka topics directly to MongoDB
  collections with ease. Ingest events from your Kakfa topics directly
  into MongoDB collections, exposing the data to your services for
  efficient querying, enrichment, and analytics.</p>
</blockquote>

<p>Note that MongoDB connector can be used as source or sink connector. In your case, you'd need the sink connector, for moving data from your Kafka topic(s) to your target table(s) in MongoDB.</p>
"
"60038211","How to view specific changes in data at particular version in Delta Lake","<p>Right now I have one test data which have 1 partition and inside that partition it has 2 parquet files</p>

<p>If I read data as:</p>

<pre><code>val df = spark.read.format(""delta"").load(""./test1510/table@v1"")
</code></pre>

<p>Then I get latest data with 10,000 rows and if I read:</p>

<pre><code>val df = spark.read.format(""delta"").load(""./test1510/table@v0"")
</code></pre>

<p>Then I get 612 rows, now my question is: How can I view only those new rows which were added in version 1 which is 10,000 - 612 = 9388 rows only</p>

<p>In short at each version I just want to view which data changed. Overall in delta log I am able to see json files and inside there json file I can see that it create separate parquet file at each version but how can I view it in code ?</p>

<p>I am using Spark with Scala</p>
","<scala><apache-spark><delta-lake><data-quality>","2020-02-03 11:21:53","2158","3","1","60043966","<p>you don't even need to go at <code>parquet</code> file level. you could simply use SQL query to achieve this. </p>

<pre><code>%sql 
SELECT * FROM test_delta VERSION AS OF 2 minus SELECT * FROM test_delta VERSION AS OF 1
</code></pre>

<p>Above code will give you a newly added rows in version 2 which were not in version 1 </p>

<p>in your case you can do the following </p>

<pre><code>val df1 = spark.read.format(""delta"").load(""./test1510/table@v1"")
val df2 = spark.read.format(""delta"").load(""./test1510/table@v0"")
display(df2.except(df1))
</code></pre>
"
"59982674","Android background and foreground data synchronization","<p>Currently i'm developing an app that needs to synchronize data to the server, every 15 minutes and manual if the sync button is pressed. The issue i'm facing at the moment is that syncs are not queued. A manual sync job can run at the same time as an automatically triggered one, and this should not be possible. The sync will go wrong if the same data is send twice at the same time. </p>

<p>I've tried JobService and WorkManager, but can't really think of a right solution. I've created a PeriodicWorkRequest and a OneTimeWorkRequest. They should queue, and run after the active task is finished.</p>

<p>Any thoughts?</p>
","<android><multithreading><background>","2020-01-30 09:24:25","825","0","1","60000139","<p>I fixed it like this. I think this should work:</p>

<p>background scheduling:</p>

<pre><code>Constraints constraints = new Constraints.Builder().setRequiredNetworkType(NetworkType.CONNECTED).build();
PeriodicWorkRequest workRequest = new PeriodicWorkRequest.Builder(SyncWorker.class,15, TimeUnit.MINUTES).setConstraints(constraints).build();
WorkManager.getInstance().enqueueUniquePeriodicWork(SyncWorker.BACKGROUND_SYNC, ExistingPeriodicWorkPolicy.KEEP, workRequest);
</code></pre>

<p>foreground / active sync:</p>

<pre><code>OneTimeWorkRequest workRequest = new OneTimeWorkRequest.Builder(SyncWorker.class).build();
WorkManager.getInstance().enqueueUniqueWork(SyncWorker.FOREGROUND_SYNC, ExistingWorkPolicy.KEEP, workRequest);
</code></pre>

<p>To manage background and foreground syncs:</p>

<pre><code>public class SyncWorker extends Worker {

    public final static String FOREGROUND_SYNC = ""FOREGROUND"";
    public final static String BACKGROUND_SYNC = ""BACKGROUND"";
    private static boolean isWorking = false;

    public SyncWorker(@NonNull Context context, @NonNull WorkerParameters workerParams) {
        super(context, workerParams);
    }

    @NonNull
    @Override
    public Result doWork() {
        if(isWorking) {
            return Result.failure();
        }

        isWorking = true;

        // sync

        isWorking = false;
        return Result.success();
    }
}
</code></pre>
"
"59737649","How firestore data architecture for restaurants","<p>we have an order tracking application for restaurants, and we use firestore.</p>

<p>We have some questions about database architecture. How should we keep the data?</p>

<p>We currently store orders as follows</p>

<p><strong>server / {restaurant} / orders / {OrderID}</strong></p>

<p>Each order document contains the tableName parameter, and we query it with the tableName field when taking orders for a table and paying for the table.</p>

<p>Should this be the right architecture?</p>

<p><strong>server / {restaurant} / tables / {TableID} / orders / {OrderID}</strong></p>

<p><strong>server / {restaurant} / tables / {TableID} / payments / {paymentÄ±d}</strong></p>

<p>This is the standart restaurant system you can image it.</p>

<p><strong><em>Restaurant have a lot of table, in this tables have orders and payments, this is the how it works...</em></strong></p>
","<firebase><google-cloud-firestore>","2020-01-14 16:08:08","477","0","1","59739862","<p>This is a highly opinionated and application specific question.</p>

<p>With Firestore / NoSQL I suggest first prototyping your application from the user's perspective. What are the different screens and associated data on each screen. This will help you identify the different queries and read/write operations that are frequently performed and how to best structure your firestore.</p>

<p>For example: It may be that you keep Table Orders in separate unique documents, but also that one of the screens includes a 'summary of orders' where a user can view the previous 100 orders. This requirement may dictate that you also want to include a summary document that contains those previous orders so that when the user views your summary screen you are not being charged 100 read operations, and instead only 1.</p>

<ul>
<li>Start by prototyping your views</li>
<li>Identify the data model for each view</li>
<li>Identify the queries you will need to perform</li>
<li>Start putting together a database infrastructure that is performant for the above requirements</li>
</ul>
"
"59695687","WearOS app does not receive data from android phone","<p>I have a simple program where I just send a message from my phone to my wearos emulator and vice versa but the onReceive function app never gets called on both devices. (and yes, I have paired my phone with the emulator)</p>

<p>This is my android phone code:</p>

<pre><code>public class MainActivity extends AppCompatActivity {

Button talkbutton;
TextView textview;
protected Handler myHandler;
int receivedMessageNumber = 1;
int sentMessageNumber = 1;

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);
    talkbutton = findViewById(R.id.talkButton);
    textview = findViewById(R.id.textView);

    //Create a message handler//
    myHandler = new Handler(new Handler.Callback() {
        @Override
        public boolean handleMessage(Message msg) {
            Bundle stuff = msg.getData();
            messageText(stuff.getString(""messageText""));
            return true;
        }
    });

    IntentFilter messageFilter = new IntentFilter(Intent.ACTION_SEND);
    Receiver messageReceiver = new Receiver();
    LocalBroadcastManager.getInstance(this).registerReceiver(messageReceiver, messageFilter);

}

public void messageText(String newinfo) {
    if (newinfo.compareTo("""") != 0) {
        textview.append(""\n"" + newinfo);
    }
}

//Define a nested class that extends BroadcastReceiver//
public class Receiver extends BroadcastReceiver {
    @Override

    public void onReceive(Context context, Intent intent) {
        String message = ""I just received a message from the wearable "" + receivedMessageNumber++;;
        textview.setText(message);
    }
}

public void talkClick(View v) {
    String message = ""Sending message.... "";
    textview.setText(message);
    new NewThread(""/my_path"", message).start();

}

//Use a Bundle to encapsulate the message//
public void sendmessage(String messageText) {
    Bundle bundle = new Bundle();
    bundle.putString(""messageText"", messageText);
    Message msg = myHandler.obtainMessage();
    msg.setData(bundle);
    myHandler.sendMessage(msg);

}

class NewThread extends Thread {
    String path;
    String message;

    //Constructor for sending information to the Data Layer//
    NewThread(String p, String m) {
        path = p;
        message = m;
    }

    public void run() {

    //Retrieve the connected devices
        Task&lt;List&lt;Node&gt;&gt; wearableList =
                Wearable.getNodeClient(getApplicationContext()).getConnectedNodes();
        try {

            List&lt;Node&gt; nodes = Tasks.await(wearableList);
            for (Node node : nodes) {
                Task&lt;Integer&gt; sendMessageTask =

                //Send the message//
                Wearable.getMessageClient(MainActivity.this).sendMessage(node.getId(), path, message.getBytes());
                Integer result = Tasks.await(sendMessageTask);
                sendmessage(""I just sent the wearable a message "" + sentMessageNumber++);
            }

        } catch (Exception exception) {
            Toast.makeText(getApplicationContext(),exception.getMessage(),Toast.LENGTH_LONG);
        }
    }
}}
</code></pre>

<p>MessageService which sends the message to the wear os app:</p>

<pre><code>public class MessageService extends WearableListenerService {

@Override
public void onMessageReceived(MessageEvent messageEvent) {

    if (messageEvent.getPath().equals(""/my_path"")) {

        final String message = new String(messageEvent.getData());

        Intent messageIntent = new Intent();
        messageIntent.setAction(Intent.ACTION_SEND);
        messageIntent.putExtra(""message"", message);
        LocalBroadcastManager.getInstance(this).sendBroadcast(messageIntent);
    }
    else {
        super.onMessageReceived(messageEvent);
    }
}
</code></pre>

<p>Since the receive and send function from both apps are very identical i think its enough to show you the code from the android device but if you like here is the receive method from the wearos app:</p>

<pre><code>    IntentFilter newFilter = new IntentFilter(Intent.ACTION_SEND);
    Receiver messageReceiver = new Receiver();

    LocalBroadcastManager.getInstance(this).registerReceiver(messageReceiver, newFilter);



    public class Receiver extends BroadcastReceiver {
     @Override
     public void onReceive(Context context, Intent intent) {

     //Display the following when a new message is received//

     String onMessageReceived = ""I just received a message from the handheld "" + receivedMessageNumber++;
     textView.setText(onMessageReceived);

     }
    }
</code></pre>
","<android><wear-os><synchronous><data-synchronization>","2020-01-11 15:13:00","315","1","1","63466752","<p>Have you added the required <a href=""https://developers.google.com/android/reference/com/google/android/gms/wearable/WearableListenerService"" rel=""nofollow noreferrer"">intent filter in your Manifest.xml for the WearableListenerService</a>? Something like this:</p>
<pre><code>     &lt;service android:name=&quot;your.package.YourWatchListenerService&quot;&gt;
            &lt;intent-filter&gt;
                &lt;action android:name=&quot;com.google.android.gms.wearable.MESSAGE_RECEIVED&quot; /&gt;
                &lt;data
                    android:host=&quot;*&quot;
                    android:pathPrefix=&quot;/my_path&quot;
                    android:scheme=&quot;wear&quot; /&gt;
            &lt;/intent-filter&gt;
        &lt;/service&gt;
</code></pre>
"
"59567634","Maintaining data quality and speeding up query with two OR statements in JOIN","<p>I am troubleshooting part of a VERY complex query that is performing very slowly.</p>

<p>As written, the query takes 40 minutes to execute and returns ~141,000 results. I am using SQL Server 2005 (yep).</p>

<p>The problematic part of the code is the multiple OR statements within the JOIN. Here's the original query. Keep in mind that the SELECT clause contains a ton of processing that I am not including for space reasons. The query subsequently joins to three more tables, but I am not including them since they are all simple joins. </p>

<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM table1 t1
LEFT JOIN table2 t2 ON t2.date_1 = t1.date
INNER JOIN table3 t3  on ( t3.sid = t1.sid )
LEFT JOIN table4 t4 ON
    ((t1.cid != 26 AND t4.srscid = t1.cid)  OR (t1.cid = 26 AND t1.pcode = t4.pcode) )
    AND (t1.sid = t4.section OR t1.sid = t4.r_section)
    and t1.nprice &gt;= t4.min_breakpoint AND t1.nprice &lt;= t4.max_breakpoint
    and t1.date &gt;= t4.start AND t1.date &lt;= t4.end
</code></pre>

<p>In an attempt to speed this up, referencing other advice I've seen here, I've converted the two OR statements to UNIONs.</p>

<pre class=""lang-sql prettyprint-override""><code>
SELECT *
FROM table1 t1
LEFT JOIN table2 t2 ON t2.date_1 = t1.date
INNER JOIN table3 t3  on ( t3.sid = t1.sid )
LEFT JOIN table4 t4 ON
    ((t1.cid != 26 AND t4.srscid = t1.cid) )
    AND (t1.sid = t4.section OR t1.sid = t4.r_section)
    and t1.nprice &gt;= t4.min_breakpoint AND t1.nprice &lt;= t4.max_breakpoint
    and t1.date &gt;= t4.start AND t1.date &lt;= t4.end
UNION

SELECT *
FROM table1 t1
LEFT JOIN table2 t2 ON t2.date_1 = t1.date
INNER JOIN table3 t3  on ( t3.sid = t1.sid )
LEFT JOIN table4 t4 ON
    ((t1.cid = 26 AND t1.pcode = t4.pcode) )
    AND (t1.sid = t4.section OR t1.sid = t4.r_section)
    and t1.nprice &gt;= t4.min_breakpoint AND t1.nprice &lt;= t4.max_breakpoint
    and t1.date &gt;= t4.start AND t1.date &lt;= t4.end
</code></pre>

<p>The above query did perform faster, executing in ~30 mins, but it returned 307,000 results.</p>

<p>The only difference I see between my case and the examples I've encountered elsewhere are that the OR statement occurs in a set of parentheses. </p>

<p>Can anyone help troubleshoot why the UNION is returning so many different results from the OR join?</p>

<p>Thanks.</p>
","<sql><sql-server><join>","2020-01-02 17:40:20","68","1","1","59569798","<p>It's because the 2 queries have a left join to table4.<br />
But with the criteria for cid 26 in the on clause.</p>
<p>To illustrate it, look at the output of these simplified examples:</p>

<blockquote>
<pre><code>--
-- sample data
--
create table tbl1 
(
    cid int primary key,
    pcode varchar(30) not null
);

create table tbl2
(
    id2 int identity(1,1) primary key,
    srscid int,
    pcode varchar(30) not null
);

insert into tbl1 (cid, pcode) values
(26, 'a'),
(31, 'b'),(32, 'c'),(33, 'd');

insert into tbl2 (pcode, srscid) values
('a', 26),('b', 26), 
('x', 31),('y', 32);
</code></pre>
</blockquote>

<blockquote>
<pre><code>-- query 1
-- cid != 26, srscid = cid
--
select t1.*, t2.id2, t2.pcode as pcode2
from tbl1 t1
left join tbl2 t2
on (t2.srscid = t1.cid and t1.cid != 26)
</code></pre>
<pre>
cid | pcode |  id2 | pcode2
--: | :---- | ---: | :-----
 26 | a     | <em>null</em> | <em>null</em>  
 31 | b     |    3 | x     
 32 | c     |    4 | y     
 33 | d     | <em>null</em> | <em>null</em>  
</pre>
</blockquote>

<blockquote>
<pre><code>-- query 2
-- cid = 26, pcode
--
select t1.*, t2.id2, t2.pcode as pcode2
from tbl1 t1
left join tbl2 t2
on (t2.pcode = t1.pcode and t1.cid = 26)
</code></pre>
<pre>
cid | pcode |  id2 | pcode2
--: | :---- | ---: | :-----
 26 | a     |    1 | a     
 31 | b     | <em>null</em> | <em>null</em>  
 32 | c     | <em>null</em> | <em>null</em>  
 33 | d     | <em>null</em> | <em>null</em>  
</pre>
</blockquote>

<blockquote>
<pre><code>-- query 3
-- cid != 26 or cid = 26
--
select t1.*, t2.id2, t2.pcode as pcode2
from tbl1 t1
left join tbl2 t2
on ((t2.srscid = t1.cid    and t1.cid != 26) or 
    (t2.pcode = t1.pcode and t1.cid = 26))
</code></pre>
<pre>
cid | pcode |  id2 | pcode2
--: | :---- | ---: | :-----
 26 | a     |    1 | a     
 31 | b     |    3 | x     
 32 | c     |    4 | y     
 33 | d     | <em>null</em> | <em>null</em>  
</pre>
</blockquote>

<blockquote>
<pre><code>-- query 4
-- union query 1 &amp; 2
--
select t1.*, t2.id2, t2.pcode as pcode2
from tbl1 t1
left join tbl2 t2
on (t2.srscid = t1.cid and t1.cid != 26)
UNION
select t1.*, t2.id2, t2.pcode
from tbl1 t1
left join tbl2 t2
on (t2.pcode = t1.pcode and t1.cid = 26)
</code></pre>
<pre>
cid | pcode |  id2 | pcode2
--: | :---- | ---: | :-----
 26 | a     | <em>null</em> | <em>null</em>  
 26 | a     |    1 | a     
 31 | b     | <em>null</em> | <em>null</em>  
 31 | b     |    3 | x     
 32 | c     | <em>null</em> | <em>null</em>  
 32 | c     |    4 | y     
 33 | d     | <em>null</em> | <em>null</em>  
</pre>
</blockquote>

<blockquote>
<pre><code>-- query 4
-- union ALL, with WHERE clauses
--
select t1.*, t2.id2, t2.pcode as pcode2
from tbl1 t1
left join tbl2 t2
on t2.srscid = t1.cid 
WHERE t1.cid != 26
UNION ALL
select t1.*, t2.id2, t2.pcode
from tbl1 t1
left join tbl2 t2
on t2.pcode = t1.pcode 
WHERE t1.cid = 26
order by t1.cid
</code></pre>
<pre>
cid | pcode |  id2 | pcode2
--: | :---- | ---: | :-----
 26 | a     |    1 | a     
 31 | b     |    3 | x     
 32 | c     |    4 | y     
 33 | d     | <em>null</em> | <em>null</em>  
</pre>
</blockquote>
<p><em>db&lt;&gt;fiddle <a href=""https://dbfiddle.uk/?rdbms=sqlserver_2017&amp;fiddle=86c7e2323d79dad5fcf7188cef0146b9"" rel=""nofollow noreferrer"">here</a></em></p>
"
"59368111","How to handle local time zone change when syncing data to server?","<p>I have an android app. The application performs some tasks and updates the status of those tasks in local DB with the timestamp. Suppose the User logged an event. So an entry is added to event table with event_name and time_stamp. For timestamp, I get the current time in milliseconds, convert it to UTC time and save it in DB.</p>

<pre><code>val cal = Calendar.getInstance(TimeZone.getTimeZone(""GMT""))
val t = cal.time.time
</code></pre>

<p>The application has a sync service running. This service basically maintains a timestamp which represents the time of the last sync. Service iterates all tables in DB finds the records whose timestamps are greater than the timestamp maintained by service and syncs those records to the server. The server in response to syncing returns its current time in UTC to sync service at the client app. And client app updates its timestamp value to the one provided by the server.</p>

<p>But the problem is when users change their current local time/time zone. In that case, the UTC time I get locally is not proper.
So whenever some event is logged in local DB it is logged with a wrong timestamp.
So when sync service starts iterating all tables it does not find correct records which are modified by the user since the last sync.</p>

<p>How do i get the proper UTC time stamp at client app even if user has changed the time locally?</p>

<p>I can not ask server to give its current time every time i go to save a record in local DB.</p>

<p>Should i get server time once my app starts and maintain a clock service which keeps track of elapsed time?</p>

<p>Or should i use some other parameter instead of time to keep track of modified rows?</p>

<p>How do I handle this problem?</p>
","<android><timestamp><client-server><data-synchronization><sql-timestamp>","2019-12-17 05:32:24","226","0","1","59369089","<p>I recommend you do not depend on the device's local timestamps there are so many cases where local time can be incorrect. the better way to manage timestamp use server time server can add a timestamp in all response, and you can save this timestamp mapped with API endpoints try to make your response link:</p>

<pre><code>{
""timestamp"": 1576566019,
""success"": true,
""message"": ""User list!"",
""data"": [
    {
   //your response body
   }
 }
</code></pre>
"
"59227750","How to sync master SQL Server database with multiple customer data to individual customer db's","<p>I have a central SQL Server database in which we store multiple customers' data.
The tables that need to be synchronized are similar but not identical. For example:</p>

<p>In the customer's db (which is SQL Server CE wrapped with EF) is a table (obviously a contrived example) with rows:</p>

<pre><code>public class Person
{
    public string FirstName;
    public string LastName;
}
</code></pre>

<p>Then, on the SQL Server, the corresponding table rows would be more like:</p>

<pre><code>public class Person
{
    public int CustomerID;
    public string FirstName;
    public string LastName;
}
</code></pre>

<p>The purpose of the extra <code>CustomerID</code> column is, of course, because there are multiple rows stored from many different customers, and we want to be able to only select one customer's information at any particular time.</p>

<p>I am looking at MS Sync technology as well as Merge Replication, but initially, these are my impressions:</p>

<p>Sync does not appear to be suitable as it appears to be one-way. It would seem to be more attuned to having some central store of information that you would like to have synchronized out to one or more customers who are in effect slaves of the master. The synchronization that I need is definitely a two-way process.</p>

<p>Merge Replication appears that it <em>may</em> be able to accomplish this, but I am not sure if it will deal with this particular situation, where all of the customer information is stored in a single table and it would be necessary to filter out the rows to be involved in the particular synchronization before performing it.</p>

<p>Our current implementation is not performant. It involves the SQL Server, on top of which we load EF, on top of which we load reflection, on top of which we load dynamic. With this stack it is taking anywhere from half a second to a full second for every row that gets added to any table.</p>

<p>Would anyone have any advice on a better way to handle this situation?</p>
","<c#><sql-server><database><data-synchronization>","2019-12-07 15:55:19","147","0","1","59227908","<p>I would just optimize your custom solution.  It doesn't sound like it's a complicated-enough scenario to benefit from a full Sync/Repl framework.  Plus you should be planning to move from CE to SqlLite, or otherwise move off of CE.</p>
"
"59007660","Passing messages between 2 parallelly running scripts(python and C++)","<p>I currently have 2 scripts running parallelly. The first one(C++) saves data and the second one(python) reads and processes it. Currently I am using a .txt file for this purpose. But that is not very efficient and is hard to synchronize. </p>

<p>I need a way to pass messages between a C++ script and python script synchronously? A Queue kind of structure would be ideal.</p>
","<python><c++><ubuntu><message-queue><data-synchronization>","2019-11-23 12:33:50","104","-1","1","59007971","<p>This post goes into detail and I think will answer your question</p>

<p><a href=""https://stackoverflow.com/questions/46619531/share-memory-between-c-c-and-python"">Share memory between C/C++ and Python</a></p>
"
"58970397","Remove extra spaces from Arabic field","<p>How do I remove trailing, leading and multiple spaces between the Arabic words. The spaces in Arabic fields are not like the space which we have in English language. In Arabic spaces will be some elongated characters different from the blank space characters that we use in English. Please suggest me a way to validate the Arabic fields and remove extra spaces form the fields in Informatica Developer perspective.</p>

<p>Thanks
Shaikh</p>
","<informatica><informatica-powercenter><data-quality>","2019-11-21 08:29:44","515","0","2","58996958","<p>Use a java transformation and split your string containing the arabic spacing on said arabic spaces:</p>

<pre class=""lang-java prettyprint-override""><code>String[] myArray = myString.split("" "");//in between quotes replace the space with Arabic space
</code></pre>

<p>Then iterate over the array concatenating all of the strings in the array</p>

<pre class=""lang-java prettyprint-override""><code>String cleanString = new String;
cleanString = """"; //create an empty string
for(String str : myArray){
    if(str.equals("" "")) //again replace the space with whatever 
        continue; //skip it if it's a space

    cleanString += str;//concatonate any string that isnt a space
}
</code></pre>
"
"58970397","Remove extra spaces from Arabic field","<p>How do I remove trailing, leading and multiple spaces between the Arabic words. The spaces in Arabic fields are not like the space which we have in English language. In Arabic spaces will be some elongated characters different from the blank space characters that we use in English. Please suggest me a way to validate the Arabic fields and remove extra spaces form the fields in Informatica Developer perspective.</p>

<p>Thanks
Shaikh</p>
","<informatica><informatica-powercenter><data-quality>","2019-11-21 08:29:44","515","0","2","59052673","<p>Check the character code and use <code>REPLACECHR</code> with a <code>CHR</code> function, like</p>

<pre><code>REPLACECHR(0, input_Port_Name, CHR(&lt;the_space_character_code&gt;), '')
</code></pre>
"
"58613288","Unable to Call a Snowflake procedure with a Data Governance tool Called Ataccama","<p>I am using a data governance too called ATaccama and I have created a SP in Snowflake but I am getting below error when I am calling the SP from Ataccama JDBC sql execute component. However, I am using latest JDBC driver for SNOWFLAKE : snowflake-jdbc-3.9.2. Your help is much appreciated!</p>

<pre><code>ERROR:
net.snowflake.client.jdbc.SnowflakeSQLException: Statement 'call PII.kc11_search...' cannot be executed using current API.
at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.executeBatch(SnowflakePreparedStatementV1.java:937)
at com.ataccama.dqc.internal.commons.sql.AtcPreparedStatement.executeBatch(AtcPreparedStatement.java:307)
at com.ataccama.dqc.io.jdbc.writer.FlatCommitStrategy.executeBatch(FlatCommitStrategy.java:115)
at com.ataccama.dqc.io.jdbc.writer.BatchWriter.executeBatch(BatchWriter.java:54)
at com.ataccama.dqc.io.jdbc.writer.BatchWriter.flush(BatchWriter.java:98)
at com.ataccama.dqc.tasks.jdbc.execute.SqlExecuteQueryProcessor.finish(SqlExecuteQueryProcessor.java:106)
at com.ataccama.dqc.tasks.jdbc.execute.SQLStepInstanceBase$SimpleProcessingStrategy.run(SQLStepInstanceBase.java:249)
at com.ataccama.dqc.tasks.jdbc.execute.SQLStepInstanceBase.run(SQLStepInstanceBase.java:136)
at com.ataccama.dqc.processor.internal.runner.ComplexStepNode.runNode(ComplexStepNode.java:69)
at com.ataccama.dqc.processor.internal.runner.RunnableNode.run(RunnableNode.java:28)
at com.ataccama.dqc.commons.threads.AsyncExecutor$RunningTask.run(AsyncExecutor.java:131)at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>[NOTE:  using - Call PII. Kc11_seach().]</p>

<p>Any ideas or otherwise beneficial recommendations?</p>
","<sql><jdbc><snowflake-cloud-data-platform><data-governance>","2019-10-29 18:17:33","737","0","1","59809000","<p>The Snowflake JDBC driver <a href=""https://github.com/snowflakedb/snowflake-jdbc/blame/v3.8.2/src/main/java/net/snowflake/client/jdbc/SnowflakePreparedStatementV1.java#L929-L937"" rel=""nofollow noreferrer"">does not currently support</a> executing any SQL statements that may return a value as part of its <code>PreparedStatement::executeBatch(â€¦)</code> call that Attacama is using under the hood. This is because the batch call handling cannot process returned values from one or more of their multiple executed SQL queries.</p>

<p>Since a <code>CALL PROCEDURE()</code> <a href=""https://docs.snowflake.net/manuals/sql-reference/sql/create-procedure.html"" rel=""nofollow noreferrer"">may return a value</a>, the driver considers it as a statement that may generate a result and throws the unsupported statement error.</p>

<p>To call a procedure via Attacama, use a different component that does not use JDBC's <code>PreparedStatement::executeBatch(â€¦)</code> calls underneath.</p>
"
"58541932","How to enable asynchronous replication in mongodb?","<p>I have a mongodb cluster with 3 nodes.</p>

<p>How can i configure the cluster for asynchronous replication in this cluster? Is there any global configuration which provides this functionality?</p>
","<mongodb><asynchronous><replication><data-synchronization>","2019-10-24 12:58:43","367","0","1","60783819","<p>Replication is already asynchronous, unless you specifically use a write concern that requires waiting on replication - Agreed with this @joe</p>

<p>Mongo PRIMARY node will record all its transactions in an opslogs [Operations log] &amp; replaying with its SECONDARY nodes, so basically its synchronise only. </p>
"
"58490234","how to read xml file in IDQ","<p>I have a requirement to read XML file in IDQ. I am new to idq and not sure how to read xml file as there is no direct transformation available like informatica power center. 
could anyone please explain in brief how to achieve it. </p>

<p>Thanks in advance </p>
","<informatica><data-quality>","2019-10-21 16:13:09","624","-2","1","58490639","<p><strong>No, IDQ can not read XML data sources</strong></p>

<p>If you have PowerCenter you can integrate your DQ mapping and pass the XML data to the IDQ mapplet but to process a XML source it will need some intermediate step to read the data prior to processing in IDQ.</p>
"
"58474293","Pandas data cleansing to assign records below a row are assigned a particular value","<p>In the below code, I'm using simple data manipulation to split the columns and remove unnecessary characters.</p>

<pre><code>input_uni_towns = pd.read_fwf(""university_towns.txt"", sep = "" "", header = None)
uni_towns = input_uni_towns.rename(columns={0: ""Raw_data""})
uni_towns['Cleaned'] = uni_towns[""Raw_data""].replace(regex=True,to_replace=[r'\[[^()]*\]'],value=r'')
uni_towns[[""State"",""University""]] = uni_towns.Cleaned.str.split(""("",n=1,expand=True) 
uni_towns[""University""] = uni_towns[""University""].str.rstrip(')')
cleaned_uni_towns = uni_towns[[""State"",""University""]]
</code></pre>

<p>After this above step, I want to assign State to records whose above record has None assigned to it.
For Ex: Auburn (Auburn University)  current State is Auburn, but I want this to be updated to Alabama and similarly for records below Alabama till code encounters next State i.e. Alaska</p>

<p>This is the current output
<a href=""https://i.stack.imgur.com/rFAu0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rFAu0.jpg"" alt=""Current Output""></a></p>

<p>This is the expected output</p>

<p><a href=""https://i.stack.imgur.com/nzNcN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nzNcN.png"" alt=""Expected  Output""></a></p>
","<python><pandas><data-cleaning>","2019-10-20 15:05:44","31","0","1","58474545","<p>You can keep put <code>NaN</code> in State when University is not <code>None</code> (i.e. that's a row with a university), then fill the <code>NaN</code> in State with the most recent non-<code>NaN</code> value.</p>

<pre><code>df = pd.DataFrame({'Cleaned': ['Alabama', 'Auburn (Auburn University)', 'Alaska']})
df[['State', 'University']] = df.Cleaned.str.split('(', n = 1, expand = True)
df.University = df.University.str.rstrip(')')

df.State = np.where(df.University.map(lambda u: u is None), df.State, np.nan)
df.State = df.State.fillna(method = 'ffill')
</code></pre>
"
"58413723","How to check for data quality in SSIS?","<p>When converting data during the transfer,I move all rejected (i.e. failed) conversions into a reject table. However, I only get an entry for the FIRST error:</p>

<p>Example source data:</p>

<pre><code>Name | Salary  | Zipcode
------------------------
Paul | 12000   | 90210
Ringo| 5000    | 12345
Peter| hundred | London
Tina | 12345   | London
</code></pre>

<p>For row 3 I only get an error that the 2nd column is wrong, the 3rd column is not listed, ergo my rejects table only has 2 entries.</p>

<p>Is there a way to get a listing for each error? </p>

<p>Currently I would simply take the rejects table and run a script that does what I want over it, thus getting a ""proper"" logfile, though I would prefer to have something within SSIS.</p>

<p>What would be the best way to do this?</p>
","<ssis><data-quality>","2019-10-16 12:49:12","142","0","1","58417071","<p>I would look into doing more explicit validation and conversion in an asynchronous Script Transformation.</p>

<p>You would have two outputs, one for ""OK"", and one for ""Error"".</p>

<p>When each row comes in, you can perform validation methods on both fields (e.g., from the look of your data you'd probably be doing something like <code>Int32.TryParse</code> (C#)), and for each invalid field you encounter, you could write that field into an ""Error"" output field (you could also write the whole row, as well as a message to say which field has the error, if you wanted to see it like that).</p>

<p>If the data in the row is OK, then just write the single converted version of the row into the ""OK"" output.</p>
"
"58217020","A single source of truth for API request/response shapes, including JSON schema, TypeScript types, and runtime validators?","<p>I'm writing a typical Node.js REST service with endpoints that receive JSON input (as POST bodies) and return JSON responses.</p>

<p>I want these three things:</p>

<ol>
<li>JSON schemas defining the shapes of request bodies and response bodies for my endpoints. (Or Swagger files, or anything suitable for defining the contract for clients.)</li>
<li>TypeScript types/interfaces reflecting the JSON schemas perfectly.</li>
<li>Runtime validation functions that I can call from my handlers to ensure an object is in the correct shape. (And once an input object has been validated, my TypeScript code should know its interface.)</li>
</ol>

<p>But I want a single source of truth, to avoid these three components going out of sync. So I only want to hand-maintain <strong>one</strong> of them (either the JSON Schemas or the TypeScript typings). The other two components should be generated from the hand-written one, and they should not be committed to version control.</p>

<p>I feel like this must be a fairly common set of requirements nowadays, but I can't find much online about it. Perhaps I'm using the wrong search terms to research it. Is there any framework or conventional pattern that addresses this goal, i.e. the goal of defining a JSON API with strict validation of input, standardised schema documentation, and all the convenience of fully typed objects in TypeScript?</p>
","<node.js><typescript><jsonschema>","2019-10-03 10:17:40","1474","6","3","58217678","<p>I am assuming you are using TypeScript in the frontend and backend.</p>

<p><strong>Custom Type Reuse</strong></p>

<p>Let's say we have a project repository containing three main folders: <code>client</code>, <code>server</code> and <code>shared</code>.</p>

<p>In <code>shared</code> you have a <code>Types.ts</code> file which contains all your custom type defintions.
For example:</p>

<pre><code>export interface Person {
  name: string;
  age: number;
}
</code></pre>

<p>Now you can import these types from any sub-folder like <code>.\client\src\components\SomeComponent.ts</code> or server <code>.\server\src\api\SomeRoute.ts</code>
with:</p>

<pre><code>import { Person } from `..\..\..\..\shared\Types.ts`;
</code></pre>

<p>So in this way you have one place for type definitions that can be used project wide in server and client.</p>

<p><strong>JSON Model and Documentation Generation</strong></p>

<p>You could use <a href=""https://github.com/microsoft/tsdoc"" rel=""nofollow noreferrer"">ts-doc</a> comments for documenting your api and <a href=""https://github.com/SimplrJS/ts-docs-gen"" rel=""nofollow noreferrer"">ts-docs-gen</a> to generate a markdown document.</p>

<p>An alternative would be <a href=""https://github.com/TypeStrong/typedoc"" rel=""nofollow noreferrer"">typedoc</a>. TypeDoc converts comments in TypeScript source code into rendered HTML documentation or a JSON model. You can configure it in a way that only the relevant api files are taken into account.</p>

<p><strong>Runtime Validation</strong></p>

<p>Regarding runtime validation of input and output you can either write your own validation function for every type which is is explained by stereobooster in this article: <a href=""https://dev.to/stereobooster/pragmatic-types-io-validation-or-how-to-handle-json-based-apis-in-statically-typed-language-1d9m"" rel=""nofollow noreferrer"">pragmatic-types-io-validation-or-how-to-handle-json-based-apis-in-statically-typed-language</a>. Or you use a library like <a href=""https://github.com/gcanti/io-ts"" rel=""nofollow noreferrer"">io-ts</a>.</p>

<p>There Probably isn't one fits it all tool, because it heavily depends on the exact requirements what you want to accomplish. So composing your toolset of different tools should guide you into the right direction. Feel free to post an answer yourself, if you have found a setup that works well for you.</p>
"
"58217020","A single source of truth for API request/response shapes, including JSON schema, TypeScript types, and runtime validators?","<p>I'm writing a typical Node.js REST service with endpoints that receive JSON input (as POST bodies) and return JSON responses.</p>

<p>I want these three things:</p>

<ol>
<li>JSON schemas defining the shapes of request bodies and response bodies for my endpoints. (Or Swagger files, or anything suitable for defining the contract for clients.)</li>
<li>TypeScript types/interfaces reflecting the JSON schemas perfectly.</li>
<li>Runtime validation functions that I can call from my handlers to ensure an object is in the correct shape. (And once an input object has been validated, my TypeScript code should know its interface.)</li>
</ol>

<p>But I want a single source of truth, to avoid these three components going out of sync. So I only want to hand-maintain <strong>one</strong> of them (either the JSON Schemas or the TypeScript typings). The other two components should be generated from the hand-written one, and they should not be committed to version control.</p>

<p>I feel like this must be a fairly common set of requirements nowadays, but I can't find much online about it. Perhaps I'm using the wrong search terms to research it. Is there any framework or conventional pattern that addresses this goal, i.e. the goal of defining a JSON API with strict validation of input, standardised schema documentation, and all the convenience of fully typed objects in TypeScript?</p>
","<node.js><typescript><jsonschema>","2019-10-03 10:17:40","1474","6","3","70215043","<p>I like the idea of a website where JSON Schema Definitions (JSD) live for your project.
This can be internal (intranet) or external (Internet) and the schemas are there to reference by URI. (see <a href=""https://www.schemastore.org/json/"" rel=""nofollow noreferrer"">https://www.schemastore.org/json/</a> for examples of what this can looks like, but at your own website URL).
You can also reasearch &quot;schema stores&quot; (like a database for your schemas, but that too hides it a bit and ties you to a pattern.
This pattern makes sense as you can validate JSON anywhere (UI, middle-tier, backend, just before repository insert, etc).
I see the issue with protobuf, Typecript models, contracts in NuGet packages, etc. is you force codebase adhesions, and get &quot;too married to a specific stack&quot;.<br />
Not agnostic enough. This problem of client/server contracts has been going on and re-hashed for ages (SOAP/WSDL, Corba/IDL, etc) and each stack, even Protobuf has you married to their stack.<br />
So one could argue the same about the data definitions being in JSD's but I contend, like Javascript, JSON is the most common transport mechanism for business data (overtaking XML some time ago).
So I like the idea of a pattern for projects that have all of the JSON data defined through JSD schemas.   The JSD schema IS the contract.
You can tie JSD's to Kafka topics (as opposed to their Avro format), you can tie JSD's to Mongo, or you can just use them in any JSON validator library your language uses (like Newtonsoft in the .NET world)
The JSD is the holy grail of language-neutral contracts.   I will concede here that binary data with gRPC and Protobuf has speed advantages, and should be considered.  I will also state that I see 99 percent of data is JSON body business data and can be done using this pattern.
Recap:   Build your JSD, put it in a place (website location) that can be reference by your code for validation at any layer of your software system.</p>
"
"58217020","A single source of truth for API request/response shapes, including JSON schema, TypeScript types, and runtime validators?","<p>I'm writing a typical Node.js REST service with endpoints that receive JSON input (as POST bodies) and return JSON responses.</p>

<p>I want these three things:</p>

<ol>
<li>JSON schemas defining the shapes of request bodies and response bodies for my endpoints. (Or Swagger files, or anything suitable for defining the contract for clients.)</li>
<li>TypeScript types/interfaces reflecting the JSON schemas perfectly.</li>
<li>Runtime validation functions that I can call from my handlers to ensure an object is in the correct shape. (And once an input object has been validated, my TypeScript code should know its interface.)</li>
</ol>

<p>But I want a single source of truth, to avoid these three components going out of sync. So I only want to hand-maintain <strong>one</strong> of them (either the JSON Schemas or the TypeScript typings). The other two components should be generated from the hand-written one, and they should not be committed to version control.</p>

<p>I feel like this must be a fairly common set of requirements nowadays, but I can't find much online about it. Perhaps I'm using the wrong search terms to research it. Is there any framework or conventional pattern that addresses this goal, i.e. the goal of defining a JSON API with strict validation of input, standardised schema documentation, and all the convenience of fully typed objects in TypeScript?</p>
","<node.js><typescript><jsonschema>","2019-10-03 10:17:40","1474","6","3","70215476","<p>I think it can be good idea to take a look at Google RPC protocol.</p>
<p><a href=""https://grpc.io/docs/languages/node/basics/"" rel=""nofollow noreferrer"">https://grpc.io/docs/languages/node/basics/</a></p>
<p>You describe all protocol interactions (aka object structures, responses, requests, etc) in single <code>.proto</code> file, and provided utilities can generate both client and server codes for nodejs, client side javascript, other programming languages.</p>
"
"58022752","How to check server connection is available or not with internet connection check mobile in android?","<p>I want to check server connection to know if its available or not.i want to implement auto synch data when server connection is available or not with internet connection check phone.</p>
","<android><json><server><httpurlconnection><data-synchronization>","2019-09-20 06:19:04","97","0","1","58022855","<p>You need to go for workmanager for this</p>

<p>References :
1)  <a href=""https://developer.android.com/topic/libraries/architecture/workmanager/basics"" rel=""nofollow noreferrer"">enter link description here</a></p>

<p><a href=""https://developer.android.com/topic/libraries/architecture/workmanager"" rel=""nofollow noreferrer"">enter link description here</a></p>
"
"57985908","Error in Syncing ALM QC (12.00) with Visual Studio Team Foundation Server 2015 using ALM Synchronizer (12.60)","<p>I have used ALM Synchronizer to connect to TFS (2015).  I have established the initial connection and am trying to obtain the TFS schema. After starting the ""Run integrity check"" operation I am receiving the following error:</p>

<p>Fetching endpoint schema:</p>

<blockquote>
  <p>buildEntitySchema: disconnected Error converting value
  ""System.Security.Principal.WindowsIdentity;S-1-5-21-220523388-1606980848-854245398-45204""
  to type 'Microsoft.VisualStudio.Services.Identity.IdentityDescriptor'.
  Path 'authenticatedUser.descriptor', line 1, position 168.</p>
</blockquote>

<p>At this point, I can see Micro Focus ALM Defect Schema in field mapping at Sychronizer client but TFS bug Schema is empty.</p>

<p>Assistance in interpreting this error would be greatly appreciated. </p>
","<testing><tfs-2015><alm><data-synchronization><hp-quality-center>","2019-09-18 05:54:30","67","1","1","58256350","<p>The synchronizer installation guide mentions a step to copy files from the specified location (C:\Program Files\Common Files\Microsoft Shared\Team Foundation Server\14.0) to the adapters directory for HP Synchronizer. 
This step is inaccurate as the necessary files are not present in the listed path.</p>

<p>I downloaded the  TFS2015.ZIP file from the link below:</p>

<p><a href=""https://softwaresupport.softwaregrp.com/doc/KM02568091"" rel=""nofollow noreferrer"">https://softwaresupport.softwaregrp.com/doc/KM02568091</a></p>

<p>and put the files in the path (C:\Program Files\HP\HP ALM Synchronizer\ adapters\net2015).
Then I registered the ""Register-TFS-Adapter.exe"" file and restarted the synchronizer service.
The problem solved this way.</p>
"
"57888173","Meteor - using snychronised non-persistent / in-memory MongoDB on the server","<p>in a Meteor app, having real-time reactive updates between all connected clients is achieved with writing in collections, publishing and subscribing the right data. In normal case this means also database writes.</p>

<p>But what if I would like to sync particular data which does not need to be persistent and I would like to save the overhead of writing in the database ? Is it possible to use mini-mongo or other in-memory caching on the server by still preserving DDP synchronisation to all clients ?</p>

<h2>Example</h2>

<p>In my app I have a multiple <em>collapsed</em>  threads and I want to show, which users currently expanded particular thread</p>

<blockquote>
  <p>Viewed by: Mike, Johny, Steven ...</p>
</blockquote>

<p>I can store the information in the <em>threads</em> collection or make make a separate <em>viewers</em> collection and publish the information to the clients. But there is actually no meaning in making this information persistent an having the overhead of database writes.</p>

<p>I am confused by the <a href=""https://docs.meteor.com/api/collections.html"" rel=""nofollow noreferrer"">collections documentation</a>. which states:</p>

<blockquote>
  <p><strong>OPTIONS</strong>
  <em>connection Object</em>
  The server connection that will manage this collection. Uses the default connection if not specified. Pass the return value of calling DDP.connect to specify a different server. Pass null to specify no connection.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>... when you pass a <em>name</em>, hereâ€™s what happens:</p>
  
  <p>...
  On the client (<strong>and on the server if you specify a connection</strong>), a Minimongo instance is created.</p>
</blockquote>

<p>But If I create a new collection and pass the option object with <code>conneciton: null</code></p>

<pre><code>// Creates a new Mongo collections and exports it
export const Presentations = new Mongo.Collection('presentations', {connection: null});

/**
 * Publications
 */
if (Meteor.isServer) {
    // This code only runs on the server
    Meteor.publish(PRESENTATION_BY_MAP_ID, (mapId) =&gt; {
        check(mapId, nonEmptyString);
        return Presentations.find({ matchingMapId: mapId });
    });
}
</code></pre>

<p>no data is being published to the clients.</p>
","<mongodb><meteor><data-synchronization><minimongo>","2019-09-11 11:30:09","246","2","2","57890416","<p>TLDR: it's not possible.</p>

<p>There is no magic in Meteor that allow data being synced between clients while the data doesn't transit by the MongoDB database. The whole sync process through publications and subscriptions is triggered by MongoDB writes. Hence, if you don't write to database, you cannot sync data between clients (using the native pub/sub system available in Meteor).</p>
"
"57888173","Meteor - using snychronised non-persistent / in-memory MongoDB on the server","<p>in a Meteor app, having real-time reactive updates between all connected clients is achieved with writing in collections, publishing and subscribing the right data. In normal case this means also database writes.</p>

<p>But what if I would like to sync particular data which does not need to be persistent and I would like to save the overhead of writing in the database ? Is it possible to use mini-mongo or other in-memory caching on the server by still preserving DDP synchronisation to all clients ?</p>

<h2>Example</h2>

<p>In my app I have a multiple <em>collapsed</em>  threads and I want to show, which users currently expanded particular thread</p>

<blockquote>
  <p>Viewed by: Mike, Johny, Steven ...</p>
</blockquote>

<p>I can store the information in the <em>threads</em> collection or make make a separate <em>viewers</em> collection and publish the information to the clients. But there is actually no meaning in making this information persistent an having the overhead of database writes.</p>

<p>I am confused by the <a href=""https://docs.meteor.com/api/collections.html"" rel=""nofollow noreferrer"">collections documentation</a>. which states:</p>

<blockquote>
  <p><strong>OPTIONS</strong>
  <em>connection Object</em>
  The server connection that will manage this collection. Uses the default connection if not specified. Pass the return value of calling DDP.connect to specify a different server. Pass null to specify no connection.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>... when you pass a <em>name</em>, hereâ€™s what happens:</p>
  
  <p>...
  On the client (<strong>and on the server if you specify a connection</strong>), a Minimongo instance is created.</p>
</blockquote>

<p>But If I create a new collection and pass the option object with <code>conneciton: null</code></p>

<pre><code>// Creates a new Mongo collections and exports it
export const Presentations = new Mongo.Collection('presentations', {connection: null});

/**
 * Publications
 */
if (Meteor.isServer) {
    // This code only runs on the server
    Meteor.publish(PRESENTATION_BY_MAP_ID, (mapId) =&gt; {
        check(mapId, nonEmptyString);
        return Presentations.find({ matchingMapId: mapId });
    });
}
</code></pre>

<p>no data is being published to the clients.</p>
","<mongodb><meteor><data-synchronization><minimongo>","2019-09-11 11:30:09","246","2","2","57995946","<p>After countless hours of trying everything possible I found a way to what I wanted:</p>

<pre><code>export const Presentations = new Mongo.Collection('presentations', Meteor.isServer ? {connection: null} : {});
</code></pre>

<p>I checked the MongoDb and no <em>presentations</em> collection is being created. Also, n every server-restart the collection is empty. There is a small downside on the client, even the collectionHanlde.ready() is truthy the <strong>findOne()</strong> first returns <em>undefined</em> and is being synced afterwards.</p>

<p>I don't know if this is the right/preferable way, but it was the only one working for me so far. I tried to leave <code>{connection: null}</code> in the client code, but wasn't able to achieve any sync even though I implemented the <code>added/changed/removed</code> methods.</p>

<p>Sadly, I wasn't able to get any further help even in the meteor forum <a href=""https://forums.meteor.com/t/using-non-persistent-mongo/38634"" rel=""nofollow noreferrer"">here</a> and <a href=""https://forums.meteor.com/t/pub-sub-data-without-mongodb-is-not-working/29262/10"" rel=""nofollow noreferrer"">here</a></p>
"
"57884849","Android offline mode (syncing data when app goes online)","<p>I want to gather data in online/offline mode, and when the internet connection is dropped data should be store in locally, and once the internet connection is back data should sync/push to the server.</p>

<p><strong>What library or technique recommended for achieve that?</strong></p>
","<java><android><api><data-synchronization><offline-mode>","2019-09-11 08:12:22","796","0","1","57885015","<p>Try to store data in offline using Room data base or Sqlite. after storing the data check internet connection continuously, if internet has connected upload that data to your server and drop local data.   </p>
"
"57862134","How to add a data quality check utility in awe glue.?","<p>How to add a job that just checks for data quality like null, correct data type etc in aws glue</p>
","<validation><aws-glue><data-quality>","2019-09-09 23:02:35","655","0","2","57875629","<p>You can write pyspark glue job for quality check.
You can also use <a href=""https://github.com/awslabs/deequ"" rel=""nofollow noreferrer"">AWS Deequ</a> library to do it but it is in scala.</p>
"
"57862134","How to add a data quality check utility in awe glue.?","<p>How to add a job that just checks for data quality like null, correct data type etc in aws glue</p>
","<validation><aws-glue><data-quality>","2019-09-09 23:02:35","655","0","2","67490180","<p>you can check pyDeequ library too. It enables you to implement python code to DQ checks.</p>
"
"57559683","How to manage polygon data synchronization from backbone with agm-core?","<p>The goal of my component is to get points (lat, lng) coming from the database to draw a polygon on a Google map. The Google map is correctly implemented and displays with drawing manager ok.</p>

<p>My problem is much more a problem of declaration.</p>

<p>I have the following code for the map:</p>

<pre class=""lang-js prettyprint-override""><code>onMapReady(map) {
  this.initDrawingManager(map);
  this.polygon = new google.maps.Polygon({
    paths: this.pointsArray,
    strokeColor: '#FF0000',
    strokeOpacity: 0.8,
    strokeWeight: 2,
    fillColor: '#FF0000',
    fillOpacity: 0.35,
    editable: true,
    draggable: true,
  });
}
</code></pre>

<p>On <code>ngOnInit()</code>, I am getting all the points in the following format:</p>

<p><a href=""https://i.stack.imgur.com/BRHFq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BRHFq.png"" alt=""enter image description here""></a></p>

<p>I'm getting it using this function set at the bottom of the component code:</p>

<pre class=""lang-js prettyprint-override""><code>getSitePoints() {
  this.siteAPI.getPoints(this.route.snapshot.paramMap.get('id'))
    .subscribe((response) =&gt; {
      this.sitePoints = response;
      const pointsArray = [];
      this.sitePoints.map((item) =&gt; {
        pointsArray.push(item.Location);
        console.log(pointsArray);
        console.log('item:' + item.Location);
      });
    });
}
</code></pre>

<p>As the polygon drawing is done on the <code>onMapReady</code> event and as my Array is populated after a subscribe, in a function, polygon is not showing. Like if it was empty when received by the drawing Manager. No clue how to check this.</p>

<p>Also, declarations are:</p>

<pre class=""lang-js prettyprint-override""><code>pointsArray = [];
sitePoints: any;
</code></pre>

<p>Any help would be really appreciated.
Thank you in advance for your help.</p>

<p>Best Regards</p>
","<angular><google-maps>","2019-08-19 15:22:39","282","0","2","57560164","<p>Looks like you're not saving the data in the class property.</p>

<pre><code>getSitePoints() {
  this.siteAPI.getPoints(this.route.snapshot.paramMap.get('id'))
    .subscribe((response) =&gt; {
      this.sitePoints = response;
      //const pointsArray = []; &lt;--- Remove
      this.sitePoints.map((item) =&gt; {
        //pointsArray.push(item.Location); &lt;-- issue
        this.pointsArray.push(item.Location);
        console.log(this.pointsArray);
        console.log('item:' + item.Location);
      });
    });
}
</code></pre>
"
"57559683","How to manage polygon data synchronization from backbone with agm-core?","<p>The goal of my component is to get points (lat, lng) coming from the database to draw a polygon on a Google map. The Google map is correctly implemented and displays with drawing manager ok.</p>

<p>My problem is much more a problem of declaration.</p>

<p>I have the following code for the map:</p>

<pre class=""lang-js prettyprint-override""><code>onMapReady(map) {
  this.initDrawingManager(map);
  this.polygon = new google.maps.Polygon({
    paths: this.pointsArray,
    strokeColor: '#FF0000',
    strokeOpacity: 0.8,
    strokeWeight: 2,
    fillColor: '#FF0000',
    fillOpacity: 0.35,
    editable: true,
    draggable: true,
  });
}
</code></pre>

<p>On <code>ngOnInit()</code>, I am getting all the points in the following format:</p>

<p><a href=""https://i.stack.imgur.com/BRHFq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BRHFq.png"" alt=""enter image description here""></a></p>

<p>I'm getting it using this function set at the bottom of the component code:</p>

<pre class=""lang-js prettyprint-override""><code>getSitePoints() {
  this.siteAPI.getPoints(this.route.snapshot.paramMap.get('id'))
    .subscribe((response) =&gt; {
      this.sitePoints = response;
      const pointsArray = [];
      this.sitePoints.map((item) =&gt; {
        pointsArray.push(item.Location);
        console.log(pointsArray);
        console.log('item:' + item.Location);
      });
    });
}
</code></pre>

<p>As the polygon drawing is done on the <code>onMapReady</code> event and as my Array is populated after a subscribe, in a function, polygon is not showing. Like if it was empty when received by the drawing Manager. No clue how to check this.</p>

<p>Also, declarations are:</p>

<pre class=""lang-js prettyprint-override""><code>pointsArray = [];
sitePoints: any;
</code></pre>

<p>Any help would be really appreciated.
Thank you in advance for your help.</p>

<p>Best Regards</p>
","<angular><google-maps>","2019-08-19 15:22:39","282","0","2","57576838","<p>I finally succeeded fixing my problem. Thanks to everyone for your support.</p>

<p>Considering that the whole angular component lifeCycle is re-run on browser refresh, manipulating thing in different lifecycle parts was not the solution.
Playing with lifeCycles is not a way to manage asynchronous data.</p>

<p>The solution was to get the points coming from DB on a subscribe() and then draw the polygon to ensure data is received before. </p>

<p>Like below : </p>

<pre><code>this.pointsArray = [];
    this.siteAPI.getPoints(this.route.snapshot.paramMap.get('id'))
        .subscribe(
          response =&gt; {
          this.sitePoints = response;
          this.pointsArray = [];
          this.sitePoints.map((item) =&gt; {
            this.pointsArray.push(item.Location);
          });
        }
        ,
        err =&gt; {},
        () =&gt; {
        this.polygon = new google.maps.Polygon({
          paths: this.pointsArray,
          strokeColor: '#FF0000',
          strokeOpacity: 0.8,
          strokeWeight: 2,
          fillColor: '#FF0000',
          fillOpacity: 0.35,
          editable: true,
          draggable: true,
          });
        // Set polygon to map
        this.polygon.setMap(map);
        }
        );}
</code></pre>
"
"57477925","how find rows where a particular column has decimal numbers using pandas?","<p>I am writing a data quality script using pandas, where the script would be checking certain conditions on each column</p>

<p>At the moment i need to find out the rows that don't have a decimal or an actual number in a a particular column. I am able to find the numbers if its a whole number, but the methods I have seen so far ie <code>isdigit() , isnumeric(), isdecimal()</code> etc fail to correctly identify when the number is a decimal number. eg: 2.5, 0.1245 etc.</p>

<p>Following is some sample code &amp; data:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([
    [np.nan, 'foo', 0],
    [1, '', 1],
    [-1.387326, np.nan, 2],
    [0.814772, ' baz', ' '],     
    [""a"", '      ', 4],
    [""  "",  'foo qux ', '  '],         
], columns='A B C'.split(),dtype=str)

&gt;&gt;&gt; df
    A   B   C
0   NaN foo 0
1   1       1
2   -1.387326   NaN 2
3   0.814772    baz 
4   a       4
5       foo qux 

&gt;&gt;&gt; df['A']
0          NaN
1            1
2    -1.387326
3     0.814772
4            a
5             
Name: A, dtype: object
</code></pre>

<p>The following method all fails to identify the decimal numbers</p>

<pre><code>df['A'].fillna('').str.isdigit()
df['A'].fillna('').str.isnumeric()
df['A'].fillna('').str.isdecimal()

0    False
1     True
2    False
3    False
4    False
5    False
Name: A, dtype: bool
</code></pre>

<p>So when i try the following I only get 1 row</p>

<pre><code>&gt;&gt;&gt; df[df['A'].fillna('').str.isdecimal()]
    A   B   C
1   1       1
</code></pre>

<p>NB: I am using <code>dtype=str</code> to get the data wihtout pandas interpreting/changing the values of the dtypes. The actual data could have spaces in column A, I will trim that out using replace(), I have kept the code simple here so as not to confuse things.</p>
","<python><pandas><data-quality>","2019-08-13 12:40:36","3705","5","2","57477960","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html"" rel=""nofollow noreferrer""><code>to_numeric</code></a> with <code>errors='coerce'</code> for non numeric to <code>NaN</code>s and then test by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.notna.html"" rel=""nofollow noreferrer""><code>Series.notna</code></a>:</p>

<pre><code>print (pd.to_numeric(df['A'], errors='coerce').notna())
0    False
1     True
2     True
3     True
4    False
5    False
Name: A, dtype: bool
</code></pre>

<p>If need return <code>True</code>s for missing values:</p>

<pre><code>print (pd.to_numeric(df['A'], errors='coerce').notna() | df['A'].isna())
0     True
1     True
2     True
3     True
4    False
5    False
Name: A, dtype: bool
</code></pre>

<p>Another solution with custom function:</p>

<pre><code>def test_numeric(x):
    try:
        float(x)
        return True
    except Exception:
        return False

print (df['A'].apply(test_numeric))
0     True
1     True
2     True
3     True
4    False
5    False
Name: A, dtype: bool

print (df['A'].fillna('').apply(test_numeric))
0    False
1     True
2     True
3     True
4    False
5    False
Name: A, dtype: bool
</code></pre>
"
"57477925","how find rows where a particular column has decimal numbers using pandas?","<p>I am writing a data quality script using pandas, where the script would be checking certain conditions on each column</p>

<p>At the moment i need to find out the rows that don't have a decimal or an actual number in a a particular column. I am able to find the numbers if its a whole number, but the methods I have seen so far ie <code>isdigit() , isnumeric(), isdecimal()</code> etc fail to correctly identify when the number is a decimal number. eg: 2.5, 0.1245 etc.</p>

<p>Following is some sample code &amp; data:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([
    [np.nan, 'foo', 0],
    [1, '', 1],
    [-1.387326, np.nan, 2],
    [0.814772, ' baz', ' '],     
    [""a"", '      ', 4],
    [""  "",  'foo qux ', '  '],         
], columns='A B C'.split(),dtype=str)

&gt;&gt;&gt; df
    A   B   C
0   NaN foo 0
1   1       1
2   -1.387326   NaN 2
3   0.814772    baz 
4   a       4
5       foo qux 

&gt;&gt;&gt; df['A']
0          NaN
1            1
2    -1.387326
3     0.814772
4            a
5             
Name: A, dtype: object
</code></pre>

<p>The following method all fails to identify the decimal numbers</p>

<pre><code>df['A'].fillna('').str.isdigit()
df['A'].fillna('').str.isnumeric()
df['A'].fillna('').str.isdecimal()

0    False
1     True
2    False
3    False
4    False
5    False
Name: A, dtype: bool
</code></pre>

<p>So when i try the following I only get 1 row</p>

<pre><code>&gt;&gt;&gt; df[df['A'].fillna('').str.isdecimal()]
    A   B   C
1   1       1
</code></pre>

<p>NB: I am using <code>dtype=str</code> to get the data wihtout pandas interpreting/changing the values of the dtypes. The actual data could have spaces in column A, I will trim that out using replace(), I have kept the code simple here so as not to confuse things.</p>
","<python><pandas><data-quality>","2019-08-13 12:40:36","3705","5","2","57477991","<p>Alternativ, if you want to keep the string structure you can use:</p>

<pre><code>df['A'].str.contains('.')

0    False
1     True
2    False
3    False
4    False
5    False
</code></pre>

<p>The only risk in that case could be that you identify words with <code>.</code>as well..which is not your wish</p>
"
"57463458","Android - Firestore offline data synchronization","<p>The <a href=""https://firebase.google.com/docs/firestore/"" rel=""nofollow noreferrer"">firestore documentation</a> says:</p>

<blockquote>
  <p>Cloud Firestore caches data that your app is actively using, so the app can write, read, listen to, and query data even if the device is offline. <strong>When the device comes back online</strong>, Cloud Firestore synchronizes any local changes back to Cloud Firestore.</p>
</blockquote>

<p>Now, I have tested this and, apparently, device coming back online is not all that it takes for the synchronization to happen.</p>

<p>I tested that by:</p>

<ul>
<li>Turning on ""Airplane mode"" on my emulator</li>
<li>Trying to send a data to firestore*</li>
<li>Closing my app</li>
<li>Turning off ""Airplane mode"" on my emulator </li>
</ul>

<p>* I used <code>FirebaseFirestore.getInstance().collection(""foo"").document().set(bar)</code></p>

<p><br></p>

<p>On the end of this test, the data was not sent to firestore remote database. I needed to open my app again in order for the data to be sent.</p>

<p>But then, I still wasn't sure what exactly was triggering the synchronization. Is it only my app being open or does it needs to have an open socket to firestore?</p>

<p><br></p>

<p>I tested one last thing, which was:</p>

<ul>
<li>Turning on ""Airplane mode"" on my emulator</li>
<li>Trying to send a data to firestore*</li>
<li>Closing my app</li>
<li>Altering my code so the app stays on the splash screen</li>
<li>Turning off ""Airplane mode"" on my emulator</li>
<li>Opened my app</li>
</ul>

<p>On the end of this test, the data was not sent to firestore remote database.</p>

<p>But then, once again, I still wasn't sure what exactly was triggering the synchronization. It's probably the open socket, but if so, does it need to be open on a query related to my collection?</p>

<p><br></p>

<p>My final question is:</p>

<p>What exactly does trigger the synchronization?</p>
","<android><firebase><google-cloud-firestore><broadcastreceiver><android-8.0-oreo>","2019-08-12 14:49:21","582","0","1","57466600","<blockquote>
  <p>Is it only my app being open or does it needs to have an open socket to firestore?</p>
</blockquote>

<p>It depends on what ""my app being open"" means. If it means on foreground, that might not make too much sense since you can have jobs running on background with an open socket to Firestore.</p>

<p><br></p>

<blockquote>
  <p>It's probably the open socket, but if so, does it need to be open on a query related to my collection?</p>
</blockquote>

<p>No, it does not. Any CRUD action on <strong>any part of your database</strong> will make the synchronization starts, whether it is by querying through a collection or fetching, saving, updating or removing a document.</p>
"
"57359033","Multi-Producer Multi-Consumer data synchronization with separated queues","<p>I have the following scenario:</p>

<ol>
<li>a variable number ( greater than three ) of queues (depends on a configuration set in a file)</li>
<li>some of these queues can either be fed with data or not (it depends on the producer that receives data through a network client: the client can be either connected or not during the same session)</li>
<li>These queues are fed at different speeds; so, for example, Queue1 can have 10 objects at a given time whereas another queue Queue2 can have just 3 objects at the same given time </li>
<li>the objects in these queues must be synchronized according to a property that is shared by all of them (an int property constantly increasing named ""SSId"")</li>
<li>the synchronization must happen only for the queues that at a given moment are fed with data (unconnected queues have to be excluded)</li>
<li>when the objects are synchronized they must be pushed to a corresponding output queue used by the related consumer: each producer is associated to a specific consumer</li>
<li>following the previous step each consumer is able to process the enqueued object with the same property value for ""SSId"" at the same time; </li>
<li>So, the final outcome should be a system where the consumers are able to process data (syncronized according to the already mentioned ""SSId"" property) at the same rate even when each producer generate it at different speeds/rates  </li>
</ol>

<p>To give a clearer idea there is a schema representing the flow described in the previous points:
<a href=""https://i.stack.imgur.com/mg8XA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mg8XA.png"" alt=""dataflow mesh""></a></p>

<p>Note that the new items with SSid greater than 100 are not pushed on the consumer queues as there are no corrisponding items in the other queues yet.</p>

<p>Could you suggest an approach for creating this kind of synchronization using either .NET TPL Dataflow or Rx.NET? Until now I've used TPL Dataflow for implementing simple sequential pipelines and I'd like a feedback on how to proceed with this scenario. 
Thanks in advance for any suggestion.</p>
","<task-parallel-library><reactive-programming><system.reactive><tpl-dataflow><rx.net>","2019-08-05 12:55:06","298","4","1","57427416","<p>How about </p>

<ol>
<li>Merging objects from all producers into one observable</li>
<li>Grouping the objects by SSId</li>
<li>Emit the group, when the group size equals the count of producers (by .Buffer())</li>
</ol>

<p>Like this: </p>

<pre class=""lang-cs prettyprint-override""><code>var syncedProducers = 
    // ConnectedProducersEvent ticks an array of connected producers, each time a producer connects or disconnects
    ConnectedProducersEvent
        .SelectMany(producers =&gt; 
            Observable
                .Merge(producers) // Put all objects, from all producers into the same observable
                .GroupBy(@object =&gt; @object.SSId) // Group objects by matching SSId
            .SelectMany(group =&gt; group.Buffer(producers.Length))); // Syncing: Emit the SSId group, when the group count matches the count of connected producers

// Now you can wire syncedProducers to consumers
var consumer1 = 
    syncedProducers
        .Select(x =&gt; x.Where(y =&gt; y.Producer == 1));
</code></pre>

<p><a href=""https://dotnetfiddle.net/YX4hiX"" rel=""nofollow noreferrer"">You can run the example on dotnetfiddle</a></p>
"
"57257202","Rails: How to manage associations between two tables syncing data?","<p>I will be having two Rails applications, one is full fledged and other one a stripped down one. Both of these applications have their own database and they will communicate to each other using APIs.</p>

<p>I have two models like this (in both the apps):</p>

<pre><code>class Scan &lt; ApplicationRecord
  has_many :background_processes, dependent: :destroy      
end

class BackgroundProcess &lt; ApplicationRecord
  belongs_to :scan
end
</code></pre>

<p>Any record saved triggers the sync across the database using web services. Now when the sync across two applications will happen, the dependent model (in this case <code>background_processes</code>) will have different <code>scan_id</code>.</p>

<p>How should we handle the associations in case of data sync? </p>
","<ruby-on-rails><web-services><associations><data-synchronization>","2019-07-29 15:40:59","424","8","2","57303598","<p>I'd recommend using another <strong>indexed</strong> column on the <code>scan</code> model where you could store another id or token that you can use to query the scan record. Maybe call it <code>sync_id</code> or something.</p>

<p>If you take this route, you don't have to worry about the differing <code>scan_id</code>s on the background process records. Just be sure to send the background process records with the scan's JSON body. (Assuming your using JSON as the format for your APIs.)</p>

<p>Here's the general idea.... You would make sure that your sending API service sends the entire scan record with dependent background processes. The receiving API service, then needs to use that scan records <code>sync_id</code> to query for an existing scan record and update it. You'll need to use some sort of unique identifier on the background process records as well to ensure you're not creating duplicates. If need be, create a <code>sync_id</code> on the background processes as well. If the scan record with that id doesn't exist, then create it and the dependent background processes.</p>

<p>Essentially, the sending service's API POST request might look something like this:</p>

<pre><code>{
id: 1,
sync_id: ""sometoken""
... # other record columns
background_process: [
  {
  id: 123,
  ... # other record columns
  }
]
}
</code></pre>

<p>Be sure the <code>sync_id</code> you use is unique. Use something like this in the scan model to generate it on a before_create hook:</p>

<pre><code>  def set_sync_id
    random_token = SecureRandom.urlsafe_base64
    while Scan.where(sync_id: random_token).present? do
      random_token = SecureRandom.urlsafe_base64
    end
    self.sync_id = random_token
  end
</code></pre>
"
"57257202","Rails: How to manage associations between two tables syncing data?","<p>I will be having two Rails applications, one is full fledged and other one a stripped down one. Both of these applications have their own database and they will communicate to each other using APIs.</p>

<p>I have two models like this (in both the apps):</p>

<pre><code>class Scan &lt; ApplicationRecord
  has_many :background_processes, dependent: :destroy      
end

class BackgroundProcess &lt; ApplicationRecord
  belongs_to :scan
end
</code></pre>

<p>Any record saved triggers the sync across the database using web services. Now when the sync across two applications will happen, the dependent model (in this case <code>background_processes</code>) will have different <code>scan_id</code>.</p>

<p>How should we handle the associations in case of data sync? </p>
","<ruby-on-rails><web-services><associations><data-synchronization>","2019-07-29 15:40:59","424","8","2","60867380","<p>You can use <code>.create</code> method call on active-record along with set <code>without_protection</code> to <code>true</code> on sync app. 
But it has its own security risk. </p>

<pre><code>Scan.create({id: &lt;&lt;primary-id&gt;&gt;, attrs....}, without_protection: true)
</code></pre>
"
"57013096","Will IGC allow me to trace where data has been sourced from or how data is being consumed, for any ETL or Data Transformation Tool?","<p>As part of our Governance initiative and regulatory requirement, we need to produce a Lineage (tractability) report, outlining the flow of data into our Warehouse, and the Reports or Services consuming its data.  We are aware that Information Governance Catalog can produce such a report automatically when DataStage is writing data to the Warehouse.  Can Information Governance Catalog do the same when we use SQL Scripts or other tooling to read or write information to our Warehouse?  Can I view a complete Lineage report, that incorporates such different information?</p>

<p>What are the steps within IGC to document or otherwise define the usage of information to support Data Lineage and Regulatory reporting?</p>
","<ibm-infosphere><data-governance><ibm-governance>","2019-07-12 19:34:44","319","1","1","57151631","<p>Yes, while we can automate the production of Lineage (traceability) reports for DataStage, IGC does offer facility to document the flow of data for other data movement scripts, tools or processes.  This will produce the same Lineage reports, that can be used to satisfy needs for compliance, or build confidence and trust in the use or consumption of data.</p>

<p>At it simplest, IGC allows one to draft a Mapping Document.  Essentially a spreadsheet that delineates the Data Source and Data Target, and documentation to support the transformation, aggregation or other logic.  The spreadsheet can be directly authored in IGC, or loaded from Excel (text file) which further supports automation of the process.  Documentation for Extension Mapping Documents can be found here:  <a href=""https://www.ibm.com/support/knowledgecenter/en/SSZJPZ_11.5.0/com.ibm.swg.im.iis.mdwb.doc/topics/c_extensionMappings.html"" rel=""nofollow noreferrer"">https://www.ibm.com/support/knowledgecenter/en/SSZJPZ_11.5.0/com.ibm.swg.im.iis.mdwb.doc/topics/c_extensionMappings.html</a>  (though suggest creating such a document from IGC, and exporting the results to Excel).</p>

<p>In addition, IGC supports a more formal process for extending the Catalog and introducing new types of Assets.  This would go one step further, and properly document and catalog the Data Processes (SQL commands, other ETL tooling) and map the data movement thru those Processes.  This will allow users to identify with the Data Process and even allow one to include operational data (as is supported for IGC).   More information on this process can be found here:  <a href=""https://www-01.ibm.com/support/docview.wss?uid=swg21699130"" rel=""nofollow noreferrer"">https://www-01.ibm.com/support/docview.wss?uid=swg21699130</a></p>

<p>Suggest to review the absolute requirements, and what information is required for the ensuing traceability report.  Starting with the Extension Mapping Document should suffice, and would be the simplest to implement and drive immediate benefit.</p>
"
"56864108","Data sync between temporal tables in SQL Server","<p>I want to Sync data between two databases which have system-versioned(temporal) tables. The data sync option in Azure does not support temporal tables and I need to find another way of doing the sync between the databases. </p>

<p>I want to copy the data from one temporal table to another temporal table of another database using Azure data factory.Will Azure data factory support the data sync between temporal tables? </p>

<p>What will be the best way of syncing data between the two databases that have identical temporal tables?</p>
","<sql-server><azure><azure-data-factory><data-synchronization><temporal-tables>","2019-07-03 06:26:15","1563","0","2","56864620","<p>I created a system-versioned(temporal) table in my Azure SQL database followed this document <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/tables/creating-a-system-versioned-temporal-table?view=sql-server-2017#creating-a-temporal-table-with-a-default-history-table"" rel=""nofollow noreferrer"">Creating a temporal table</a>:</p>

<pre><code>CREATE TABLE Department   
(    
     DeptID int NOT NULL PRIMARY KEY CLUSTERED  
   , DeptName varchar(50) NOT NULL  
   , ManagerID INT  NULL  
   , ParentDeptID int NULL  
   , SysStartTime datetime2 GENERATED ALWAYS AS ROW START NOT NULL  
   , SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL  
   , PERIOD FOR SYSTEM_TIME (SysStartTime, SysEndTime)     
)   
WITH    
   (   
      SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)   
   )   
;
</code></pre>

<p>I created two temporal tables Department and Department2 in different Azure SQL Server. </p>

<p>I test in the Data Factory copy active, choose the Department as source dataset, we can see the temporal table from the settings:
<a href=""https://i.stack.imgur.com/GuCLM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GuCLM.png"" alt=""enter image description here""></a></p>

<p>Choose the Department2 as link dataset, Table mapping::
<a href=""https://i.stack.imgur.com/mKWQV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mKWQV.png"" alt=""enter image description here""></a></p>

<p>Column mapping:
<a href=""https://i.stack.imgur.com/HzKRI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HzKRI.png"" alt=""enter image description here""></a></p>

<p>Active run succeeded:
<a href=""https://i.stack.imgur.com/W86lm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W86lm.png"" alt=""enter image description here""></a></p>

<p>That's mean Azure Data Factory support copy the data from one temporal table to another temporal table of another database.</p>

<p><strong>Update:</strong></p>

<p>Please reference this document: <a href=""https://sqland.wordpress.com/2016/06/30/temporal-tables-in-sql-server-2016-part-iii/"" rel=""nofollow noreferrer"">Temporal Tables in SQL Server 2016 â€“ Part III</a>.</p>

<p>Now, starting by INSERT, remember there are two datetime2 special columns in a temporal table declared as GENERATED ALWAYS AS ROW START / END.
These are the PERIOD columns and they are mandatory but you cannot insert an explicit value into a GENERATED ALWAYS column. Its values will be automatically filled.
When you insert a new row in the table, the â€œROW STARTâ€ column will have the value of SYSUTCDATETIME() (yes, donâ€™t forget itâ€™s UTC time!) and the â€œROW ENDâ€ column will have the value: â€˜9999-12-31 23:59:59.9999999â€™
Basically when inserting a new row in a temporal table, regarding these two columns, you can use of the following options:
1. Use a column list and omit these two columns;
2. Use a column list without omitting these two columns and specify DEFAULT in the values list for each.
3. Donâ€™t use a column list and specify DEFAULT in the values list for each.</p>

<p>Congratulations, the error has solved by yourself:</p>

<blockquote>
  <ol>
  <li>Loading the tables after switching off the versioning on the destination table and it worked.</li>
  </ol>
</blockquote>

<p><code>ALTER TABLE [dbo].[Department] SET(SYSTEM_VERSIONING = OFF); ALTER TABLE [dbo].[Department] DROP PERIOD FOR SYSTEM_TIME;</code> </p>

<blockquote>
  <ol start=""2"">
  <li>Then after the copy active done, switch it back.</li>
  </ol>
</blockquote>

<pre><code>ALTER TABLE [dbo].[Department] ADD PERIOD FOR SYSTEM_TIME (SysStartTime, SysEndTime); ALTER TABLE [dbo].[Department] SET(SYSTEM_VERSIONING = ON);
</code></pre>

<p>Hope this helps.</p>
"
"56864108","Data sync between temporal tables in SQL Server","<p>I want to Sync data between two databases which have system-versioned(temporal) tables. The data sync option in Azure does not support temporal tables and I need to find another way of doing the sync between the databases. </p>

<p>I want to copy the data from one temporal table to another temporal table of another database using Azure data factory.Will Azure data factory support the data sync between temporal tables? </p>

<p>What will be the best way of syncing data between the two databases that have identical temporal tables?</p>
","<sql-server><azure><azure-data-factory><data-synchronization><temporal-tables>","2019-07-03 06:26:15","1563","0","2","59258515","<p>It sounds like this thread got the answer it needed, but for future searches... the Azure SQL Data Sync within Azure SQL Database <strong>does</strong> support syncing temporal tables.</p>

<p>You just have to remember to not sync the system generated date columns (because you can't explicitly INSERT into those columns, and Data Sync will try :-))</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-sync-data#sync-req-lim"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sql-database/sql-database-sync-data#sync-req-lim</a></p>

<p>HTH</p>
"
"56748630","How to Synch two databases in Microservice architecture in CQRS separate ones for read/write","<p>I was asked this question in this interview:</p>

<p>How to Synch two database data? There will be time delays etc. How do we handle? </p>

<p>The background: I mentioned about Microservice architecture and also using CQRS for performance (Separate Read/get query database) and separate write command database.</p>

<p>Now, if the customer enters or modifies data, how it will be replicated/synched in to the read database? </p>

<p>I was talking about stuffs like cosmos db options etc which prevents dirty read etc. I also mentioned about cache. But I am not certain what are all variousoptions to do synch. Interviewer specifically asked me in SQL DB level how do I synch between two DBs.</p>
","<sql-server><microservices><cqrs><data-synchronization>","2019-06-25 07:12:00","3373","1","2","56749337","<ol>
<li>CQRS is a pattern which dictates that the responsibility of <em>Command</em> and <em>Query</em> operations be seperated. </li>
<li>Now there are multiple ways you can sychronize the data between databases. You can use <strong>Master-Slave Configuration</strong> or <strong>Oplog Replication Mechanism</strong> or something very much specific to the database.</li>
<li>But what's more important here is to decide what strategy to use. Since, you are using CQRS pattern now you have more than one data store (write store, read store) and there are fair chances that these data stores are network partitioned. In which case you would have to decide what really matters to you the most <strong>Consistency</strong> or <strong>Availabililty</strong>, which is generally goverened by what businesses require.</li>
</ol>

<p>So in general, what replication strategy is to be used depends on whether your businesses require <strong>Consistency</strong> or <strong>Availabililty</strong>.</p>

<p><strong>References</strong>:</p>

<ol>
<li><strong>CAP Theroem</strong>: <a href=""https://en.wikipedia.org/wiki/CAP_theorem"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/CAP_theorem</a></li>
<li><strong>Replication</strong> (Driven by CAP Theorem): <a href=""https://www.brianstorti.com/replication/"" rel=""nofollow noreferrer"">https://www.brianstorti.com/replication/</a></li>
</ol>
"
"56748630","How to Synch two databases in Microservice architecture in CQRS separate ones for read/write","<p>I was asked this question in this interview:</p>

<p>How to Synch two database data? There will be time delays etc. How do we handle? </p>

<p>The background: I mentioned about Microservice architecture and also using CQRS for performance (Separate Read/get query database) and separate write command database.</p>

<p>Now, if the customer enters or modifies data, how it will be replicated/synched in to the read database? </p>

<p>I was talking about stuffs like cosmos db options etc which prevents dirty read etc. I also mentioned about cache. But I am not certain what are all variousoptions to do synch. Interviewer specifically asked me in SQL DB level how do I synch between two DBs.</p>
","<sql-server><microservices><cqrs><data-synchronization>","2019-06-25 07:12:00","3373","1","2","56750381","<p>There are a couple of options for database syncing in SQL server.</p>

<p><strong>1.</strong> <strong>SQL Server Always on Feature (SQL 2012 Onwards)</strong> - By using this feature, You need to make a primary and secondary replica (could be multiple secondry replica), Once Always On feature is configured, the Second replicas automatically updated based on Primary replica updates. This also provides HADR feature, if the primary replica goes down, the secondary replica will be active and play primary replica role.
<a href=""https://learn.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-2017"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-2017</a></p>

<p><strong>2. SQL Server Replication -</strong> Merge replication, Transaction replication etc.
<a href=""https://learn.microsoft.com/en-us/sql/relational-databases/replication/types-of-replication?view=sql-server-2017"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/relational-databases/replication/types-of-replication?view=sql-server-2017</a></p>
"
"56319379","REST API design for data synchronization service","<p>What is best practice for data synchronization operation between client and server?</p>

<p>We have 2 (or more) resources:</p>

<ol>
<li><code>cars</code> -> year, model, engine</li>
<li><code>toys</code> -> color, brand, weight</li>
</ol>

<p>And we need to get updated resources from server in case of any updates on them. For example: someone made changes from another client on the same data and we need to transfer those updates to our client application.</p>

<p><strong>Request:</strong></p>

<p><code>http://api.example.com/sync?data=cars,toys</code> (verb?)</p>

<p><code>http://api.example.com/synchronizations?data=cars,toys</code> (virtual resource ""synchronizations"")</p>

<p><strong>Response with mixed data:</strong></p>

<p>status code: 200</p>

<pre><code>{
  message: ""ok"",
  data: {
    cars: [
        {
          year: 2015,
          model: ""Fiat 500""
          engine: 0.9
        },
        {
          year: 2004,
          model: ""Nissan Sunny""
          engine: 1.3
        }
    ],
    toys: [
        {
          color: ""yellow"",
          brand: ""Bruder""
          weight: 2
        }
    ],
  }
}
</code></pre>

<p>or response with status code 204 if no updates available. In my opinion making separated http calls in not a good solution. What if we have 100 resources (=100 http calls)?</p>
","<rest><api>","2019-05-27 03:36:23","1074","0","1","56319501","<p>I am not an expert, but one method I have used in the past is to ask for a ""signature"" of the data, as opposed to always going and getting the data.  The signature can be a hash of the data you are looking for.  So, flow would be something like:</p>

<ol>
<li>Get signature hash of the data </li>
</ol>

<pre><code>http://api.example.com/sync/signature/cars
</code></pre>

<p>Which returns the signature hash</p>

<ol start=""2"">
<li><p>Check if the signature is different from the last time you retrieved the data</p></li>
<li><p>If the signature is different, go and get the data</p></li>
</ol>

<pre><code>http://api.example.com/sync/cars
</code></pre>

<ol start=""4"">
<li>Have the REST also add the new signature to the data</li>
</ol>

<pre><code>{
  message: ""ok"",
  data: {
    cars: [
        {
          year: 2015,
          model: ""Fiat 500""
          engine: 0.9
        },
        {
          year: 2004,
          model: ""Nissan Sunny""
          engine: 1.3
        },
    ],
    signature: ""570a90bfbf8c7eab5dc5d4e26832d5b1""
  }
}
</code></pre>
"
"55873579","Firestore: How can I force data synchronization when coming back online","<p>I am building a flutter app with cloud firestore and I am using the offline capabilities.
When coming back online after I made changes offline, it seems like the changes take quite some time to synchronise (sometime up to a minute).</p>

<p>Is there any ways to force the synchronisation manually so that I can could trigger it myself when listening for the device to get back online?</p>

<p>Thanks a lot for your help!</p>
","<firebase><flutter><google-cloud-firestore>","2019-04-26 19:02:23","889","1","1","55873853","<p>The native SDKs for iOS, Android, and Web have <a href=""https://firebase.google.com/docs/firestore/manage-data/enable-offline#disable_and_enable_network_access"" rel=""nofollow noreferrer"">API calls that allow you to explicitly manage connection state</a>. While those are not explicitly made for your use-case, it'd be worth a try to see if disabling/re-enabling the network in short succession makes a difference.</p>

<p>Unfortunately those methods are currently not wrapped in the <a href=""https://pub.dartlang.org/documentation/cloud_firestore/latest/cloud_firestore/Firestore-class.html"" rel=""nofollow noreferrer""><code>Firestore</code> class</a> of the FlutterFire library.</p>

<p>There is an open <a href=""https://github.com/flutter/flutter/issues/21300"" rel=""nofollow noreferrer"">issue in the Github repo</a> to track demand and progress on it. I just gave it an upvote.</p>
"
"55783664","Check for similar text in arrays php","<p>I have this code which displays logos with a link to a video file. I frequently upload files and I want to keep thinks in order. At the moment it just attaches them in order of name. So for example if I upload a movie and haven't gotten around to uploading a photo with the exact same name then from that point on any photos will be displayed with mismatching links.</p>

<p>What I'd like to do is display a temporary photo if the name doesn't match.</p>

<p>I know you can do something like if(array[1] === array2[1]) but the file extensions would be different so that would return false every time.</p>

<p>Code:</p>

<pre><code>&lt;?php
$images = glob('./*.{jpeg,jpg,png}', GLOB_BRACE);
$movies = glob('./*.{mp4,m4v}', GLOB_BRACE);
$movieLink = 0;

foreach($images as $image) {
    echo '&lt;a href=""' .$movies[$movieLink].'""&gt;
            &lt;img src=""'.$image.'"" style=""width:300px;height:350px;border:0;""&gt;
        &lt;/a&gt;';
    $movieLink++;
}

?&gt;
</code></pre>

<p>Example of server directory (400+ movies and >30 photos):</p>

<p>Dir1</p>

<ul>
<li>Movie1.mp4 â†°__ correct pair</li>
<li>Movie1.png &nbsp;â†²</li>
<li>Movie2.m4v â†°__ correct pair</li>
<li>Movie2.png &nbsp;â†²</li>
<li>Movie3.mp4 â†°__ incorrect pair</li>
<li>Movie4.mp4 â†²</li>
<li>Movie4.png â†-- no image to pair with</li>
</ul>

<p>When this runs it displays 3 photos side by side which when clicked the first 2 (Movie1.png &amp;&amp; Movie2.png) you are taken to the correct movies for each. However, when you click ""Movie 4.png"" you are taken to ""Movie3.mp4"".</p>
","<php><arrays><data-synchronization>","2019-04-21 14:36:15","76","1","1","55783813","<p>Use <code>pathinfo($filename, PATHINFO_FILENAME)</code> to construct a lookup array for easy verification.  The lookup array uses ""extensionless"" movie filenames as keys which are paired with the image filename (with its extension).</p>

<p>You should be looping the <code>$movies</code> array, if your project logic states that there will always be more movies than images.</p>

<pre><code>$movies = glob('./*.{mp4,m4v}', GLOB_BRACE);
$images = glob('./*.{jpeg,jpg,png}', GLOB_BRACE);
foreach ($images as $image) {
    $lookup[pathinfo($image, PATHINFO_FILENAME)] = $image;
}

foreach ($movies as $movie) {
    $image = $lookup[pathinfo($movie, PATHINFO_FILENAME)] ?? 'default.jpg';
    echo '&lt;a href=""' . $movie . '""&gt;
            &lt;img src=""' . $image . '"" style=""width:300px;height:350px;border:0;""&gt;
        &lt;/a&gt;';
}
</code></pre>

<p>The above snippet is not tested, but it should be pretty close.  In case you are unfamiliar, the <code>??</code> is the null coalescing operator.  It basically says assign the lookup value unless it is missing, in which case use the default value.</p>

<p>Here's a demo of the lookup array construction: <a href=""https://3v4l.org/HJhVR"" rel=""nofollow noreferrer"">https://3v4l.org/HJhVR</a></p>
"
"55691305","Will WKC running on-premise leverage StoredIQ for data connectors or does it leverage connectors in IBM Governance Catalog IGC?","<p>Will WKC running on-premise leverage StoredIQ for data connectors or does it leverage connectors in IGC?</p>
","<watson-knowledge-catalog><data-governance>","2019-04-15 14:22:34","61","0","1","55695449","<p>WKC running on-premises support the same connectors as those available in WKC as a public Saas in IBM Cloud. </p>

<p>You can find the full list of supported connectors for WKC on-prem <a href=""https://www.ibm.com/support/knowledgecenter/SSZJPZ_11.7.0/wsj/manage-data/conn_types.html"" rel=""nofollow noreferrer"">here</a>.</p>
"
"55672926","CAP Theorem - async writes & consistency","<p>When saying a system is CP (consistent &amp; partitioned), does that mean that we cannot use asynchronous synchronization between replicated data nodes, and that every write must be copied synchronously (and even transactionally)?</p>

<p>As I understand, consistency means that for every write, following reads (from any node) will get the latest update.
In case we write to a specific node and synchronize the other nodes asynchronously, the reads that will occur before synchronization ends may not get the latest write.</p>
","<database><distributed-system><data-synchronization><cap-theorem>","2019-04-14 07:22:43","378","1","1","55678330","<blockquote>
  <p>When saying a system is CP (consistent &amp; partitioned), does that mean that we cannot use asynchronous synchronization between replicated data nodes</p>
</blockquote>

<p>Yes, it's not possible to build <code>CP</code> system on the basis of asynchronous replication.</p>

<p>Also I can't agree that in <code>CP</code> <code>P</code> stands for ""partitioned"", because partitioning relates to database scalability problems. I think that <code>CP</code> should be treated as ""consistent in case of network partition"".</p>

<blockquote>
  <p>and that every write must be copied synchronously?</p>
</blockquote>

<p>True, but there is an optimization: replicate data synchronously not to all nodes, but to the majority of nodes, and asynchronously to the rest of nodes.</p>

<blockquote>
  <p>(and even transactionally)</p>
</blockquote>

<p>I think it depends on the kind of the database (whether it support transactions or not) and how to define a transaction term itself (e.g., ACID-compatible or not). From my point of view it does not relate to the main points of CAP-theorem actually.</p>

<blockquote>
  <p>consistency means that for every write, following reads (from any node) will get the latest update.</p>
</blockquote>

<p>Yes in general, but there are much more consistency models, please refer to <a href=""https://jepsen.io/consistency"" rel=""nofollow noreferrer"">https://jepsen.io/consistency</a></p>

<blockquote>
  <p>In case we write to a specific node and synchronize the other nodes asynchronously, the reads that will occur before synchronization ends may not get the latest write.</p>
</blockquote>

<p>True.</p>
"
"55155609","How to transfer rules and configuration to edge devices?","<p>In our application we have a server which contains entities along with their relations and processing rules stored in DB. To that server there will be n no.of clients like raspberry pi , gateways, android apps are connected. </p>

<p>I want to push configuration &amp; processing rules to those clients, so when they read some data they can process on their own. This is to make the edge devices self sustainable, avoid outages when server/network is down. </p>

<p>How to push/pull the configuration. I don't want to maintain DBs at client and configure replication. But the problem is maintenance and patching of DBs for those no.of client will be tough.</p>

<p>So any other better alternative.?</p>

<p>At the same time I have to push logs to upstream (server). </p>

<p>Thanks in advance.</p>
","<database><architecture><iot><data-synchronization>","2019-03-14 05:37:24","38","0","1","55191629","<p>I have been there. You need an on-device data store. For this range of embedded Linux, in order of growing development complexity:</p>

<ul>
<li><strong>Variables</strong>: Fast to change and retrieve, makes sense if the data fits in memory. Lost if the process ends.</li>
<li><strong>Filesystem</strong>: Requires no special libraries, just read/write access somewhere. Workable if the data is small enough to fit in memory and does not change much during execution (read on startup when lacking network, write on update from server). If your data can be structured as a few object variables, you could write them to JSON files, and there is plenty of documentation on other file storage options for Android apps.</li>
<li><strong>In-memory datastore</strong> like Redis: Lightweight dependency, can automate messaging and filesystem-stored backup. Provides a managed framework/hybrid of the previous two.</li>
<li><strong>Lightweight databases</strong>, especially <strong>SQLite</strong>: Lightweight SQL database, stored in one file and popular with Android apps (probably already installed on many of the target devices). It could work for frequent changes on a larger block of data in a memory-constrained environment, but does not look like a great fit. It gets worse for anything heavier.</li>
</ul>

<p><a href=""https://redis.io/topics/replication"" rel=""nofollow noreferrer"">Redis replication</a> is easy, but indiscriminate, so mainly sensible if your devices receive a changing but identical ruleset. Otherwise, in all these cases, the easiest transfer option may be to request and receive the whole configuration (GET a string, download a JSON file, etc.) and parse the received values.</p>
"
"55023396","Data sync between on Prem MySQL and Azure Data Store","<p>We have a legacy system which talks to on-premise MySQL database. We are building a new system and its hosted in Azure. The database it uses to store the data is in Azure Storage. Since the legacy application will be functional for some time, we would like to sync the data from MySQL to Azure Table Storage. Is there any tool that can help in the synchronization of the data from MySQL to Azure Table Storage?</p>
","<mysql><data-synchronization><azure-data-sync>","2019-03-06 12:42:11","210","0","1","55060288","<p>You could use Data-in Replication feature for synchronization of data between MYSQL and Azure database.</p>

<p>Please refer <a href=""https://learn.microsoft.com/en-us/azure/mysql/concepts-data-in-replication"" rel=""nofollow noreferrer"">this</a> document for more information.</p>

<p>Hope it helps.</p>
"
"54817237","I want to detect latin characters with umlaut mark anywhere in given string by using informatica","<p>I want to detect latin characters with umlaut mark anywhere in given string by using informatica. 
Requirement is whenever I found atleast one Latin character with umlaut mark anywhere in a string, I'll give output as Fail else pass.</p>
","<informatica><informatica-powercenter><data-quality>","2019-02-21 22:28:46","113","0","2","54830423","<p>Use a function:</p>

<p><strong>REG_MATCH</strong> : Returns whether a value matches a regular expression pattern.</p>
"
"54817237","I want to detect latin characters with umlaut mark anywhere in given string by using informatica","<p>I want to detect latin characters with umlaut mark anywhere in given string by using informatica. 
Requirement is whenever I found atleast one Latin character with umlaut mark anywhere in a string, I'll give output as Fail else pass.</p>
","<informatica><informatica-powercenter><data-quality>","2019-02-21 22:28:46","113","0","2","54852441","<pre><code>REG_MATCH ( column, pattern ) provides you the proper result.
</code></pre>

<p>Go through below link for more on regular expressions. <br>
<a href=""https://marketplace.informatica.com/mpresources/docs/RegularExpressions_2255.pdf"" rel=""nofollow noreferrer"">https://marketplace.informatica.com/mpresources/docs/RegularExpressions_2255.pdf</a> <br>
<a href=""http://www.disoln.org/2012/12/Data-Cleansing-and-Standardization-Using-Regular-Expression-informatica-powercenter.html"" rel=""nofollow noreferrer"">http://www.disoln.org/2012/12/Data-Cleansing-and-Standardization-Using-Regular-Expression-informatica-powercenter.html</a></p>
"
"54656518","Synchronizing data from external source to db used in django project?","<p>I need to synchronize data from a csv-file every night to update the data in my django project (I'm using mysql as backend). The data contains user information - sometimes users are to be removed, sometimes new user has to be added and sometimes some of the information about a specific user has changed and needs to be updated. I need to keep this in sync. Are there any built in functions to do this (i.e. manage.py/django-admin)? Do I write a python script and work with my django models to manipulate the data? Is it ok/safe to work directly with the tables in the database, bypassing the models? What is best practice?</p>

<p>I'm using Python 3.7 and Django 2.1 if this helps.</p>
","<python><mysql><django><database><data-synchronization>","2019-02-12 18:30:06","1496","1","1","54656798","<p>There are no prebuilt commands that will likely meet your exact needs, but python has good <a href=""https://docs.python.org/3/library/csv.html"" rel=""nofollow noreferrer"">csv</a> reading utilities and you could write a custom <a href=""http://%20https://docs.djangoproject.com/en/2.1/howto/custom-management-commands/"" rel=""nofollow noreferrer"">Django command</a> to read your csv file and add, delete or update your User models.</p>

<p>Unless you've got special requirements - ie, huge datasets or other constraints - I'd recommend interacting with Django's model layer to make the changes, rather than interacting directly with the database. This way, if you have any special validation on your User model that is not represented on the database-level (choices, integer ranges, so on), you will avoid introducing data inconsistencies.</p>

<p>A final thought - in your command, consider making use of the following excellent methods that Django's ORM provides: <a href=""https://docs.djangoproject.com/en/2.1/ref/models/querysets/#get-or-create"" rel=""nofollow noreferrer"">Manager.get_or_create</a> and <a href=""https://docs.djangoproject.com/en/2.1/ref/models/querysets/#update-or-create"" rel=""nofollow noreferrer"">Manager.update_or_create</a>. They are useful to avoid duplicating data and elegantly telling the ORM to create or modify in-place, etc.</p>

<p>Good luck!</p>
"
"54568863","How to get label activity reports ( O365 Data Governance) user level and file level programatically?","<p>I used labels and policies in Data Governance. After I auto-published the label, I see the individual files in OneDrive getting tagged with a label.</p>

<p>I need to programmatically check if there are any labels associated with the file and categorize them. I don't see the labels data while querying through Graph as well.</p>

<p><a href=""https://graph.microsoft.com/v1.0/me/drive/root/children/test.txt"" rel=""nofollow noreferrer"">https://graph.microsoft.com/v1.0/me/drive/root/children/test.txt</a></p>

<p>Do we have any powershell commands or REST API to get these data in granular level?</p>

<p><a href=""https://learn.microsoft.com/en-us/office365/securitycompliance/view-label-activity-for-documents?redi"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/office365/securitycompliance/view-label-activity-for-documents?redi</a>...</p>

<p>Thanks,
Madhan</p>
","<microsoft-graph-api><office365><microsoft-graph-security><data-governance>","2019-02-07 08:10:05","159","0","1","54627725","<p>Have you tried this for retrieving LabelActivity:</p>

<p><a href=""http://%20%20%20%20https://learn.microsoft.com/en-us/powershell/module/exchange/policy-and-compliance-retention/get-compliancetag?view=exchange-ps"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/exchange/policy-and-compliance-retention/get-compliancetag?view=exchange-ps</a></p>

<p>For your first question on retrieving Labels from Graph, you can try the below query to check if it provides label info. I haven't tried myself.</p>

<p>GET <a href=""http://%20https://graph.microsoft.com/v1.0/sites/%7Bsite-id%7D/lists/%7Blist-id%7D/items/%7Bitem-id%7D?expand=fields"" rel=""nofollow noreferrer"">https://graph.microsoft.com/v1.0/sites/{site-id}/lists/{list-id}/items/{item-id}?expand=fields</a></p>

<p>You can constructSharePointId using the information from SharePointIds from the query below:
<a href=""https://graph.microsoft.com/v1.0/drive/root/children?$select=Name,sharepointIds"" rel=""nofollow noreferrer"">https://graph.microsoft.com/v1.0/drive/root/children?$select=Name,sharepointIds</a></p>
"
"54546927","Hybrid database synchronization without data sync agent","<p>I'm working on a project to create hybrid SQL database per tenant model. While we were able to replicate the on-premise databases to databases in Azure. I'm not able to find a way to continuously sync both on-prem DB and cloud DB (We cannot use data sync agent or transaction replication). We are looking for any other alternatives that we can try to achieve our purpose.</p>

<p>Also, how does synchronization works when the internet is down and cannot sync with cloud?</p>

<p>Sorry for my ignorance since I'm new to this field.</p>

<p>Thanks.</p>
","<multi-tenant><data-synchronization>","2019-02-06 05:05:26","101","0","1","54776194","<p>why can't you use data sync agent? It can be installed in other machine that has access to the database and internet. </p>
"
"54510317","How do I drop all numbers as data cleansing on pandas effectively?","<p>Here's my dataset</p>

<pre><code>id                                             descriptions
0                       kartu debit 20 10 indomaretcipete r
1                                         tarikan atm 20 10
2                                         tarikan atm 19 10
3                                                 biaya adm
4                       trsf 18 10 wsid 23881 indah lestari
</code></pre>

<p>Here's what I did</p>

<pre><code>def cleaning(text):
    stops = {'10', '18','19', '20', '23881'}
    text = [word for word in text if not word in stops]
    text = "" "".join(text)
return(text)

df['description_clean'] = df['description'].apply(cleaning)
</code></pre>

<p>Here's what I got</p>

<pre><code>  id                                              descriptions
  0                             kartu debit indomaretcipete r
  1                                               tarikan atm
  2                                               tarikan atm
  3                                                 biaya adm
  4                                   trsf wsid indah lestari
</code></pre>

<p>This is not effective I keep add new numbers to improve stopwords, how do in one time?</p>
","<python><regex><pandas><dataframe>","2019-02-04 04:55:01","57","1","3","54510344","<p>IIUC, you need to remove numbers from the dataframe, use below:</p>

<pre><code>df_new=df.replace('\d+ ','',regex=True)
print(df_new)

   id                   descriptions
0   0  kartu debit indomaretcipete r
1   1                 tarikan atm 10
2   2                 tarikan atm 10
3   3                      biaya adm
4   4        trsf wsid indah lestari
</code></pre>

<p>For just one series: <code>df['descriptions']=df['descriptions'].replace('\d+ ','',regex=True)</code></p>

<p>Note: i have added a space after <code>d+</code> in the regex depending on your example, you can do without it if you want to. </p>
"
"54510317","How do I drop all numbers as data cleansing on pandas effectively?","<p>Here's my dataset</p>

<pre><code>id                                             descriptions
0                       kartu debit 20 10 indomaretcipete r
1                                         tarikan atm 20 10
2                                         tarikan atm 19 10
3                                                 biaya adm
4                       trsf 18 10 wsid 23881 indah lestari
</code></pre>

<p>Here's what I did</p>

<pre><code>def cleaning(text):
    stops = {'10', '18','19', '20', '23881'}
    text = [word for word in text if not word in stops]
    text = "" "".join(text)
return(text)

df['description_clean'] = df['description'].apply(cleaning)
</code></pre>

<p>Here's what I got</p>

<pre><code>  id                                              descriptions
  0                             kartu debit indomaretcipete r
  1                                               tarikan atm
  2                                               tarikan atm
  3                                                 biaya adm
  4                                   trsf wsid indah lestari
</code></pre>

<p>This is not effective I keep add new numbers to improve stopwords, how do in one time?</p>
","<python><regex><pandas><dataframe>","2019-02-04 04:55:01","57","1","3","54510368","<p>You need:</p>

<pre><code>def replace_numbers(s):
    return re.sub(r'\d*', '', s)


df['description'] = df['description'].apply(replace_numbers)
</code></pre>
"
"54510317","How do I drop all numbers as data cleansing on pandas effectively?","<p>Here's my dataset</p>

<pre><code>id                                             descriptions
0                       kartu debit 20 10 indomaretcipete r
1                                         tarikan atm 20 10
2                                         tarikan atm 19 10
3                                                 biaya adm
4                       trsf 18 10 wsid 23881 indah lestari
</code></pre>

<p>Here's what I did</p>

<pre><code>def cleaning(text):
    stops = {'10', '18','19', '20', '23881'}
    text = [word for word in text if not word in stops]
    text = "" "".join(text)
return(text)

df['description_clean'] = df['description'].apply(cleaning)
</code></pre>

<p>Here's what I got</p>

<pre><code>  id                                              descriptions
  0                             kartu debit indomaretcipete r
  1                                               tarikan atm
  2                                               tarikan atm
  3                                                 biaya adm
  4                                   trsf wsid indah lestari
</code></pre>

<p>This is not effective I keep add new numbers to improve stopwords, how do in one time?</p>
","<python><regex><pandas><dataframe>","2019-02-04 04:55:01","57","1","3","54510382","<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extractall.html"" rel=""nofollow noreferrer""><code>str.extractall</code></a> and <a href=""https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a>:</p>

<pre><code>df['descriptions'] = (df['descriptions'].str.extractall('([a-zA_Z]+)')
                                        .groupby(level=0).agg({0:' '.join}))
</code></pre>

<p>Or:</p>

<pre><code>df['descriptions'] = (df['descriptions'].str.replace('\d+','')
                                        .str.replace('  ',''))
</code></pre>

<p>Or:</p>

<pre><code>df['descriptions'] = [' '.join(re.findall('[a-zA-Z]+',s)) for s in df['descriptions']]
</code></pre>

<hr>

<pre><code>print(df)
   id                   descriptions
0   0  kartu debit indomaretcipete r
1   1                    tarikan atm
2   2                    tarikan atm
3   3                      biaya adm
4   4        trsf wsid indah lestari
</code></pre>
"
"54478456","SQL column update to solve data quality issue","<p>I am unable to figure out how to write 'smart code' for this. <a href=""https://i.stack.imgur.com/lab4M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lab4M.png"" alt=""problem sample""></a></p>

<p>In this case I would like the end result for the first two case to be:</p>

<p><strong>product_cat_name</strong></p>

<p>A_SEE</p>

<p>A_BEE</p>

<p>Business is rule is such that one <code>product_cat_name</code> can belong to only one <code>group</code> but due to data quality issues we sometimes have a <code>product_cat_name</code> belonging to 2 different <code>group</code>s. As a special case in such a situation we would like to append <code>group</code> to the <code>product_cat_name</code> so that <code>product_cat_name</code> becomes unique.</p>

<p>It sounds so simple yet I am cracking my head over this.
Any help much appreciated.</p>
","<sql-server><sql-server-2008>","2019-02-01 11:19:47","141","0","2","54483335","<p>Something like this:</p>

<pre><code>with names as (

    select prod_cat_nm , prod_cat_nm+group as new_nm from (query that joins 3 tables together) as qry
    join
    (Select prod_cat_nm,  count(distinct group)
    from (query that joins 3 tables together) as x
    group by 
    prod_cat_nm
    having count(distinct group) &gt; 1) dups
    on dups.prod_cat_nm = qry.prod_cat_nm
    )

    SELECT prod_cat_nm, STRING_AGG(New_nm, '') WITHIN GROUP (ORDER BY New_Nm ASC) AS new_prod_cat_nm
    FROM names
    GROUP BY prod_cat_nm;
</code></pre>

<p>I've used the 2017 STRING_AGG() here as its shortest to write - But you could easily change this to use Recursions or XML path </p>
"
"54478456","SQL column update to solve data quality issue","<p>I am unable to figure out how to write 'smart code' for this. <a href=""https://i.stack.imgur.com/lab4M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lab4M.png"" alt=""problem sample""></a></p>

<p>In this case I would like the end result for the first two case to be:</p>

<p><strong>product_cat_name</strong></p>

<p>A_SEE</p>

<p>A_BEE</p>

<p>Business is rule is such that one <code>product_cat_name</code> can belong to only one <code>group</code> but due to data quality issues we sometimes have a <code>product_cat_name</code> belonging to 2 different <code>group</code>s. As a special case in such a situation we would like to append <code>group</code> to the <code>product_cat_name</code> so that <code>product_cat_name</code> becomes unique.</p>

<p>It sounds so simple yet I am cracking my head over this.
Any help much appreciated.</p>
","<sql-server><sql-server-2008>","2019-02-01 11:19:47","141","0","2","55048326","<p>It is simple if you break it down into small pieces.</p>

<p>You need to UPDATE the table obviously, and change the value of <code>product_cat_name</code>.   That's easy.   </p>

<p>The new value should be <code>group + product_cat_name</code>.  That's easy.</p>

<p>You only want to do this when a <code>product_cat_name</code> is associated with more than one group.  That's probably the tricky part, but it can also be broken down into small pieces that are easy.</p>

<p>You need to identify which <code>product_cat_name</code>s have more than one group.  That's easy.  <code>GROUP BY product_cat_name HAVING COUNT(DISTINCT Group) &gt; 1</code>.</p>

<p>Now you need to use that to limit your UPDATE to only those <code>product_cat_name</code>s.  That's easy.  <code>WHERE product_cat_name IN (Subquery using above logic to get PCNs that have more than one Group)</code>.</p>

<p>All easy steps.  Put them together and you've got your solution.</p>
"
"53915927","TIdCmdTCPServer and data synchronization with main thread [anomaly?]","<p>Situation looks like this. External application client.exe sends a command <strong>MONITOR_ENCODING</strong> every ~250ms to the server. In server application I use <strong>IdCmdTCPServer1BeforeCommandHandler</strong> to read sent command from client. 
I copy received command (<strong>AData</strong> string) to global variable <strong>command</strong> and then show it in memo1. Meanwhile I have constantly running TSupervisorThread which copies global variable command to local Copiedcommand variable and then assigns new text to command variable. Is this an expected behaviour that from time to time instead of MONITOR_ENCODING text I get RESET?</p>

<p><a href=""https://i.stack.imgur.com/UWNRX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UWNRX.png"" alt=""enter image description here""></a></p>

<pre><code>procedure TForm1.IdCmdTCPServer1BeforeCommandHandler(ASender: TIdCmdTCPServer; var AData: String; AContext: TIdContext);
begin

  command:=AData;  //command is a global variable
  form1.Memo1.Lines.Add(IntToStr(form1.Memo1.Lines.Count+1)+'|'+IntToStr(GetTickCount)+'|'+command);

end;

procedure TSupervisorThread.CopyGlobalVariables;
begin

  CopiedCommand:=command; //Copiedcommand declared in TSupervisorThread
  command:='RESET'; 

end;

procedure TSupervisorThread.Execute;
begin

  while Terminated=false do
  begin
    Synchronize(CopyGlobalVariables);
    sleep(250);
  end;

end;
</code></pre>
","<delphi><indy10>","2018-12-24 16:54:19","139","0","1","53916610","<p>FYI, the <code>OnBeforeCommandHandler</code> event is not the correct place to read commands with <code>TIdCmdTCPServer</code>. You are <em>supposed</em> to add an entry for each command to its <code>CommandHandlers</code> collection and then assign an <code>OnCommand</code> handler to each entry.  The <code>OnBeforeCommandHandler</code> event is triggered before <code>TIdCmdTCPServer</code> parses a received command. That is OK for logging purposes, just make sure you don't use it to drive your processing logic. Leave that to the individual <code>OnCommand</code> events.</p>

<p>But, either way, command reading is done in a worker thread. You are not synchronizing with the main UI thread when adding received commands to your UI. You MUST synchronize. There are many ways to do that - <code>TThread.Synchronize()</code>, <code>TThread.Queue()</code>, <code>TIdSync</code>, <code>TIdNotify</code>, <code>(Send|Post)Message()</code>, etc, just to name a few. </p>

<p>More importantly, you have 2 threads (or more, depending on how many clients are connected at the same time) fighting over the same global variable without syncing access to it at all. You need a lock around the variable, such as a <code>TCriticalSection</code> or <code>TMutex</code>, or use Indy's <code>TIdThreadSafeString</code> class.</p>

<p>But, that won't solve the race condition that your code has. Between the time that your <code>OnBeforeCommandHandler</code> assigns a new value to <code>command</code> and the time that it reads <code>command</code> back to add it to the UI, your <code>TSupervisorThread</code> is free to modify <code>command</code>. That is exactly what you are seeing happen.  It is not an anomaly in Indy, it is a timing issue in your code.</p>

<p>The easiest solution to that race condition is to simply add <code>AData</code> to your UI instead of <code>command</code>. That way, it won't matter if <code>TSupervisorThread</code> modifies <code>command</code>, your UI won't see it.</p>

<p>But, why are you using a global <code>command</code> variable at all? What is its real purpose? Are you trying to make an outside thread modify the commands that <code>TIdCmdTCPServer</code> parses? You are not controlling which clients get to parse real commands and which get fake commands. Why are you doing this?</p>

<p>Besides that, having <code>TSupervisorThread</code> perform 99% of its work in the main UI thread is a poor use of a worker thread, you may as well just use a <code>TTimer</code> in the UI instead. Otherwise, you need to coordinate your threads better, such as by using <code>TEvent</code> objects to signal when <code>command</code> is assigned to, and when it is reset.</p>

<p>I think you need to rethink your design.</p>
"
"53856202","exclude google users from syncing - google GCDS","<p>I have google users created under my G-suite domain (user@myDomain.com) and I want to exlude them from syncing into my microsoft AD, beacuase it shows that they will be disabled since they don't exist in my AD. </p>
","<active-directory><google-workspace><data-synchronization>","2018-12-19 17:15:11","640","0","1","53856396","<p>You would create an exclusion rule, specifically a ""Google domain exclusion rule"": <a href=""https://support.google.com/a/answer/6138405?hl=en"" rel=""nofollow noreferrer"">https://support.google.com/a/answer/6138405?hl=en</a></p>

<p>You would either add an exclusion for each user specifically:</p>

<pre><code>Type: User Email Address
Match Type: Exact Match
Rule: user@domain.com
</code></pre>

<p>Or find some other property that identifies those users and use that in your rule.</p>
"
"53740246","R: Data Quality Check: Zip Code matching the City","<p>Can someone help me to realize an idea in R?</p>

<p>I want to achieve, that when R gets an Input File with e.g. a list of companies and their address, it will check wether the zip Code fits to the City for each Company. I have a list of all cities and Zip codes from a certain Country. How can I implement the list into an if sentence?</p>

<p>Did someone Programm something similar before?</p>

<p>Thanks for ur help!
Sandra</p>
","<r><data-quality>","2018-12-12 09:50:58","241","-2","1","53740515","<p>Just a quick example of what one could do. It is, however, probably better to use fuzzy matching for your cities.</p>

<pre><code># City codes (all city codes can be found at https://www.allareacodes.com/)
my_city_codes &lt;- data.frame(code = c(201:206), 
                            cities = c(""Jersey City, NJ"", ""District of Columbia"", ""Bridgeport, CT"", ""Manitoba"", ""Birmingham, AL"", ""Seattle, WA""),
                            stringsAsFactors = FALSE)

# Function for checking if city/city-code matches those in the registries
adress_checker &lt;- function(adress, citycodes) {
  # Finding real city
  real_city &lt;- my_city_codes$cities[which(adress$code == my_city_codes$code)]

  # Checking if cities are the same
  if(real_city == adress$city) {
    return(""Correct city"")
  } else {
    return(""Incorrect city"")
  }
}

# Adresses to check
right_city &lt;- data.frame(code = 205, city = c(""Birmingham, AL""), stringsAsFactors = FALSE)
wrong_city &lt;- data.frame(code = 205, city = c(""Las Vegas""), stringsAsFactors = FALSE)

# Testing function
adress_checker(right_city, my_city_codes)
[1] ""Correct city""
adress_checker(wrong_city, my_city_codes)
[1] ""Incorrect city""
</code></pre>
"
"53150620","what happens if I turn off a device before ""sync""?","<p>I have an embedded device (Linux + BusyBox) on which I loaded a file <code>""my_file.txt""</code>. I used <code>cat</code> to check the contens of the file, and it was satisfactory. Then, I pulled the plug and after reboot, I saw the file was still there, but with 0 bytes size...</p>

<p>Can that be caused by an unsync file-system? This is actually a double-sided question:</p>

<ol>
<li>Is the createion of a file and the copy of its contens happen in different stages? (allowing a phase wehre a file with 0 bytes exists)</li>
<li>Is it possible that I ""see"" the file <em>[meaning I successfully managed to <code>cat ""my_file.txt""</code>]</em>, but what I actually see, is a cached version that will not be there after reboot, unless a <code>sync</code> will be called?  <sub><em>and by ""not be there"" I mean the contens, as the file itself remains</em></sub></li>
</ol>

<p><em>BTW, when does Linux flush filesystems? I know that <code>stdout</code>, for example, is flushed (by default) when a <code>""\n""</code> is introduced [and can be configured somehow, don't remember exactly how]. Is there a rule for filesystems as well?</em></p>
","<c><linux><data-synchronization>","2018-11-05 08:18:18","452","-1","3","53150906","<p>if you turn off the device while you are working with file in your project it can be destroyed your file because for example you want erase your file's document and write again into it.my be you device turn off between these two stage and your file destroy. </p>
"
"53150620","what happens if I turn off a device before ""sync""?","<p>I have an embedded device (Linux + BusyBox) on which I loaded a file <code>""my_file.txt""</code>. I used <code>cat</code> to check the contens of the file, and it was satisfactory. Then, I pulled the plug and after reboot, I saw the file was still there, but with 0 bytes size...</p>

<p>Can that be caused by an unsync file-system? This is actually a double-sided question:</p>

<ol>
<li>Is the createion of a file and the copy of its contens happen in different stages? (allowing a phase wehre a file with 0 bytes exists)</li>
<li>Is it possible that I ""see"" the file <em>[meaning I successfully managed to <code>cat ""my_file.txt""</code>]</em>, but what I actually see, is a cached version that will not be there after reboot, unless a <code>sync</code> will be called?  <sub><em>and by ""not be there"" I mean the contens, as the file itself remains</em></sub></li>
</ol>

<p><em>BTW, when does Linux flush filesystems? I know that <code>stdout</code>, for example, is flushed (by default) when a <code>""\n""</code> is introduced [and can be configured somehow, don't remember exactly how]. Is there a rule for filesystems as well?</em></p>
","<c><linux><data-synchronization>","2018-11-05 08:18:18","452","-1","3","53151055","<blockquote>
  <p>Is the createion of a file and the copy of its contens happen in different stages? (allowing a phase where a file with 0 bytes exists)</p>
</blockquote>

<p>Yes. The normal operation on files are </p>

<ol>
<li>open/create the file </li>
<li>read/write data.</li>
<li>close the file.</li>
</ol>

<blockquote>
  <p>Is it possible that I ""see"" the file [meaning I successfully managed to cat ""my_file.txt""], but what I actually see, is a cached version that will not be there after reboot, unless a sync will be called?</p>
</blockquote>

<p>Yes. If step 1 above was synced to the hard drive, but step 2 was not, you lose the file content.</p>

<blockquote>
  <p>BTW, when does Linux flush filesystems? I know that stdout, for example, is flushed (by default) when a ""\n"" is introduced [and can be configured somehow, don't remember exactly how]. Is there a rule for filesystems as well?</p>
</blockquote>

<p>No, there are no general rules - it's complicated. The OS/Kernel and filesystem caches data in RAM and writes it to disk when its internal algorithms figure out it's a good time to do so.</p>

<p>Note that there are flush/sync'ing on many levels. The flushing you talk about ""when a ""\n"" is introduced"", is only a flush from a program down to the operating system. The operating system might then keep the data in RAM only and flush it to a hard drive later.
The hard drive might even cache it in RAM onboard the harddrive and write it to permanent storage later on.</p>

<p>Normally you can run the <code>sync</code> command on a command line to ensure all cached data is written from the OS to the hard drive. (Albeit on low-end hard drives with onboard RAM that is not battery backed up, this could still lose data that resides in the ram onboard a harddrive if power is cut).</p>
"
"53150620","what happens if I turn off a device before ""sync""?","<p>I have an embedded device (Linux + BusyBox) on which I loaded a file <code>""my_file.txt""</code>. I used <code>cat</code> to check the contens of the file, and it was satisfactory. Then, I pulled the plug and after reboot, I saw the file was still there, but with 0 bytes size...</p>

<p>Can that be caused by an unsync file-system? This is actually a double-sided question:</p>

<ol>
<li>Is the createion of a file and the copy of its contens happen in different stages? (allowing a phase wehre a file with 0 bytes exists)</li>
<li>Is it possible that I ""see"" the file <em>[meaning I successfully managed to <code>cat ""my_file.txt""</code>]</em>, but what I actually see, is a cached version that will not be there after reboot, unless a <code>sync</code> will be called?  <sub><em>and by ""not be there"" I mean the contens, as the file itself remains</em></sub></li>
</ol>

<p><em>BTW, when does Linux flush filesystems? I know that <code>stdout</code>, for example, is flushed (by default) when a <code>""\n""</code> is introduced [and can be configured somehow, don't remember exactly how]. Is there a rule for filesystems as well?</em></p>
","<c><linux><data-synchronization>","2018-11-05 08:18:18","452","-1","3","53151173","<blockquote>
  <ol>
  <li>Is the createion of a file and the copy of its contens happen in different stages? (allowing a phase wehre a file with 0 bytes exists)</li>
  </ol>
</blockquote>

<p>Yes, copying a file is not an atomic operation as you first call to <code>open()</code> and then <code>write()</code> after... Opening with <code>O_CREAT</code> mode will create an empty file, so yes : first an empty file which is filled after.</p>

<blockquote>
  <ol start=""2"">
  <li>Is it possible that I ""see"" the file [meaning I successfully managed to cat ""my_file.txt""], but what I actually see, is a cached
  version that will not be there after reboot, unless a sync will be
  called? and by ""not be there"" I mean the contens, as the file itself
  remains</li>
  </ol>
</blockquote>

<p>Yes exactly, what you saw is a cached version of the previous operations.</p>

<blockquote>
  <p>when does Linux flush filesystems?</p>
</blockquote>

<p>The general rule is that the kernel flushes things when it wants. The only thing you can do is to ask for the flushing, but alas even this is just an asking and doesn't means that the flush occurred, it just means that the flush will occur soon. The corresponding command line is <code>sync</code>.</p>

<blockquote>
  <p>Is there a rule for filesystems as well?</p>
</blockquote>

<p>You may mount a filesystem requesting that IOs will be made in direct mode, or you can request for it on a file-by-file basis (see <code>O_DIRECT</code> and alikes in <code>open</code>). But be aware that direct mode generally drops the performances...</p>
"
"53106821","React / Redux: single source of truth when navigating into a details view","<p>I have an architectural / best practices question regarding react and redux.</p>

<p>I'm using redux middleware to query a graphql service and expose the result in props to connected components. One of the advantages of graphql is being able to retrieve only the information you need using their query format. </p>

<p>I have a page that lists entities and when you click a specific entity you get a view entity detail page with the id in the query string.</p>

<p>I want an array of entities in my state to be my single source of truth and a selectedEntityId property to serve as a lookup on that list of entities. </p>

<p>My problem is that the list of entities in the list page is just the name and description while the view page contains much much more information.</p>

<p><strong>Solutions I can think of:</strong></p>

<p>1) Retrieve all the entities information (not just name and description) in the list page (and view page). The problem is that I'd be querying a bunch of information I don't need on the list page which goes against the idea of graphql.</p>

<p>2) Have 2 properties on my state. One will contain entities with just name and description while the other property will contain a list of entities with more information. The problem with this solution is that I have 2 single sources of truth.</p>

<p>3) I can ""marry"" the 2 entity lists, so if I navigate from a list to a view page, I can saturate the list of entities I already have from the list page with the entities I get from the view page (the entities with many more properties besides name and description) and use that list as my single source of truth. This seems to be the best solution but I don't know a clean way to do this.</p>

<p>Can anyone shed light on the situation?</p>

<p>Thanks!</p>
","<redux><react-redux>","2018-11-01 17:59:44","334","0","1","53107929","<p>I'm not sure if thats the best one, but based on your solutions, I would say the third works alright.</p>

<p>You could try adding a new value to each list item that identifies if the specific item has all the details or not.</p>

<p>Imagine the store like so:</p>

<pre><code>items: {
  id1: {
    isFull: false,
    title: 'Item 1',
    id: 'id1'
  },
  id2: {
    isFull: true,
    title: 'Item 2',
    id: 'id2',
    description: '..',
    lorem: '',
    ...
  }
}
</code></pre>

<p>Then get <code>isFull</code> in the details page and fetch in case of false.</p>
"
"53052917","data-synchronization problem in batch script when querying register data, I need something like RegFlushKey but in batch","<p>I'm querying register values as part of unit testing for a batch script that will set up a network for our customers. I've identified a race condition wherein querying a register after setting it to a value in the same function causes the old value to appear untill you re-run the batch file. An example in my block of code is as follows:</p>

<pre><code>:MyFunctionName
REM some initialization code

powercfg /change monitor-timeout-ac 2 1&gt; nul 
powercfg /change standby-timeout-ac 2 1&gt; nul 
powercfg /change hibernate-timeout-ac 2 1&gt; nul

REM code that gets register paths setup and does some other irrelevant stuff
REM MonitorTimeoutPath is the path to the first powercfg setting
REM StandbyTimeoutPath is the path to the second setting
REM HibernateTimeoutPath is the path to the third setting

FOR /F ""tokens=1-3 skip=2"" %%A IN ('reg query ""!MonitorTimeoutPath!"" /v ACSettingIndex') DO (set /A ""ActualMonitorTimeout=%%C/60"")
FOR /F ""tokens=1-3 skip=2"" %%A IN ('reg query ""!StandbyTimeoutPath!"" /v ACSettingIndex') DO (set /A ""ActualStandbyTimeout=%%C/60"")
FOR /F ""tokens=1-3 skip=2"" %%A IN ('reg query ""!HibernateTimeoutPath!"" /v ACSettingIndex') DO (set /A ""ActualHibernateTimeout=%%C/60"")

REM code that tests values but is important for now

echo M:!ActualMonitorTimeout! S:!ActualStandbyTimeout! H:!ActualHibernateTimeout!
goto:EOF
</code></pre>

<p>Now here is the important thing, when I set the three powercfg settings to different values (say 1,1,2 respectively) the FIRST time I run the batch file, the echo statement shows me values of 0,0,0 and only when I run the file a SECOND time do I see the true values of 1,1,2. Since I'm 99.99% sure I'm not accessing it wrong because I use VERY similar FOR statements to grab register data when I do it outside of the same function like this:</p>

<pre><code>reg add ""HKEY_CURRENT_USER\Control Panel\Accessibility\StickyKeys"" /v Flags /t REG_SZ /d 10 /f 1&gt; nul 
call :RegTestFunc ""HKEY_CURRENT_USER\Control Panel\Accessibility\StickyKeys"" Flags 10

:RegTestFunc
REM some code
FOR /F ""tokens=1-3 skip=2"" %%A IN ('reg query ""%~1"" /v %~2 2^&gt;nul') DO (set /A ""RegisterData=%%C"")
REM some more code
goto:EOF
</code></pre>

<p>So there is virtually no difference except some math differences (hence why I couldn't use the function) and some syntax (because its a function). The point is, my syntax seems to be fine here and it doesn't cause race conditions when I use the <code>RegTestFunc</code>. It only happens when I directly run the querying operation in the same function that I set the register value.</p>

<p>To try to solve this I tried researching what (if any) form of timing control batch had and I came across the timeout command. I used it, thinking I could delay the processor so the register value would be done changing by the time it was queried. Specifically I put it before the 3 <code>FOR</code> statements that query the data from the register. This didn't work hence why I'm asking here because Google is a useless search engine for anything interesting. Is there an easy way to make some sort of lock on the querying operation or its results so I always get the most recent changes? I'm pretty sure that cmd is just going so fast that the powercfg /change command can't get it's job done before the reg query command comes along and asks for outdated data. If need be I could just make another function and call it and see if that changes things but thats messy so I'd like to avoid that.</p>

<p>For reference: What would be the technical term for this problem? I don't see it as a race condition because there aren't multiple threads vying for the same register data, it isn't a consumer producer problem because the data is there, it just isn't being recognized. However, those concepts are very similar to whats actually happening (at least for me) so what would this ""problem"" be called? The data is changing/being queryed too fast but what would that formally be called (if anything)? My first thought was race condition, then consumer-producer, then stale data then I decided that synchronization problem would be the best words I would use to describe what I think is happening.</p>

<h1>UPDATE</h1>

<p>I've tried changing all the important delayed expansion (!) variables in my code to non delayed (%) thinking that maybe it was grabbing old values via expansion and scope but this didn't work, the registers were still outdated by one run of the script. I also tried querying the registers twice and using a helper function to seperate the code and see if that worked but still it didn't work. My only guess is that changes to powercfg aren't finalized until the last statement of the batch script is reached so then my question is, how do I query accurate data before then?</p>

<h1>UPDATE 2</h1>

<p>I've tried exporting the register data right after I set it.</p>

<pre><code>REM RUNNING THE FILE THE FIRST TIME ASSUMING THE VALUES WERE ALL 0 BEFORE
powercfg /change monitor-timeout-ac 3 1&gt; nul 
powercfg /change standby-timeout-ac 3 1&gt; nul 
powercfg /change hibernate-timeout-ac 3 1&gt; nul

REM some code
REM the below isn't runnable but the idea is that it contains the path to 
REM powercfg /change monitor-timeout-ac
reg export %MonitorTimeoutPath% foo.reg
</code></pre>

<p>(in foo.reg)</p>

<blockquote>
  <p>[HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Control\Power\User\PowerSchemes\f4e62c59-ee57-456a-94c4-3662e9d6ceb9\7516b95f-f776-4464-8c53-06167f40cc99\3c0bc021-c8a8-4e07-a973-6b14cbcb2b7e]
  ""ACSettingIndex""=dword:00000000</p>
</blockquote>

<p>Notice the hex value of 00 at the end (0 seconds or 0 minutes)</p>

<p>Then if you run the above script EVEN without the lines:</p>

<pre><code>REM RUNNING THE FILE THE SECOND TIME
powercfg /change monitor-timeout-ac 3 1&gt; nul 
powercfg /change standby-timeout-ac 3 1&gt; nul 
powercfg /change hibernate-timeout-ac 3 1&gt; nul
</code></pre>

<p>You get the correct value</p>

<blockquote>
  <p>[HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Control\Power\User\PowerSchemes\f4e62c59-ee57-456a-94c4-3662e9d6ceb9\7516b95f-f776-4464-8c53-06167f40cc99\3c0bc021-c8a8-4e07-a973-6b14cbcb2b7e]
  ""ACSettingIndex""=dword:000000B4</p>
</blockquote>

<p>(hex 180 seconds or 3 minutes)</p>

<p>And only after running the batch file twice does the actual value update EVEN IF I comment out the powercfg write lines on the second run so that the write to the registry happens only once. What this means is that the registry itself hasn't gotten the data yet when we query right after we change powercfg settings. I've researched the issue and what I need is an implementation of</p>

<pre><code>RegFlushKey(hkey key);
</code></pre>

<p>Which apparently is something that exists in C and forces an update to the registry of the specified key (it writes that registry to 'the disk' immediately and halts everything else thus being slow). All that is detailed much better here: <a href=""https://learn.microsoft.com/en-gb/windows/desktop/api/winreg/nf-winreg-regflushkey"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-gb/windows/desktop/api/winreg/nf-winreg-regflushkey</a> and I need a way to do basically that in batch.</p>
","<function><batch-file><registry><data-synchronization>","2018-10-29 19:58:35","241","0","1","53071067","<h1>The issue</h1>
<p>I found out what was wrong. I was querying a <code>DuplicateScheme</code> rather than the <code>ActiveScheme</code>.</p>
<p>When we were developing this script we had to include functionality for changing power settings on Windows 10 pro. You can't normally set the active scheme to the default GUID for high performance right off the bat because windows 10 pro is weird. Instead you have to use <code>DuplicateScheme</code> on the high performance guid so you can generate a new guid with the same settings. That same line of code also sets it as the <code>ActiveScheme</code> (that term will be important).</p>
<p>When the script was in development originally it was intended to also work with Windows 2016 server version (and some other versions as well). These versions didn't require the <code>DuplicateScheme</code> technique and instead you had to set the <code>ActiveScheme</code> to high powered directly. Trying to use the <code>DuplicateScheme</code> method on windows 2016 versions <strong>shouldn't and didn't</strong> work as expected.</p>
<p>My mistake was that even though I ran the <code>DuplicateScheme</code> method on Windows 2016 Server version, I thought it wouldn't affect anything else. I was wrong. Since the <code>DuplicateScheme</code> technique doesn't work on Windows 2016 Server verison, the PowerScheme Guid I was querying was never actually set to the <code>ActiveScheme</code>. I had an error check in place to find this error because I <strong>expected</strong> it to happen (and it did which is good). However, Since I was querying the results of <code>Powercfg</code> changes, I was trying to query my <code>DuplicateScheme</code> for changes that were made on the <code>ActiveScheme</code>. Since the changes were first made on the <code>ActiveScheme</code>, I didn't see them on the <code>DuplicateScheme</code> in time.</p>
<p>The standard behavior seems to be as follows:</p>
<ol>
<li><p>running <code>Powercfg</code> commands that update registers for the <code>ActiveScheme</code> take affect immediately and can be queried without flushing them as I thought was needed</p>
</li>
<li><p>running <code>Powercfg</code> commands that update registers for Duplicates of the ActiveScheme (i.e. our <code>DuplicateScheme</code>) take effect after the batch file is done executing. Waiting for them to update with timeouts, pauses or some other means does not work. My Hypothesis is that DuplicateSchemes take on the changes of their parents after the batch file has concluded. My confusion was because I didn't recognize this behavior, I thought I was always editing the <code>ActiveScheme</code> but I was wrong.</p>
</li>
</ol>
<h1>Code example of problem:</h1>
<pre><code>REM setting the ActiveScheme as per win10 spec
REM this creates the DuplicateScheme but only in windows 10 pro does it set this new 
REM scheme as the ActiveScheme also. In my version it did NOT do this
FOR /F &quot;tokens=*&quot; %%F IN ('powercfg /duplicatescheme 8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c') DO (SET PowerScheme=%%F)

REM some code to process things

REM code to modify the ActiveScheme
powercfg /change monitor-timeout-ac 1 1&gt; nul 

REM some code to create MonitorTimeoutPath and other things

REM code to query the DuplicateScheme
FOR /F &quot;tokens=1-3 skip=2&quot; %%A IN ('reg query &quot;%MonitorTimeoutPath%&quot; /v ACSettingIndex') DO (set /A &quot;ActualMonitorTimeout=%%C/60&quot; &amp; echo C:%%C)

REM IF we assume that we had ran 
REM [powercfg /change monitor-timeout-ac 0 1&gt; nul]
REM previously, then running the above code results in output of:
C: 0

REM when whats expected is:
C: 1
</code></pre>
<p>The solution is to change <code>%MonitorTimeoutPath%</code> to the oath of the <code>ActiveScheme</code> not the <code>DuplicateScheme</code>.</p>
<p>So it turns out the actual problem was <strong>completely unrelated</strong> to flushing the register and I didn't need to go down that rabbit hole at all. Thanks brain. I accidentally discovered the solution by querying the <code>ActiveScheme</code> directly by key path rather than using the <code>%MonitorTimeoutPath%</code> which held the <code>DuplicateScheme</code> path.</p>
<h1>Tl;dr</h1>
<p>The problem was I was updating an <code>ActiveScheme</code> and querying a <code>DuplicateScheme</code> and that should only work as I expected in Windows 10 Pro NOT Windows 2016 Server version (which I was running)</p>
"
"53014674","Python pandas data cleansing","<p>I am new to python pandas, I am having difficulty in achieving the below data cleansing, kindly help. </p>

<p>My actual data (csv file link - <a href=""https://s3.amazonaws.com/rajaampledata/data.csv"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/rajaampledata/data.csv</a>)</p>

<pre><code>Date,Description,Description,Ref. No,Amount,Balance
30/08/2012,TFR-TFR:0000000101-,,,""1,952.50-"",""4,000.000""
"""",Kumar - S/O To:,,,,
"""",600010013441,,,,
30/08/2012,FDR-,,,10.50-,""5,114,897.40""
"""",AU;541411;301218;RAJA,,,,
"""",J;RTGS-AUTO-,,,,
"""",TRANSAC,,,,
26/08/2012,DEP-IN162071/D61519,,,""1,000.83"",""6,100,098.32""
26/08/2012,WDL-IN B CM 20120826,,,180.32-,""789,126.31""
25/08/2012,103-,,,""1,000,000.00"",""3,225,700.00""
"""",IN;112138;100318;BANK,,,,
"""",ACC;,,,,
</code></pre>

<p>I would like to get the data as below </p>

<pre><code>30/08/2012,TFR-TFR:0000000101-Kumar - S/O To:600010013441,,,""1,952.50"",""4,000.000""
30/08/2012,FDR-AU;541411;301218;RAJAJ;RTGS-AUTO-TRANSAC,,,10.50-,""5,114,897.40""
26/08/2012,DEP-IN162071/D61519,,,""1,000.83"",""6,100,098.32""
26/08/2012,WDL-IN B CM 20120826,,,180.32-,""789,126.31""
25/08/2012,103-IN;112138;100318;BANKACC;,,,""1,000,000.00"",""3,225,700.00""
</code></pre>
","<python><pandas>","2018-10-26 18:46:19","139","1","1","53015139","<p>Try appending to a previous line if the current line starts with a blank. Once you have your data, join them into a string with your comma delimiter.</p>

<pre><code>with open('data.csv') as f:
    reader = csv.reader(f)
    headers = next(reader)
    lines = []
    for r in reader:
        if r[0] == '':
            lines[-1][1] = lines[-1][1] + r[1]
        else:
            lines.append(r)

lines = [','.join(i) for i in lines]

print(lines)
&gt;&gt;['30/08/2012,TFR-TFR:0000000101-Kumar - S/O To:6.0001E+11,,,1,952.50-,4,000.00',
 '30/08/2012,FDR-AU;541411;301218;RAJAJ;RTGS-AUTO-TRANSAC,,,10.50-,5,114,897.40',
 '26/08/2012,DEP-IN162071/D61519,,,1,000.83,6,100,098.32',
 '26/08/2012,WDL-IN B CM 20120826,,,180.32-,789,126.31',
 '25/08/2012,103-IN;112138;100318;BANKACC;,,,1,000,000.00,3,225,700.00']
</code></pre>

<p>If you want <code>headers</code> read the first line of the csv.</p>
"
"52827090","Mobile database with client-side synchronisation of local databases required","<p>I am building a mobile app with the following business requirements:</p>

<ol>
<li><p>Db to be stored locally on the device for use when disconnected from
the cloud.</p></li>
<li><p>A NoSQL type store is required to provide for future changes without requiring complex db rebuild and data migration. </p></li>
<li>Utilises a SQL query language for simple programming.</li>
<li><p>Run on all target platforms - Windows, Android, iOS</p></li>
<li><p>No central database server - data to be synchronised by matching two local copies of the db file.</p></li>
</ol>

<p>I have examined a lot of dbs for mobile and none provide all these features except Couchbase Lite 2.1 Enterprise Edition. The downside of that is that the EE license might be price prohibitive in my use case.<br>
[EDIT: yes the EE license is USD$35K for &lt;= 1000 devices to that option is out for me sadly.]</p>

<p>Are there any other such products out there that someone could point me to?</p>
","<database><mobile><data-synchronization>","2018-10-16 02:26:48","209","3","1","58754315","<p>The client-side synchronization of local databases done by Couchbase Lite is a way to replicate data from one mobile device to another. Though is a limited feature because it works on P2P. Take as an example BitTorrent, the fastest and most effective P2P protocol. It still has flaws, risk of data corruption and partial data loss. A P2P synchronization would only be safe when running between two distinct applications on the same mobile device.</p>

<p>In case both databases are in the same mobile device and managed by the <strong>same</strong> application, it would be much simpler. You could do the synchronization yourself by reading data from one and saving in the other, and dealing with conflicts if needed.</p>

<p>I'm curious, why is it a requirement not to have a central database server? You can fine tune what data is shared and between which users is it shared. Here is how it works: </p>

<p>On server-side user registry, each user is assigned a list of channel names. At the same time, each JSON document added or updated is also linked to a list of channel names. For every pair of user x document with at least one channel name in common, the server allows push/pull replications to occur.</p>

<p>Good luck !</p>
"
"52226539","How does UTF-16 achieve self-synchronization?","<p>I know that UTF-16 is a self-synchronizing encoding scheme. I also read the below Wiki, but did not quite get it.</p>

<p><a href=""https://simple.wikipedia.org/wiki/Self-synchronizing_code"" rel=""nofollow noreferrer"">Self Synchronizing Code</a></p>

<p>Can you please explain me with an example of UTF-16?</p>
","<unicode><character-encoding><utf-16><data-synchronization>","2018-09-07 16:37:06","422","1","1","52226773","<p>In UTF-16 characters outside of the BMP are represented using a <a href=""https://en.wikipedia.org/wiki/UTF-16"" rel=""nofollow noreferrer"">surrogate pair</a> in with the first <a href=""https://en.wikipedia.org/wiki/Code_unit"" rel=""nofollow noreferrer"">code unit</a> (CU) lies between 0xD800â€”0xDBFF and the second one between 0xDC00â€”0xDFFF. Each of the CU represents 10 bits of the code point. Characters in the BMP is encoded as itself.</p>
<p>Now the synchronization is easy. Given the position of any arbitrary code unit:</p>
<ul>
<li>If the code unit is in the 0xD800â€”0xDBFF range, it's the first code unit of two, just read the next one and decode. VoilÃ , we have a full character outside of BMP</li>
<li>If the code unit is in the 0xDC00â€”0xDFFF range, it's the second code unit of two, just go back one unit to read the first part, or advance to the next unit to skip the current character</li>
<li>If it's in neither of those ranges then it's a character in BMP. We don't need to do anything more</li>
</ul>
<p>In UTF-16 CU is the unit, i.e. the smallest element. We work at the CU level and read the CU one-by-one instead of byte-by-byte. <strong>Because of that along with historical reasons UTF-16 is only self-synchronizable at CU level.</strong></p>
<p>The point of self-synchronization is to know whether we're in the middle of something immediately instead of having to read again from the start and check. UTF-16 allows us to do that</p>
<blockquote>
<p>Since the ranges for the high surrogates, low surrogates, and valid BMP characters are <a href=""https://en.wikipedia.org/wiki/Disjoint_sets"" rel=""nofollow noreferrer"">disjoint</a>, it is not possible for a surrogate to match a BMP character, or for (parts of) two adjacent characters to look like a legal surrogate pair. This simplifies searches a great deal. It also means that UTF-16 is <em>self-synchronizing</em> on 16-bit words: whether a code unit starts a character can be determined without examining earlier code units. UTF-8 shares these advantages, but many earlier multi-byte encoding schemes (such as <a href=""https://en.wikipedia.org/wiki/Shift_JIS"" rel=""nofollow noreferrer"">Shift JIS</a> and other Asian multi-byte encodings) did not allow unambiguous searching and could only be synchronized by re-parsing from the start of the string (UTF-16 is not self-synchronizing if one byte is lost or if traversal starts at a random byte).</p>
<p><a href=""https://en.wikipedia.org/wiki/UTF-16#Description"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/UTF-16#Description</a></p>
</blockquote>
<p>Of course that means UTF-16 may be not suitable for working over a medium without error correction/detection like a bare network environment. However in a proper local environment it's a lot better than working without self-synchronization. For example in <a href=""https://en.wikipedia.org/wiki/DOS/V"" rel=""nofollow noreferrer"">DOS/V for Japanese</a> every time you press <kbd>Backspace</kbd> you must iterate from the start to know which character was deleted because in the awful Shift-JIS encoding there's no way to know how long the character before the cursor is without a length map</p>
"
"52192209","Check data quality by join of a query result to a sub query with by using test data in the query with union all","<p>I'm working on checking data quality. We want to check a list of names a user gives us by forming it in a subquery because we don't have permission to create tables or temp tables since it's a third party database. I can quickly concatenate the list together in spreadsheet formulas. How do I right join to the result of a query to find out if any are missing. This is also a great example of how to include test data in your script without creating test tables or temp tables!
Version SQL Server 2008 R</p>

<pre><code>--table to check for data quality

SELECT LAST_NAME, FIRST_NAME, PEOPLE_CODE_ID FROM 
test.DBO.PEOPLE WHERE
last_name = 'Berd' and first_name = 'Alivia' OR
last_name = 'Arny' and first_name = 'Jase' OR
last_name = 'Barny' and first_name = 'Cale'
AS A

right join --list from user
        (
select 'Abbie' as first_name, 'Bail' as last_name
UNION select 'Jenny' as first_name, 'Bleee' as last_name
UNION select 'Jase' as first_name, 'Arny' as last_name
UNION select 'Jason' as first_name, 'Bussey' as last_name
) AS B
WHERE A.LAST_NAME = B.LAST_NAME
AND A.FIRST_NAME = B.FIRST_NAME
</code></pre>
","<sql>","2018-09-05 19:30:09","691","1","2","52192881","<p>I'm not completely sure I'm understanding your question but it sounds like you want to compare your <code>people</code> table against a list of users and return which users aren't in that list?  </p>

<p>If so, here's one option using <code>not exists</code>:</p>

<pre><code>with userstosearch as (
    select 'Abbie' as first_name, 'Bail' as last_name
    union all select 'Jenny', 'Bleee' 
    union all select 'Jase', 'Arny' 
    union all select 'Jason', 'Bussey'
) 
select u.* 
from userstosearch u
where not exists (
    select 1
    from test.DBO.PEOPLE p
    where u.firstname = p.firstname and u.lastname = p.lastname
)
</code></pre>

<hr>

<p>If you prefer to use an <code>outer join</code>:</p>

<pre><code>...
select u.* 
from userstosearch u
    left join test.DBO.PEOPLE p on u.firstname = p.firstname and u.lastname = p.lastname
where p.firstname is null
</code></pre>
"
"52192209","Check data quality by join of a query result to a sub query with by using test data in the query with union all","<p>I'm working on checking data quality. We want to check a list of names a user gives us by forming it in a subquery because we don't have permission to create tables or temp tables since it's a third party database. I can quickly concatenate the list together in spreadsheet formulas. How do I right join to the result of a query to find out if any are missing. This is also a great example of how to include test data in your script without creating test tables or temp tables!
Version SQL Server 2008 R</p>

<pre><code>--table to check for data quality

SELECT LAST_NAME, FIRST_NAME, PEOPLE_CODE_ID FROM 
test.DBO.PEOPLE WHERE
last_name = 'Berd' and first_name = 'Alivia' OR
last_name = 'Arny' and first_name = 'Jase' OR
last_name = 'Barny' and first_name = 'Cale'
AS A

right join --list from user
        (
select 'Abbie' as first_name, 'Bail' as last_name
UNION select 'Jenny' as first_name, 'Bleee' as last_name
UNION select 'Jase' as first_name, 'Arny' as last_name
UNION select 'Jason' as first_name, 'Bussey' as last_name
) AS B
WHERE A.LAST_NAME = B.LAST_NAME
AND A.FIRST_NAME = B.FIRST_NAME
</code></pre>
","<sql>","2018-09-05 19:30:09","691","1","2","52192941","<p>If you are looking for the lines that DO NOT match:</p>

<pre><code>SELECT A.LAST_NAME, A.FIRST_NAME, PEOPLE_CODE_ID
FROM  test.DBO.PEOPLE AS A  

right join --list from user
        (
select 'Abbie' as first_name, 'Bail' as last_name
UNION select 'Jenny' as first_name, 'Bleee' as last_name
UNION select 'Jase' as first_name, 'Arny' as last_name
UNION select 'Jason' as first_name, 'Bussey' as last_name
) AS B ON  A.LAST_NAME = B.LAST_NAME AND A.FIRST_NAME = B.FIRST_NAME


WHERE (A.last_name = 'Berd' and A.first_name = 'Alivia')
    OR (A.last_name = 'Arny' and A.first_name = 'Jase')
    OR (A.last_name = 'Barny' and A.first_name = 'Cale')
AND B.LAST_NAME IS NULL
</code></pre>

<p>Change to IS NOT NULL for the matching lines or use something like</p>

<pre><code>CASE WHEN B.last_name IS NULL THEN 0 ELSE 1 END AS HasMatch
</code></pre>
"
"52042836","Multiple users for private Docker registry?","<p>I have a private Docker registry running.</p>

<p>Any user should be able to push and pull any image. Therefore, right now I am not using any user identification at all.</p>

<p>However, a user should not be able to trick the registry to overwrite the Images of other users.</p>

<p>If user A uploads ourRegistry/myProgram:version_1, then user B should not be able to upload something tagged ourRegistry/myProgram:version_2.</p>

<p>Is there a way to add user authentification to a private registry to do this?</p>

<p>Additionally, the registry is part of a server that already has its own database of registered users. Is there a way to synchronize the users, so that the users don't have to remember two passwords?</p>
","<docker><authentication><user-permissions><docker-registry><data-synchronization>","2018-08-27 15:46:22","6857","5","1","52345136","<p>The official documentation on docker registry authentication is located here: <a href=""https://docs.docker.com/registry/deploying/#native-basic-auth"" rel=""nofollow noreferrer"">https://docs.docker.com/registry/deploying/#native-basic-auth</a>. Since it uses htpasswd to handle its authentication I'm not sure if there's any way to use your user database dynamically (obviously you can write a script to import all your users using htpasswd mentioned in this documentation)</p>
"
"51674355","Matching a list of names to a column with bad data quality (Python)","<p>I have a table with 5 columns, one of which is a list of names in terrible data quality. I managed to clean it up as far as possible in R, but it still looks something like this (formatted as code for easier reading):</p>

<pre><code>Neville Longbottomx
Severus Snape Slyth
Granger, Hermioone
Miss Lovegoo
Nott: Theodore
Mr Potter Gryffindor
Malfoy, Draco
Bulstrode, Millicent
McGonagall, Minerv
Seamus Finnigan Mister
Miss Abbott, Hannah
Ernie Macmillan M
Dumbledore, Albus
Parkinson, Pans"" Slyth
</code></pre>

<p>Now, I have another list with names like this:</p>

<pre><code>Lovegood, Luna
Longbottom, Neville
Macmillan, Ernie
Nott, Theodore
Parkinson, Pansy
</code></pre>

<p>And I want to find the names in the second list in the first one. I looked up different articles on this and tried <a href=""https://bergvca.github.io/2017/10/14/super-fast-string-matching.html"" rel=""nofollow noreferrer"" title=""this method"">this method</a> because ngrams seemed like a smart way to go, but I first got this error:</p>

<pre><code>def ngrams(string, n=3):
    string = re.sub(r'[,-./]|\sBD',r'', string)
    ngrams = zip(*[string[i:] for i in range(n)])
    return [''.join(ngram) for ngram in ngrams]


company_names = names['NAMECOLUMN']
vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)
tf_idf_matrix = vectorizer.fit_transform(company_names)

Traceback (most recent call last):

  File ""&lt;ipython-input-4-687c2896bcf2&gt;"", line 17, in &lt;module&gt;
    tf_idf_matrix = vectorizer.fit_transform(company_names)

  File ""C:\Program Files\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 1305, in fit_transform
    X = super(TfidfVectorizer, self).fit_transform(raw_documents)

  File ""C:\Program Files\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 817, in fit_transform
    self.fixed_vocabulary_)

  File ""C:\Program Files\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 752, in _count_vocab
    for feature in analyze(doc):

  File ""&lt;ipython-input-4-687c2896bcf2&gt;"", line 10, in ngrams
    string = re.sub(r'[,-./]|\sBD',r'', string)

  File ""C:\Program Files\Anaconda3\lib\re.py"", line 182, in sub
    return _compile(pattern, flags).sub(repl, string, count)

TypeError: expected string or bytes-like object
</code></pre>

<p>And after trying it as a string:</p>

<pre><code>ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>

<p>I'm not even sure that I'm going in the right direction with this but it was the best link I could find that corresponded with what I need to do, and I'm not sure what I need to do better. It doesn't help that I'm a complete noob with Python :( So I hope you have some patience with me.</p>

<p>Alas, I'd be super grateful for suggestions on how to deal with the above problem and/or code. </p>

<p>Thanks so much in advance!!</p>

<p><strong>Edit:</strong> Completely forgot to mention that the ideal solution would match &amp; grab the complete row from my ugly table, as I need the information stored in the other columns for the names.</p>
","<python><n-gram>","2018-08-03 14:01:20","142","0","2","51674812","<p>I would suggest to have a look at <code>fuzzywuzzy</code> package to do this kind of matching. For your needs, I think filtering names with <code>fuzz.token_sort_ratio</code> or <code>fuzz.token_set_ratio</code> score greater than a certain threshold (say 75%) would be enough</p>

<pre><code>&gt;&gt;&gt; from fuzzywuzzy import fuzz
&gt;&gt;&gt; from itertools import takewhile
&gt;&gt;&gt; 
&gt;&gt;&gt; lstA = ['Neville Longbottomx', 'Severus Snape Slyth', 'Granger, Hermioone', 'Miss Lovegoo', 'Nott: Theodore', 'Mr Potter Gryffindor', 'Malfoy, Draco', 'Bulstrode, Millicent', 'McGonagall, Minerv', 'Seamus Finnigan Mister', 'Miss Abbott, Hannah', 'Ernie Macmillan M', 'Dumbledore, Albus', 'Parkinson, Pans"" Slyth']
&gt;&gt;&gt; lstB = ['Lovegood, Luna', 'Longbottom, Neville', 'Macmillan, Ernie', 'Nott, Theodore', 'Parkinson, Pansy']
&gt;&gt;&gt; 
&gt;&gt;&gt; dict((name,next(takewhile(lambda n: fuzz.token_sort_ratio(n, name)&gt;75, lstA), '')) for name in lstB)
{'Lovegood, Luna': '', 'Longbottom, Neville': 'Neville Longbottomx', 'Macmillan, Ernie': '', 'Nott, Theodore': '', 'Parkinson, Pansy': ''}
</code></pre>
"
"51674355","Matching a list of names to a column with bad data quality (Python)","<p>I have a table with 5 columns, one of which is a list of names in terrible data quality. I managed to clean it up as far as possible in R, but it still looks something like this (formatted as code for easier reading):</p>

<pre><code>Neville Longbottomx
Severus Snape Slyth
Granger, Hermioone
Miss Lovegoo
Nott: Theodore
Mr Potter Gryffindor
Malfoy, Draco
Bulstrode, Millicent
McGonagall, Minerv
Seamus Finnigan Mister
Miss Abbott, Hannah
Ernie Macmillan M
Dumbledore, Albus
Parkinson, Pans"" Slyth
</code></pre>

<p>Now, I have another list with names like this:</p>

<pre><code>Lovegood, Luna
Longbottom, Neville
Macmillan, Ernie
Nott, Theodore
Parkinson, Pansy
</code></pre>

<p>And I want to find the names in the second list in the first one. I looked up different articles on this and tried <a href=""https://bergvca.github.io/2017/10/14/super-fast-string-matching.html"" rel=""nofollow noreferrer"" title=""this method"">this method</a> because ngrams seemed like a smart way to go, but I first got this error:</p>

<pre><code>def ngrams(string, n=3):
    string = re.sub(r'[,-./]|\sBD',r'', string)
    ngrams = zip(*[string[i:] for i in range(n)])
    return [''.join(ngram) for ngram in ngrams]


company_names = names['NAMECOLUMN']
vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)
tf_idf_matrix = vectorizer.fit_transform(company_names)

Traceback (most recent call last):

  File ""&lt;ipython-input-4-687c2896bcf2&gt;"", line 17, in &lt;module&gt;
    tf_idf_matrix = vectorizer.fit_transform(company_names)

  File ""C:\Program Files\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 1305, in fit_transform
    X = super(TfidfVectorizer, self).fit_transform(raw_documents)

  File ""C:\Program Files\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 817, in fit_transform
    self.fixed_vocabulary_)

  File ""C:\Program Files\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 752, in _count_vocab
    for feature in analyze(doc):

  File ""&lt;ipython-input-4-687c2896bcf2&gt;"", line 10, in ngrams
    string = re.sub(r'[,-./]|\sBD',r'', string)

  File ""C:\Program Files\Anaconda3\lib\re.py"", line 182, in sub
    return _compile(pattern, flags).sub(repl, string, count)

TypeError: expected string or bytes-like object
</code></pre>

<p>And after trying it as a string:</p>

<pre><code>ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>

<p>I'm not even sure that I'm going in the right direction with this but it was the best link I could find that corresponded with what I need to do, and I'm not sure what I need to do better. It doesn't help that I'm a complete noob with Python :( So I hope you have some patience with me.</p>

<p>Alas, I'd be super grateful for suggestions on how to deal with the above problem and/or code. </p>

<p>Thanks so much in advance!!</p>

<p><strong>Edit:</strong> Completely forgot to mention that the ideal solution would match &amp; grab the complete row from my ugly table, as I need the information stored in the other columns for the names.</p>
","<python><n-gram>","2018-08-03 14:01:20","142","0","2","51674874","<p>You can use fuzzy matching algorithms :)</p>

<pre><code>from fuzzywuzzy import fuzz

a = ['Neville Longbottomx','Severus Snape Slyth','Granger, Hermioone','Miss Lovegoo',
    'Nott: Theodore','Mr Potter Gryffindor','Malfoy, Draco','Bulstrode, Millicent',
    'McGonagall, Minerv','Seamus Finnigan Mister','Miss Abbott, Hannah','Ernie Macmillan M',
    'Dumbledore, Albus','Parkinson, Pans"" Slyth']

b = ['Lovegood, Luna','Longbottom, Neville','Macmillan, Ernie','Nott, Theodore','Parkinson, Pansy']
get_match_a = []
for name1 in b:
    for name2 in a:
        if fuzz.partial_ratio(name2,name1)&gt;50: # Tune this to fit your need
            get_match_a.append(name2)
            #print(name1,':',name2,'||',fuzz.partial_ratio(name2,name1))
            #uncomment above to see the matching
</code></pre>

<p>As you can see below, it works very well. I hope this will guide you to where you want to go :)<a href=""https://i.stack.imgur.com/9YHb6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9YHb6.png"" alt=""enter image description here""></a></p>
"
"51589798","Efficient join technique for huger master table and even huger permission table","<p>I am working on a concept for database synchronization recently. 
The scenario is as follows:</p>

<ul>
<li><p>there is a master table ""Items"" with 1M+ rows in it</p>

<pre><code>    CREATE TABLE [dbo].[Item](
        [Id] [uniqueidentifier] NOT NULL,
        [Title] [nvarchar](50) NULL,
        [Modified] [datetime2](7) NOT NULL,
     CONSTRAINT [PK_Item] PRIMARY KEY CLUSTERED ([Id] ASC) ON [PRIMARY]
</code></pre></li>
<li><p>we want to sync data to a client in a very flexible way - so we are playing around with a ""Items_sync"" table, that contains an entry for every user and every item they should download during a sync process.</p>

<pre><code>CREATE TABLE [dbo].[Item_syncfilter](
    [Id] [bigint] IDENTITY(1,1) NOT NULL,
    [ItemId] [uniqueidentifier] NOT NULL,
    [Modified] [datetime2](7) NOT NULL,
    [IsDeleted] [bit] NOT NULL,
    [UserId] [bigint] NOT NULL,
 CONSTRAINT [PK_Item_syncfilter] PRIMARY KEY CLUSTERED ([Id] ASC) ON [PRIMARY]
</code></pre></li>
</ul>

<p>Now what makes this a bit complicated is the following:
There are numerous reasons, why a particular user may get the permission to download a particular row (he is added to the contributors group, he is added to the administrators group, an item is directly assigned to him/her).
Thus, there may be multiple rows for the same user for a single item stating that she is allowed to download that item.</p>

<p>Also, the sync process needs to work incrementally.
Meaning: 
 * If user Andrew has access to item A, and it is modified, next time he syncs he should receive the newest version
 * Is user Andrew did not have access to item A, but then he is added to the administrators group (=> gets a corresponding Item_sync entry) he should download the item the next time he syncs.
 * If Andrew had already synced item A and is added to the administrators group, nothing should be synced.</p>

<p>Now what we came up with up until now is the following query:</p>

<pre><code>declare @userid bigint;
declare @date datetime2(7);
set @date = '2018-05-02 13:00:00.0000000';
set @userid = 5;

select i.*, 0 as Toombstoned from item i
where 
-- clause 1: get all modified items where there exists at least one non-deleted sync row
(i.modified &gt;= @date
    -- and there exists at least one non-deleted syncfilter
    and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0))
-- clause 2: get all items, which were not modified, but their sync rows are newer (toombstoned or not)
or (i.modified &lt;  @date
    -- and there is at least one younger, non-deleted syncfilter (permission was added to user)
    and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0 and modified &gt;  @date)
    -- make sure this item was not already synced by an older valid and non-deleted filter
    and not exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0 and modified &lt;  @date))

union all
select i.*, 1 as Toombstoned from item i
where 
-- clause 3: get all toombstoned items
--                  - where no non-deleted syncfilter exists
--                  - and there is a deleted sync filter younger than ""date""
(not exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0)
    and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 1 and modified &gt;  @date))
</code></pre>

<p>However this performs quite bad due to the 5 usages of ""exists"": i.e. for 1 million rows in the main table, the query runs for 5 seconds and the STATISTICS IO output shows a looooot of reads, even if the query only returns a small subset of data.</p>

<p>Can you give me any hint how we could improve this query dramatically?</p>

<p><strong>UPDATE</strong>
Thanks for your responses.
The following sql snippet shows 
* the complete table schema
* including indexes that I use
* and some testdata which showcases how the query works</p>

<pre><code>-- ########################
-- ## Sync Item
-- ########################

CREATE TABLE [dbo].[Item](
    [Id] [uniqueidentifier] NOT NULL,
    [Title] [nvarchar](100) NULL,
    [Modified] [datetime2](7) NOT NULL,
 CONSTRAINT [PK_Item] PRIMARY KEY CLUSTERED ([Id] ASC) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]
) ON [PRIMARY]
GO
ALTER TABLE [dbo].[Item] ADD  CONSTRAINT [DF_Item_Id]  DEFAULT (newid()) FOR [Id]
GO
ALTER TABLE [dbo].[Item] ADD  CONSTRAINT [DF_Item_Modified]  DEFAULT (getutcdate()) FOR [Modified]
GO

CREATE NONCLUSTERED INDEX [IX_ItemModified] ON [dbo].[Item]
(
    [Modified] DESC
) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, SORT_IN_TEMPDB = OFF, DROP_EXISTING = OFF, ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]
GO


-- ########################
-- ## sync filter
-- ########################

CREATE TABLE [dbo].[Item_syncfilter](
    [Id] [bigint] IDENTITY(1,1) NOT NULL,
    [ItemId] [uniqueidentifier] NOT NULL,
    [Modified] [datetime2](7) NOT NULL,
    [IsDeleted] [bit] NOT NULL,
    [UserId] [bigint] NOT NULL,
 CONSTRAINT [PK_Item_syncfilter] PRIMARY KEY CLUSTERED ([Id] ASC) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]
) ON [PRIMARY]
GO
ALTER TABLE [dbo].[Item_syncfilter] ADD  CONSTRAINT [DF_Item_syncfilter_Modified]  DEFAULT (getutcdate()) FOR [Modified]
GO
ALTER TABLE [dbo].[Item_syncfilter] ADD  CONSTRAINT [DF_Item_syncfilter_IsDeleted]  DEFAULT ((0)) FOR [IsDeleted]
GO
ALTER TABLE [dbo].[Item_syncfilter] ADD  CONSTRAINT [DF_Item_syncfilter_UserId]  DEFAULT (CONVERT([int],((20)+(1))*rand())) FOR [UserId]
GO

CREATE NONCLUSTERED INDEX [IX_SyncItemModified] ON [dbo].[Item_syncfilter]
(
    [UserId] ASC,
    [ItemId] ASC,
    [IsDeleted] ASC,
    [Modified] DESC
)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, SORT_IN_TEMPDB = OFF, DROP_EXISTING = OFF, ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]
GO
CREATE NONCLUSTERED INDEX [IX_SyncItemItemId] ON [dbo].[Item_syncfilter]
(
    [ItemId] ASC
)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, SORT_IN_TEMPDB = OFF, DROP_EXISTING = OFF, ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]
GO


-- ########################
-- ## TestData
-- ########################

INSERT [dbo].[Item] ([Id], [Title], [Modified]) VALUES (N'a14ae781-b595-4fa8-942f-3abf8d848bdf', N'1 new deleted, 1 old still valid  NOTINSYNC', CAST(N'2018-05-01T14:10:25.8400000' AS DateTime2))
INSERT [dbo].[Item] ([Id], [Title], [Modified]) VALUES (N'45b71309-49d9-4457-a784-52dcc1331ec2', N'Modified and all filters new', CAST(N'2018-05-03T06:33:04.7200000' AS DateTime2))
INSERT [dbo].[Item] ([Id], [Title], [Modified]) VALUES (N'cf01ebde-7f11-4bad-a32c-54caa6fca14b', N'No new filter NOTINSYNC', CAST(N'2018-05-01T14:10:11.0833333' AS DateTime2))
INSERT [dbo].[Item] ([Id], [Title], [Modified]) VALUES (N'80fc71ff-e984-4dae-bdf1-98e02d27c926', N'All deleted', CAST(N'2018-05-02T14:09:48.6200000' AS DateTime2))
INSERT [dbo].[Item] ([Id], [Title], [Modified]) VALUES (N'a5fa6d29-5c2b-4edb-8390-aeec44232368', N'Modified', CAST(N'2018-05-02T14:09:48.6200000' AS DateTime2))
INSERT [dbo].[Item] ([Id], [Title], [Modified]) VALUES (N'5995209d-c571-40b8-9ff6-b650add6ffbf', N'Some filters new NOTINSYNC', CAST(N'2018-05-01T14:10:04.2900000' AS DateTime2))
INSERT [dbo].[Item] ([Id], [Title], [Modified]) VALUES (N'd79a3967-780c-46e3-b1ec-e6038214e711', N'All filters new', CAST(N'2018-05-01T14:10:04.2900000' AS DateTime2))
SET IDENTITY_INSERT [dbo].[Item_syncfilter] ON 

INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (1, N'a5fa6d29-5c2b-4edb-8390-aeec44232368', CAST(N'2018-05-01T14:13:30.5000000' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (2, N'a5fa6d29-5c2b-4edb-8390-aeec44232368', CAST(N'2018-05-01T14:13:37.8133333' AS DateTime2), 1, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (3, N'd79a3967-780c-46e3-b1ec-e6038214e711', CAST(N'2018-05-02T16:15:04.5933333' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (4, N'd79a3967-780c-46e3-b1ec-e6038214e711', CAST(N'2018-05-02T16:15:07.1266667' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (5, N'a14ae781-b595-4fa8-942f-3abf8d848bdf', CAST(N'2018-05-01T14:13:37.8133333' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (6, N'a14ae781-b595-4fa8-942f-3abf8d848bdf', CAST(N'2018-05-02T14:15:31.7666667' AS DateTime2), 1, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (7, N'cf01ebde-7f11-4bad-a32c-54caa6fca14b', CAST(N'2018-05-01T14:13:37.8133333' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (8, N'cf01ebde-7f11-4bad-a32c-54caa6fca14b', CAST(N'2018-05-01T14:13:37.8133333' AS DateTime2), 1, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (9, N'80fc71ff-e984-4dae-bdf1-98e02d27c926', CAST(N'2018-05-01T14:13:37.8133333' AS DateTime2), 1, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (10, N'80fc71ff-e984-4dae-bdf1-98e02d27c926', CAST(N'2018-04-30T14:13:37.8133333' AS DateTime2), 1, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (11, N'5995209d-c571-40b8-9ff6-b650add6ffbf', CAST(N'2018-04-30T14:13:37.8133333' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (12, N'5995209d-c571-40b8-9ff6-b650add6ffbf', CAST(N'2018-05-02T16:39:20.5066667' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (13, N'5995209d-c571-40b8-9ff6-b650add6ffbf', CAST(N'2018-05-02T16:39:21.7066667' AS DateTime2), 1, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (14, N'45b71309-49d9-4457-a784-52dcc1331ec2', CAST(N'2018-05-03T06:33:34.7900000' AS DateTime2), 0, 1)
INSERT [dbo].[Item_syncfilter] ([Id], [ItemId], [Modified], [IsDeleted], [UserId]) VALUES (15, N'45b71309-49d9-4457-a784-52dcc1331ec2', CAST(N'2018-05-03T06:33:38.0300000' AS DateTime2), 1, 1)
SET IDENTITY_INSERT [dbo].[Item_syncfilter] OFF
</code></pre>

<p>To generate the testdata I used:</p>

<pre><code>   -- ## Create many items items
   DECLARE @startnum INT=1;
   DECLARE @endnum INT=5000;

   WITH gen AS (
       SELECT @startnum AS num
       UNION ALL
       SELECT num+1 FROM gen WHERE num+1&lt;=@endnum
   ) 

   insert into [Item] ([Id], [Title], [Modified])
   (SELECT newId() as [Id]
         ,[Title]  + ' -#'+ CONVERT(varchar(1000), n.num) as [Title]
         ,[Modified]
     FROM [Item]
     cross join gen as n)
   option (maxrecursion 10000);

   select count(*) as item_count from item;

   -- ## generate syncfilter rows for 10 users
   set @startNum = 1;
   set @endNum = 10;

   WITH gen AS (
       SELECT @startnum AS num
       UNION ALL
       SELECT num+1 FROM gen WHERE num+1&lt;=@endnum
   )    

    insert into item_syncfilter ([ItemId],[Modified],[IsDeleted],[UserId])
    (select i.[Id], DATEADD(month, -6, i.[Modified]), 0 as IsDeleted, n.num as [Userid] from item i 
       left outer join item_syncfilter s on s.itemid = i.id
         cross join gen as n
       where s.id is null)
    option (maxrecursion 10000);

    select count(*) item_syncfilter_count from Item_syncfilter;
</code></pre>

<p>this creates 35K items and 350K syncfilte rows</p>

<p>the statistics IO output is </p>

<pre><code>(15003 rows affected)
Table 'Item_syncfilter'. Scan count 35013, logical reads 105648, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
Table 'Item'. Scan count 1, logical reads 610, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.

(21 rows affected)

(1 row affected)
</code></pre>

<p>You can download the execution plan from <a href=""https://1drv.ms/u/s!AhVbOiFFuzx_1yFHFWnUwEks1e_f"" rel=""nofollow noreferrer"">here</a></p>
","<sql><sql-server><sql-server-2008><data-synchronization>","2018-07-30 08:24:36","46","1","1","51602579","<p>Update:</p>

<p>I went along looked at the execution plan of the separate sub-clauses of the query.</p>

<p>For clause 3 (finding tombstoned items), the execution plan showed that the following index would improve the performance:</p>

<pre><code>    CREATE NONCLUSTERED INDEX [IX_SyncItemDeletedItems] ON [dbo].[Item_syncfilter] 
    (
        [IsDeleted],
        [UserId],
        [Modified])
    INCLUDE ([ItemId])
</code></pre>

<p>Running only clause 3 of the query with ""statistics IO on"":</p>

<pre><code>    set statistics io on

    declare @userid bigint;
    declare @date datetime2(7);
    set @date = '2018-05-02 13:00:00.0000000';
    set @userid = 5;

    select i.*, 1 as Toombstoned from item i
    where 
    -- clause 3: get all toombstoned items
    --                  - where no non-deleted syncfilter exists
    --                  - and there is a deleted sync filter younger than ""date""
    (not exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0)
        and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 1 and modified &gt;  @date))
</code></pre>

<p>shows before:</p>

<pre><code>  (0 rows affected)
  Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
  Table 'Item_syncfilter'. Scan count 1, logical reads 1433, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.

  (1 row affected)
</code></pre>

<p>after:  </p>

<pre><code>  (0 rows affected)
  Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
  Table 'Item_syncfilter'. Scan count 1, logical reads 3, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.

  (1 row affected)
</code></pre>

<p>... so this greatly reduces the number of logical reads!</p>

<p>For clause 1 and 2, it was more interesting. If run separately, they performed quite well, but combined, they resulted in a terrible execution plan:</p>

<pre><code>    set statistics io on

    declare @userid bigint;
    declare @date datetime2(7);
    set @date = '2018-05-02 13:00:00.0000000';
    set @userid = 5;

    select i.*, 0 as Toombstoned from item i
    where 
    -- clause 1: get all modified items where there exists at least one non-deleted sync row
    (i.modified &gt;= @date
        -- and there exists at least one non-deleted syncfilter
        and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0))
    -- clause 2: get all items, which were not modified, but their sync rows are newer (toombstoned or not)
    or (i.modified &lt;  @date
        -- and there is at least one younger, non-deleted syncfilter (permission was added to user)
        and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0 and modified &gt;  @date)
        -- make sure this item was not already synced by an older valid and non-deleted filter
        and not exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0 and modified &lt;  @date))
</code></pre>

<p>Returns</p>

<pre><code>    (0 rows affected)
    Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
    Table 'Item_syncfilter'. Scan count 229376, logical reads 688128, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
    Table 'Item'. Scan count 1, logical reads 3980, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.

    (1 row affected)
</code></pre>

<p>The reason is the following execution step in the execution plan:</p>

<p><a href=""https://i.stack.imgur.com/TWJLx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TWJLx.png"" alt=""enter image description here""></a></p>

<p>So as you can see, sql server scans the clustered index for items where the modified date is >= @date OR &lt; @date, which more or less returns the whole table -> thus, those many reads</p>

<p>So what I did was to simply split the two clauses, which were combined using ""OR"" into two separate queries which are just combined using UNION ALL:</p>

<pre><code>    set statistics io on

    declare @userid bigint;
    declare @date datetime2(7);
    set @date = '2018-05-02 13:00:00.0000000';
    set @userid = 5;

    select i.*, 0 as Toombstoned from item i
    where 
    -- clause 1: get all modified items where there exists at least one non-deleted sync row
    (i.modified &gt;= @date
        -- and there exists at least one non-deleted syncfilter
        and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0))

    -- clause 2: get all items, which were not modified, but their sync rows are newer (toombstoned or not)
    union all
    select i.*, 0 as Toombstoned from item i
        where i.modified &lt;  @date
        -- and there is at least one younger, non-deleted syncfilter (permission was added to user)
        and exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0 and modified &gt;  @date)
        -- make sure this item was not already synced by an older valid and non-deleted filter
        and not exists (select id from item_syncfilter where itemid = i.id and userid = @userid and isdeleted = 0 and modified &lt;  @date)
</code></pre>

<p>which interestingly yields the following statistics</p>

<pre><code>    (0 rows affected)
    Table 'Workfile'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
    Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
    Table 'Item_syncfilter'. Scan count 3, logical reads 12, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
    Table 'Item'. Scan count 2, logical reads 8, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.

    (1 row affected)
</code></pre>

<p>=> so from 229376 down to 3, from 688128 down to 12, etc.
This is a huge gain!</p>
"
"51554108","Java: best practice to update data online when internet can be interrupted","<p><br>
I have 2 applications:<br></p>

<ul>
<li>desktop (java)<br></li>
<li>web (symfony)<br></li>
</ul>

<p>I have some data in the desktop app that must be consistent with the data in the web app.<br>
So basically I send a <code>POST</code> request from the desktop app to the web app to update online data.<br><br>
But the problem is that the internet cannot always be available when I send my request and in the same time I can't prevent the user from updating the desktop data<br>
So far, this is what I have in mind to make sure to synchronize data when the internet is available.<br><br>
<a href=""https://i.stack.imgur.com/T7vMO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T7vMO.png"" alt=""enter image description here""></a>
Am I on the right direction or not ?<br>
If not, I hope you guys put me in the right path to achieve my goal in a professional way.<br>
Any link about this kind of topics will be appreciated. </p>
","<java><data-synchronization>","2018-07-27 08:39:46","157","5","3","51554375","<p>In this case the usefull pattern is to assume that sending data is asynchronous by default. The data, after collecting, are stored in some intermediate structure and wait for a sutable moment to be send. I think the queue could be useful because it can be backend with a database and prevent data lost in case of the sending server failure. Separate thread (e.g. a job) check for data in the queue and if exists, read them and try to send. If sending was performed correctly the data are removed from queue. If failure occurs, the data stays in queue and an attempt will be made to send them next time. </p>
"
"51554108","Java: best practice to update data online when internet can be interrupted","<p><br>
I have 2 applications:<br></p>

<ul>
<li>desktop (java)<br></li>
<li>web (symfony)<br></li>
</ul>

<p>I have some data in the desktop app that must be consistent with the data in the web app.<br>
So basically I send a <code>POST</code> request from the desktop app to the web app to update online data.<br><br>
But the problem is that the internet cannot always be available when I send my request and in the same time I can't prevent the user from updating the desktop data<br>
So far, this is what I have in mind to make sure to synchronize data when the internet is available.<br><br>
<a href=""https://i.stack.imgur.com/T7vMO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T7vMO.png"" alt=""enter image description here""></a>
Am I on the right direction or not ?<br>
If not, I hope you guys put me in the right path to achieve my goal in a professional way.<br>
Any link about this kind of topics will be appreciated. </p>
","<java><data-synchronization>","2018-07-27 08:39:46","157","5","3","51554522","<p>This is a typical scenario when you want to send a message to an un-transactional external system in a transaction and you need to garantee that data will be transfered to the external system as soon as possible without losing it.</p>

<p>2 solutions come up in my mind, maybe the second fits better to your architecture.</p>

<p><strong>Use case 1)</strong></p>

<p>You can use message queue + redelivery limit setting with dead letter pattern. In t that case you need to have an application server. </p>

<p><a href=""https://www.enterpriseintegrationpatterns.com/patterns/messaging/DeadLetterChannel.html"" rel=""nofollow noreferrer"">Here</a> you can read details about the Dead letter pattern.</p>

<p><a href=""http://blog.sysco.no/files/guides/JMSErrorDestinationRedeliveryLimit.pdf"" rel=""nofollow noreferrer"">This</a> document explain how redelivery limit works on Weblogic server.</p>

<p><strong>Use case 2)</strong></p>

<p>You can create an interface table in the database of the destop application. Then insert your original data into database and insert a new record into the interface table as well (all in same transaction). The data what you want to POST needs to be inserted into the interface table as well. The status flag of the new record in the interface table can be ""<em>ARRIVED</em>"". Then create an independent timer in your desktop app which search periodically for records in the interface table with status ""<em>ARRIVED</em>"". This timer controlled process will try to POST data to webservice. If the HTTP response is 200 then update the status of the record to ""<em>SENT</em>"".</p>

<p>Boot can work like a charm.</p>
"
"51554108","Java: best practice to update data online when internet can be interrupted","<p><br>
I have 2 applications:<br></p>

<ul>
<li>desktop (java)<br></li>
<li>web (symfony)<br></li>
</ul>

<p>I have some data in the desktop app that must be consistent with the data in the web app.<br>
So basically I send a <code>POST</code> request from the desktop app to the web app to update online data.<br><br>
But the problem is that the internet cannot always be available when I send my request and in the same time I can't prevent the user from updating the desktop data<br>
So far, this is what I have in mind to make sure to synchronize data when the internet is available.<br><br>
<a href=""https://i.stack.imgur.com/T7vMO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T7vMO.png"" alt=""enter image description here""></a>
Am I on the right direction or not ?<br>
If not, I hope you guys put me in the right path to achieve my goal in a professional way.<br>
Any link about this kind of topics will be appreciated. </p>
","<java><data-synchronization>","2018-07-27 08:39:46","157","5","3","51555124","<p>You can solve it many way. Here give 2 way:</p>

<p>1.You can use <code>circuit breaker pattern</code>. You can get link about it from <a href=""https://stackoverflow.com/questions/30285637/circuit-breaker-design-pattern-implementation"">here</a></p>

<ol start=""2"">
<li>You can use <a href=""https://docs.oracle.com/javaee/6/tutorial/doc/bncdq.html"" rel=""nofollow noreferrer"">JMS</a> concept to manage this.</li>
</ol>
"
"51472745","Data governance with scala/spark","<p>I have an ETL to analyze large data and all my tables are DataFrames with Spark 2.2.X. Now, I have to add data governance in order to know what is the origin of the data. For example:</p>

<p>Table A  </p>

<pre><code>| Col1 | Col2 |  
| ---- | ---- |  
| test | hello |  
| test3 | bye |
</code></pre>

<p>Table B  </p>

<pre><code>| Col1 | Col2 |  
| ---- | ---- |  
| test2 | hey |  
| test3 | bye |
</code></pre>

<p>Now I have my two tables, what I do is a join by <code>Col1</code> and <code>Col2 + Col2</code>. Resulting table:</p>

<p>Final Table  </p>

<pre><code>| Col1 | Col2 |  
| ---- | ---- |  
|test3 | byebye|  
</code></pre>

<p>My question is, Is there any function in Spark DataFrame, API or something that does not make me change the code so much and I can show all transformations in the DataFrame that I have?</p>
","<scala><apache-spark><apache-spark-sql><data-governance>","2018-07-23 06:29:14","336","0","1","51479883","<p>If you want a quick solution for this, you can have a look at <code>RDD#toDebugString</code>. You can call the <code>rdd</code> method on your <code>DataFrame</code> and than show its lineage through this method.</p>

<p>Here is an example from <a href=""https://legacy.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details"" rel=""nofollow noreferrer"">Jacek Laskowski's book ""Mastering Apache Spark""</a>:</p>

<pre><code>scala&gt; val wordCount = sc.textFile(""README.md"").flatMap(_.split(""\\s+"")).map((_, 1)).reduceByKey(_ + _)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at &lt;console&gt;:24

scala&gt; wordCount.toDebugString
res13: String =
(2) ShuffledRDD[21] at reduceByKey at &lt;console&gt;:24 []
 +-(2) MapPartitionsRDD[20] at map at &lt;console&gt;:24 []
    |  MapPartitionsRDD[19] at flatMap at &lt;console&gt;:24 []
    |  README.md MapPartitionsRDD[18] at textFile at &lt;console&gt;:24 []
    |  README.md HadoopRDD[17] at textFile at &lt;console&gt;:24 []
</code></pre>

<p>This snippet, along with a detailed explanation about RDD lineage and <code>toDebugString</code> is available <a href=""https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-lineage.html#__a_id_todebugstring_a_getting_rdd_lineage_graph_code_todebugstring_code_method"" rel=""nofollow noreferrer"">here</a>.</p>
"
"51261515","DB synchronization on Visual Studio 2015 is hanged","<p>I tried to sync database on Visual Studio 2015 after creating a project, EDT, Enum and a Table in order to create a new screen on Dynamics 365.</p>

<p>When I tried to synchronize it, it was stopped in the middle during schema checking process. Though it seems that the DB synchronization doesn't have problem for the first few minutes, it always stops during this process as I describe below. </p>

<p>Log Details:</p>

<blockquote>
  <p>""Schema has not changed between new table 'DPT_TableDT' and old table
  'DPT_TableDT' with table id '3997'. Returning from
  ManagedSyncTableWorker.ExecuteModifyTable() Syncing Table Finished:
  DPT_TableDT. Time elapsed: 0:00:00:00.0010010""</p>
</blockquote>

<p>Could you tell me how to solve this issue? </p>

<p>Thanks in advance.</p>

<p>Full database synchronization log
<a href=""https://i.stack.imgur.com/YWIhc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YWIhc.png"" alt=""log""></a></p>

<p>DB Sync Log</p>
","<visual-studio><axapta><x++><data-synchronization><dynamics-365-operations>","2018-07-10 09:19:11","397","0","2","51593915","<p>From what you've described and also shown in your screenshot, this does not look like an error but is simply describing X++ and Dynamics AX/365FO behaviour.</p>

<p>When you say that it ""doesn't have a problem for the first few minutes"" I'm guessing you're just not being patient enough. Full database syncs should generally take between 10-30 minutes, but can take shorter or longer depending on a variety of factors such as how much horsepower your development environment has, how many changes are being sync'd etc.  I would wait <em>at least</em> one hour before considering the possibility that the sync engine has errors (or even run it overnight and see what information it has for you in the morning).</p>

<p>The message you've posted from the log (""Schema has not changed"") isn't an error message; it is just an informational log from the sync engine. It is simply letting you know that the table did not have any changes to propagate to SQL Server.</p>

<p><strong>Solution</strong>: Run the sync overnight and post a screenshot of the results or the error list window in Visual Studio.</p>
"
"51261515","DB synchronization on Visual Studio 2015 is hanged","<p>I tried to sync database on Visual Studio 2015 after creating a project, EDT, Enum and a Table in order to create a new screen on Dynamics 365.</p>

<p>When I tried to synchronize it, it was stopped in the middle during schema checking process. Though it seems that the DB synchronization doesn't have problem for the first few minutes, it always stops during this process as I describe below. </p>

<p>Log Details:</p>

<blockquote>
  <p>""Schema has not changed between new table 'DPT_TableDT' and old table
  'DPT_TableDT' with table id '3997'. Returning from
  ManagedSyncTableWorker.ExecuteModifyTable() Syncing Table Finished:
  DPT_TableDT. Time elapsed: 0:00:00:00.0010010""</p>
</blockquote>

<p>Could you tell me how to solve this issue? </p>

<p>Thanks in advance.</p>

<p>Full database synchronization log
<a href=""https://i.stack.imgur.com/YWIhc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YWIhc.png"" alt=""log""></a></p>

<p>DB Sync Log</p>
","<visual-studio><axapta><x++><data-synchronization><dynamics-365-operations>","2018-07-10 09:19:11","397","0","2","52836822","<p>I've recently been stymied by a long running application where Access v2003 replicas refused to synchronize. The message returned was ""not enough memory"". This was on machines running Windows 10. The only way I was able to force synchronizing was to move the replicas onto an old machine still running Windows 98 with Office XP, which allowed synchronizing and conflict resolution. When I moved the synchronized files back to the Windows 10 machine they still would not synchronize. </p>

<p>I finally had to create a blank database and link to a replica, then use make-table queries to select only data fields to create new tables. I was then able to create new replicas that would synchronize. </p>

<p>From this I've come to suspect the following: </p>

<ol>
<li>Something in Windows 10 has changed and caused the problem with synchronizing/conflict resolution. </li>
<li>Something in the hidden/protected fields added to the replica sets is seen as a problem under Windows 10 that is not a problem under Windows 98. </li>
<li>One thing I noticed is that over the years the number of replicas in the synchronizing list had grown to over 900 sets, but the only way to clear the table was to create a new clean database. </li>
</ol>
"
"51218723","IDQ input parameter file error in windows","<p>In IDQ process, I have generated a workflow parameter in server location. </p>

<p>When I am trying to call with the batch script the parameter file with different source file names facing an error which shows the parameter file is not found.</p>

<p>It was not able to pick the diff file names with multiple input parameter's.</p>
","<batch-file><informatica><informatica-powercenter><data-quality>","2018-07-06 23:26:41","155","-1","1","51278925","<p>In IDQ there is option in values where the default value must be set for the port where the name is specified.</p>

<p>It might be taking the name mentioned there and giving error for the parameter value.</p>
"
"51176514","data synchronization between local system and a remote central server","<p><strong>Scenario:</strong> There are many local servers (500+) and a central remote server. client application connects to local server to perform some business process and the resulting data is saved in the local data store (PgSQL with many tables)</p>

<p>Requirements:</p>

<ul>
<li>application should be able to switch to the central remote server
anytime and continue to work. For this to happen, data needs to be replicated and synced (near real-time data sync) between the local servers and the central remote server</li>
</ul>

<p>Any suggestions??
It would be easier if there was just one table to sync ( store the final state of the data). Instead, the data is spread across multiple tables.</p>
","<postgresql><rest><data-synchronization>","2018-07-04 14:59:04","857","1","1","51182938","<p>There is both an <a href=""https://wiki.postgresql.org/wiki/FAQ#What_replication_solutions_are_available.3F"" rel=""nofollow noreferrer"">FAQ</a> and <a href=""https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling"" rel=""nofollow noreferrer"">an entire section</a> in the postgres documentation devoted to this topic.</p>

<p>I suggest you read the <a href=""https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling#Comparison_matrix"" rel=""nofollow noreferrer"">comparison matrix</a> and see what best fits your use case.</p>

<p>As it stands, your question is too broad to offer any meaningful answer to... but probably the answer is, there's no really good, out of the box solution for this in postgres; pick one of the 3rd party solutions, and ask a more specific question about how to configure / use that solution.</p>
"
"51141775","Can I sync data from app to the server even if the app is closed?","<p>I'm developing an Ionic project that stores data locally when the user is offline (in a SQLite database) and should sync this data when the user gets back online, storing this data in a MySQL database in the server.</p>

<p>I check in the app if the user is online from time to time and I'm using this <a href=""https://ionicframework.com/docs/native/background-mode/"" rel=""nofollow noreferrer"">Background Mode</a> plugin to run the app in background. However, I'm facing some problems. When the user has the app closed and later gets online, the app isn't really running, so I can't send the data to the server. As far as I can tell, this plugin only helps if the user doesn't ""clean"" the app from memory (by swiping it off from the recent apps list), and even the plugin's author says to ""Use the plugin by your own risk!"".</p>

<p>I found this <a href=""https://stackoverflow.com/questions/41383080/sync-data-with-server-when-app-is-closed"">question</a> but it doesn't really help my case.</p>

<p>Is there any way that I can sync this data with the server even if the app is closed?</p>
","<ionic-framework><synchronization><ionic3><data-synchronization>","2018-07-02 18:19:55","943","0","2","51143369","<p>I have no experience with MySql, however with Google Firestore such functionality is very easily implementable. I recommend you give it a try, especially since nonSql databases are beginning to become the new norm. </p>
"
"51141775","Can I sync data from app to the server even if the app is closed?","<p>I'm developing an Ionic project that stores data locally when the user is offline (in a SQLite database) and should sync this data when the user gets back online, storing this data in a MySQL database in the server.</p>

<p>I check in the app if the user is online from time to time and I'm using this <a href=""https://ionicframework.com/docs/native/background-mode/"" rel=""nofollow noreferrer"">Background Mode</a> plugin to run the app in background. However, I'm facing some problems. When the user has the app closed and later gets online, the app isn't really running, so I can't send the data to the server. As far as I can tell, this plugin only helps if the user doesn't ""clean"" the app from memory (by swiping it off from the recent apps list), and even the plugin's author says to ""Use the plugin by your own risk!"".</p>

<p>I found this <a href=""https://stackoverflow.com/questions/41383080/sync-data-with-server-when-app-is-closed"">question</a> but it doesn't really help my case.</p>

<p>Is there any way that I can sync this data with the server even if the app is closed?</p>
","<ionic-framework><synchronization><ionic3><data-synchronization>","2018-07-02 18:19:55","943","0","2","51146488","<p>Yes, you can. There are many ways to do it, according to the needs of the project. 
One way could be to store data in LocalStorage, create a service that runs when you open the app to verify if the data has been modified.
Another is using the Ionic Native's BackgrounMode plugin, (<a href=""https://ionicframework.com/docs/native/background-mode/"" rel=""nofollow noreferrer"">https://ionicframework.com/docs/native/background-mode/</a>) and execute a function every one period of time. These are two possible ways, you can share more information about the project to see some best practice.</p>
"
"51109098","IDQ input parameter file error","<p>In IDQ infacmd I am trying to execute multiple wf with source input parameter file, The first infacmd gets success but the second infacmd mapping fails because of input parameter is taking the default value not the assigned value.</p>
","<informatica><data-quality>","2018-06-29 20:38:50","359","0","1","51128952","<p>I have got resolved  it, I have created a parameter in workflow level and assigned the mapping parameter to wf parameter and called the wf level parameter in the infacmd . It just  done my work.</p>
"
"50949354","Synchronize data from Oracle to PostgreSQL","<p>We would like to synchronize data (insert, update) from Oracle (11g) to PostgreSQL (10). Our approach was the following:</p>

<ul>
<li>A trigger on the table in Oracle updates a column with nextval from a sequence before insert and update.</li>
<li>PostgreSQL knows the last sequence number processed and fetches the rows from Oracle > lastSequenceNumberFetched.</li>
</ul>

<p>We now have the following problem:</p>

<ul>
<li>Session 1 in Oracle inserts a row, sequence number (let's say 45) is written but no COMMIT is done in Oracle.</li>
<li>Session 2 in Oracle inserts a row, sequence number is written (let's say 49 (because sequences in Oracle can have gaps)) and a COMMIT is done in Oracle.</li>
<li>Session in PostgreSQL fetches rows from Oracle with sequenceNumber > 44 (because the lastSequenceNumberFetched is 44) and gets the row with sequenceNumber 49. So this is the new lastSequenceNumberFetched.</li>
<li>Session 1 in Oracle makes a commit.</li>
<li>Session in PostgreSQL fetches rows from Oracle with sequenceNumber > 49. Problem is that the row with sequenceNumber 45 is never fetched.</li>
</ul>

<p>Are there any better approaches for our use case avoiding our problem with missing data?</p>
","<database><oracle><postgresql><data-synchronization>","2018-06-20 13:25:07","3591","1","3","50949731","<p>Is this purely a data project or do you have some client here. If you do have a middle tier you could use an ORM to abstract some of this and do writes to both. Do you care whether the sequences are the same? It would be possible to do something like collect all the data to synchronize since a particular timestamp (every table would have to have a UTC timestamp) and then take a hash of all the data and compare with what is in Postgres.</p>

<p>It might be useful to have some more of your requirements for the synchronization of data and the reasoning behind this e.g.</p>

<p>Do the keys need to be the same against both environments? Why?
Who views the data, is the same consumer looking at both sources.
Why wouldn't you just use an ORM to target only one db why do you need oracle and postgres?</p>
"
"50949354","Synchronize data from Oracle to PostgreSQL","<p>We would like to synchronize data (insert, update) from Oracle (11g) to PostgreSQL (10). Our approach was the following:</p>

<ul>
<li>A trigger on the table in Oracle updates a column with nextval from a sequence before insert and update.</li>
<li>PostgreSQL knows the last sequence number processed and fetches the rows from Oracle > lastSequenceNumberFetched.</li>
</ul>

<p>We now have the following problem:</p>

<ul>
<li>Session 1 in Oracle inserts a row, sequence number (let's say 45) is written but no COMMIT is done in Oracle.</li>
<li>Session 2 in Oracle inserts a row, sequence number is written (let's say 49 (because sequences in Oracle can have gaps)) and a COMMIT is done in Oracle.</li>
<li>Session in PostgreSQL fetches rows from Oracle with sequenceNumber > 44 (because the lastSequenceNumberFetched is 44) and gets the row with sequenceNumber 49. So this is the new lastSequenceNumberFetched.</li>
<li>Session 1 in Oracle makes a commit.</li>
<li>Session in PostgreSQL fetches rows from Oracle with sequenceNumber > 49. Problem is that the row with sequenceNumber 45 is never fetched.</li>
</ul>

<p>Are there any better approaches for our use case avoiding our problem with missing data?</p>
","<database><oracle><postgresql><data-synchronization>","2018-06-20 13:25:07","3591","1","3","50949941","<ol>
<li><p>In case you don't have delete operations in your tables and the tables are not very big then I suggest to use Oracle System Change Number (SCN) on the row level which is returned by the pseudo column <code>ORA_ROWSCN</code> (<a href=""https://docs.oracle.com/database/121/SQLRF/pseudocolumns007.htm"" rel=""nofollow noreferrer"">link</a>). This is the commit time presented by number. By default the SCN is tracked for the data block, but you can enable tracking on the row level (keyword <code>rowdependencies</code>). So you have to recreate your table with this keyword. At the sync procedure launch you get the current scn by the function call <code>dbms_flashback.get_system_change_number</code>, then scan all tables <code>where ora_rowscn between _last_scn_value_ and _current_scn_value_</code>. The disadvantage is that this pseudo columns is not indexed, so you will have full table scans, which is slow for big tables.</p></li>
<li><p>If you use delete statements then you have to track the records which were deleted. For this purpose you can use one log table having the following columns: table_name, table_id_value, operation (insert/update/delete). Table is filled by the trigger on base tables. So for your case when session 1 commits data in base table - then you have the record in log table to process. And you don't see it until the session commits. So no issues with sequence numbers that you described.</p></li>
</ol>

<p>Hope that helps.</p>
"
"50949354","Synchronize data from Oracle to PostgreSQL","<p>We would like to synchronize data (insert, update) from Oracle (11g) to PostgreSQL (10). Our approach was the following:</p>

<ul>
<li>A trigger on the table in Oracle updates a column with nextval from a sequence before insert and update.</li>
<li>PostgreSQL knows the last sequence number processed and fetches the rows from Oracle > lastSequenceNumberFetched.</li>
</ul>

<p>We now have the following problem:</p>

<ul>
<li>Session 1 in Oracle inserts a row, sequence number (let's say 45) is written but no COMMIT is done in Oracle.</li>
<li>Session 2 in Oracle inserts a row, sequence number is written (let's say 49 (because sequences in Oracle can have gaps)) and a COMMIT is done in Oracle.</li>
<li>Session in PostgreSQL fetches rows from Oracle with sequenceNumber > 44 (because the lastSequenceNumberFetched is 44) and gets the row with sequenceNumber 49. So this is the new lastSequenceNumberFetched.</li>
<li>Session 1 in Oracle makes a commit.</li>
<li>Session in PostgreSQL fetches rows from Oracle with sequenceNumber > 49. Problem is that the row with sequenceNumber 45 is never fetched.</li>
</ul>

<p>Are there any better approaches for our use case avoiding our problem with missing data?</p>
","<database><oracle><postgresql><data-synchronization>","2018-06-20 13:25:07","3591","1","3","56878775","<p>I have seen a similar setup. An application on Postgres mostly for reporting and other secondary tasks while main app was on Oracle.</p>

<p>Some of the main app tables are cached in Postgres for convenience. But this setup brings in the sync problem.</p>

<p>The compromise solution was a mix of incremental sequence-based sync during daytime and full table copy overnight</p>

<p>Regarding other solutions proposed here:</p>

<ul>
<li><p>Postgres fdw is slow for complex queries and it puts extra load on foreign db especially when where clause refer to both local and foreign tables.<br>
The same query will run much faster if foreign table is cached in postgres.</p></li>
<li><p>Incremental/differential sync using sequence numbers -tried this and works acceptable for small tables, but the nightmare starts with child relations maybe an orm can help here</p></li>
<li><p>The ideal solution in my opinion would probably be to stream Oracle changes to Postgres or intermediary process that replicates changes to Postgres</p></li>
</ul>

<p>I have no clue about how to do this as I understood it requires Oracle golden gate app (+ licence)</p>
"
"50581867","SymmetricDS taking so long to start","<p>I am trying to synchronize data between DB2(in AS400) and SQL Server using SymmetricDS.
I have configured DB2 as my master node and SQL server as client node. I have started my master node with the following command </p>

<pre><code>sym --engine server --port 8084
</code></pre>

<p>It is taking nearly 50 minutes to start up. It spends a lot of time in two places while starting.</p>

<ol>
<li>Checking if SymmetricDS tables need created or altered</li>
<li>Synchronizing triggers</li>
</ol>

<p>Is this normal behavior or do I need to do something to make the startup faster?</p>
","<ibm-midrange><data-synchronization><symmetricds>","2018-05-29 10:18:39","149","0","1","50583267","<p>The behaviour is normal, but the performance of your database seems not. A DBA should take a look at it and fix the issues.</p>
"
"50479226","Does frequently executing DDL affect the data synchronization speed in Syncer?","<p>I am trying to import data incrementally into TiDB using the Syncer tool. I am wondering if I frequently execute DDL statements, will it affect the data synchronizing speed?</p>
","<ddl><distributed-database><tidb>","2018-05-23 03:07:01","31","0","1","50479311","<p>Yes, it will. Syncer will first finish executing all the DML statements ahead, and then execute the DDL statements. And Syncer wonâ€™t start executing new DML statements until it finishes executing the DDL statements.</p>
"
"50437784","Syncing offline Mysqlite to a Remote Database","<p>I am working on a project where one of the requirements is the app working in offline mode. So I created an offline MySQLite database, which is the copy of the online DB. Now in the android app the employees will do their job while some of the rows in some of the tables will get updated, or some rows will get inserted, but never deleted. </p>

<p>I went with the ""Version"" method, where I added a column ""Version"" to the tables, and each time a row gets updated it will increment the version.</p>

<p>The problem is with the insert part. The app is multiuser, if there were 5 rows in the table before the app could no longer connect to the remote database (thus the app must use the offline DB) and if User 1 adds 2 rows and User 2 adds 2 rows too, in their respective offline databases (on different device) they will both have the same IDs and initially, I wanted to check maxID from remote DB and maxID from offline mysqli DB and insert all new rows to online DB, but this wont work.</p>

<p>Any easy solutions how to synch new rows of offline DB to remote DB once connection is back online?</p>
","<database><data-synchronization>","2018-05-20 17:51:52","72","0","1","50437886","<p>syncing data using id is not good idea <br>
you should make a update request from you mobile app to server that send it's time stamp of last update (and if you don't add row from you offline devise app to remote server db is so much easy ) 
<br>
you should request where created or updated row is newer than you mobile timestamp </p>

<p><br> 
ps: in app database for android is SQLite , didn't know you can use other db </p>
"
"50266557","How to identify duplicate records using client name and address in SQL while both of them is in free text","<p>I have a database with millions of client contacts. However, a lot of them are duplicated and may I ask some hero from here to advise how to identify those duplicates using Oracle SQL, PL/SQL or Excel.</p>

<p>Following is the data structure:</p>

<p><strong>Client_Header</strong></p>

<pre><code>id integer (Primary Key)
Client_First_Name (varchar2)
Client_Last_Name (varchar2)
Client_Date_Of_Birth (timestamp)
</code></pre>

<p><strong>Client_Address</strong></p>

<pre><code>Client_Id (Foreign Key ref Client_header)
Address_Line1 (varchar2)
Address_Line2 (varhchar2)
Adderss_Line3 (varchar2)
Suburb (Varchar2)
State (varchar2)
Country (varchar2)
</code></pre>

<p>My challenge is other than <code>Client_Date_Of_Birth</code> and those key fields, all fields are free text only.</p>

<p>For example, we have a client like following</p>

<pre><code>Surname : Jones

First name : David

Client_Date_Of_Birth: 10/05/1975

Address: Unit 10 Floor 1, 20 Railway Parade, St Peter,  NSW 2044
</code></pre>

<p>However, as those fields are free text, I have a lot of data issues and following link (jpeg file only) illustrated some of those issues</p>

<p><a href=""https://i.stack.imgur.com/HAJek.jpg"" rel=""nofollow noreferrer"">Sample of data issues</a></p>

<p><strong>Note:</strong></p>

<ol>
<li>Other than those issues, sometime we may miss the first name or last name of the client (but not both) too</li>
<li>Sometimes multiple problems can be find within the same record.</li>
<li><p>Also sometime, the address may simply be the name of a school,
shopping center etc.</p></li>
<li><p>The system does not store any other id that can uniquely identify the client.</p></li>
</ol>

<p>I understand it is close to impossible to gather all duplicate records where the client address is a school or shopping center. However, for other cases, is there anyway to identify most of the duplication.</p>

<p>Thank you for your help!</p>
","<sql><oracle><duplicates><data-quality>","2018-05-10 06:08:24","414","0","2","50268081","<p>Not a pretty sight, and I'm afraid I don't have good news for you. </p>

<p>This is a common problem in databases, especially if the data entry personnel are insufficiently trained. One of the main objectives in data entry training is to make the problem well understood and show ways to avoid it. Something to keep in mind in the future. </p>

<p>Unfortunately, there isn't any ""magic wand"" that will clean your data for you. I'm sorry, but you have before you one of the most tedious tasks in database maintenance. You're going to have to basically remove the duplicates by hand, and the job requires more of an editor than a database administrator.</p>

<p>If you have millions of records, of which perhaps a million are actually duplicates, I would estimate that it will take an expert working full time for at least two years -- and probably longer -- to clean up your problem: to do it in two years would require fixing 2000 records a day, with time off on weekends and two weeks of vacation. </p>

<p>In the end, the only sure way to remove all the duplicates is to compare all of them and remove them one at a time. But there are plenty of tricks you can use to get rid of blocks of them at once. Here are a few that I can think of with your data sample:</p>

<ol>
<li>Change ""Dave"" to ""David"" in both first and last name fields. (Make sure that nobody actually has the last name ""Dave."")</li>
<li>Change all instances of ""Jones David"" to ""David Jones."" (Make sure that there are no people named ""Jones David"".)</li>
<li>Change ""1/F"" to ""Floor 1.""</li>
</ol>

<p>The idea is to focus on some of the fields, and in those fields get all of the duplicates to be exact duplicates. Once you have that done, you delete all the records with the target values in the fields, except the one with the primary key of the record that you want to keep (if your table isn't keyed, you'll have to find another way to do it, such as selecting the top record into a new table). </p>

<p>This technique speeds things up for records with a large number of duplicates. Where you have only a few duplicates, it's quicker to just identify them one by one. One way to do this quickly is to go into edit mode on a table, work with a particular field (for example, the postal code field in this case), and put a unique value in that field when you want to mark it for deletion (in this case, perhaps a single zero). Then you can periodically delete all the records with that value in the field.</p>

<p>You'll also need to sort the data in multiple ways to find the duplicates, which it appears you already know.</p>

<p>As for your notes, don't try to identify all the ways that the data is messed up. Once you identify one record as a duplicate of another, you don't care what's wrong with it, you just have to get rid of it. If you have two records and each contains data that you want to keep that the other one is missing, then you'll have to consolidate them and delete one of them. And then go on to the next, and the next, and the next...</p>
"
"50266557","How to identify duplicate records using client name and address in SQL while both of them is in free text","<p>I have a database with millions of client contacts. However, a lot of them are duplicated and may I ask some hero from here to advise how to identify those duplicates using Oracle SQL, PL/SQL or Excel.</p>

<p>Following is the data structure:</p>

<p><strong>Client_Header</strong></p>

<pre><code>id integer (Primary Key)
Client_First_Name (varchar2)
Client_Last_Name (varchar2)
Client_Date_Of_Birth (timestamp)
</code></pre>

<p><strong>Client_Address</strong></p>

<pre><code>Client_Id (Foreign Key ref Client_header)
Address_Line1 (varchar2)
Address_Line2 (varhchar2)
Adderss_Line3 (varchar2)
Suburb (Varchar2)
State (varchar2)
Country (varchar2)
</code></pre>

<p>My challenge is other than <code>Client_Date_Of_Birth</code> and those key fields, all fields are free text only.</p>

<p>For example, we have a client like following</p>

<pre><code>Surname : Jones

First name : David

Client_Date_Of_Birth: 10/05/1975

Address: Unit 10 Floor 1, 20 Railway Parade, St Peter,  NSW 2044
</code></pre>

<p>However, as those fields are free text, I have a lot of data issues and following link (jpeg file only) illustrated some of those issues</p>

<p><a href=""https://i.stack.imgur.com/HAJek.jpg"" rel=""nofollow noreferrer"">Sample of data issues</a></p>

<p><strong>Note:</strong></p>

<ol>
<li>Other than those issues, sometime we may miss the first name or last name of the client (but not both) too</li>
<li>Sometimes multiple problems can be find within the same record.</li>
<li><p>Also sometime, the address may simply be the name of a school,
shopping center etc.</p></li>
<li><p>The system does not store any other id that can uniquely identify the client.</p></li>
</ol>

<p>I understand it is close to impossible to gather all duplicate records where the client address is a school or shopping center. However, for other cases, is there anyway to identify most of the duplication.</p>

<p>Thank you for your help!</p>
","<sql><oracle><duplicates><data-quality>","2018-05-10 06:08:24","414","0","2","52890232","<p>Some years ago I had a similar task and I tooks about one years to clean the data. 
What I did in short:</p>

<ol>
<li>send the address to api.addressdoctor.com for validation and split into single fields (with maps.googleapis.com it is also possible)</li>
<li>use a first name and last name match list to check the names (we used namepedia.org). A lot depends on the quality of this list. This list should base on country of birth or of the first address. From the results we made a propability what kind of name it is (first/last/company). </li>
<li>with this improved date you should create some normalized and fuzzy attributes. Normalized fields from names and address...like upper and just with alpha-numeric </li>
<li>List item</li>
<li>at the end I would change the data model a little bit to improve the data quality by design. I recommend you adding pre-title, post-title, middle-name and post-name fields. You should also add the splitted address fields like street, streetno, zip, location, longitude, latitude, etc... 
I would also change the relation between Client_Header and Client_Address with an extra address_Id as primary key...but this depends on the requirements. And at the end I would add some constraints to prevent duplicated entries. </li>
<li>after all that is the deduplication not hard. Group just all normalized or fuzzy data together and greate a dense_rank. (I group by person, household, ...) Make a ranking over the attributes (I used data quality, data fillrate and transaction history for a score value) Finally it is your choice if you just want to delete the duplicates and copy the corresponding data to the living client or virtually connect the data via Client_Id in an extra Field. </li>
<li>for insert and update processes you should create PL/SQL functions that check if fuzzy last-name (eg. first-name) + fuzzy address exist. Split the names and address fileds and check them with the address API's and match them with the names reference. If it is a single tuple data entry, show the best results to the user and let him decide. </li>
</ol>
"
"50002202","Outlook doesn't send answer to event invitation via iCal","<p>I am trying to send an invitation to an event in form of an iCal email attachment. I'm testing it with 3Â clients: Zimbra, Gmail and Outlook. All three interpret the attachment correctly: They show the respective RSVP buttons (Accept/Tentative/Decline). However, after pressing the button, only Zimbra sends an email with an iCal file containing the answer. Gmail and Outlook don't.</p>

<p>Is this a normal behavior, or am I doing something wrong? I would expect the clients to send an email with an answer. Saving the data only to a local calendar doesn't make much sense to me.</p>

<p>Example of the iCal I am sending:</p>

<pre><code>BEGIN:VCALENDAR
VERSION:2.0
CALSCALE:GREGORIAN
PRODID:-//Calendar APP
METHOD:REQUEST
BEGIN:VEVENT
DTSTAMP:20180424T084322Z
DTSTART:20180424T000000
DTEND:20180424T235959
SUMMARY:e107
TZID:Europe/Vienna
LOCATION:Besprechungsraum 1
SEQUENCE:0
ORGANIZER:mailto:organizer@mail.com
UID:20180424T084322Z-confId=795:timeTableId=18997@fe80:0:0:0:e73:2050:cc3
 d:6035%utun0
ATTENDEE;ROLE=REQ-PARTICIPANT;CN=;PARTSTAT=NEEDS_ACTION;RSVP=TRUE:mailto:
 user@mail.com
ATTENDEE;ROLE=REQ-PARTICIPANT;CN=;PARTSTAT=NEEDS_ACTION;RSVP=TRUE:mailto:
 another_user@mail.com
END:VEVENT
END:VCALENDAR
</code></pre>

<p>Thank you in advance.</p>
","<outlook><gmail><icalendar><data-synchronization><zimbra>","2018-04-24 12:52:49","336","0","1","53404945","<p>Not sure you're still looking for an answer on this but I do know (from learning the hard way) that if you send your test message from a gmail account but are answering on behalf of a DIFFERENT gmail account, then gmail will not send back a response. So if you're using testacct1@gmail to send a recipient (testacct2@gmail) on behalf of a THIRD acct (testacct3@gmail) gmail will not send the reply. Google checks the sender against the invitee. If they don't match, you will not get a response.</p>
"
"49963694","Delete a row in pandas given a conditions data cleansing?","<p>I have a DataFrame like </p>

<pre><code>Classification     Value_1      Value_2       
churn                 1.0            2.0             
not_churn             2.0            3.0
not_churn             0.0            0.0
churn                 0.0            1.0
</code></pre>

<p>I know that with all values = 0, the classification should be churn. Then I need to delete all the rows with all values is 0 and classification is not_churn. I tried: </p>

<pre><code> df.drop((df['value_1'] == 0 
              &amp; df['value_2'] == 0
              &amp; df['classification']== 'not_churn').index)
'TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]'
</code></pre>
","<python-3.x><pandas>","2018-04-22 07:45:06","41","0","2","49963711","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a> with change conditions for not equal <code>!=</code> and change <code>&amp;</code> to <code>|</code> (or):</p>

<pre><code>df = df[(df['Value_1'] != 0 ) | (df['Value_2'] != 0) | (df['Classification'] != 'not_churn')]
print (df)
  Classification  Value_1  Value_2
0          churn      1.0      2.0
1      not_churn      2.0      3.0
3          churn      0.0      1.0
</code></pre>
"
"49963694","Delete a row in pandas given a conditions data cleansing?","<p>I have a DataFrame like </p>

<pre><code>Classification     Value_1      Value_2       
churn                 1.0            2.0             
not_churn             2.0            3.0
not_churn             0.0            0.0
churn                 0.0            1.0
</code></pre>

<p>I know that with all values = 0, the classification should be churn. Then I need to delete all the rows with all values is 0 and classification is not_churn. I tried: </p>

<pre><code> df.drop((df['value_1'] == 0 
              &amp; df['value_2'] == 0
              &amp; df['classification']== 'not_churn').index)
'TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]'
</code></pre>
","<python-3.x><pandas>","2018-04-22 07:45:06","41","0","2","49963743","<p>If you want to keep rows according to what you don't want, I would try:</p>

<pre><code>  df = df[~((df['value_1'] == 0) &amp; (df['value_2'] == 0) &amp; (df['classification'] == 'not churn'))]
</code></pre>
"
"49893775","MySQL 5.6 Data Replication Syncronization Checker Between Servers (Master & Slave)","<p>I have one Master and Slave</p>

<p>OS : MySQL5.6 &amp; RHEL 7.0</p>

<p>Replication Status : Running Fine</p>

<p>Everything working fine and correctly
I want to check that the data is correctly replicated between servers (Master to Slave) and with the help of <strong>mysqlrplsync</strong>, i execute this command</p>

<pre><code> mysqlrplsync --master=repuser:123@localhost:3306 \ --slaves=repuser:123@192.168.10.11:3306,repuser:123@localhost:3306
</code></pre>

<p>but this command show that error</p>

<pre><code>[root@master common]# mysqlrplsync --master=root:pass@host1:3306 \ --slaves=rpl:pass@host2:3306
WARNING: Using a password on the command line interface can be insecure.
Usage: mysqlrplsync --master=user:pass@host:port --slaves=user:pass@host:port \
[&lt;db_name&gt;[.&lt;tbl_name&gt;]]
mysqlrplsync: error: Option --discover-slaves-login or --slaves is required.
</code></pre>

<p><strong>Note : Even i have tried by adding the database name in the above command</strong></p>

<p>My this method to check Data synchronization is correct ?</p>

<p>If not then can you please guide me how to perform Synchronization of Data Step by Step ?</p>

<p>Best Regards</p>

<p>Mubashar Iftikhar </p>
","<mysql><database-replication><data-synchronization>","2018-04-18 07:34:33","498","0","1","49900433","<p>This isn't a tool I've used myself (I use pt-table-sync) however, you do say </p>

<blockquote>
  <p>I have one Master and Slave</p>
</blockquote>

<p>but go on to have two definitions in --slave. </p>

<p>The definition,  </p>

<pre><code>repuser:123@localhost:3306
</code></pre>

<p>then appears for both the --master and --slave. The second one cannot be correct, at least the port number must be different.</p>

<p>Also, you should lose the ""\"".</p>
"
"49439977","Avoiding client/server data synchronization gap","<p>I have a client/server architecture where the client does the following on connect:</p>

<ul>
<li>Open socket to server and request a stream of changes (the ""stream"").</li>
<li>Fetch all data from server (the ""snapshot"").</li>
</ul>

<p>So the idea here is that the snapshot will get the client all the relevant data at the point of connect, and the stream will keep sending changes - so that the client does not have to do polling and get snapshots over and over again.</p>

<p>The problem with this is that there is a gap in between the snapshot and stream starting that could result in missed data. Example:</p>

<ol>
<li>Client requests snapshot and stream from server.</li>
<li>Server fetches snapshot from db.</li>
<li>Another client commits a change to db (snapshot is thus outdated). Change is sent out to all streams.</li>
<li>Stream for client has been inited.</li>
</ol>

<p>As you can see above, the client ends up with an outdated snapshot and since the stream init is asynchronous it can miss out on changes to the database (it never sees the change in step 3).</p>

<p>I have a few ideas on how to solve this, but I am not super happy with any of them so far:</p>

<p>1) Init the stream first, and then fetch the snapshot. I am not convinced this one is safe since this assumes that the streaming server (rabbitmq/kafka/pulsar) doesnÂ´t miss out on messages in transit.
2) Have the stream also push the changes made in the last X seconds before the connection time. This is not great because it makes time assumptions and bets on probability.</p>

<p>All input is appreciated!</p>
","<database><asynchronous><synchronization><client-server><message-queue>","2018-03-22 23:18:03","360","1","2","49444949","<p>Some suggestions:</p>

<ol>
<li>Let someone else handle this for you (e.g. using Google Cloud Firestore).</li>
<li>I'm not sure exactly what the messages will contain but if could make it so that the state of the db is just the sum of all transaction message sent in the stream (i.e. if you replay all the messages in chronological order you always end up with the same state) then you could let the server give each message a monotonically increasing id. When the client reconnects it tells the server its max number and the servers sends all messages with a higher id.</li>
</ol>
"
"49439977","Avoiding client/server data synchronization gap","<p>I have a client/server architecture where the client does the following on connect:</p>

<ul>
<li>Open socket to server and request a stream of changes (the ""stream"").</li>
<li>Fetch all data from server (the ""snapshot"").</li>
</ul>

<p>So the idea here is that the snapshot will get the client all the relevant data at the point of connect, and the stream will keep sending changes - so that the client does not have to do polling and get snapshots over and over again.</p>

<p>The problem with this is that there is a gap in between the snapshot and stream starting that could result in missed data. Example:</p>

<ol>
<li>Client requests snapshot and stream from server.</li>
<li>Server fetches snapshot from db.</li>
<li>Another client commits a change to db (snapshot is thus outdated). Change is sent out to all streams.</li>
<li>Stream for client has been inited.</li>
</ol>

<p>As you can see above, the client ends up with an outdated snapshot and since the stream init is asynchronous it can miss out on changes to the database (it never sees the change in step 3).</p>

<p>I have a few ideas on how to solve this, but I am not super happy with any of them so far:</p>

<p>1) Init the stream first, and then fetch the snapshot. I am not convinced this one is safe since this assumes that the streaming server (rabbitmq/kafka/pulsar) doesnÂ´t miss out on messages in transit.
2) Have the stream also push the changes made in the last X seconds before the connection time. This is not great because it makes time assumptions and bets on probability.</p>

<p>All input is appreciated!</p>
","<database><asynchronous><synchronization><client-server><message-queue>","2018-03-22 23:18:03","360","1","2","49447718","<p>Why don't you start the stream first and then request the snapshot?</p>

<ol>
<li>Start the stream &amp; queue all received records</li>
<li>Request a snapshot</li>
<li>Process the snapshot</li>
<li>Process queued stream entries (and ignore entries older than the snapshot)</li>
<li>Process the stream </li>
</ol>

<p>Do not process anything from the stream (only queue it for processing internally) until the snapshot have been processed.</p>

<p>Each record should have some sort of token which identifies when the record was changed. So if the stream contains a record older than the one in the snapshot, simply do not process it (during the synchronization phase)</p>
"
"49428059","How to do Real time data synchronization between windows forms and remote server","<p><a href=""https://i.stack.imgur.com/jOmMb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jOmMb.png"" alt=""Flow""></a></p>

<p>My scenario is as shown in the above image. Here customer and shop will be using a windows form application and there is local DB for both. 
For example if customer checks for shops products and place order, it will store in local DB of customer and then push it to central server, from there particular shop should get the order details notification. similarly if order is approved, the data should be updated in central server as well as it should push to customer local DB as update.</p>

<p>we tried using SignalR For this, based on this article <a href=""https://stackoverflow.com/questions/31261987/is-it-correct-to-use-signalr-for-desktop-applications?answertab=active#tab-top"">Is it correct to use SignalR for desktop applications?</a>  there is no direct support of signalR for windows Forms.</p>

<p>what about using <a href=""https://msdn.microsoft.com/en-us/library/bb902854(v=sql.110).aspx"" rel=""nofollow noreferrer"">Microsoft Sync Framework</a> for this, will it be fine and do the job for me?</p>

<p>So is there any other technique to implement real time data sync in c# windows forms.</p>
","<c#><signalr><real-time><microsoft-sync-framework>","2018-03-22 11:55:11","1974","2","2","52069697","<p>Some days ago, </p>

<p>I faced this problem and find out a solution. I hope you can get idea from my solution.</p>

<p>I called an api for transferring data(json) between shop to server and server to shop.
I used same database structure for shop and server, Just added one extra field named shop_id to server and build up an composite primary key to server. </p>

<p>When needs to sync, user will press sync buttons then code will check the is_sync false data and send that data to api</p>

<pre><code>Table structure:
       id,....,is_sync

Interactions:
   Shop-&gt;Server
   Server-&gt;Shop
</code></pre>
"
"49428059","How to do Real time data synchronization between windows forms and remote server","<p><a href=""https://i.stack.imgur.com/jOmMb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jOmMb.png"" alt=""Flow""></a></p>

<p>My scenario is as shown in the above image. Here customer and shop will be using a windows form application and there is local DB for both. 
For example if customer checks for shops products and place order, it will store in local DB of customer and then push it to central server, from there particular shop should get the order details notification. similarly if order is approved, the data should be updated in central server as well as it should push to customer local DB as update.</p>

<p>we tried using SignalR For this, based on this article <a href=""https://stackoverflow.com/questions/31261987/is-it-correct-to-use-signalr-for-desktop-applications?answertab=active#tab-top"">Is it correct to use SignalR for desktop applications?</a>  there is no direct support of signalR for windows Forms.</p>

<p>what about using <a href=""https://msdn.microsoft.com/en-us/library/bb902854(v=sql.110).aspx"" rel=""nofollow noreferrer"">Microsoft Sync Framework</a> for this, will it be fine and do the job for me?</p>

<p>So is there any other technique to implement real time data sync in c# windows forms.</p>
","<c#><signalr><real-time><microsoft-sync-framework>","2018-03-22 11:55:11","1974","2","2","54774685","<p>Answering this after almost an year but it might help someone else.</p>

<p>I recently worked on similar requirement. There are few libraries which could be helpful for this.</p>

<ol>
<li><a href=""https://github.com/jstott/socketio4net"" rel=""nofollow noreferrer"">SocketIO4Net</a></li>
<li><a href=""https://github.com/Quobject/SocketIoClientDotNet"" rel=""nofollow noreferrer"">SocketIoClientDotNet</a></li>
</ol>

<p>I used SocketIoClientDotNet and it is porting of javascript library <a href=""https://github.com/socketio/socket.io"" rel=""nofollow noreferrer"">Socket.IO</a>, so it is very easy to integrate it. It is working great in my project but now it is deprecated. Still, you can use it as last version is very stable and there are very few open issues.</p>
"
"49423171","Database sharing using SymmetricDS among different stores","<p>We are evaluating SymmetricDS for one of our applications.</p>

<p>We want to synchronize database among multiple stores. The data needs to flow from one store to the central database and then back to different stores.</p>

<p>In SymmetricDS, we are able to achieve the data flows as follows:</p>

<pre><code>store -&gt; central database // whenever we modify/insert/delete data in store, 
</code></pre>

<p>and also from</p>

<pre><code>central database -&gt; store // whenever we modify/insert/delete data in central db
</code></pre>

<p>but what we require is a bit back and forth</p>

<pre><code>store -&gt; central database -&gt; stores
</code></pre>

<p>i.e. store inserts a data row in its database, it should go to central db and then back to selected stores</p>

<p><strong>Edited the question as below to elaborate which table is not synced now</strong></p>

<p>We have following heirarchy</p>

<p>Stores
Chains of stores
There is Central DB having information of all stores and chains</p>

<p>Each store must have a basic information like address of all the stores within its own chain.
Store can add another store to its chain. Once that happens the added store must then get information through syncing of all the stores in this chain.</p>

<p>Tables are like below:</p>

<p>STORE_CHAINS (having mapping between STORE_ID, CHAIN_ID).</p>

<p>Example of STORE_CHAINS:</p>

<pre><code>STORE_ID    CHAIN_ID
A           CHAIN1
B           CHAIN1
C           CHAIN1
Y           CHAIN2
Z           CHAIN2
</code></pre>

<p>There is a STORES table which has details about each store with STORE_ID being the primary key.</p>

<p>Example of STORES table:</p>

<pre><code>STORE_ID    STORE_ADDRESS   ...
A           AddrA
B           AddrB
C           AddrC
D           AddrD
...         
X           AddrX
Y           AddrY
Z           AddrZ
</code></pre>

<p>Each store is synced a portion of the CENTRAL STORES table database with some basic information. Only those stores' details are synced which share the same chain.</p>

<p>For example here, if a new mapping (D, CHAIN1) is added to STORE_CHAINS table by Store C, the newly added store D should then receive through sync the mapping entry itself and details about the Stores A, B, C from the STORES table because they share the same chain CHAIN1. Also the Stores A, B will be sent STORE_CHAINS mapping entry and STORES entry for D.</p>

<p>Our problem is that the STORES entry for A,B,C is not getting sent to D. Only the newly created mapping (D, CHAIN1) gets synced to D.</p>

<p>The table STORE_CHAINS is the only one that has actually had a data changed, the STORES table has not changed. How do we ensure that the triggers of STORES table also get fired and its data is sent to all the relevant stores whenever a new (STORE_ID, CHAIN_ID) mapping is created.</p>

<p>We are using subselect for all cases to select which stores/store-group mapping entries should be sent. The selection logic works ok if we do an initial load for any store.</p>

<p>Thanks in advance.</p>
","<database><data-synchronization><symmetricds>","2018-03-22 07:47:51","379","0","1","49425919","<p>Set the value for the column sync_on_incoming_batch to 1 as explained in the documentation <a href=""http://www.symmetricds.org/doc/3.8/html/user-guide.html#_bi_directional_synchronization"" rel=""nofollow noreferrer"">http://www.symmetricds.org/doc/3.8/html/user-guide.html#_bi_directional_synchronization</a></p>

<p>Use CUSTOM_BEFORE_UPDATE_TEXT or CUSTOM_BEFORE_INSERT_TEXT to trigger a sunny update on all dependency rows from other tables that need to get synced</p>
"
"49278353","Handling input data quality with checks & default value","<p>I am trying to write a code for squaring the user input number in Python. I've created function my1() ... </p>

<p>What I want to do is to make Python to take user input of a number and square it but if user added no value it gives a print statement and by default give the square of a default number for e.g 2</p>

<p>Here is what I've tried so far</p>

<pre><code>def my1(a=4):
 if my1() is None:
    print('You have not entered anything')
 else:
    b=a**2
    print (b)

my1(input(""Enter a Number""))
</code></pre>
","<python><python-3.x>","2018-03-14 12:56:07","324","1","4","49278485","<p>This is a better solution:</p>

<pre><code>def my1(a=4):
    if not a:
        return 'You have not entered anything'
    else:
        try:
            return int(a)**2
        except ValueError:
            return 'Invalid input provided'

my1(input(""Enter a Number""))
</code></pre>

<p><strong>Explanation</strong></p>

<ul>
<li>Have your function <code>return</code> values, instead of simply printing. This is good practice.</li>
<li>Use <code>if not a</code> to test if your string is empty. This is a Pythonic idiom.</li>
<li>Convert your input string to numeric data, e.g. via <code>int</code>.</li>
<li>Catch <code>ValueError</code> and return an appropriate message in case the user input is invalid.</li>
</ul>
"
"49278353","Handling input data quality with checks & default value","<p>I am trying to write a code for squaring the user input number in Python. I've created function my1() ... </p>

<p>What I want to do is to make Python to take user input of a number and square it but if user added no value it gives a print statement and by default give the square of a default number for e.g 2</p>

<p>Here is what I've tried so far</p>

<pre><code>def my1(a=4):
 if my1() is None:
    print('You have not entered anything')
 else:
    b=a**2
    print (b)

my1(input(""Enter a Number""))
</code></pre>
","<python><python-3.x>","2018-03-14 12:56:07","324","1","4","49278512","<p>In your second line, it should be 
    if a is None:</p>

<p>I think what you want to do is something like the following:</p>

<pre><code>def m1(user_input=None):
    if user_input is None or isinstance(user_input, int):
        print(""Input error!"")
        return 4
    else:
        return int(user_input)**2
print(my1(input(""Input a number"")))
</code></pre>
"
"49278353","Handling input data quality with checks & default value","<p>I am trying to write a code for squaring the user input number in Python. I've created function my1() ... </p>

<p>What I want to do is to make Python to take user input of a number and square it but if user added no value it gives a print statement and by default give the square of a default number for e.g 2</p>

<p>Here is what I've tried so far</p>

<pre><code>def my1(a=4):
 if my1() is None:
    print('You have not entered anything')
 else:
    b=a**2
    print (b)

my1(input(""Enter a Number""))
</code></pre>
","<python><python-3.x>","2018-03-14 12:56:07","324","1","4","49278537","<p>You're getting an infinite loop by calling my1() within my1(). I would make the following edits:</p>

<pre><code>def my1(a):
    if a is '':
        print('You have not entered anything')
    else:
        b=int(a)**2
        print (b)

my1(input(""Enter a Number""))
</code></pre>
"
"49278353","Handling input data quality with checks & default value","<p>I am trying to write a code for squaring the user input number in Python. I've created function my1() ... </p>

<p>What I want to do is to make Python to take user input of a number and square it but if user added no value it gives a print statement and by default give the square of a default number for e.g 2</p>

<p>Here is what I've tried so far</p>

<pre><code>def my1(a=4):
 if my1() is None:
    print('You have not entered anything')
 else:
    b=a**2
    print (b)

my1(input(""Enter a Number""))
</code></pre>
","<python><python-3.x>","2018-03-14 12:56:07","324","1","4","49278694","<p>When I read your code, I can see that you are very confused about what you are writing. Try to organize your mind around the <strong>tasks</strong> you'll need to perform. Here, you want to :</p>

<ol>
<li>Receive your user inputs.</li>
<li>Compute the data.</li>
<li>Print accordingly.</li>
</ol>

<p>First, take your input.</p>

<pre><code>user_choice = input(""Enter a number :"")
</code></pre>

<p>Then, compute the data you received.</p>

<pre><code>my1(user_choice)
</code></pre>

<p>You want your function, as of now, to <code>print an error message if your type data is not good</code>, else print the squared number.</p>

<pre><code>def my1(user_choice): # Always give meaning to the name of your variables.
    if not user_choice:
        print 'Error'
    else:
        print user_choice ** 2
</code></pre>

<p>Here, you are basically saying ""If my user_choice doesn't exists..."". Meaning it equals <code>False</code> (it is a bit more complicated than this, but in short, you need to remember this). An empty string doesn't contain anything for instance. The other choice, <code>else</code>, is if you handled your error case, then your input must be right, so you compute your data accordingly.</p>
"
"49149782","Update a local db table from remote SQL Server db table","<p>I would like to have a <strong>local copy of a remote db table, updated every N minutes</strong>. How can I accomplish this with sql management studio (possibly) without build a script from myself? </p>

<p><em>This is the table</em></p>

<p><a href=""https://i.stack.imgur.com/39ant.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/39ant.png"" alt=""enter image description here""></a></p>
","<sql-server><sql-update><ssms><remote-server><data-synchronization>","2018-03-07 10:36:09","974","0","1","49157191","<p>The sql server import and export wizard seems to be not the right way for me: infact it does not support the import from a table to another of only new content. </p>

<p>I wrote a simple procedure, for syncing a local table with a remote one, with only new data, depending on timestamp</p>

<pre><code>create procedure p_sync_from_remote
as
begin
    declare @lastitem Datetime = 
         (select top 1 Time_Stamp from table_destination order by Time_Stamp desc)
    if (@lastitem is null) or (len(@lastitem) &lt;= 0)
    begin
        insert into table_destination
        select * from table_source
    end
    else
    begin
        insert into table_destination
        select * from table_source where Time_Stamp &gt; @lastitem
    end
end

exec p_sync_from_remote
</code></pre>
"
"49101953","How to implement a Thread Safe HashTable (PhoneBook) Data Structure in Swift?","<p>I am trying to implement a Thread-Safe PhoneBook object. The phone book should be able to add a person, and look up a person based on their name and phoneNumber. From an implementation perspective this simply involves two hash tables, one associating name -> Person and another associating phone# -> Person. </p>

<p>The caveat is I want this object to be threadSafe. This means I would like to be able to support concurrent lookups in the PhoneBook while ensuring only one thread can add a Person to the PhoneBook at a time. This is the basic reader-writers problem, and I am trying to solve this using GrandCentralDispatch and dispatch barriers. I am struggling to solve this though as I am running into issues.. Below is my Swift playground code:</p>

<pre><code>//: Playground - noun: a place where people can play

import UIKit
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

public class Person: CustomStringConvertible {
    public var description: String {
        get {
            return ""Person: \(name), \(phoneNumber)""
        }
    }

    public var name: String
    public var phoneNumber: String
    private var readLock = ReaderWriterLock()

    public init(name: String, phoneNumber: String) {
        self.name = name
        self.phoneNumber = phoneNumber
    }


    public func uniquePerson() -&gt; Person {
        let randomID = UUID().uuidString
        return Person(name: randomID, phoneNumber: randomID)
    }
}

public enum Qos {
    case threadSafe, none
}

public class PhoneBook {

    private var qualityOfService: Qos = .none
    public var nameToPersonMap = [String: Person]()
    public var phoneNumberToPersonMap = [String: Person]()
    private var readWriteLock = ReaderWriterLock()


    public init(_ qos: Qos) {
        self.qualityOfService = qos
    }

    public func personByName(_ name: String) -&gt; Person? {
        var person: Person? = nil
        if qualityOfService == .threadSafe {
            readWriteLock.concurrentlyRead { [weak self] in
                guard let strongSelf = self else { return }
                person = strongSelf.nameToPersonMap[name]
            }
        } else {
            person = nameToPersonMap[name]
        }

        return person
    }

    public func personByPhoneNumber( _ phoneNumber: String) -&gt; Person? {
        var person: Person? = nil
        if qualityOfService == .threadSafe {
            readWriteLock.concurrentlyRead { [weak self] in
                guard let strongSelf = self else { return }
                person = strongSelf.phoneNumberToPersonMap[phoneNumber]
            }
        } else {
            person = phoneNumberToPersonMap[phoneNumber]
        }

        return person
    }

    public func addPerson(_ person: Person) {
        if qualityOfService == .threadSafe {
            readWriteLock.exclusivelyWrite { [weak self] in
                guard let strongSelf = self else { return }
                strongSelf.nameToPersonMap[person.name] = person
                strongSelf.phoneNumberToPersonMap[person.phoneNumber] = person
            }
        } else {
            nameToPersonMap[person.name] = person
            phoneNumberToPersonMap[person.phoneNumber] = person
        }
    }

}


// A ReaderWriterLock implemented using GCD and OS Barriers.
public class ReaderWriterLock {

    private let concurrentQueue = DispatchQueue(label: ""com.ReaderWriterLock.Queue"", attributes: DispatchQueue.Attributes.concurrent)
    private var writeClosure: (() -&gt; Void)!

    public func concurrentlyRead(_ readClosure: (() -&gt; Void)) {
        concurrentQueue.sync {
            readClosure()
        }
    }

    public func exclusivelyWrite(_ writeClosure: @escaping (() -&gt; Void)) {
        self.writeClosure = writeClosure
        concurrentQueue.async(flags: .barrier) { [weak self] in
            guard let strongSelf = self else { return }
            strongSelf.writeClosure()
        }
    }

}

// MARK: Testing the synchronization and thread-safety

for _ in 0..&lt;5 {
    let iterations = 1000
    let phoneBook = PhoneBook(.none)

    let concurrentTestQueue = DispatchQueue(label: ""com.PhoneBookTest.Queue"", attributes: DispatchQueue.Attributes.concurrent)
    for _ in 0..&lt;iterations {
        let person = Person(name: """", phoneNumber: """").uniquePerson()
        concurrentTestQueue.async {
            phoneBook.addPerson(person)
        }
    }

    sleep(10)
    print(phoneBook.nameToPersonMap.count)
}
</code></pre>

<p>To test my code I run 1000 concurrent threads that simply add a new Person to the PhoneBook. Each Person is unique so after the 1000 threads complete I am expecting the PhoneBook to contain a count of 1000. Everytime I perform a write I perform a dispatch_barrier call, update the hash tables, and return. To my knowledge this is all we need to do; however, after repeated runs of the 1000 threads I get the number of entries in the PhoneBook to be inconsistent and all over the place:</p>

<pre><code>Phone Book Entries: 856
Phone Book Entries: 901
Phone Book Entries: 876
Phone Book Entries: 902
Phone Book Entries: 912
</code></pre>

<p>Can anyone please help me figure out what is going on? Is there something wrong with my locking code or even worse something wrong with how my test is constructed? I am very new to this multi-threaded problem space, thanks!</p>
","<swift><multithreading><grand-central-dispatch><data-synchronization><barrier>","2018-03-05 00:24:33","6721","10","3","49118051","<p>I donâ€™t think you are using it wrong :).</p>

<p>The original (on macos) generates:</p>

<pre><code>0  swift                    0x000000010c9c536a PrintStackTraceSignalHandler(void*) + 42
1  swift                    0x000000010c9c47a6 SignalHandler(int) + 662
2  libsystem_platform.dylib 0x00007fffbbdadb3a _sigtramp + 26
3  libsystem_platform.dylib 000000000000000000 _sigtramp + 1143284960
4  libswiftCore.dylib       0x0000000112696944 _T0SSwcp + 36
5  libswiftCore.dylib       0x000000011245fa92 _T0s24_VariantDictionaryBufferO018ensureUniqueNativeC0Sb11reallocated_Sb15capacityChangedtSiF + 1634
6  libswiftCore.dylib       0x0000000112461fd2 _T0s24_VariantDictionaryBufferO17nativeUpdateValueq_Sgq__x6forKeytF + 1074
</code></pre>

<p>If you remove the â€˜.concurrentâ€™ from your ReaderWriter queue, ""the problem disappearsâ€.Â©
If you restore the .concurrent, but change the async invocation in the writer side to be sync:</p>

<p>swift(10504,0x70000896f000) malloc: *** error for object 0x7fcaa440cee8: incorrect checksum for freed object - object was probably modified after being freed.</p>

<p>Which would be a bit astonishing if it werenâ€™t swift?
I dug in, replaced your â€˜stringâ€™ based array with an Int one by interposing a hash function, replaced the sleep(10) with a barrier dispatch to flush any laggardly blocks through, and that made it more reproducibly crash with the somewhat more helpful:</p>

<p>x(10534,0x700000f01000) malloc: *** error for object 0x7f8c9ee00008: incorrect checksum for freed object - object was probably modified after being freed.</p>

<p>But when a search of the source revealed no malloc or free, perhaps the stack dump is more useful.</p>

<p>Anyways, best way to solve your problem:  use go instead; it actually makes sense.</p>
"
"49101953","How to implement a Thread Safe HashTable (PhoneBook) Data Structure in Swift?","<p>I am trying to implement a Thread-Safe PhoneBook object. The phone book should be able to add a person, and look up a person based on their name and phoneNumber. From an implementation perspective this simply involves two hash tables, one associating name -> Person and another associating phone# -> Person. </p>

<p>The caveat is I want this object to be threadSafe. This means I would like to be able to support concurrent lookups in the PhoneBook while ensuring only one thread can add a Person to the PhoneBook at a time. This is the basic reader-writers problem, and I am trying to solve this using GrandCentralDispatch and dispatch barriers. I am struggling to solve this though as I am running into issues.. Below is my Swift playground code:</p>

<pre><code>//: Playground - noun: a place where people can play

import UIKit
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

public class Person: CustomStringConvertible {
    public var description: String {
        get {
            return ""Person: \(name), \(phoneNumber)""
        }
    }

    public var name: String
    public var phoneNumber: String
    private var readLock = ReaderWriterLock()

    public init(name: String, phoneNumber: String) {
        self.name = name
        self.phoneNumber = phoneNumber
    }


    public func uniquePerson() -&gt; Person {
        let randomID = UUID().uuidString
        return Person(name: randomID, phoneNumber: randomID)
    }
}

public enum Qos {
    case threadSafe, none
}

public class PhoneBook {

    private var qualityOfService: Qos = .none
    public var nameToPersonMap = [String: Person]()
    public var phoneNumberToPersonMap = [String: Person]()
    private var readWriteLock = ReaderWriterLock()


    public init(_ qos: Qos) {
        self.qualityOfService = qos
    }

    public func personByName(_ name: String) -&gt; Person? {
        var person: Person? = nil
        if qualityOfService == .threadSafe {
            readWriteLock.concurrentlyRead { [weak self] in
                guard let strongSelf = self else { return }
                person = strongSelf.nameToPersonMap[name]
            }
        } else {
            person = nameToPersonMap[name]
        }

        return person
    }

    public func personByPhoneNumber( _ phoneNumber: String) -&gt; Person? {
        var person: Person? = nil
        if qualityOfService == .threadSafe {
            readWriteLock.concurrentlyRead { [weak self] in
                guard let strongSelf = self else { return }
                person = strongSelf.phoneNumberToPersonMap[phoneNumber]
            }
        } else {
            person = phoneNumberToPersonMap[phoneNumber]
        }

        return person
    }

    public func addPerson(_ person: Person) {
        if qualityOfService == .threadSafe {
            readWriteLock.exclusivelyWrite { [weak self] in
                guard let strongSelf = self else { return }
                strongSelf.nameToPersonMap[person.name] = person
                strongSelf.phoneNumberToPersonMap[person.phoneNumber] = person
            }
        } else {
            nameToPersonMap[person.name] = person
            phoneNumberToPersonMap[person.phoneNumber] = person
        }
    }

}


// A ReaderWriterLock implemented using GCD and OS Barriers.
public class ReaderWriterLock {

    private let concurrentQueue = DispatchQueue(label: ""com.ReaderWriterLock.Queue"", attributes: DispatchQueue.Attributes.concurrent)
    private var writeClosure: (() -&gt; Void)!

    public func concurrentlyRead(_ readClosure: (() -&gt; Void)) {
        concurrentQueue.sync {
            readClosure()
        }
    }

    public func exclusivelyWrite(_ writeClosure: @escaping (() -&gt; Void)) {
        self.writeClosure = writeClosure
        concurrentQueue.async(flags: .barrier) { [weak self] in
            guard let strongSelf = self else { return }
            strongSelf.writeClosure()
        }
    }

}

// MARK: Testing the synchronization and thread-safety

for _ in 0..&lt;5 {
    let iterations = 1000
    let phoneBook = PhoneBook(.none)

    let concurrentTestQueue = DispatchQueue(label: ""com.PhoneBookTest.Queue"", attributes: DispatchQueue.Attributes.concurrent)
    for _ in 0..&lt;iterations {
        let person = Person(name: """", phoneNumber: """").uniquePerson()
        concurrentTestQueue.async {
            phoneBook.addPerson(person)
        }
    }

    sleep(10)
    print(phoneBook.nameToPersonMap.count)
}
</code></pre>

<p>To test my code I run 1000 concurrent threads that simply add a new Person to the PhoneBook. Each Person is unique so after the 1000 threads complete I am expecting the PhoneBook to contain a count of 1000. Everytime I perform a write I perform a dispatch_barrier call, update the hash tables, and return. To my knowledge this is all we need to do; however, after repeated runs of the 1000 threads I get the number of entries in the PhoneBook to be inconsistent and all over the place:</p>

<pre><code>Phone Book Entries: 856
Phone Book Entries: 901
Phone Book Entries: 876
Phone Book Entries: 902
Phone Book Entries: 912
</code></pre>

<p>Can anyone please help me figure out what is going on? Is there something wrong with my locking code or even worse something wrong with how my test is constructed? I am very new to this multi-threaded problem space, thanks!</p>
","<swift><multithreading><grand-central-dispatch><data-synchronization><barrier>","2018-03-05 00:24:33","6721","10","3","49307028","<p>The problem is your <code>ReaderWriterLock</code>. You are saving the <code>writeClosure</code> as a property, and then asynchronously dispatching a closure that calls that saved property. But if another <code>exclusiveWrite</code> came in during the intervening period of time, your <code>writeClosure</code> property would be replaced with the new closure.</p>
<p>In this case, it means that you can be adding the same <code>Person</code> multiple times. And because you're using a dictionary, those duplicates have the same key, and therefore don't result in you're seeing all 1000 entries.</p>
<p>You can actually simplify <code>ReaderWriterLock</code>, completely eliminating that property. Iâ€™d also make <code>concurrentRead</code> a generic, returning the value (just like <code>sync</code> does), and rethrowing any errors (if any).</p>
<pre><code>public class ReaderWriterLock {
    private let queue = DispatchQueue(label: &quot;com.domain.app.rwLock&quot;, attributes: .concurrent)
    
    public func concurrentlyRead&lt;T&gt;(_ block: (() throws -&gt; T)) rethrows -&gt; T {
        return try queue.sync {
            try block()
        }
    }
    
    public func exclusivelyWrite(_ block: @escaping (() -&gt; Void)) {
        queue.async(flags: .barrier) {
            block()
        }
    }
}
</code></pre>
<hr />
<p>A couple of other, unrelated observations:</p>
<ol>
<li><p>By the way, this simplified <code>ReaderWriterLock</code> happens to solves another concern. That <code>writeClosure</code> property, which we've now removed, could have easily introduced a strong reference cycle.</p>
<p>Yes, you were scrupulous about using <code>[weak self]</code>, so there wasn't any strong reference cycle, but it was possible. I would advise that wherever you employ a closure property, that you set that closure property to <code>nil</code> when you're done with it, so any strong references that closure may have accidentally entailed will be resolved. That way a persistent strong reference cycle is never possible. (Plus, the closure itself and any local variables or other external references it has will be resolved.)</p>
</li>
<li><p>You're sleeping for 10 seconds. That should be more than enough, but I'd advise against just adding random <code>sleep</code> calls (because you never can be 100% sure). Fortunately, you have a concurrent queue, so you can use that:</p>
<pre><code>concurrentTestQueue.async(flags: .barrier) { 
    print(phoneBook.count) 
}
</code></pre>
<p>Because of that barrier, it will wait until everything else you put on that queue is done.</p>
</li>
<li><p>Note, I did not just print <code>nameToPersonMap.count</code>. This array has been carefully synchronized within <code>PhoneBook</code>, so you can't just let random, external classes access it directly without synchronization.</p>
<p>Whenever you have some property which you're synchronizing internally, it should be <code>private</code> and then create a thread-safe function/variable to retrieve whatever you need:</p>
<pre><code>public class PhoneBook {

    private var nameToPersonMap = [String: Person]()
    private var phoneNumberToPersonMap = [String: Person]()

    ...

    var count: Int {
        return readWriteLock.concurrentlyRead {
            nameToPersonMap.count
        }
    }
}
</code></pre>
</li>
<li><p>You say you're testing thread safety, but then created <code>PhoneBook</code> with <code>.none</code> option (achieving no thread-safety). In that scenario, I'd expect problems. You have to create your <code>PhoneBook</code> with the <code>.threadSafe</code> option.</p>
</li>
<li><p>You have a number of <code>strongSelf</code> patterns. That's rather unswifty. It is generally not needed in Swift as you can use <code>[weak self]</code> and then just do optional chaining.</p>
</li>
</ol>
<p>Pulling all of this together, here is my final playground:</p>
<pre><code>PlaygroundPage.current.needsIndefiniteExecution = true

public class Person {
    public let name: String
    public let phoneNumber: String
    
    public init(name: String, phoneNumber: String) {
        self.name = name
        self.phoneNumber = phoneNumber
    }
    
    public static func uniquePerson() -&gt; Person {
        let randomID = UUID().uuidString
        return Person(name: randomID, phoneNumber: randomID)
    }
}

extension Person: CustomStringConvertible {
    public var description: String {
        return &quot;Person: \(name), \(phoneNumber)&quot;
    }
}

public enum ThreadSafety { // Changed the name from Qos, because this has nothing to do with quality of service, but is just a question of thread safety
    case threadSafe, none
}

public class PhoneBook {
    
    private var threadSafety: ThreadSafety
    private var nameToPersonMap = [String: Person]()        // if you're synchronizing these, you really shouldn't expose them to the public
    private var phoneNumberToPersonMap = [String: Person]() // if you're synchronizing these, you really shouldn't expose them to the public
    private var readWriteLock = ReaderWriterLock()
    
    public init(_ threadSafety: ThreadSafety) {
        self.threadSafety = threadSafety
    }
    
    public func personByName(_ name: String) -&gt; Person? {
        if threadSafety == .threadSafe {
            return readWriteLock.concurrentlyRead { [weak self] in
                self?.nameToPersonMap[name]
            }
        } else {
            return nameToPersonMap[name]
        }
    }
    
    public func personByPhoneNumber(_ phoneNumber: String) -&gt; Person? {
        if threadSafety == .threadSafe {
            return readWriteLock.concurrentlyRead { [weak self] in
                self?.phoneNumberToPersonMap[phoneNumber]
            }
        } else {
            return phoneNumberToPersonMap[phoneNumber]
        }
    }
    
    public func addPerson(_ person: Person) {
        if threadSafety == .threadSafe {
            readWriteLock.exclusivelyWrite { [weak self] in
                self?.nameToPersonMap[person.name] = person
                self?.phoneNumberToPersonMap[person.phoneNumber] = person
            }
        } else {
            nameToPersonMap[person.name] = person
            phoneNumberToPersonMap[person.phoneNumber] = person
        }
    }
    
    var count: Int {
        return readWriteLock.concurrentlyRead {
            nameToPersonMap.count
        }
    }
}

// A ReaderWriterLock implemented using GCD concurrent queue and barriers.

public class ReaderWriterLock {
    private let queue = DispatchQueue(label: &quot;com.domain.app.rwLock&quot;, attributes: .concurrent)
    
    public func concurrentlyRead&lt;T&gt;(_ block: (() throws -&gt; T)) rethrows -&gt; T {
        return try queue.sync {
            try block()
        }
    }
    
    public func exclusivelyWrite(_ block: @escaping (() -&gt; Void)) {
        queue.async(flags: .barrier) {
            block()
        }
    }
}


for _ in 0 ..&lt; 5 {
    let iterations = 1000
    let phoneBook = PhoneBook(.threadSafe)
    
    let concurrentTestQueue = DispatchQueue(label: &quot;com.PhoneBookTest.Queue&quot;, attributes: .concurrent)
    for _ in 0..&lt;iterations {
        let person = Person.uniquePerson()
        concurrentTestQueue.async {
            phoneBook.addPerson(person)
        }
    }
    
    concurrentTestQueue.async(flags: .barrier) {
        print(phoneBook.count)
    }
}
</code></pre>
<hr />
<p>Personally, I'd be inclined to take it a step further and</p>
<ul>
<li>move the synchronization into a generic class; and</li>
<li>change the model to be an array of <code>Person</code> object, so that:
<ul>
<li>The model supports multiple people with the same or phone number; and</li>
<li>You can use value types if you want.</li>
</ul>
</li>
</ul>
<p>For example:</p>
<pre><code>public struct Person {
    public let name: String
    public let phoneNumber: String
    
    public static func uniquePerson() -&gt; Person {
        return Person(name: UUID().uuidString, phoneNumber: UUID().uuidString)
    }
}

public struct PhoneBook {
    
    private var synchronizedPeople = Synchronized([Person]())
    
    public func people(name: String? = nil, phone: String? = nil) -&gt; [Person]? {
        return synchronizedPeople.value.filter {
            (name == nil || $0.name == name) &amp;&amp; (phone == nil || $0.phoneNumber == phone)
        }
    }
    
    public func append(_ person: Person) {
        synchronizedPeople.writer { people in
            people.append(person)
        }
    }
    
    public var count: Int {
        return synchronizedPeople.reader { $0.count }
    }
}

/// A structure to provide thread-safe access to some underlying object using reader-writer pattern.

public class Synchronized&lt;T&gt; {
    /// Private value. Use `public` `value` computed property (or `reader` and `writer` methods)
    /// for safe, thread-safe access to this underlying value.
    
    private var _value: T
    
    /// Private reader-write synchronization queue
    
    private let queue = DispatchQueue(label: Bundle.main.bundleIdentifier! + &quot;.synchronized&quot;, qos: .default, attributes: .concurrent)
    
    /// Create `Synchronized` object
    ///
    /// - Parameter value: The initial value to be synchronized.
    
    public init(_ value: T) {
        _value = value
    }
    
    /// A threadsafe variable to set and get the underlying object, as a convenience when higher level synchronization is not needed        
    
    public var value: T {
        get { reader { $0 } }
        set { writer { $0 = newValue } }
    }
    
    /// A &quot;reader&quot; method to allow thread-safe, read-only concurrent access to the underlying object.
    ///
    /// - Warning: If the underlying object is a reference type, you are responsible for making sure you
    ///            do not mutating anything. If you stick with value types (`struct` or primitive types),
    ///            this will be enforced for you.
    
    public func reader&lt;U&gt;(_ block: (T) throws -&gt; U) rethrows -&gt; U {
        return try queue.sync { try block(_value) }
    }
    
    /// A &quot;writer&quot; method to allow thread-safe write with barrier to the underlying object
    
    func writer(_ block: @escaping (inout T) -&gt; Void) {
        queue.async(flags: .barrier) {
            block(&amp;self._value)
        }
    }
}
</code></pre>
"
"49101953","How to implement a Thread Safe HashTable (PhoneBook) Data Structure in Swift?","<p>I am trying to implement a Thread-Safe PhoneBook object. The phone book should be able to add a person, and look up a person based on their name and phoneNumber. From an implementation perspective this simply involves two hash tables, one associating name -> Person and another associating phone# -> Person. </p>

<p>The caveat is I want this object to be threadSafe. This means I would like to be able to support concurrent lookups in the PhoneBook while ensuring only one thread can add a Person to the PhoneBook at a time. This is the basic reader-writers problem, and I am trying to solve this using GrandCentralDispatch and dispatch barriers. I am struggling to solve this though as I am running into issues.. Below is my Swift playground code:</p>

<pre><code>//: Playground - noun: a place where people can play

import UIKit
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

public class Person: CustomStringConvertible {
    public var description: String {
        get {
            return ""Person: \(name), \(phoneNumber)""
        }
    }

    public var name: String
    public var phoneNumber: String
    private var readLock = ReaderWriterLock()

    public init(name: String, phoneNumber: String) {
        self.name = name
        self.phoneNumber = phoneNumber
    }


    public func uniquePerson() -&gt; Person {
        let randomID = UUID().uuidString
        return Person(name: randomID, phoneNumber: randomID)
    }
}

public enum Qos {
    case threadSafe, none
}

public class PhoneBook {

    private var qualityOfService: Qos = .none
    public var nameToPersonMap = [String: Person]()
    public var phoneNumberToPersonMap = [String: Person]()
    private var readWriteLock = ReaderWriterLock()


    public init(_ qos: Qos) {
        self.qualityOfService = qos
    }

    public func personByName(_ name: String) -&gt; Person? {
        var person: Person? = nil
        if qualityOfService == .threadSafe {
            readWriteLock.concurrentlyRead { [weak self] in
                guard let strongSelf = self else { return }
                person = strongSelf.nameToPersonMap[name]
            }
        } else {
            person = nameToPersonMap[name]
        }

        return person
    }

    public func personByPhoneNumber( _ phoneNumber: String) -&gt; Person? {
        var person: Person? = nil
        if qualityOfService == .threadSafe {
            readWriteLock.concurrentlyRead { [weak self] in
                guard let strongSelf = self else { return }
                person = strongSelf.phoneNumberToPersonMap[phoneNumber]
            }
        } else {
            person = phoneNumberToPersonMap[phoneNumber]
        }

        return person
    }

    public func addPerson(_ person: Person) {
        if qualityOfService == .threadSafe {
            readWriteLock.exclusivelyWrite { [weak self] in
                guard let strongSelf = self else { return }
                strongSelf.nameToPersonMap[person.name] = person
                strongSelf.phoneNumberToPersonMap[person.phoneNumber] = person
            }
        } else {
            nameToPersonMap[person.name] = person
            phoneNumberToPersonMap[person.phoneNumber] = person
        }
    }

}


// A ReaderWriterLock implemented using GCD and OS Barriers.
public class ReaderWriterLock {

    private let concurrentQueue = DispatchQueue(label: ""com.ReaderWriterLock.Queue"", attributes: DispatchQueue.Attributes.concurrent)
    private var writeClosure: (() -&gt; Void)!

    public func concurrentlyRead(_ readClosure: (() -&gt; Void)) {
        concurrentQueue.sync {
            readClosure()
        }
    }

    public func exclusivelyWrite(_ writeClosure: @escaping (() -&gt; Void)) {
        self.writeClosure = writeClosure
        concurrentQueue.async(flags: .barrier) { [weak self] in
            guard let strongSelf = self else { return }
            strongSelf.writeClosure()
        }
    }

}

// MARK: Testing the synchronization and thread-safety

for _ in 0..&lt;5 {
    let iterations = 1000
    let phoneBook = PhoneBook(.none)

    let concurrentTestQueue = DispatchQueue(label: ""com.PhoneBookTest.Queue"", attributes: DispatchQueue.Attributes.concurrent)
    for _ in 0..&lt;iterations {
        let person = Person(name: """", phoneNumber: """").uniquePerson()
        concurrentTestQueue.async {
            phoneBook.addPerson(person)
        }
    }

    sleep(10)
    print(phoneBook.nameToPersonMap.count)
}
</code></pre>

<p>To test my code I run 1000 concurrent threads that simply add a new Person to the PhoneBook. Each Person is unique so after the 1000 threads complete I am expecting the PhoneBook to contain a count of 1000. Everytime I perform a write I perform a dispatch_barrier call, update the hash tables, and return. To my knowledge this is all we need to do; however, after repeated runs of the 1000 threads I get the number of entries in the PhoneBook to be inconsistent and all over the place:</p>

<pre><code>Phone Book Entries: 856
Phone Book Entries: 901
Phone Book Entries: 876
Phone Book Entries: 902
Phone Book Entries: 912
</code></pre>

<p>Can anyone please help me figure out what is going on? Is there something wrong with my locking code or even worse something wrong with how my test is constructed? I am very new to this multi-threaded problem space, thanks!</p>
","<swift><multithreading><grand-central-dispatch><data-synchronization><barrier>","2018-03-05 00:24:33","6721","10","3","73592088","<p>In some cases you use might <a href=""https://developer.apple.com/documentation/foundation/nscache"" rel=""nofollow noreferrer"">NSCache</a> class. The documentation claims that it's thread safe:</p>
<blockquote>
<p>You can add, remove, and query items in the cache from different threads without having to lock the cache yourself.</p>
</blockquote>
<p>Here is an <a href=""https://www.swiftbysundell.com/articles/caching-in-swift/"" rel=""nofollow noreferrer"">article</a> that describes quite useful tricks related to <code>NSCache</code></p>
"
"48989136","How can I synchronize local database to cpanel database","<p>i am creating a laravel application and want to host windows PC with data synchronization on my live host cpanel. It is possible or not? if yes, how can i do synchronization my local Xampp project phpmyadmin database in my cpanel phpmyadmin database. Xampp is compatible for large amount of data or not?</p>

<p>Give the local database to live database synchronization idea.</p>
","<xampp><cpanel><data-synchronization>","2018-02-26 13:00:04","255","0","1","48989185","<p>You can export your database from your xampp control panel and import it in your phpmyadmin of cpanel</p>
"
"48944776","Files & Database Automatic Synchronisation","<p>I have to realise an web travel guide for our trains.</p>

<p>On the one hand, I have the <strong>backend</strong>, based on the Wordpress API, where the user can add and modify content. This site is hosted on a simple public server. </p>

<p>On the second hand, I have the <strong>frontend</strong>, a progressive web app based on the  Wordpress API, where the user can see the content. This webapp is hosted on a Raspberry Pi accessible from a hotspot in a no network place.</p>

<p>One is always <strong>online</strong>, the other is almost always <strong>offline</strong>.</p>

<p>I think the Wordpress API should be installed on the both, but I don't know how can I synchronise the backend modification (MySQL Database and maybe WP files) with the offline PWA each time the hotspot has a internet connection.</p>

<p>Anyone has an idea ? </p>

<p>Thank you</p>
","<mysql><wordpress><wordpress-rest-api><data-synchronization><offlineapps>","2018-02-23 09:31:27","63","-1","1","48944927","<p>Firstly !
Have the API on your hand with access to Database , Have Same Connection of Database on both End's  , so you can have your database synced !</p>
"
"48761159","Execute xp_cmdshell using C#","<p>We have created a Windows Forms application to synchronize databases using merge publication and merge pull subscription. We have created publication and subscription successfully. Now We want to start synchronize data.</p>

<p>For that we want to execute below SQL Command:</p>

<pre class=""lang-sql prettyprint-override""><code>-- Declare the variables.  
DECLARE @publicationDB AS sysname;
DECLARE @Subscriber AS sysname;
DECLARE @Publisher AS sysname;
DECLARE @SubscriptionDB AS sysname;
DECLARE @Publication AS sysname;
DECLARE @sql VARCHAR(8000);
DECLARE @sqlDist VARCHAR(8000);
SET @Publisher = 'MSSQLSERVER2014';
SET @Subscriber = 'SQLEXPRESS2014'; 
SET @PublicationDB = 'ServerDB' ;
SET @SubscriptionDB = 'ClinetDB';
SET @Publication = 'ServerDB_PUBLICATION';

--Start the Merge Agent with concurrent upload and download processes.  
SET @sql = '""C:\Program Files\Microsoft SQL Server\130\COM\REPLMERG.EXE"" -Publication ' + @Publication+ ' -Publisher ' + @Publisher + ' -Subscriber ' + @Subscriber +   ' -Distributor ' + @Publisher + ' -PublisherDB ' + @PublicationDB +   ' -SubscriberDB ' + @SubscriptionDB + ' -PublisherSecurityMode 0 -PublisherLogin sa -PublisherPassword Abc@1234  -OutputVerboseLevel 2  -SubscriberSecurityMode 1  -SubscriptionType 1 -DistributorSecurityMode 0   -DistributorLogin sa -DistributorPassword Abc@1234 -Validate 3  -ParallelUploadDownload 1'
EXEC master..xp_cmdshell @sql
</code></pre>

<p>How to execute above command in C#? We don't wont to use cmd or powershell.</p>

<p>We have tried below but get syntax error at @publication:</p>

<pre><code>string SP_xp_cmdshell = ""'C:\\Program Files\\Microsoft SQL Server\\130\\COM\\REPLMERG.EXE' -Publication "" + mPublication + "" -Publisher "" + mPublisher + "" -Subscriber "" + mSubscriber + "" -Distributor "" + mPublisher + "" -PublisherDB "" + mPublicationDatabase + "" -SubscriberDB "" + mSubscriptionDatabase + "" -PublisherSecurityMode 0 -PublisherLogin "" + mLogin + "" -PublisherPassword "" + mPassword + "" -OutputVerboseLevel 2 -SubscriberSecurityMode 1 -SubscriptionType 1 -DistributorSecurityMode 0 -DistributorLogin "" + mLogin + "" -DistributorPassword "" + mPassword + "" -Validate 3 -ParallelUploadDownload 1"";
cmd.Connection = mConnection; //master connection
cmd.CommandText = ""xp_cmdshell ""+ SP_xp_cmdshell;
cmd.CommandType = CommandType.Text;
cmd.ExecuteNonQuery();
</code></pre>
","<c#><database-replication><data-synchronization><merge-replication><xp-cmdshell>","2018-02-13 07:01:40","1026","1","1","49148861","<p>xp_cmdshell  is used to call Command prompt from Sql Server Management Studio. 
To execute REPLMERG.EXE from SSMS you need to execute xp_cmdshell  with path and parameters. </p>

<p>To execute REPLMERG.EXE from C# see below code:</p>

<pre><code>      string Query= @""""""C:\\Program Files\\Microsoft SQL Server\\130\\COM\\REPLMERG.EXE"""" "" +
@""-Publisher [SQLSERVER] -PublisherDB [Server_Database] -Publication [PUBLICATION_NAME] "" +
@""-Subscriber [SQLEXPRESS] -SubscriberDB [Express_Database] -SubscriptionType 1 "" +
@""-SubscriberSecurityMode 0 -SubscriberLogin SQLLogin -SubscriberPassword SQLPass -Distributor [SQLSERVER] "" +
@""-OutputVerboseLevel 2 -PublisherSecurityMode 0 -PublisherLogin SQLLogin -PublisherPassword SQLPass "" +
@""-DistributorSecurityMode 0 -DistributorLogin SQLLogin -DistributorPassword SQLPass -Validate 1  -ParallelUploadDownload 1"";

             try
             {
                 Process proc = new Process();
                 proc.StartInfo.FileName = ""CMD.exe"";
                 proc.StartInfo.Arguments = ""/c "" + Query;
                 proc.StartInfo.WindowStyle = ProcessWindowStyle.Hidden;  //to hide console window
                 proc.Start();
                 proc.WaitForExit();

                 int result = proc.ExitCode;
                 if (result != 0) // exitcode is 0 for successful synchronization
                 {
                     throw new Exception();
                 }
             }
             catch (Exception ex)
             {
                 // Implement appropriate error handling here.
                 Console.WriteLine(""error:"" + ex.Message);
             }
</code></pre>
"
"48463910","CouchDB - Does synchronization copy a database to all users?","<p>I was looking for various information on <strong>Pouchdb</strong> on the Internet and I liked the idea of <strong>synchronizing</strong> remote databases with local browser database. The first question that came to my mind is whether it stores a whole database on each device and creates copies on each device? Imagine that I'm creating a chat application and all communications will be stored on all devices using this app. One user can then look into the conversation of others. I looked at the Pouchdb documentation and found <strong>filtered replication</strong>. Could anybody explain how this synchronization works and whether it is possible to synchronize data only to a particular user?</p>

<p>Example: </p>

<p><strong>Here is a remote database e.g. in cloud, where are saved all informations.</strong></p>

<p><strong><em>Remote database:</em></strong></p>

<p>/ user / secret informations /</p>

<p>/ Joe / ""secret information"" / <br>
/ Dan / ""secret info again"" /</p>

<p><strong>Here i want to synchronize data only for a particular user. Do not sync whole database but only user specific data.</strong></p>

<p><strong><em>Joe's device</em></strong></p>

<p>/ user / secret informations /</p>

<p>/ Joe / ""secret information"" / </p>

<p><strong><em>Dan's device</em></strong></p>

<p>/ user / secret informations /</p>

<p>/ Dan / ""secret info again"" / </p>

<p>I believe I explained my question. Thanks for the anwers. </p>
","<database><couchdb><indexeddb><pouchdb><data-synchronization>","2018-01-26 14:57:03","901","4","1","48466158","<p>PouchDB can be used in a many different configurations but a common pattern is to have a PouchDB database in a web browser syncing to a remote Apache CouchDB or Cloudant database.</p>

<p>Because the permissions model for Apache CouchDB is ""per database"" (i.e. you have read/write/admin access on a whole database or not, there is not currently ""per document"" access control) the mobile PouchDB database synchronises to a cloud ""per user"" database. That is, if your app has 10,000 users, it would have 10,000 PouchDB databases on each users' individual browsers and 10,000 CouchDB databases on the server side.</p>

<p>This ""one database per user"" approach is adopted by frameworks such as <a href=""http://hood.ie/"" rel=""noreferrer"">Hood.ie</a> - it neatly keeps each user's data separate from each others. The drawback is that it is very difficult to query the entire data set as a whole - continuous replication from the ""per user"" databases to a central database doesn't scale well.</p>

<p>Other solution in this space are </p>

<ul>
<li><a href=""https://github.com/cloudant-labs/envoy"" rel=""noreferrer"">Cloudant Envoy</a> which stores all the users' data in a single server-side database, but separates each user by manipulation of the the <code>_id</code> field</li>
<li><a href=""https://github.com/redgeoff/spiegel"" rel=""noreferrer"">Spiegel</a> which uses ""one database per user"" and a replication agent that manages the movement of ""per user"" data into an additional database, for reporting purposes.</li>
</ul>
"
"48379097","Is there a way to implement a lock-free solution to ensure the data integrity across multiple instances","<p>I am running multiple instances of a java application, and all those instances are connected to one mysql database, I came a cross a problem where I have some data (numerical) that I want to ensure the integrity of.</p>

<p>For example I got a request on instance A saying that the value should be incremented by 5, and at the same time another request on instance B incremented the value by 1, and so on...</p>

<p>I have been looking at different implementations for my problem, and so far all of them used some sort of a locking mechanism, for example this piece of code is what I currently have.</p>

<pre><code>    protected Lock getLock(String seqName) {
        Lock oldLock, dbSequenceLock = dbSequenceLocks.get(seqName)
        if (dbSequenceLock == null) {
            dbSequenceLock = new ReentrantLock()
            oldLock = dbSequenceLocks.putIfAbsent(seqName, dbSequenceLock)
            if (oldLock != null) return oldLock
        }
        return dbSequenceLock
    }

     protected String update(String seqName, int value) {
        try{
            Lock dbSequenceLock = getDbSequenceLock(seqName)
            dbSequenceLock.lock()
            Type record = selectForUpdate(seqName);//This selects the 
            entity for update
            record.set(""val"", record.get(""val"") + val);//increment or decrement the value...
            record.update();
        } finally {
            dbSequenceLock.unlock()
        }
</code></pre>

<p>I am not sure if this is the best way to do this, and if I can get rid of the locking somehow, any suggestions?</p>
","<java><mysql><locking><data-synchronization>","2018-01-22 10:12:04","72","0","1","48379432","<p>Assuming that you're maintaining the ACIDity of the database, these transactions will be in isolation of each other. So, unless you have very slow queries for updates, it won't matter. The query that runs later will update the record and it will persist.</p>

<p>The easiest but inefficient solution would be to push your changes in a queue and execute them one after another. This way, you can track which of the changes was done earlier and which was done later. You can also use Redis's feature of atomic increment wherein you can push your changes for that variable concurrently and only the latest one will persist. Obviously, this means that there won't be immediate consistency but only eventual consistency.</p>
"
"48064589","Firebase function in firebase function","<p>The following function has to return all of my users's friends list. However it only does it for one of the friends. I know this is because the 2 firebase functions are running async, however I am not sure what I have to change the function so that it runs the way is should. That is retrieve all friends.</p>

<pre><code>///retrieves all of user's friends
    func fetchFriends(completion: @escaping ([FriendModel])-&gt;()){
        FRIEND_REQ_REF.child(CURRENT_USER_ID).observe(.childAdded, with: {(snapshot) in
            var friends = [FriendModel]()
            if snapshot.value as? Int == 0 {
                self.USERS_REF.child(snapshot.key).observeSingleEvent(of: .value, with: {(snap) in
                    if let dictionary = snap.value as? [String : AnyObject]{
                        let friend = FriendModel()
                        friend.setValue(dictionary[""userName""], forKey: ""userName"")
                        friend.setValue(dictionary[""name""], forKey: ""name"")
                        friends.append(friend)
                        completion(friends)
                    }
                })
            }
        })
    }
</code></pre>

<p>this is my data structure:</p>

<pre><code>FRIEND_REQ_REF
              firebaseUserID
                 friendFirebasegivenID : 0
                 anotherFriendFirebasegivenID : 0
USERS_REF
            friendFirebasegivenID
                 userName : String 
                 name : String 
            anotherFriendFirebasegivenID : 0
                 userName : String 
                 name : String 
</code></pre>
","<swift><firebase><asynchronous><firebase-realtime-database><data-synchronization>","2018-01-02 16:35:17","115","0","1","48065939","<pre><code>///retrieves all of user's friends
    func fetchFriends(completion: @escaping ([FriendModel])-&gt;()){
        FRIEND_REQ_REF.child(CURRENT_USER_ID).observe(.value, with: {(snapshot) in
            var friends = [FriendModel]()

            if let dict = snapshot.value as? [String : AnyObject] {
                for (_,k) in dict.enumerated() {

                    if k.value == 0 {

                        self.USERS_REF.child(k.key).observeSingleEvent(of: .value, with: {(snap) in
                    if let dictionary = snap.value as? [String : AnyObject]{
                        let friend = FriendModel()
                        friend.setValue(dictionary[""userName""], forKey: ""userName"")
                        friend.setValue(dictionary[""name""], forKey: ""name"")
                        friends.append(friend)
                        completion(friends)
                    }
                })
                    }


                }
            }

        })
    }
</code></pre>
"