QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"64402337","Establish a connection between Azure CosmosDB SQL API and Google Bigquery","<p>I'm using Azure CosmosDB SQL API and I'm looking a way to send data from CosmosDB to Google Big query. I'm
planning to use Kafka or Azure ADF for the same. I'm not sure this is correct approach/tools.</p>
<p>Is there any best practice or tool or connecter which I can use to send data from CosmosDB to Google Bigquery.</p>
","<apache-kafka><google-bigquery><azure-cosmosdb><azure-data-factory>","2020-10-17 12:16:29","199","0","1","64438142","<p>Data Factory supports Azure Cosmos DB (SQL API) as source and sink, but doesn't support <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-bigquery"" rel=""nofollow noreferrer"">Google Bigquery</a> as the sink.
<a href=""https://i.stack.imgur.com/vhnKF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vhnKF.png"" alt=""enter image description here"" /></a></p>
<p>It means that we can not copy the data from Cosmos DB(SQL API) to Google Bigquery.</p>
"
"64402220","Accessing bearer token every few mins in Azure","<p>I have an ADF pipeline which has an associated trigger which executes it in a fixed cycle (e.g. every 15 mins scheduled trigger). It calls some REST APIs associated with Azure resources. So every time it needs to get a bearer token. Now it's not a good idea to get bearer token every 15 mins because bearer tokens are valid up to an hour (is it a problem if we do get fresh bearer token every 15 mins?). So is there any way to persist and use the same token up to an hour? I could be storing it in a key vault but again I need to issue a REST call to access it from key vault -- so what's the benefit then. What's the standard Azure recommended way to get bearer token when we are going to need it pretty much now and then (say every 10-15 mins)?</p>
","<azure><rest><oauth-2.0><azure-data-factory>","2020-10-17 12:00:51","404","1","1","64405694","<p>This doesn't seem like a problem worth solving to me. I assume you're using the client credential flow to acquire the token, so no user interaction is needed. It should be fine to get a new token every 15 minutes.</p>
<p>Caching this token somewhere will make your solution more complicated with virtually no benefit. Azure AD does not care whether you call it once an hour or 4 times an hour.</p>
"
"64394108","Base64 Encoded secrets","<p>Our SQL passwords as base64 encoded in Key Vault. Is there a way to have data factory decode the secret before it connects to SQL? I looked at the <code>base64ToString</code> function but I can't find the correct syntax for data factory to first get the secret from kv before decoding it.</p>
<p>Thanks</p>
","<encoding><base64><azure-data-factory>","2020-10-16 17:54:09","902","0","2","64422148","<p>Data Factory will get the secret from key vault(through secret name) to connect the Azure SQL database directly.
<a href=""https://i.stack.imgur.com/dqEhP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dqEhP.png"" alt=""enter image description here"" /></a></p>
<p>In linked service key vault settings, it doesn't support the expression and function <code>base64ToString</code> with <code>Add dynamic content</code>.
<a href=""https://i.stack.imgur.com/ZLyW6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZLyW6.png"" alt=""enter image description here"" /></a></p>
<p>In one word, there isn't a way to have data factory decode the secret before it connects to SQL. We can  not do that.</p>
"
"64394108","Base64 Encoded secrets","<p>Our SQL passwords as base64 encoded in Key Vault. Is there a way to have data factory decode the secret before it connects to SQL? I looked at the <code>base64ToString</code> function but I can't find the correct syntax for data factory to first get the secret from kv before decoding it.</p>
<p>Thanks</p>
","<encoding><base64><azure-data-factory>","2020-10-16 17:54:09","902","0","2","64430557","<p>Yes, while reading the SQL Password from Azure Key Vault directly in linked service, it cannot decode the value. However, we can use below approach using web activity along with copy activity that enables us to decode password from Key Vault while accessing SQL database. The output can be marked as &quot;Secure&quot; in web activity that enables us to ensure password is not displayed during execution of pipeline. Please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-use-azure-key-vault-secrets-pipeline-activities"" rel=""nofollow noreferrer"">document</a> for accessing secrets using web activity.</p>
<p><a href=""https://i.stack.imgur.com/RSH0s.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSH0s.gif"" alt=""enter image description here"" /></a></p>
"
"64394055","How to copy all files and folders in specific directory using azure data factory","<p>I have one folder in adls gen2 say it as mysource1 folder .. which has 100's of subfolder s and each subfolder again contains folders and many files ..</p>
<p>How can I copy all of the folders and files in mysource1 using azure data factory ..</p>
","<azure><azure-data-factory><azure-data-lake>","2020-10-16 17:49:21","9762","4","2","64410422","<p>You could use binary as source format. It will help you copy all the folders and files in source to sink.</p>
<p>For example: this is my container <code>test</code>:
<a href=""https://i.stack.imgur.com/Nx6pp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Nx6pp.png"" alt=""enter image description here"" /></a></p>
<p><strong>Source dataset:</strong>
<a href=""https://i.stack.imgur.com/5xaO7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5xaO7.png"" alt=""enter image description here"" /></a></p>
<p><strong>Sink dataset:</strong>
<a href=""https://i.stack.imgur.com/t91mb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/t91mb.png"" alt=""enter image description here"" /></a></p>
<p><strong>Copy active:</strong>
<a href=""https://i.stack.imgur.com/OtWRe.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OtWRe.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output:</strong>
<a href=""https://i.stack.imgur.com/5yVNi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5yVNi.png"" alt=""enter image description here"" /></a></p>
<p>You can follow my steps.</p>
"
"64394055","How to copy all files and folders in specific directory using azure data factory","<p>I have one folder in adls gen2 say it as mysource1 folder .. which has 100's of subfolder s and each subfolder again contains folders and many files ..</p>
<p>How can I copy all of the folders and files in mysource1 using azure data factory ..</p>
","<azure><azure-data-factory><azure-data-lake>","2020-10-16 17:49:21","9762","4","2","68560029","<p>use ingest tab on ADF Home page, there you could specify source location using linked service and target location</p>
"
"64390248","Rows to Columns in ADF","<p>is there a way to transpose rows to columns in Azure Data Factory e.g.
I have an excel file with some vertical data in 2 column c1 &amp; c2</p>
<pre><code>c1 c2

A   B 
1   2
C   D
</code></pre>
<p>to</p>
<pre><code>A  1  C
B  2  D
</code></pre>
<p>where A - 1 - C are the new columns and one row with the values B - 2 - D .</p>
<p>Thanks .</p>
","<azure><azure-data-factory>","2020-10-16 13:34:55","1200","0","1","64393800","<p>Pivot transformation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-pivot"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-pivot</a></p>
<p>Unpivot transformation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-unpivot"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-unpivot</a></p>
"
"64386422","Error while loading data from ADLS Gen2 to Azure Synapse","<p>I am trying to perform a copy activity from ADLS Gen2 to Azure Synapse warehouse using polybase direct copy. while copying i am getting the error like :</p>
<p>&quot; Please make sure SQL DW has access to ADLS Gen2&quot;. but why does it need access to ADLS Gen2 when data factory can take care of it? and also how do i give that access if i need to give that.</p>
<p>Thanks.</p>
","<azure><azure-data-factory><azure-data-lake><azure-synapse>","2020-10-16 09:23:51","739","0","1","64420489","<p>On my side, I can success copy.</p>
<p>You can check the whether the settings is different bettween your and mine.</p>
<p>This is the firewall settings of my sql server:</p>
<p><a href=""https://i.stack.imgur.com/0Twih.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Twih.png"" alt=""enter image description here"" /></a></p>
<p>I didn't set any firewall limit, I allow my client IP and other azure service to access my sql server.</p>
<p>And this is my copy activity:</p>
<p><a href=""https://i.stack.imgur.com/uDE6W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uDE6W.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/eCccZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eCccZ.png"" alt=""enter image description here"" /></a></p>
<p>This pipeline works fine:</p>
<p><a href=""https://i.stack.imgur.com/m6sSa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m6sSa.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/DPe3p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DPe3p.png"" alt=""enter image description here"" /></a></p>
"
"64385611","DevOps deployment based on releases","<p>I have a data factory instance which is linked to github, used for development.</p>
<p>I am having two different changes in the two different branches of data factory.
<strong>change01</strong> and <strong>change02</strong></p>
<p>I have merged these two changes into <strong>master</strong> branch and did a publish.</p>
<p>While doing a CI/CD even though these two changes are now available in the dev data factory instance, is it possible to deploy only change01 into other environments?</p>
<p>How can we control which release/change should go for deployment into other environments?
Can we do a build directly from a branch and push to prod?</p>
","<azure><azure-data-factory>","2020-10-16 08:32:37","54","0","2","64389473","<p>I would suggest to use separate branch and configure your builds to use proper one. Verify you builds in Azure Dev Ops.</p>
<p>It can be also helpful to cherry-pick changes which shouldn't be deployed.</p>
"
"64385611","DevOps deployment based on releases","<p>I have a data factory instance which is linked to github, used for development.</p>
<p>I am having two different changes in the two different branches of data factory.
<strong>change01</strong> and <strong>change02</strong></p>
<p>I have merged these two changes into <strong>master</strong> branch and did a publish.</p>
<p>While doing a CI/CD even though these two changes are now available in the dev data factory instance, is it possible to deploy only change01 into other environments?</p>
<p>How can we control which release/change should go for deployment into other environments?
Can we do a build directly from a branch and push to prod?</p>
","<azure><azure-data-factory>","2020-10-16 08:32:37","54","0","2","64428768","<p>To best accomplish this will have to to publish outside of the Data Factory editor.  Each branch contains the necessary ARM components to publish the Data Factory ARM templates.  The issue is when clicking the publish button Data Factory/ADO behind the scenes consolidates the ARM templates to just one .json file to make it easier to deploy while simultaneously deploying to the Data Factory destination.</p>
<p>The best course of action here might be to determine how to publish the ARM templates w/o clicking the publish button.  This can be done by using ARM deployments or Powershell.</p>
<p>Furthermore I'd consider the <a href=""https://blog.johnfolberth.com/understanding-data-factory-in-a-ci-cd-world/"" rel=""nofollow noreferrer"">potential options</a> you have when considering how to managed and deploy Data Factory under CI/CD.</p>
"
"64374678","Only Copying the Latest Folder/Files for a specific day with Azure Data Factory","<p>Thanks Steve i have followed your steps and my pipeline job now runs successfully, but the destination table does not populate with any data, can you please check to see where i went wrong?</p>
<p>Pipeline Output:</p>
<p><a href=""https://i.stack.imgur.com/5RAlz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5RAlz.png"" alt=""enter image description here"" /></a></p>
<p>Variables and For Each container Details:</p>
<p><a href=""https://i.stack.imgur.com/wKneW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wKneW.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/fYd37.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fYd37.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/IEqBc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IEqBc.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/6e0Et.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6e0Et.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2020-10-15 15:13:01","1217","0","1","64381883","<p><strong>Screenshot of my test pipeline:</strong></p>
<p><a href=""https://i.stack.imgur.com/pRYnO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pRYnO.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step:</strong>
1.create two variable.</p>
<p><a href=""https://i.stack.imgur.com/wzcRE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wzcRE.png"" alt=""enter image description here"" /></a></p>
<p>2.create a Get Metadata activity.</p>
<p><a href=""https://i.stack.imgur.com/TPeuH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TPeuH.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/dj9bs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dj9bs.png"" alt=""enter image description here"" /></a></p>
<p>3.create a For Each activity and use this expression(check Sequential option):</p>
<p><code>@activity('Get Metadata1').output.childItems</code>
<a href=""https://i.stack.imgur.com/9EvSm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9EvSm.png"" alt=""enter image description here"" /></a></p>
<p>4.inside of For Each activity:</p>
<p><a href=""https://i.stack.imgur.com/NLeKG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NLeKG.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/wDrmq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wDrmq.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/8vbuO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8vbuO.png"" alt=""enter image description here"" /></a></p>
<p>5.copy activity:
<a href=""https://i.stack.imgur.com/ATxcg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ATxcg.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/0Kzp8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Kzp8.png"" alt=""enter image description here"" /></a></p>
"
"64371538","Error when trying to move data from on-prem SQL database to Azure Delta lake","<p>I am trying to move large amounts of reference data from on-prem SQL server to Delta lake to be used in databricks processing. To move this data, I am trying use Azure Data Factory via simple Copy data activity. but as soon as I start the pipeline I get the below error.  I googled this error but could not find any matches.</p>
<p>Note that sink delta table is not present in the delta lake ? does this error mean that I have create tables manually before moving data to delta lake ?</p>
<p><em>Operation on target Copy data1 failed: ErrorCode=AzureDatabricksTableIsNotDeltaFormat,The table <code>benefit</code> is not delta format.</em></p>
","<azure><azure-data-factory><delta-lake>","2020-10-15 12:25:49","127","0","1","64448697","<p>Resolved this using data flow rather than copy tasks.</p>
"
"64361129","Best Azure serverless service to run python data processing project","<p>I am quite new to Azure and I am getting a bit lost in all the available services.</p>
<p><strong>What do I want to do</strong>:
I want to run a Python project serverless on Azure which gets data from a database, processes it, does some analysis and writes it to a database again. After it's done, it should stop the server again. This can be triggered by some data uploaded to a storage location or has to run periodically. Most optimal I would like to be able to build it through CD (GitHub Actions).</p>
<p><strong>What did I find</strong>
Reading through the documentation and some other resources, these are the services I think I can use in descending order, but I am not 100% sure.</p>
<ol>
<li>Azure Functions</li>
<li>Azure Container Instances</li>
<li>Azure Web Apps</li>
</ol>
<p>Also I found <a href=""https://stackoverflow.com/questions/45716757/options-for-running-python-scripts-in-azure"">this</a>, but seems outdated.</p>
<p><strong>Question</strong>:
Which Azure service matches the best for my use case.</p>
","<azure><azure-web-app-service><azure-functions><etl><azure-data-factory>","2020-10-14 20:35:56","139","0","1","64361685","<p>What you are trying to accomplish has a name - <strong>ETL (Extract-Transform-Load)</strong>. This is a general pattern when you need to take data from its source (DB in your case), manipulate it, and offload it to some destination (DB in your case again).</p>
<p>You listed some valid options. From your list, <em>Azure Function</em> will be a truly serverless option as you aren't billed when it's idling. Other options can also accomplish the task, but you will pay also for hours when your code does nothing.</p>
<p>There's also a service just for your need: <strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal"" rel=""nofollow noreferrer"">Azure Data Factory</a></strong>. You can design your data flow by using the UI and include your Python functions as steps. The overall result will be a data pipeline (like CD for data). And of course it's <em>serverless</em>. You will be billed only for the time the pipeline is executing.</p>
"
"64360580","Using parameters in Azure Data Factory Data Mapping Activity","<p>I am using a 'mapping data flow' activity in ADF to extract and load some records. I have created 2 parameters in the data flow activity - table_name, schema_name - which I'm trying to use to make the same activity work for multiple tables.</p>
<p>In the Source Options tab on Source(within data flow activity) I have the following query -</p>
<pre class=""lang-sql prettyprint-override""><code>select * from '{$SchemaName}'.'{$TableName}'
</code></pre>
<p>I'm getting the following error message when I debug -</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;message&quot;: &quot;Job failed due to reason: at Source 'source1'(Line 2/Col 11): Column operands are not allowed in literal expressions. Details:at Source 'source1'(Line 2/Col 11): Column operands are not allowed in literal expressions&quot;,
  &quot;failureType&quot;: &quot;UserError&quot;,
  &quot;target&quot;: &quot;GetLatestData&quot;,
  &quot;errorCode&quot;: &quot;DFExecutorUserError&quot;
}
</code></pre>
<p>Anyone have any thoughts on what I'm doing wrong?</p>
","<azure><azure-data-factory>","2020-10-14 19:50:38","728","1","1","64363724","<p>Please try this expression:</p>
<pre><code>&quot;select * from {$SchemaName}.{$TableName}&quot;
</code></pre>
<p>My test:</p>
<p>1.create two parameters:
<a href=""https://i.stack.imgur.com/MZ49y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MZ49y.png"" alt=""enter image description here"" /></a></p>
<p>2.click 'Add dynamic content [Alt+P]'
<a href=""https://i.stack.imgur.com/OE98P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OE98P.png"" alt=""enter image description here"" /></a></p>
<p>3.type this expression: <code>&quot;select * from {$SchemaName}.{$TableName}&quot;</code>
<a href=""https://i.stack.imgur.com/uXoNR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uXoNR.png"" alt=""enter image description here"" /></a></p>
<p>Data preview:
<a href=""https://i.stack.imgur.com/szdFk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/szdFk.png"" alt=""enter image description here"" /></a></p>
"
"64360273","How to dynamically place Azure Data flow output in folders named after date","<p>I have a dataflow in a pipeline that outputs a json file. Every time it runs I want the json file to be placed in a folder that is name the dynamically the current time stamp down to the second that the folder is pipeline is ran. I have tried put a utcNow function in the file path setup of the output dataset <a href=""https://i.stack.imgur.com/xRvqY.jpg"" rel=""nofollow noreferrer"">dataset folderpath setup screenshot</a>. But I get a</p>
<p>&quot;Cannot create through WASB that has colons in the name&quot; error.</p>
<p>Even though I have created folders with colons in the name through Azure storage before. Please help.</p>
","<azure-files><azure-data-factory>","2020-10-14 19:27:41","338","0","1","64365271","<p>I tested that set folder name with colons and expression in Data Flow.</p>
<p>No matter we set the folder name with colons or expression, We will get the error:</p>
<pre><code>{&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to reason: JSON parsing error, unsupported encoding or multiline&quot;,&quot;Details&quot;:&quot;at Sink 'sink1': org.apache.spark.SparkException: Job aborted.&quot;}
</code></pre>
<p><a href=""https://i.stack.imgur.com/iZVus.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iZVus.png"" alt=""enter image description here"" /></a></p>
<p>According the error message, folder name with colons and expression function are not supported in Data Flow.</p>
<p>It only works in Copy active sink dataset.</p>
"
"64355410","Source linked service should not have ""service principal"" as authentication method","<p>I am trying to copy data from Azure data lake Gen2 to Azure synapse(SQL data warehouse) through Azure data factory. Following are some details:</p>
<p>source(ADLS) linked service authentication type: service principal
sink(Synapse) linked service authentication type: managed identity</p>
<p>Copy method selected : Polybase</p>
<p>While validating, i am getting this error: &quot;Source linked service should not have authentication method as Service principal&quot;.
when i selected &quot;bulk insert&quot; copy type, it works fine.. can anyone help me understand this? is it written anywhere that for polybase we should have same authentication type for linked service?</p>
","<azure><azure-data-factory><azure-data-lake><azure-synapse>","2020-10-14 14:23:27","332","0","1","64365851","<p>This is because direct copy by using PolyBase from Azure Data Lake Gen2 only support Account key authentication or managed identity authentication. You can refer to this  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#direct-copy-by-using-polybase"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>So if you want to direct copy by using PolyBase, you need change your authentication method to account key or managed identity.</p>
<p>There is a workaround, Staged copy by using PolyBase. You can refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#staged-copy-by-using-polybase"" rel=""nofollow noreferrer"">documentation</a> about this.</p>
<p><a href=""https://i.stack.imgur.com/PZRpV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PZRpV.png"" alt=""enter image description here"" /></a></p>
"
"64353742","ADF activity does not return proper error codes","<p>when an activity failed because of unauthorized
it is returning below error message with bad request code - not 401 error.
is there any better way we can capture error messages in ADF</p>
<pre><code>Operation on target copy_api_to_file failed: 
  Failure happened on 'Source' side.
  ErrorCode=UserErrorHttpStatusCodeIndicatingFailure,
  'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
  Message=The HttpStatusCode 401 indicates failure,
  xxxx,Source=Microsoft.DataTransfer.ClientLibrary,'
</code></pre>
","<azure><.net-core><azure-devops><azure-data-factory>","2020-10-14 12:54:11","780","0","1","64368581","<p>We can see that the error message contain <code>Message=The HttpStatusCode 401 indicates failure.</code> The HTTP status 401 response code indicates that Unauthorized because it lacks valid authentication credentials for the target resource.</p>
<p>Since this is Azure data factory product issue, we recommend that you can raise a new ticket <a href=""https://azure.microsoft.com/en-us/services/data-factory/"" rel=""nofollow noreferrer"">here</a>. Product team will check it and reply you.</p>
<p>In addition, we could Monitor and Alert Data Factory by using Azure Monitor, you can refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#configure-diagnostic-settings-and-workspace"" rel=""nofollow noreferrer"">doc</a> for more details.</p>
"
"64352195","Azure Data Factory Get Metadata to get blob filenames and transfer them to Azure SQL database table part 2","<p>I am trying to use Get Metadata activity in Azure Data Factory in order to get blob filenames and copy them to Azure SQL database table.
I added the stored procedure activity after Get Metadata activity. Here is my new activity stored procedure, I added the parameter as suggested however, I changed the name to JsonData as my stored procedure requires this parameter.</p>
<p><a href=""https://i.stack.imgur.com/WMeMW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WMeMW.png"" alt=""storedprocedure1"" /></a></p>
<p>This is my stored procedure.</p>
<pre><code>/****** Object:  StoredProcedure [dbo].[InsertDataJSON]    Script Date: 10/14/2020 11:01:30 AM ******/
SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO
/*USE datafactorypoc1*/

ALTER PROCEDURE [dbo].[InsertDataJSON] (
    @JsonData NVARCHAR (MAX)
)
AS
BEGIN
    DECLARE @err int

    INSERT INTO extractFileNameTest1 (ItemName, ItemType, EffIntegrationRunTieme, ExecutionDuration, DurationInQueue)
    SELECT ItemName, ItemType, EffIntegrationRunTieme, ExecutionDuration, DurationInQueue
    FROM OPENJSON (@JsonData, N'$') WITH (
        ItemName VARCHAR(255) N'$.ItemName',
        ItemType VARCHAR(255) N'$.ItemType',
        EffIntegrationRunTieme VARCHAR(255) N'$.EffIntegrationRunTieme',
        ExecutionDuration INT N'$.ExecutionDuration',
        DurationInQueue INT N'$.DurationInQueue'
    )    

    SELECT @err = @@ERROR
    RETURN (@err)
END
</code></pre>
<p><a href=""https://i.stack.imgur.com/Fr4lk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fr4lk.png"" alt=""storedprocedure2"" /></a></p>
<p>I get this error at the stored procedure:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2402&quot;,
    &quot;message&quot;: &quot;Execution fail against sql server. Sql error number: 13609. Error Message: JSON text is not properly formatted. Unexpected character 'S' is found at position 0.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Stored procedure1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/3eIyA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3eIyA.png"" alt=""storedprocedure3"" /></a></p>
<p>But when I check the input, it seems like it already successfully reading the json string itemName.</p>
<p><a href=""https://i.stack.imgur.com/oqvfW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqvfW.png"" alt=""storedprocedure4"" /></a></p>
<p>But, when I check output, it's not there.</p>
<p><a href=""https://i.stack.imgur.com/nXIxD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nXIxD.png"" alt=""storedprocedure5"" /></a></p>
<p>Could you please help me check what I did wrong here? Is it my stored procedure? Thank you very much in advance.</p>
<p><em><strong><strong>Update 15/10/2020</strong></strong></em>
I created a new pipeline and move the Stored Procedure inside ForEach activity to connect to Get_Filename_2 instead of the first one ad followed:</p>
<p><a href=""https://i.stack.imgur.com/LrwSH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LrwSH.png"" alt=""ADFPipelineTwo1"" /></a></p>
<p>Here I changed the value of parameter to Get_Filename_2 and output as itemName instead of childitems (because I got an error with using childitems because childitems is from Get_Filename_1 and not 2).</p>
<p><a href=""https://i.stack.imgur.com/fwCWP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fwCWP.png"" alt=""ADFpipelinetwo2"" /></a></p>
<p>After executing the pipeline (which was failed), the input of the last stored procedure is:</p>
<pre><code>{
    &quot;storedProcedureName&quot;: &quot;[dbo].[InsertDataJSON]&quot;,
    &quot;storedProcedureParameters&quot;: {
        &quot;JsonData&quot;: {
            &quot;value&quot;: &quot;FRRNSC84FIN1_A2276801_20200103-152534.json&quot;,
            &quot;type&quot;: &quot;String&quot;
        }
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/kIMHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kIMHA.png"" alt=""ADFpipelinetwo3"" /></a></p>
<p>And the output is:</p>
<pre><code>{
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (West Europe)&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;ExternalActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/0y5jf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0y5jf.png"" alt=""ADFpipelinetwo4"" /></a></p>
<p>For this pipeline, it's failed with following error message.</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2402&quot;,
    &quot;message&quot;: &quot;Execution fail against sql server. Sql error number: 13609. Error Message: JSON text is not properly formatted. Unexpected character 'F' is found at position 0.\r\nFRRNSC84FIN1_A2276801_20200103-152534.json&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Stored procedure1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/KAmtK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KAmtK.png"" alt=""ADFpipelinetwo4"" /></a></p>
<p>On my old pipeline where I have stored procedure outside of ForEach loop. The pipeline did not fail:</p>
<p><a href=""https://i.stack.imgur.com/IVANF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IVANF.png"" alt=""ADFpipeline1"" /></a></p>
<p>Here is the input of the last stored procedure activity:</p>
<pre><code>{
    &quot;storedProcedureName&quot;: &quot;[dbo].[InsertDataJSON]&quot;,
    &quot;storedProcedureParameters&quot;: {
        &quot;JsonData&quot;: {
            &quot;value&quot;: &quot;[{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2274001_20200103-143748_back_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2274001_20200103-143748_right_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2274801_20200103-144811_right_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2275201_20200103-145229_right_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2276801_20200103-152534.json\&quot;,\&quot;type\&quot;:\&quot;File\&quot;}]&quot;
        }
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/mpqzK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mpqzK.png"" alt=""ADFpipieline2"" /></a></p>
<p>And here is the output:</p>
<pre><code>{
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (West Europe)&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;ExternalActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/Lk6Wn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lk6Wn.png"" alt=""ADFpipeline3"" /></a></p>
<p>This pipeline ran successfully however the result in the SQL is not what I really expected, also many NULL values before the first json string but it insert all itemName in the same location. This is because it's outside of ForEach loop I think. But why it insert so many NULL?</p>
<p><a href=""https://i.stack.imgur.com/8yzhz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8yzhz.png"" alt=""SQLresult"" /></a></p>
<p>Here is my stored procedure that I modified:</p>
<pre><code>/****** Object:  StoredProcedure [dbo].[InsertDataJSON]    Script Date: 15/10/2020 10:31:51 ******/
SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO
/*USE datafactorypoc1*/

ALTER PROCEDURE [dbo].[InsertDataJSON] (
    @JsonData NVARCHAR (MAX)
)
AS
BEGIN
    PRINT @JsonData

    /*INSERT INTO Logs values (DEFAULT, @JsonData)*/
    INSERT INTO extractFileNameTest1 values (@JsonData, DEFAULT)
    SELECT * FROM
        OPENJSON (@JsonData)
    WITH (
        ItemName VARCHAR(255) N'$.ItemName',
        ItemType VARCHAR(255) N'$.ItemType'
    )
END
</code></pre>
<p>I tried to delete all data in my test SQL table to see the output of the 2nd fail pipeline. I notice that it did parse correctly the json string filename inside my table and all of them (I only have 5 files in blob storage). But other data are NULL.</p>
<p><a href=""https://i.stack.imgur.com/Sd1R7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sd1R7.png"" alt=""ADFpipelinetwo6"" /></a></p>
<p>To conclude, the result in SQL table of the new pipeline is much better, each file name on the different cell of the table but I got pipeline run failed. Could you please help me check what I did wrong here? Is it the stored procedure or the expression in my last Stored procedure activity inside ForEach loop?</p>
<p>Thank you very much in advance.</p>
","<json><azure><stored-procedures><azure-sql-database><azure-data-factory>","2020-10-14 11:23:38","887","1","2","64441751","<p>For this scenario, I use only 1 Get Metadata activity with:
Dataset of Get Metadata activity = Binary files from Blob storage
Field List of Get Metadata = Child items</p>
<p>This Get Metadata activity has output connected to ForEach activity:
ForEach activity settings =&gt; Items =&gt; @activity('Get_FileName_1').output.childItems</p>
<p>Inside ForEach activity, there is 1 activity which is Stored Procedure:
Stored Procedure Settings has Linked Service = Azure SQL Database with selected Stored Procedure. In the stored procedure that I wrote, I defined the columns that I will read from blob file names. Then, for the parameters of Stored Procedure activity, I define exact same parameters numbers = all columns of @JsonData in stored procedure.
Then, for each value of the parameter, I use Azure Data Factory function @substring to extract different parts of blob file name. Then insert these to the correct columns in Azure SQL table.</p>
<p>Source for Substring function: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#substring"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#substring</a></p>
"
"64352195","Azure Data Factory Get Metadata to get blob filenames and transfer them to Azure SQL database table part 2","<p>I am trying to use Get Metadata activity in Azure Data Factory in order to get blob filenames and copy them to Azure SQL database table.
I added the stored procedure activity after Get Metadata activity. Here is my new activity stored procedure, I added the parameter as suggested however, I changed the name to JsonData as my stored procedure requires this parameter.</p>
<p><a href=""https://i.stack.imgur.com/WMeMW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WMeMW.png"" alt=""storedprocedure1"" /></a></p>
<p>This is my stored procedure.</p>
<pre><code>/****** Object:  StoredProcedure [dbo].[InsertDataJSON]    Script Date: 10/14/2020 11:01:30 AM ******/
SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO
/*USE datafactorypoc1*/

ALTER PROCEDURE [dbo].[InsertDataJSON] (
    @JsonData NVARCHAR (MAX)
)
AS
BEGIN
    DECLARE @err int

    INSERT INTO extractFileNameTest1 (ItemName, ItemType, EffIntegrationRunTieme, ExecutionDuration, DurationInQueue)
    SELECT ItemName, ItemType, EffIntegrationRunTieme, ExecutionDuration, DurationInQueue
    FROM OPENJSON (@JsonData, N'$') WITH (
        ItemName VARCHAR(255) N'$.ItemName',
        ItemType VARCHAR(255) N'$.ItemType',
        EffIntegrationRunTieme VARCHAR(255) N'$.EffIntegrationRunTieme',
        ExecutionDuration INT N'$.ExecutionDuration',
        DurationInQueue INT N'$.DurationInQueue'
    )    

    SELECT @err = @@ERROR
    RETURN (@err)
END
</code></pre>
<p><a href=""https://i.stack.imgur.com/Fr4lk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fr4lk.png"" alt=""storedprocedure2"" /></a></p>
<p>I get this error at the stored procedure:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2402&quot;,
    &quot;message&quot;: &quot;Execution fail against sql server. Sql error number: 13609. Error Message: JSON text is not properly formatted. Unexpected character 'S' is found at position 0.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Stored procedure1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/3eIyA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3eIyA.png"" alt=""storedprocedure3"" /></a></p>
<p>But when I check the input, it seems like it already successfully reading the json string itemName.</p>
<p><a href=""https://i.stack.imgur.com/oqvfW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqvfW.png"" alt=""storedprocedure4"" /></a></p>
<p>But, when I check output, it's not there.</p>
<p><a href=""https://i.stack.imgur.com/nXIxD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nXIxD.png"" alt=""storedprocedure5"" /></a></p>
<p>Could you please help me check what I did wrong here? Is it my stored procedure? Thank you very much in advance.</p>
<p><em><strong><strong>Update 15/10/2020</strong></strong></em>
I created a new pipeline and move the Stored Procedure inside ForEach activity to connect to Get_Filename_2 instead of the first one ad followed:</p>
<p><a href=""https://i.stack.imgur.com/LrwSH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LrwSH.png"" alt=""ADFPipelineTwo1"" /></a></p>
<p>Here I changed the value of parameter to Get_Filename_2 and output as itemName instead of childitems (because I got an error with using childitems because childitems is from Get_Filename_1 and not 2).</p>
<p><a href=""https://i.stack.imgur.com/fwCWP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fwCWP.png"" alt=""ADFpipelinetwo2"" /></a></p>
<p>After executing the pipeline (which was failed), the input of the last stored procedure is:</p>
<pre><code>{
    &quot;storedProcedureName&quot;: &quot;[dbo].[InsertDataJSON]&quot;,
    &quot;storedProcedureParameters&quot;: {
        &quot;JsonData&quot;: {
            &quot;value&quot;: &quot;FRRNSC84FIN1_A2276801_20200103-152534.json&quot;,
            &quot;type&quot;: &quot;String&quot;
        }
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/kIMHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kIMHA.png"" alt=""ADFpipelinetwo3"" /></a></p>
<p>And the output is:</p>
<pre><code>{
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (West Europe)&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;ExternalActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/0y5jf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0y5jf.png"" alt=""ADFpipelinetwo4"" /></a></p>
<p>For this pipeline, it's failed with following error message.</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2402&quot;,
    &quot;message&quot;: &quot;Execution fail against sql server. Sql error number: 13609. Error Message: JSON text is not properly formatted. Unexpected character 'F' is found at position 0.\r\nFRRNSC84FIN1_A2276801_20200103-152534.json&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Stored procedure1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/KAmtK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KAmtK.png"" alt=""ADFpipelinetwo4"" /></a></p>
<p>On my old pipeline where I have stored procedure outside of ForEach loop. The pipeline did not fail:</p>
<p><a href=""https://i.stack.imgur.com/IVANF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IVANF.png"" alt=""ADFpipeline1"" /></a></p>
<p>Here is the input of the last stored procedure activity:</p>
<pre><code>{
    &quot;storedProcedureName&quot;: &quot;[dbo].[InsertDataJSON]&quot;,
    &quot;storedProcedureParameters&quot;: {
        &quot;JsonData&quot;: {
            &quot;value&quot;: &quot;[{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2274001_20200103-143748_back_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2274001_20200103-143748_right_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2274801_20200103-144811_right_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2275201_20200103-145229_right_camera_0.jpeg\&quot;,\&quot;type\&quot;:\&quot;File\&quot;},{\&quot;name\&quot;:\&quot;FRRNSC84FIN1_A2276801_20200103-152534.json\&quot;,\&quot;type\&quot;:\&quot;File\&quot;}]&quot;
        }
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/mpqzK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mpqzK.png"" alt=""ADFpipieline2"" /></a></p>
<p>And here is the output:</p>
<pre><code>{
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (West Europe)&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;ExternalActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/Lk6Wn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lk6Wn.png"" alt=""ADFpipeline3"" /></a></p>
<p>This pipeline ran successfully however the result in the SQL is not what I really expected, also many NULL values before the first json string but it insert all itemName in the same location. This is because it's outside of ForEach loop I think. But why it insert so many NULL?</p>
<p><a href=""https://i.stack.imgur.com/8yzhz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8yzhz.png"" alt=""SQLresult"" /></a></p>
<p>Here is my stored procedure that I modified:</p>
<pre><code>/****** Object:  StoredProcedure [dbo].[InsertDataJSON]    Script Date: 15/10/2020 10:31:51 ******/
SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO
/*USE datafactorypoc1*/

ALTER PROCEDURE [dbo].[InsertDataJSON] (
    @JsonData NVARCHAR (MAX)
)
AS
BEGIN
    PRINT @JsonData

    /*INSERT INTO Logs values (DEFAULT, @JsonData)*/
    INSERT INTO extractFileNameTest1 values (@JsonData, DEFAULT)
    SELECT * FROM
        OPENJSON (@JsonData)
    WITH (
        ItemName VARCHAR(255) N'$.ItemName',
        ItemType VARCHAR(255) N'$.ItemType'
    )
END
</code></pre>
<p>I tried to delete all data in my test SQL table to see the output of the 2nd fail pipeline. I notice that it did parse correctly the json string filename inside my table and all of them (I only have 5 files in blob storage). But other data are NULL.</p>
<p><a href=""https://i.stack.imgur.com/Sd1R7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sd1R7.png"" alt=""ADFpipelinetwo6"" /></a></p>
<p>To conclude, the result in SQL table of the new pipeline is much better, each file name on the different cell of the table but I got pipeline run failed. Could you please help me check what I did wrong here? Is it the stored procedure or the expression in my last Stored procedure activity inside ForEach loop?</p>
<p>Thank you very much in advance.</p>
","<json><azure><stored-procedures><azure-sql-database><azure-data-factory>","2020-10-14 11:23:38","887","1","2","67671321","<p>I also faced same issue while loading output of <code>get_metadata</code> activity with help of <code>sp</code>.</p>
<p>The simple answer is while feeding output of <code>get_metadata</code> activity to <code>SP</code> parameter, you need to convert it to string. Like below:</p>
<p><code>@string(activity('get_metadata').output.childitems)</code></p>
"
"64351259","How can you validate a JSON source against a JSON schema?","<p>How can you validate a JSON source against a JSON schema?</p>
<p>The (newly) added <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-xml#xml-as-source"" rel=""nofollow noreferrer"">XML format</a> source has got an option for validation against XSD schema</p>
<p>But as far as I can see, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-json#json-as-source"" rel=""nofollow noreferrer"">JSON format</a> doesn't have an equivalent?</p>
<p>So how can you validate any JSON ?</p>
<p><strong>EDIT:</strong> <em>it seems like it's not currently possible, so please vote for this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/41688562-add-a-validation-option-to-the-json-format"" rel=""nofollow noreferrer"">feedback</a></em></p>
","<azure><azure-data-factory>","2020-10-14 10:20:56","300","1","1","64436781","<p>like you said, we can't validate the json format in data factory.</p>
<p>XML format provides the validationMode but JSON format doesn't.
<a href=""https://i.stack.imgur.com/JwnuC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JwnuC.png"" alt=""enter image description here"" /></a></p>
<p>According my experience, many customers get errors that caused by the invalid JSON format. I usually suggest them using third-part tool to valid its format before copying data.</p>
<p>It's a very good suggestion that I think you can post a feedback. We can help you vote it up that Data Factory product team can see it.</p>
<p><strong>Update:</strong>
It seems like it's not currently possible, so please vote for this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/41688562-add-a-validation-option-to-the-json-format"" rel=""nofollow noreferrer"">feedback</a>
<a href=""https://i.stack.imgur.com/TF5aL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TF5aL.png"" alt=""enter image description here"" /></a>
<strong>Update 2:</strong>
Weird, there are now 6 votes, so is it now available to vote for ?</p>
"
"64350017","Tumbling Window trigger in Azure Data Factory - Self Run","<p>I have a got an ADF V2 Pipeline that runs hourly between 7am and 5pm only. So far I have been using an &quot;Event&quot; trigger that runs hourly and it was fine.
But somehow the load started to run for more than one hour.
As a result, the next load would start while the previous one was still running.</p>
<p>I have been trying to use a &quot;Tumbling Window&quot; trigger to create a self dependency on this pipeline so that it waits for the previous one to be completed before running but could not make it work.</p>
<p>If someone has some experience about how to tackle this problem, any insight would be much appreciated.</p>
","<azure><triggers><azure-data-factory>","2020-10-14 09:06:59","921","1","1","64448991","<p>Max Concurrency property can be used in tumbling window triggers where if it is set to 1 it will not let the other instance of trigger to be initiated until the other one completes.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger#tumbling-window-trigger-type-properties"" rel=""nofollow noreferrer"">Document</a> can be referred for the details</p>
<p><a href=""https://i.stack.imgur.com/Y0txi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y0txi.png"" alt=""enter image description here"" /></a></p>
"
"64349705","Is it possible to dynamically pass the value here --> activity('AcitivityName').output.rowsCopied instead of hardcoding the 'Activityname' in ADF","<p>I am trying to retrieve the logs of the copy activity. The rowsread and rowscopied, challenge here is I have multiple copy activities and not just one. The goal is to store the logs in SQL server table. Below is the query I am using,</p>
<pre><code>****SELECT '@{pipeline().DataFactory}' as DataFactory_Name,
'@{pipeline().Pipeline}' as Pipeline Name,
'@{activity('ActivityName').output.rowsCopied}' as rows Copied,
'@{activity('ActivityName').output.rowsRead}' as Rows Read****
</code></pre>
<p>I need help in figuring out a way to pass Multiple 'AcitivtyName' in a single command eg : '@{activity('ActivityName').output.rowsCopied}' as rows Copied using foreach.</p>
","<sql-server-2012><azure-data-factory>","2020-10-14 08:46:56","566","1","1","64386045","<p>I've tried a lot tests. We can pass the activity name to the command dynamically. but due to the limitation of the ADF the command cannot be executed.</p>
<ol>
<li>I manually create a string array to store the ActivityName</li>
</ol>
<p><a href=""https://i.stack.imgur.com/QezuB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QezuB.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>In the ForEach Activity, I use <code>@concat('select ''@{activity(''',item(),''').output.dataRead}''')</code> to dynamically pass the value to the SQL query.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/V0GoF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V0GoF.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>When I run debug, the parameters were parsed but didn't execute the SQL query.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/n2SRy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n2SRy.png"" alt=""enter image description here"" /></a></p>
<p>Hope my answer is helpful to you, thanks.</p>
"
"64340002","Transferring data from Azure Blob to Salesforce Marketing Cloud","<p>I want to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">Azure Data Factory</a> for this task but not able to find sink for &quot;Salesforce Marketing Cloud&quot;. Although there is sink for &quot;Salesforce&quot; and &quot;Salesforce Service Cloud&quot; . What other option are there to achieve this task apart/within Azure Data Factory  ?</p>
","<cloud><azure-data-factory><salesforce-marketing-cloud>","2020-10-13 17:14:08","199","0","1","64345296","<p>Copy active doesn't support Salesforce Marketing Cloud as sink for now:</p>
<p><a href=""https://i.stack.imgur.com/EbOTj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EbOTj.png"" alt=""enter image description here"" /></a></p>
<p>You may achieve that in code level, such as Azure Function and Databricks. Then run the scripts in Data Factory.</p>
"
"64337817","Stored Procedure to take json string as an input then insert these string into SQL table as output in Azure Data Factory","<p>Continuing from my previous post:
<a href=""https://stackoverflow.com/questions/64227251/azure-data-factory-get-metadata-to-get-blob-filenames-and-transfer-them-to-azure/64241502?noredirect=1#comment113629157_64241502"">Azure Data Factory Get Metadata to get blob filenames and transfer them to Azure SQL database table</a></p>
<p>I am trying to write a stored procedure to take json string as input and insert this string into SQL table as an output in Azure Data Factory. This is the Stored Procedure activity in my Azure Data Factory. I already wrote and save the begging of my stored procedure which is in Azure SQL server.</p>
<p><a href=""https://i.stack.imgur.com/bcwOK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bcwOK.png"" alt=""StoredProcedure1"" /></a></p>
<p>Here is the beginning of my Stored procedure:
I got some ideas from this post: <a href=""https://stackoverflow.com/questions/62326689/creating-a-stored-procedure-which-ingests-data-after-reading-a-json-string"">Creating a stored procedure which ingests data after reading a JSON string</a></p>
<p><a href=""https://i.stack.imgur.com/g7qDb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7qDb.png"" alt=""StoredProcedure2"" /></a></p>
<p>Here is my sample output.json file that I got it from the previous activity, Get Metadata of my pipeline. I basically need to get the value of itemName in this json file as an input of the stored procedure.</p>
<p><a href=""https://i.stack.imgur.com/n4UdM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n4UdM.png"" alt=""StoredProcedure3"" /></a></p>
<p>Could you please help me how to continue this stored procedure? Thank you very much in advance.</p>
","<stored-procedures><azure-data-factory>","2020-10-13 15:02:50","1389","0","1","64436958","<p>If I were you I could have read the output of the metadata as below and I passed those to the proc parameter .</p>
<pre><code>@activity('Get Metadata1').output.durationInQueue.integrationRuntimeQueue
@activity('Get Metadata1').itemName
@activity('Get Metadata1').itemType
@activity('Get Metadata1').effectiveIntegrationRuntime
</code></pre>
<p>The proc should something like</p>
<pre><code>CREATE PROC XXXXX 
@effectiveIntegrationRuntime int 
,@itemName varchar(100)
,@itemType varchar(100)
,Metadata1').effectiveIntegrationRuntime int 

AS 
INSERT INTO SOMETABLEXXX  VALUES (@effectiveIntegrationRuntime .......)
</code></pre>
"
"64333575","How to Override parameters for CI/CD pipeline for azure daafactory For Databricks (Workspace Url and Cluster ID)","<p>I am doing CI/CD integration for one data factory to another I am Successfully able to Create the release and abe to copy from my Dev to UAT environment I am able to copy my pipelines Triggers and the Link Service
The problem I am facing In just copying Databricks Links Service</p>
<p>As we know we have to override the parameters of our environment, In Databricks Option comes only to override for an Access token.
And Databricks require three parameters workspace URL and ClusterID, As there is no option to override these two. My workspace URL and Cluster ID is in the production environment is copied of MY Dev environment. Although the Token is copied Successfully which results in the Unsuccessful Connection due to workspace URL NA clusterId.</p>
<p><a href=""https://i.stack.imgur.com/TM8Lk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TM8Lk.png"" alt=""enter image description here"" /></a></p>
<p>As you see inside of pic for data bricks three params are required
and Override Template parameter is giving for an only access token</p>
","<azure><azure-data-factory><azure-databricks>","2020-10-13 10:37:00","736","0","1","64353374","<p>Those properties can be parameterized from ARM Template by defining parameters for &quot;Domain&quot; and &quot;existingclusterID&quot; properties as below.</p>
<p><a href=""https://i.stack.imgur.com/rDGE5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rDGE5.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/6qmqJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6qmqJ.png"" alt=""enter image description here"" /></a></p>
"
"64333301","Trigger at First Business Day of Month","<p>I need to run a pipeline that starts at the first business day of month. It can be set in a trigger or in an if condition to run all the activities at this day.</p>
<p>Thanks in advance!</p>
","<azure-data-factory>","2020-10-13 10:19:24","505","2","1","64339780","<p>You could use an if condition activity with the following code:</p>
<pre><code>@or(and(equals(dayOfMonth(utcnow()),1), and(greater(dayOfWeek(utcnow()),0),less(dayOfWeek(utcnow()),6))),or(and(equals(dayOfMonth(utcnow()),2), equals(dayOfWeek(utcnow()),1)),and(equals(dayOfMonth(utcnow()),3), equals(dayOfWeek(utcnow()),1))))
</code></pre>
"
"64330999","Run a Pipeline when another Pipeline completes on another Data factory","<p>I have two separate Data Factories on my Azure Subscription, lets call them <strong>DF-A</strong> and the other <strong>DF-B</strong>
In Data Factory <strong>DF-A</strong> I have a pipeline and when this has completed, I would like the Pipeline on <strong>DF-B</strong> to run; how would I achieve this?</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2020-10-13 07:57:42","2198","0","2","64337379","<p>While it's possible, it's much more complicated than one pipeline executing another from within the same Azure Data Factory.</p>
<p>In DF-A create a pipeline called ExecuteExternalPipeline copying the following JSON into the Code tab:</p>
<pre><code>{
    &quot;name&quot;: &quot;ExecuteExternalPipeline&quot;,
    &quot;properties&quot;: {
        &quot;description&quot;: &quot;Executes an ADF pipeline in a different ADF&quot;,
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;StartPipelineThenWait&quot;,
                &quot;description&quot;: &quot;Calls the ADF REST API to start a pipeline in another ADF running using the MSI of this current ADF. Then it waits on a webhook callback&quot;,
                &quot;type&quot;: &quot;WebHook&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: {
                        &quot;value&quot;: &quot;@concat(\n 'https://management.azure.com/subscriptions/',\n pipeline().parameters.SubscriptionID,\n '/resourceGroups/',pipeline().parameters.ResourceGroup,\n '/providers/Microsoft.DataFactory/factories/',\n pipeline().parameters.DataFactory,\n '/pipelines/',\n pipeline().parameters.Pipeline,\n '/createRun?api-version=2018-06-01'\n)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;method&quot;: &quot;POST&quot;,
                    &quot;body&quot;: {
                        &quot;value&quot;: &quot;@json(\n concat(\n  '{\n   \&quot;InputFileName\&quot;: \&quot;', pipeline().parameters.InputFileName, '\&quot;\n  }'\n )\n)\n&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;timeout&quot;: &quot;20:00:00&quot;,
                    &quot;authentication&quot;: {
                        &quot;type&quot;: &quot;MSI&quot;,
                        &quot;resource&quot;: &quot;https://management.azure.com&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ThrowErrorIfFailure&quot;,
                &quot;type&quot;: &quot;IfCondition&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;StartPipelineThenWait&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;expression&quot;: {
                        &quot;value&quot;: &quot;@if(equals(activity('StartPipelineThenWait').status,'success'),true,json('throw an error!'))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;SubscriptionID&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;12345abcd-468e-472a-9761-9da416b14c0d&quot;
            },
            &quot;ResourceGroup&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;DF-B-RG&quot;
            },
            &quot;DataFactory&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;DF-B&quot;
            },
            &quot;Pipeline&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;ChildPipeline&quot;
            },
            &quot;InputFileName&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;File1.txt&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>Then create ChildPipeline in DF-B with the following code:</p>
<pre><code>{
    &quot;name&quot;: &quot;ChildPipeline&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;DoYourLogicHere&quot;,
                &quot;description&quot;: &quot;&quot;,
                &quot;type&quot;: &quot;WebActivity&quot;,
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: {
                        &quot;value&quot;: &quot;https://google.com&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;method&quot;: &quot;GET&quot;
                }
            },
            {
                &quot;name&quot;: &quot;CallbackSuccess&quot;,
                &quot;description&quot;: &quot;Do not remove this activity. It notifies the process which executed this pipeline that the pipeline is complete.&quot;,
                &quot;type&quot;: &quot;WebActivity&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;DoYourLogicHere&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.callBackUri&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;method&quot;: &quot;POST&quot;,
                    &quot;body&quot;: {
                        &quot;value&quot;: &quot;@json(concat('{\&quot;status\&quot;: \&quot;success\&quot;, \&quot;pipelineRunId\&quot;: \&quot;',pipeline().RunId,'\&quot;}'))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;CallbackFail&quot;,
                &quot;description&quot;: &quot;Do not remove this activity. It notifies the process which executed this pipeline that the pipeline is complete.&quot;,
                &quot;type&quot;: &quot;WebActivity&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;DoYourLogicHere&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Failed&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.callBackUri&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;method&quot;: &quot;POST&quot;,
                    &quot;body&quot;: {
                        &quot;value&quot;: &quot;@json(concat('{\&quot;status\&quot;: \&quot;failure\&quot;, \&quot;pipelineRunId\&quot;: \&quot;',pipeline().RunId,'\&quot;}'))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;callBackUri&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;https://google.com&quot;
            },
            &quot;InputFileName&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;File1.txt&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>Replace the DoYourLogicHere activity with your own activities but leave the two callback activities.
<a href=""https://i.stack.imgur.com/brMPE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/brMPE.png"" alt=""ChildPipeline screenshot"" /></a></p>
<p>Then you need to find the MSI (see the Properties tab of your DF-A in the Azure Portal) for DF-A and make it a Data Factory Contributor on DF-B so that it can execute the pipeline in the other ADF.</p>
"
"64330999","Run a Pipeline when another Pipeline completes on another Data factory","<p>I have two separate Data Factories on my Azure Subscription, lets call them <strong>DF-A</strong> and the other <strong>DF-B</strong>
In Data Factory <strong>DF-A</strong> I have a pipeline and when this has completed, I would like the Pipeline on <strong>DF-B</strong> to run; how would I achieve this?</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2020-10-13 07:57:42","2198","0","2","64345756","<p>In Logic app designer, you can create two pipeline run steps to trigger the two pipelines in different Data Factory running.<br />
It is more easier by using logic apps to achieve this.</p>
<ol>
<li>create a <strong>Recurrence</strong> trigger to schedule the executions and two <strong>Azure Data Factory</strong> operations to trigger the pipeline running.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/QrVC5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QrVC5.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>In the <strong>Azure Data Factory</strong> operations, select <strong>Create a pipeline run</strong> Action.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/CnZh5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CnZh5.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>The summary is here:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/JbUmk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JbUmk.png"" alt=""enter image description here"" /></a></p>
"
"64322187","Azure Data Factory Event Based Trigger not working as Expected","<p>Storage Explorer Version: 1.14.2
Build Number: 20200715.2
Platform/OS: Windows 10
Architecture: ia32
Regression From: Not sure</p>
<p>Hello All,</p>
<p>I have created a event based trigger for triggering some pipelines. So the issue is when i try to add csv files manually through storage explorer to designated blob location my trigger is working perfectly, but when an external source like I have a backend python code which is pushing the files into the blob location, when this is happening the event based trigger is not triggering. I just checked the content type for manual upload the content type is vnd.ms-excel and for python code based upload is octet-stream. Is the issue something related to this or any other. My Storage explorer version is 1.14.2.</p>
","<azure><azure-data-factory><azure-triggers>","2020-10-12 17:02:26","634","0","1","64330753","<p>Please check the version of the Python SDK.<br />
I'm using <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python"" rel=""nofollow noreferrer"">Python v12 SDK</a> to upload blob to Azure blob storge and it works well.</p>
<p>Here is my python code:</p>
<pre class=""lang-py prettyprint-override""><code>import os, uuid
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__

try:
    print(&quot;Azure Blob storage v&quot; + __version__ + &quot; - Python quickstart sample&quot;)
    # Quick start code goes here
    # Create the BlobServiceClient object which will be used to create a container client
    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
    
    blob_service_client = BlobServiceClient.from_connection_string(connect_str)
    

    # Get container
    container_name = &quot;test&quot;     

    print(container_name)
    # Create the container
    container_client = blob_service_client.get_container_client(container_name)
    
    
    # Create a file in local data directory to upload and download
    local_path = &quot;./data&quot;
    local_file_name = &quot;Test.csv&quot;
    upload_file_path = os.path.join(local_path, local_file_name)
    
    print(upload_file_path)
    
    # Create a blob client using the local file name as the name for the blob
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=local_file_name)
    
    print(&quot;\nUploading to Azure Storage as blob:\n\t&quot; + local_file_name)
    
    # Upload the  file
    with open(upload_file_path, &quot;rb&quot;) as data:
        blob_client.upload_blob(data)
except Exception as ex:
    print('Exception:')
    print(ex)

</code></pre>
<p>When I use python to upload the csv file to the Azure blob storage, the event trigger triggered the pipeline running and it  works well:</p>
<p><a href=""https://i.stack.imgur.com/uDk5T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uDk5T.png"" alt=""enter image description here"" /></a></p>
"
"64321033","Azure data factory dataflow SELECT DISTINCT","<p>I have a dataflow with a few joins and when making the join #5, the number of row goes from 10,000 to 320,000 (to make an example of how the quantity is increased), but after that i have more joins to make so the dataflow is taking longer to complete.</p>
<p>What I do is to add an Aggregate transformation after the joins, to groupby the field that I will use later, using that in a way that I use a SELECT DISTINCT in a query on the database, but still taking soooo long to finish.</p>
<p>How can make this dataflow run faster?</p>
<p>Should I use an Aggregate (and groupby the fields) between every join, to avoid the duplicates or just add the Aggregate (and groupby the fields...) after the join where the  rows starts to increase?</p>
<p>Thanks.</p>
","<aggregate-functions><azure-data-factory>","2020-10-12 15:45:04","1432","2","1","64326291","<p>Can you switch to Lookups instead of Join and then choose &quot;run single row&quot;. That provides the SELECT DISTINCT capability in a single step.</p>
<p>Also, to speed up the processing end-to-end, try bumping up to memory optimized and raise the core count.</p>
"
"64319272","Transform selected excel columns to json string in data factory v2","<p>I am having the excel sheet with multiple columns. I need to transform selected column of excel sheet to json string and store in seperate column in excel using Azure Data Factory V2.</p>
<p>In data factory v2 using data flow we can create and update the existing columns using Derived Column Transformation.</p>
<p>I am having below excel file:</p>
<p><a href=""https://i.stack.imgur.com/kfg8S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kfg8S.png"" alt=""enter image description here"" /></a></p>
<p>With Azure Data Factory data flow, I need to transform the file to below:</p>
<p><a href=""https://i.stack.imgur.com/OL3ny.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OL3ny.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know if this is possible to achieve in Derived Columns data flow transformation or I have to use custom activity to achieve the format.</p>
","<json><azure-data-factory>","2020-10-12 13:54:19","1264","0","1","64327185","<p>Yes, Data Flow could make it as a JSON string. But it's not a real JSON object.</p>
<p>You could try bellow expressions:</p>
<pre><code>concat('[{&quot;',Column2,'&quot;:',Column3,'}]' )
</code></pre>
<p><a href=""https://i.stack.imgur.com/AdraB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AdraB.png"" alt=""enter image description here"" /></a></p>
"
"64316735","Azure Data Factory unable to output JSON data","<p>Hi I am working in azure data factory. I am trying to construct dynamic json object as below.</p>
<p>json('{&quot;fullName&quot;: &quot;Sophia Owen&quot;}') This results in</p>
<pre><code>{
  &quot;fullName&quot;: &quot;Sophia Owen&quot;
}
</code></pre>
<p>this works but when I do</p>
<p><code>@json('{&quot;fullName&quot;:@{pipeline().parameters.UserId}}')</code> this results in</p>
<pre><code>&quot;json('{\&quot;fullName\&quot;:myname@mydomain.com}')&quot;
</code></pre>
<p>Any idea how can I get</p>
<pre><code>{
  &quot;fullName&quot;: &quot;myname@mydomain.com&quot;
}
</code></pre>
<p>any help would be appreciated. Thank you</p>
","<json><azure-data-factory>","2020-10-12 11:12:50","54","0","1","64317563","<p>I used below expression and it worked fine.</p>
<pre><code>@json(concat('{&quot;key&quot;: &quot;', pipeline().parameters.UserId, '&quot;}'))
</code></pre>
"
"64316082","Access issue in Azure data lake","<p>We are moving files from SFTP to Azure data lake using Azure data factory (V2). We have already created a static folder structure inside the lake and the current user has all the permission inside the folder. But when we move the files from the Source (SFTP) to the folder current user don't have any access for file but the folder permission are look like same as before the transformation</p>
<p><a href=""https://i.stack.imgur.com/xdMTI.png"" rel=""nofollow noreferrer"">File Permission</a></p>
<p><a href=""https://i.stack.imgur.com/ox7Xm.png"" rel=""nofollow noreferrer"">Folder Permission</a></p>
<p><a href=""https://i.stack.imgur.com/bYQoF.png"" rel=""nofollow noreferrer"">Access Control exception</a></p>
","<azure><azure-storage><azure-data-factory><azure-data-lake>","2020-10-12 10:31:04","71","0","1","64329517","<p>This is caused by insufficient permissions, you can give it read, write, and execute permissions.</p>
<p><a href=""https://i.stack.imgur.com/z5qPG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z5qPG.png"" alt=""enter image description here"" /></a></p>
"
"64314246","How to output JSON data in Azure Data Facory","<p>Hi I am working in azure data factory and azure c# function. I am trying to send some data as json from azure data factory to c# azure function. In add dynamic content I want to send some of the parameters as JSON. For example, we have parameters UserID and ID. Below is my sample model of c# azure function</p>
<pre><code>public class MyModel
{
public string UserID {get;set;}
public string ID {get;set;}
}
</code></pre>
<p>My ADF dynamic content is</p>
<pre><code>@pipeline().parameters.UserId
</code></pre>
<p>But in the above dynamic content I would like to send two parameters  @pipeline().parameters.UserId and  @pipeline().parameters.ID as JSON to my azure function. Can someone help me to fix this. Any help would be appreciated. Thank you</p>
","<json><azure-functions><azure-data-factory>","2020-10-12 08:29:53","488","0","1","64314959","<p>In the <a href=""https://learn.microsoft.com/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> in your pipeline, select 'POST' as method. Then in 'Body', use dynamic content as</p>
<pre><code>{
  &quot;UserID&quot;: &quot;@{pipeline().parameters.UserId}&quot;,
  &quot;ID&quot;: &quot;@{pipeline().parameters.ID}&quot;
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/bH1X8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bH1X8.png"" alt=""enter image description here"" /></a></p>
"
"64312586","How to update Machine Learning model deployed using Azure Function","<p>I have a machine model that I deployed using docker on Azure Function(HTTP Trigger). Routinely, the directory of the machine learning model is expected to change and updated once a new file is uploaded to an Azure DataLake.</p>
<p>Please, How do I automate this process of updating the model on Azure Functions?
Thanks.</p>
","<azure-functions><azure-data-factory><azure-function-app><azure-functions-core-tools>","2020-10-12 06:17:05","118","0","1","64595646","<p>Since its a docker container, you would just want to build a new container with the new model and update the function app with the new image tag.</p>
<p>Azure Data Lake Storage supports Events that you can listen to, triggering the pipeline for the machine learning model, and then follow up with a CI/CD pipeline for building the docker image.</p>
"
"64308209","How to save a stored procedure into Azure SQL Server","<p>I am writing a test stored procedure on visual studio code (I cannot install SQL Server Management Studio or Visual Studio as I don't think its available on Linux Ubuntu).</p>
<p><a href=""https://i.stack.imgur.com/cUsG4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cUsG4.png"" alt=""storedProcedure1"" /></a></p>
<p>I need to save this stored procedure on my Azure SQL Server because I am adding stored procedure activity to my pipeline in Azure Data Factory, and I have to select the store procedure while connecting to the Azure SQL Server.</p>
<p><a href=""https://i.stack.imgur.com/p8frX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p8frX.png"" alt=""storedProc2"" /></a></p>
<p>Here is my Azure SQL Server where I can find the system stored procedure but I cannot find the place where I can save my stored procedure or queries to the SQL Server.</p>
<p><a href=""https://i.stack.imgur.com/i71rW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i71rW.png"" alt=""storeproc3"" /></a></p>
","<stored-procedures><azure-data-factory><azure-sql-server>","2020-10-11 19:32:46","198","0","1","64308460","<p>I just found out a solution. I need to be able to execute successfully the stored procedure first on my vscode. Then; after I see the stored procedure that I wrote appeared on Azure SQL server.</p>
"
"64295269","Azure Data Factory Web Activity save output","<p>Is there a way to save the output of an Azure Data Factory Web Activity into a dataset?</p>
<p>Here is my current use case:</p>
<ol>
<li>I have to dynamically build a JSON post request</li>
<li>The API I'm trying to reach requires a SSL certificate so I have to use the Web Activity <strong>Authentication</strong> Client Certificate option.</li>
<li>The API also requires basic authentication so I input the Content -Type and authorization guid in the header section of the Web Activity.</li>
<li>Once I get the JSON response from my post request I need to save the response into a blob storage some where.</li>
</ol>
<p>I tried using the Copy Data Set HTTPS or Rest API as a data set source but both only allow one type of authentication certificate or Basic authentication.</p>
<p>Is there a way I can configure the Rest API or HTTPS data set source handle both types of authentications (SSL and Basic Authorization) or capture all the Web Activity output into a blob storage?</p>
<p>Thank you all for your help! I'm desperate at the moment lol..</p>
<p>Here is what my Web Activity looks like (sorry I had hide part of the credentials for security purposes:
<a href=""https://i.stack.imgur.com/umFuY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/umFuY.jpg"" alt=""Web Activity"" /></a></p>
","<azure><azure-web-app-service><azure-blob-storage><azure-data-factory>","2020-10-10 15:45:09","1675","0","1","64313518","<p>Please use Http dataset in your Coput Activity.</p>
<p>When we create the linked service of the Http dataset, select <strong>client certificate</strong> option and embedded data, then we need to upload the SSL certificate.</p>
<p><a href=""https://i.stack.imgur.com/UGdEt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UGdEt.png"" alt=""enter image description here"" /></a></p>
<p>The offical document is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http#linked-service-properties"" rel=""nofollow noreferrer"">here</a>.</p>
"
"64293127","Azure Data Factory - iterating multiple data set option","<p>I request your support for the following clarification,
I have a scenario where to read data from multiple HubSpot portals (for example Chile, Peru, Colombia, etc.,) through Azure Data Factory.
It’s a simple copy operation from HubSpot to Azure SQL Server, but I stuck up with iterating single copy activity for above the mentioned multiple region portals.
I have created a separate dataset for each region but not sure how to iterate that with Copy Activity.</p>
","<azure><azure-data-factory>","2020-10-10 11:53:29","77","0","2","64308845","<p>Data Factory supports a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach</a> operation. You can wrap Data Factory sub-processes (ex. <em>Copy Activity</em>) within the loop.</p>
"
"64293127","Azure Data Factory - iterating multiple data set option","<p>I request your support for the following clarification,
I have a scenario where to read data from multiple HubSpot portals (for example Chile, Peru, Colombia, etc.,) through Azure Data Factory.
It’s a simple copy operation from HubSpot to Azure SQL Server, but I stuck up with iterating single copy activity for above the mentioned multiple region portals.
I have created a separate dataset for each region but not sure how to iterate that with Copy Activity.</p>
","<azure><azure-data-factory>","2020-10-10 11:53:29","77","0","2","64384713","<p>It seems you want to make your source dynamical in copy activity. As far as I know, this is not supported now.</p>
"
"64291133","How to pass the output from a Web Activity into a Copy activity for storage","<p>I am new to Azure Data Factory and wonder if you can please help me in acheiving with the following scenario:</p>
<ol>
<li><p>I want to get data from a REST Endpoint using API. The REST Endpoint is stored in a SQL Database table and therefore I fetch the URL using a Lookup activity</p>
</li>
<li><p>Further on, I am storing the URL value in a variable using a &quot;Set Variables&quot; activity</p>
</li>
<li><p>Post that I am fetching the data from the Endpoint using REST API in a Web activity.</p>
</li>
<li><p>Now , I want to store the output data from the Web activity into a Blob storage. For this, i am using Copy activity , but I am not able to get this working at all. Meaning , I am unable to collect the output from the Web Activity into my Copy activity.</p>
</li>
</ol>
<p>In case any of you have come across this situation then it will be of very good help for me indeed.</p>
<p><a href=""https://i.stack.imgur.com/iQPei.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iQPei.png"" alt=""Failing in the Copy activity"" /></a></p>
","<azure><azure-data-factory>","2020-10-10 07:39:24","680","0","1","64501345","<p>Seems you are not using correct output from web activity in copy activity. To check what is the output from web activity follow this GIF and pass those values appropriately to copy activity</p>
<p><a href=""https://i.stack.imgur.com/fDzuO.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fDzuO.gif"" alt=""enter image description here"" /></a></p>
"
"64287958","Single Instance Azure Synapse with Multiple Environments","<p>We are currently deploying several test vnets (each one representing an environment) into a single subscription for testing and to restrict access we have decided to have three separate Azure Data Factories (we assume as these charge at runtime, the costs should be relatively the same), for each ADF we have a Data Lake where the data is stored (again we assume as this is charged per GB, whether we have 1 or 3 the cost is relatively the same), but what we can't quite work out is can we have 1 instance of Azure synapse in a separate Vnet querying against the 3 vnets (environments)?</p>
<p>So what I'm trying to ascertain is can Azure Synapse work against 3 data lakes or data factories, or do I need one instance of Synapse per data factory?</p>
<p><em>Assumptions:
Azure Data Lake is charged per GB, whether we have 1 or 3 the cost is relatively the same?
As ADF only charges at runtime, the costs should be relatively the same?</em>
Are these assumptions correct?</p>
<p>Hope this makes sense and thanks for your help.. New to data so forgive any schoolboy errors :)</p>
","<azure><azure-data-factory><azure-data-lake-gen2><azure-synapse>","2020-10-09 22:13:24","809","0","1","64295990","<p>A SQL Pool is charged by the hour when on and also for storage. So if you want to create one per environment then look at programmatically pausing and resuming to save cost.</p>
<p>If it’s a Synapse Analytics (formerly Azure SQL DW) (connection string that ends in .database.windows.net) then the following <a href=""https://github.com/furmangg/automating-azure-sql-dw/blob/master/README.md#pauseazuresqldw"" rel=""nofollow noreferrer"">sample is an ADF pipeline to pause the DW</a>.</p>
<p>If it’s a Synapse Analytics Workspace (connection ending in sql.azuresynapse.net) then I don’t have a sample handy.</p>
<p>Either can query one or more data lakes.</p>
"
"64285700","Azure Data Factory - Inner join a table with a SQL query inside stored procedure activity","<p>Situation:</p>
<p>I have two databases DB1 and DB2. DB1 has 9 tables (out of these 9, any of the tables could get updated with only column to track that change i.e TIME_STAMP column).</p>
<p>Now I am trying to bring only the Updated/New records from these 9 tables from DB1 to DB2 (each table in DB1 map to one table in DB2) using Azure Data Factory and then run my final query on the tables in DB2. I have only read access on DB1.</p>
<p>Logic:</p>
<ol>
<li>Find the set of the Ids using UNION which got updated/inserted in these 9 tables</li>
<li>Inner join the result of the UNION query with each of the 9 tables individually</li>
</ol>
<p>Union Query Example:</p>
<pre><code>SELECT DISTINCT idcolumn 
FROM table1
WHERE TIME_STAMP BETWEEN '2020-11-06T13:14:13.807Z' AND  '2020-11-07T13:14:13.807Z' 

UNION 

SELECT DISTINCT idcolumn 
FROM table2
WHERE TIME_STAMP BETWEEN '2020-11-06T13:14:13.807Z' AND  '2020-11-07T13:14:13.807Z'

UNION 

SELECT DISTINCT idcolumn 
FROM table3
WHERE TIME_STAMP BETWEEN '2020-11-06T13:14:13.807Z' AND  '2020-11-07T13:14:13.807Z'

UNION 
    
SELECT DISTINCT idcolumn 
FROM table4
WHERE TIME_STAMP BETWEEN '2020-11-06T13:14:13.807Z' AND  '2020-11-07T13:14:13.807Z' 
</code></pre>
<p>And similarly for 5 other tables.</p>
<p>Problem:</p>
<p>Is there a way in ADF with which I can find the union query result only once and then join the resultant data with all the 9 tables in one go rather than running the union query 9 times with each table in 9 different copy activity?</p>
","<azure-data-factory><azure-sql>","2020-10-09 18:46:50","2149","-1","1","64474024","<p>You can achieve that with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data Flow</a>. For example,</p>
<ol>
<li>Create a Source 1: run the query to get source dataset.</li>
<li>Create Source 2.</li>
<li>Add a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join"" rel=""nofollow noreferrer"">Join active</a> to join with Source 1 and Source 2.</li>
</ol>
<p>Here's the data flow overview:
<a href=""https://i.stack.imgur.com/RBqdT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RBqdT.png"" alt=""enter image description here"" /></a></p>
<p>Just with Copy active in ADF pipeline, it's impossible. We can not join the A Copy active  source to B copy active's source.</p>
"
"64280080","Web Activity endless running In Azure Data Factory","<p>Currently it seems that web activity is broken.</p>
<p>When using simple pipeline</p>
<pre><code> {
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Webactivity&quot;,
                &quot;type&quot;: &quot;WebActivity&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: &quot;https://www.microsoft.com/&quot;,
                    &quot;connectVia&quot;: {
                        &quot;referenceName&quot;: &quot;AutoResolveIntegrationRuntime&quot;,
                        &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
                    },
                    &quot;method&quot;: &quot;GET&quot;,
                    &quot;body&quot;: &quot;&quot;
                }
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>When debugging it never finishes. There is &quot;in progress&quot; for several minutes.
I tried Web hook and it works.</p>
<p>Is there something else I could try?</p>
","<azure-data-factory>","2020-10-09 12:28:28","229","0","2","64281105","<p>A quick note on the &quot;never finishes&quot; issue: one of my pet peeves with Data Factory is that the default timeout for all activities is 7 DAYS. While I've had a few activities that needed to run for 7 hours, a WEEK is a ridiculous default timeout value. One of the first things I do in any production scenario is address the timeout values of all the activities.</p>
<p>As to the Web activity question: I set up a quick example in my test bed and it returned just fine:</p>
<p><a href=""https://i.stack.imgur.com/7Ih8F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Ih8F.png"" alt=""enter image description here"" /></a></p>
<p>Looking at the generated code, the only real difference I see is the absence of the &quot;connectVia&quot; property that was in your example:</p>
<p><a href=""https://i.stack.imgur.com/EO6Ix.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EO6Ix.png"" alt=""enter image description here"" /></a></p>
"
"64280080","Web Activity endless running In Azure Data Factory","<p>Currently it seems that web activity is broken.</p>
<p>When using simple pipeline</p>
<pre><code> {
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Webactivity&quot;,
                &quot;type&quot;: &quot;WebActivity&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: &quot;https://www.microsoft.com/&quot;,
                    &quot;connectVia&quot;: {
                        &quot;referenceName&quot;: &quot;AutoResolveIntegrationRuntime&quot;,
                        &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
                    },
                    &quot;method&quot;: &quot;GET&quot;,
                    &quot;body&quot;: &quot;&quot;
                }
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>When debugging it never finishes. There is &quot;in progress&quot; for several minutes.
I tried Web hook and it works.</p>
<p>Is there something else I could try?</p>
","<azure-data-factory>","2020-10-09 12:28:28","229","0","2","64281992","<p>Ok I've found it.</p>
<p>The default AutoResolveIntegrationRuntime only had managed private network which I couldn't change. So I created a new Integration Runtime with public network setting.
This is a litte bit strange as I started today with a brand new Azure Data Factory.</p>
<p><a href=""https://i.stack.imgur.com/SXuEq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SXuEq.png"" alt=""enter image description here"" /></a></p>
<p><strong>I wonder why I cannot change the default Integration Runtime to disable virtual network:</strong></p>
<p><a href=""https://i.stack.imgur.com/oqwGG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqwGG.png"" alt=""enter image description here"" /></a></p>
"
"64278357","Is there any way to connect to Yellowbrick database in Azure Data Factory","<p>I wanted to connect to Yellowbrick(YB) database (Postgresql) as a source in the Azure Data Factory. The YB instance is on the cloud. And I had set the encryption to 'No Encryption'. In the server name, if I pass the domain name, then the error I get is &quot;Socket closed.&quot; If I pass IP address, the error I get is &quot;Verify hostname and port number.&quot;</p>
","<postgresql><azure-data-factory><yellowbrick>","2020-10-09 10:37:23","232","0","2","64313885","<p>Good day sir!</p>
<p>To tell you the truth that I never met your problem, but I google 'Verify hostname and port number', and got some message that I thought may do some help.</p>
<p>Here is the link:<a href=""https://knowledgebase.progress.com/articles/Article/9334"" rel=""nofollow noreferrer"">https://knowledgebase.progress.com/articles/Article/9334</a></p>
<p>Wish you to solve your problem soon.</p>
"
"64278357","Is there any way to connect to Yellowbrick database in Azure Data Factory","<p>I wanted to connect to Yellowbrick(YB) database (Postgresql) as a source in the Azure Data Factory. The YB instance is on the cloud. And I had set the encryption to 'No Encryption'. In the server name, if I pass the domain name, then the error I get is &quot;Socket closed.&quot; If I pass IP address, the error I get is &quot;Verify hostname and port number.&quot;</p>
","<postgresql><azure-data-factory><yellowbrick>","2020-10-09 10:37:23","232","0","2","64533428","<p>Okay. So my Yellowbrick database is an on-prem database. So, all I did is created a Self-Hosted Integration-Runtime for my database location and then used it in the data factory and it worked fine.</p>
"
"64270628","Is there a way to find the oldest file in a directory using Azure Data Lake?","<p>Is there a way to find the oldest file in a directory using Azure Data Lake?</p>
<p>I had assumed I could use the meta data activity to get all the file names and dates (which I can). I then thought I could use the forEach to set two variables in the pipeline (Name &amp; Date) with the values from the list if they were older than the current value of the variables. This does not work as all the files are processed in parallel. This really should not be this hard.</p>
","<azure-data-factory>","2020-10-08 21:08:06","187","0","1","64272629","<p>Yes, ForEach activity in Azure Data Factory works in parallel by default , but you change to work sequentially through checking Sequential option.</p>
<p><a href=""https://i.stack.imgur.com/hEarS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hEarS.png"" alt=""enter image description here"" /></a></p>
<p>More details, you can refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#type-properties"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"64265177","Conditional sink in Azure data factory","<p>I have a mapping data flow that is checking for invalid data, with conditional split, in my file. (example row in column one &lt; than in column two.)</p>
<p>I the case I don't have any invalid data I would like to copy the file in new folder.
if I have at least one error in my file I want to copy the file in an error folder.</p>
<p>The problem is that the validation is on the row level and I can't use a lookup.</p>
<p>Thank you for your help</p>
","<azure><azure-data-factory>","2020-10-08 14:53:06","545","0","2","64267733","<p>You could have two copy activities that would execute depending on the result of your dataflow activity, <a href=""https://i.stack.imgur.com/XYf5U.png"" rel=""nofollow noreferrer"">for example</a> and <a href=""https://www.mssqltips.com/sqlservertip/6128/build-azure-data-factory-pipeline-dependencies/"" rel=""nofollow noreferrer"">more info here</a></p>
"
"64265177","Conditional sink in Azure data factory","<p>I have a mapping data flow that is checking for invalid data, with conditional split, in my file. (example row in column one &lt; than in column two.)</p>
<p>I the case I don't have any invalid data I would like to copy the file in new folder.
if I have at least one error in my file I want to copy the file in an error folder.</p>
<p>The problem is that the validation is on the row level and I can't use a lookup.</p>
<p>Thank you for your help</p>
","<azure><azure-data-factory>","2020-10-08 14:53:06","545","0","2","64280266","<p>Possible approach I can think of is by using the additional file for invalid data and later the size can be evaluated to check if there any invalid records are not and assign the folder name accordingly. The invalid data can be retrieved by using filter activity in dataflow. Below GIF shows the details of the approach used.</p>
<p><a href=""https://i.stack.imgur.com/TpMFN.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TpMFN.gif"" alt=""enter image description here"" /></a></p>
"
"64265067","Connect Oracle Autonomous Database in Microsoft Azure Data Factory","<p>Anyone in here have already connected Oracle Autonomous DB in Azure Data Factory?</p>
<p>We are trying to implement some data copies from SQL Server to Oracle, but I am not able to reach Oracle Autonomous Database, using standard host, port, service name and credentials.
Maybe any security layer related to the Wallet is missing?
I can't find any documents explaining.</p>
<p>The error when trying to connect is:
ERROR [08S01] [Microsoft][ODBC Oracle Wire Protocol driver]Socket closed.
ERROR [08001] [Microsoft][ODBC Oracle Wire Protocol driver][Oracle]Connection Dead.</p>
<p>Thanks,
Guilherme</p>
","<oracle><azure><azure-data-factory><oracle-cloud-infrastructure>","2020-10-08 14:46:34","1813","0","1","64265958","<p>You are using a 3rd Party ODBC Driver in sense of Oracle - please have a look here for other issues using the Microsoft ODBC driver</p>
<p><a href=""https://support.microsoft.com/en-us/help/4537072/fix-creating-an-external-table-against-an-oracle-database-in-sql-serve"" rel=""nofollow noreferrer"">https://support.microsoft.com/en-us/help/4537072/fix-creating-an-external-table-against-an-oracle-database-in-sql-serve</a></p>
<p>I'm not sure if the workaround applying the patch for SQL Server will help - same for setting</p>
<p>sqlnet.allowed_logon_version_server=8</p>
<p>on Oracle Server machin - but at least its a better idea to use the Oracle ODBC Driver or OLEDB Provider connecting from SQL Server to Oracle DB (and you got the oppertunity to open Service request at Oracle when not working :-)</p>
<p>placing on Oracle $ORACLE_HOME/network/admin or wherelse sqlnet.ora is stored on database will work</p>
"
"64254858","How can I resolve a timeout using Azure Data Factory (ADF) when connecting to MySQL?","<p>I am trying to copy data from a MySQL database to Azure SQL Server but I am getting a timeout error:</p>
<blockquote>
<p>Operation on target Copy MyTable failed:
ErrorCode=UserErrorFailedToConnectOdbcSource,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR
[08001] [Microsoft][MySQL] (1004) The connection has timed out while
connecting to server: xxxxx.xxxxxx.us-west-2.rds.amazonaws.com at
port:
1234.,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,''Type=System.Data.Odbc.OdbcException,Message=ERROR
[08001] [Microsoft][MySQL] (1004) The connection has timed out while
connecting to server: xxxxx.xxxxxx.us-west-2.rds.amazonaws.com at
port: 1234.,Source=,'</p>
</blockquote>
<p>I can preview data while looking at the source of my Copy Data task. There is no timeout. I see all of the rows and columns. I even changed the query to limit the results to 2 rows.</p>
<pre><code>SELECT mytable.id, mytable.name FROM myschema.mytable LIMIT 2;
</code></pre>
<p>However, when I publish the pipeline and trigger it to run I get the timeout error. How can I resolve the timeout using Azure Data Factory (ADF) when connecting to MySQL?</p>
","<azure-data-factory>","2020-10-08 02:17:52","5426","1","1","64266346","<p>The error message was not the most helpful. I discovered what the problem was. The problem was that the IP Addresses used by ADF had to be added to the &quot;Outbound IP&quot; list from AWS MySQL. Everything started working once I updated the outbound IP address list.</p>
"
"64252945","How to use pipeline parameters in DF Data Flow, Dataset source options","<p>I've been at this for a day now, trying every variation possible and searching for others solutions. I have a DF pipeline with a few DataBricks notebooks with the end result being saved to blob storage, but needs to be joined with a SQL table in order to update some values. I am using dynamic folder names to pull the blob file which works fine, but when I try the same thing for my SQL query, it doesn't fail, but it doesn't seem to select any records. So how does one use pipeline parameters in a Data Flow SQL query?</p>
<p>Overview of the pipeline:<a href=""https://i.stack.imgur.com/88xvp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/88xvp.png"" alt=""enter image description here"" /></a></p>
<p>The failing source:<a href=""https://i.stack.imgur.com/yf7d7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yf7d7.png"" alt=""enter image description here"" /></a></p>
<p>My query to use the variable:
<a href=""https://i.stack.imgur.com/Af4yj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Af4yj.png"" alt=""enter image description here"" /></a></p>
<p>Finally the results:
<a href=""https://i.stack.imgur.com/GnNnJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GnNnJ.png"" alt=""enter image description here"" /></a></p>
<p>Any help is greatly appreciated!</p>
","<azure-data-factory><azure-databricks>","2020-10-07 22:03:46","2452","0","1","64254465","<p>The Azure SQL database source dataset doesn't work in data flow.</p>
<p>You need to create a pipeline paremeter a data flow parameter, use data flow parameter in the query.</p>
<p>For example:</p>
<p><strong>Create the data flow parameter:</strong>
<a href=""https://i.stack.imgur.com/Guo6P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Guo6P.png"" alt=""enter image description here"" /></a></p>
<p><strong>Query option:</strong></p>
<pre><code>concat('select * from ', $Category)
</code></pre>
<p><a href=""https://i.stack.imgur.com/aWqfk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aWqfk.png"" alt=""enter image description here"" /></a></p>
<p><strong>Pipeline parameter:</strong>
<a href=""https://i.stack.imgur.com/zSx3x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zSx3x.png"" alt=""enter image description here"" /></a></p>
<p><strong>Set the Data Flow parameter value from pipeline</strong>:
<a href=""https://i.stack.imgur.com/YBnsk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YBnsk.png"" alt=""enter image description here"" /></a></p>
"
"64248612","Delete resource in ADF through ARM templates in CI/CD not working","<p>In my ADF CI/CD setup <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#hotfix-production-environment"" rel=""nofollow noreferrer"">mentioned here</a>, when I deploy to ACC or PRD after I have deleted some resources (linked service or pipeline), it is not getting deleted in ACC or PRD. But those things I add or edit are updated correctly. How can I fix it ?</p>
","<azure><azure-rm-template><azure-data-factory>","2020-10-07 16:44:59","626","1","1","64251203","<p>I'm assuming the deployment task is set to Incremental as in your documentation.  If this is the case then the deployment is doing a delta on the existing defined resources.  Since the ones deleted out of your ARM template the Deployment task does not know to evaluate them.  You can set it to &quot;deployment&quot; mode as stated in the link you provided:
<a href=""https://i.stack.imgur.com/7qiLf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7qiLf.png"" alt=""enter image description here"" /></a></p>
<p>Keep in mind this will delete and recreate and potentially involve some downtime while recreation occurs.</p>
<p>For more information check out the different <a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/deployment-modes"" rel=""nofollow noreferrer"">Azure Resource Manager deployment modes</a>.</p>
"
"64248567","Azure ADF - how to proceed with pipeline activities only after new files arrival?","<p>I have written generic datafiles arrival checking routine using databricks notebooks which accepts filenames and time which specifies acceptable freshness of files. many pipeline uses this notebook and passes filenames tuples and at end notebook returns True or False, to indicate if next workflow activity could start or not. so far so good.
now my question is how to use this in Azure ADF pipeline such that if it fails it should wait for 30 minutes or so and check again by running above notebook again?
this notebook shall run first so that if new files are already there then it should not wait</p>
","<azure-data-factory><azure-databricks>","2020-10-07 16:42:13","140","0","1","64253658","<p>Since you are talking about the notebook activity , you can add a wait activity &quot;on failue  &quot; and set the time for the wait . after wait add a executepipelien actvity . This execute pipeline should point to a pipeline with a execute pipeline ( again ) pointing to the main pipeline which has the notebook activity . Basically this is just a cycle , but will only execute when you have a failure .</p>
"
"64247278","How can I find the IP address for my Azure Data Factory v2 (ADF)?","<p>I need to know the IP Address being used by my Azure Data Factory. I need it so I can add it to the whitelist used by a cloud service.</p>
<p>When I need to find the IP Address for my local machine, I can go to whatismyipaddress.com.</p>
<p>How can I find the IP address for my Azure Data Factory (ADF)?</p>
","<azure-data-factory>","2020-10-07 15:25:57","10604","5","1","64249131","<p>Here is a json file with all the IP addresses. <a href=""https://www.microsoft.com/en-us/download/confirmation.aspx?id=56519"" rel=""noreferrer"">Azure IP Ranges</a>. Search for the string <code>&quot;name&quot;: &quot;DataFactory.</code> and the reigon that your Data Factory is located in. For example: <code>&quot;name&quot;: &quot;DataFactory.CentralUS&quot;</code></p>
<pre><code>{
  &quot;name&quot;: &quot;DataFactory.CentralUS&quot;,
  &quot;id&quot;: &quot;DataFactory.CentralUS&quot;,
  &quot;properties&quot;: {
    &quot;changeNumber&quot;: 3,
    &quot;region&quot;: &quot;centralus&quot;,
    &quot;regionId&quot;: 31,
    &quot;platform&quot;: &quot;Azure&quot;,
    &quot;systemService&quot;: &quot;DataFactory&quot;,
    &quot;addressPrefixes&quot;: [
      &quot;13.89.174.192/28&quot;,
      &quot;20.37.154.0/23&quot;,
      &quot;20.37.156.0/26&quot;,
      &quot;20.40.206.224/29&quot;,
      &quot;20.44.10.64/28&quot;,
      &quot;52.182.141.16/28&quot;,
      &quot;2603:1030:10:1::480/121&quot;,
      &quot;2603:1030:10:1::500/122&quot;,
      &quot;2603:1030:10:1::700/121&quot;,
      &quot;2603:1030:10:1::780/122&quot;,
      &quot;2603:1030:10:402::330/124&quot;,
      &quot;2603:1030:10:802::210/124&quot;,
      &quot;2603:1030:10:c02::210/124&quot;
    ],
    &quot;networkFeatures&quot;: [
      &quot;API&quot;,
      &quot;NSG&quot;,
      &quot;UDR&quot;,
      &quot;FW&quot;
    ]
  }
},
</code></pre>
"
"64242491","How to store Azure DataFactory logs to ApplicationInsights","<p>When running Azure Data Factory pipeline, We want to do audit logging in Azure Application Insights.
Can anyone help with any step by step guide on how to log data into Application Insights from Azure DataFactory pipeline tasks?</p>
<p>Thanks</p>
","<logging><azure-data-factory><azure-application-insights>","2020-10-07 10:49:48","1538","0","1","64256960","<p>Currently, ADF is not directly hooked up with Application Insights. But as per <a href=""https://stackoverflow.com/a/61393250/10185816"">this post</a>, you can try to use Web Activity in the ADF to invoke the Application Insights REST API after execution of your main activities.</p>
<p>And for ADF, we do suggest using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#configure-diagnostic-settings-and-workspace"" rel=""nofollow noreferrer"">Azure Monitor</a> instead of Application Insights.</p>
"
"64241009","ADF Dynamic Expression - concat/if missing period","<p>In what I would have thought was relatively simple code, I can't figure out what the issue is with adding another string to my list of concatenations</p>
<p>Below is the code that I currently have and I get expected output</p>
<p><code>@concat('start ', if(equals(coalesce(pipeline().parameters.p_source_object.TYPE,''),'x'), 'a', 'b'))</code></p>
<p>However, I want to add more strings to the concatenation but when I add a comma between the two end brackets like so</p>
<p><code>@concat('start ', if(equals(coalesce(pipeline().parameters.p_source_object.TYPE,''),'x'), 'a', 'b'), )</code></p>
<p>I get an &quot;Invalid&quot; error with &quot;Missing period&quot; message. If I put a period before the comma, the error goes away (but obviously invalid syntax)</p>
<p>What is it expecting here?</p>
<p>On a related note, is there a better way to concatenate while also doing some functions that output strings? It's the most unintuitive interface imaginable (Microsoft do seem to pride themselves in the ridiculous!)</p>
<p>Hoping someone can find my sanity for me!</p>
","<azure><azure-data-factory>","2020-10-07 09:20:47","2050","4","1","64258630","<p>In the end, I completely reworked it avoiding the layers... however, I have discovered a resolution</p>
<p><code> @{concat('start ', if(equals(coalesce(pipeline().parameters.p_source_object.TYPE,''),'x'), 'a', 'b'), 'dd')}</code></p>
<p>While it doesn't stand out as to how... there's a space at the start of the line, this stops it being considered &quot;Dynamic content&quot; but instead uses string interpolation</p>
"
"64235654","Is there where to do line breaks in the expression builder of azure data flow?","<p>I am using the Expression Builder of Derived Column setting of the Azure Data Flow Activity. I am trying to code ac line break. So it looks like:</p>
<p>Value1
Value2</p>
<p>What is the syntax to code this?</p>
","<line-breaks><azure-data-factory><expressionbuilder>","2020-10-07 00:44:52","1694","0","1","64236203","<p>You can use <code>'\n'</code> to code ac line break in the DerivedColumn.</p>
<p>In my case, I use <code>concat(Column1,concat('\n',Column2))</code> epression to concat two columns.
<a href=""https://i.stack.imgur.com/6IjZ0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6IjZ0.png"" alt=""enter image description here"" /></a></p>
<p>So I get the results, the Column3 looks like:<br />
Value1 Value2</p>
<p><a href=""https://i.stack.imgur.com/Of9mn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Of9mn.png"" alt=""enter image description here"" /></a></p>
"
"64233463","Executing datafactory pipeline results undocumented 3208 error","<p>This issue was already reported in <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/50030"" rel=""nofollow noreferrer"">GitHub</a>. We have the same issue and trying my luck in this forum for speedy resolution. Very little technical details are known on this error. This issued showed up in our newly created resourceGroup that has vNet (resourceGrough that doesn't have vNet or SelfHostedRuntime works fine).</p>
<p>Where is the issue Datafactory or Databricks?
We have vNet, Datafactory, SelfHostedRuntime, Databricks (with managed subnets).</p>
<p>We are getting below variant of the same error...</p>
<blockquote>
<p>{
&quot;errorCode&quot;: &quot;3208&quot;,
&quot;message&quot;: &quot;An error occurred while sending the request.&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;process-demo-source-data&quot;,
&quot;details&quot;: []
}</p>
</blockquote>
","<databricks><azure-databricks><vnet><azure-data-factory>","2020-10-06 20:36:06","67","0","1","64384873","<p>Azure Data Flows internally uses Azure Databricks. Dataflows helps build orchestration, activity and resource management and then Azure Databricks helps to build compute.</p>
<p>Generally, you will receive this error message for the following reason.</p>
<ul>
<li>There is a network connection issue with the  Databricks service.</li>
<li>Azure Databricks service was interrupted due to any specific outages.</li>
</ul>
<p>Make sure to subscribe to Azure Databricks service updates:<a href=""https://learn.microsoft.com/en-us/azure/databricks/status"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/databricks/status</a></p>
"
"64227251","Azure Data Factory Get Metadata to get blob filenames and transfer them to Azure SQL database table","<p>I am trying to use Get Metadata activity in Azure Data Factory in order to get blob filenames and copy them to Azure SQL database table.
I follow this tutorial: <a href=""https://www.mssqltips.com/sqlservertip/6246/azure-data-factory-get-metadata-example/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/6246/azure-data-factory-get-metadata-example/</a></p>
<p>Here is my pipeline, Copy Data &gt; Source is the source destination of the blob files in my Blob storage. I need to specify my source file as binary because they are *.jpeg files.</p>
<p><a href=""https://i.stack.imgur.com/HKFIm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HKFIm.png"" alt=""GetMetadata0"" /></a></p>
<p>For my Copy Data &gt; Sink, its the Azure SQL database, I enable the option &quot;Auto Create table&quot;</p>
<p><a href=""https://i.stack.imgur.com/g2spY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g2spY.png"" alt=""GetMetadata1"" /></a></p>
<p>In my Sink dataset config, I had to choose one table because the validation won't pass if I don't select the table in my SQL database even though this table is not related at all to the blob filenames that I want to get.</p>
<p><a href=""https://i.stack.imgur.com/sE3wn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sE3wn.png"" alt=""GetMetadata2"" /></a></p>
<p>Question 1: Am I supposed to create a new table in SQL DB before to have the columns matching the blob filenames that I want to extract?</p>
<p>Then, I tried to validate the pipeline and I get this error.</p>
<pre><code>Copy_Data_1
Sink must be binary when source is binary dataset.
</code></pre>
<p><a href=""https://i.stack.imgur.com/62Gu7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/62Gu7.png"" alt=""Getmetadata3"" /></a></p>
<p>Question 2: How can I resolve this error? I had to select the file type of the source as binary as it's one of the step when creating source dataset. Therefore, when I choose sink dataset that is Azure SQL table, I didn't have to select the type of dataset so it doesn't seem to match.</p>
<p>Thank you very much in advance.</p>
<p>New screenshot of the new pipeline, I can now get itemName of filenames in the json output files.</p>
<p><a href=""https://i.stack.imgur.com/VAhap.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VAhap.png"" alt=""GetFileName1"" /></a></p>
<p>Now I add Copy Data activity just after Get_File_Name2 activity and connect them together to try to get the json output files as source dataset.</p>
<p><a href=""https://i.stack.imgur.com/cBTI1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cBTI1.png"" alt=""GetFileName2"" /></a></p>
<p>However, I need to choose the source dataset location first before specify type as json. But, as far as I understand these output json files are the output from Get_File_Name2 activity and they are not yet stored on Blob storage. How do I make the copy data activity reading these json output file as source dataset?</p>
<p><a href=""https://i.stack.imgur.com/O8cCs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O8cCs.png"" alt=""GetFileName3"" /></a></p>
<p><em><strong><strong>Update 10/14/2020</strong></strong></em>
Here is my new activity stored procedure, I added the parameter as suggested however, I changed the name to JsonData as my stored procedure requires this parameter.</p>
<p><a href=""https://i.stack.imgur.com/tUe2J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tUe2J.png"" alt=""storedprocedure1"" /></a></p>
<p>This is my stored procedure.</p>
<p><a href=""https://i.stack.imgur.com/UrG0k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UrG0k.png"" alt=""storedprocedure2"" /></a></p>
<p>I get this error at the stored procedure:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2402&quot;,
    &quot;message&quot;: &quot;Execution fail against sql server. Sql error number: 13609. Error Message: JSON text is not properly formatted. Unexpected character 'S' is found at position 0.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Stored procedure1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/RJbk4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RJbk4.png"" alt=""storedprocedure3"" /></a></p>
<p>But when I check the input, it seems like it already successfully reading the json string itemName.</p>
<p><a href=""https://i.stack.imgur.com/doZOD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/doZOD.png"" alt=""storedprocedure4"" /></a></p>
<p>But, when I check output, it's not there.</p>
<p><a href=""https://i.stack.imgur.com/TERuF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TERuF.png"" alt=""storedprocedure5"" /></a></p>
","<stored-procedures><azure-sql-database><metadata><azure-data-factory><copydataset>","2020-10-06 13:52:45","5260","0","1","64241502","<p>Actually, you may could using Get metadata output json as the parameter and then call the stored procedure: <code>Get metedata</code>--&gt;<code>Stored Procedure</code>!</p>
<p>You just need focus on the coding of the stored procedure.</p>
<p>Get Metadata output <code>childitems</code>:</p>
<pre><code>{
   &quot;childItems&quot;: [
        {
            &quot;name&quot;: &quot;DeploymentFiles.zip&quot;,
            &quot;type&quot;: &quot;File&quot;
        },
        {
            &quot;name&quot;: &quot;geodatalake.pdf&quot;,
            &quot;type&quot;: &quot;File&quot;
        },
        {
            &quot;name&quot;: &quot;test2.xlsx&quot;,
            &quot;type&quot;: &quot;File&quot;
        },
        {
            &quot;name&quot;: &quot;word.csv&quot;,
            &quot;type&quot;: &quot;File&quot;
        }
}
</code></pre>
<p><strong>Stored Procedure:</strong></p>
<pre><code>@activity('Get Metadata1').output.childitems
</code></pre>
<p><a href=""https://i.stack.imgur.com/RKUou.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RKUou.png"" alt=""enter image description here"" /></a></p>
<p>About how to create the stored procedure(get data from json object), you could ref this blog: <a href=""https://chris.koester.io/index.php/2018/03/25/retrieve-json-data-from-sql-server-using-a-stored-procedure-and-csharp/"" rel=""nofollow noreferrer"">Retrieve JSON Data from SQL Server using a Stored Procedure</a>.</p>
"
"64227051","Azure Data Factory connection to Databricks doesn't work when using Key Vault to retrieve token","<p>I have a Databricks instance which does some work. Jobs are triggered from Azure Data Factory. There is several environments and each one has its own Key Vault to store secrets.</p>
<p>As long as I kept access token - let's say &quot;hardcoded&quot; - within a Databricks linked service configuration everything worked fine. But I need to comply with security standards, so keeping it in JSON which lays somewhere isn't an option - it was fine for the time being.</p>
<p>Key Vault to the rescue - access token to the Databricks is created via API and stored in a Key Vault, now I wanted to use the Key Vault as linked service in Databricks linked service to populate access token, and the surprise comes here - it doesn't work.</p>
<p><strong>I can't debug pipeline, I can't trigger it, I can't even test a connection, it always fails with 403 Invalid access token:</strong></p>
<p><a href=""https://i.stack.imgur.com/qfRcD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfRcD.png"" alt=""enter image description here"" /></a></p>
<p>The JSON for this linked service:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;name&quot;: &quot;ls_databricks&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureDatabricks&quot;,
        &quot;typeProperties&quot;: {
            &quot;domain&quot;: &quot;https://**************.azuredatabricks.net&quot;,
            &quot;accessToken&quot;: {
                &quot;type&quot;: &quot;AzureKeyVaultSecret&quot;,
                &quot;store&quot;: {
                    &quot;referenceName&quot;: &quot;ls_keyVault&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                },
                &quot;secretName&quot;: &quot;DatabricksAccessToken&quot;
            },
            &quot;existingClusterId&quot;: &quot;*********&quot;
        }
    }
}
</code></pre>
<hr />
<p>While, using Postman I can easily access Databricks API using the same access token:
<a href=""https://i.stack.imgur.com/ViNN3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ViNN3.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>Key Vault linked service itself works fine and connection test passes:
<a href=""https://i.stack.imgur.com/RYEec.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RYEec.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>I have configured different linked service to connect to ADLS using Key Vault and it works as expected:
<a href=""https://i.stack.imgur.com/Nmxo1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nmxo1.png"" alt=""enter image description here"" /></a></p>
<p>Does anybody have any ideas what's wrong here?
It is just broken or I'm doing something wrong?</p>
<p>p.s. Apologies for flooding you with all of these screenshots :)</p>
<p>I'm using <a href=""https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html</a> SCIM API to entitle my service principal to a proper group on Databricks instance.</p>
","<azure><azure-data-factory><http-status-code-403><azure-keyvault><azure-databricks>","2020-10-06 13:40:15","2705","0","2","64228952","<p>Have you set up the appropriate access policy in the Key Vault? The preferred methodology would be to turn on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">Managed Identity for Data Factory</a> and then add the <a href=""https://learn.microsoft.com/en-us/azure/key-vault/general/assign-access-policy-portal"" rel=""nofollow noreferrer"">Data Factory identity to the Key Vault</a> access policy</p>
<p>Key Vault has a separate tier of access to the secrets which needs to be configured for Data Factory since Data Factory is trying to access it and needs to be provisioned access to the secrets.</p>
"
"64227051","Azure Data Factory connection to Databricks doesn't work when using Key Vault to retrieve token","<p>I have a Databricks instance which does some work. Jobs are triggered from Azure Data Factory. There is several environments and each one has its own Key Vault to store secrets.</p>
<p>As long as I kept access token - let's say &quot;hardcoded&quot; - within a Databricks linked service configuration everything worked fine. But I need to comply with security standards, so keeping it in JSON which lays somewhere isn't an option - it was fine for the time being.</p>
<p>Key Vault to the rescue - access token to the Databricks is created via API and stored in a Key Vault, now I wanted to use the Key Vault as linked service in Databricks linked service to populate access token, and the surprise comes here - it doesn't work.</p>
<p><strong>I can't debug pipeline, I can't trigger it, I can't even test a connection, it always fails with 403 Invalid access token:</strong></p>
<p><a href=""https://i.stack.imgur.com/qfRcD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfRcD.png"" alt=""enter image description here"" /></a></p>
<p>The JSON for this linked service:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;name&quot;: &quot;ls_databricks&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureDatabricks&quot;,
        &quot;typeProperties&quot;: {
            &quot;domain&quot;: &quot;https://**************.azuredatabricks.net&quot;,
            &quot;accessToken&quot;: {
                &quot;type&quot;: &quot;AzureKeyVaultSecret&quot;,
                &quot;store&quot;: {
                    &quot;referenceName&quot;: &quot;ls_keyVault&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                },
                &quot;secretName&quot;: &quot;DatabricksAccessToken&quot;
            },
            &quot;existingClusterId&quot;: &quot;*********&quot;
        }
    }
}
</code></pre>
<hr />
<p>While, using Postman I can easily access Databricks API using the same access token:
<a href=""https://i.stack.imgur.com/ViNN3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ViNN3.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>Key Vault linked service itself works fine and connection test passes:
<a href=""https://i.stack.imgur.com/RYEec.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RYEec.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>I have configured different linked service to connect to ADLS using Key Vault and it works as expected:
<a href=""https://i.stack.imgur.com/Nmxo1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nmxo1.png"" alt=""enter image description here"" /></a></p>
<p>Does anybody have any ideas what's wrong here?
It is just broken or I'm doing something wrong?</p>
<p>p.s. Apologies for flooding you with all of these screenshots :)</p>
<p>I'm using <a href=""https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html</a> SCIM API to entitle my service principal to a proper group on Databricks instance.</p>
","<azure><azure-data-factory><http-status-code-403><azure-keyvault><azure-databricks>","2020-10-06 13:40:15","2705","0","2","64277484","<p>Finally I was able to solve that using Databricks CLI extensions.</p>
<p>Working solution (wonder for how long, it's experimental extension):</p>
<pre><code>  az extension add --name datafactory
  
  $lsDatabricks = @{
      &quot;type&quot; = &quot;AzureDatabricks&quot;
      &quot;typeProperties&quot; = @{
          &quot;domain&quot; = &quot;https://***********.azuredatabricks.net&quot;
          &quot;existingClusterId&quot; = &quot;************-vale**&quot;
          &quot;accessToken&quot; = @{ 
              &quot;type&quot; = &quot;SecureString&quot;
              &quot;value&quot; = &quot;dapi******************************&quot;
          }
      }
  }

  $lsJson = $lsDatabricks | ConvertTo-Json -Compress 
  $lsJson = $lsJson -Replace '&quot;', '\&quot;'

  az datafactory linked-service create --factory-name &quot;yourAdf&quot; --name &quot;yourDatabricksLinkckedService&quot; --resource-group &quot;yourGroup&quot; --properties &quot;$lsJson&quot;
</code></pre>
<p>I traveled very bumpy road just to configure silly linked service. By the way, I was trying do it via parameterized template, like:</p>
<pre><code>&quot;AzureDatabricks&quot;: {
    &quot;properties&quot;: {
        &quot;typeProperties&quot;: {
            &quot;domain&quot;: &quot;=&quot;,
            &quot;existingClusterId&quot;: &quot;=&quot;,
            &quot;accessToken&quot;: &quot;=:accessToken:secureString&quot;
        }
    }
</code></pre>
<p>Unfortunately, whatever value I was overriding deploying ARM template, final value was unchanged.</p>
"
"64226508","Azure Data Factory Copy JSON to a row in a table","<p>I have a working ADF workflow copying the data from GET API call using For Each loop changing the query string each time based on the lookup JSON file and saving separate files to a BLOB storage as JSON files. I have a question - is it possible to load this data into SQL table with a structure of (id, timestamp, filename, json) which means storing each API call result in this table in a new sperate row? I have a problem with mapping the fields to the SQL final table as I can't use simple item().File or get contents of the JSON file that is now stored in container.</p>
","<json><azure><azure-data-factory>","2020-10-06 13:10:21","3150","0","1","64315339","<p>You can use Stored Procedure activity to sink the json object into one column.</p>
<p>I made a simple test here:</p>
<p>1.I use Lookup activity to get a json array from a rest api.</p>
<p><a href=""https://i.stack.imgur.com/bi4Fe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bi4Fe.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>I create sql table and stored procedure in Azure SQL:</li>
</ol>
<pre class=""lang-sql prettyprint-override""><code>create table dbo.Logs (
    _id bigint primary key IDENTITY(1,1),
    log nvarchar(max)
);

--Strored procedure
SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO

CREATE PROCEDURE [dbo].[spUpsertLogs]

@logs nvarchar(max)

AS

BEGIN

INSERT INTO dbo.Logs (log) values(@logs)

END
</code></pre>
<p>3.Then I set the Stroed procedure activity, specify the name and import parameters of the Stroed procedure,  use expression <code>@string(activity('Lookup1').output.value)</code> to convert the json array to String type.</p>
<p><a href=""https://i.stack.imgur.com/3pLY7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3pLY7.png"" alt=""enter image description here"" /></a></p>
<p>4.Run debug, the json array will be copied into one column in the sql table. The result shows:</p>
<p><a href=""https://i.stack.imgur.com/Nos0m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nos0m.png"" alt=""enter image description here"" /></a></p>
<p>Hope my answer is helpful for you. Store JSON documents in SQL Server, you can reference <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/json/store-json-documents-in-sql-tables?view=sql-server-ver15"" rel=""nofollow noreferrer"">here</a>.</p>
"
"64225209","How to capture the error message of the each activity among the multiple activities(3) in a pipeline using single stored procedure","<p>I have a ADF pipeline with 3 different activities in sequence. I want to capture the error message of the activity whichever gets failed. So I have created a stored procedure by passing a parameter with which it can capture using <code>@activity('Activity name').Error.Message</code></p>
<p>But using that expression I can only get the error message for the specified activity.</p>
<p>How can I capture the error message of <em>any</em> activity (of three activities in the pipeline)
This would be the pipeline output as the activities are in sequence.</p>
","<azure><azure-data-factory><azure-monitoring>","2020-10-06 11:49:43","339","0","2","64386776","<p>As far as I know, you need to create three stored procedure to capture error message of correspond activity.</p>
"
"64225209","How to capture the error message of the each activity among the multiple activities(3) in a pipeline using single stored procedure","<p>I have a ADF pipeline with 3 different activities in sequence. I want to capture the error message of the activity whichever gets failed. So I have created a stored procedure by passing a parameter with which it can capture using <code>@activity('Activity name').Error.Message</code></p>
<p>But using that expression I can only get the error message for the specified activity.</p>
<p>How can I capture the error message of <em>any</em> activity (of three activities in the pipeline)
This would be the pipeline output as the activities are in sequence.</p>
","<azure><azure-data-factory><azure-monitoring>","2020-10-06 11:49:43","339","0","2","72777045","<pre><code>@concat(activity('Activity1').Error?.message,'|',activity('Activity2')?.Error?.message,'|',activity('Activity3')?.Error?.message)
</code></pre>
"
"64222551","AZURE | ADF | Logging the pipeline runs (execution details) in sql table","<p>I want to design a feature in ADF pipeline by which I can store the execution details in sql table or CSV files. execution details for example at which activity pipeline failed pipeline name ,how many records got inserted updated in table by running the activity. this kind of information i want to log for multiple pipelines.</p>
<p>Thanks,
Amol</p>
","<azure><azure-data-factory><azure-sql-reporting>","2020-10-06 09:01:18","370","0","1","64228141","<p>You will have to manage this external to ADF. Here are a couple links that may help get you started:</p>
<p><a href=""https://stackoverflow.com/questions/59085000/method-to-put-alerts-on-long-running-azure-data-factory-pipeline/59290603#59290603"">SO: Method to put alerts on long running azure data factory pipeline</a></p>
<p>This is a link to a presentation I gave earlier this year about creating an ADF Pipeline Management System:
www .youtube.com/watch?v=V8dLIIb6qGY</p>
<p>[NOTE: I'm not sure the rules on such self-promotional items, that's why this isn't a link.]</p>
"
"64220765","Why does external I.P. need access to on-prem sql database when moving data with ADF to Azure SQL?","<p>Why does external I.P. need access to on-prem sql database when copying data with ADF to Azure SQL?</p>
<p>It looks like on-prem sql makes a direct connection to Azure SQL (bypassing ADF). Is this by design or do I follow the wrong workflow?</p>
","<azure-sql-database><azure-data-factory>","2020-10-06 06:56:35","89","0","1","64236349","<p>Data Factory use the integration runtime to help us create the connection to the Source/Sink dataset. Azure integration runtime for cloud dataset and Self-host integration runtime for on-premise source/sink dataset.</p>
<ul>
<li>The integration runtime (IR) is the compute infrastructure that Azure
Data Factory uses to provide data-integration capabilities across
different network environments. For details about IR, see Integration
runtime overview.</li>
<li>A self-hosted integration runtime can run copy activities between a
cloud data store and a data store in a private network. It also can
dispatch transform activities against compute resources in an
on-premises network or an Azure virtual network. The installation of
a self-hosted integration runtime needs an on-premises machine or a
virtual machine inside a private network.</li>
</ul>
<p>Azure integration runtime is provides by ADF in default. The self-host integration runtime must be created manually.</p>
<p>That means Data Factory can not access the on-prem SQL database directly. It need the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">self-host integration runtime</a> to help us connect to the on-prem SQL database.</p>
<p>It means that the on-prem sql does not make a direct connection to Azure SQL(bypassing ADF. That why external I.P. need access to on-prem sql database when copying data with ADF to Azure SQL.</p>
<p>HTH.</p>
"
"64220471","How to fetch Azure Databricks notebook run details","<p>I am using Azure Data Factory to run my databricks notebook, which creates job cluster at runtime, Now I want to know the status of those jobs, I mean whether they are Succeeded or Failed.
So may I know, how can I get that status of runs by using job id or run id.</p>
<p>Note: I have not created any jobs in my databricks workspace, I am running my notebooks using Azure Data Factory which created job cluster at the runtime and it runs that notebook on top of that cluster and then it terminated that cluster</p>
","<apache-spark><pyspark><azure-data-factory><databricks><azure-databricks>","2020-10-06 06:31:18","3122","2","3","64234920","<p>You'll have to go to the monitor page in Azure Data Factory. You'll be able to filter by runId here.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook#monitor-the-pipeline-run"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook#monitor-the-pipeline-run</a></p>
"
"64220471","How to fetch Azure Databricks notebook run details","<p>I am using Azure Data Factory to run my databricks notebook, which creates job cluster at runtime, Now I want to know the status of those jobs, I mean whether they are Succeeded or Failed.
So may I know, how can I get that status of runs by using job id or run id.</p>
<p>Note: I have not created any jobs in my databricks workspace, I am running my notebooks using Azure Data Factory which created job cluster at the runtime and it runs that notebook on top of that cluster and then it terminated that cluster</p>
","<apache-spark><pyspark><azure-data-factory><databricks><azure-databricks>","2020-10-06 06:31:18","3122","2","3","64383187","<pre><code>import json
import requests

gethooks= &quot;https://&quot; + databricks_instance_name + &quot;/api/2.0/jobs/runs/list&quot;     #add your databricks workspace instance name over here
headers={&quot;Authorization&quot;: &quot;Bearer ********************&quot;}        # Add your databricks access token
response = requests.get(gethooks, headers=headers)

print(response.json())      # you will get all cluster and job related info over here in json format

# traversing through response.json
for element in response.json()['runs']:
    job_id = element['job_id']
    status = element['state']['result_state']
    job_path = element['task']['notebook_task']['notebook_path']
    job_name = job_path.split('/')

</code></pre>
"
"64220471","How to fetch Azure Databricks notebook run details","<p>I am using Azure Data Factory to run my databricks notebook, which creates job cluster at runtime, Now I want to know the status of those jobs, I mean whether they are Succeeded or Failed.
So may I know, how can I get that status of runs by using job id or run id.</p>
<p>Note: I have not created any jobs in my databricks workspace, I am running my notebooks using Azure Data Factory which created job cluster at the runtime and it runs that notebook on top of that cluster and then it terminated that cluster</p>
","<apache-spark><pyspark><azure-data-factory><databricks><azure-databricks>","2020-10-06 06:31:18","3122","2","3","68816791","<p><a href=""https://forums.databricks.com/questions/12134/possible-to-get-job-run-id-of-notebook-run-by-dbut.html"" rel=""nofollow noreferrer"">https://forums.databricks.com/questions/12134/possible-to-get-job-run-id-of-notebook-run-by-dbut.html</a></p>
<pre><code>dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson() 
</code></pre>
"
"64216566","How to execute Power Automate at the successful completion of Azure data factory pipeline?","<p>I need to create and run Power Automate flow to refresh Power BI dataset at the successful completion of my Azure data factory pipeline.</p>
<p>Flow should not run if data pipeline fails. How could I do it?</p>
","<powerbi><azure-data-factory><power-automate>","2020-10-05 21:40:27","1552","1","2","64216871","<p>You don't need a Power Automate flow to refresh the Power BI dataset, when ADF pipeline is successful. That can be done directly from ADF pipeline to Power BI dataset.</p>
<p><a href=""https://www.moderndata.ai/2019/05/powerbi-dataset-refresh-using-adf/"" rel=""nofollow noreferrer"">How To Trigger A Power BI Dataset Refresh Using An Azure Data Factory Web Activity</a></p>
"
"64216566","How to execute Power Automate at the successful completion of Azure data factory pipeline?","<p>I need to create and run Power Automate flow to refresh Power BI dataset at the successful completion of my Azure data factory pipeline.</p>
<p>Flow should not run if data pipeline fails. How could I do it?</p>
","<powerbi><azure-data-factory><power-automate>","2020-10-05 21:40:27","1552","1","2","73135255","<p>You can use <strong>Do Until</strong> after <strong>Create a pipeline run</strong> activity.
Inside <strong>Do Until</strong> you will have to specify until what(condition) your <em><strong>loop</strong></em> should run. Here you have to check the if the <strong>Status of the pipeline is not equal to InProgress</strong>. Now the loop should have 2 flows, namely - <strong>get a pipeline run</strong> (to fetch the status) and <strong>delay timer of 10s</strong> to recheck the pipeline status in every 10s. After the do until loop, <strong>set a powerbi refresh</strong> and voila.</p>
<p><img src=""https://i.stack.imgur.com/0WpKG.png"" alt=""1"" /></p>
"
"64208970","Is it possible to rename file name with MD5 hash encoding in Azure data factory?","<p>I have this scenario where I have a blob with multiple binary files and I would like to copy these files into another blob storage using Azure data factory.
The issue is my destination file name should be MD5 encoded . Now I would like to know if it is possible in ADF to change the file name to MD5 encoded type in destination and if yes can someone outline the steps on how to do it.</p>
<p>I can copy the files but i cant rename it to MD5 encoding.</p>
<p>Waiting for experts comments!!!</p>
<p>Thanks
zzz</p>
","<azure-data-factory>","2020-10-05 12:50:27","154","0","1","64235938","<p>As I know about DF, no, we can't.</p>
<p>The file name doesn't support MD5 expression or functions, it only support GUID(). Some Ideas are that you may try it with code level.</p>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">Expressions and functions in Azure Data Factory</a></p>
"
"64206081","Wildcard file paths with Azure Data Factory","<p>I have time series data generated in blob store organized with folders like 2020/10/05/23/file1.json
Can a single copy activity in Azure Data Factory Copy be configured to process all such files and push it to data warehouse ? It is not clear to me, how can I configure to detect unprocessed folders?</p>
<p>Any pointers shall be appreciated.</p>
","<azure><azure-data-factory><azure-synapse>","2020-10-05 09:38:36","4273","0","1","64230486","<p>You could use a copy activity with wildcards:
<a href=""https://i.stack.imgur.com/ADgVE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ADgVE.png"" alt=""Wildcard source"" /></a></p>
"
"64202886","How can setup routing for Power BI via firewall in Azure","<p>In this scenario requirement is connecting to azure synapse and data factory which in public end point.
Public end point should be accessed through firewall in virtual network</p>
","<azure><powerbi><azure-data-factory><azure-synapse>","2020-10-05 05:26:34","350","0","1","64203665","<p>You need to allow the relevant <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=41653"" rel=""nofollow noreferrer"">Microsoft Azure Datacenter IP's</a> to connect through to the VNET. The relevant ranges will depend on your PowerBI instance's home location. If you log in to PowerBI online, click the ? and select &quot;About PowerBI&quot; you will get the box below and you can tell from the &quot;Your data is stored in&quot; attribute.</p>
<p><a href=""https://i.stack.imgur.com/ZDfBh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZDfBh.png"" alt=""you need to add the correct ranges to your VNETs firewall."" /></a></p>
<p>It is updated weekly but changes affecting the PowerBI don't seem to occur often, in fact, in the last 6 months this has been stable for us. We struggled to find the right range, so we set up a simple PowerBI report that connects to an open Azure storage account with logging enabled and then refreshed the report from PowerBI service and looked in the storage account's firewall logs to see the IP the requests are originating from. This helped us to narrow down the IP's we needed to white list.</p>
"
"64193542","How to get number of miliseconds since epoch in ADF","<p>I am trying hard to get Unix Timestamp for current time in ADF. Basically I need number of number of milliseconds (ms) since epoch. I tried dabbling with built-in ticks function in ADF but it's not what I need. Also the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#ticks"" rel=""nofollow noreferrer"">documentation</a> on the ticks is not really clear. It says number of ticks since specified timestamp is what the function returns. 1 tick = 100 nanoseconds &amp; 1000000 nanoseconds = 1 ms. So considering this, I used the following expression in set variable activity:-</p>
<pre><code>@{ div(ticks('1970-01-01T00:00:00.0000000Z'),10000)   }
</code></pre>
<p>So the expectation is that whenever I run it, it should give me number of milliseconds since the epoch ( up to the moment of execution ) -- so by this definition every time I run it, it should return me a different value. But it returns a fixed value 62135568000000 every time I run it. So either the documentation is not correct or it's not really calculating what I really need.</p>
","<azure><azure-data-factory>","2020-10-04 10:31:45","1166","1","1","64237926","<p>Function ticks() return the number of ticks from '0001-01-01T00:00:00.0000000Z' to parameter of ticks(), not from '1970-01-01T00:00:00.0000000Z'.This is why you always get a fixed value 62135568000000.</p>
<p>I have tried <code>@{ticks('0001-01-01T00:00:00.0000000Z')}</code>, result is 0.</p>
<p><a href=""https://i.stack.imgur.com/SPAC1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SPAC1.png"" alt=""enter image description here"" /></a></p>
<p>So if you want to get ms from '1970-01-01T00:00:00.0000000Z' to current time, you can try this:<code>@{div(sub(ticks(utcnow()),ticks('1970-01-01T00:00:00.0000000Z')),10000)}</code>.</p>
"
"64184833","How to implement solution to extract files from share point to blob or adls gen2","<p>I have requirement to implement a solution for extracting files from share point to adls gen2 or blob storage.</p>
<p>I know we can do by using logic apps.
But looking for other some other possiblities.</p>
","<azure-logic-apps><azure-data-lake><azure-data-factory>","2020-10-03 13:32:11","111","0","1","64425059","<p>The docs have a small mention about how you can <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list#copy-file-from-sharepoint-online"" rel=""nofollow noreferrer"">use the Copy Activity with HTTP Connector for SharePoint Online</a> that you could explore.</p>
<p>If this doesn't work out for you, Logic Apps are indeed a great to implement this. You could also trigger the Logic App from your Data Factory pipeline when needed.</p>
<p>You could use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity"" rel=""nofollow noreferrer"">Webhook Activity</a> to trigger your Logic App.</p>
"
"64181501","Azure DataFactory Log Analytics Access","<p>We have a Log Analytics workspace '<code>XYZ</code>' in one subscription where all azure services logs are being sent to, from all other subscriptions as well.</p>
<p>We have our Data Factory Solution in another subscription where one user has Owner access. Its logs are also being stored in '<code>XYZ</code>'.</p>
<p>The challenge we are facing is, this user wants to access Data Factory logs but we can't give it as the Log Analytics workspace contains logs from other services as well such as backup.</p>
<p>Is there a way to grant this user access only on Data Factory logs?</p>
","<azure><azure-data-factory><azure-log-analytics>","2020-10-03 06:49:26","123","0","1","64588284","<p>You can grant users and groups only the amount of access they need to work with monitoring data in a workspace by using role-based access control(RBAC). More detail, you can refer to this <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/manage-access"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"64180296","How to get all files and folders recursively using GetMetadata activity","<p>I need to delete many folders with an exempt list which has a list of folders and files that I should not delete. So I tried <code>delete activity</code> and tried to use <code>if</code> function of <code>Add dynamic content</code> to check whether the file or folder name is the same of the specified ones. But I do not have what should be the parameters @if(). In other words, to use these functions, how do we get the file name or folder name?</p>
","<azure-data-factory>","2020-10-03 03:02:02","785","0","1","64310723","<p>It is difficult to get all files and folders by using GetMetadata activity.</p>
<p>As workarounds, you can try these ways:</p>
<p>1.create a Delete activity and select List of files option. Then create a file filled with those path of files and folders need to be deleted.(relative path to the path configured in the dataset).
<a href=""https://i.stack.imgur.com/uHlh8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uHlh8.png"" alt=""enter image description here"" /></a></p>
<p>2.using blob SDK to do this thing.</p>
"
"64170513","How do I authenticate with Azure in order to run a data factory pipeline test with python behave?","<p>I have a pipeline on Azure Data Factory that I want to write a test for using python behave. For now I just want to run a test locally. The following command won't run just now as I haven't authenticated in any way.</p>
<pre><code>get_client_from_cli_profile(DataFactoryManagementClient)
</code></pre>
<p>The error message says I need to run 'az login' to setup account.</p>
<pre><code>knack.util.CLIError: Please run 'az login' to setup account.
</code></pre>
<p>Could somebody give an example of how I do this?</p>
<p>Feature</p>
<pre><code>Feature: Run pipeline
    Scenario: Get pipeline
        Given we get the pipeline
</code></pre>
<p>Step</p>
<pre><code>@given('we get the pipeline')
def get_pipeline(context):
    pipeline_name = &quot;xxx&quot;
    resource_group = &quot;yyy&quot;
    data_factory = &quot;zzz&quot;
    parameters={}
    pipeline = get_datafactory_pipeline(pipeline_name, resource_group, data_factory, parameters)
</code></pre>
<p>Code to get pipeline</p>
<pre><code>from azure.common.client_factory import get_client_from_cli_profile
from azure.mgmt.datafactory import DataFactoryManagementClient

def get_datafactory_pipeline(pipeline_name, resource_group, data_factory, parameters):
    return get_client_from_cli_profile(DataFactoryManagementClient().pipelines.create_run(
        resource_group_name = resource_group,
        factory_name = data_factory,
        pipeline_name = pipeline_name,
        parameters = parameters)
</code></pre>
","<python><azure-data-factory><python-behave>","2020-10-02 11:09:48","282","0","1","64255111","<p>Two way:</p>
<p>1.install Azure CLI and then <code>az login</code> access to Azure.(download link:<a href=""https://learn.microsoft.com/en-us/cli/azure/install-azure-cli"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/cli/azure/install-azure-cli</a>)</p>
<p><a href=""https://i.stack.imgur.com/uZBX7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uZBX7.png"" alt=""enter image description here"" /></a></p>
<p>2.no need to install Azure CLI but change your code like this:</p>
<pre><code>def get_datafactory_pipeline(subscription_id,credentials,pipeline_name, resource_group, data_factory, parameters):
    return DataFactoryManagementClient(credentials,subscription_id).pipelines.create_run(
        resource_group_name=resource_group,
        factory_name=data_factory,
        pipeline_name=pipeline_name,
        parameters=parameters)
</code></pre>
<p>and your step like this:</p>
<pre><code>@given('we get the pipeline')
def get_pipeline(context):
    subscription_id = '&lt;Specify your Azure Subscription ID&gt;'
    credentials = ServicePrincipalCredentials(client_id='&lt;Active Directory application/client ID&gt;', secret='&lt;client secret&gt;', tenant='&lt;Active Directory tenant ID&gt;')
    pipeline_name = &quot;xxx&quot;
    resource_group = &quot;yyy&quot;
    data_factory = &quot;zzz&quot;
    parameters={}
    pipeline = get_datafactory_pipeline(subscription_id,credentials,pipeline_name, resource_group, data_factory, parameters)
</code></pre>
"
"64168780","Is there any Azure service that can simulate the concept of a global Azure 'variable' to hold a single value?","<p>I am looking for some Azure service that can store a value and then I can fetch it from any other Azure service. It's a storage basically but extremely lightweight storage -- it should allow one to define a variable for a given subscription and then its value can be updated from any other Azure service. In Azure Data Factory there is a recent introduction of global parameter at data factory level , even this could serve purpose to some limited extent if it was mutable, but it's a parameter not a variable. So its value can't be updated. Even if I can get some solution that will work within data factory that's fine too. One could always store such a value in SQL or blob but that sounds like an overkill. Having a global Azure variable is a genuine requirement -- so wondering if there is anything like that.</p>
","<azure><azure-data-factory>","2020-10-02 08:56:27","46","0","2","64168914","<p>you have several options:</p>
<ol>
<li>cosmosdb table api</li>
<li>redis</li>
<li>table storage</li>
</ol>
<p>ref: <a href=""https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview#keyvalue-stores"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview#keyvalue-stores</a></p>
"
"64168780","Is there any Azure service that can simulate the concept of a global Azure 'variable' to hold a single value?","<p>I am looking for some Azure service that can store a value and then I can fetch it from any other Azure service. It's a storage basically but extremely lightweight storage -- it should allow one to define a variable for a given subscription and then its value can be updated from any other Azure service. In Azure Data Factory there is a recent introduction of global parameter at data factory level , even this could serve purpose to some limited extent if it was mutable, but it's a parameter not a variable. So its value can't be updated. Even if I can get some solution that will work within data factory that's fine too. One could always store such a value in SQL or blob but that sounds like an overkill. Having a global Azure variable is a genuine requirement -- so wondering if there is anything like that.</p>
","<azure><azure-data-factory>","2020-10-02 08:56:27","46","0","2","64168939","<p>Please consider <a href=""https://learn.microsoft.com/en-us/azure/key-vault/general/basic-concepts"" rel=""nofollow noreferrer"">Azure KeyVault</a>. You can define there a secret to hold this value. However I'm not sure what integration with other Azure services you need.</p>
"
"64168201","How to save API output data to a Dataset in Azure Data Factory","<p>I'm currently working on a project in Azure Data Factory, which involves collecting data from a Dataset, using this data to make API calls, and thereafter taking the output of the calls, and posting them to another dataset.
In this way I wish to end up with a dataset containing various different data, that the API call returns to me.
My current difficulty with this is, that do not know how to make the &quot;Web activity&quot; (which I use to make the API Call) save its output to my dataset.
I have tried numerous different solutions found online, however none of them seem to work. I am not sure if the official documentation is outdated or if I'm misunderstanding parts of it. Below I've listed links to the solutions I've tried and failed:</p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">Copy data from a REST source</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">Copy data from an HTTP source</a></li>
<li>(among others, including similar posts to mine.)</li>
</ol>
<p>The current flow in my pipeline is, that a &quot;Lookup&quot; collects a list of variables named &quot;User_ID&quot;. These user ID's are put in to a ForEach loop, which makes an API call with the &quot;Web&quot; activity, using each of the USER_ID's. And this is where in the pipeline I wish to implement an activity or other, that can post each of these Web activity outputs into my new dataset.</p>
<p>I've tried to use the &quot;Copy data&quot; activity, but all it seems to do, is copying data straight from one dataset to another, and not being able to manipulate the data (which i wish to do with my api call).</p>
<p>Does anyone have a solution to how this is done?
Thanks a lot in advance.</p>
","<api><rest><post><dataset><azure-data-factory>","2020-10-02 08:09:47","992","0","1","64175016","<p>Not sure why you could not achieve this following <a href=""https://learn.microsoft.com/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">Copy data from a REST endpoint</a>. I tested the below which works fine. I used <a href=""https://learn.microsoft.com/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""nofollow noreferrer"">schema mapping</a> feature of 'Copy data' activity.</p>
<p>For example, I used a sample API <a href=""http://dummy.restapiexample.com/api/v1/employees"" rel=""nofollow noreferrer"">http://dummy.restapiexample.com/api/v1/employees</a> as source and for my testing, I used CosmosDB as sink. Of course you can choose any other dataset as per your requirement.</p>
<ol>
<li>Create 'Linked Service' for the REST API. For simplicity I do not have authentication for this API. Of course, you have that option if required.
<a href=""https://i.stack.imgur.com/rucQh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rucQh.png"" alt=""enter image description here"" /></a></li>
<li>Create 'Linked Service' for the target data store. In my case, it is CosmosDB.
<a href=""https://i.stack.imgur.com/FMExe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FMExe.png"" alt=""enter image description here"" /></a></li>
<li>Create Dataset for the REST API and link to the linked service created in #1.
<a href=""https://i.stack.imgur.com/LRpHt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LRpHt.png"" alt=""enter image description here"" /></a></li>
<li>Create Dataset for the Data store (in my case CosmosDB) and link to the linked service created in #2.
<a href=""https://i.stack.imgur.com/OwALb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OwALb.png"" alt=""enter image description here"" /></a></li>
<li>In the pipeline, add a 'Copy data' activity like below with source as the REST dataset created in #3 and sink as the dataset created in #4. Also, in my case I had to add schema mapping to select the employees array from the API output and map to each field in my datastore.
<a href=""https://i.stack.imgur.com/IQRwq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IQRwq.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/6hf9C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6hf9C.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/mq0t8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mq0t8.png"" alt=""enter image description here"" /></a></li>
<li>And voila, that's it. When I run the pipeline, it calls the REST API and saves the output in my DB with my desired mapping.
<a href=""https://i.stack.imgur.com/FtbuN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FtbuN.png"" alt=""enter image description here"" /></a></li>
</ol>
"
"64165536","Data Ingestion Patterns in Data Factory using REST API","<p>I am reaching out to you gather best practices around ingestion of data from various possible API's into a Blob Storage. I am considering to interface with all the various possible API's using Data Factory and here are the possible set of API's which I have currently:</p>
<ol>
<li><p>Ingesting from an API service within the same Resource Group in Azure Cloud</p>
</li>
<li><p>Ingesting from an API service from a different resource group but within the same Azure subscription</p>
</li>
<li><p>Ingestion from an API service from a different Azure Subscription / VNET</p>
</li>
<li><p>Ingestion from an API service available publicly such as Twitter, Facebook</p>
</li>
<li><p>Ingestion from an API service which is available on-premises</p>
</li>
<li><p>Any other possible API services</p>
</li>
</ol>
<p>My questions around the above API services are:</p>
<p>a) What are the specific security related settings I need to take care in order to interface with the above API's (Managed Identity, Service Principal etc.)</p>
<p>b) When to use which security setting ?</p>
<p>c) Along with Azure Data Factory, is there any other Azure service which can be leveraged for the above ingestion from the API's</p>
<p>d) What are the specifics of Runtimes / Linked services which I should be taking care about e) Any specifics around AAD resource and Authentication type</p>
","<azure><azure-data-factory><rest>","2020-10-02 02:36:13","298","1","2","64492255","<p>Just wanted to add that for #5 , you will have to use SHIR ( Self hosted IR ) . Please read about this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#prerequisites"" rel=""nofollow noreferrer"">here</a> .</p>
"
"64165536","Data Ingestion Patterns in Data Factory using REST API","<p>I am reaching out to you gather best practices around ingestion of data from various possible API's into a Blob Storage. I am considering to interface with all the various possible API's using Data Factory and here are the possible set of API's which I have currently:</p>
<ol>
<li><p>Ingesting from an API service within the same Resource Group in Azure Cloud</p>
</li>
<li><p>Ingesting from an API service from a different resource group but within the same Azure subscription</p>
</li>
<li><p>Ingestion from an API service from a different Azure Subscription / VNET</p>
</li>
<li><p>Ingestion from an API service available publicly such as Twitter, Facebook</p>
</li>
<li><p>Ingestion from an API service which is available on-premises</p>
</li>
<li><p>Any other possible API services</p>
</li>
</ol>
<p>My questions around the above API services are:</p>
<p>a) What are the specific security related settings I need to take care in order to interface with the above API's (Managed Identity, Service Principal etc.)</p>
<p>b) When to use which security setting ?</p>
<p>c) Along with Azure Data Factory, is there any other Azure service which can be leveraged for the above ingestion from the API's</p>
<p>d) What are the specifics of Runtimes / Linked services which I should be taking care about e) Any specifics around AAD resource and Authentication type</p>
","<azure><azure-data-factory><rest>","2020-10-02 02:36:13","298","1","2","69934837","<p>for c) Along with Azure Data Factory, is there any other Azure service which can be leveraged for the above ingestion from the API's</p>
<p>Logic Apps has connectors backed in for many existing API's</p>
"
"64155442","Azure DataFactory Can you directly rename a file?","<p>I want to rename a file, in place, as part of an ADF pipeline.
I can currently do that by copying it with a new name and deleting after the copy, but that's slow for large files.</p>
<p>Is there any way to do it faster?</p>
","<file><azure-data-factory>","2020-10-01 12:29:46","952","0","1","64164696","<p>Actually, no, there's isn't.</p>
<p>According you description, it's more related to the copy performance. If the copy active could be faster, then the file will be copied to the destination with new name faster too. You want the copy performance be faster, you could follow this document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">Copy activity performance and scalability guide</a>.</p>
<p>Data Factory doesn't support change the source file name directly. Not only the the file name, but also the file data schema.</p>
<p>I don't think it's a good choice to only change the file name with Data Factory, it more focus on data transferring.</p>
"
"64142654","Azure Data Factory grab file from folder based on size","<p>I ran a copy activity that used a http linked service to pull a zip file from an online and then extract the zip to a folder with multiple files within an Azure blob storage container. What I want to do now is dynamically pull the largest file from that newly created folder and run it through a data flow transformation while also deleting the folder through ADF. I am trying with a Get metadata activity that outputs the child items of the folder. The output is then connected to a ForEach activity with '@activity('Get Metadata1').output.childItems.' being passed in the Items of the ForEach setting with an inner GetMetadata activity to get the file sizes. But it errors on retrieving the file size giving me this..</p>
<p>{
&quot;errorCode&quot;: &quot;3500&quot;,
&quot;message&quot;: &quot;Field 'size' failed with error: 'Type=Microsoft.WindowsAzure.Storage.StorageException,Message=The remote server returned an error: (404) Not Found.,Source=Microsoft.WindowsAzure.Storage,''Type=System.Net.WebException,Message=The remote server returned an error: (404) Not Found.,Source=System,'.&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;Get Metadata2&quot;,
&quot;details&quot;: []
}</p>
<p>Is it not possible to get the file sizes of a folder's child items?. I was following this documentation.</p>
<p><a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/a83712ef-9a1a-4741-80b5-0e2ee8288ef5/get-child-items-size?forum=AzureDataFactory&amp;prof=required"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/azure/en-US/a83712ef-9a1a-4741-80b5-0e2ee8288ef5/get-child-items-size?forum=AzureDataFactory&amp;prof=required</a></p>
","<azure-data-factory><azure-logic-apps><azure-blob-storage>","2020-09-30 17:18:23","844","0","1","64153113","<ol>
<li>Create a data factory</li>
<li>Setup a scheduled trigger, or trigger it a different way if you know exactly when all the files are done extracting/loading.</li>
<li>Create a metadata activity that will return metadata on a specific folder.</li>
<li>Grab the largest file from blob based on the metadata.</li>
</ol>
"
"64140858","Azure Postgres and Data Factory - Firewall Rules","<p>I have an Azure Postgres database, and am looking to query it in an Azure Data Factory pipeline. When I go to add the database as a linked service, I see it fails as the IP address is not whitelisted. Looking at <a href=""https://learn.microsoft.com/en-us/azure/virtual-network/service-tags-overview#discover-service-tags-by-using-downloadable-json-files"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/virtual-network/service-tags-overview#discover-service-tags-by-using-downloadable-json-files</a> , I see that there's a whole lot of IPs in the region for Sql. I was wondering if there's some other way to allow Data Factory to read from the postgres database without enabling the button that allows all subscriptions (not even mine) network access to the database. Ideally, I'd like to say &quot;allow all connections from this azure subscription&quot;. Is this doable?</p>
","<azure-data-factory><azure-virtual-network><azure-postgresql>","2020-09-30 15:27:57","323","1","1","64148111","<p>You could try add the access role for the Data Factory:
<a href=""https://i.stack.imgur.com/Oe6om.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oe6om.png"" alt=""enter image description here"" /></a></p>
<p>Grant the Data Factory one of bellow roles:</p>
<ul>
<li><strong>Contributor</strong>: Grants full access to manage all resources, but does not
allow you to assign roles in Azure RBAC.</li>
<li><strong>Reader</strong>: View all resources, but does not allow you to make any
changes.</li>
</ul>
<p>usually, there are only the tree ways to control the access: 1. add IP roles, 2. Allow access Azure. 3.Access control(IAM). If the IAM doesn't work, we only can add the IP or allow access from Azure.</p>
<p>For all the Azure Database(as I know), the only thing we can set to achieve your request is &quot;Allow access to Azure Service&quot;:</p>
<p><a href=""https://i.stack.imgur.com/Bt0p4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bt0p4.png"" alt=""enter image description here"" /></a></p>
<p>HTH.</p>
"
"64137058","Unzip in Azure Data Factory","<p>I have a zip file with the size of 32GB. I am required to import this to a data lake storage service account. I am trying to unzip and move the file through Azure data factory.</p>
<p>Zip file is uploaded to the Azure Blob Storage. However I cannot see the ZIP extension as the Source data format.</p>
<p>Is it possible to do the unzip operation through the data factory<a href=""https://i.stack.imgur.com/maoAq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/maoAq.png"" alt=""AZ Data Factory"" /></a>?</p>
","<azure><zip><azure-data-lake><azure-data-factory>","2020-09-30 11:54:34","1980","4","1","64139234","<p>There is no Zip data set type, but you can Zip and Unzip using Binary data sets:</p>
<p><a href=""https://i.stack.imgur.com/zLs1E.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zLs1E.png"" alt=""enter image description here"" /></a></p>
<p>In the Copy activity, you can elect whether to retain the Zip file name as a folder:</p>
<p><a href=""https://i.stack.imgur.com/BOAEJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BOAEJ.png"" alt=""enter image description here"" /></a></p>
"
"64129194","Set row as a header Azure Data Factory [mapping data flow]","<p>Currently, I have an Excel file that I'm processing using a mapping data flow to remove some null values.</p>
<p>This is my input file:
<a href=""https://i.stack.imgur.com/UFt05.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UFt05.png"" alt=""enter image description here"" /></a></p>
<p>and after remove the null values I have:</p>
<p><a href=""https://i.stack.imgur.com/77fd2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/77fd2.png"" alt=""enter image description here"" /></a></p>
<p>I'm sinking my data into a Cosmos DB but I need to change the names of the columns and set my first row as headers...</p>
<p>I need to do this (First row as a header) in the previous step before Sink and y Can't use the mapping option a set manually the names of the columns because maybe some of these position of columns can change</p>
<p>Any idea to do this?</p>
<p>Thanks</p>
","<azure-data-factory>","2020-09-30 00:01:17","2486","0","1","64149685","<p>First row as a header can only check in dataset connection.</p>
<p>As a work around, you can save your excel to blob(csv format) after removing null value.</p>
<p>Then create a copy data activity or data flow, use this csv file as source(check first row as header), Cosmos DB as sink.</p>
<hr />
<p><strong>Update</strong></p>
<p>Setting of sink in data flow:</p>
<p><a href=""https://i.stack.imgur.com/XIGue.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XIGue.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/GcG4q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GcG4q.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/lCRIy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lCRIy.png"" alt=""enter image description here"" /></a></p>
<p>Data preview of sink:</p>
<p><a href=""https://i.stack.imgur.com/AGR2l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AGR2l.png"" alt=""enter image description here"" /></a></p>
<p>Result:</p>
<p><a href=""https://i.stack.imgur.com/Rsawh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rsawh.png"" alt=""enter image description here"" /></a></p>
"
"64124963","How to add a validation in azure data factory pipeline to check file size?","<p>I have multiple data sources I want to add a validation in azure data factory before loading into tables it should check for file size so that it is not empty. So if the file size is more than 10 kb or if it is not empty loading should start and if it is empty then loading should not start.
I checked validation activity in Azure Data Factory but it is not showing size for multiple files in a folder.
Any suggestions appreciated basically if I can add any python notebook for this validation will also do.</p>
","<azure><pyspark><azure-data-factory><azure-data-lake><azure-databricks>","2020-09-29 17:49:50","3254","1","2","64125762","<p>Use <code>GetMetadata</code> under General Activities, then send the result to an <code>If Condition</code>.
<a href=""https://i.stack.imgur.com/FkQST.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FkQST.png"" alt=""Get Metadata for File"" /></a></p>
<p>You will then need to get the file size from the Dataset.<code>@item().name</code> is the name of the file you want to get the size of.
<a href=""https://i.stack.imgur.com/rIRGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rIRGA.png"" alt=""Get File Size"" /></a></p>
<p>If you are working with a directory do the following:
<a href=""https://i.stack.imgur.com/6zPK7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6zPK7.png"" alt=""Get Metadata for content of folder"" /></a></p>
<p><a href=""https://i.stack.imgur.com/tLTzi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tLTzi.png"" alt=""Get child items"" /></a>
Then check the file size of each file.</p>
<p>This is what the ForEach settings looks like. Then you can use <code>@item().name</code> inside the ForEach to get at the file.</p>
<p><a href=""https://i.stack.imgur.com/2wxxA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2wxxA.png"" alt=""For Each Settings"" /></a></p>
<p>The data source will need to have the parameter FileName.
<a href=""https://i.stack.imgur.com/ZZgD5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZZgD5.png"" alt=""Parameters for Datasource"" /></a></p>
"
"64124963","How to add a validation in azure data factory pipeline to check file size?","<p>I have multiple data sources I want to add a validation in azure data factory before loading into tables it should check for file size so that it is not empty. So if the file size is more than 10 kb or if it is not empty loading should start and if it is empty then loading should not start.
I checked validation activity in Azure Data Factory but it is not showing size for multiple files in a folder.
Any suggestions appreciated basically if I can add any python notebook for this validation will also do.</p>
","<azure><pyspark><azure-data-factory><azure-data-lake><azure-databricks>","2020-09-29 17:49:50","3254","1","2","64264739","<p>Following GIF shows step by step process on how to achieve the above requirement in ADF.</p>
<p><a href=""https://i.stack.imgur.com/wdC2u.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wdC2u.gif"" alt=""enter image description here"" /></a></p>
"
"64123023","Azure Data Factory Mapping ADFv2","<p>95% of my Mapping in the Copy Activity of ADFv2 is straight one to one mapping. However, I have a case where I need to map Source &quot;MIDDLE_INIT&quot; Column to the Sink Column in which the String must be
SUBSTR(PS_NAMES.MIDDLE_NAME,1,1). Can this be done in ADFv2 Copy Activity? (The government cloud doesn't have Data Flows so I have to use the Copy Activity).
Thanks
Mike</p>
","<azure-data-factory>","2020-09-29 15:42:14","117","0","2","64129798","<p>No, copy activity cant do this.</p>
<p>As wBob comments, you could copy data from your source to a table type and transform <code>SUBSTR(PS_NAMES.MIDDLE_NAME,1,1)</code> in stored procedure. Then use this table as source, copy to your sink.</p>
"
"64123023","Azure Data Factory Mapping ADFv2","<p>95% of my Mapping in the Copy Activity of ADFv2 is straight one to one mapping. However, I have a case where I need to map Source &quot;MIDDLE_INIT&quot; Column to the Sink Column in which the String must be
SUBSTR(PS_NAMES.MIDDLE_NAME,1,1). Can this be done in ADFv2 Copy Activity? (The government cloud doesn't have Data Flows so I have to use the Copy Activity).
Thanks
Mike</p>
","<azure-data-factory>","2020-09-29 15:42:14","117","0","2","64186938","<p>If you have Azure SQL Database in your architecture, you can use it to shred JSON directly from your data lake, using its built-in abilities, namely <code>OPENROWSET</code> and <code>OPENJSON</code>.  Here is a simple example:</p>
<pre><code>SELECT *
FROM OPENROWSET (
    BULK 'raw/parliament/2020/09/25/members.json',
    DATA_SOURCE = 'somejsonstore',
    SINGLE_CLOB
) x
CROSS APPLY OPENJSON ( BulkColumn, '$.result.items' )
WITH (
    fullName            NVARCHAR(MAX)       '$.fullName._value',
    gender              NVARCHAR(100)       '$.gender._value',
    party               NVARCHAR(100)       '$.party._value'
)
</code></pre>
<p>Read more about OPENJSON <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/json/import-json-documents-into-sql-server?view=sql-server-ver15#import-json-documents-from-azure-blob-storage"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Here is another simple example shredding JSON which I imagine is similar to yours.  NB in this I create the JSON, but you'll need to import it using the above technique:</p>
<pre><code>SET @json = '{
&quot;PS_NAMES&quot;: {
    &quot;FIRST_NAME&quot;: &quot;w&quot;,
    &quot;MIDDLE_NAME&quot;: &quot;Susan&quot;,
    &quot;LAST_NAME&quot;: &quot;Bob&quot;
    }
}'

;WITH cte AS (
SELECT *
FROM OPENJSON ( @json, '$.PS_NAMES' )
WITH (
    FIRST_NAME          NVARCHAR(100)       '$.FIRST_NAME',
    MIDDLE_NAME         NVARCHAR(100)       '$.MIDDLE_NAME',
    LAST_NAME           NVARCHAR(100)       '$.LAST_NAME'
    )
)
SELECT 'original'AS [source], FIRST_NAME, MIDDLE_NAME, LAST_NAME
FROM cte
UNION ALL
SELECT 'new', FIRST_NAME, LEFT( MIDDLE_NAME, 1 ) AS MIDDLE_INIT, LAST_NAME
FROM cte;
</code></pre>
<p>If you do not have an Azure SQL DB in your architecture, then write back with what you do have, eg do you have Databricks, ADLA, Azure Synapse workspace, Azure Functions, Logic Apps, something else?</p>
"
"64122572","Cannot find the object % because it does not exist or you do not have permissions","<p>I am trying to write data to an Azure SQL DB with Azure Data Factory. I'm using a Copy Data Task within a For Each that looks through all the rows in an ETL table in the DB. In the pre-copy script, I have</p>
<pre><code>TRUNCATE TABLE [master].[dbo].[@{item().DestinationObjectName}]
</code></pre>
<p>DestinationObjectName being the name of the table that is being loaded in the ETL table. The problem I'm having is that for some of the tables (not all, some work perfectly fine) I am getting the error 'Cannot find the object % because it does not exist or you do not have permissions'. The account I'm using has all the necessary privileges as well. I am able to see the script that is sent to ADF which I have copied into the DB and confirmed this script works sometimes but not every time. If I select top 1000 from the table in question and replace that object for the one in the truncate table script, it works. I'm really at a loss here. Like I said the truncate works for a majority of the tables but not all. I have also double checked that the object names are the exact same.</p>
<p>Any help is appreciated.</p>
","<sql><azure-sql-database><ssms><azure-data-factory><dynamics-365>","2020-09-29 15:16:12","3489","0","1","64155833","<p>This issue has been solved. I had to drop the affected tables and remove the brackets surrounding each in the create table statements and recreate without the brackets. very strange issue.</p>
"
"64121793","send the dta afrom azure blob container to azure function using azure datafactory activity","<p>I have copied data to azure blob container using the copy activity.i was able to use that to trigger my azure function using Blob trigger.However my req is to call the azure function activity that can be configured in azure datafactory pipeline.to that i need to pass the blob container path so that the azure function based on the HTTP trigger can read from this path.Blob trigger works but isnt allowed.Any idea as to how to get the path of the container and pass it to the azure function activity?</p>
<p>Edit:-
i added this
<a href=""https://i.stack.imgur.com/w7O8K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w7O8K.png"" alt=""enter image description here"" /></a></p>
<p>And the output of the path in the request sent to the HTTPTrigger of azure func looks like this</p>
<p><a href=""https://i.stack.imgur.com/IGbyl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IGbyl.png"" alt=""enter image description here"" /></a></p>
<p>This is where i need the fully formed path post the copy say <strong>folder/myfolder/2010/10/01</strong>
However i dont.</p>
<hr />
<p>-----------------------UPDATE----------------------------------------------------
this is the sink dataset
<a href=""https://i.stack.imgur.com/iTiul.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iTiul.png"" alt=""enter image description here"" /></a></p>
<p>with the connection of the dataset(sink)like this
<a href=""https://i.stack.imgur.com/j9oLn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j9oLn.png"" alt=""enter image description here"" /></a></p>
<p>and my copypipeline looks like this
<a href=""https://i.stack.imgur.com/tyHBG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tyHBG.png"" alt=""enter image description here"" /></a></p>
<p>ran the debug and the copy instead of folder/myfolder/2020/10/01 gives folder/myfolder/@variables('data')</p>
<p><a href=""https://i.stack.imgur.com/hkElD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hkElD.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-functions><azure-data-factory>","2020-09-29 14:32:35","103","1","1","64130283","<p>According to the description of your question, it seems you do not know the target blob path of the &quot;Copy&quot; activity. I guess you use pipeline parameter to input the blob path in your data factory. Something like below:</p>
<p><a href=""https://i.stack.imgur.com/Askcj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Askcj.png"" alt=""enter image description here"" /></a></p>
<p>So in the HTTP trigger function request body, you just need to choose the <code>testPath</code>.</p>
<p><a href=""https://i.stack.imgur.com/NuioK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NuioK.png"" alt=""enter image description here"" /></a></p>
<p>If your function request body need to be like <code>{&quot;path&quot;:&quot;xxx&quot;}</code>, you can use &quot;<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#concat"" rel=""nofollow noreferrer"">concat()</a>&quot; function in data factory to join the string together.</p>
<p>==================================<strong>Update</strong>=================================</p>
<p><a href=""https://i.stack.imgur.com/KA6yY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KA6yY.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/GMYX3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMYX3.png"" alt=""enter image description here"" /></a></p>
"
"64121597","Data Flow output to Azure SQL Database contains only NULL data on Azure Data Factory","<p>I'm testing the data flow on my Azure Data Factory. I created Data Flow with the following details:
Source dataset linked service - from CSV files dataset from Blob storage
Sink linked service - Azure SQL database with pre-created table
My CSV files are quite simple as they contain only 2 columns (PARENT, CHILD). So, my table in SQL DB also have only 2 columns.</p>
<p><a href=""https://i.stack.imgur.com/D6yBR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D6yBR.png"" alt=""CSVdataset"" /></a></p>
<p>For the sink setting of my data flow, I have allowed insert data and leaving other options as default.</p>
<p><a href=""https://i.stack.imgur.com/xYAoh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xYAoh.png"" alt=""SinkAllowInsert"" /></a></p>
<p>I have also mapped the 2 columns for input and output columns as per screenshot.</p>
<p><a href=""https://i.stack.imgur.com/LIioF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LIioF.png"" alt=""mapColumns"" /></a></p>
<p><a href=""https://i.stack.imgur.com/IAVKE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IAVKE.png"" alt=""mapColumns2"" /></a></p>
<p>The pipeline with data flow ran successfully when I checked the result, I could see thqat 5732 rows were processed. Is this the correct way to check? As this is the first time I try this functionality in Azure Data Factory.</p>
<p><a href=""https://i.stack.imgur.com/AuMcg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AuMcg.png"" alt=""rowProceesed"" /></a></p>
<p>But, when I click on <strong>Data preview</strong> tab, they are all NULL value.</p>
<p><a href=""https://i.stack.imgur.com/dBUrj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dBUrj.png"" alt=""allNULLvalues"" /></a></p>
<p>And; when I checked my Azure SQL DB in the table where I tried to insert the data from CSV files from Blob storage with selecting top 1000 rows from this table, I don't see any data.</p>
<p><a href=""https://i.stack.imgur.com/jpPKI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jpPKI.png"" alt=""SQLnoData"" /></a></p>
<p>Could you please let me know what I configured incorrectly on my Data Flow? Thank you very much in advance.</p>
<p>Here is the screenshot of ADF data flow source data, it does see the data on the right side as they are not NULL, but on the left side are all NULLs. I imagine that the right side are the data from the CSV from the source on the blob right? And the left side is the sink destination as the table is empty for now?</p>
<p><a href=""https://i.stack.imgur.com/HQNet.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HQNet.png"" alt=""ADFsourceData"" /></a></p>
<p>And here is the screenshot for the sink inspect input, I think this is correct as it reads the 2 columns correctly (Parent, Child), is it?</p>
<p><a href=""https://i.stack.imgur.com/2nQfw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2nQfw.png"" alt=""ADFsinkDataInspect"" /></a></p>
<p>After adding Map drifted, to map &quot;Parent&quot; =&gt; &quot;parent&quot; and &quot;Child&quot; =&gt; &quot;child&quot;</p>
<p><a href=""https://i.stack.imgur.com/2LaLz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2LaLz.png"" alt=""mapDrift0"" /></a></p>
<p>I get this error message after running the pipeline.</p>
<p><a href=""https://i.stack.imgur.com/zvtTi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zvtTi.png"" alt=""mapdrift1"" /></a></p>
<p>When checking on sink data preview, I get this error message. It seems like there is incorrect mapping?</p>
<p><a href=""https://i.stack.imgur.com/Rh9x7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rh9x7.png"" alt=""mapdrifted1"" /></a></p>
<p>I rename the MapDrifted1 expression to &quot;toString(byName('Parent1))&quot; and Child1 as suggested.</p>
<p><a href=""https://i.stack.imgur.com/jH5in.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jH5in.png"" alt=""MapDrifted1"" /></a></p>
<p>The data flow executed successfully, however I still get NULL data in the sink SQL table.</p>
<p><a href=""https://i.stack.imgur.com/HkIhs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HkIhs.png"" alt=""NULLdata1"" /></a></p>
<p><a href=""https://i.stack.imgur.com/N6zTf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N6zTf.png"" alt=""NULLdata2"" /></a></p>
","<sql><azure-sql-database><azure-data-factory><dataflow>","2020-09-29 14:23:10","1022","1","1","64123960","<p>Can you copy/paste the script behind your data flow design graph? Go to the ADF UI, open the data flow, then click the Script button on top right.</p>
<p>In your Source transformation, click on Data Preview to see the data. Make sure you are seeing your data, not NULLs. Also, look at the Inspect on the <em>INPUT</em> for your Sink, to see if ADF is reading additional columns.</p>
"
"64121275","Azure Data Factory - SQL","<p>I would like to ask you if is it possible to use &quot;output inserted&quot; SQL statement in data flow (Azure Data Factory)</p>
<p>Currently, I am getting an error:</p>
<blockquote>
<p>at Source 'source1': shaded.msdataflow.com.microsoft.sqlserver.jdbc.SQLServerException: A nested INSERT, UPDATE, DELETE, or MERGE statement is not allowed in a SELECT statement that is not the immediate source of rows for an INSERT statement.</p>
</blockquote>
<p>Using this statement:
<a href=""https://i.stack.imgur.com/VprgL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VprgL.jpg"" alt=""enter image description here"" /></a></p>
<p>What I did:</p>
<p>I also checked stored procedures but they do not provide any output.</p>
<p>What I want:</p>
<p>I would like to get the 'Id' of row of inserted elements (Insert+Select in one operation).</p>
","<azure><azure-data-factory>","2020-09-29 14:03:43","441","1","1","64132012","<p>As error shows, insert statement isn't allowed.</p>
<p>As a work around, you can do like this.</p>
<ol>
<li>create a Lookup activity and enter your SQL like this:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/8qoY3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8qoY3.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>create a data flow and create a parameter inside it.
expression:<code>@activity('Lookup1').output.firstRow.ID</code></li>
</ol>
<p><a href=""https://i.stack.imgur.com/30Ot8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/30Ot8.png"" alt=""enter image description here"" /></a></p>
<p>3.Inside data flow, choose your table as your source and enter this expression:</p>
<pre><code>concat('select ID from dbo.test6 where ID =',toString($ID))
</code></pre>
<p>4.If your need to do some transform, you can create a <code>DerivedColumn</code> and do like this:</p>
<p><a href=""https://i.stack.imgur.com/e7Lcq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e7Lcq.png"" alt=""enter image description here"" /></a></p>
"
"64118489","ADF data flow not giving cosmos query results with the parameters","<p><strong>Disclaimer : I am very new to Azure Development</strong></p>
<p>In Azure Data factory Dataflow in source option when I have hardcoded the date string and  used below query it gives the results as expected for cosmos DB.</p>
<pre><code>“select c.column1,c.column2 from c where c.time_stamp &gt;= '2010-01-01T20:28:45Z' and c.time_stamp &lt;= '2020-09-11T20:28:45Z'”
</code></pre>
<p>When I have passed the parameters which I have mapped in pipeline and use the query with parameters I am not getting any result.</p>
<pre><code>&quot;oldwatermark&quot;: &quot;'2010-01-01T20:28:45Z'&quot;,
&quot;newwatermark&quot;: &quot;'2020-09-11T20:28:45Z'&quot;


“select c.column1,c.column2 from c where c.time_stamp &gt;= ‘$oldwatermark’ and c.time_stamp &lt;= ‘$oldwatermark’”
</code></pre>
<p>Could you please suggest what am I doing wrong here as my parameter values and hardcoded values are same.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-09-29 11:16:32","375","0","1","64129981","<p>Just from your worked statements, your query should be:</p>
<pre><code>select c.column1,c.column2 from c where c.time_stamp &gt;= $oldwatermark and c.time_stamp &lt;= $newwatermark
</code></pre>
<p>not <code>where c.time_stamp &gt;= $oldwatermark and c.time_stamp &lt;= $oldwatermark</code>.</p>
<p><strong>Please don't use the quotes for the parameter in the query</strong>.</p>
<p>Please try this query:</p>
<pre><code>concat('select c.column1,c.column2 from c where c.time_stamp &gt;= ',$oldwatermark,'and c.time_stamp &lt;= ',$newwatermark)
</code></pre>
<p><a href=""https://i.stack.imgur.com/gPeGC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gPeGC.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/AJD8T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AJD8T.png"" alt=""enter image description here"" /></a></p>
"
"64117923","How to connect Service bus queue to Azure Data Factory as source?","<p>I have a service bus configured where i get messages in my subscription, I would like to move the data from the messages in the service bus queue to table storage using Azure Data factory, I would like to know if it is possible to do it.i couldn't find any online resource which talks about service bus as a source so would like to know if anyone has any experience here.</p>
<p>Thanks</p>
","<azure><azureservicebus><azure-data-factory>","2020-09-29 10:39:20","1767","1","1","64135465","<p>This is the answer I got from Microsoft, so posting it here for community&gt;</p>
<blockquote>
<p>Data Factory does not have a connector for Service bus. However there
are several options available to you.</p>
<p>You can create a consumer for Data Factory to call upon.</p>
<p>You can raise a feature request in the feedback forum.</p>
<p>You could re-route your messages to be written to blob, and then
leverage the Blob Event Trigger.</p>
<p>Use ADF Web Activity to retrieve a message.</p>
<p>By &quot;create a consumer for Data Factory to call upon,&quot; I mean either
create a Function App which batch-reads the messages, and returns
them, utilizing ADF Azure Function, or , create some code to do the
same with the ADF Batch Service Custom Activity. There are more
variations as well.</p>
<p>Which one to use, depends upon your volume and cadence (frequency).</p>
</blockquote>
"
"64109538","CosmosDB sink in data flow does not conform to target dataset schema","<p>I have defined 1 pipeline with 1 data flow.The data flow does the following:</p>
<ol>
<li>Reads a document from Blob source with av JSON schema</li>
<li>Lookup over a second source from CosmosDB with the same schema. Simple lookup for equality on 1 field.</li>
<li>Merge an array property that comes from the database with the one that comes from the blob</li>
<li>Upsert the resulting document back to the same CosmosDB collection through a corresponding sink.</li>
</ol>
<p>Even though the document schemas are the same for the different sources and the sink, i have defined 2 different schemas - 1 for the blob and 1 for the CosmosDB, used by both source and sink.</p>
<p>The JSON doc schema itself is nothing complex - few properties under the root and an array property of flat document with few properties, that is getting merged. Some of these properties are ints or doubles, rest are strings. Documents are getting correctly processed with the content in the array correctly merged, and then either udpated or inserted in the CosmosDB collection.</p>
<p>However none of the int fields are written as such - they are all converted to strings. Doubles seem to be handled correctly. The schema is correct throughout the data flow. Tried even with adding explicit transformation in order to set the type to int prior to the sink, yet still the same outcome.</p>
<p>I looked then a bit under the hood and found out that the script that is created behind the scenes contains a sink definition with the wrong field types - instead of ints, the fields are all strings. Then i decided to outsmart the ADF and edited the script manually. After running a Publish though i was proven that ADF is smarter than me. In the publish branch the script was magically reverted back to its original state - strings instead of int for the fields in the sink. While at the same time the Dev branch clearly contains the correctly defined types (though manually).Very annoying indeed!</p>
<p>ADF has taken a long way since v1 and resembles a lot the dev experience for SSIS (and even better), yet lack of control of data types of the fields/columns, at least at the source/sink points seems somewhat childish. And in addition this magic transformation of the types from int to strings during publish (!?!) adds 2 more points in direction south for the time being :(</p>
<p>Any idea if this is a known issue, and moreover, if there is a known workaround will be highly appreciated!</p>
","<azure-data-factory>","2020-09-28 20:58:07","130","0","1","64116857","<p>You can create a <code>DerivedColumn</code> after step 3, and use this expression <code>toInteger(your column)</code>.</p>
<p>I think this is a similar <a href=""https://stackoverflow.com/questions/63930814/migrate-json-data-from-azure-sql-db-to-cosmos-db-results-in-string-values/63985596#63985596"">question</a>, you can refer to it.</p>
"
"64105436","Linked Service with self-hosted integration runtime is not supported in data flow in Azure Data Factory","<p>Step to reproduce:</p>
<ol>
<li><p>I created a Copy Data first in the pipeline to simple transfer CSV files frol Azure VM to Azure Blob storage. I always use IRPOC1 as a connection via integration runtime and connect using SAS URI and SAS Token to my Blob Storage</p>
</li>
<li><p>After validate and run my first Copy Data, I successfully have CSV file transfer from my VM to Blob storage</p>
</li>
<li><p>I tried to add a new Data Flow after the Copy Data activity</p>
</li>
<li><p>In my Data Flow, my source is the Blob storage containing the CSV files transferred from VM, my Sink is my Azure SQL Database with successful connection</p>
</li>
<li><p>However, when I ran validation, I got the error message on my Data Flow Source:</p>
<p>Linked Service with self-hosted integration runtime is not supported in data flow.</p>
</li>
</ol>
<p>I saw someone replied on Microsoft Azure Document issue Github that I need to use Copy Data to transfer data to Blob first. Then use the source from this blob with data. This is what I did but I still have the same error. Could you please let me know how I can fix this?</p>
","<azure-data-factory>","2020-09-28 15:47:00","12893","4","1","64108509","<p>The Data Flow source dataset must use a Linked Service that uses an Azure IR, not a self-hosted IR.</p>
<p>Go to the dataset in your data flow Source, click &quot;Open&quot;. In the dataset page, click &quot;Edit&quot; next to Linked Service.</p>
<p>In the Linked Service dialog, make sure you are using an Azure Integration Runtime, not a Self-hosted IR.</p>
<p><a href=""https://i.stack.imgur.com/F1Pmd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/F1Pmd.png"" alt=""enter image description here"" /></a></p>
"
"64097416","Can I create a Custom Error Statement instead of auto generate error statement in Azure data factory","<p><a href=""https://i.stack.imgur.com/Z41PF.png"" rel=""nofollow noreferrer"">SYSTEM GENERATED ERROR</a></p>
<p>in the image, you can see there is a system generated error after failing a pipeline. But, I want a custom error( altered/ edited message by a user for any failure )that will pop up in place of system generated error after running a pipeline.</p>
","<azure><error-handling><azure-data-factory>","2020-09-28 07:07:05","35","0","1","64112037","<p>No, it is not possible. Error message cannot be changed.</p>
"
"64095960","Passing dynamic content inside a SQL Query in lookup stage of Azure Data Factory","<p>I'm using a lookup stage as a source to fetch some data and i want to pass that output as the input to the next lookup stage. I tried adding @activity('Step1').output.firstRow.Col and it failed with scalar variable was not being declared. Also tried with @{activity('Step1').output.firstRow.col} which failed and the log shows only default expressions supported. Please help if it is possible.</p>
<p>I have accomplished this using dataflow, but considering the performance i would like to know if it can be done in a pipeline.</p>
","<azure><azure-data-factory>","2020-09-28 04:27:14","4039","0","1","64098057","<p>Please try this：</p>
<p>Query:<code>select * from dbo.test5 where col = '@{activity('Lookup1').output.firstRow.col}'</code></p>
<p><a href=""https://i.stack.imgur.com/HiR7F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HiR7F.png"" alt=""enter image description here"" /></a></p>
<p>Output:
<a href=""https://i.stack.imgur.com/C7MF7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C7MF7.png"" alt=""enter image description here"" /></a></p>
"
"64092349","Export data from Azure Timeseries insights using Azure REST API's or Azure Data Factory","<p>Req: This actually for export last 6 months data from Timeseries insights to CSV file to download to User from web application.
The historical data is available on Blob &amp; Timeseries insights.</p>
<p>Approach:
Create azure data factory pipeline which pulls the data from azure time series insights based on aggregate/filter query and convert it into CSV to store in Blob storage.</p>
<p>Is it possible to do that or any other approach can be considered for this requirement.</p>
<p>Please suggest.</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2020-09-27 19:13:31","271","1","1","64094884","<p>Data Factory doesn't support Azure Time Series Insights as the dataset(Source or Sink).</p>
<p>Ref the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">connector overview</a>.</p>
<p>That means we can't create azure data factory pipeline which pulls the data from azure time series insights based on aggregate/filter query and convert it into CSV to store in Blob storage.</p>
<p>Some other way, you may could using Azure Blob storage client library to create and upload the file to Blob Storage.</p>
<p>Here's the tutorials may be helpful:</p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/time-series-insights/time-series-insights-update-query-data-csharp"" rel=""nofollow noreferrer"">Query data from the Azure Time Series Insights Gen2 environment
using C Sharp</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-dotnet"" rel=""nofollow noreferrer"">Quickstart: Azure Blob storage client library v12 for .NET</a></li>
</ol>
"
"64081770","Azure Data Factory dataflow ""No space left on device""","<p>I am running a pipeline in Azure Data Factory with a dataflow with some sources and transformations, but the pipeline runs for around 40 hrs and then fail with the error:</p>
<p><a href=""https://i.stack.imgur.com/xOJC4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xOJC4.png"" alt=""enter image description here"" /></a></p>
<p>There is a way that I can debug the reason of this error? or what it means with &quot;No space left on device&quot;?</p>
<p>The data used are located in a datalake and then trying to sink into a Azure Synapse DB.</p>
<p>Thanks</p>
","<azure-pipelines><azure-data-factory>","2020-09-26 19:34:09","262","-1","1","64124900","<p>This is a Spark error coming from the working happening behind the scenes. As this is an uncaught exception, I would recommend opening an Azure support ticket so that our engineers can get to the root cause for you.</p>
<p>The root cause here is likely the worker filling up due to job taking so long. To avoid this, I would recommend implementing <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance"" rel=""nofollow noreferrer"">performance best practices</a> to reduce the length of the job as 40 hrs is far too long.</p>
<p>Thanks,
Daniel</p>
"
"64080460","How to create a dataset of type Azure Data Lake storage Gen 2 in data factory?","<p>I want to create a dataset of type Azure Data Lake storage Gen 2 in data factory. I followed the steps:</p>
<ol>
<li><p>Click on &quot;New Dataset&quot;</p>
</li>
<li><p>In &quot;Select data store&quot;, I selected &quot;Azure data lake storage gen 2&quot; and hit &quot;continue&quot;</p>
</li>
<li><p>In &quot;choose format type of your data&quot;, I do not want to select any particular format but this is a mandatory step.</p>
</li>
</ol>
<p>How do I create the required dataset without having to select a particular format in Step 3?</p>
","<azure-data-factory><azure-data-lake><azure-data-lake-gen2>","2020-09-26 17:09:27","106","0","1","64094783","<p>Short answer, no, you must select a particular format in Step 3.</p>
<p>About file formats in Data Lake Gen 2, you can refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#dataset-properties"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"64070667","How to pass optional column in TABLE VALUE TYPE in SQL from ADF","<p>I have the following table value type in SQL which is used in Azure Data Factory to import data from a flat file in a bulk copy activity via a stored procedure.  File 1 has all three columns in it so this works fine.  File 2 only has <code>Column1</code> and <code>Column2</code>, but <strong>NOT</strong> <code>Column3</code>.  I figured since the column was defined as NULL it would be ok but ADF complains that its attempting to pass in 2 columns when the table type expects 3.  Is there a way to reuse this type for both files and make Column3 optional?</p>
<pre><code> CREATE TYPE [dbo].[TestType] AS TABLE(
    Column1 varchar(50) NULL,
    Column2 varchar(50) NULL,
    Column3 varchar(50) NULL
)
</code></pre>
<blockquote>
<p>Operation on target LandSource failed:
ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
database operation failed with the following error: 'Trying to pass a
table-valued parameter with 2 column(s) where the corresponding
user-defined table type requires 3 column(s)</p>
</blockquote>
<p>Would be nice if the copy activity behavior was consistent regardless of whether or not a stored procedure with table type is used or native BCP in the activity.  When not using the table type and using the default bulk insert, missing columns in the source file end up being NULL in the target table without error (assumming the column is NULLABLE).</p>
","<sql-server><azure-data-factory><table-valued-parameters>","2020-09-25 20:03:15","1196","1","2","64097749","<p>It will cause the mapping error in ADF.</p>
<ol>
<li>In the Copy Activity, every column needs to be mapped.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/W8kGq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W8kGq.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>If the source file only has two columns, it will cause mapping error.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/2QLor.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2QLor.png"" alt=""enter image description here"" /></a></p>
<p>So, I suggest you to create two different Copy activities and create a two columns table type.</p>
"
"64070667","How to pass optional column in TABLE VALUE TYPE in SQL from ADF","<p>I have the following table value type in SQL which is used in Azure Data Factory to import data from a flat file in a bulk copy activity via a stored procedure.  File 1 has all three columns in it so this works fine.  File 2 only has <code>Column1</code> and <code>Column2</code>, but <strong>NOT</strong> <code>Column3</code>.  I figured since the column was defined as NULL it would be ok but ADF complains that its attempting to pass in 2 columns when the table type expects 3.  Is there a way to reuse this type for both files and make Column3 optional?</p>
<pre><code> CREATE TYPE [dbo].[TestType] AS TABLE(
    Column1 varchar(50) NULL,
    Column2 varchar(50) NULL,
    Column3 varchar(50) NULL
)
</code></pre>
<blockquote>
<p>Operation on target LandSource failed:
ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
database operation failed with the following error: 'Trying to pass a
table-valued parameter with 2 column(s) where the corresponding
user-defined table type requires 3 column(s)</p>
</blockquote>
<p>Would be nice if the copy activity behavior was consistent regardless of whether or not a stored procedure with table type is used or native BCP in the activity.  When not using the table type and using the default bulk insert, missing columns in the source file end up being NULL in the target table without error (assumming the column is NULLABLE).</p>
","<sql-server><azure-data-factory><table-valued-parameters>","2020-09-25 20:03:15","1196","1","2","64130254","<p>You can pass optional column, I've made a test successfully, but the steps will be a bit complex. In my case, File 1 has all three columns, File 2 only has Column1 and Column2,  but NOT Column3. It will use Get Metadata activity, Set Variable activity, ForEach activity, IfCondition activity.</p>
<p>Please follow my steps:</p>
<ol>
<li><p>You need to define a variable FileName to foreach.
<a href=""https://i.stack.imgur.com/dytLD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dytLD.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the Get Metadata1 activity, I specified the file path.
<a href=""https://i.stack.imgur.com/pF2DW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pF2DW.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the ForEach1 activity, use <code>@activity('Get Metadata1').output.childItems</code> to foreach the filelist. It need to be <strong>Sequential</strong>.
<a href=""https://i.stack.imgur.com/KXFO5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KXFO5.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Inside the ForEach1 activity, use Set Variable1 to set the <strong>FileName</strong> variable.
<a href=""https://i.stack.imgur.com/15zhd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/15zhd.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the Get Metadata2, use <code>item().name</code> to specify the file.
<a href=""https://i.stack.imgur.com/Ke5ri.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ke5ri.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the Get Metadata2, use <strong>Column count</strong> to get the column count from the file.
<a href=""https://i.stack.imgur.com/RTxLu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RTxLu.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the If Contdition1, use <code>@greater(activity('Get Metadata2').output.columnCount,2)</code> to determine whether the file is larger than two columns.
<a href=""https://i.stack.imgur.com/FUcwD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUcwD.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the <strong>True</strong> activity, use variable <strong>FileName</strong> to specify the file.
<a href=""https://i.stack.imgur.com/yR8Pc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yR8Pc.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the <strong>False</strong> activity, use <strong>Additional columns</strong> to add a Column.
<a href=""https://i.stack.imgur.com/Pepaf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pepaf.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>When I run debug, the result shows:<br />
<a href=""https://i.stack.imgur.com/NQizC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NQizC.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
"
"64065961","Is there a way to programatically generate the adf_publish content in Azure Data Factory?","<p>I am new to Azure Data Factory and reading thorugh the docs I found that to generate an artifact to deploy to other DF envs, you need to publish in the dev DF, which generates an adf_publish branch with the jsons to deploy. My question is whether I can run this publish programatically and thus generate the jsons from any branch?</p>
","<azure-data-factory>","2020-09-25 14:21:03","144","0","2","64068480","<p>Not sure about programmatically publishing to adf_publish.</p>
<p>But, it's very possible to skip the adf_publish branch entirely and deploy using <a href=""https://www.youtube.com/watch?v=2vcXIiSD8-Y"" rel=""nofollow noreferrer"">Azure DevOps</a> or <a href=""https://www.youtube.com/watch?v=BhQE_D1m76g"" rel=""nofollow noreferrer"">PowerShell</a> straight from the source json instead.</p>
"
"64065961","Is there a way to programatically generate the adf_publish content in Azure Data Factory?","<p>I am new to Azure Data Factory and reading thorugh the docs I found that to generate an artifact to deploy to other DF envs, you need to publish in the dev DF, which generates an adf_publish branch with the jsons to deploy. My question is whether I can run this publish programatically and thus generate the jsons from any branch?</p>
","<azure-data-factory>","2020-09-25 14:21:03","144","0","2","64080120","<p>Currently the only way to update the 'adf_publish' branch is by manually clicking the publish button in the UX.</p>
<p>The product group is currently designing a solution to be able to do this programmatically via a DevOps build task. No exact ETA unfortunately.</p>
<p>Thanks,
Daniel</p>
"
"64059189","How can I copy a data of one folder from Blob Storage to another Blob Storage in python code?","<p>I want to copy data of one folder from a particular blob storage to another blob storage.</p>
<p>I want to write a python code in Azure Function to copy the data of a particular folder, also I want to write a python code to convert hot tier file into archive tier blob storage.</p>
<p>Please suggest me the code to do this.</p>
","<python><powershell><azure-functions><azure-data-factory><azure-function-app>","2020-09-25 06:48:18","918","0","1","64059386","<p>Use this to copy the blob:</p>
<p><a href=""https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#start-copy-from-url-source-url--metadata-none--incremental-copy-false----kwargs-"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#start-copy-from-url-source-url--metadata-none--incremental-copy-false----kwargs-</a></p>
<p>Use these to set the tier:</p>
<p><a href=""https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#set-premium-page-blob-tier-premium-page-blob-tier----kwargs-"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#set-premium-page-blob-tier-premium-page-blob-tier----kwargs-</a></p>
<p><a href=""https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#set-standard-blob-tier-standard-blob-tier----kwargs-"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#set-standard-blob-tier-standard-blob-tier----kwargs-</a></p>
"
"64056942","Loading Excel sheet in Azure Data Factory","<p>Timeout error while loading excel sheet name in Azure Data Factory. Size of the file is 51 MB. Do we have any size restriction for Excel? No Articles on this on MSDN</p>
<p><a href=""https://i.stack.imgur.com/t2cRN.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/t2cRN.jpg</a></p>
","<azure><azure-data-factory>","2020-09-25 02:29:48","761","0","2","64068301","<p>I've loaded 10+ GB CSV files from Azure Blob Storage without ADF flinching in the slightest.</p>
<p>Not sure where your file is hosted, but try staging it in Azure Blob Storage first since the proximity to ADF will optimize retrieval.  Beyond that, try referencing your source file as a CSV instead, if possible.  To get a specific sheet out, you may have to write a simple little Azure Function to programmatically pull that sheet out of the XLSX and stage it as a CSV as a workaround.</p>
"
"64056942","Loading Excel sheet in Azure Data Factory","<p>Timeout error while loading excel sheet name in Azure Data Factory. Size of the file is 51 MB. Do we have any size restriction for Excel? No Articles on this on MSDN</p>
<p><a href=""https://i.stack.imgur.com/t2cRN.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/t2cRN.jpg</a></p>
","<azure><azure-data-factory>","2020-09-25 02:29:48","761","0","2","73708822","<p>Try sample data with a much smaller size. ADF has limitations to show sheet name if the file is &quot;too big&quot;. You will be able to do it with smaller size.</p>
"
"64049193","While copying data from API using Azure Data Factory to Azure SQL, I'm only getting first row in table. The API response is Nested JSON","<p>While copying data from API using Azure Data Factory to Azure SQL, I'm only getting first row in table. The API response is Nested JSON. I've mapped source and sink as well.
<a href=""https://i.stack.imgur.com/B5u9i.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<json><azure><azure-sql-database><azure-data-factory>","2020-09-24 15:01:59","573","2","1","64063139","<p>Issue resolved by enabling the Collection reference in mapping option in copy activity.</p>
"
"64035021","How to create a audit table in azure data factory which will hold the status for the pipeline run in Azure Data Factory","<p>I have a requirement where a Azure Data Pipeline is running and inside that we have a data flow where different tables are loaded from ADLS to Azure Sql Database. So the issue is I wanted to store the status of the pipeline like success or failure in an audit table as well as Primary Key column ID which is present in Azure SQL database table  so that when I want to filter  job I on the primary key like for which ID job is success I should get from the audit table.i managed to did something in stored procedure and store the status in a table but I am unable to add a column like ID .Below is the screen shot of pipeline.</p>
<p>The Report_id column is from the table which is loaded from Dataload pipeline.How to add that in audit table so the every time when a pipline runs Report_id is captured and stored in audit table</p>
<p>Audit Table where I want to add Report id</p>
<p><a href=""https://i.stack.imgur.com/BSMxw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BSMxw.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/umIr7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/umIr7.png"" alt=""enter image description here"" /></a></p>
<p>Any help will be appreciated.Thanks</p>
","<azure><azure-functions><azure-pipelines><etl><azure-data-factory>","2020-09-23 19:28:43","1056","1","1","64037119","<p>The Data Flow must have a sink.  So, after the Data Flow completes, you need to use a Lookup activity to get the value of that Report_Id from the sink.  Then, you can set that to a variable and pass that into your Stored Procedure.  (You could also just pass it directly to the Stored Procedure from the Lookup using the same expression you would use to set the variable.)</p>
<p><a href=""https://i.stack.imgur.com/aE2WW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aE2WW.png"" alt=""enter image description here"" /></a></p>
"
"64033915","Using existing data sets withing Azure data factory in a new data flow","<p>I'm having trouble using existing data sets withing Azure data factory when I want to create a new data flow. under data set combo box in data factory doesn't load the existing data sets.
Also when I want to create a new data set most of the data sources such as SQL server is disable.</p>
<p>Is there any idea?</p>
<p>please see this screen shot</p>
<p><a href=""https://i.stack.imgur.com/HAs53.png"" rel=""nofollow noreferrer"">I Can not select the data sets which was built before</a>
When I press new data set sql server is disable and I can not select it.</p>
<p><a href=""https://i.stack.imgur.com/hmoRC.png"" rel=""nofollow noreferrer"">SQL server in this list is disabled and can not be selected</a></p>
","<azure><azure-data-factory>","2020-09-23 18:10:25","229","0","2","64038159","<p>Data flow supports seven type source by now, SQL server is not supported. If your exist dataset isn't within these types, you can't choose them. More Details please refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-source"" rel=""nofollow noreferrer"">this documentation</a>.</p>
<p><a href=""https://i.stack.imgur.com/PuuBD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PuuBD.png"" alt=""enter image description here"" /></a></p>
"
"64033915","Using existing data sets withing Azure data factory in a new data flow","<p>I'm having trouble using existing data sets withing Azure data factory when I want to create a new data flow. under data set combo box in data factory doesn't load the existing data sets.
Also when I want to create a new data set most of the data sources such as SQL server is disable.</p>
<p>Is there any idea?</p>
<p>please see this screen shot</p>
<p><a href=""https://i.stack.imgur.com/HAs53.png"" rel=""nofollow noreferrer"">I Can not select the data sets which was built before</a>
When I press new data set sql server is disable and I can not select it.</p>
<p><a href=""https://i.stack.imgur.com/hmoRC.png"" rel=""nofollow noreferrer"">SQL server in this list is disabled and can not be selected</a></p>
","<azure><azure-data-factory>","2020-09-23 18:10:25","229","0","2","64068611","<p>Since you can't use SQL Server within a Data Flow, prior to the Data Flow you need to run a Copy activity to stage your data from SQL Server into a location reachable by the subsequent Data Flow.  The easiest option to use is a Delimited text file (CSV) in Azure Blob Storage.</p>
"
"64026477","Azure Data Factory V2 using yesterdays date when triggering a pipeline run","<p>I have an Azure Data Factory (v2) that I use to backup the contents of a database to a blob store nightly (2am UTC). However, I expect the name of the file to contain the day of the month (<code>dd</code>) that the backup was generated, but it's always the day before.</p>
<p>The file name is generated using an expression -</p>
<pre><code>@{formatDateTime(pipeline().parameters.windowStart,'dd')}.json
</code></pre>
<p>So for example the run at 3am today should have been called <code>23.json</code>, but it was actually called <code>22.json</code>. 3am is the expected run time as I'm in the UK, which is currently on BST (UTC+1)</p>
<p>Looking at the parameters of the run, I can see that the <code>windowStart</code> is indeed a day out. For example, todays run which was triggered at 2am on the 23rd had <code>9/22/2020 2:00:00 AM</code>.</p>
<p>Is anybody able to explain why Data Factory is behaving in this way, and hopefully how I can make it work as expected.</p>
<p>Here is the trigger as exported from the Data Factory.</p>
<pre><code>{
    &quot;name&quot;: &quot;Trigger_Copy_Transactions&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;runtimeState&quot;: &quot;Started&quot;,
        &quot;pipeline&quot;: {
            &quot;pipelineReference&quot;: {
                &quot;referenceName&quot;: &quot;Copy_Transactions&quot;,
                &quot;type&quot;: &quot;PipelineReference&quot;
            },
            &quot;parameters&quot;: {
                &quot;windowStart&quot;: &quot;@trigger().outputs.windowStartTime&quot;
            }
        },
        &quot;type&quot;: &quot;TumblingWindowTrigger&quot;,
        &quot;typeProperties&quot;: {
            &quot;frequency&quot;: &quot;Hour&quot;,
            &quot;interval&quot;: 24,
            &quot;startTime&quot;: &quot;2020-08-24T02:00:00Z&quot;,
            &quot;delay&quot;: &quot;00:00:00&quot;,
            &quot;maxConcurrency&quot;: 50,
            &quot;retryPolicy&quot;: {
                &quot;intervalInSeconds&quot;: 30
            },
            &quot;dependsOn&quot;: []
        }
    }
}
</code></pre>
","<azure><triggers><azure-data-factory>","2020-09-23 10:59:11","1043","0","1","64029950","<p>One thing you could try is to force the file to be generated in the same time zone that your IR is running in. For example we have a Self Hosted IR so when we would generate files it would not match EST times. In that case I did the following:</p>
<pre><code>@concat('File_name',formatDateTime(
convertFromUtc(utcnow(),'Eastern Standard Time'),'yyyy-MM-dd'),'.txt')
</code></pre>
<p>Perhaps doing that would force the proper date?</p>
<p>Are you also using Auto-generated IR or Self-Hosted IR when running this job?</p>
"
"64022044","ADF Tumbling Window Trigger: Dependency trigger not working as expected","<p>I have created a tumbling window trigger for my Azure Data Factory Pipeline <strong>Test_Daily</strong> with recurrence as 24 hours. For this pipeline, i have added a dependency trigger lets say <strong>Test_Hourly</strong> (which runs every hour) with offset as 1.00:00:00(1 day).</p>
<p>Test_Daily pipeline is not getting triggered <strong>even though the dependency trigger has run successfully</strong>. For example, if the daily pipeline windowStartTime is 2020-09-20 00:00:00 and Test_Hourly with WindowStartTime 2020-09-21 00:00:00 has run successfully, then the daily pipeline should get triggered. However, this is not the case and Test_Daily gets triggered only when Test_Hourly has completed 2020-09-22 00:00:00(i.e. with 2 day offset).</p>
<p>Please let me know how to resolve this issue. Thanks.</p>
","<azure-data-factory>","2020-09-23 06:15:15","2025","1","2","64043229","<p>I think it is your setting problem.</p>
<ul>
<li>Your <strong>Test_Daily</strong> will start at 2020-09-20 00:00:00 and end at 2020-09-21 00:00:00.</li>
<li>Your <strong>Test_Hourly</strong> will wait for <strong>Test_Daily</strong> to complete and then delay 1.00:00:00(1 day) to complete at (2020-09-22 00:00:00).</li>
<li><strong>Test_Hourly</strong> will Waiting on dependency.</li>
</ul>
"
"64022044","ADF Tumbling Window Trigger: Dependency trigger not working as expected","<p>I have created a tumbling window trigger for my Azure Data Factory Pipeline <strong>Test_Daily</strong> with recurrence as 24 hours. For this pipeline, i have added a dependency trigger lets say <strong>Test_Hourly</strong> (which runs every hour) with offset as 1.00:00:00(1 day).</p>
<p>Test_Daily pipeline is not getting triggered <strong>even though the dependency trigger has run successfully</strong>. For example, if the daily pipeline windowStartTime is 2020-09-20 00:00:00 and Test_Hourly with WindowStartTime 2020-09-21 00:00:00 has run successfully, then the daily pipeline should get triggered. However, this is not the case and Test_Daily gets triggered only when Test_Hourly has completed 2020-09-22 00:00:00(i.e. with 2 day offset).</p>
<p>Please let me know how to resolve this issue. Thanks.</p>
","<azure-data-factory>","2020-09-23 06:15:15","2025","1","2","64341584","<p>yes, it is behaving as per the design with the applied settings.</p>
<p>Tumbling window trigger starts at window end time (For daily pipeline it is 2020-09-21 00:00:00) and it adds offset (1 day) to it and pipeline actually runs at 2020-09-22 00:00:00.</p>
<p>The same is replicated when I have used hourly and 5minute triggers. In this case, hourly is dependent on 5minute trigger with an offset of 5minutes and the delay of 5min after window time is seen in triggering time of hourly trigger.</p>
"
"64015087","How to create a trigger in Azure Data Factory which triggers once file available in ADLS","<p>I have a web app where some python code is running which generates csv files and stores it in ADLS ,I wanted to have ADF pipeline which triggers when files arrives in ADLS and load data into DB.
I wanted to know is there any automated triggering facility available in ADF as my files will be based on user input from a front end tool and we do not have idea when user will generate the files it can be very random.I went through Event based triggering option but it says that only 500 triggers are allowed per storage account but in our case there might be more that 500 in single day. Is there any way to achieve the trigger or am I understanding 500 triggers as something wrong.Any suggestion</p>
","<azure><azure-data-factory><azure-data-lake>","2020-09-22 17:44:52","253","0","1","64021785","<ul>
<li>Azure Data Factory only supports a maximum of 500 event triggers per storage account, it means that you can only create  a maximum of 500 event triggers per storage account.</li>
<li>When you created an event trigger for the pipeline, the trigger times  up to your quantity of the created files and it is not limited.</li>
</ul>
<p>I've made a test here that shows one pipeline can be triggered more than 500 times in one day:</p>
<p><a href=""https://i.stack.imgur.com/AVHuf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AVHuf.png"" alt=""enter image description here"" /></a></p>
"
"64015083","How do you update the Cosmos DB account key used in an Azure Data Factory linked service","<p>I am attempting to update a Data Factory V2 linked service through PowerShell, but I'm unable to get a working definition file.</p>
<p>My scenario is that after a Cosmos DB account key has been rotated, the Data Factory linked service that connects to a database on the account should be updated with the new key. To do this, I'm pulling the existing properties out from the linked service, updating the <code>EncryptedCredential</code> and <code>AdditionalProperties.typeProperties.encryptedCredential</code> properties and then firing it back.</p>
<pre><code>$definitionFile = &quot;{0}/cosmosDbDefinition.json&quot; -f $PSScriptRoot
$definition = (Get-AzDataFactoryV2LinkedService -Name $LinkedServiceName -DataFactoryName $Name -ResourceGroupName $ResourceGroupName).Properties
$definition.EncryptedCredential = ConvertTo-SecureString -String $key -AsPlainText
Set-Content -Path $definitionFile -Value ($definition | ConvertTo-Json)
Set-AzDataFactoryV2LinkedService -Name $Name -DataFactoryName $DataFactoryName -ResourceGroupName $ResourceGroupName -DefinitionFile $definitionFile -Force
</code></pre>
<p>However, I'm clearly doing something wrong as <code>Set-AzDataFactoryV2LinkedService</code> is failing -</p>
<blockquote>
<p>Invalid linked service payload, the 'typeProperties' nested in payload is null.</p>
</blockquote>
<p>In deference to the error, <code>typeProperties</code> is not null in the payload. However, I'm unsure if simply firing the properties back is the right thing to do.</p>
<p>The <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.cosmosdblinkedservice?view=azure-dotnet"" rel=""nofollow noreferrer"">documentation</a> does not include any kind of example definition file, and I can't find any working examples (perhaps I'm searching for the wrong thing).</p>
<p>How can I properly update the key for a Data Factory linked Service for Cosmos DB?</p>
","<powershell><azure-powershell><azure-data-factory>","2020-09-22 17:44:13","270","0","1","64038390","<p>The defined file of Azure data factory Linked service should be like as below. For more details, please refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services#linked-service-json"" rel=""nofollow noreferrer"">here</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#linked-service-properties"" rel=""nofollow noreferrer"">here</a>.</p>
<pre><code>{
    &quot;name&quot;: &quot;&lt;Name of the linked service&gt;&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;&lt;Type of the linked service&gt;&quot;,
        &quot;typeProperties&quot;: {
              &quot;&lt;data store or compute-specific type properties&gt;&quot;
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;&lt;name of Integration Runtime&gt;&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}
</code></pre>
<p>If you want to update Azure CosmosDB account key, please refer to the following script</p>
<pre><code>$key=&quot;&lt;account key&gt;&quot;
$definition = (Get-AzDataFactoryV2LinkedService -Name $LinkedServiceName -DataFactoryName $Name -ResourceGroupName $ResourceGroupName).Properties
$newdef=@{
  &quot;properties&quot; =@{
    &quot;type&quot;=&quot;CosmosDb&quot;
    &quot;typeProperties&quot;= @{
      &quot;connectionString&quot;= $definition.ConnectionString+ &quot;AccountKey=$($key)&quot;
     
    }
  
  }
} | ConvertTo-Json -Depth 10

$newdef | Out-File E:\test.json

Set-AzDataFactoryV2LinkedService -Name $LinkedServiceName -DataFactoryName $Name -ResourceGroupName $ResourceGroupName -DefinitionFile E:\test.json
 
</code></pre>
<p><a href=""https://i.stack.imgur.com/uQv4P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uQv4P.png"" alt=""enter image description here"" /></a></p>
"
"64008851","How to pass content in Body for a POST request in Web Activity in Azure Data Factory","<p>I want to generate a access token for salesforce using Azure data factory Web Activity. But I am getting error of grant type invalid or not supported.</p>
<p>URL:<a href=""https://login.salesforce.com/services/oauth2/token"" rel=""nofollow noreferrer"">https://login.salesforce.com/services/oauth2/token</a></p>
<p>Header:Content-Type:application/x-www-form-urlencoded</p>
<p>body:username=XXX&amp;password=XXX&amp;grant_type=password&amp;client_id=XXX&amp;client_secret=XXX</p>
<p>Is there any information or content that i have missed in this?</p>
<p>anyone can help?</p>
","<salesforce><access-token><azure-data-factory><bearer-token>","2020-09-22 11:32:04","302","0","1","64024098","<p>Congratulations you have solved the problem. The answer is that we have to pass Salesforce Security Token along with password.</p>
"
"64005460","How can I connect to a database server within a VPN via Azure Portal?","<p>There is a ORACLE database that is only accessible within the context of a VPN.
I would like to connect to this database using Data Factory, for example.</p>
<p>When I try to connect, it says that the server is not reachable. I believe it is because of the VPN.</p>
<p>How can I configure Azure Portal to create a connection to this VPN? Is it possible?</p>
","<azure><vpn><azure-data-factory>","2020-09-22 08:01:13","1561","0","1","64112302","<p>When we want to access the on-premise source from Data Factory, we must need the self-host integration runtime.</p>
<p>And congratulations that the situation was solved:</p>
<p>&quot;This situation was solved using a Self-Hosted Integration Runtime properly set up. I used one that the company had already configurated. &quot;</p>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime</a></p>
"
"64004654","Azure Data Factory Entities Dependencies Finding","<p>How do we find out the dependencies between various ADF entities such as Pipelines, Datasets &amp; Linked Service?</p>
<p><strong>Example</strong>: I have one dataset DS_ASQL_DB. How do we check if this dataset is being used/referred to in any ADF pipelines?</p>
","<azure-data-factory>","2020-09-22 07:06:31","1467","1","1","64007107","<p>In the ADF UI, we can click the entity and see the <strong>Related</strong> tab.
<a href=""https://i.stack.imgur.com/wt4a3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wt4a3.png"" alt=""enter image description here"" /></a></p>
"
"63994381","Rounding transformation with Data Flow in Azure Data Factory is not working properly","<p>I’m currently creating Data flow with a Derived column that has a Rounding transformation through DontNet.SDK. The Source and Sink datasets I am using are parameterized. And I’m assigning values for them at runtime through pipeline parameters. Please refer below json files of two data flows.</p>
<p><a href=""https://i.stack.imgur.com/92jJv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/92jJv.png"" alt=""enter image description here"" /></a></p>
<p>I have a scenario which is rounding salary in to two decimal points from three decimal points. When I created this manually in ADF it’s successfully rounding. Below is the output result file of transformation</p>
<p><a href=""https://i.stack.imgur.com/SKsxC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SKsxC.png"" alt=""enter image description here"" /></a></p>
<p>But when I am creating this using .net SDK, it’s not working. I’m not getting the column name as expected but the value is coming correctly. Below is the .SDK output</p>
<p><a href=""https://i.stack.imgur.com/DLTwn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DLTwn.png"" alt=""enter image description here"" /></a></p>
<p>Below is the Json format of Data Flow I created</p>
<pre><code>{
&quot;name&quot;: &quot;Rounding_Auto__Transformation&quot;,
&quot;properties&quot;: {
    &quot;type&quot;: &quot;MappingDataFlow&quot;,
    &quot;typeProperties&quot;: {
        &quot;sources&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;defaultdataflowSourcedataset&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;source&quot;
            }
        ],
        &quot;sinks&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;defaultdataflowSinkdataset&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;sink&quot;
            }
        ],
        &quot;transformations&quot;: [
            {
                &quot;name&quot;: &quot;DerivedColumn0&quot;
            }
        ],
        &quot;script&quot;: &quot;source(output(\n\t\tid as string,\n\t\tsal as string,\n\t\tgender as string,\n\t\tname as string,\n\t\tisMarried as string,\n\t\ttags as string,\n\t\taddress as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~&gt; source\nsource derive(NewSal = round(toFloat(sal),2,2)) ~&gt; DerivedColumn0\nDerivedColumn0 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['customer_post_with_round.csv'],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~&gt; sink&quot;
    }
}
</code></pre>
<p>}</p>
<p>I also compared the json created for the manual (as it works) in ADF directly– here is the one for manual</p>
<pre><code>{
&quot;name&quot;: &quot;Rounding_Manually&quot;,
&quot;properties&quot;: {
    &quot;type&quot;: &quot;MappingDataFlow&quot;,
    &quot;typeProperties&quot;: {
        &quot;sources&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;SourcDS&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;source1&quot;
            }
        ],
        &quot;sinks&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;SinkDS&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;sink1&quot;
            }
        ],
        &quot;transformations&quot;: [
            {
                &quot;name&quot;: &quot;DerivedColumn1&quot;
            }
        ],
        &quot;script&quot;: &quot;source(output(\n\t\tid as string,\n\t\tsal as string,\n\t\tgender as string,\n\t\tname as string,\n\t\tisMarried as string,\n\t\ttags as string,\n\t\taddress as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~&gt; source1\nsource1 derive(NewSal = round(toFloat(sal),2,2)) ~&gt; DerivedColumn1\nDerivedColumn1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['customer_post_with_round.csv'],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~&gt; sink1&quot;
    }
}
</code></pre>
<p>}</p>
<p>Please help.</p>
","<azure><azure-data-factory><dataflow><azure-sdk-.net>","2020-09-21 14:30:46","807","0","2","64017944","<p>I imported your data flow definition in my environment and I do see the column name in the metadata inspect and mapping list. Can you do the same and copy / paste the data flow script into the UI and make sure that you see everything there just fine?</p>
<p><a href=""https://i.stack.imgur.com/tOhsJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tOhsJ.png"" alt=""enter image description here"" /></a></p>
"
"63994381","Rounding transformation with Data Flow in Azure Data Factory is not working properly","<p>I’m currently creating Data flow with a Derived column that has a Rounding transformation through DontNet.SDK. The Source and Sink datasets I am using are parameterized. And I’m assigning values for them at runtime through pipeline parameters. Please refer below json files of two data flows.</p>
<p><a href=""https://i.stack.imgur.com/92jJv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/92jJv.png"" alt=""enter image description here"" /></a></p>
<p>I have a scenario which is rounding salary in to two decimal points from three decimal points. When I created this manually in ADF it’s successfully rounding. Below is the output result file of transformation</p>
<p><a href=""https://i.stack.imgur.com/SKsxC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SKsxC.png"" alt=""enter image description here"" /></a></p>
<p>But when I am creating this using .net SDK, it’s not working. I’m not getting the column name as expected but the value is coming correctly. Below is the .SDK output</p>
<p><a href=""https://i.stack.imgur.com/DLTwn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DLTwn.png"" alt=""enter image description here"" /></a></p>
<p>Below is the Json format of Data Flow I created</p>
<pre><code>{
&quot;name&quot;: &quot;Rounding_Auto__Transformation&quot;,
&quot;properties&quot;: {
    &quot;type&quot;: &quot;MappingDataFlow&quot;,
    &quot;typeProperties&quot;: {
        &quot;sources&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;defaultdataflowSourcedataset&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;source&quot;
            }
        ],
        &quot;sinks&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;defaultdataflowSinkdataset&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;sink&quot;
            }
        ],
        &quot;transformations&quot;: [
            {
                &quot;name&quot;: &quot;DerivedColumn0&quot;
            }
        ],
        &quot;script&quot;: &quot;source(output(\n\t\tid as string,\n\t\tsal as string,\n\t\tgender as string,\n\t\tname as string,\n\t\tisMarried as string,\n\t\ttags as string,\n\t\taddress as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~&gt; source\nsource derive(NewSal = round(toFloat(sal),2,2)) ~&gt; DerivedColumn0\nDerivedColumn0 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['customer_post_with_round.csv'],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~&gt; sink&quot;
    }
}
</code></pre>
<p>}</p>
<p>I also compared the json created for the manual (as it works) in ADF directly– here is the one for manual</p>
<pre><code>{
&quot;name&quot;: &quot;Rounding_Manually&quot;,
&quot;properties&quot;: {
    &quot;type&quot;: &quot;MappingDataFlow&quot;,
    &quot;typeProperties&quot;: {
        &quot;sources&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;SourcDS&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;source1&quot;
            }
        ],
        &quot;sinks&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;SinkDS&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;sink1&quot;
            }
        ],
        &quot;transformations&quot;: [
            {
                &quot;name&quot;: &quot;DerivedColumn1&quot;
            }
        ],
        &quot;script&quot;: &quot;source(output(\n\t\tid as string,\n\t\tsal as string,\n\t\tgender as string,\n\t\tname as string,\n\t\tisMarried as string,\n\t\ttags as string,\n\t\taddress as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~&gt; source1\nsource1 derive(NewSal = round(toFloat(sal),2,2)) ~&gt; DerivedColumn1\nDerivedColumn1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['customer_post_with_round.csv'],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~&gt; sink1&quot;
    }
}
</code></pre>
<p>}</p>
<p>Please help.</p>
","<azure><azure-data-factory><dataflow><azure-sdk-.net>","2020-09-21 14:30:46","807","0","2","73910790","<p>In this case the headers in the sink files were missing, and therefore they were not appearing the new ones during the transformation.
By adding the option First row as header issue is solved.</p>
"
"63991975","adf until activity and if condition activity","<p>Looking for some guidance
I am building a pipeline which checks a condition using If- condition activity
If condition is true I am using until activity (inside the true section of If activity) to process my data transfer requirement</p>
<p>while debug I am getting this error
&quot;code&quot;:&quot;BadRequest&quot;,&quot;message&quot;:&quot;Container activity cannot include another container activitynull&quot;,&quot;target&quot;:&quot;pipeline/Pipeline1/runid/526d67ff-867c-432a-b449-b0e951f1c40&quot;,&quot;details&quot;:null,&quot;error&quot;:null}</p>
<p>Any suggestion for above issue</p>
","<azure><azure-data-factory>","2020-09-21 12:06:51","2143","0","1","64014293","<p>You can get around the limitations on nested activities within Azure Data Factory by wrapping your inner activities in another pipeline.</p>
<p><a href=""https://i.stack.imgur.com/BDyiX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BDyiX.png"" alt=""Pipeline 1"" /></a></p>
<p><a href=""https://i.stack.imgur.com/7Og2A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Og2A.png"" alt=""Pipeline 2"" /></a></p>
<p><a href=""https://i.stack.imgur.com/pvxor.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pvxor.png"" alt=""Until Nested Within If"" /></a></p>
"
"63990096","Power shell command to disable the networking setting of Azure Data Factory","<p>In Azure Data Factory under the Settings options of &quot;Networking&quot;, we have to disable the &quot;Public Network Access&quot;, under the &quot;Network access&quot; option.
If the public network access is &quot;Enabled&quot; then it is Open to Internet, which states that - &quot;All Networks, including Internet can access Data Factory&quot;.
And this is more threat of exposing the Data Factory to internet. For which we need a power-shell/AZ CLI command which will help us to disable the 'Public Network Access&quot;.</p>
<p><img src=""https://i.stack.imgur.com/4Q6iW.png"" alt=""enter image description here"" /></p>
","<azure><powershell><azure-data-factory><azure-cli>","2020-09-21 10:03:16","826","1","1","64001922","<p>You could set it with <code>properties.publicNetworkAccess</code>, please see the code bellow:</p>
<pre><code>Connect-AzAccount

$a= Get-AzResource -ResourceType Microsoft.DataFactory/factories -ResourceGroupName ChinaCXPTeam-Resources -ResourceName dfleon
$a.Properties.publicNetworkAccess = &quot;Disabled&quot;
$a | Set-AzResource -Force
</code></pre>
<p>Here are the module versions I use:</p>
<p><a href=""https://i.stack.imgur.com/DLBQH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DLBQH.png"" alt=""enter image description here"" /></a></p>
<p>Update:</p>
<p>This command also works well:</p>
<pre><code>$a = Get-AzResource -ResourceType Microsoft.DataFactory/factories -ResourceGroupName &quot;&lt;resource-group-name&gt;&quot; -ResourceName &quot;&lt;datafactory-name&gt;&quot;
$a.Properties | Add-Member -MemberType NoteProperty -Name &quot;publicNetworkAccess&quot; -Value &quot;Disabled&quot; -Force
$a | Set-AzResource -Force
</code></pre>
"
"63987951","Powershell command to Change Public Access of Data Factory from Enable to Disabled","<p>Powershell command to Change Public Access of Data Factory from <code>Enabled</code> to <code>Disabled</code>.
This is coming under the <code>Networking</code> options of the Data Factory.</p>
<p><a href=""https://i.stack.imgur.com/TZie8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TZie8.png"" alt=""enter image description here"" /></a></p>
","<azure><powershell><networking><azure-data-factory>","2020-09-21 07:35:02","314","0","1","64002354","<p>Just use the command below, it works on my side.</p>
<pre><code>$factory = Get-AzResource -ResourceType Microsoft.DataFactory/factories -ResourceGroupName &lt;resource-group-name&gt; -ResourceName &lt;datafactory-name&gt;
$factory.Properties.publicNetworkAccess = &quot;Disabled&quot;
$factory | Set-AzResource -Force
</code></pre>
<p><a href=""https://i.stack.imgur.com/U2DPK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U2DPK.png"" alt=""enter image description here"" /></a></p>
<p>Check in the portal:</p>
<p><a href=""https://i.stack.imgur.com/O2B5E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O2B5E.png"" alt=""enter image description here"" /></a></p>
<p><strong>Update:</strong></p>
<p>You could use the command below, no matter you have changed the setting or not.</p>
<pre><code>$factory = Get-AzResource -ResourceType Microsoft.DataFactory/factories -ResourceGroupName &quot;&lt;resource-group-name&gt;&quot; -ResourceName &quot;&lt;datafactory-name&gt;&quot;
$factory.Properties | Add-Member -MemberType NoteProperty -Name &quot;publicNetworkAccess&quot; -Value &quot;Disabled&quot; -Force
$factory | Set-AzResource -Force
</code></pre>
<p><a href=""https://i.stack.imgur.com/4EZQy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4EZQy.png"" alt=""enter image description here"" /></a></p>
"
"63981373","Copy Different type of file from Gen1 Azur lake to Azur Gen2 lake with attribute( like last updated)","<p>I need to migrate all my data from Azur data lake Gen1 to Lake Gen2.
In my lake we have different types of file mixed (.txt, .zip,.json and many other).
We want to move them as-it-is to GEN2 lake. Along with that we also want to maintain last updated time for all files as GEN1 lake.</p>
<p>I was looking to use ADF for this use case. But for that we need to define dataset, and to define dataset we have to define data format(Avro,json,xml, binary etc). As we have different type of data mixed, I tried to use binary format. But with binary format all file at destination have content type &quot;application/octate-stream&quot;. Also not able to retain file update time.</p>
","<azure><azure-data-lake><azure-data-factory><azure-data-lake-gen2>","2020-09-20 16:44:43","221","1","2","63985724","<p>As you said, when the files are copied to Data Lake Gen2, all the files  properties will be changed, such as 'LAST MODIFIED' time.</p>
<p>Like file uploading, these files are new created in Gen 2, and Azure will create the new properties for them. That's why We can not keep the old property in Gen 1.</p>
<p>When using binary format as the dataset, all the content type is <code>application/octate-stream</code>, we also can not change it.</p>
<p>The property difference between Gen1 and Gen 2(I copied files from Gen 1 to Gen 2):
<a href=""https://i.stack.imgur.com/qSvWu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qSvWu.png"" alt=""enter image description here"" /></a></p>
<p>Unless we download the 'word.csv' file and re-upload, the content type will change to <code>application/vnd.ms-excel</code>:</p>
<p><a href=""https://i.stack.imgur.com/3eYOU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3eYOU.png"" alt=""enter image description here"" /></a></p>
<p>HTH.</p>
"
"63981373","Copy Different type of file from Gen1 Azur lake to Azur Gen2 lake with attribute( like last updated)","<p>I need to migrate all my data from Azur data lake Gen1 to Lake Gen2.
In my lake we have different types of file mixed (.txt, .zip,.json and many other).
We want to move them as-it-is to GEN2 lake. Along with that we also want to maintain last updated time for all files as GEN1 lake.</p>
<p>I was looking to use ADF for this use case. But for that we need to define dataset, and to define dataset we have to define data format(Avro,json,xml, binary etc). As we have different type of data mixed, I tried to use binary format. But with binary format all file at destination have content type &quot;application/octate-stream&quot;. Also not able to retain file update time.</p>
","<azure><azure-data-lake><azure-data-factory><azure-data-lake-gen2>","2020-09-20 16:44:43","221","1","2","64283023","<p>Last Modified Time is system metadata that represents that modification in the filesystem/container and it cannot be updated. Adding user meta data to capture meta data from the source is work around and powershell/.net/java sdk can be used for updating additional property. Below the workaround is implemented in PowerShell</p>
<p><a href=""https://i.stack.imgur.com/WdIoY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WdIoY.png"" alt=""enter image description here"" /></a></p>
"
"63962445","Slow API execution in Azure DataFactory","<p>I have a flow in Azure Logic Apps that consume some API services, in AzureLogicApps the execution is very fast, but when performing the same process in Azure DataFactory the process becomes much much slower, what could be the reason?</p>
<p><img src=""https://i.stack.imgur.com/yfLeZ.png"" alt=""LogicApp Api Configuration: "" /></p>
<p>The process in ADF runs in a ForEach Loop</p>
<p><img src=""https://i.stack.imgur.com/Iu7IP.png"" alt=""Web Activity Configuration in ADF"" /></p>
","<azure-data-factory><azure-logic-apps>","2020-09-18 20:28:23","459","0","1","64002930","<p>For this problem, it may caused by the for each loop in data factory. When request the api in for each loop, it may complete the first few times fast. But when loop many times, it will take much more time to complete each request.</p>
<p>It seems you configure the api in azure api management. The root cause of this problem is your original api can just process a request in 1 second(just for example). But many requests will be sent to request the api in for each loop(<strong>in parallel</strong>). So some requests will be queued. You can set the for each loop to execute the request one by one by enable &quot;<strong>Sequential</strong>&quot; as below screenshot.</p>
<p><a href=""https://i.stack.imgur.com/6ahKV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6ahKV.png"" alt=""enter image description here"" /></a></p>
<p>After this configuration, the request will be executed one by one. Each of them will not take much more time, <strong>but I think the total time may not reduce</strong>.</p>
<p>Or you should improve the load of your api (if it is possible) to allow more request be executed at the same time.</p>
"
"63958219","How to trim data in Azure Data Factory source with csv file format without using Dataflows and Databricks or any other transformation tool","<p>In source csv file the data contains white spaces. How to remove those without using any transformation tool and just using Azure Data Factory. I tried &quot;For each&quot; activity on copy activity but the For each @items is of JSON array and string functions doesn't apply on it. Also, Data factory does not support custom functions and expressions. Is there any way to remove the white spaces from the source or during the copy process to the sink? Source and Sink are &quot;Azure Files&quot;.</p>
","<azure-data-factory>","2020-09-18 15:00:47","1274","0","2","64069608","<p>The most performant way to achieve this would be to temporarily stage the data in Azure SQL or Cosmos DB and then trim each column with an explicit SELECT statement as the source of the subsequent Copy activity moving the data to your sink file.</p>
"
"63958219","How to trim data in Azure Data Factory source with csv file format without using Dataflows and Databricks or any other transformation tool","<p>In source csv file the data contains white spaces. How to remove those without using any transformation tool and just using Azure Data Factory. I tried &quot;For each&quot; activity on copy activity but the For each @items is of JSON array and string functions doesn't apply on it. Also, Data factory does not support custom functions and expressions. Is there any way to remove the white spaces from the source or during the copy process to the sink? Source and Sink are &quot;Azure Files&quot;.</p>
","<azure-data-factory>","2020-09-18 15:00:47","1274","0","2","64078489","<p>If not all the csv data contains white spaces, as I know about DF and per my experience, it's impossible to achieve that data conversion only with Copy active! Using data flow or others tools is very easy.</p>
<p>There isn't a way to achieve this using ADF only or directly.</p>
<p>HTH.</p>
"
"63957729","Automagically create a view that unions all other views with the same name but different prefix and different schema","<p>I have a database that contains two or more schema's. Each schema contains several views.</p>
<p>In schema <code>ALPHA</code> we find the following views:</p>
<ul>
<li>vw_a_schools</li>
<li>vw_a_classes</li>
<li>vw_a_teachers</li>
</ul>
<p>In schema <code>BETA</code> we find the following views:</p>
<ul>
<li>vw_b_schools</li>
<li>vw_b_classes</li>
<li>vw_b_automobiles</li>
</ul>
<p>What I am looking for is a procedure that will create a schema <code>ALLTOGETHERNOW</code> that unions all views with the same name.
So schema <code>ALLTOGETHERNOW</code> will look like this:</p>
<ul>
<li>vw_schools       =      (select * from vw_a_schools UNION ALL select * from vw_b_schools)</li>
<li>vw_classes       =      (select * from vw_a_classes UNION ALL select * from vw_b_classes)</li>
<li>vw_teachers      =      (select * from vw_a_teachers)</li>
<li>vw_automobiles   =      (select * from vw_b_automobiles)</li>
</ul>
<p>The following apply:</p>
<ul>
<li>the amount of schemas is not known beforehand. If necessary though we can make a table that lists all the schema's to be joined</li>
<li>the amount of views is not known beforehand.</li>
<li>Views always look like <code>vw_&lt;something&gt;_viewname</code>. We do know for sure that views with the same name have the same columns.</li>
</ul>
<p>SO the idea is to somehow loopt through the schemas, collect all views, substract the suffix (the <code>vw_&lt;something&gt;</code> part, then create a statement that will create the view that unions all the views.</p>
<p>I have been playing with information schemas and such and can create a list with viewnames. But there I hit a dead end....</p>
","<sql><tsql><azure-sql-database><azure-data-factory>","2020-09-18 14:32:19","180","0","1","64050930","<p>&quot;Automagically&quot; - there is no magic, just dynamic SQL that reads metadata and generate single output query:</p>
<pre><code>DECLARE @create_view_sql NVARCHAR(MAX);

WITH vw(t) AS (
   SELECT DISTINCT RIGHT(TABLE_NAME, LEN(TABLE_NAME)-5) --assumption prefix has 4 chars
   FROM INFORMATION_SCHEMA.VIEWS
   WHERE TABLE_SCHEMA NOT IN ('ALTOGETHER', 'dbo')
)
SELECT @create_view_sql = STRING_AGG(s.sql, CHAR(13)) WITHIN GROUP(ORDER BY vw.t)
FROM vw
CROSS APPLY(
SELECT  N'EXEC(''CREATE OR ALTER VIEW ALTOGETHER.' + QUOTENAME(N'vw_' + vw.t) + ' AS ' +  
       STUFF(STRING_AGG(FORMATMESSAGE('UNION ALL SELECT * FROM %s.%s'
     ,QUOTENAME(TABLE_SCHEMA), QUOTENAME(TABLE_NAME))
    , CHAR(13)) WITHIN GROUP(ORDER BY TABLE_NAME), 1,10,'')
    + N''');'
FROM INFORMATION_SCHEMA.VIEWS
WHERE TABLE_NAME LIKE 'vw_[a-z]_' + vw.t
  AND TABLE_SCHEMA NOT IN ('ALTOGETHER', 'dbo')
)s(sql);

PRINT @create_view_sql;

EXEC sp_executesql @create_view_sql;
</code></pre>
<p>It could be enclosed within stored procedure and run in a scheduled manner.</p>
<p><strong><a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=16c4841488bfdf22d32c4e72ff2a7d57"" rel=""nofollow noreferrer"">db&lt;&gt;fiddle.com demo</a></strong></p>
"
"63957353","How to execute a single databricks notebook in a data factory pipeline","<p>I have a data factory pipeline with multiple databricks notebooks. The debug option is executing the whole pipeline and I couldn't find any option to execute a single notebook. Is there any way we can execute only a single notebook instead of the whole pipeline?</p>
","<azure><azure-data-factory>","2020-09-18 14:08:55","268","0","1","63959290","<p>No, we can't.  it's not supported for now.</p>
<p>We can't execute only a single notebook instead of the whole pipeline.</p>
<p>But if the single notebook is failed,  we can only run the failed actives in pipeline without the whole pipeline:</p>
<ul>
<li>From the pipeline run view inside ADF monitoring pane, you can see
the list of activities, their run status, and from there you can
choose the option to rerun from a certain activity in the middle of
the pipeline, rather from the beginning of the pipeline.</li>
</ul>
<p>Ref here: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/34270504-run-individual-failed-activities-only-instead-comp"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/34270504-run-individual-failed-activities-only-instead-comp</a></p>
"
"63957031","Azure Data Factory Monitor showing no data for Dashboard","<p>I have two ADF pipelines. When I run them in debug, I can see them in the Pipeline runs tab (Debug part). So if I run a pipeline 10 times, I will have 10 entries. However, my Dashboard is empty. Even after waiting, doing several refreshes, I have &quot;No result - There are no records to show you right now.&quot; in Pipeline, Activity and Trigger runs. I am Owner on the ressource.</p>
<p><a href=""https://i.stack.imgur.com/5cLfq.png"" rel=""nofollow noreferrer"">empty dashboard</a></p>
<p>I could not find anything on the web, except for the IAM part.</p>
<p>Thanks for your help</p>
","<azure><azure-data-factory>","2020-09-18 13:48:42","430","0","1","63987870","<p>So, it would appear that the dashboard shows only triggered pipeline runs (and not debug one). You need to trigger your pipeline (Add Trigger -&gt; Trigger Now) if you want to be able to test that the Monitoring is working. That was driving me crazy.</p>
"
"63955445","Azure Data Factory Copy Data using XML Source","<p>Lets assume I have a simple XML file source which I've mapped to a corresponding sink in my SQL server database.</p>
<pre><code>&lt;Date Date=&quot;2020-03-13Z&quot;&gt;
    &lt;Identification&gt;
        &lt;Identifier&gt;Maverick&lt;/Identifier&gt;
    &lt;/Identification&gt;
    &lt;Pilot HomeAirport=&quot;New York&quot;&gt;
        &lt;AirportICAOCode&gt;USA&lt;/AirportICAOCode&gt;
    &lt;/Pilot&gt;
&lt;/Date&gt;
</code></pre>

<p>And then the schema</p>
<pre><code>CREATE TABLE pilots
identifier VARCHAR(20),
ICAO_code VARCHAR(3)
)
</code></pre>
<p>I created a stored procedure in my sql server database that takes an input of the user-defined table type pilots_type which corresponds to the above schema to merge my data correctly.</p>
<p>But the pipeline fails when run with the error:</p>
<pre><code>{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;ErrorCode=UserErrorInvalidPluginType,'Type=Microsoft.DataTransfer.Common.Shared.PluginNotRegisteredException,Message=Invalid type 'XmlFormat' is provided in 'format'. Please correct the type in payload and retry.,Source=Microsoft.DataTransfer.ClientLibrary,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;Sink XML&quot;,
&quot;details&quot;: []
}
</code></pre>
<p>See image<br />
Here the source is a blob that contains the XML.
<a href=""https://i.stack.imgur.com/VMaoV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VMaoV.png"" alt=""enter image description here"" /></a></p>
<p>Is XML not supported as a source after all?</p>
","<azure><azure-data-factory>","2020-09-18 12:05:07","1911","0","1","63990127","<p>XML is supported as a source.<br />
I've made a same test according to your sample xml file and sql table successfully.</p>
<ol>
<li>I created a Table Type named <code>ct_pilot_type</code>:</li>
</ol>
<pre class=""lang-sql prettyprint-override""><code>CREATE TYPE ct_pilot_type AS TABLE(
identifier  nvarchar(MAX),
ICAO_code nvarchar(MAX)
)
</code></pre>
<ol start=""2"">
<li>I created the stored procedure named <code>spUpsertPolit</code>:</li>
</ol>
<pre class=""lang-sql prettyprint-override""><code>CREATE PROCEDURE spUpsertPolit

@polit ct_pilot_type READONLY

AS

BEGIN

MERGE [dbo].[pilot_airports] AS target_sqldb

USING @polit AS source_tblstg

ON (target_sqldb.identifier = source_tblstg.identifier)

WHEN MATCHED THEN

UPDATE SET

identifier = source_tblstg.identifier,

ICAO_code = source_tblstg.ICAO_code


WHEN NOT MATCHED THEN

INSERT (

identifier,

ICAO_code

)

VALUES (

source_tblstg.identifier,

source_tblstg.ICAO_code

);

END
</code></pre>
<ol start=""3"">
<li>I set the sink in the Copy activity:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/otXEO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/otXEO.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>I set the mapping:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/8Cm0l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Cm0l.png"" alt=""enter image description here"" /></a></p>
<ol start=""5"">
<li><p>It cpoied successfully:
<a href=""https://i.stack.imgur.com/07oFm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/07oFm.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>The result shows:<br />
<a href=""https://i.stack.imgur.com/SCSJD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SCSJD.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
"
"63955183","Azure Data Factory - How do I iterate through records in a CSV file using a ForEach loop","<p>What I am trying to achieve:</p>
<ul>
<li>I have a CSV (FlattenedListDocument.csv) with the following columns</li>
</ul>
<p>DocumentKey, DocumentName</p>
<p>Sample values are as follows (there are approximately 240,000 rows in this CSV file):</p>
<p>12212, Hitch Hikers Guide to the Galaxy
12233, MoneyBall</p>
<ul>
<li>I have to create a JSON file for each of the rows in the CSV file that will be used by another utility (one file per row).</li>
<li>I am stuck with how to push the values inside the CSV table to a ForEach activity to get it to loop iteratively through the CSV file.</li>
</ul>
","<azure><csv><azure-data-factory>","2020-09-18 11:46:53","5430","0","2","63958428","<p>This is a really interesting problem to solve in Data Factory. The only option I see is to have a Data Flow with a Sink partition that outputs files based on a Derived Column.</p>
<ol>
<li>Create a Derived Column that generates the unique blob name. <strong>Be sure to include the folder path</strong>:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/pFzmp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pFzmp.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>In the Sink, under Settings, change the &quot;File name option&quot; to &quot;As data in column&quot;, and select the FileName column you created in step 1:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/9FTRs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9FTRs.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>Optional, but in the Sink under &quot;Mapping&quot;, remove the FileName column:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/k4IWY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k4IWY.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/1TL1t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1TL1t.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>Once finished, you should have this in Blob storage:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/v9gQs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v9gQs.png"" alt=""enter image description here"" /></a></p>
<p>The caveat, of course, is that the file name needs to be unique, so I based it on the first column in your sample (which I named &quot;Id&quot;). I have no idea what performance will be like with 280K files, but this should get the result that you want.</p>
"
"63955183","Azure Data Factory - How do I iterate through records in a CSV file using a ForEach loop","<p>What I am trying to achieve:</p>
<ul>
<li>I have a CSV (FlattenedListDocument.csv) with the following columns</li>
</ul>
<p>DocumentKey, DocumentName</p>
<p>Sample values are as follows (there are approximately 240,000 rows in this CSV file):</p>
<p>12212, Hitch Hikers Guide to the Galaxy
12233, MoneyBall</p>
<ul>
<li>I have to create a JSON file for each of the rows in the CSV file that will be used by another utility (one file per row).</li>
<li>I am stuck with how to push the values inside the CSV table to a ForEach activity to get it to loop iteratively through the CSV file.</li>
</ul>
","<azure><csv><azure-data-factory>","2020-09-18 11:46:53","5430","0","2","63985302","<p>I have implemented the following that helped me solve the core problem I was having (reading a CSV file and passing the values to a ForEach loop).</p>
<p>Steps 1 and 2 are not part of the core problem but will help anyone else reading this understand how the FlattenedListDocument.csv turned up.</p>
<p><strong>Overall Schema</strong></p>
<p><a href=""https://i.stack.imgur.com/i6TuR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i6TuR.png"" alt=""enter image description here"" /></a></p>
<p>Step 1: Call web service to obtain the JSON file (XML file as a dataset and a JSON file as a sink).</p>
<p>Step 2: Create a data flow to flatten the file. Create a CSV as a Sink (and partition the sink into 100 equal pieces (to allow for growth of up to 500,000 records in the CSV file).</p>
<p><a href=""https://i.stack.imgur.com/yeNv2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yeNv2.png"" alt=""enter image description here"" /></a></p>
<p>Step 3: Perform a look up on the file and obtain it's contents (this will eventually be wrapped up in GetMetadata / ForEach loop that invokes another pipeline to extract the contents of each file).</p>
<p>Step 4: Extract the contents using a ForEach loop (this calls another WebService with the the Name value from the lookup within the pipeline as a parameter).</p>
<p><a href=""https://i.stack.imgur.com/7hoTE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7hoTE.png"" alt=""enter image description here"" /></a></p>
"
"63954564","Migrate DataStage Job in to ADF,Subscriptions required and any plugins needed","<p>We have a DataStage Job which currently does simple transform from 3 different On Prem Sql DB Sources in to single destination SQL source(On Prem) again.We are planning to migrate Datastage in to ADF.Have the below questions as I am newbie to ADF.
1)How do request for Azure subscription with ADF enabled?
2)Since the source and destination are on Prem DB Servers,does this require any plug ins or framework to allow the connections between Azure and OnPrem servers</p>
","<azure><azure-data-factory><subscription><datastage>","2020-09-18 11:05:51","489","-1","1","63960292","<p>You can connect to on-prem sources directly from ADF using a self-hosted Integration Runtime. To set that up, it will require installation of a service executable on a server or VM that has connectivity to your on-prem data. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime</a></p>
"
"63942948","Azure Data Factory linking GitHub repository for Source Control?","<p>Is there a way to create an ARM template resource so that a GitHub repository can be added into an Azure Data Factory for source control?</p>
<p>I was able to do it in the UI easily. Is this only something that can be done manually after the factory is spun up?</p>
<p>I am not talking about linking GitHub as a service in ADF, that is a different thing. I am looking to have the source control connection made for any published pipelines/datasets to be put into GitHub with their respective parameter files. Mainly looking to automate this for an initial ADF deployment so there is less clicking around done in the UI.</p>
","<azure><github><azure-data-factory>","2020-09-17 17:05:12","180","1","1","63946779","<p>You can download the ARM template and parameter file to automate both factory creation and source control setup. See the image for reference.</p>
<p><a href=""https://i.stack.imgur.com/jQ6os.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jQ6os.png"" alt=""enter image description here"" /></a></p>
"
"63939865","How to call the Power BI Activity Log API","<p>I would like to call the Power BI Activity Log API with data factory each day to download the activity events into a blob store. I thought I'd use a Copy data activity with an HTTP dataset as source to call the REST API. How do I authenticate in the HTTP dataset's linked service? Do I need to register an Azure AD application with Power BI admin rights and refer to it in some way in the linked service?</p>
","<powerbi><azure-data-factory>","2020-09-17 14:06:42","247","0","1","67097610","<p>Maybe you can have a look at <a href=""https://learn.microsoft.com/en-us/power-bi/admin/read-only-apis-service-principal-authentication"" rel=""nofollow noreferrer"">Service Principals</a>, it can help you to easily authenticate and access Power BI admin read-only APIs (ActivityEvents API included) without tying you to a particular user account. Its in preview feature as of today, but should be soon out of preview.</p>
<p>Also, for automating the fetching of logs using Azure Data Factory and ingesting it in Azure Blob, you may have a look at this great <a href=""https://justb.dk/blog/2021/02/extracting-the-power-bi-activity-log-with-data-factory/"" rel=""nofollow noreferrer"">article</a> which explains in detail, all the steps that will be required to create the pipeline. Should be helpful!</p>
"
"63935659","Is it possible to copy GeoJson data into PostGIS with Azure Data Factory?","<p>I am looking if it is possible to transition from Airflow to Azure Data Factory.
I have a REST API from which I extract GeoJSON and would like to export this to a Postgres Database with PostGIS. I tried to do this with the Copy Data activity, but this only provides a simple mapping between the GeoJSON fields and similar fields in my table.
Normally I would use ogr2ogr to do this, but am not sure how to approach this with Azure Data Factory.</p>
<p>Does anyone know if my use case would be possible? If yes, how would you suggest to do it?</p>
","<postgresql><azure><postgis><azure-data-factory>","2020-09-17 10:00:10","349","0","1","64319450","<p>I fixed my own question. I created an Azure Function which runs Python in a self assigned docker container (one of the options in Azure Functions). I installed gdal in the standard Azure Functions Python Docker container and run subprocess.run() to execute ogr2ogr with the parameters I pass to it via the body of the Azure Functions POST request. I can run this Azure Function via Azure Data Factory.</p>
<p>Hope this can help anyone else searching for a similar approach.</p>
"
"63933355","Azure Data factory transformation pipeline take more than 5 minutes to execute when there are Data Flow activity with Transformation","<p>I'm have created a pipeline with a data flow activity which is concatenating two fields in my input file. My input file contains 5 rows of data and 4 Fields. I'm creating these things through visual studio using .net SDK. When I create this job and run it, it's taking more than 5 minutes to execute. What I feel it's internally enabling the Data Flow debugger and that's why it's taking that much of time. Just want to know is my answer is correct or not. If not please tell me why it's taking that much of time even for 5 records? Thanks</p>
","<azure><asp.net-core><asp.net-core-webapi><azure-data-factory>","2020-09-17 07:38:35","266","0","1","64014474","<p>Any time you use a Data Flow within ADF it has to provision the Integration Runtime for the Data Flow.  You will always incur this spin-up time when Data Flows are involved.</p>
<p>(If you have multiple sequential Data Flows within the same pipeline, the first Data Flow will always incur the spin-up time, but if you set Time to Live (TTL) on the Integration Runtime config you can have subsequent Data Flows use that initial runtime and skip the spin-up time on the downstream Data Flows which fire within the TTL window.)</p>
<p>With just a single Data Flow, reevaluate whether you absolutely need it.  If you don't, then you can remove it and skip that spin-up time.  But, if you need the Data Flow there is no way to avoid that initial spin-up.</p>
"
"63930752","Azure databricks connect to on-prem databases through ADF","<p>Is possible to connect an Azure Databricks notebook with Azure Data Factory linkedservices (connections to on prem DBs)?</p>
<p>On ADF, I have connections to on prem gateways through linked services to connect to local DBs. I need to connect my Databricks notebook with that linked services on ADF. It is possible?</p>
<p>Regards.</p>
","<azure-data-factory><azure-databricks>","2020-09-17 03:19:23","140","1","1","63931302","<p>Yes you can,</p>
<p>You need to ensure your databricks resource is configured to be in the VNET which can talk to these databases.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject</a></p>
"
"63927629","How to replace CR and LF in Data Factory expressions","<p>There doesn't seem to be a clear answer to this, I am using a mapping data flow and a derived column expression.  I have data coming through that has spaces, carriage returns and line feeds, which I want to replace with nothing. For instance:</p>
<pre><code>Jenny Mc Carthy  -  15 Somewhere Road
Someplace SomePostcode
SomeCity
</code></pre>
<p>This is columns: FirstName/LastName/Address. I want it to be:</p>
<pre><code>JennyMcCarthy-15SomewhereRoadSomeplaceSomePostcodeSomeCity
</code></pre>
<p>Data Factory expressions should let me do this, I tried this and it didn't seem to work:</p>
<pre><code>replace(replace(replace(FirstName+LastName+Address,' ',''),'\\r',''),'\\n','')
</code></pre>
<p>Anyone know how to do this?</p>
","<expression><azure-data-factory>","2020-09-16 20:38:25","2368","1","2","63927814","<p>Oh I figured this out looking around at what others had done, I will put the answer here for other people to use.  It looks like you have to use the regexReplace function instead:</p>
<pre><code>regexReplace(regexReplace(replace(firstname+lastname+address1,' ',''),`[\n]`,''),`[\r]`,'')
</code></pre>
<p>Note the weird quote marks in the regexReplace, yes your eyes aren't playing tricks on you, it's a <a href=""https://en.wikipedia.org/wiki/Grave_accent"" rel=""nofollow noreferrer"">grave character</a>.</p>
"
"63927629","How to replace CR and LF in Data Factory expressions","<p>There doesn't seem to be a clear answer to this, I am using a mapping data flow and a derived column expression.  I have data coming through that has spaces, carriage returns and line feeds, which I want to replace with nothing. For instance:</p>
<pre><code>Jenny Mc Carthy  -  15 Somewhere Road
Someplace SomePostcode
SomeCity
</code></pre>
<p>This is columns: FirstName/LastName/Address. I want it to be:</p>
<pre><code>JennyMcCarthy-15SomewhereRoadSomeplaceSomePostcodeSomeCity
</code></pre>
<p>Data Factory expressions should let me do this, I tried this and it didn't seem to work:</p>
<pre><code>replace(replace(replace(FirstName+LastName+Address,' ',''),'\\r',''),'\\n','')
</code></pre>
<p>Anyone know how to do this?</p>
","<expression><azure-data-factory>","2020-09-16 20:38:25","2368","1","2","74735321","<p>For me, this works to replace the newline. Note the newline (I used shift-enter) between the single quotes as the 2nd value passed into the function.</p>
<p>@replace(FirstName+LastName+Address,'<br/>
', '')</p>
<p>This does NOT work: @replace(FirstName+LastName+Address,'\n', '')</p>
<p>Nor does this: @replace(FirstName+LastName+Address,'\\n', '')</p>
<p>By the way, my data factory doesn't have the function regexReplace, so that was not an option.</p>
"
"63923010","XML validation in Azure Data Factory","<p>I am trying to use XSD validation in Azure Data Factory copy activity (with XML as source). I have an external XSD file.
I am also trying to understand how it can be used in Azure Data Factory. In the copy activity, under the Validation type section there are two options XSD and DTD. But, it does not have any way to specify any external file as XSD.
The documentation is also not clear about this.  (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-xml"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-xml</a>). This is what the official doc says:</p>
<pre><code>XML schema validation:
You can choose to not validate schema, or validate schema using XSD or DTD.
When using XSD or DTD to validate XML files, the XSD/DTD must be referred inside the XML 
files through relative path
</code></pre>
<p>I am confused about the second point here.</p>
<p>What do they mean by &quot;XML must be referred inside the XML files&quot;?</p>
","<xml><azure><xsd><azure-data-factory>","2020-09-16 15:10:57","1764","1","1","64006282","<p>I think it says, we should reference the XSD file(use relative path) in the source xml file.</p>
<p>For example:<br />
In the ADF I set a Copy activity:
<a href=""https://i.stack.imgur.com/eA16G.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eA16G.png"" alt=""enter image description here"" /></a>
Here my xml file and xsd file are in the same folder and as follow:<br />
<a href=""https://i.stack.imgur.com/2B1hT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2B1hT.png"" alt=""enter image description here"" /></a></p>
<p>In the order.xml, use <code>xsi:noNamespaceSchemaLocation=&quot;order.xsd&quot;</code> to specify the xsd file:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;  
&lt;order xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  
          xsi:noNamespaceSchemaLocation=&quot;order.xsd&quot;&gt;  
          &lt;orderItem&gt;43546533&lt;/orderItem&gt;
          &lt;orderItem&gt;53546533&lt;/orderItem&gt;
          &lt;orderItem&gt;73546533&lt;/orderItem&gt;
          &lt;orderItem&gt;93546533&lt;/orderItem&gt;
          &lt;orderItem&gt;43546533&lt;/orderItem&gt;
          &lt;orderItem&gt;73546533&lt;/orderItem&gt;
          &lt;orderItem&gt;03546533&lt;/orderItem&gt;
          &lt;orderItem&gt;33546533&lt;/orderItem&gt;
          &lt;orderItem&gt;43216533&lt;/orderItem&gt;
          &lt;orderItem&gt;1246533&lt;/orderItem&gt;
          &lt;orderItem&gt;4466533&lt;/orderItem&gt;
&lt;/order&gt;  
</code></pre>
<p>order.xsd:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;  
&lt;xsd:schema xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot;  
elementFormDefault=&quot;qualified&quot; attributeFormDefault=&quot;qualified&quot;&gt;  
    &lt;xsd:element name=&quot;order&quot;&gt;  
       &lt;xsd:complexType&gt;  
         &lt;xsd:sequence&gt;  
            &lt;xsd:element name=&quot;orderItem&quot; type=&quot;xsd:string&quot;  maxOccurs=&quot;10&quot;/&gt;  
         &lt;/xsd:sequence&gt;  
      &lt;/xsd:complexType&gt;  
    &lt;/xsd:element&gt;  
&lt;/xsd:schema&gt; 
</code></pre>
<p>In my case, when the order.xml doesn't meet my verification rules, it will fails:
<a href=""https://i.stack.imgur.com/CVhWm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CVhWm.png"" alt=""enter image description here"" /></a></p>
<p>Hope my answer is helpful to you!</p>
"
"63917126","SAP Table Connector cannot find FM in SAP ECC 6","<p>Our version of SAP (ECC6 EHP 0) does not have the FM ZRFC_READ_TABLE2. We created a copy and called it ZFM_INTERNAL_ZRFC_READ_TABLE2; however, when trying to use it from ADF with the SAP connector (version 3.0.23 64bit) we get an error.</p>
<p>Type: SAP.Middleware.Connector.RfcInvalidParameterException</p>
<p>Message: Element QUERY_TABLE of container metadata XXXXX/ZFM_INTERNAL_ZRFC_READ_TABLE2 unknown.</p>
<p>I posted this question in the GitHub feedback page (<a href=""https://github.com/MicrosoftDocs/azure-docs/issues/62650#issue-701809327"" rel=""nofollow noreferrer"">link</a>) and was told this is a better forum to answer this type of questions.</p>
<p>The FM has the QUERY_TABLE parameter and it has been tested to be working (using Python scripts).
Screenshots of the configuration and tests performed.</p>
<p><a href=""https://i.stack.imgur.com/h1zTm.png"" rel=""nofollow noreferrer"">FM Config</a></p>
<p><a href=""https://i.stack.imgur.com/91KGL.png"" rel=""nofollow noreferrer"">Test Configuration</a></p>
<p><a href=""https://i.stack.imgur.com/yewbT.png"" rel=""nofollow noreferrer"">Test Results</a></p>
<p>Any ideas where the problem could be?
Thanks in advance for your help!</p>
","<azure><azure-data-factory><sap-dotnet-connector>","2020-09-16 09:31:06","396","2","2","63924997","<p>Under Attribute tab, make sure you select &quot;Remote-Enabled Module&quot;.
<a href=""https://i.stack.imgur.com/RmYbp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RmYbp.png"" alt=""enter image description here"" /></a></p>
"
"63917126","SAP Table Connector cannot find FM in SAP ECC 6","<p>Our version of SAP (ECC6 EHP 0) does not have the FM ZRFC_READ_TABLE2. We created a copy and called it ZFM_INTERNAL_ZRFC_READ_TABLE2; however, when trying to use it from ADF with the SAP connector (version 3.0.23 64bit) we get an error.</p>
<p>Type: SAP.Middleware.Connector.RfcInvalidParameterException</p>
<p>Message: Element QUERY_TABLE of container metadata XXXXX/ZFM_INTERNAL_ZRFC_READ_TABLE2 unknown.</p>
<p>I posted this question in the GitHub feedback page (<a href=""https://github.com/MicrosoftDocs/azure-docs/issues/62650#issue-701809327"" rel=""nofollow noreferrer"">link</a>) and was told this is a better forum to answer this type of questions.</p>
<p>The FM has the QUERY_TABLE parameter and it has been tested to be working (using Python scripts).
Screenshots of the configuration and tests performed.</p>
<p><a href=""https://i.stack.imgur.com/h1zTm.png"" rel=""nofollow noreferrer"">FM Config</a></p>
<p><a href=""https://i.stack.imgur.com/91KGL.png"" rel=""nofollow noreferrer"">Test Configuration</a></p>
<p><a href=""https://i.stack.imgur.com/yewbT.png"" rel=""nofollow noreferrer"">Test Results</a></p>
<p>Any ideas where the problem could be?
Thanks in advance for your help!</p>
","<azure><azure-data-factory><sap-dotnet-connector>","2020-09-16 09:31:06","396","2","2","64265124","<blockquote>
<p>As discussed in <a href=""https://learn.microsoft.com/en-us/answers/questions/96758/data-factory-sap-table-connector-cannot-find-fm-in.html"" rel=""nofollow noreferrer"">Microsoft Q&amp;A Platform</a> the above issue can be resolved if we are specifying function module alone without the function group.</p>
</blockquote>
"
"63913186","Options to make Azure Data Factory ETL job work with changing schema of files uploaded by data providers","<p>So i have data providers who will upload files using Power Apps, than a ETL job will run and read the content of file and save to DB in cloud. I want a solution in which when the schema of the file changes (column added, removed or changed ) the ETL job will handle it itself.
Is it possible in ADF?</p>
","<azure><azure-data-factory>","2020-09-16 04:33:42","47","0","1","63914189","<p>Yes. You will use ADF Data Flows for this ETL scenario. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift</a></p>
"
"63911265","Concurrent update to delta lake table via multiple jobs","<p>I have a delta table, where multiple jobs via databricks can merge/upsert data into the delta table concurrently.</p>
<p>How can I prevent from getting <code>ConcurrentAppendException</code>?</p>
<p>I cannot use this <a href=""https://docs.databricks.com/delta/concurrency-control.html#concurrentappendexception"" rel=""nofollow noreferrer"">solution</a>, as the incoming changes can be a part of any partition and I cannot filter any partition.</p>
<p>Is there a way to check whether the Delta table is being appended/merged/updated/deleted and wait until its completed and then we acquire the locks and start the merge for the second job?</p>
<p>Just FYI, these are 2 independent Azure Datafactory jobs trying to update one delta table.</p>
<p>Cheers!</p>
","<apache-spark><azure-data-factory><databricks><azure-databricks><delta-lake>","2020-09-15 23:49:50","9869","3","1","64088540","<p>You should handle <a href=""https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html"" rel=""noreferrer"">concurrent appends to Delta</a> as any other data store with <a href=""https://martinfowler.com/eaaCatalog/optimisticOfflineLock.html"" rel=""noreferrer"">Optimistic Offline Locking</a> - by adding application-specific retry logic to your code whenever that particular exception happens.</p>
<p><a href=""https://i.stack.imgur.com/PuEkn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PuEkn.png"" alt=""enter image description here"" /></a></p>
<p>Here's a good video on <a href=""https://www.youtube.com/watch?v=F91G4RoA8is&amp;feature=youtu.be&amp;ab_channel=Databricks"" rel=""noreferrer"">inner workings of Delta</a>.</p>
"
"63909093","Azure Data Factory Data Flow source query support for FOR JSON AUTO","<p>I am trying to use below query as source for my data flow but I keep getting errors. Is the fuctionality not supported in data flow?</p>
<pre><code>SELECT  customer.customerid  AS 'customerid',
        customer.customer_fname AS 'fname',    
        customer.customer_lname AS 'lname',
        customer.customer_phone AS 'Phone',
        address.customer_addressid as 'addressid',
        address.Address_type as 'addresstype',  
        address.street1  as 'street1'
FROM customer customer  
   INNER JOIN customer_address address
     ON customer.customerid = address.customerid  
order by customer.customerid
FOR JSON AUTO, ROOT('customer')
</code></pre>
<p>I get the following error:</p>
<pre><code>Notifications
Column name needs to be specified in the query, set an alias if using a SQL function
</code></pre>
<p>ADF V2, Data Flows, Source</p>
","<azure-data-factory>","2020-09-15 20:03:09","1856","1","1","64019627","<p>The error is cause by that Data Flow Query doesn't support <code>order by</code> statement, not the 'FOR JOSN AUTO'.</p>
<p>See the error bellow:
<a href=""https://i.stack.imgur.com/K1CNG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K1CNG.png"" alt=""enter image description here"" /></a></p>
<p>Please refence Data <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#source-transformation"" rel=""nofollow noreferrer"">Flow Source transformation</a>:</p>
<p><strong>Query:</strong> If you select Query in the input field, enter a SQL query for your source. This setting overrides any table that you've chosen in the dataset. <strong>Order By clauses aren't supported here, but you can set a full SELECT FROM statement.</strong> You can also use user-defined table functions. select * from udfGetData() is a UDF in SQL that returns a table. This query will produce a source table that you can use in your data flow. Using queries is also a great way to reduce rows for testing or for lookups.</p>
<pre><code>SQL Example: Select * from MyTable where customerId &gt; 1000 and customerId &lt; 2000
</code></pre>
<p>The query work well in Copy active but false in Data Flow. You need to change the query.</p>
"
"63906910","Filter Lookup Results from values in Second Lookup in Azure Data Factory","<p>I have two lookups within an Until activity in ADF.  The first lookup (<code>BookList</code>) is a list of books that look like the JSON listed below.</p>
<pre><code>[
  {
    &quot;BookID&quot;: 1,
    &quot;BookName&quot;: &quot;Book A&quot;
  },
  {
    &quot;BookID&quot;: 2,
    &quot;BookName&quot;: &quot;Book B&quot;
  }
]
</code></pre>
<p>The second lookup is a list of books that I want to exclude from the first list (<code>ExcludedBooks</code>) which is listed below.</p>
<pre><code>[
  {
    &quot;BookID&quot;: 2,
    &quot;BookName&quot;: &quot;Book B&quot;
  }
]
</code></pre>
<p>After these two lookups, I have a Filter activity whose items are the values from <code>BookList</code> lookup.  I would like the filter condition to be based on the BookID value not being listed in the <code>ExcludedBooks</code> values, but I'm not sure how to write this condition based on the collection functions in ADF.  What I have is listed below which does not work.</p>
<pre><code>@not(contains(activity('ExcludedBooks').output.value, item().BookID))
</code></pre>
<p>I realize one way to solve this is to loop through each record of the ExcludedBooks and use a SetVariable
activity to build an array of BookIDs which WOULD work with the collection function Contains(), but ADF does not allow nested activity groups for some reason (ForEach within an Until).</p>
<p>I also cannot set the list of excluded books outside of the Until activtity as it will change with each iteration of the Until activity.  I also realize the workaround to the nested group activity restriction is to create a completely different pipeline, but that is not ideal and creates unnecessary complexity when trying to return the results.</p>
<p>Does anyone have any suggestions for how to filter the results of a lookup based on the results of another lookup?</p>
","<azure-data-factory>","2020-09-15 17:18:55","4454","2","1","63912431","<p>Below expression doesn't work because item of <code>activity('ExcludedBooks').output.value</code> is object,<code>item().BookID</code> is number.</p>
<p><code>@not(contains(activity('ExcludedBooks').output.value, item().BookID))</code></p>
<p>If your each item in <code>ExcludedBooks</code> is the same as your item in <code>BookList</code>(like your provide sample),you can use this expression:<code>@not(contains(activity('ExcludedBooks').output.value, item()))</code>.</p>
<p>My test result:</p>
<p><a href=""https://i.stack.imgur.com/5DW6W.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5DW6W.png"" alt=""enter image description here"" /></a></p>
<p>For another hand,if your item in <code>ExcludedBooks</code> like this json(<code>BookList</code> is the same as your provided):</p>
<pre><code>[
  {
    &quot;BookID&quot;: 2,
    &quot;BookName&quot;: &quot;Book B&quot;,
    &quot;num&quot;: 22
  }
]
</code></pre>
<p>you can only compare their <code>BookID</code> by using this expression:
<code>@not(contains(join(activity('ExcludedBooks').output.value,','),concat('&quot;BookID&quot;:',item().BookID,',')))</code></p>
<p>(cast <code>activity('ExcludedBooks').output.value</code> to string,concat item() in 'BookList' as <code>&quot;BookID&quot;:2,</code> and check whether 'ExcludedBooks' string  contains 'BookList' item string)</p>
<p>My test result:
<a href=""https://i.stack.imgur.com/izsjm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/izsjm.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you.</p>
"
"63905395","Mapping Dataflow vs SQL Stored Procedure in ADF pipeline","<p>I have a requirement where I need to choose between Mapping Data Flow vs SQL Stored Procedures in an ADF pipeline to implement some business scenarios. The data volume is not too huge now but might get larger at a later stage.
The business logic are at times complex where I will have to join multiple tables, write sub queries, use windows functions, nested case statements, etc.</p>
<p>All of my business requirements could be easily implemented through a SP but there is a slight inclination towards mapping data flow considering that it runs spark underneath and can scale up as required.
Does ADF Mapping data flow has an upper hand over SQL Stored Procedures when used in an ADF pipeline?
Some of the concerns that I have with the mapping data flow are as below.</p>
<ol>
<li>Time taken to implement complex logic using data flows is much more
than a stored procedure</li>
<li>The execution time for a mapping data flow is
much higher considering the time it takes to spin up the spark
cluster.</li>
</ol>
<p>Now, if I decide to use SQL SPs in the pipeline, what could be the disadvantages?
Would there be issues with the scalability if the data volume grows rapidly at some point in time?</p>
","<azure><azure-data-factory>","2020-09-15 15:43:46","1660","3","2","63906514","<p>This is kind of an opinion question which doesn't tend to do well on stackoverflow, but the fact you're comparing Mapping Data Flows with stored procs tells me that you have Azure SQL Database (or similar) <em>and</em> Azure Data Factory (ADF) in your architecture.</p>
<p>If you think about the fact Mapping Data Flows is backed by Spark clusters, and you already have Azure SQL DB, then what you really have is two types of compute.  So why have both?  There's nothing better than SQL at doing joins, nested queries etc.  Azure SQL DB can easily be scaled up and down (eg via its REST API) - that seemed to be one of your points.</p>
<p>Having said that, Mapping Data Flows is powerful and offers a nice low-code experience.  So if your requirement is to have low-code with powerful transforms then it could be a good choice.  Just bear in mind that if your data is already in a database and you're using Mapping Data Flows, that what you're doing is taking data out of SQL, up into a Spark cluster, processing it, then pushing it back down.  This seems like duplication to me, and I reserve Mapping Data Flows (and Databricks notebooks) for things I <strong>cannot already do in SQL</strong>, eg advanced analytics, hard maths, complex string manipulation might be good candidates.  Another use case might be work offloading, where you deliberately want to offload work from your db.  Just remember the cost implication of having two types of compute running at the same time.</p>
<p>I also saw an example recently where someone had implemented a slowly changing dimension type 2 (SCD2) using Mapping Data Flows but had used 20+ different MDF components to do it.  This is low-code in name only to me, with high complexity, hard to maintain and debug.  The same process can be done with a single <code>MERGE</code> statement in SQL.</p>
<p>So my personal view is, use Mapping Data Flows for things that you can't already do with SQL, particularly when you already have SQL databases in your architecture.  I personally prefer an ELT pattern, using ADF for orchestration (not MDF) which I regard as easier to maintain.</p>
<p>Some other questions you might ask are:</p>
<ul>
<li>what skills do your team have? SQL is a fairly common skill.  MDF is still low-code but niche.</li>
<li>what skills do your support team have? Are you going to train them on MDF when you hand this over?</li>
<li>how would you rate the complexity and maintainability of the two approaches, given the above?</li>
</ul>
<p>HTH</p>
"
"63905395","Mapping Dataflow vs SQL Stored Procedure in ADF pipeline","<p>I have a requirement where I need to choose between Mapping Data Flow vs SQL Stored Procedures in an ADF pipeline to implement some business scenarios. The data volume is not too huge now but might get larger at a later stage.
The business logic are at times complex where I will have to join multiple tables, write sub queries, use windows functions, nested case statements, etc.</p>
<p>All of my business requirements could be easily implemented through a SP but there is a slight inclination towards mapping data flow considering that it runs spark underneath and can scale up as required.
Does ADF Mapping data flow has an upper hand over SQL Stored Procedures when used in an ADF pipeline?
Some of the concerns that I have with the mapping data flow are as below.</p>
<ol>
<li>Time taken to implement complex logic using data flows is much more
than a stored procedure</li>
<li>The execution time for a mapping data flow is
much higher considering the time it takes to spin up the spark
cluster.</li>
</ol>
<p>Now, if I decide to use SQL SPs in the pipeline, what could be the disadvantages?
Would there be issues with the scalability if the data volume grows rapidly at some point in time?</p>
","<azure><azure-data-factory>","2020-09-15 15:43:46","1660","3","2","69935191","<p>One disadvantage with using SP's in your pipeline, is that your SP will run directly against the database server. So if you have any other queries/transactions or jobs running against the DB at the same time that your SP is executing you may experience longer run times for each (depending on query complexity, records read, etc.). This issue could compound as data volume grows.</p>
<p>We have decided to use SP's in our organization instead of Mapping Data Flows. The cluster spin up time was an issue for us as we scaled up. To address the issue I mentioned previously with SP's, we stagger our workload, and schedule jobs to run during off-peak hours.</p>
"
"63902221","SAP OData Service Filter is not filtering","<p>I'm learning to get delta data from SAP Fiori sample gateway to Azure SQL by using Azure Data Factory and filter feature on OData service.
I'm using OData Service that exposed by Fiori sample, and one of the table sample is PurchaseOrders.</p>
<p>I tried like this:</p>
<blockquote>
<p>$filter=ChangedAt ge datetime '2020-09-08T22:00:00'</p>
</blockquote>
<p>But it is still return all the records.</p>
<p>I found sap:filterable is false at metadata
<a href=""https://i.stack.imgur.com/RGrjT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RGrjT.png"" alt=""enter image description here"" /></a></p>
<p>Is that filterable false is made me cannot filter this?</p>
<p>Is there any other way to do delta extraction on OData rather than using filter?</p>
<p>Thank You</p>
","<odata><azure-data-factory><sap-fiori><sap-gateway>","2020-09-15 12:50:30","832","1","1","67215600","<p>As @Boghyon wrote above the &quot;sap:filterable&quot; is just an Annotation, which can help to build the UI. You have to check the DPC_EXT class's GET_ENTITY_SET method of the entity type that you try to filter. If filtering isn't implemented then (1.) in case of standard service you're more or less stuck (it cannot be filtered for a reason) (2.) in case of custom service you can implement filtering</p>
"
"63899060","The length of execution output is over limit (around 4MB currently)","<p>Calling Rest Api in webactivity using azure a=data factory. the activity fails with the error &quot;The length of execution output is over limit (around 4MB currently)&quot;. Any solution for this??</p>
","<azure><azure-data-factory>","2020-09-15 09:35:06","2285","0","1","63906216","<p>Unfortunately this error is due to a hard limit (4MB) on the length of execution output.  Possible workaround is to restrict the size of the output or use <a href=""https://learn.microsoft.com/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">pagination</a> in the response.</p>
<p>But feel free to submit your feedback to ADF product team through ADF user voice forum. All the feedback shared in this forum are reviewed and monitored by ADF engineering team.</p>
<p>ADF feedback forum: <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory</a></p>
<p>Please feel free reach us out in <a href=""https://learn.microsoft.com/answers/products/azure?product=all"" rel=""nofollow noreferrer"">Microsoft Q&amp;A</a> forum for such Azure product related queries.</p>
"
"63897383","ADF Mapping Data Flow byNames expression exception","<p>Azure Data Factory(ADF) Mapping Data Flow byNames expression is throwing an exception in the derived column block. Actually I need to access multiple columns values in a single derived column.</p>
<pre><code>toString(byNames(['parent', 'child']))
Exception: DF-TX-115 - Variable results are allowed in assignments - EXE-0001,[390 436 536 677],
           Dataflow cannot be analyzed as a graph,
</code></pre>
<p><a href=""https://i.stack.imgur.com/Fr1kY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fr1kY.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#bynames"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#bynames</a></p>
","<azure><azure-data-factory>","2020-09-15 07:49:12","2752","0","1","63898861","<p>I tried the expression and  get the same error:
<a href=""https://i.stack.imgur.com/lhW9P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lhW9P.png"" alt=""enter image description here"" /></a></p>
<p>Just from Data Flow, we could use bellow expression to achieve that:</p>
<p><a href=""https://i.stack.imgur.com/j9ivp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j9ivp.png"" alt=""enter image description here"" /></a></p>
<ol>
<li><p><code>array(byNames(['parent','child']))</code>
<a href=""https://i.stack.imgur.com/cLZnw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cLZnw.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p><code>array(parent,child)</code>
<a href=""https://i.stack.imgur.com/WaYGg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WaYGg.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>toString function only can convert a primitive datatype to a string, I think that the reason which cause the error.</p>
"
"63893928","Dynamically refer to Json value in Data Factory copy","<p>I have ADF CopyRestToADLS activity which correctly saves json complex object to Data Lake storage. But I additionally need to pass one of the json values (myextravalue) to a stored procedure.  I tried referencing it in the stored procedure parameter as <code>@{activity('CopyRESTtoADLS').output.myextravalue</code> but I am getting error</p>
<p>The actions CopyRestToADLS refernced by 'inputs' in the action ExectuteStored procedure1 are not defined in the template</p>
<pre><code>{
    &quot;items&quot;: [1000 items],
    &quot;count&quot;: 1000,
    &quot;myextravalue&quot;: 15983444
}
</code></pre>
<p>I would like to try to dynamically reference this value because the CopyRestToADLS source REST dataset dynamically calls different REST endpoints so the structure of JSON object is different each time. But the myextravalue is always present in each JSON call.</p>
<p>How is it possible to refernce myextravalue and use it as a parameter?</p>
<p>Rich750</p>
","<json><azure-data-factory>","2020-09-15 01:50:01","669","0","1","63897787","<p>You could create another lookup active on REST data source to get the json value. Then pass it to the Stored Procedure active.</p>
<p>Yes, it will create a new REST request, and it seams to be an easy way to achieve your purpose. Lookup active to get the content of the source and won't save it.</p>
<p>The another solution may be get the value from the copy active output file, after the copy active completed.</p>
<p>I'm glad you solved it by this way:</p>
<p>&quot;I created a Data Flow to read from the folder where Copy Activity saves dynamically named output json filenames. After importing schema from sample file, I selected the myextravalue as the only mapping in the Sink Mapping section.&quot;</p>
"
"63892447","Azure DataFactory: Get status Message from an api","<p>How can I get the status message from an api, I am calling it from a web activity in azure datafactory but the answer it gives me is: error 2108 which corresponds to an error message from Azure and I need a response from 201 or 504 depending on the case.</p>
<p>Thanks for your help</p>
","<azure-data-factory>","2020-09-14 22:14:19","381","0","1","63912325","<p>we are not able to get the Http status code.<br />
The infomation we can get from the web activity is as follows:<br />
<a href=""https://i.stack.imgur.com/hhljj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hhljj.png"" alt=""enter image description here"" /></a>
In my case, the first four properties are from my rest api response. You can define a customer code property in your web api and write to your json response. So you can get your customer code.</p>
"
"63883266","Azure Data Factory Convert Date to YYYYMMDD then to String","<p>I'm new to data factory so please bear with me.</p>
<p>I have files names coming in via metadata activity and I need to filter on all files that contain in the filename todays data in format YYYYMMDD</p>
<p>File format example: <code>DailyStock20200914.csv</code></p>
<p>In my filter I have a <code>@contains(item().name, '20200914')</code> which works.</p>
<p>I want to add an expression something like</p>
<pre><code>@contains(item().name, toString(toDate(utcnow(),'YYYYMMDD')))
</code></pre>
<p>but can't workout the date part and can't track down the answer on the internet.</p>
","<azure><azure-data-factory>","2020-09-14 11:21:01","1264","0","1","63893544","<p>Congratulations you figured it out with the expression bellow:</p>
<pre><code>@contains(item().name, utcnow(yyyyMMdd))
</code></pre>
<p>I help post it as answer and this can be beneficial to other community members. Thank you.</p>
"
"63882819","Create Folder Based on File Name in Azure Data Factory","<p>I have a requirement to copy few files from an ADLS Gen1 location to another ADLS Gen1 location, but have to create folder based on file name.</p>
<p>I am having few files as below in the source ADLS:</p>
<pre><code>ABCD_20200914_AB01_Part01.csv.gz
ABCD_20200914_AB02_Part01.csv.gz
ABCD_20200914_AB03_Part01.csv.gz
ABCD_20200914_AB03_Part01.json.gz
ABCD_20200914_AB04_Part01.json.gz
ABCD_20200914_AB04_Part01.csv.gz
</code></pre>
<p><strong>Scenario-1</strong>
I have to copy these files into destination ADLS as below with only csv file and create folder from file name (If folder exists, copy to that folder) :</p>
<pre><code>AB01-
    |-ABCD_20200914_AB01_Part01.csv.gz
AB02-
    |-ABCD_20200914_AB02_Part01.csv.gz
AB03-
    |-ABCD_20200914_AB03_Part01.csv.gz
AB04-
    |-ABCD_20200914_AB04_Part01.csv.gz
</code></pre>
<p><strong>Scenario-2</strong>
I have to copy these files into destination ADLS as below with only csv and json files and create folder from file name (If folder exists, copy to that folder):</p>
<pre><code>AB01-
    |-ABCD_20200914_AB01_Part01.csv.gz
AB02-
    |-ABCD_20200914_AB02_Part01.csv.gz
AB03-
    |-ABCD_20200914_AB03_Part01.csv.gz
    |-ABCD_20200914_AB03_Part01.json.gz
AB04-
    |-ABCD_20200914_AB04_Part01.csv.gz
    |-ABCD_20200914_AB04_Part01.json.gz
</code></pre>
<p>Is there any way to achieve this in Data Factory?
Appreciate any leads!</p>
","<azure><azure-data-lake><azure-data-factory>","2020-09-14 10:50:41","3096","0","1","63887488","<p>So I am not sure if this will entirely help, but I had a similar situation where we have 1 zip file and I had to copy those files out into their own folders.</p>
<p>So what you can do is use parameters in the datasink that you would be using, plus a variable activity where you would do a substring.</p>
<p>The job below is more for the delta job, but I think has enough stuff in it to hopefully help. My job can be divided into 3 sections.</p>
<p><a href=""https://i.stack.imgur.com/lieX4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lieX4.png"" alt=""enter image description here"" /></a></p>
<p>The first Orange section gets the latest file name date from ADLS gen 1 folder that you want to copy.</p>
<p>It is then moved to the orange block. On the bottom I get the latest file name based on the ADLS gen 1 date and then I do a sub-string where I take out the date portion of the file. In your case you might be able to do an array and capture all of the folder names that you need.</p>
<p>Getting file name
<a href=""https://i.stack.imgur.com/Okrux.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Okrux.png"" alt=""enter image description here"" /></a></p>
<p>Getting Substring
<a href=""https://i.stack.imgur.com/kTNq9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kTNq9.png"" alt=""enter image description here"" /></a></p>
<p>On the top section I get first extract and unzip that file into a test landing zone.</p>
<p>Source
<a href=""https://i.stack.imgur.com/pKdyx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pKdyx.png"" alt=""enter image description here"" /></a></p>
<p>Sink
<a href=""https://i.stack.imgur.com/4na2j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4na2j.png"" alt=""enter image description here"" /></a></p>
<p>I then get the names of all the files that were in that zip file to them be used in the ForEach Activity. These file names will then become folders for the copy activity.</p>
<p>Get File names from initial landing zone:
<a href=""https://i.stack.imgur.com/wNQF3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wNQF3.png"" alt=""enter image description here"" /></a></p>
<p>I then pass on those childitems from &quot;Get list of staged files&quot; into ForEach:</p>
<p><a href=""https://i.stack.imgur.com/fyl9R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fyl9R.png"" alt=""enter image description here"" /></a></p>
<p>In that ForEach activity I have one copy activity. For that I made to datasets. One to grab the files from the initial landing zone that we have created. For this example lets call it Staging (forgive the ms paint drawing):</p>
<p><a href=""https://i.stack.imgur.com/JpGh1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JpGh1.png"" alt=""enter image description here"" /></a></p>
<p>The purpose of this is to go to that dummy folder and grab each file that was just copied into there. From that 1 zip file we expect 5 files.</p>
<p>In the Sink section what I did is create a new dataset with a parameter for folder and file name. In that dataset I have am putting that data into same container, but created a new folder called &quot;Stage&quot; and concatenated it with the item name. I also added a &quot;replace&quot; command to remove the &quot;.txt&quot; from the file name.</p>
<p><a href=""https://i.stack.imgur.com/8RIn7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8RIn7.png"" alt=""enter image description here"" /></a></p>
<p>What this will do then is what ever the file name that is coming from that dummy staging it will then have a folder name specifically for each file. Based on your requirements I am not sure if that is what you want to do, but you can always rework that to be more specific.</p>
<p>For Item name I basically get the same file name, then replace the &quot;.txt&quot;, concat the name of the date value, and only after that add the &quot;.txt&quot; extension. Otherwise I would have had to &quot;.txt&quot; in the file name.</p>
<p>In the end I have created a delete activity that will then be used to delete all the files (I am not sure if have set that up properly so feel free to adjust obviously).</p>
<p><a href=""https://i.stack.imgur.com/RBL5I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RBL5I.png"" alt=""enter image description here"" /></a></p>
<p>Hopefully the description above gave you an idea on how to use parameters for your files. Let me know if this helps you in your situation.</p>
"
"63881789","How can I handle Message=The data type SqlDecimal is not supported. in Azure DWH v2","<p>I'm facing difficulties solving the error message below received from the Azure datafactory v2 while tying to run a pipeline to copy a csv to a SQL table</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=DataTypeNotSupported,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The data type SqlDecimal is not supported.,Source=,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy Invervence Blob to SQL from csv&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>What I've tried so far is changing the original .xslx file to a .CSV to eliminate possible formatting issues. In the mapping settings, when I delete the 'BEDRAG_2020' column from the copy task in the pipeline works fine, see snippet below.
<a href=""https://i.stack.imgur.com/7tGOX.png"" rel=""nofollow noreferrer"">Snippet of data preview</a></p>
<p>Can someone help me troubleshooting this error?</p>
","<sql><azure><pipeline><azure-data-factory>","2020-09-14 09:38:03","454","0","1","63883023","<p>Answer to self:</p>
<p>For unknown reasons the error appears in cases where the string value in another column exceeds the allocated varchar length. This affects the decimal type column in such way the Azure pipeline terminates. If someone knows some more details that rases this error please leave a comment.</p>
"
"63879198","Azure Data Factory Unable to convert string column value to Datetime","<p>I am using a existing ADF pipeline to copy data from a blob to sql. My source data has a column value like '20000101'. I am getting error when ADF try to convert this to dateTime format from string. I am using json translators like &quot;dateTimeFormat&quot;: &quot;yyyy-MM-dd HH:mm:ss.fff&quot;. But still end up getting error as &quot;Exception occured when converting the value '20000101' cannot convert string to datetime type.</p>
<p>Any suggession?</p>
<p>Note: I have to handle this only through json mapping. Cannot edit the existing pipeline. Any possibilities here?</p>
","<azure><azure-data-factory>","2020-09-14 06:35:21","4331","1","1","63883189","<p>No matter the data in blob or  NFS in on prem, Data Factory copy active can not convert '20000101' to datetime. Even we set it as DateTime column.</p>
<pre><code>&quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=TypeConversionFailure,Exception occurred when converting value '20000101' for column name 'dd' from type 'String' (precision:, scale:) to type 'DateTime' (precision:, scale:). Additional info: String was not recognized as a valid DateTime.&quot;
</code></pre>
<p>To achieve that, we must do some data conversion in data flow With <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">DerivedColumn</a>:</p>
<p><a href=""https://i.stack.imgur.com/S8RtS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S8RtS.png"" alt=""enter image description here"" /></a></p>
<p>Expression:</p>
<pre><code>toTimestamp(toDate(concat(substring(dd,0, 4), '-',substring(dd,5, 2),'-',substring(dd,7, 2))))
</code></pre>
<p><a href=""https://i.stack.imgur.com/8Pbtc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Pbtc.png"" alt=""enter image description here"" /></a></p>
<p>Then it could be mapping to your sink table.</p>
"
"63876890","Azure Data Factory - GUID Look Up","<p>I am working on a SQL to CRM copy data mapping. I want to either update an account record if it exists or insert a new one.</p>
<p>In CRM the accound_id GUID field is the primary key. I want to be able to look that up in CRM if it exists or generate a new one.</p>
<p>I think I am already generating one correctly:</p>
<pre><code>SELECT
CAST(NULL as uniqueidentifier) AS ACCOUNTID,
A.VENDORACCOUNTNUMBER AS ACCOUNTNO,
11 AS ACCOUNTTYPE,
VENDORORGANIZATIONNAME AS LEGALACCOUNTNAME,
VENDORSEARCHNAME AS ACCOUNTNAME,
CURRENCYCODE AS CURRENCY,
DEFAULTDELIVERYTERMSCODE AS DELIVERYTERMS,
DEFAULTDELIVERYMODEID AS MODEOFDELIVERY,
CASHDISCOUNTCODE AS CASHTERMS,
DEFAULTPAYMENTTERMSNAME AS PAYMENTTERMS,
'YES' AS ISPRIMARY,
'BILL TO' AS ADDRESSPURPOSE,
A.ADDRESSDESCRIPTION AS BILLTONAME,
A.ADDRESSCOUNTRYREGIONID AS BILLTOCOUNTRY,
A.ADDRESSCOUNTYID AS BILLTOCOUNTY,
A.ADDRESSCITY AS BILLTOCITY,
A.ADDRESSSTATEID AS BILLTOSTATE,
A.ADDRESSSTREET AS BILLTOST,
A.ADDRESSZIPCODE AS BILLTOZIP,
A.FORMATTEDPRIMARYADDRESS AS BILLTOADDRESS,
PRIMARYPHONENUMBER AS BILLTOPHONE,
PRIMARYPHONENUMBERDESCRIPTION AS BILLTOPHONEDESC,
'199 - CORPORATE' AS DISTRICTNAME,
'f97564c6-93a1-e311-9696-6c3be5a8ce94' AS DISTRICT
FROM [dbo].[VendVendorV2Staging] A
WHERE A.ADDRESSLOCATIONROLES = 'Business';
</code></pre>
<p>What I don't know is how to look up a GUID if it exits or use a new one.</p>
","<dynamics-crm><crm><azure-data-factory>","2020-09-14 00:53:13","881","0","2","63877617","<p>You have to configure an alternate key in CRM side for account entity, then you can do upsert (update if exists or else create) based on that key. <a href=""https://learn.microsoft.com/en-us/answers/questions/36657/adf-v2-define-keys-for-upsert-behavior.html"" rel=""nofollow noreferrer"">Read more </a></p>
<p>If you don’t want do that, then you can always query the CRM and check for existence before create a new record based on some unique attribute like name, phone number or email.</p>
"
"63876890","Azure Data Factory - GUID Look Up","<p>I am working on a SQL to CRM copy data mapping. I want to either update an account record if it exists or insert a new one.</p>
<p>In CRM the accound_id GUID field is the primary key. I want to be able to look that up in CRM if it exists or generate a new one.</p>
<p>I think I am already generating one correctly:</p>
<pre><code>SELECT
CAST(NULL as uniqueidentifier) AS ACCOUNTID,
A.VENDORACCOUNTNUMBER AS ACCOUNTNO,
11 AS ACCOUNTTYPE,
VENDORORGANIZATIONNAME AS LEGALACCOUNTNAME,
VENDORSEARCHNAME AS ACCOUNTNAME,
CURRENCYCODE AS CURRENCY,
DEFAULTDELIVERYTERMSCODE AS DELIVERYTERMS,
DEFAULTDELIVERYMODEID AS MODEOFDELIVERY,
CASHDISCOUNTCODE AS CASHTERMS,
DEFAULTPAYMENTTERMSNAME AS PAYMENTTERMS,
'YES' AS ISPRIMARY,
'BILL TO' AS ADDRESSPURPOSE,
A.ADDRESSDESCRIPTION AS BILLTONAME,
A.ADDRESSCOUNTRYREGIONID AS BILLTOCOUNTRY,
A.ADDRESSCOUNTYID AS BILLTOCOUNTY,
A.ADDRESSCITY AS BILLTOCITY,
A.ADDRESSSTATEID AS BILLTOSTATE,
A.ADDRESSSTREET AS BILLTOST,
A.ADDRESSZIPCODE AS BILLTOZIP,
A.FORMATTEDPRIMARYADDRESS AS BILLTOADDRESS,
PRIMARYPHONENUMBER AS BILLTOPHONE,
PRIMARYPHONENUMBERDESCRIPTION AS BILLTOPHONEDESC,
'199 - CORPORATE' AS DISTRICTNAME,
'f97564c6-93a1-e311-9696-6c3be5a8ce94' AS DISTRICT
FROM [dbo].[VendVendorV2Staging] A
WHERE A.ADDRESSLOCATIONROLES = 'Business';
</code></pre>
<p>What I don't know is how to look up a GUID if it exits or use a new one.</p>
","<dynamics-crm><crm><azure-data-factory>","2020-09-14 00:53:13","881","0","2","63909452","<p>You can use <a href=""https://learn.microsoft.com/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy"" rel=""nofollow noreferrer"">Additional column feature</a> in ADF copy activity to generate a guild value column as shown below.</p>
<p><a href=""https://i.stack.imgur.com/00vnI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/00vnI.png"" alt=""enter image description here"" /></a></p>
<p>As Dynamics CRM supports Upsert, you can set the <code>write behavior</code> of your sink settings as <code>Upsert</code> and <code>Ignore Null values</code> as <code>True</code> as shown below. You can leave the <code>alternate Key</code> as empty if primary key is used (<a href=""https://learn.microsoft.com/dynamics365/customerengagement/on-premises/developer/define-alternate-keys-entity"" rel=""nofollow noreferrer"">Define alternate keys for an entity</a> -  An alternate key is used to uniquely identify a record in Customer Engagement in place of the primary key).</p>
<p>And then map the <code>AdditionalGuidColumn</code> to the destination upsert column i.e., <code>accound_id</code> GUID field which is the primary key.</p>
<p><a href=""https://i.stack.imgur.com/TlMFd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TlMFd.png"" alt=""enter image description here"" /></a></p>
<p>For additional info about Dynamics sink settings, please refer to this doc: <a href=""https://learn.microsoft.com/azure/data-factory/connector-dynamics-crm-office-365#dynamics-as-a-sink-type"" rel=""nofollow noreferrer"">Dynamics as a sink type</a></p>
<p><a href=""https://i.stack.imgur.com/KtiFQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KtiFQ.png"" alt=""enter image description here"" /></a></p>
"
"63875844","Azure Data Factory - Pipeline Copy from SQL to CRM - Error","<p><a href=""https://i.stack.imgur.com/HN0tD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HN0tD.png"" alt=""enter image description here"" /></a>I am currently working on a Azure Data Factory Mapping.</p>
<p>SQL Server to CRM</p>
<p>There is a Entity Reference look up field that I am trying to update. Do not know how to. The field is: new_districtlink --&gt; Territory/Territories</p>
<pre><code>using JSON we use:
{\&quot;new_districtlink@odata.bind\&quot;:\&quot;/territories(c8a29516-4001-e311-98f3-78e3b5089b9d)\&quot;}&quot;
</code></pre>
<p>Don't know how to reference this field in AZURE DATA FACTORY so it updates correctly. Currently I have tried Mapping using newdistrictlink@EntityReference</p>
<p>I get the following error:</p>
<blockquote>
<p>&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;Failure happened on 'Sink' side. ErrorCode=UserErrorDynamicsOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Dynamics operation failed with error code: -2147220891, error message: The real-time workflow named &quot;ACCOUNT: Pull in Region and Division from District(Territory)&quot; failed with the error &quot;For a Vendor Account Type, select 199 as the district.&quot;.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,''Type=System.ServiceModel.FaultException`1[[Microsoft.Xrm.Sdk.OrganizationServiceFault, Microsoft.Xrm.Sdk, Version=9.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35]],Message=The creator of this fault did not specify a Reason.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;CMA CRM VENDOR&quot;,
&quot;details&quot;: []
}</p>
</blockquote>
<p>The error I am getting is in Azure Data Factory. The JSON was just an example of how we are doing it in C# currently.</p>
<p>I am setting District as GUID and Mapping District to new_districtlink@EntityReference.</p>
<pre><code>SELECT
ACCOUNTID,
'2EC67250-E3C4-E311-ABE1-6C3BE5BDA9AC' AS DISTRICT
FROM [dbo].[VendVendorV2Staging]
WHERE ADDRESSLOCATIONROLES = 'Business';
</code></pre>
","<azure><dynamics-crm><crm><azure-data-factory>","2020-09-13 21:42:00","880","0","1","63875980","<p>I see there’s no issue with Json payload you used, it is correct to map an EntityReference (lookup).</p>
<p>I guess this is data validation message coming back from CRM Real time workflow which is validating the territory (district) against the created account type.</p>
<p>The error message clearly says - the value you passed (guid in district link Json payload for territory) is not expected value based on business logic validation in real time workflow. For vendor type account, 199 is the expected district and find out the guid of that territory record and update the Json.</p>
<p>Or change the account type to something else than vendor. Then it should work, or check with CRM dev team or BA for business rules before implementing and testing ADF integration for creating the account record.</p>
"
"63874155","Microsoft Integration Runtime losing Authentication Key on Azure VM","<p>I currently have an Azure Virtual Machine that is running Windows Server 2019 Datacenter that I use as a connection point for Azure Data Factory. The Integration Runtime works and so do the pipelines that utilize it.</p>
<p>The issue arises when the Virtual Machine is shut down at night and started back up in the morning using Runbooks. <a href=""https://i.stack.imgur.com/oAMvY.png"" rel=""nofollow noreferrer"">Screenshot</a>. &quot;The Authentication Key is invalid or empty. Specify a valid Authentication Key from the portal&quot;</p>
<p>This does not always happen. The Integration Runtime can work for a few days and then it stops with the error above. It seems like the only solution is to re-install the Integration Runtime and re-do the linked services in Azure Data Factory.</p>
<p>I have spent some time trying to figure out what could be causing this error and it seems a little weird that it can work for several days before the error comes up. Has anyone else encountered a similar issue?</p>
","<azure><azure-virtual-machine><azure-data-factory>","2020-09-13 18:24:13","1970","0","1","63875702","<p>This typically happens when the server clock (windows VM in this case with self-hosted IR running) time is not in sync with the remote server (the ADF service).</p>
<p>As a prerequisite, you should sync the clock on the Windows VM with the internet or update this yourself before you register the IR again.</p>
"
"63866007","Trigger Jupyter Notebook in Azure ML workspace from ADF","<p>How do I trigger a notebook in my Azure Machine Learning notebook workspace from Azure Data Factory</p>
<p>I want to run a notebook in my Azure ML workspace when there are changes to my Azure storage account.</p>
","<azure-data-factory><azure-machine-learning-service>","2020-09-13 00:12:01","1607","3","2","63878663","<p>This feature is currently supported by Azure ML Notebooks. You can also use <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-trigger-published-pipeline"" rel=""nofollow noreferrer"">Logic apps</a> to trigger a run of your Machine Learning pipeline when there are changes to your Azure storage account.</p>
"
"63866007","Trigger Jupyter Notebook in Azure ML workspace from ADF","<p>How do I trigger a notebook in my Azure Machine Learning notebook workspace from Azure Data Factory</p>
<p>I want to run a notebook in my Azure ML workspace when there are changes to my Azure storage account.</p>
","<azure-data-factory><azure-machine-learning-service>","2020-09-13 00:12:01","1607","3","2","63889711","<p>My understanding is that your use case is 100% valid and it is currently possible with the <code>azureml-sdk</code>. It requires that you create the following:</p>
<ol>
<li>Create an Azure ML Pipeline. Here's a <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-your-first-pipeline"" rel=""nofollow noreferrer"">great introduction</a>.</li>
<li>Add a <code>NotebookRunnerStep</code> to your pipeline. <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/3c9cb89c1ada8626f496d364c3d1b1bdf78bbdb3/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"" rel=""nofollow noreferrer"">Here is a notebook demoing the feature</a>. I'm not confident that this feature is still being  maintained/supported, but IMHO it's a valid and valuable feature. I've opened <a href=""https://github.com/Azure/MachineLearningNotebooks/issues/1146"" rel=""nofollow noreferrer"">this issue</a> to learn more</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-trigger-published-pipeline"" rel=""nofollow noreferrer"">Create a trigger using Logic apps</a> to run your pipeline anytime a change in the datastore is detected.</li>
</ol>
<p>There's certainly a learning curve to Azure ML Pipelines, but I'd argue the payoff is in the flexibility you get in composing steps together and easily scheduling and orchestrating the result.</p>
"
"63861942","Where to specify REST API parameter for GET method in ADF Web Activity","<p>Lets say we have the following URL</p>
<p><a href=""https://management.someurl.com?api-version=2020-06-01"" rel=""nofollow noreferrer"">https://management.someurl.com?api-version=2020-06-01</a></p>
<p>Now I have a list of parameters for the REST API call. The method I am trying to call using the web activity is GET -- so there is no body section there. So where do I exactly specify these parameters? Header is clearly not meant for this purpose as these are specific application level API parameters. In case of POST calls I could successfully specify the parameters in the body section of the web activity.</p>
","<azure-data-factory>","2020-09-12 15:30:00","3141","0","2","63875648","<p>You should create dataset parameters.</p>
<p>In this case, create a base URL parameter which will have the base URL (i.e. the management.someURL.com URL)</p>
<p>Create a relative URL parameter which will have details of the query parameters. Use dynamic expressions in dataset configuration to create this query string.</p>
<p>Alternatively, you can follow <a href=""https://datasavvy.me/2020/01/30/parameterizing-a-rest-api-linked-service-in-data-factory/"" rel=""nofollow noreferrer"">this</a> blog to understand the design process.</p>
"
"63861942","Where to specify REST API parameter for GET method in ADF Web Activity","<p>Lets say we have the following URL</p>
<p><a href=""https://management.someurl.com?api-version=2020-06-01"" rel=""nofollow noreferrer"">https://management.someurl.com?api-version=2020-06-01</a></p>
<p>Now I have a list of parameters for the REST API call. The method I am trying to call using the web activity is GET -- so there is no body section there. So where do I exactly specify these parameters? Header is clearly not meant for this purpose as these are specific application level API parameters. In case of POST calls I could successfully specify the parameters in the body section of the web activity.</p>
","<azure-data-factory>","2020-09-12 15:30:00","3141","0","2","63877611","<p>Add to @Raunak Jhawar's answer.
You can define some variables previously, as follows:
<a href=""https://i.stack.imgur.com/jXujn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jXujn.png"" alt=""enter image description here"" /></a></p>
<p>Then you can add dynamic content to the <strong>URL</strong> field.</p>
<pre><code>@concat(variables('BaseUrl'),concat(variables('Path'),concat('?productId=',variables('QueryParam'))))
</code></pre>
<p><a href=""https://i.stack.imgur.com/vI13o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vI13o.png"" alt=""enter image description here"" /></a></p>
<p>The input shows:</p>
<p><a href=""https://i.stack.imgur.com/ZISkf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZISkf.png"" alt=""enter image description here"" /></a></p>
"
"63856333","Azure Factory v2 Wildcard","<p>I am trying to create a new dataset in ADF that looks for csv files that meet a certain naming convention. These files are located within a series of different folders in my Azure Blob Storage.</p>
<p>For instance, in the sample directory below, I am trying to pull out csv files that contain the word &quot;cars&quot;.</p>
<pre><code>Folder A 
    fastcars.csv 
    fasttrucks.csv
Folder B
   slowcars.csv
   slowtrucks.csv
</code></pre>
<p>Ideally , I would end up with the files &quot;slowcars.csv&quot; and &quot;fastcars.csv&quot;. I've seen examples out there were people were able to wildcard the file name. I have been playing around with that, but have had no luck. (See image below for one example of what I have been doing).
<a href=""https://i.stack.imgur.com/veX8c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/veX8c.png"" alt=""ee"" /></a></p>
<p>Is what I am trying to do even possible? Would appreciate any advice you guys may have. Please let me know if I can provide further clarification.</p>
","<azure-data-factory><azure-data-lake-gen2>","2020-09-12 02:39:19","69","1","1","63877069","<p>According to the description of filename in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#dataset-properties"" rel=""nofollow noreferrer"">documentation</a>,</p>
<blockquote>
<p>The file name under the given fileSystem + folderPath. If you want to
use a wildcard to filter files, skip this setting and specify it in
activity source settings.</p>
</blockquote>
<p>so you need to specify it in activity not in file path.</p>
<p>A easy sample in copy activity:
<a href=""https://i.stack.imgur.com/s35Ks.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s35Ks.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you.</p>
"
"63852696","How to connect to Salesforce Einstein using Azure Data Factory?","<p>I am trying to Integrate SalesForce Einstein (Sink) to Azure DataFactory (Source), Could anyone share knowledge article on this ?</p>
","<azure-data-factory><salesforce-einstein>","2020-09-11 18:48:03","199","0","1","63877070","<p>Data Factory only support SalesForce, Salesforce Service Cloud and Salesforce Marketing Cloud. SalesForce Einstein is not supported for now, we can not integrate it to Data Factory.</p>
<p>Please reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Azure Data Factory connector overview</a></p>
"
"63852633","untar files in Azure Functions App using 7zip","<p>Is it possible to extract files from .tar.gz using Azure Data Factory or Functions App to be ingested by ETL process in ADF?</p>
<p>I tried to use 7zip in Functions Apps, which worked fine to extract a test .tar.gz file uploaded to the Functions App itself but throws an error for files stored in Blob container.</p>
<p>Here's my command in run.ps1 in Functions</p>
<pre><code>.\7za.exe x 1.tar.gz
</code></pre>
<p>#below, the file is a blob in Blob Container - doesn't work</p>
<pre><code>.\7za.exe x $InputBlob
</code></pre>
<p>The error I receive for the command above</p>
<blockquote>
<p>2020-09-11T16:06:42.721 [Error] ERROR: Program '7za.exe' failed to run: StandardOutputEncoding is only supported when standard output is redirected.At D:\home\site\wwwroot\tools\run.ps1:9 char:1+ .\7za.exe e $InputBlob+ ~~~~~~~~~~~~~~~~~~~~~~.Exception :Type : System.Management.Automation.ApplicationFailedExceptionErrorRecord :Exception :Type : System.Management.Automation.ParentContainsErrorRecordExceptionMessage : Program '7za.exe' failed to run: StandardOutputEncoding is only supported when standard output is redirected.At D:\home\site\wwwroot\tools\run.ps1:9 char:1+ .\7za.exe e $InputBlob+ ~~~~~~~~~~~~~~~~~~~~~~.HResult : -2146233087CategoryInfo : ResourceUnavailable: (:) [], ParentContainsErrorRecordExceptionFullyQualifiedErrorId : NativeCommandFailedInvocationInfo :ScriptLineNumber : 9OffsetInLine : 1HistoryId : -1ScriptName : D:\home\site\wwwroot\tools\run.ps1Line : .\7za.exe e $InputBlobPositionMessage : At D:\home\site\wwwroot\tools\run.ps1:9 char:1+ .\7za.exe e $InputBlob+ ~~~~~~~~~~~~~~~~~~~~~~PSScriptRoot : D:\home\site\wwwroot\toolsPSCommandPath</p>
</blockquote>
","<azure-data-factory><azure-function-app>","2020-09-11 18:42:48","670","0","1","64219845","<blockquote>
<p>Unfortunately there is no out-of-box functionality in Azure Data Factory to extract contents from TAR file.</p>
</blockquote>
<p>Here is an existing user voice feature request thread, I would encourage you to please up-vote and/or comment on the feature request suggestion to increase the priority of feature implementation.</p>
<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/34575520-support-extracting-contents-from-tar-file"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/34575520-support-extracting-contents-from-tar-file</a></p>
<p>But as a workaround you could try using the extensibility features of Azure Data Factory to transform files that aren't supported. Two options include Azure Functions and custom tasks by using Azure Batch (Custom Activity in ADF).</p>
<p>Your <a href=""https://learn.microsoft.com/en-us/answers/questions/92973/extract-files-from-targz-files-store-in-blob-conta.html"" rel=""nofollow noreferrer"">question</a> is already answered on Microsoft Q&amp;A platform.</p>
"
"63848724","Mixed properties in 'source' column/fields in Azure Data Factory","<p>I'm using Azure Data Factory to copy data from a source folder (Azure Blob) that has multiple folders inside it (and each one of those folders has a year as it's name, and inside the folders are the Excel spreadsheets with the data) to a SQL Server table. I want to iterate through the folders, select the folder name, and insert the name into a column in a table, so for each data read inside the files in the folder, the folder name where this data is will be in the table, like this:</p>
<pre><code>Data 1   |Data 2   |Year
------------------------
A        |abc      |2020
B        |def      |2020
C        |ghi      |2021
D        |jkl      |2022
E        |lmn      |2023
</code></pre>
<p>My pipeline is like this:
<img src=""https://i.stack.imgur.com/RxeAN.png"" alt=""1"" /></p>
<p>I have a Get Metadata activity called Get Metadata1 <img src=""https://i.stack.imgur.com/CYLkY.png"" alt=""2"" /> pointing to the folders, and after that a ForEach to iterate through the folders <img src=""https://i.stack.imgur.com/K1Vw3.png"" alt=""3"" /> with two activities: one &quot;Set variable&quot; activity setting a variable named FolderYear <img src=""https://i.stack.imgur.com/kB3rG.png"" alt=""4"" /> with @item().name as value (to select the folder name) and a Copy activity which creates a additional column <img src=""https://i.stack.imgur.com/Wm9Mh.png"" alt=""5"" /> into the dataset named Year using the variable.</p>
<p>I'm trying to map the additional Year column to a column in the table, but when I debug the pipeline, the following error appears:</p>
<pre><code>{ &quot;errorCode&quot;: &quot;2200&quot;, &quot;message&quot;: &quot;Mixed properties are used to reference 'source' columns/fields in copy activity mapping. Please only choose one of the three properties 'name', 'path' and 'ordinal'. The problematic mapping setting is 'name': 'Year', 'path': '','ordinal': ''. &quot;, &quot;failureType&quot;: &quot;UserError&quot;, &quot;target&quot;: &quot;Copy data1&quot;, &quot;details&quot;: [] }
</code></pre>
<p>It's possible to insert the folder name which I'm currently iterating into a database column?</p>
","<azure><azure-data-factory>","2020-09-11 14:08:31","1756","0","1","63877154","<p>I've made a same test and copied the data(include the folder name) into a SQL table successfully.<br />
I have two folders in the container and each folder contains one cvs file for test.
<a href=""https://i.stack.imgur.com/UTYQC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UTYQC.png"" alt=""enter image description here"" /></a></p>
<p>The previous settings are the same as you.<br />
Inside the <strong>ForEach</strong> activity, I use the <strong>Additional columns</strong> to add the folder name to the datasource.</p>
<p><a href=""https://i.stack.imgur.com/5xKF1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5xKF1.png"" alt=""enter image description here"" /></a></p>
<p>After copied into a SQL table, the results show as follow:
<a href=""https://i.stack.imgur.com/BDMNY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BDMNY.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>Update:</strong><br />
My file structure is as follows:
<a href=""https://i.stack.imgur.com/twWLc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/twWLc.png"" alt=""enter image description here"" /></a>
You can use expression <code>@concat('FolderA/FolderB/',item().name)</code>:
<a href=""https://i.stack.imgur.com/fAxow.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fAxow.png"" alt=""enter image description here"" /></a></p>
"
"63843485","Capture Service Principal's Name instead of it's ID in TSQL, Azure Database","<p>I am capturing user names in an Azure Database table column using:</p>
<pre><code>ALTER TABLE [***] ADD  DEFAULT original_login() FOR [ChangedBy]
</code></pre>
<p>The problem is that when using Service Principal used by Azure Data Factory i am catching it's ID instead of Name.
Anyone knows how to catch it's name instead?</p>
<p>So I record: <code>r2s1as1d6-9317-4226-ah89-8...</code> instead of it's name from <code>CREATE USER [User Name] FROM EXTERNAL PROVIDER;</code></p>
<p>Would appreciate help here</p>
<p>Best!</p>
","<sql><tsql><azure-data-factory><service-principal>","2020-09-11 08:28:53","50","1","1","63989680","<p>As far as I am aware, this can not achieve with using Service Principal. You can change to use SQL authentication.</p>
"
"63841759","Why does SSIS on Prem require its own Azure SSIS IR (Proxy)?","<p>I have a linked service from Azure data factory to onprem SQL server.
The linked service is using a Self-Hosted IR.
Why on earth do I need an Azure-SSIS Proxy to connect to the SSIS? What am I gaining??
Why can't it use the Self-Hosted IR?</p>
<p>The communication is working, but I just need to understand why.</p>
<p>THanks</p>
","<azure><ssis><azure-data-factory>","2020-09-11 06:22:15","127","1","1","63843241","<p>If data movement uses Data Factory copy activity, it does not need Azure SSIS integration runtime.
If data movement logic is inside SSIS package, and uses Execute SSIS package activity, it needs Azure SSIS integration runtime, and self-hosted IR as proxy.</p>
"
"63840429","Azure Data Factory V2 exploring pipeline dependencies","<p>I am working with quite a lot of pipelines, and with that involves a lot of dependencies between pipelines.</p>
<p>This isn't ideal for a couple reasons:</p>
<ul>
<li>It gets harder to know if you change something in one pipeline, what other pipelines could be affected</li>
<li>Being able to document the overall data factory structure</li>
</ul>
<p>Ideally I should be able to &quot;select&quot; a random pipeline and be able to know what pipelines dependencies it has for both before and after execution.</p>
<p>I was thinking about using the Data Factory SDK's to try and build the dependency structure of all my pipelines. But thought I would chuck this out there to see if anyone has discovered any solutions for this, or have any ideas before going down a rabbit hole.</p>
<p>I appreciate any advice.</p>
<p>Cheers, Brendan</p>
","<azure><azure-data-factory>","2020-09-11 03:32:23","753","1","2","63842679","<p>Brendan, our ADF is connected to git and so when I need to know what will be affected if I change the pipeline with say name <code>somePipelineName</code>, I goto git bash and type out</p>
<pre><code>grep --color=always -4 &quot;somePipelineName&quot; * 
</code></pre>
<p>on the pipelines folder</p>
<p>This helps me find all places from where the pipeline may be called.</p>
<p><strong>Update: 2020-09-17</strong></p>
<p>I noticed today that we now have the related pipelines listing</p>
<p><a href=""https://i.stack.imgur.com/buVHb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/buVHb.png"" alt=""enter image description here"" /></a></p>
"
"63840429","Azure Data Factory V2 exploring pipeline dependencies","<p>I am working with quite a lot of pipelines, and with that involves a lot of dependencies between pipelines.</p>
<p>This isn't ideal for a couple reasons:</p>
<ul>
<li>It gets harder to know if you change something in one pipeline, what other pipelines could be affected</li>
<li>Being able to document the overall data factory structure</li>
</ul>
<p>Ideally I should be able to &quot;select&quot; a random pipeline and be able to know what pipelines dependencies it has for both before and after execution.</p>
<p>I was thinking about using the Data Factory SDK's to try and build the dependency structure of all my pipelines. But thought I would chuck this out there to see if anyone has discovered any solutions for this, or have any ideas before going down a rabbit hole.</p>
<p>I appreciate any advice.</p>
<p>Cheers, Brendan</p>
","<azure><azure-data-factory>","2020-09-11 03:32:23","753","1","2","75891223","<p>using Az.Datafactory module (in Powershell...)
with a particular pipeline object from Get-AzDataFactoryV2Pipeline</p>
<p>$azDFPipelines = Get-AzDataFactoryV2Pipeline -ResourceGroupName = &quot;$azRG&quot; -DataFactoryName = &quot;$AzADFName&quot;</p>
<p>the &quot;Activities&quot; property can be expanded, as can its DependsOn property:</p>
<p>$azDFPipelines[X] | select ADFName -ExpandProperty Activities | select adfName, name, description -expandproperty DependsOn</p>
<p>ADFName: name of the pipeline
Name:  name of the activity/object in the pipeline
Description: description of the activity/object in the pipeline
DependsOn: data from the activity/object's dependencies (e.g., the objects it's &quot;connected&quot; to).</p>
<p>I've got a script that does this, and runs the output thru out-gridview. From there, I can then add different criteria fields to help look for stuff throughout the entire pipeline collection on my &quot;server&quot;. Kinda helpful, if not really very user-friendly.</p>
<pre><code>import-module Az.Accounts
import-module Az.DataFactory
$azAcct = Connect-AzAccount -subscription 'your_subscription_name'
#$azRgName, $azDFName are &quot;empirically determined&quot;
enter code here
$azDFPipelines = Get-AzDataFactoryV2Pipeline -ResourceGroupName &quot;$azRGname&quot; -DataFactoryName &quot;$azDFName&quot; 
###need to coerce the Name property to ADFName because it's also a member of the Activities object/property...
$adf = $azDFPipelines | select-object @{N='ADFName';E={$_.name}},`
@{N='Activities';E={$_.Activities}}

Next-level would be making it essentially walk the tree in a given pipeline or from a pipeline's specific pipeline object, and spit out graphviz &quot;dot&quot; or Mermaid graph language .md (or .vsdx...)

###expand Activities, and also select just a few properties from Activities:
$adflist = $adf | select ADFName -ExpandProperty Activities | select ADFName, name, description, notebookpath, additionalproperties
$adfList | out-gridview
</code></pre>
<p>Next-level would be making it essentially walk the tree in a given pipeline or from a pipeline's specific pipeline object, and spit out graphviz &quot;dot&quot; or Mermaid graph language .md (or .vsdx...)</p>
<p>###expand Activities, and also select just a few properties from Activities:
$adflist = $adf | select ADFName -ExpandProperty Activities | select ADFName, name, description, notebookpath, additionalproperties
$adfList | out-gridview</p>
<p>the DependsOn can be extracted from the additionalproperties collection...</p>
<p>the AdditionalProperties has the name of the &quot;next&quot; thing to be run in it, what it is, etc.</p>
<p>Like SSIS, execution flow through a ADF pipeline is &quot;parallel&quot; and are invoked non-deterministically, unless they're connected to each other serially. same goes for connected objects - they're invoked &quot;in parallel&quot; unless they're connected in series.</p>
"
"63835605","Is there a way to use dynamic SQL in a view?","<p>I have 5 tables and view pairs.</p>
<p>Example:</p>
<ul>
<li>Table <code>dynamics.Customer</code> (100 columns)</li>
<li>View <code>staging.CustomerView</code> (20 columns)</li>
</ul>
<p>The <code>CustomerView</code> is built from <code>staging.Customer</code> table and other staging tables, not <code>dynamics.Customer</code>.</p>
<p>I want to compare all rows with all fields that exists in <code>staging.CustomerView</code> to <code>dynamics.Customer</code> to see if changes has occurred to staging. I want this to show as a column in my <code>staging.CustomerView</code>.</p>
<p>Since I have 5 tables and view pairs I tried to create a function with dynamic SQL that got all rows from staging view and compare it to dynamics table. Then I called that function from each view. I got an error executing the function, as it used <code>sp_executesql</code>.</p>
<p>Is there a way to create a function that dynamically gets each column and compares it? The view is used in Azure Data Factory to Copy data into Dynamics365 and because of performance, I only want to insert/update the rows that has changed.</p>
","<sql><tsql><azure-sql-database><azure-data-factory>","2020-09-10 18:23:37","102","0","1","63836574","<p>Views are strongly typed, and cannot be created dynamically.</p>
<p>you can write trigger which records changes to another log table on updates and query that table.</p>
"
"63814546","In Data Factory, how do you access a property where the name contains an '@'?","<p>Using this sample JSON list:</p>
<pre><code>   &quot;value&quot;: [
        {
            &quot;@microsoft.graph.downloadUrl&quot;: &quot;http://anyurl1&quot;
        },
        {
            &quot;@microsoft.graph.downloadUrl&quot;: &quot;http://anyurl2&quot;
        }
    ]
</code></pre>
<p>Within Data Factory, I have a ForEach activity which is assigned to loop through each 'value', so in this case that is two loops. My question is, how do I access the value assigned to @microsoft.graph.downloadUrl? I have tried the following:</p>
<pre><code>@item().@microsoft.graph.downloadUrl 
</code></pre>
<p>But I get the following error:</p>
<p><em>The expression 'item().@microsoft.graph.downloadUrl' is not valid: the string character '@' at position '7' is not expected.</em></p>
<p>I understand the '@' is a problem because it denotes to Data Factory to evaluate an expression, therefore I am looking for a way to escape this character.</p>
","<azure-data-factory>","2020-09-09 15:08:24","173","0","1","63821151","<p>Please try this:</p>
<pre><code>@item()['@microsoft.graph.downloadUrl']
</code></pre>
<p>Hope this can help you.</p>
"
"63810474","Azure Data Factory Mapping Data Flow - Azure Managed Instance is NO longer valid as Connector?","<p>I noticed all my datasets from Azure Managed Instance are NO longer available on any Mapping Data Flow activity. And I am pretty sure they were working on few weeks back (before Sep 2020). After some research on Internet, I can only see a new document was released by Microsoft on 31/08/2020 and it is indicating the MI is NOT valid as Connector for Azure Data Factory, Mapping Data Flow. The document is in here, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-overview</a> .</p>
<p>Is anyone having the same issue on this? Or was there an announcement from Microsoft on this change?  Any information would be really helpful.</p>
","<azure><azure-data-factory>","2020-09-09 11:14:18","78","0","1","63818437","<p>SQL MI in data flows will soon be available as a public preview. You can use it in private preview until then. We'll just need your Azure Subscription ID to enable it for you.</p>
<p>Can you click on the ADF feedback link at the top of the ADF UI and submit a feedback request stating your request to enable SQL MI as private preview with your Azure sub ID?</p>
<p>Please include my name on there: Mark Kromer</p>
<p><a href=""https://i.stack.imgur.com/WzxcX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WzxcX.png"" alt=""enter image description here"" /></a></p>
"
"63809695","Azure Data Factory Set Parameter with SQL Query","<p>What is the alternative to SSIS' <code>Execute SQL Task</code> in ADF?</p>
<p>I've created a Pipeline parameter called <code>ExtractDate</code> (i know there isn't a date datatype option so I'm using a string datatype here) that I want to populate with the result of a SQL Query and then pass this into other pipelines.</p>
<p>I might be searching for the wrong terms but there doesn't seem to be many tutorials on how to write a SQL query within <code>dynamic content</code> to populate a paramater.</p>
<p>Any examples would be appreciated</p>
","<parameters><azure-data-factory><execute-sql-task>","2020-09-09 10:27:45","5805","2","2","63821760","<p>Data Factory has the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Stored Procedure activity</a> can help us execute the stored procedure in Azure SQL or SQL Server. Or we also could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup active</a> to get the SQL query result.</p>
<p>When the Azure SQL /SQL Server as the source, Data Factory supports query operation.</p>
<p>But for the pipeline parameter, we only can pass the value form the pipeline parameter to pipeline active, we can't pass or set  the value from the inner active result to parameter. That means It's impossible to populate with the result of a SQL Query and then pass this into other pipelines.</p>
"
"63809695","Azure Data Factory Set Parameter with SQL Query","<p>What is the alternative to SSIS' <code>Execute SQL Task</code> in ADF?</p>
<p>I've created a Pipeline parameter called <code>ExtractDate</code> (i know there isn't a date datatype option so I'm using a string datatype here) that I want to populate with the result of a SQL Query and then pass this into other pipelines.</p>
<p>I might be searching for the wrong terms but there doesn't seem to be many tutorials on how to write a SQL query within <code>dynamic content</code> to populate a paramater.</p>
<p>Any examples would be appreciated</p>
","<parameters><azure-data-factory><execute-sql-task>","2020-09-09 10:27:45","5805","2","2","63837707","<p>You can make this work using a variable if you are calling one pipeline from within another pipeline, but you can't pass values between pipelines which are peers of one another.</p>
<p>You cannot change a pipeline parameter from within the pipeline.</p>
<p><a href=""https://i.stack.imgur.com/xwhuT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xwhuT.png"" alt=""OK"" /></a></p>
<p><a href=""https://i.stack.imgur.com/xUNU7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xUNU7.png"" alt=""No good"" /></a></p>
<p>For the first situation which will work, here are the details...
<a href=""https://i.stack.imgur.com/pcfX0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pcfX0.png"" alt=""Lookup"" /></a></p>
<p><a href=""https://i.stack.imgur.com/5maWC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5maWC.png"" alt=""Set Variable"" /></a></p>
<p><a href=""https://i.stack.imgur.com/FQj0q.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FQj0q.png"" alt=""Pass to Pipeline"" /></a></p>
"
"63809094","POSTing form data using Azure data factory web activity returns different results to C# script","<p>I am getting to the stage of hair pulling with this one, I'm hoping someone can see if I'm doing anything wrong.</p>
<p>I'm trying to POST some form data to website using Azure data factory web activity however whilst I get a response (I get the page and some headers) it is different to the response I get if I make the <em><strong>exact</strong></em> same request using C# and HttpClient code. I've used fiddler to view the request being post'd using my C# script and according to the request information given in data factory they are exactly the same - so same headers, same content format etc...</p>
<p>This POST request is to login to a website which has a custom login mechanism, so no OAuth or anything like that unfortunately. It is supposed to return a cookie, which it does if I use my C# script, but if I make the same POST request using data factory web activity then I get different html sent back (it just returns the same login screen) and also different set of response headers in the &quot;ADFWebActivityResponseHeaders&quot; part of the activity output!?! See below for what is returned in the web activity output response headers:-</p>
<pre><code>&quot;ADFWebActivityResponseHeaders&quot;: {
    &quot;Pragma&quot;: &quot;no-cache&quot;,
    &quot;Vary&quot;: &quot;Accept-Encoding&quot;,
    &quot;X-Frame-Options&quot;: &quot;DENY&quot;,
    &quot;Cache-Control&quot;: &quot;no-store, must-revalidate, no-cache, post-check=0, pre-check=0&quot;,
    &quot;Date&quot;: &quot;Wed, 09 Sep 2020 08:09:30 GMT&quot;,
    &quot;Server&quot;: &quot;Microsoft-IIS/8.5&quot;
}
</code></pre>
<p>If I do this via C# I also get a 'Set-Cookie' as well (strangely if I make a 'GET' request for the homepage of this site I do get a 'Set-Cookie' in the response!!!), but never when doing this via data factory. I'm struggling to see how this is possible unless data factory is modifying my request in some fashion? Below is my C# code, pretty simple/standard:-</p>
<pre><code>var handler = new HttpClientHandler();
handler.CookieContainer = new CookieContainer();
handler.UseCookies = true;
handler.UseDefaultCredentials = false;

// Create our http client which will perform our web requests
var HttpClient = new HttpClient(handler);
HttpClient.BaseAddress = new Uri(&quot;**REMOVED**&quot;);

// Some of the extracts take a LONG time, so set the timeout for default of 30mins
HttpClient.Timeout = TimeSpan.FromMinutes(30);

// Set the 'form' parameters we're going to POST to the server in the request
var parameters = new Dictionary&lt;string, string&gt;
{                
    { &quot;username&quot;, &quot;**REMOVED**&quot; },
    { &quot;password&quot;, &quot;**REMOVED**&quot; }
};

// URL encode the parameters
var content = new FormUrlEncodedContent(parameters);

// Submit our POST with the parameters
var response = await HttpClient.PostAsync(&quot;**REMOVED**&quot;, content);
</code></pre>
<p>Running this code and using fiddler I see the following request with headers, these are the only headers:-</p>
<pre><code>Content-Length: 80 
Content-Type: application/x-www-form-urlencoded

username=REMOVED&amp;password=REMOVED
</code></pre>
<p>and in the 'input' side of the web activity is the details of the request, I've added the headers in the web activity and these are correct:-</p>
<pre><code>&quot;method&quot;: &quot;POST&quot;,
&quot;headers&quot;: {
    &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;,
    &quot;Content-Length&quot;: 80
},
&quot;body&quot;: &quot;username=REMOVED&amp;password=REMOVED&quot;
</code></pre>
<p>Note that in the data factory I'm using a self hosted integration runtime as this website blocks addresses that do not come from the specific IP addresses used externally by our on-prem network/firewall. I know that is not the problem as I'm getting a response with the normal login page from the site (if I use the Azure integration runtime I get a denied response).</p>
<p>Here is a screen shot of the web activity in data factory:-</p>
<p><a href=""https://i.stack.imgur.com/kHg7N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kHg7N.png"" alt=""data factory web activity"" /></a></p>
<p>Really hope someone out there can see what I'm missing or whatever...</p>
","<c#><azure><web><cookies><azure-data-factory>","2020-09-09 09:50:34","2226","1","1","66970849","<p>Turns out this does work and will list the cookies in the JSON output from the activity as shown below (note this is to be found in the output of the ADF activity, so you would pick up the cookie from the output a bit like... <strong>@activity('Login and get cookie').output.ADFWebActivityResponseHeaders[&quot;Set-Cookie&quot;]</strong> )</p>
<p><a href=""https://i.stack.imgur.com/YJJki.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YJJki.png"" alt=""enter image description here"" /></a></p>
<p>However, in my case the url I was POSTing to was responding with a 302 (moved temporarily) but the 'Location' header which should be there is not in the ADFWebActivityResponseHeaders - which is why I missed it. I tried using Chrome with the developer tools and looked at the response directly which is where I found the 302 response code. After that, I just used the new URL given in the response headers (i.e. the url in the 'Location') that I found when using the browser dev tools.</p>
<p>Unfortunately at the time of writing, the Azure data factory HTTP activity does not follow redirects (and doesn't list all the response headers either!) so if anyone encounters the same problem they will need to <em><strong>manually</strong></em> find out and get the url's for any redirects. In other words, try using a tool like browser/postman and look at the response if it doesn't work in ADF... you might find there is a redirect going on :-)</p>
<p>There is a feature request logged for this <a href=""https://feedback.azure.com/d365community/idea/4112c6e7-7226-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">here</a>, be sure to add your vote :)</p>
<ul>
<li>edited to update the Azure feedback change of URL after MS decided to change things on the feedback site!?!</li>
</ul>
"
"63807676","Azure Data Factory DataFlow Filter is taking a lot of time","<p>I have an ADF Pipleline which executes a DataFlow.
The Dataflow has Source A table which has around 1 Million Rows,
Filter which has a query to select only yesterday's records from the source table,
Alter Row settings which uses upsert,
Sink which is archival table where the records are getting upsert</p>
<p>This whole pipeline is taking around 2 hours or so which is not acceptable. Actually, the records being transferred / upserted are around 3000 only.</p>
<p>Core count is 16. Tried the partitioning with round robin and 20 partitions.
Similar archival doesn't take more than 15 minutes for another table which has around 100K records.</p>
<p>I thought of creating source which would select only yesterday's record but the dataset we can select only table.</p>
<p>Please suggest if I am missing anything to optimize it.</p>
","<azure><azure-data-factory>","2020-09-09 08:26:36","317","1","1","63837813","<p>The table of the Data Set really doesn't matter.  Whichever activity you use to access that Data Set can be toggled to use a query instead of the whole table, so that you can pass in a value to select only yesterday's data from the database.</p>
<p>Or course, if you have the ability to create a stored procedure on the source, you could also do that.</p>
<p>When migrating really large sets of data, you'll get much better performance using a Copy activity to stage the data into an Azure Storage Blob before using another Copy activity to pull from that Blob into the source.  But, for what you're describing here, that doesn't seem necessary.</p>
"
"63805156","Writing in sub directory - Azure Mapping Data Flow","<p>In my Data flow, the sink is ADLS.
My source files are present in SoureDump/Data  and I am reading from path SoureDump/Data. I am doing few transformations and I am trying write the output files into SoureDump/Rawzone.
Output file name is created from the data.
When I trigger the pipeline, the output files are generated as expected but are written in the parent directory SoureDump.</p>
<p>My work:</p>
<p><a href=""https://i.stack.imgur.com/pibTX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pibTX.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/0dlXb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0dlXb.png"" alt=""enter image description here"" /></a></p>
<p>DataSet screenshot</p>
<p><a href=""https://i.stack.imgur.com/Q09gw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q09gw.png"" alt=""DataSet screenshot"" /></a></p>
<p>Please let me know if I have given anything wrong.</p>
<p>Thanks.</p>
","<azure><azure-data-factory>","2020-09-09 05:16:40","157","0","1","63805481","<p>&quot;As data in column&quot; defaults back to your dataset container object. Check the info bubble next to the column name field:</p>
<p><a href=""https://i.stack.imgur.com/F7VAV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F7VAV.png"" alt=""enter image description here"" /></a></p>
<p>Just set your target folder with a Derived Column prior to your sink and append the value like this:</p>
<p>tgt_file_name_w_path = '/mypath/output/'+tgt_file_name</p>
<p>Then use tgt_file_name_w_path instead in your column with file name property.</p>
"
"63800915","Azure Synapse - Incremental Data Load","<p>We load data from on-prem database servers to <em>Azure Data Lake Storage Gen2</em> using <em>Azure Data Factory</em> and <em>Databricks</em> store them as parquet files. Every run, we get only get the new and modified data from last run and UPSERT into existing parquet files using databricks <em><strong>merge</strong></em> statement.</p>
<p>Now we are trying to move this data from parquet files <em>Azure Synapse</em>. Ideally, I would like to do this.</p>
<ul>
<li>Read incremental load data into a external table. (CETAS or COPY
INTO)</li>
<li>Use above as staging table.</li>
<li>Merge staging table with production       table.</li>
</ul>
<p>The problem is merge statement is not available in Azure Syanpse. Here is the solution Microsoft <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?toc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Ftoc.json&amp;bc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Fbreadcrumb%2Ftoc.json&amp;view=azure-sqldw-latest#l-use-ctas-to-simplify-merge-statements"" rel=""nofollow noreferrer"">suggests</a> for incremental load</p>
<pre class=""lang-sql prettyprint-override""><code>    CREATE TABLE dbo.[DimProduct_upsert]
    WITH
    (   DISTRIBUTION = HASH([ProductKey])
    ,   CLUSTERED INDEX ([ProductKey])
    )
    AS
    -- New rows and new versions of rows
    SELECT      s.[ProductKey]
    ,           s.[EnglishProductName]
    ,           s.[Color]
    FROM      dbo.[stg_DimProduct] AS s
    UNION ALL  
    -- Keep rows that are not being touched
    SELECT      p.[ProductKey]
    ,           p.[EnglishProductName]
    ,           p.[Color]
    FROM      dbo.[DimProduct] AS p
    WHERE NOT EXISTS
    (   SELECT  *
        FROM    [dbo].[stg_DimProduct] s
        WHERE   s.[ProductKey] = p.[ProductKey]
    )
    ;
    
    RENAME OBJECT dbo.[DimProduct]          TO [DimProduct_old];
    RENAME OBJECT dbo.[DimProduct_upsert]  TO [DimProduct];
</code></pre>
<p>Basically dropping and re-creating the production table with CTAS. Will work fine with small dimenstion tables, but i'm apprehensive about large fact tables with 100's of millions of rows with indexes. Any suggestions on what would be the best way to do incremental loads for really large fact tables. Thanks!</p>
","<azure><azure-sql-database><azure-data-factory><azure-synapse>","2020-09-08 20:08:56","4039","1","1","63876224","<p>Till the time SQL MERGE is officially supported, the recommended way fwd to update target tables is to use T SQL insert/update commands between the delta records and target table.</p>
<p>Alternatively, you can also use Mapping Data Flows (in ADF) to emulate SCD transactions for dimensional/fact data load.</p>
"
"63795346","Update after a Copy Data Activity in Azure Data Factory","<p>I've got this doubt in Azure Data Factory. My pipeline has a copy data activity, and after loading the information in the table I need to update a field in that destination based on a parameter. It is a simple update, but given that we do not have a SQL task (present in SSIS) I do not what to use. Create a SP for this does not seem to be the most appropriate solution, besides, modify the database is complicated. I thought the option &quot;Use Query&quot; in the Lookup activity could be a solution, but this does not allow me to create a SQL query with a parameter, just like in a Source.</p>
<p><a href=""https://i.stack.imgur.com/1Sg0p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Sg0p.png"" alt=""enter image description here"" /></a></p>
<p>What could be a possible workaround?</p>
","<sql><azure><azure-data-factory>","2020-09-08 13:49:48","2966","0","1","63799293","<p>You are on the right track with the Lookup.  That is definitely the way to go. The query field there will allow you to create dynamic SQL just like you did within the copy activity.  You just need to reference the variable/parameter properly.</p>
<p>Also, with the Lookup, it will always expect something returned.  You don't have to do anything with that returned value.  Just ignore it, but the Lookup will not work without returning something. So, that query field would contain something like:</p>
<pre><code>UPDATE dbo.MyTable SET IsComplete = 1 WHERE RunId = @{pipeline().parameters.runId};
SELECT 0 AS DummyValue; -- Necessary for Lookup to work
</code></pre>
<p><a href=""https://i.stack.imgur.com/sJJC0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sJJC0.png"" alt=""enter image description here"" /></a></p>
"
"63795120","How to execute SQL Query dynamically in Data Flows Activity","<p>I tried creating a parameter and stored the sql query in that paramter. And used that parameter in dynamic content. But i am not able to achieve this requirement. Can anyone help me out in this context.</p>
<p>(i am calling Data flows activity after a lookup activity)</p>
","<azure><azure-data-factory>","2020-09-08 13:37:49","366","1","1","63798480","<p>I'm not sure about your exact example, perhaps you can reply with a few specifics? Generally, speaking, you would pass in a string parameter to your data flow and use that parameter as &quot;dynamic content&quot; in the Source transformation's query property:</p>
<p><a href=""https://i.stack.imgur.com/esju0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/esju0.png"" alt=""enter image description here"" /></a></p>
<p>In my super-simple sample, I just put my string in double-quotes as &quot;select * from movies&quot;:</p>
<p><a href=""https://i.stack.imgur.com/19SmA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/19SmA.png"" alt=""enter image description here"" /></a></p>
"
"63791120","Dynamics cloud RetrieveRecordChangeHistory Data factory","<p>Any good ideas how to get record changes from Dynamics with data factory ?</p>
<p>This is as far as I have gotten:</p>
<p>Using the Dynamics connector and this is as far as I have gotten with googling</p>
<pre><code>@concat('RetrieveRecordChangeHistory(Target=@Target)?@Target={%22accountid%22:%0000-000-000-0000-00000000%22,%22@odata.type%22:%22Microsoft.Dynamics.CRM.account%22}')
</code></pre>
<p>but can't for some reason get the data out.</p>
<p>Has someone made it work ?</p>
<p>has anyone done something like that with data factory?</p>
","<azure><azure-data-factory><microsoft-dynamics>","2020-09-08 09:35:12","136","1","1","63883908","<p>There’s no native way of interacting with Dynamics CRM/CDS REST web api from Azure data factory pipeline.</p>
<p>But we can setup the AAD app registration to get auth token, CRM application user and ADF web activity to hit the CRM web api successfully. <a href=""https://crmchap.co.uk/interacting-with-the-dynamics-365-common-data-service-web-api-from-azure-data-factory/"" rel=""nofollow noreferrer"">Read more </a></p>
"
"63781415","Remove specific columns using Azure Data Factory","<p>I have set of files in ADLS. Number of columns varies in each file.</p>
<p>File 1 will have the columns : Row_Number, Col_A, Col_B, null</p>
<p>File 2 will have the columns : Row_Number, Col_1, Col_2, Col_3 , null</p>
<p>File 3 will have the columns : Row_Number, Col_A1, Col_A2, null</p>
<p>There will be more than 50 files in my ADLS.
I would need to remove first and last column in each file or to be specific , I need to remove columns which has columns names as Row_Number and null.</p>
<p>Can any help me what should be my expressions in Derived column / Select.</p>
<p>Thanks for your help.</p>
","<azure><azure-data-factory>","2020-09-07 16:44:37","4205","2","1","63784059","<p>I would use a Select rule with this for the matching condition:</p>
<p>name != 'Row_Number' &amp;&amp; name != 'null' &amp;&amp; left(name,2) != '_c'</p>
<p>and this for the output column name: $$</p>
<p><a href=""https://i.stack.imgur.com/FIfRO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FIfRO.png"" alt=""enter image description here"" /></a></p>
"
"63778287","Content-type header not being sent from web activity in ADF v2","<p>We use a web activity in ADF v2 to interact with the Azure Automation Account API (Job creation) based on this specification <a href=""https://learn.microsoft.com/en-us/rest/api/automation/job/create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/automation/job/create</a></p>
<p>Since 09-04-2020 the PUT web call isn't working because the header &quot;Content-Type : application/json&quot; is not being sent. The pipeline that contain the activity hasn't changed in months, just stopped working suddenly with the error:</p>
<p>Operation on target Automation - AAS Processing failed:</p>
<pre><code>{
    &quot;code&quot;: &quot;UnsupportedMediaType&quot;,
    &quot;message&quot;: &quot;{\&quot;Message\&quot;:\&quot;The request entity's media type 'text/plain' is not supported for this resource.\&quot;}&quot;
}
</code></pre>
<p>But the input request for the pipeline, does include the header:</p>
<pre><code>{
    &quot;url&quot;: &quot;https://management.azure.com/subscriptions/xxxxx/resourceGroups/xxxxx/providers/Microsoft.Automation/automationAccounts/xxxxx/jobs/55a1bfa2-340c-4120-ba42-43df9e9f4e14?api-version=2017-05-15-preview&quot;,
    &quot;method&quot;: &quot;PUT&quot;,
    &quot;headers&quot;: {
        &quot;Content-type&quot;: &quot;application/json&quot;
    },
    &quot;body&quot;: &quot;{\&quot;properties\&quot;:{\&quot;runbook\&quot;:{\&quot;name\&quot;:\&quot;aas-masterdata-management\&quot;},\&quot;parameters\&quot;:{\&quot;Masterdata_Table\&quot;:\&quot;RLS\&quot;},\&quot;runOn\&quot;:\&quot;\&quot;}}&quot;,
    &quot;authentication&quot;: {
        &quot;type&quot;: &quot;MSI&quot;,
        &quot;resource&quot;: &quot;https://management.azure.com&quot;
    }
}
</code></pre>
<p>I've tried so far:</p>
<ul>
<li>Recreate the pipeline</li>
<li>Test in a different ADF instance</li>
<li>Delete and redeploy all the pipelines</li>
<li>Delete the header</li>
<li>Change the header to lowercase, uppercase, etc</li>
<li>Add the header twice</li>
<li>Use a self-hosted integration runtime</li>
<li>Test in Debug mode</li>
</ul>
<p>Any of these tests have been successful. Just for confirmation, I've run the same call from Postman and from the rest api debug tool included within the Api documentation, both worked perfectly, if I set the content header to text/plain in postman or in the web tool I get exactly the same error than in ADF v2. It seems that something has changed in the web call activity of ADF v2 that hardcodes the content header as &quot;text/plain&quot; somehow.</p>
<p>Is someone facing this same issue ? As I said, pipeline definition hasn't changed in months, just stopped working few days ago.</p>
","<azure-data-factory>","2020-09-07 13:10:51","1507","0","2","63808490","<p>Answer from the ADF platform team:</p>
<p>I got update from Product team that using &quot;Content-Type&quot; instead of &quot;Content-type&quot; will resolve this issue. This can be referred in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">documentation</a></p>
<p>It seems that they accepted &quot;Content-type&quot; before, as it was working several months with that config, but not anymore. So if someone experiences the same problem, here is the answer.</p>
<p>Thanks.</p>
"
"63778287","Content-type header not being sent from web activity in ADF v2","<p>We use a web activity in ADF v2 to interact with the Azure Automation Account API (Job creation) based on this specification <a href=""https://learn.microsoft.com/en-us/rest/api/automation/job/create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/automation/job/create</a></p>
<p>Since 09-04-2020 the PUT web call isn't working because the header &quot;Content-Type : application/json&quot; is not being sent. The pipeline that contain the activity hasn't changed in months, just stopped working suddenly with the error:</p>
<p>Operation on target Automation - AAS Processing failed:</p>
<pre><code>{
    &quot;code&quot;: &quot;UnsupportedMediaType&quot;,
    &quot;message&quot;: &quot;{\&quot;Message\&quot;:\&quot;The request entity's media type 'text/plain' is not supported for this resource.\&quot;}&quot;
}
</code></pre>
<p>But the input request for the pipeline, does include the header:</p>
<pre><code>{
    &quot;url&quot;: &quot;https://management.azure.com/subscriptions/xxxxx/resourceGroups/xxxxx/providers/Microsoft.Automation/automationAccounts/xxxxx/jobs/55a1bfa2-340c-4120-ba42-43df9e9f4e14?api-version=2017-05-15-preview&quot;,
    &quot;method&quot;: &quot;PUT&quot;,
    &quot;headers&quot;: {
        &quot;Content-type&quot;: &quot;application/json&quot;
    },
    &quot;body&quot;: &quot;{\&quot;properties\&quot;:{\&quot;runbook\&quot;:{\&quot;name\&quot;:\&quot;aas-masterdata-management\&quot;},\&quot;parameters\&quot;:{\&quot;Masterdata_Table\&quot;:\&quot;RLS\&quot;},\&quot;runOn\&quot;:\&quot;\&quot;}}&quot;,
    &quot;authentication&quot;: {
        &quot;type&quot;: &quot;MSI&quot;,
        &quot;resource&quot;: &quot;https://management.azure.com&quot;
    }
}
</code></pre>
<p>I've tried so far:</p>
<ul>
<li>Recreate the pipeline</li>
<li>Test in a different ADF instance</li>
<li>Delete and redeploy all the pipelines</li>
<li>Delete the header</li>
<li>Change the header to lowercase, uppercase, etc</li>
<li>Add the header twice</li>
<li>Use a self-hosted integration runtime</li>
<li>Test in Debug mode</li>
</ul>
<p>Any of these tests have been successful. Just for confirmation, I've run the same call from Postman and from the rest api debug tool included within the Api documentation, both worked perfectly, if I set the content header to text/plain in postman or in the web tool I get exactly the same error than in ADF v2. It seems that something has changed in the web call activity of ADF v2 that hardcodes the content header as &quot;text/plain&quot; somehow.</p>
<p>Is someone facing this same issue ? As I said, pipeline definition hasn't changed in months, just stopped working few days ago.</p>
","<azure-data-factory>","2020-09-07 13:10:51","1507","0","2","73393476","<p>If you are creating the ADF webhook activity using a bicep file, I found this approach works to set the Content-Type in the header parameter.</p>
<ol>
<li>Define a string variable with the hyphenated Content-Type name</li>
<li>Use string interpolation to include the variable in the headers param</li>
</ol>
<ul>
<li></li>
</ul>
<pre><code>var contentType = 'Content-Type'

activities: [
{
dependsOn: []
description: ''
name: 'MyWebHook'
userProperties: []
type: 'WebHook'
typeProperties: {
  body: {
    value: '@json(concat(\'{&quot;data&quot;:&quot;\',item().data,\'&quot;,&quot;startTime&quot;:&quot;\',item().startTime,\'&quot;,&quot;endTime\\&quot;:&quot;\',item().endTime,\'&quot;}\'))'
    type: 'Expression'
  }
  headers: {
    '${contentType}': 'application/json'
  }
  method: 'POST'
  reportStatusOnCallBack: true
  timeout: '00:10:00'
  url: listCallbackURL('${logic_app_id}/triggers/manual', '2016-10-01').value
 }
 }
 ]
</code></pre>
"
"63778055","SSIS Integraion Runtime ADF- Install Microsoft OLE DB Provider for Visual FoxPro 9.0","<p>I have used the SSIS script task(C# code) to load the DBF File to SQL Server
I have noticed the C# code has reference to the &quot;Microsoft OLE DB Provider for Visual FoxPro 9.0&quot;, I have installed and loaded data in my local machine.</p>
<p>Now, I don't have access to install the SSIS Integration Runtime on ADF, I would like to know if the SSIS IR allows us to install the driver &quot;Microsoft OLE DB Provider for Visual FoxPro 9.0&quot;which are not present built in.</p>
","<ssis><visual-foxpro><azure-data-factory>","2020-09-07 12:55:30","804","0","2","63786664","<p>SSIS Integration Runtime in ADF can be [customized] <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup</a></p>
"
"63778055","SSIS Integraion Runtime ADF- Install Microsoft OLE DB Provider for Visual FoxPro 9.0","<p>I have used the SSIS script task(C# code) to load the DBF File to SQL Server
I have noticed the C# code has reference to the &quot;Microsoft OLE DB Provider for Visual FoxPro 9.0&quot;, I have installed and loaded data in my local machine.</p>
<p>Now, I don't have access to install the SSIS Integration Runtime on ADF, I would like to know if the SSIS IR allows us to install the driver &quot;Microsoft OLE DB Provider for Visual FoxPro 9.0&quot;which are not present built in.</p>
","<ssis><visual-foxpro><azure-data-factory>","2020-09-07 12:55:30","804","0","2","65418461","<p>Put something similar to this in your main.cmd file that SSIS-IR looks at to install custom things.</p>
<pre><code>echo Installing ODBC Driver 17 for SQL Server...
msiexec /i msodbcsql.msi /quiet /passive /qn /lv %CUSTOM_SETUP_SCRIPT_LOG_DIR%\msodbcsql.log IACCEPTMSODBCSQLLICENSETERMS=YES ADDLOCAL=ALL
echo Installation completed

echo Installing OLE DB Driver for SQL Server...
msiexec /i msoledbsql.msi /quiet /passive /qn /lv %CUSTOM_SETUP_SCRIPT_LOG_DIR%\msoledbsql.log IACCEPTMSODBCSQLLICENSETERMS=YES
echo Installation completed
</code></pre>
"
"63748519","ADF Pipeline Adding Sequential Value in Copy Activity","<p>Apologies if this has been asked and answered elsewhere. If it is, please do refer to the url in comments on replies. So here is the situation,</p>
<p>I am making an API Request, in response I get <strong>auth_token</strong> which I use in the Copy Activity as Authorization to retrieve data in JSON format and Sink it to Azure SQL Database. I am able to Map all the elements I'm receiving in JSON to the columns of Azure SQL Database. However, there are two columns (<strong>UploadId</strong> and <strong>RowId</strong>) that still need to be populated.</p>
<ul>
<li><strong>UploadId</strong> is a GUID which will be same for the whole batch of rows (this I've managed to solve)</li>
<li><strong>RowId</strong> will be a <em>sequence</em> starting from 1 to end of that batch entry, and then for next batch (with new GUID value) it resets back to 1.</li>
</ul>
<p>The database will look something like this,</p>
<pre><code>| APILoadTime |      UploadId     |    RowId    |
|  2020-02-01 | 29AD7-12345-22EwQ |      1      |
|  2020-02-01 | 29AD7-12345-22EwQ |      2      |
|  2020-02-01 | 29AD7-12345-22EwQ |      3      |
|  2020-02-01 | 29AD7-12345-22EwQ |      4      |
|  2020-02-01 | 29AD7-12345-22EwQ |      5      |
--------------------------------------------------&gt; End of Batch One / Start of Batch Two
|  2020-02-01 | 30AD7-12345-22MLK |      1      |
|  2020-02-01 | 30AD7-12345-22MLK |      2      |
|  2020-02-01 | 30AD7-12345-22MLK |      3      |
|  2020-02-01 | 30AD7-12345-22MLK |      4      |
|  2020-02-01 | 30AD7-12345-22MLK |      5      |
--------------------------------------------------&gt; End of Batch Two and so on ... 
</code></pre>
<p>Is there a way in Azure Pipeline's Copy Activity to achieve this <strong>RowId</strong> behavior ... Or even if it's possible within Azure SQL Database.</p>
<p>Apologies for a long description, and Thank you in advance for any help!
Regards</p>
","<azure><azure-sql-database><azure-data-factory>","2020-09-04 22:06:33","1306","0","2","63799543","<p>You need to use a Window Function to achieve this.  ADF Data Flows have <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-window"" rel=""nofollow noreferrer"">Window Transformation activities</a> that are designed to do this exact thing.</p>
<p>Otherwise, you could load the data into a staging table and then <a href=""https://www.sqlservertutorial.net/sql-server-window-functions/sql-server-row_number-function/"" rel=""nofollow noreferrer"">use Azure SQL to window the data</a> as you select it out like...</p>
<pre><code>SELECT
    APILoadTime
    ,UploadId
    ,ROW_NUMBER() OVER (PARTITION BY UploadId ORDER BY APILoadTime) AS RowId
FROM dbo.MyTable;
</code></pre>
"
"63748519","ADF Pipeline Adding Sequential Value in Copy Activity","<p>Apologies if this has been asked and answered elsewhere. If it is, please do refer to the url in comments on replies. So here is the situation,</p>
<p>I am making an API Request, in response I get <strong>auth_token</strong> which I use in the Copy Activity as Authorization to retrieve data in JSON format and Sink it to Azure SQL Database. I am able to Map all the elements I'm receiving in JSON to the columns of Azure SQL Database. However, there are two columns (<strong>UploadId</strong> and <strong>RowId</strong>) that still need to be populated.</p>
<ul>
<li><strong>UploadId</strong> is a GUID which will be same for the whole batch of rows (this I've managed to solve)</li>
<li><strong>RowId</strong> will be a <em>sequence</em> starting from 1 to end of that batch entry, and then for next batch (with new GUID value) it resets back to 1.</li>
</ul>
<p>The database will look something like this,</p>
<pre><code>| APILoadTime |      UploadId     |    RowId    |
|  2020-02-01 | 29AD7-12345-22EwQ |      1      |
|  2020-02-01 | 29AD7-12345-22EwQ |      2      |
|  2020-02-01 | 29AD7-12345-22EwQ |      3      |
|  2020-02-01 | 29AD7-12345-22EwQ |      4      |
|  2020-02-01 | 29AD7-12345-22EwQ |      5      |
--------------------------------------------------&gt; End of Batch One / Start of Batch Two
|  2020-02-01 | 30AD7-12345-22MLK |      1      |
|  2020-02-01 | 30AD7-12345-22MLK |      2      |
|  2020-02-01 | 30AD7-12345-22MLK |      3      |
|  2020-02-01 | 30AD7-12345-22MLK |      4      |
|  2020-02-01 | 30AD7-12345-22MLK |      5      |
--------------------------------------------------&gt; End of Batch Two and so on ... 
</code></pre>
<p>Is there a way in Azure Pipeline's Copy Activity to achieve this <strong>RowId</strong> behavior ... Or even if it's possible within Azure SQL Database.</p>
<p>Apologies for a long description, and Thank you in advance for any help!
Regards</p>
","<azure><azure-sql-database><azure-data-factory>","2020-09-04 22:06:33","1306","0","2","63800338","<p>Thanks a lot @Leon Yue and @JeffRamos, I've managed to figure out the solution, so posting it here for everyone else who might encounter the same situation,</p>
<p>The solution I found was to use a <strong>Stored Procedure</strong> within Azure Data Factory from where I call the Azure Data Flow Activity. This is the code I used for creating the RowId seed function,</p>
<pre><code>CREATE PROCEDURE resetRowId
AS
BEGIN
    DBCC CHECKIDENT ('myDatabase', RESEED, 0)
END
GO
</code></pre>
<p>Once I have this Stored Procedure, all I did was something like this,</p>
<p><a href=""https://i.stack.imgur.com/XBjaJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBjaJ.png"" alt=""Azure Data Factory Pipeline Reset RowId"" /></a></p>
<p>This does it for you, the reason I kept it 0 so that when new data comes in, it starts from 1 again ...</p>
<p>Hope this helps others too ...</p>
<p>Thank you all who helped in someway</p>
"
"63740524","Azure Data Factory - Copy files to SFTP resolving destination from foreach item","<p>Another Azure Data Factory question.</p>
<p>I'm trying to use a 'Copy Data' activity within a ForEach, setting the destination sink to an item of the foreach.</p>
<p>My setup is as follows:</p>
<ul>
<li>Lookup activity to read a json file.</li>
</ul>
<p>The format of the json file:</p>
<pre><code>{
    &quot;OutputFolders&quot;:[
     {   
        &quot;Source&quot;: &quot;aaa/bb1/Output&quot;,
        &quot;Destination&quot;: &quot;Dest002/bin&quot;
     },
     {   
        &quot;Source&quot;: &quot;aaa/bbb2/Output&quot;,
        &quot;Destination&quot;: &quot;Dest002/bin&quot;
     },
     {   
        &quot;Source&quot;: &quot;aaa/bb3/Output&quot;,
        &quot;Destination&quot;: &quot;Dest002/bin&quot;
     }
    ]
}
</code></pre>
<ul>
<li>Foreach activity with items set to @activity('Read json config').output.value[0].OutputFolders</li>
<li>Within the foreach activity a 'Copy Data' activity</li>
</ul>
<p>This Sink has the following Sink dataset:</p>
<p><a href=""https://i.stack.imgur.com/uYUXf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uYUXf.png"" alt=""enter image description here"" /></a></p>
<p>When I run this pipeline however I get the following error message:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Sink' side. ErrorCode=SftpPermissionDenied,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Permission denied to access '/@item().Destination'.,Source=Microsoft.DataTransfer.ClientLibrary.SftpConnector,''Type=Renci.SshNet.Common.SftpPermissionDeniedException,Message=Permission denied,Source=Renci.SshNet,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy output files&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>So Message=Permission denied to access '/@item().Destination' seems to indicate that the destination folder is not resolved. Since this folder does not exist I get a SftpPermissionDenied.</p>
<p>I used the same method to copy files to a file share and there it seemed to work.</p>
<p>Does somebody have an idea how to make this destination resolve correctly?</p>
","<azure><azure-data-factory>","2020-09-04 11:43:30","588","1","2","63740734","<p>Ok, I tried some more and apparently if I use a concat function it works.</p>
<p>So @concat(item().Destination)</p>
<p>I do get a warning 'item' is not a recognized function, but it does the trick.</p>
<p>Not very straightforward and I wonder why the initial approach doesn't work.</p>
<p><a href=""https://i.stack.imgur.com/Elm4E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Elm4E.png"" alt=""enter image description here"" /></a></p>
"
"63740524","Azure Data Factory - Copy files to SFTP resolving destination from foreach item","<p>Another Azure Data Factory question.</p>
<p>I'm trying to use a 'Copy Data' activity within a ForEach, setting the destination sink to an item of the foreach.</p>
<p>My setup is as follows:</p>
<ul>
<li>Lookup activity to read a json file.</li>
</ul>
<p>The format of the json file:</p>
<pre><code>{
    &quot;OutputFolders&quot;:[
     {   
        &quot;Source&quot;: &quot;aaa/bb1/Output&quot;,
        &quot;Destination&quot;: &quot;Dest002/bin&quot;
     },
     {   
        &quot;Source&quot;: &quot;aaa/bbb2/Output&quot;,
        &quot;Destination&quot;: &quot;Dest002/bin&quot;
     },
     {   
        &quot;Source&quot;: &quot;aaa/bb3/Output&quot;,
        &quot;Destination&quot;: &quot;Dest002/bin&quot;
     }
    ]
}
</code></pre>
<ul>
<li>Foreach activity with items set to @activity('Read json config').output.value[0].OutputFolders</li>
<li>Within the foreach activity a 'Copy Data' activity</li>
</ul>
<p>This Sink has the following Sink dataset:</p>
<p><a href=""https://i.stack.imgur.com/uYUXf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uYUXf.png"" alt=""enter image description here"" /></a></p>
<p>When I run this pipeline however I get the following error message:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Sink' side. ErrorCode=SftpPermissionDenied,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Permission denied to access '/@item().Destination'.,Source=Microsoft.DataTransfer.ClientLibrary.SftpConnector,''Type=Renci.SshNet.Common.SftpPermissionDeniedException,Message=Permission denied,Source=Renci.SshNet,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy output files&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>So Message=Permission denied to access '/@item().Destination' seems to indicate that the destination folder is not resolved. Since this folder does not exist I get a SftpPermissionDenied.</p>
<p>I used the same method to copy files to a file share and there it seemed to work.</p>
<p>Does somebody have an idea how to make this destination resolve correctly?</p>
","<azure><azure-data-factory>","2020-09-04 11:43:30","588","1","2","73783321","<p>What you would usually do in this type of situation is to create a <strong>Parameter</strong> in the Dataset which you would then reference in the File Path you are trying to construct.</p>
<p><a href=""https://i.stack.imgur.com/mVaXw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mVaXw.png"" alt=""Creating Parameters in your Dataset"" /></a></p>
<p><a href=""https://i.stack.imgur.com/0XEkf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0XEkf.png"" alt=""Reference Parameters in the Table details of the Dataset"" /></a></p>
<p>This way, you can input your '@item().Destination' to this Parameter in your Copy Activity, as it will appear on the Dataset in the Pipeline.</p>
<p><a href=""https://i.stack.imgur.com/MXCbf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MXCbf.png"" alt=""Give input to the Parameters in the Dataset in the Copy Activity"" /></a></p>
<p>There is also an example here: <a href=""https://www.mssqltips.com/sqlservertip/6187/azure-data-factory-foreach-activity-example/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/6187/azure-data-factory-foreach-activity-example/</a></p>
"
"63740033","Connecting to a third party JDBC Connector from Azure Data Factory","<p>I am planning, implementing and running an azure-based reporting architecture for my customer. The source is their ERP systems database, ETL and data storage takes place in an Azure SQL DB, everything in between and the &quot;orchestration&quot; (triggering the right things in the right order at the right time) is mainly handled with ADF.
They are currently running their ERP systems database on an on premise SQL Server, which I connect to from ADF using a Self Hosted Integration Runtime.
Now they want to switch to a SaaS solution their ERP provider offers, which only offers a JDBC connection to query the ERP systems database.</p>
<p>So my question is, is there any possibility to query a JDBC connection directly from ADF?
The basic requirement is getting the data from the ERP database and writing it into staging tables in the Azure SQL DB. However, I would strongly prefer doing it with ADF since the current implementation and all the other ETL stuff is there.</p>
<p>After I've spent some days on Google trying to find possible solutions, I still haven't found much useful information.
The only possible ways I have found are by &quot;misusing&quot; Data Bricks (to read from an JDCB connection and write to Azure SQL DB) or by writing a .NET application and running it as a webjob in Azure.</p>
<p>I'm very thankful for any input that could possibly be helpful!</p>
","<jdbc><azure-sql-database><azure-data-factory>","2020-09-04 11:11:05","1756","0","1","63787516","<p>Can you be more specific and let us know about the driver.</p>
<p>From Azure datafctory, you can always call notebook on Adb side ( you neeed to use the note book activity).</p>
"
"63739683","How to perform push down optimization in Azure Data Factory for snowflake connection","<p>Recently Microsoft launched the snowflake connection for data flow in ADF. Is there any way to turn on the push down optimization in ADF so that if my source and target is Snowflake only then instead of pulling data out of snowflake environment it should trigger a query in snowflake to do the task. Like a normal ELT process instead of ETL.</p>
<p>Let me know if you need some more clarification.</p>
","<azure><azure-data-factory><snowflake-cloud-data-platform>","2020-09-04 10:48:58","717","0","1","63787453","<p>As I understand the intent here is to fire a query from ADF on snowflake data so that possibley the data can be scrubbed ( or something similar ) . I see that Lookup activity also supports snowflake and probably that should help you . My knowledge on SF is limited , but i know that you can call a proc/query  from lookup activity and that should help .</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a></p>
<p>&quot;Lookup activity reads and returns the content of a configuration file or table. It also returns the result of executing a query or stored procedure. The output from Lookup activity can be used in a subsequent copy or transformation activity if it's a singleton value. The output can be used in a ForEach activity if it's an array of attributes.&quot;</p>
"
"63730264","How to pass a DataPath PipelineParameter from AzureDatafactory to AzureMachineLearningExecutePipeline Activity?","<BR>
<ul>
<li><p>I am trying to read a file from a Blob Storage, load to pandas and write it to a BlobStorage</p>
</li>
<li><p>I have an Azure Machine Learning Pipeline with a PythonScriptStep that takes 2 PipelineParameters and are DataPaths as below.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Datastore
from azureml.data.datapath import DataPath, DataPathComputeBinding, DataReference
from azureml.pipeline.core import PipelineParameter

datastore = Datastore(ws, &quot;SampleStore&quot;)
in_raw_path_default = 'somefolder/raw/alerts/2020/08/03/default_in.csv'
in_cleaned_path_default= 'somefolder/cleaned/alerts/2020/08/03/default_out.csv'

in_raw_datapath = DataPath(datastore=datastore, path_on_datastore=in_raw_path_default)
in_raw_path_pipelineparam = PipelineParameter(name=&quot;inrawpath&quot;, default_value=in_raw_datapath)
raw_datapath_input = (in_raw_path_pipelineparam, DataPathComputeBinding(mode='mount'))

in_cleaned_datapath = DataPath(datastore=datastore, path_on_datastore=in_cleaned_path_default)
in_cleaned_path_pipelineparam = PipelineParameter(name=&quot;incleanedpath&quot;, default_value=in_cleaned_datapath)
cleaned_datapath_input = (in_cleaned_path_pipelineparam, DataPathComputeBinding(mode='mount'))

from azureml.pipeline.steps import PythonScriptStep

source_directory = script_folder + '/pipeline_Steps'
dataprep_step = PythonScriptStep(
    script_name=&quot;SimpleTest.py&quot;, 
    arguments=[&quot;--input_data&quot;, raw_datapath_input, &quot;--cleaned_data&quot;, cleaned_datapath_input],
    inputs=[raw_datapath_input, cleaned_datapath_input],    
    compute_target=default_compute, 
    source_directory=source_directory,
    runconfig=run_config,
    allow_reuse=True
)

from azureml.pipeline.core import Pipeline
pipeline_test = Pipeline(workspace=ws, steps=[dataprep_step])

test_raw_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/raw/alerts/2017/05/31/test.csv')
test_cleaned_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/cleaned/alerts/2020/09/03')
pipeline_run_msalerts = Experiment(ws, 'SampleExperiment').submit(pipeline_test, pipeline_parameters={&quot;inrawpath&quot;  : test_raw_path,
                                                                                                        &quot;incleanedpath&quot; : test_cleaned_path})```

</code></pre>
</li>
</ul>
<p>This is the Script Used(SimpleTest.py):<BR></p>
<pre class=""lang-py prettyprint-override""><code>import os
import sys
import argparse
import pathlib
import azureml.core
import pandas as pd

parser = argparse.ArgumentParser(&quot;datapreponly&quot;)
parser.add_argument(&quot;--input_data&quot;, type=str)
parser.add_argument(&quot;--cleaned_data&quot;, type=str)

args = parser.parse_args()

print(&quot;Argument 1: %s&quot; % args.input_data)
print(&quot;Argument 2: %s&quot; % args.cleaned_data)

testDf = pd.read_csv(args.input_data, error_bad_lines=False)
print('Total Data Shape' + str(testDf.shape))

if not (args.cleaned_data is None):
    output_path = args.cleaned_data
    os.makedirs(output_path, exist_ok=True)
    outdatapath = output_path + '/alert.csv'    
    testDf.to_csv(outdatapath, index=False)
</code></pre>
<p><strong>Triggering this AzureMLPipeline from AzureDataFactory :</strong><BR>
The above code works fine by executing the ML pipeline in AzureMLWorkspace/PipelineSDK. I am trying to trigger the AzureMLpipeline from AzureDataFactory(AzureMachineLearningExecutePipeline) activity as follows<BR></p>
<p><img src=""https://i.stack.imgur.com/Jo0dD.png"" alt=""enter image description here"" /></p>
<p>Tried a debug run as follows by passing 2 string input paths<BR>
rawdatapath = &quot;samplefolder/raw/alerts/2017/05/31/test.csv&quot;<BR>
cleaneddatapath = &quot;samplefolder/raw/cleaned/2020/09/03/&quot;</p>
<p><img src=""https://i.stack.imgur.com/BByYd.png"" alt=""enter image description here"" /></p>
<pre><code>Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/workspaceblobstore/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade
Preparing to call script [ SimpleTest.py ] 
with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']
After variable expansion, calling script [ SimpleTest.py ] with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']

Script type = None
Argument 1: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv
Argument 2: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv
.......................
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv'
</code></pre>
<p>It shows that the default path is taken instead of the pipeline parameter(<em>No such File or directory error is less important as the main point is the default path is taken instead of the pipeline parameters</em>). I doubt its because of pass the pipelineparameter as a string instead of a datapath.<BR>
<BR><br />
<strong>FINALLY THE QUESTION</strong> :  How to pass a datapath to an AzureMLPipelineActivity from Azure Data Factory?</p>
<p><BR>Thanks.</p>
","<azure-data-factory><azure-machine-learning-service>","2020-09-03 19:14:41","2168","3","3","63911428","<p>The input parameters seem to be defined as string, please try modifying them as Object datatype. As per <a href=""https://learn.microsoft.com/azure/data-factory/transform-data-machine-learning-service#type-properties"" rel=""nofollow noreferrer"">documentation</a>, it expects object
{&quot;Key&quot; : &quot;value&quot;} parameters.</p>
<p><a href=""https://i.stack.imgur.com/Ibqqw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ibqqw.png"" alt=""enter image description here"" /></a></p>
"
"63730264","How to pass a DataPath PipelineParameter from AzureDatafactory to AzureMachineLearningExecutePipeline Activity?","<BR>
<ul>
<li><p>I am trying to read a file from a Blob Storage, load to pandas and write it to a BlobStorage</p>
</li>
<li><p>I have an Azure Machine Learning Pipeline with a PythonScriptStep that takes 2 PipelineParameters and are DataPaths as below.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Datastore
from azureml.data.datapath import DataPath, DataPathComputeBinding, DataReference
from azureml.pipeline.core import PipelineParameter

datastore = Datastore(ws, &quot;SampleStore&quot;)
in_raw_path_default = 'somefolder/raw/alerts/2020/08/03/default_in.csv'
in_cleaned_path_default= 'somefolder/cleaned/alerts/2020/08/03/default_out.csv'

in_raw_datapath = DataPath(datastore=datastore, path_on_datastore=in_raw_path_default)
in_raw_path_pipelineparam = PipelineParameter(name=&quot;inrawpath&quot;, default_value=in_raw_datapath)
raw_datapath_input = (in_raw_path_pipelineparam, DataPathComputeBinding(mode='mount'))

in_cleaned_datapath = DataPath(datastore=datastore, path_on_datastore=in_cleaned_path_default)
in_cleaned_path_pipelineparam = PipelineParameter(name=&quot;incleanedpath&quot;, default_value=in_cleaned_datapath)
cleaned_datapath_input = (in_cleaned_path_pipelineparam, DataPathComputeBinding(mode='mount'))

from azureml.pipeline.steps import PythonScriptStep

source_directory = script_folder + '/pipeline_Steps'
dataprep_step = PythonScriptStep(
    script_name=&quot;SimpleTest.py&quot;, 
    arguments=[&quot;--input_data&quot;, raw_datapath_input, &quot;--cleaned_data&quot;, cleaned_datapath_input],
    inputs=[raw_datapath_input, cleaned_datapath_input],    
    compute_target=default_compute, 
    source_directory=source_directory,
    runconfig=run_config,
    allow_reuse=True
)

from azureml.pipeline.core import Pipeline
pipeline_test = Pipeline(workspace=ws, steps=[dataprep_step])

test_raw_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/raw/alerts/2017/05/31/test.csv')
test_cleaned_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/cleaned/alerts/2020/09/03')
pipeline_run_msalerts = Experiment(ws, 'SampleExperiment').submit(pipeline_test, pipeline_parameters={&quot;inrawpath&quot;  : test_raw_path,
                                                                                                        &quot;incleanedpath&quot; : test_cleaned_path})```

</code></pre>
</li>
</ul>
<p>This is the Script Used(SimpleTest.py):<BR></p>
<pre class=""lang-py prettyprint-override""><code>import os
import sys
import argparse
import pathlib
import azureml.core
import pandas as pd

parser = argparse.ArgumentParser(&quot;datapreponly&quot;)
parser.add_argument(&quot;--input_data&quot;, type=str)
parser.add_argument(&quot;--cleaned_data&quot;, type=str)

args = parser.parse_args()

print(&quot;Argument 1: %s&quot; % args.input_data)
print(&quot;Argument 2: %s&quot; % args.cleaned_data)

testDf = pd.read_csv(args.input_data, error_bad_lines=False)
print('Total Data Shape' + str(testDf.shape))

if not (args.cleaned_data is None):
    output_path = args.cleaned_data
    os.makedirs(output_path, exist_ok=True)
    outdatapath = output_path + '/alert.csv'    
    testDf.to_csv(outdatapath, index=False)
</code></pre>
<p><strong>Triggering this AzureMLPipeline from AzureDataFactory :</strong><BR>
The above code works fine by executing the ML pipeline in AzureMLWorkspace/PipelineSDK. I am trying to trigger the AzureMLpipeline from AzureDataFactory(AzureMachineLearningExecutePipeline) activity as follows<BR></p>
<p><img src=""https://i.stack.imgur.com/Jo0dD.png"" alt=""enter image description here"" /></p>
<p>Tried a debug run as follows by passing 2 string input paths<BR>
rawdatapath = &quot;samplefolder/raw/alerts/2017/05/31/test.csv&quot;<BR>
cleaneddatapath = &quot;samplefolder/raw/cleaned/2020/09/03/&quot;</p>
<p><img src=""https://i.stack.imgur.com/BByYd.png"" alt=""enter image description here"" /></p>
<pre><code>Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/workspaceblobstore/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade
Preparing to call script [ SimpleTest.py ] 
with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']
After variable expansion, calling script [ SimpleTest.py ] with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']

Script type = None
Argument 1: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv
Argument 2: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv
.......................
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv'
</code></pre>
<p>It shows that the default path is taken instead of the pipeline parameter(<em>No such File or directory error is less important as the main point is the default path is taken instead of the pipeline parameters</em>). I doubt its because of pass the pipelineparameter as a string instead of a datapath.<BR>
<BR><br />
<strong>FINALLY THE QUESTION</strong> :  How to pass a datapath to an AzureMLPipelineActivity from Azure Data Factory?</p>
<p><BR>Thanks.</p>
","<azure-data-factory><azure-machine-learning-service>","2020-09-03 19:14:41","2168","3","3","64122492","<p><a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"" rel=""nofollow noreferrer"">This notebook</a> demonstrates the use of <code>DataPath</code> and <code>PipelineParameters</code> in AML Pipeline. You will learn how strings and <code>DataPath</code> can be parameterized and submitted to AML Pipelines via <code>PipelineParameters</code>. You can parameterize input dataset and here is the <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"" rel=""nofollow noreferrer"">sample</a> notebook that shows how to do it.</p>
<p>Currently, <code>ParallelRunStep</code> accepts dataset as the data input.  you can add one more step before <code>ParallelRunStep</code> to create a dataset object pointing to the new data and pass to <code>ParallelRunStep</code>. Here’s <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"" rel=""nofollow noreferrer"">an example</a> of using multiple steps:</p>
<p>For output, if you use <code>append_row</code> output action, you can customize the output file name through <code>append_row_file_name</code> config. The output will be stored in the default blob. To move it to other store, we suggest to use another <code>DataTransferStep</code> after <code>ParallelRunStep</code>.
Please follow <a href=""https://github.com/mlonazure/AzureMachineLearning/tree/master/AML%20Data%20Transfer"" rel=""nofollow noreferrer"">this example</a> for data transfer step:</p>
"
"63730264","How to pass a DataPath PipelineParameter from AzureDatafactory to AzureMachineLearningExecutePipeline Activity?","<BR>
<ul>
<li><p>I am trying to read a file from a Blob Storage, load to pandas and write it to a BlobStorage</p>
</li>
<li><p>I have an Azure Machine Learning Pipeline with a PythonScriptStep that takes 2 PipelineParameters and are DataPaths as below.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Datastore
from azureml.data.datapath import DataPath, DataPathComputeBinding, DataReference
from azureml.pipeline.core import PipelineParameter

datastore = Datastore(ws, &quot;SampleStore&quot;)
in_raw_path_default = 'somefolder/raw/alerts/2020/08/03/default_in.csv'
in_cleaned_path_default= 'somefolder/cleaned/alerts/2020/08/03/default_out.csv'

in_raw_datapath = DataPath(datastore=datastore, path_on_datastore=in_raw_path_default)
in_raw_path_pipelineparam = PipelineParameter(name=&quot;inrawpath&quot;, default_value=in_raw_datapath)
raw_datapath_input = (in_raw_path_pipelineparam, DataPathComputeBinding(mode='mount'))

in_cleaned_datapath = DataPath(datastore=datastore, path_on_datastore=in_cleaned_path_default)
in_cleaned_path_pipelineparam = PipelineParameter(name=&quot;incleanedpath&quot;, default_value=in_cleaned_datapath)
cleaned_datapath_input = (in_cleaned_path_pipelineparam, DataPathComputeBinding(mode='mount'))

from azureml.pipeline.steps import PythonScriptStep

source_directory = script_folder + '/pipeline_Steps'
dataprep_step = PythonScriptStep(
    script_name=&quot;SimpleTest.py&quot;, 
    arguments=[&quot;--input_data&quot;, raw_datapath_input, &quot;--cleaned_data&quot;, cleaned_datapath_input],
    inputs=[raw_datapath_input, cleaned_datapath_input],    
    compute_target=default_compute, 
    source_directory=source_directory,
    runconfig=run_config,
    allow_reuse=True
)

from azureml.pipeline.core import Pipeline
pipeline_test = Pipeline(workspace=ws, steps=[dataprep_step])

test_raw_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/raw/alerts/2017/05/31/test.csv')
test_cleaned_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/cleaned/alerts/2020/09/03')
pipeline_run_msalerts = Experiment(ws, 'SampleExperiment').submit(pipeline_test, pipeline_parameters={&quot;inrawpath&quot;  : test_raw_path,
                                                                                                        &quot;incleanedpath&quot; : test_cleaned_path})```

</code></pre>
</li>
</ul>
<p>This is the Script Used(SimpleTest.py):<BR></p>
<pre class=""lang-py prettyprint-override""><code>import os
import sys
import argparse
import pathlib
import azureml.core
import pandas as pd

parser = argparse.ArgumentParser(&quot;datapreponly&quot;)
parser.add_argument(&quot;--input_data&quot;, type=str)
parser.add_argument(&quot;--cleaned_data&quot;, type=str)

args = parser.parse_args()

print(&quot;Argument 1: %s&quot; % args.input_data)
print(&quot;Argument 2: %s&quot; % args.cleaned_data)

testDf = pd.read_csv(args.input_data, error_bad_lines=False)
print('Total Data Shape' + str(testDf.shape))

if not (args.cleaned_data is None):
    output_path = args.cleaned_data
    os.makedirs(output_path, exist_ok=True)
    outdatapath = output_path + '/alert.csv'    
    testDf.to_csv(outdatapath, index=False)
</code></pre>
<p><strong>Triggering this AzureMLPipeline from AzureDataFactory :</strong><BR>
The above code works fine by executing the ML pipeline in AzureMLWorkspace/PipelineSDK. I am trying to trigger the AzureMLpipeline from AzureDataFactory(AzureMachineLearningExecutePipeline) activity as follows<BR></p>
<p><img src=""https://i.stack.imgur.com/Jo0dD.png"" alt=""enter image description here"" /></p>
<p>Tried a debug run as follows by passing 2 string input paths<BR>
rawdatapath = &quot;samplefolder/raw/alerts/2017/05/31/test.csv&quot;<BR>
cleaneddatapath = &quot;samplefolder/raw/cleaned/2020/09/03/&quot;</p>
<p><img src=""https://i.stack.imgur.com/BByYd.png"" alt=""enter image description here"" /></p>
<pre><code>Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/workspaceblobstore/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade
Preparing to call script [ SimpleTest.py ] 
with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']
After variable expansion, calling script [ SimpleTest.py ] with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']

Script type = None
Argument 1: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv
Argument 2: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv
.......................
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv'
</code></pre>
<p>It shows that the default path is taken instead of the pipeline parameter(<em>No such File or directory error is less important as the main point is the default path is taken instead of the pipeline parameters</em>). I doubt its because of pass the pipelineparameter as a string instead of a datapath.<BR>
<BR><br />
<strong>FINALLY THE QUESTION</strong> :  How to pass a datapath to an AzureMLPipelineActivity from Azure Data Factory?</p>
<p><BR>Thanks.</p>
","<azure-data-factory><azure-machine-learning-service>","2020-09-03 19:14:41","2168","3","3","64135741","<p>Got an answer from Microsoft(please refer to this thread <a href=""https://learn.microsoft.com/en-us/answers/questions/91785/azure-data-factory-how-to-pass-datapath-as-a-param.html"" rel=""nofollow noreferrer"">here</a>). Azure Data Factory  Product team confirms that there is no datatype supported for &quot;DataPath&quot; parameter today in Azure Data Factory(ADF). However, there is a feature already raised for the same and work is in progress for it. This feature will be a part of the November release.</p>
"
"63728206","403/307 error in azure data factory pipeline execution","<p>We need to implement transformation in azure while copying file from azure blob storage to azure data lake gen2. In the current implementation we are trying to achieve it using data flow in data factory. We have then embedded the data flow block in pipeline for execution. During execution it intermittently but most times fails with below error. While monitoring the debug data flow is seen as complete but the file is not copied.Also have attached the screenshot from the data factory debug window. Please advise me on this further to get this issue resolved.</p>
<p>Error Message:</p>
<pre><code>{&quot;message&quot;:&quot;at Sink 'sink1': java.lang.Exception: Fail to reach https://eu.frontend.clouddatahub.net/subscriptions/4ce3448b-6b04-43ab-a4c4-9fa36dfb3bfb/authservice/ams/api/v2/acquiretoken with status code:403, payload:{\&quot;ErrorCode\&quot;:307,\&quot;Message\&quot;:\&quot;ErrorCode : AuthSasRevoked, InnerException : null, ServiceTrace : \&quot;,\&quot;ServiceStackTrace\&quot;:\&quot;\&quot;}, CorrelationId:2f7f8529-a6fe-4a79-a3ea-952d9213a727, RunId:3679a5bd-eeaa-418b-8d98-be63c116ddd8. Details:at Sink 'sink1': java.lang.Exception: Fail to reach https://eu.frontend.clouddatahub.net/subscriptions/4ce3448b-6b04-43ab-a4c4-9fa36dfb3bfb/authservice/ams/api/v2/acquiretoken with status code:403, payload:{\&quot;ErrorCode\&quot;:307,\&quot;Message\&quot;:\&quot;ErrorCode : AuthSasRevoked, InnerException : null, ServiceTrace : \&quot;,\&quot;ServiceStackTrace\&quot;:\&quot;\&quot;}, CorrelationId:2f7f8529-a6fe-4a79-a3ea-952d9213a727, RunId:3679a5bd-eeaa-418b-8d98-be63c116ddd8&quot;,&quot;failureType&quot;:&quot;UserError&quot;,&quot;target&quot;:&quot;data_flow_raw_storage_to_adls_transformation&quot;,&quot;errorCode&quot;:&quot;DFExecutorUserError&quot;}
</code></pre>
<p>Debug record:</p>
<p><a href=""https://i.stack.imgur.com/g0IQV.png"" rel=""nofollow noreferrer"">Debug View for Data Flow</a></p>
","<azure><azure-data-lake-gen2><azure-data-factory>","2020-09-03 16:46:40","193","0","1","64209995","<p>This is a product bug with the Azure Data Flows. And the product bug has been fixed.</p>
"
"63728187","Azure Data Factory to Azure Blob Storage Permissions","<p>I'm connecting ADF to blob storage v2 using a managed identity following this doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#linked-service-properties"" rel=""nofollow noreferrer"">Doc1</a></p>
<p>When it comes to test the connection with my first dataset, I am successful when I test the connection to the linkedservice. When I try by the filepath, and enter &quot;testfolder&quot; (which exists in the blob) it fails returning a generic forbidden error displayed at the end of this post.
However, when I opt to &quot;browse&quot; the folders in the dataset portal, the folder &quot;testfolder&quot; does show up. But when I select it, it will not show me anything within that folder.</p>
<p>The Data Factory managed instance is given the role of Contributor, granting full access to manage all resources. Is there some other hidden issue or possible way to narrow down the issue? My instinct is that this is something within the blob container since I can view the containers, but not their contents.</p>
<p>Error message:</p>
<p><a href=""https://i.stack.imgur.com/kqmG0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kqmG0.png"" alt=""Error Message1"" /></a></p>
","<azure><azure-data-factory><azure-blob-storage>","2020-09-03 16:46:06","1966","2","1","63735316","<p>It seems that you don't give the role of azure blob storage.
Please fellow this:</p>
<p>1.click IAM in azure blob storage,navigate to Role assignments and add role assignment.
<a href=""https://i.stack.imgur.com/Doki4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Doki4.png"" alt=""enter image description here"" /></a></p>
<p>2.choose role according your need and select your data factory.
<a href=""https://i.stack.imgur.com/NZHqS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NZHqS.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/WRQb1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WRQb1.png"" alt=""enter image description here"" /></a></p>
<p>3.A few minute later,you can retry to choose file path.</p>
<p>Hope this can help you.</p>
"
"63727226","I need to upload media content (video or image) storage in SQL Azure to Azure BLOB Container","<p>Actually the information is stored in BASE64 in a varchar field. I've created a stored procedure to convert that field into varbinary. With the following code:</p>
<pre><code>cast(N'' as xml).value('xs:base64Binary(sql:column(&quot;[name of the column]&quot;))', 'varbinary(MAX)')
</code></pre>
<p>The result is shown in the following picture:</p>
<p><a href=""https://i.stack.imgur.com/qCzQ1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qCzQ1.png"" alt="""" /></a></p>
<p>The media is storage (SQL Azure) in varbinary, after storing the media content as file in the  BLOB Container, I cannot be able to render the media to watch its content if I donwload it in my local computer. Even If I try to open the file with a local editor I can see the same value as the image shows above.
All the convertion process is running in Azure SQL Server and call it using a ADF process.</p>
<p>What is the best approach to load those media using Azure Data Factory? Is it another way to save BASE64 to image from SQL to Blob storage in AZURE?
Thanks</p>
","<sql><azure><blob><azure-data-factory>","2020-09-03 15:46:55","351","0","1","63729023","<p>You will have to convert Varbinary data type back to Base 64 and then save it to Blob.</p>
<p>You can refer to <a href=""https://stackoverflow.com/questions/54804415/convert-image-data-type-into-base64-and-base64-into-image-data-type"">this</a> and see which method you would like to implement out of an answer by <em>Manoj Choudhari</em> or <em>Divya Agarwal</em>.</p>
"
"63725121","ADF Transformation with parameterized dataset in mapping data flow","<p>I'm currently doing an ADF data transformation using .net SDK. I’m using Data Flow for this transformation. I have a CSV file with three columns which are First Name, Last Name and Age. I want to Concatenate First Name and Last Name in to full Name.
But the problem is input file is generated at runtime (Data sets are parameterized). Below is my Data Flow Diagram. I also tried adding parameters for Concat function.</p>
<p><a href=""https://i.stack.imgur.com/Uqd4d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uqd4d.png"" alt=""enter image description here"" /></a></p>
<p>I was able to do this for an existing file in blob storage and below is that Data Flow Diagram.</p>
<p><a href=""https://i.stack.imgur.com/fsaxs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fsaxs.png"" alt=""enter image description here"" /></a></p>
<p>But I want to do this with a file which is created at runtime. Please assist</p>
","<c#><azure><asp.net-core><azure-data-factory><azure-sdk>","2020-09-03 13:54:19","162","0","1","63730462","<p>This expression will work inside of a Derived Column regardless of the contents of the incoming file, so long as FirstName and LastName exist somewhere in the incoming metadata:</p>
<p>byName('FirstName')+byName('LastName')</p>
"
"63724462","Copy nested JSON to Azure sql with Azure Data Factory","<p>I have a nested JSON stored in a Data Lake on Azure , it has this format:</p>
<pre><code>    {&quot;proto&quot;: &quot;01&quot;,
     &quot;type&quot;: &quot;A&quot;,

 &quot;description&quot;: &quot;heartbeat&quot;,
 &quot;geometry&quot;: {&quot;y0_1&quot;: {&quot;tag&quot;: &quot;Normal&quot;,
   &quot;probability&quot;: 0.40,
   &quot;x&quot;: 39,
   &quot;y&quot;: 13},
  &quot;y0_2&quot;: {&quot;tag&quot;: &quot;category_3&quot;, &quot;probability&quot;: 0.8, &quot;x&quot;: 48, &quot;y&quot;: 13},
  &quot;y0_3&quot;: {&quot;tag&quot;: &quot;Normal&quot;, &quot;probability&quot;: 0.9, &quot;x&quot;: 27, &quot;y&quot;: 10},
&quot;Test&quot;: {&quot;proba&quot;: 0.65}}}
</code></pre>
<p>I want to create ADF Pipeline (with triggers) to move it from Data Lake to Azure Sql.
The problem is when I create a copy Activity, the mapping isn't recognized by ADF ,
It creates a table with 4 columns: proto, type, description, but the 4th one geometry contains all the rest of the json file in one row.
While I want to have an output table in this format:</p>
<pre><code>proto    type    description    tag       probability    x    y     proba
01        A      heartbeat      Normal     0.40          39   13     0.65
01        A      heartbeat      category_3 0.8           48   13     0.65
01        A      heartbeat      Normal     0.9           27   10     0.65

</code></pre>
<p>I tried to parse the json directly on SQL using CROSS APPLY tool, but I have trouble making the JSON to copy from ADLS to SQL directly with the wanted mapping on ADF
If anyone has some guidance or any idea that I can follow, it will be much appreciated</p>
","<json><stored-procedures><azure-sql-database><azure-data-factory>","2020-09-03 13:15:26","1375","1","2","63734133","<p>Per my experience, Data Factory doesn't work well with the nested json.</p>
<p>To get your expect output, you may need create three copy actives to achieve that. Each active with the same source and sink. And create the table firstly in sink database.</p>
<p>Pipeline overview:
<a href=""https://i.stack.imgur.com/7nJkO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7nJkO.png"" alt=""enter image description here"" /></a></p>
<p>The differences are the mapping setting in each copy active.</p>
<p>Copy active1: copy data <code>geometry.y0_1</code> to sink:
<a href=""https://i.stack.imgur.com/z7l7c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z7l7c.png"" alt=""enter image description here"" /></a></p>
<p>Copy active2: copy data <code>geometry.y0_2</code> to sink:
<a href=""https://i.stack.imgur.com/8H6L8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8H6L8.png"" alt=""enter image description here"" /></a></p>
<p>Copy active3: copy data <code>geometry.y0_3</code> to sink:
<a href=""https://i.stack.imgur.com/62VR0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/62VR0.png"" alt=""enter image description here"" /></a></p>
<p>Output data in sink table:
<a href=""https://i.stack.imgur.com/NzMrK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NzMrK.png"" alt=""enter image description here"" /></a></p>
<p>Some other ways, you could create a stored procedure in database to deal with the JSON data, choose the stored procedure in sink like bellow:
<a href=""https://i.stack.imgur.com/A9Ktu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A9Ktu.png"" alt=""enter image description here"" /></a></p>
"
"63724462","Copy nested JSON to Azure sql with Azure Data Factory","<p>I have a nested JSON stored in a Data Lake on Azure , it has this format:</p>
<pre><code>    {&quot;proto&quot;: &quot;01&quot;,
     &quot;type&quot;: &quot;A&quot;,

 &quot;description&quot;: &quot;heartbeat&quot;,
 &quot;geometry&quot;: {&quot;y0_1&quot;: {&quot;tag&quot;: &quot;Normal&quot;,
   &quot;probability&quot;: 0.40,
   &quot;x&quot;: 39,
   &quot;y&quot;: 13},
  &quot;y0_2&quot;: {&quot;tag&quot;: &quot;category_3&quot;, &quot;probability&quot;: 0.8, &quot;x&quot;: 48, &quot;y&quot;: 13},
  &quot;y0_3&quot;: {&quot;tag&quot;: &quot;Normal&quot;, &quot;probability&quot;: 0.9, &quot;x&quot;: 27, &quot;y&quot;: 10},
&quot;Test&quot;: {&quot;proba&quot;: 0.65}}}
</code></pre>
<p>I want to create ADF Pipeline (with triggers) to move it from Data Lake to Azure Sql.
The problem is when I create a copy Activity, the mapping isn't recognized by ADF ,
It creates a table with 4 columns: proto, type, description, but the 4th one geometry contains all the rest of the json file in one row.
While I want to have an output table in this format:</p>
<pre><code>proto    type    description    tag       probability    x    y     proba
01        A      heartbeat      Normal     0.40          39   13     0.65
01        A      heartbeat      category_3 0.8           48   13     0.65
01        A      heartbeat      Normal     0.9           27   10     0.65

</code></pre>
<p>I tried to parse the json directly on SQL using CROSS APPLY tool, but I have trouble making the JSON to copy from ADLS to SQL directly with the wanted mapping on ADF
If anyone has some guidance or any idea that I can follow, it will be much appreciated</p>
","<json><stored-procedures><azure-sql-database><azure-data-factory>","2020-09-03 13:15:26","1375","1","2","63772726","<p>Thanks Leon Yue for your answer , the only issue is that I have a very long json, and I can't create manually many copy activities.</p>
"
"63722680","Azure Data Factory - Use system variable in Dynamic Content","<p>I'm trying to use system variable '@pipeline().TriggerTime' in a dynamic content field.</p>
<p>I have a 'Copy Data' activity which has a sink dataset to a folder.</p>
<p><a href=""https://i.stack.imgur.com/dFupS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dFupS.png"" alt=""enter image description here"" /></a></p>
<p>Inside this Sink dataset, I try to set the filepath to</p>
<pre><code>@concat('Trigger_',formatDateTime(@pipeline().TriggerTime, 'ddMMyyyyHHmmss'), '.trg')
</code></pre>
<p><a href=""https://i.stack.imgur.com/ACWfl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ACWfl.png"" alt=""enter image description here"" /></a></p>
<p>But I get the following error message.</p>
<p><a href=""https://i.stack.imgur.com/noTFX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/noTFX.png"" alt=""enter image description here"" /></a></p>
<p>The activity is contained in an 'If Condition' block which itself is contained in a 'ForEach' but this variable should be global in the pipeline so I don't see why it shouldn't work.</p>
<p>Thanks for any help.</p>
","<azure><azure-data-factory>","2020-09-03 11:27:59","805","0","1","63733520","<p>As Joel comments,just change &quot;@pipeline&quot; to &quot;pipeline&quot;.</p>
<pre><code>@concat('Trigger_',formatDateTime(pipeline().TriggerTime, 'ddMMyyyyHHmmss'), '.trg')
</code></pre>
<p>If you want to use multiple functions,you just add @ at the beginning.
If you want to get the string of functions,you need to add double @,such as &quot;Answer is: @@{pipeline().parameters.myNumber}&quot; return the string Answer is: @{pipeline().parameters.myNumber}.</p>
<p>More detail,you can refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#expressions"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"63721355","Group datafactories and give this whole group acces to Azure Keyvault","<p>I have a template datafactory. This datfactory has access to (and uses) a Azure Keyvault.
When I copy this Datafactory, I have to give the new datafactory access to this keyvault.</p>
<p>Copying is done by creating a new github-repository, copying the contents of the repository belonging to the 'template factory' to the newly made github-repository. When that is done a new datafactory is set up and connected to the newly made repository. Voila! A copy!</p>
<p>What I would like to accomplishe is that this ADF copy automatically has access to the keyvault.
I was thinking: can I somehow put this template datafactory in a securitygroup that has access to the Keyvault? So that, if I make a copy of the datafactory, this copy automatically has this same group and has access to the keyvault?</p>
<p>Or can I somehow tell the keyvault that all my factories have access to the vault?</p>
","<github><azure-data-factory><azure-keyvault>","2020-09-03 10:06:55","56","0","1","63821499","<p>You can change the access policy at Key Vault.</p>
<ol>
<li>You can add the data factory to the key vault manually according to this <a href=""https://learn.microsoft.com/en-us/azure/key-vault/general/assign-access-policy-portal"" rel=""nofollow noreferrer"">document</a>.</li>
<li>Use role-based access control (RBAC) according to this <a href=""https://learn.microsoft.com/en-us/azure/key-vault/general/overview-security#identity-and-access-management"" rel=""nofollow noreferrer"">document</a>.</li>
</ol>
"
"63713289","Logic App connector for Azure Data Factory not showing existing factory pipeline","<p>I am not sure if I found a bug, or I am doing something wrong.</p>
<p>I created Data Factory &quot;A&quot; (ADF A) with a pipeline (TesterPipe). I published changes into Azure DevOps in &quot;master branch&quot;. I then created second Data Factory &quot;B&quot; (ADF B), connected it to same Azure DevOps and used &quot;master branch&quot; to populate it with contents from ADF A.</p>
<p>Problem is, when I want to execute pipeline from Logic Apps, &quot;<strong>TesterPipe</strong>&quot; is fully visible in Logic App action when <strong>ADF A</strong> is selected. However, when I try to run same pipeline in <strong>ADF B</strong> via Logic App, I am not able to select it in the dropdown of available pipelines and when I write its name manually in, and run the logic app, I get an error &quot;<em>Entity TesterPipe&quot; not found</em>&quot;.</p>
<p>Notes:</p>
<ul>
<li>ADFs are in different resource groups</li>
<li>when I remove git repo from ADF B, create a new pipeline, save and publish ADF B without git repo, I can run it from the same Logic App</li>
</ul>
<p>Do you have an idea why is this happening? I can execute this pipeline in both ADFs manually, everything works as expected, except this.</p>
<p>Thank you for your help.</p>
<p><a href=""https://i.stack.imgur.com/e6IN3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e6IN3.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><azure-logic-apps>","2020-09-02 20:44:24","924","0","1","63738470","<p>This happens if you do not commit and deploy your code. I had to do it and then it started to show up</p>
"
"63709505","Azure Data Factory replace \ with '\\'. How to remove this?","<p>I have a Azure SQL Table that store file storage location copy activity. Like this:</p>
<pre><code>FileName    Location
text.csv    \\Server\Test\TargetFolder
</code></pre>
<p>When I try to build a Lookup + Copy in Azure Data Factory. The lookup result always return this
<code>\\\\Server\\Test\\TargetFolder &lt;&lt; ADF add an extra &quot;\&quot; for each &quot;\&quot;</code>.</p>
<p>Is there anyways I can remove this behavior?
Thank you in advance.</p>
","<azure><azure-data-factory>","2020-09-02 16:14:44","1229","0","1","63961377","<p>Please use the replace function , something like <code>@replace(activity('Lookup1').output.firstRow.Location,'\\','\')</code> . I added a Setvariable activity and add the expression</p>
<p>I did some testing and it does the trick .</p>
<p><a href=""https://i.stack.imgur.com/iPK2t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iPK2t.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/Dzzio.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dzzio.png"" alt=""enter image description here"" /></a></p>
"
"63707533","How to set up an email alert whenever a new azure data factory instance is created in any subscription within same organization","<p>I want to know how to set up an email alert whenever a new data factory instance is created in any subscription(e.g. Dev, UAT, Prod) within the same organization.
I only want it on ADF creation and not on Update on ADF.</p>
","<azure><factory><azure-data-factory>","2020-09-02 14:17:43","156","0","1","63710945","<p>One solution could be using Azure Monitor Alert Rules, but it is not possible to assign it on organization level, you have to create one rule for each subscription.</p>
<p>The easiest way is to use default notification mechanism but, it will trigger on both create and update event type. If you want to filter it, use action with logicApp/Function method where you can apply your own custom logic to filter event, add more information (e.g. region) and send mails.</p>
<ol>
<li>Go to Azure Monitor and click &quot;New Alert rule&quot;</li>
</ol>
<p><a href=""https://i.stack.imgur.com/ZO9m8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZO9m8.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Choose a subscription and 'Data Factory' as resource type</li>
</ol>
<p><a href=""https://i.stack.imgur.com/eI8QN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eI8QN.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>Choose &quot;Create or update&quot; signal</li>
</ol>
<p><a href=""https://i.stack.imgur.com/gDFWT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gDFWT.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>Add email for notification/action to trigger</li>
</ol>
<p><a href=""https://i.stack.imgur.com/O7R54.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O7R54.png"" alt=""enter image description here"" /></a></p>
<ol start=""5"">
<li>Create a rule</li>
</ol>
<p>Default mail looks like this:</p>
<p><a href=""https://i.stack.imgur.com/yHYpz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yHYpz.png"" alt=""enter image description here"" /></a></p>
"
"63699939","Get Fixed Width file row as single column in Azure Data Factory - Mapping data flow","<p>I have a set of Fixed width files in ADLS.
I am reading the file with the following flow:</p>
<p>MetadataActivity --&gt; ForEach.</p>
<p>Inside ForEach, I have a Mapping Data Flow with this source settings:</p>
<p><a href=""https://i.stack.imgur.com/qpT9W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qpT9W.png"" alt=""enter image description here"" /></a></p>
<p>My projection looks like :</p>
<p><a href=""https://i.stack.imgur.com/9jryV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9jryV.png"" alt=""enter image description here"" /></a></p>
<p>My expected projection is :</p>
<p><a href=""https://i.stack.imgur.com/6p6qF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6p6qF.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know what changes I have to do to get the  above projection. Thank you.</p>
","<azure><azure-data-factory>","2020-09-02 06:33:30","471","-1","2","63700370","<p>I've tried serval times to  reproduce the problem.</p>
<p>1.Have you set any value to the <strong>Column delimiter</strong> at the <strong>Source dataset</strong>?</p>
<p><a href=""https://i.stack.imgur.com/ORrfl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ORrfl.png"" alt=""enter image description here"" /></a></p>
<p>2.In my source data, it contains the ',' as follows
<a href=""https://i.stack.imgur.com/kMrt5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kMrt5.png"" alt=""enter image description here"" /></a></p>
<p>3.ADF will divide this column into three columns automatically.
<a href=""https://i.stack.imgur.com/c6MdT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c6MdT.png"" alt=""enter image description here"" /></a></p>
<p>4.If so, you should select <code>No delimiter</code>.
<a href=""https://i.stack.imgur.com/Afuhh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Afuhh.png"" alt=""enter image description here"" /></a></p>
<p>5.After select <code>No delimiter</code>, there will be just one column.
<a href=""https://i.stack.imgur.com/eI2y6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eI2y6.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>Additional</strong><br />
My Wildcard is as follows:
<a href=""https://i.stack.imgur.com/DGsCe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DGsCe.png"" alt=""enter image description here"" /></a>
I guess your source data contains any other delimiter?  Please correct me if I understand you wrong in the answer.</p>
"
"63699939","Get Fixed Width file row as single column in Azure Data Factory - Mapping data flow","<p>I have a set of Fixed width files in ADLS.
I am reading the file with the following flow:</p>
<p>MetadataActivity --&gt; ForEach.</p>
<p>Inside ForEach, I have a Mapping Data Flow with this source settings:</p>
<p><a href=""https://i.stack.imgur.com/qpT9W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qpT9W.png"" alt=""enter image description here"" /></a></p>
<p>My projection looks like :</p>
<p><a href=""https://i.stack.imgur.com/9jryV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9jryV.png"" alt=""enter image description here"" /></a></p>
<p>My expected projection is :</p>
<p><a href=""https://i.stack.imgur.com/6p6qF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6p6qF.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know what changes I have to do to get the  above projection. Thank you.</p>
","<azure><azure-data-factory>","2020-09-02 06:33:30","471","-1","2","63702153","<p>The issue got resolved after changing the schema in source dataset.</p>
<p>It was csv before:</p>
<p><a href=""https://i.stack.imgur.com/cpOla.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cpOla.png"" alt=""enter image description here"" /></a></p>
<p>Issue was fixed after change the schema to &quot; From Files with &quot;*&quot;&quot;</p>
<p><a href=""https://i.stack.imgur.com/kj9kG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kj9kG.png"" alt=""enter image description here"" /></a></p>
"
"63696985","Get filename in datafactory","<p>I am using the Get Metada activity and I need to get the filename from the activity output:</p>
<p><strong>Output Actitity</strong></p>
<pre><code>Output
{
    &quot;childItems&quot;: [
        {
            &quot;name&quot;: &quot;FILENAME.mdb&quot;,
            &quot;type&quot;: &quot;File&quot;
        }
    ],
    &quot;effectiveIntegrationRuntime&quot;: &quot;xxxxx&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 5
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;SelfhostedIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p>I appreciate any help</p>
","<azure-data-factory>","2020-09-02 00:10:45","107","0","1","63697277","<p>Use this expression:</p>
<pre><code>@activity('Get Metadata1').output.childItems[0].name
</code></pre>
<p>My test pipeline:</p>
<p><a href=""https://i.stack.imgur.com/cfLNA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cfLNA.png"" alt=""enter image description here"" /></a></p>
<p>Output of 'Set variable1':</p>
<p><a href=""https://i.stack.imgur.com/q2gyk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q2gyk.png"" alt=""enter image description here"" /></a></p>
"
"63691347","Cannot see linked service or management hub in ADF","<p>I am new to azure cloud infrastructure, I am trying to create a azure data factory, which I did now I am trying to create a linked service to another SaaS provider &quot;salesforce&quot;. I am not seeing any place to create one.</p>
<p>I have consulted the following links, but could not find anything yet.</p>
<p>I cannot see management hub
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/author-management-hub"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/author-management-hub</a></p>
<p>or in azure portal
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services</a></p>
<p>Thank you</p>
","<azure><azure-data-factory>","2020-09-01 16:01:36","339","0","1","63697360","<p>Please ref this tutorial: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce"" rel=""nofollow noreferrer"">Copy data from and to Salesforce by using Azure Data Factory</a></p>
<ul>
<li>This article outlines how to use Copy Activity in Azure Data Factory
to copy data from and to Salesforce. It builds on the Copy Activity
overview article that presents a general overview of the copy
activity.</li>
</ul>
<p>You could create the Salesforce linked service from here Data Factory UI on Portal:</p>
<p>Manage--&gt;linked services--&gt;new--&gt;Salesforce:
<a href=""https://i.stack.imgur.com/CeVX7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CeVX7.png"" alt=""enter image description here"" /></a></p>
<p>Configure the Salesforce:</p>
<p><a href=""https://i.stack.imgur.com/H7Cx2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H7Cx2.png"" alt=""enter image description here"" /></a></p>
"
"63685948","How to pass dynamic parameter to Azure Function","<p>I have a look up activity which retrives data from table and then I need to pass that data to a azure function using azure function activity.</p>
<pre><code>LOOKUP --&gt; AZUREFUNCTION
</code></pre>
<p>I am passing the below in the body of the azure function since it is a POST method</p>
<pre><code>@Activity('lookup1).output.value
</code></pre>
<p>But the pipeline fails. At the same time if I hardcode the value in body, pipeline executes.</p>
<p>Can you help me with how to go about solving this problem?</p>
","<azure><azure-functions><azure-data-factory>","2020-09-01 10:31:46","2172","1","1","63697765","<p>Can you show the details of the error you get?</p>
<p>On my side it is no problem.</p>
<p>This is the setting of my lookup activity:</p>
<p><a href=""https://i.stack.imgur.com/iv5WF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iv5WF.png"" alt=""enter image description here"" /></a></p>
<p>And this is the setting of my function activity:</p>
<p><a href=""https://i.stack.imgur.com/eb9Wt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eb9Wt.png"" alt=""enter image description here"" /></a></p>
<p>MY azure function is written by C# script:</p>
<pre><code>#r &quot;Newtonsoft.Json&quot;

using System.Net;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Primitives;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;

public static JObject Run(HttpRequest req, ILogger log)
{
    string a = &quot;{'test':'123'}&quot;;
    JObject json = JObject.Parse(a);
    return json;
}
</code></pre>
<p>It works fine:</p>
<p><a href=""https://i.stack.imgur.com/PPbbM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PPbbM.png"" alt=""enter image description here"" /></a></p>
"
"63682601","Passing File names from Foreach to Data Flow - Azure Data Factory","<p>I am trying to read ADLS files in a directory, read the content of the file, do some processing and store the file in adls but the destination file name will depend on one of the column values of input file.</p>
<p>To start with, this is my flow:
<a href=""https://i.stack.imgur.com/ho8ki.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ho8ki.png"" alt=""enter image description here"" /></a></p>
<p>Inside Metadata:</p>
<p><a href=""https://i.stack.imgur.com/WuwqU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WuwqU.png"" alt=""enter image description here"" /></a></p>
<p>Inside Foreach:</p>
<p><a href=""https://i.stack.imgur.com/YDXIy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YDXIy.png"" alt=""enter image description here"" /></a></p>
<p>I am triggering a Mapping Data Flow inside ForEach activity:</p>
<p><a href=""https://i.stack.imgur.com/SOoCt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SOoCt.png"" alt=""enter image description here"" /></a></p>
<p>My Mapping data flow inside ForEach :</p>
<p><a href=""https://i.stack.imgur.com/D1N8d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D1N8d.png"" alt=""enter image description here"" /></a></p>
<p>Source settings of Mapping data flow:</p>
<p><a href=""https://i.stack.imgur.com/ZzsPO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZzsPO.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/F8uyx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F8uyx.png"" alt=""enter image description here"" /></a></p>
<p>Inside Metadata container dataset :</p>
<p><a href=""https://i.stack.imgur.com/yTtAD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yTtAD.png"" alt=""enter image description here"" /></a></p>
<p>Filename is not getting resolved. I am getting the error message :</p>
<p><a href=""https://i.stack.imgur.com/6Z8SM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Z8SM.png"" alt=""enter image description here"" /></a></p>
<p>I am not sure if I am missing anything. Can any help. Thanks.</p>
<p>Note: I have referred this post too. Thanks
<a href=""https://stackoverflow.com/questions/59635685/azure-adf-v2-foreach-file-copydata-from-blob-storage-to-sql-table"">Azure ADF V2 ForEach File CopyData from Blob Storage to SQL Table</a></p>
","<azure><azure-data-factory>","2020-09-01 06:40:04","2646","1","2","63684444","<p>I got the answer.
I gave the full path in wildcard as below:</p>
<p><a href=""https://i.stack.imgur.com/aoXe0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aoXe0.png"" alt=""enter image description here"" /></a></p>
"
"63682601","Passing File names from Foreach to Data Flow - Azure Data Factory","<p>I am trying to read ADLS files in a directory, read the content of the file, do some processing and store the file in adls but the destination file name will depend on one of the column values of input file.</p>
<p>To start with, this is my flow:
<a href=""https://i.stack.imgur.com/ho8ki.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ho8ki.png"" alt=""enter image description here"" /></a></p>
<p>Inside Metadata:</p>
<p><a href=""https://i.stack.imgur.com/WuwqU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WuwqU.png"" alt=""enter image description here"" /></a></p>
<p>Inside Foreach:</p>
<p><a href=""https://i.stack.imgur.com/YDXIy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YDXIy.png"" alt=""enter image description here"" /></a></p>
<p>I am triggering a Mapping Data Flow inside ForEach activity:</p>
<p><a href=""https://i.stack.imgur.com/SOoCt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SOoCt.png"" alt=""enter image description here"" /></a></p>
<p>My Mapping data flow inside ForEach :</p>
<p><a href=""https://i.stack.imgur.com/D1N8d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D1N8d.png"" alt=""enter image description here"" /></a></p>
<p>Source settings of Mapping data flow:</p>
<p><a href=""https://i.stack.imgur.com/ZzsPO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZzsPO.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/F8uyx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F8uyx.png"" alt=""enter image description here"" /></a></p>
<p>Inside Metadata container dataset :</p>
<p><a href=""https://i.stack.imgur.com/yTtAD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yTtAD.png"" alt=""enter image description here"" /></a></p>
<p>Filename is not getting resolved. I am getting the error message :</p>
<p><a href=""https://i.stack.imgur.com/6Z8SM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Z8SM.png"" alt=""enter image description here"" /></a></p>
<p>I am not sure if I am missing anything. Can any help. Thanks.</p>
<p>Note: I have referred this post too. Thanks
<a href=""https://stackoverflow.com/questions/59635685/azure-adf-v2-foreach-file-copydata-from-blob-storage-to-sql-table"">Azure ADF V2 ForEach File CopyData from Blob Storage to SQL Table</a></p>
","<azure><azure-data-factory>","2020-09-01 06:40:04","2646","1","2","63685532","<p>Yes, you are absolutely right. Add to yours answer. Have you set the value of sink <strong>File Name</strong> text field. So that it will create the same file in the target file folder.
<a href=""https://i.stack.imgur.com/kl6Ju.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kl6Ju.png"" alt=""enter image description here"" /></a></p>
<p>It's value was set previous in the Foreach activity:
<a href=""https://i.stack.imgur.com/IgpQl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IgpQl.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>update:</strong><br />
What's yours <strong>Data preview</strong> looks like?
<a href=""https://i.stack.imgur.com/43tfv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/43tfv.png"" alt=""enter image description here"" /></a></p>
"
"63675748","Non-standard REST pagination approach in Data Factory?","<p>I'm trying to figure out how to work with a REST api call that is paginated.  The JSON response from each page of pagination has a flag <code>&quot;lastPage&quot;: True</code> or <code>&quot;lastPage&quot;: False</code> to specify if you've reached the last page.</p>
<p>In the Data Factory <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">REST Connector article</a>, the pagination section mentions several supported pagination schemes:</p>
<ol>
<li>Absolute or relative path of next page in response body or header</li>
<li>Query parameter for the next page in response body or header</li>
<li>Header value for next page in response body or header</li>
</ol>
<p>None of these three approaches seem to describe the type of response I'm dealing with.  What solution would work in this case?</p>
<p><a href=""https://marketplace.att.com/docs#/"" rel=""nofollow noreferrer"">Here's the documentation</a> for the API I'm working with.</p>
","<azure><azure-data-factory>","2020-08-31 17:50:07","96","0","1","63733275","<p>About your request, it's not supported for now.
You could post the feedback here: <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory</a></p>
<p>We can help you vote it up to make product team know.</p>
"
"63673480","MD5 hash of blob uploaded on Azure does not match with same file on local machine","<p>I am currently working on uploading a .json file on Azure Blob storage. All is working fine except when i try to macth the MD5 hash of uploaded file with the local file (exactly same one which was uploaded). Local file returns a byte array where are blob.Properties.ContentMD5 returns a string and both do not match.</p>
<p>Local MD5 hash: 67a45ac2700d14cc867c897182fde402 (is in hex)</p>
<p>blob.Properties.ContentMD5: c9QoHkamgiKTRANifltOGQ==</p>
<p>Any possible way to match both these?</p>
","<azure-data-factory>","2020-08-31 15:15:06","772","1","1","63680399","<p>Blob ContentMD5 is Base64 String, they are different type and can not math directly.</p>
<p>Please convert MD5 byte array into Base64 string and see if that matches.</p>
<p>Ref: <a href=""https://stackoverflow.com/questions/31183477/md5-hash-of-blob-uploaded-on-azure-doesnt-match-with-same-file-on-local-machine"">MD5 hash of blob uploaded on Azure doesnt match with same file on local machine</a></p>
"
"63670636","Azure Data Factory - ingest XML using source connector and store as CSV","<p>I have a data set to fetch an XML document from a storage account. When I push that through and store it as a CSV (or JSON) <em>(edited for clarity)</em> in ADLS gen2 <em>(edit)</em>  using either a Mapping Data Flow or just a regular pipeline activity I end up with a file containing only the first line of the document. Anyone who has been through this that can give me a tip on what I'm doing wrong?
The XML map looks like this if that helps:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;xs:element name=&quot;SAMPLE_XML&quot;&gt;
    &lt;xs:complexType&gt;
      &lt;xs:sequence&gt;
        &lt;xs:element maxOccurs=&quot;unbounded&quot; name=&quot;record&quot;&gt;
          &lt;xs:complexType&gt;
            &lt;xs:sequence&gt;
              &lt;xs:element name=&quot;LABEL&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;AUFNR&quot; type=&quot;xs:unsignedInt&quot; /&gt;
              &lt;xs:element minOccurs=&quot;0&quot; name=&quot;SYSST&quot; /&gt;
              &lt;xs:element minOccurs=&quot;0&quot; name=&quot;STTXT&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;STTXU&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;AUART&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;QMNUM&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;QMTXT&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;TPLNR&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;BEARB&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;EQUNR&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;INGPR&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;VAPLZ&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;GSTRP&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;GLTRP&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;ZZLTRMN&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;PRIOK&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;REVNR&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;ILART&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;KTEXT&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;UDATE_UTIME&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;VORNR&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;VSTTXT&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;ARBPL&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;ARBEI&quot; type=&quot;xs:decimal&quot; /&gt;
              &lt;xs:element name=&quot;ISMNW&quot; type=&quot;xs:decimal&quot; /&gt;
              &lt;xs:element name=&quot;AUFNT&quot; /&gt;
              &lt;xs:element name=&quot;PROID&quot; /&gt;
              &lt;xs:element name=&quot;ERNAM&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;ERDAT&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;AENAM&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;AEDAT&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;LTXA1&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;ANLZU&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;FSAVD&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;FSAVZ&quot; type=&quot;xs:time&quot; /&gt;
              &lt;xs:element name=&quot;FSEDD&quot; type=&quot;xs:string&quot; /&gt;
              &lt;xs:element name=&quot;FSEDZ&quot; type=&quot;xs:string&quot; /&gt;
            &lt;/xs:sequence&gt;
          &lt;/xs:complexType&gt;
        &lt;/xs:element&gt;
      &lt;/xs:sequence&gt;
    &lt;/xs:complexType&gt;
  &lt;/xs:element&gt;
&lt;/xs:schema&gt;

</code></pre>
","<azure-data-factory>","2020-08-31 12:14:44","445","0","1","63704610","<p>@MartinJaffer-MSFT in a comment to my question suggested I used a collectionReference in the mapping settings and that worked.
I hadn't used that as the description refers to JSON only. &quot;Select or specify the JSONPath of a nested JSON array for cross-apply.&quot;
But yes, that worked as he said and problem solved. Thanks.</p>
"
"63670577","Benchmarks to compare two azure data factory pipelines","<p>I have built two pipelines with different transformations for the same functionality.</p>
<p>Are there any benchmarks to compare these two pipelines in terms of efficiency and/or resource utilization?</p>
<p>To explain in detail:
Pipeline 1 : Uses only 2 Mapping data flows. One with 4 transformations and other with 20 transformations.
Pipeline 2 : Uses 2 Mapping data flows. One with 4 transformations , second DF other with 15 transformations and with Databricks notebook.</p>
<p>I want to compare these two pipelines in terms of
1.Efficieny
2.Resource utilization
3.Costs</p>
<p>Any inputs?</p>
<p>Thank you</p>
","<azure><azure-data-factory><azure-databricks>","2020-08-31 12:11:16","205","0","1","63684280","<p>I think you could compare the outputs of the pipelines, the output contains the value what you want.</p>
<p>Here's the output example of pipeline executing:</p>
<pre><code>{
    &quot;dataRead&quot;: 8192,
    &quot;dataWritten&quot;: 612,
    &quot;filesRead&quot;: 1,
    &quot;sourcePeakConnections&quot;: 1,
    &quot;sinkPeakConnections&quot;: 2,
    &quot;rowsRead&quot;: 1,
    &quot;rowsCopied&quot;: 1,
    &quot;copyDuration&quot;: 12,
    &quot;throughput&quot;: 0.667,
    &quot;errors&quot;: [],
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (East US)&quot;,
    &quot;usedDataIntegrationUnits&quot;: 4,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;DataMovement&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.06666666666666667,
                &quot;unit&quot;: &quot;DIUHours&quot;
            }
        ]
    },
    &quot;usedParallelCopies&quot;: 1,
    &quot;executionDetails&quot;: [
        {
            &quot;source&quot;: {
                &quot;type&quot;: &quot;AzureBlobStorage&quot;,
                &quot;region&quot;: &quot;Central US&quot;
            },
            &quot;sink&quot;: {
                &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                &quot;region&quot;: &quot;East US&quot;
            },
            &quot;status&quot;: &quot;Succeeded&quot;,
            &quot;start&quot;: &quot;2020-09-01T08:20:09.1734161Z&quot;,
            &quot;duration&quot;: 12,
            &quot;usedDataIntegrationUnits&quot;: 4,
            &quot;usedParallelCopies&quot;: 1,
            &quot;profile&quot;: {
                &quot;queue&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 9
                },
                &quot;transfer&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 3,
                    &quot;details&quot;: {
                        &quot;listingSource&quot;: {
                            &quot;type&quot;: &quot;AzureBlobStorage&quot;,
                            &quot;workingDuration&quot;: 0
                        },
                        &quot;readingFromSource&quot;: {
                            &quot;type&quot;: &quot;AzureBlobStorage&quot;,
                            &quot;workingDuration&quot;: 0
                        },
                        &quot;writingToSink&quot;: {
                            &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                            &quot;workingDuration&quot;: 0
                        }
                    }
                }
            },
            &quot;detailedDurations&quot;: {
                &quot;queuingDuration&quot;: 9,
                &quot;transferDuration&quot;: 3
            }
        }
    ],
    &quot;dataConsistencyVerification&quot;: {
        &quot;VerificationResult&quot;: &quot;NotVerified&quot;
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    }
}
</code></pre>
<p>On Portal:</p>
<p><a href=""https://i.stack.imgur.com/sOycG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sOycG.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/bFlG6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bFlG6.png"" alt=""enter image description here"" /></a></p>
"
"63649267","Publish from PowerShell directly to Data Lake Storage or output as a sink for ADF","<p>I have a PowerShell script which downloads audit logs from Azure. The Export-CSV function outputs the file to my local computer. My plan however is to run this script every night using Azure Data Factory and then output the log file directly to Data Lake Storage, not locally.</p>
<p><strong>ADF &gt; PowerShell Script &gt; Data Lake Storage</strong></p>
<p>I need to amend this script so that it either outputs the CSV file directly to Data Lake Storage OR it outputs it so that ADF can channel it to a sink (Data Lake Storage).</p>
<pre><code>Set-ExecutionPolicy RemoteSigned 

#This is better for scheduled jobs
$User = &quot;admin@M365XXXXXX.onmicrosoft.com&quot;
$PWord = ConvertTo-SecureString -String &quot;XXXXXXXX&quot; -AsPlainText -Force
$UserCredential = New-Object -TypeName &quot;System.Management.Automation.PSCredential&quot; -ArgumentList $User, $PWord

#This will prompt the user for credential
#$UserCredential = Get-Credential

$Session = New-PSSession -ConfigurationName Microsoft.Exchange -ConnectionUri https://outlook.office365.com/powershell-liveid/ -Credential $UserCredential -Authentication Basic -AllowRedirection
Import-PSSession $Session

$startDate=(get-date).AddDays(-5)
$endDate=(get-date)
$scriptStart=(get-date)

$sessionName = (get-date -Format 'u')+'pbiauditlog'
# Reset user audit accumulator
$aggregateResults = @()
$i = 0 # Loop counter
Do { 
    $currentResults = Search-UnifiedAuditLog -StartDate $startDate -EndDate $enddate -SessionId $sessionName -SessionCommand ReturnLargeSet -ResultSize 1000 -RecordType PowerBIAudit 
    if ($currentResults.Count -gt 0) {
        Write-Host (&quot;  Finished {3} search #{1}, {2} records: {0} min&quot; -f [math]::Round((New-TimeSpan -Start $scriptStart).TotalMinutes,4), $i, $currentResults.Count, $user.UserPrincipalName )
        # Accumulate the data
        $aggregateResults += $currentResults
        # No need to do another query if the # recs returned &lt;1k - should save around 5-10 sec per user
        if ($currentResults.Count -lt 1000) {
            $currentResults = @()
        } else {
            $i++
        }
    }
} Until ($currentResults.Count -eq 0) # --- End of Session Search Loop --- #

$data=@()

foreach ($auditlogitem in $aggregateResults) {
    $d=convertfrom-json $auditlogitem.AuditData
    $datum = New-Object –TypeName PSObject
    $d=convertfrom-json $auditlogitem.AuditData
    $datum | Add-Member –MemberType NoteProperty –Name Id –Value $d.Id
    $datum | Add-Member –MemberType NoteProperty –Name CreationDateTime –Value $auditlogitem.CreationDate
    $datum | Add-Member –MemberType NoteProperty –Name CreationTimeUTC –Value $d.CreationTime
    $datum | Add-Member –MemberType NoteProperty –Name RecordType –Value $d.RecordType
    $datum | Add-Member –MemberType NoteProperty –Name Operation –Value $d.Operation
    $datum | Add-Member –MemberType NoteProperty –Name OrganizationId –Value $d.OrganizationId
    $datum | Add-Member –MemberType NoteProperty –Name UserType –Value $d.UserType
    $datum | Add-Member –MemberType NoteProperty –Name UserKey –Value $d.UserKey
    $datum | Add-Member –MemberType NoteProperty –Name Workload –Value $d.Workload
    $datum | Add-Member –MemberType NoteProperty –Name UserId –Value $d.UserId
    $datum | Add-Member –MemberType NoteProperty –Name ClientIP –Value $d.ClientIP
    $datum | Add-Member –MemberType NoteProperty –Name UserAgent –Value $d.UserAgent
    $datum | Add-Member –MemberType NoteProperty –Name Activity –Value $d.Activity
    $datum | Add-Member –MemberType NoteProperty –Name ItemName –Value $d.ItemName
    $datum | Add-Member –MemberType NoteProperty –Name WorkSpaceName –Value $d.WorkSpaceName
    $datum | Add-Member –MemberType NoteProperty –Name DashboardName –Value $d.DashboardName
    $datum | Add-Member –MemberType NoteProperty –Name DatasetName –Value $d.DatasetName
    $datum | Add-Member –MemberType NoteProperty –Name ReportName –Value $d.ReportName
    $datum | Add-Member –MemberType NoteProperty –Name WorkspaceId –Value $d.WorkspaceId
    $datum | Add-Member –MemberType NoteProperty –Name ObjectId –Value $d.ObjectId
    $datum | Add-Member –MemberType NoteProperty –Name DashboardId –Value $d.DashboardId
    $datum | Add-Member –MemberType NoteProperty –Name DatasetId –Value $d.DatasetId
    $datum | Add-Member –MemberType NoteProperty –Name ReportId –Value $d.ReportId
    $datum | Add-Member –MemberType NoteProperty –Name OrgAppPermission –Value $d.OrgAppPermission
    
    #option to include the below JSON column however for large amounts of data it may be difficult for PBI to parse
    #$datum | Add-Member –MemberType NoteProperty –Name Datasets –Value (ConvertTo-Json $d.Datasets)

    #below is a PowerShell statement to grab one of the entries and place in the DatasetName if any exist
    foreach ($dataset in $d.datasets) {
        $datum.DatasetName = $dataset.DatasetName
        $datum.DatasetId = $dataset.DatasetId
    }
    $data+=$datum
}

$datestring = $startDate.ToString(&quot;yyyyMMdd&quot;)
$fileName = (&quot;C:\Users\Client\Audit Logging\Logs\&quot; + $datestring + &quot;.csv&quot;)
Write-Host (&quot;Writing to file {0}&quot; -f $fileName) 
$data | Export-csv -Path $fileName

Remove-PSSession -Id $Session.Id
</code></pre>
<p>I did start writing some code to connect to Data Lake Storage as follows, but not sure how to integrate this with the above Export-CSV function. How do I get the CSV file to be published to Data Lake Storage (as it won't be stored locally) or output so that ADF can direct it to a sink store?</p>
<pre><code># Variable Declaration
$rgName = &quot;Audit&quot;
$subscriptionID = &quot;dabdhnca9-0742-48b2-98d5-af476d62c6bd&quot;
$dataLakeStoreName = &quot;pbiauditingstorage12&quot;
$myDataRootFolder = &quot;/auditlogs&quot;
#$sourceFilesPath = &quot;C:\Users\Downloads\datasets\&quot;
 
# Log in to your Azure account
 Login-AzureRmAccount
# List all the subscriptions associated to your account
 Get-AzureRmSubscription
# Select a subscription
Set-AzureRmContext -SubscriptionId $subscriptionID
 
# See if folder exists.
# If a folder or item does not exiss, then you will see
#  Get-AzureRmDataLakeStoreChildItem : Operation returned an invalid status code 'NotFound'
Get-AzureRmDataLakeStoreChildItem -AccountName $dataLakeStoreName -Path $myDataRootFolder
 
# Create new folder
New-AzureRmDataLakeStoreItem -Folder -AccountName $dataLakeStoreName -Path $myDataRootFolder/population

# Upload folder and its contents recursively and force ovewrite existing
Import-AzureRmDataLakeStoreItem -AccountName $dataLakeStoreName `
    -Path $sourceFilesPath\ `
    -Destination $myDataRootFolder `
    -Recurse `
    -Force
</code></pre>
<p>Please advise, many thanks!</p>
","<powershell><azure-data-factory><azure-data-lake-gen2>","2020-08-29 16:24:26","550","0","1","63670890","<p>Managed to make it work after passing the exported file's path ($filepath) as the &quot;-File&quot; source parameter using Set-AzStorageBlobContent function:</p>
<pre><code>$User = &quot;sdcadmin@M36dcdcdc.onmicrosoft.com&quot;
$PWord = ConvertTo-SecureString -String &quot;eVadcdcdcR&quot; -AsPlainText -Force
$UserCredential = New-Object -TypeName &quot;System.Management.Automation.PSCredential&quot; -ArgumentList $User, $PWord

$dateTimestring = $startDate.ToString(&quot;yyyyMMdd&quot;) + &quot;_&quot; + (Get-Date -Format &quot;yyyyMMdd&quot;) + &quot;_&quot; + (Get-Date -Format &quot;HHmm&quot;)
$fileName = ($dateTimestring + &quot;.csv&quot;)
Write-Host (&quot;Writing to file {0}&quot; -f $fileName) 
$filePath = &quot;$Env:temp/&quot; + $fileName
$data | Export-csv -Path $filePath

# File transfer to Azure storage account 
Connect-AzAccount -Credential $UserCredential
Get-AzVM -ResourceGroupName &quot;Audit&quot; -status
    $Context = New-AzStorageContext -StorageAccountName &quot;storageaccountname&quot; -StorageAccountKey &quot;sdfvsdvdsvsfvIdb6JgnnazfLIPDU8kOozDDn15262591efq5sdfvsdfv3M5ew==&quot;
    Set-AzStorageBlobContent -Force -Context $Context -Container &quot;auditlogs&quot; -File $filename -Blob $filename 
</code></pre>
"
"63639372","ADF / Dataflow - Convert Multiple CSV to Parquet","<p>In ADLS Gen2, TextFiles folder has 3 CSV files. Column names are different in each file.</p>
<p>We need to convert all 3 CSV files to 3 parquet files and put it in ParquetFiles folder</p>
<p>I tried to use Copy Activity and it fails because the column names have empty space in it and parquet files doesn't allow it</p>
<p>To remove spaces, I used Data flow: Source -&gt; Select (replace space by underscore in col name) and sink. This worked for a single file. When I tried to do it for all 3 files, it tries to merge 3 files and generates single file with incorrect data.</p>
<p>How to solve this, mainly removing spaces from column names in all files. What would be the other options here?</p>
","<azure-data-factory>","2020-08-28 18:46:57","1639","0","2","63639476","<p>Pipeline: ForEach activity (loop over CSV files in folder and send in current iteration item to data flow as param) -&gt; Data Flow activity with source that points to that folder (parameterize the file name in the source path)</p>
"
"63639372","ADF / Dataflow - Convert Multiple CSV to Parquet","<p>In ADLS Gen2, TextFiles folder has 3 CSV files. Column names are different in each file.</p>
<p>We need to convert all 3 CSV files to 3 parquet files and put it in ParquetFiles folder</p>
<p>I tried to use Copy Activity and it fails because the column names have empty space in it and parquet files doesn't allow it</p>
<p>To remove spaces, I used Data flow: Source -&gt; Select (replace space by underscore in col name) and sink. This worked for a single file. When I tried to do it for all 3 files, it tries to merge 3 files and generates single file with incorrect data.</p>
<p>How to solve this, mainly removing spaces from column names in all files. What would be the other options here?</p>
","<azure-data-factory>","2020-08-28 18:46:57","1639","0","2","69475270","<p>I created 2 datasets, one in csv with wildcard format, the other in parquet. I used the Data Copy Activity using the parquet data set as sink and csv data set as source. I set the copy behavior to Merge files.</p>
"
"63637332","ADF Dataflow - Replace spaces with underscore in column names","<p>To remove spaces with underscore I am using replace($$,' ','_') expression in Select transformation</p>
<p>It works for a column &quot;Period Key&quot; and makes it &quot;Period_Key&quot; but for another column &quot;Week in Month Description&quot; it makes it &quot;Week_in Month Description&quot;. So it is replacing only first occurrence</p>
<p>Can someone try this? Or how can we write regex for this?</p>
<p><a href=""https://i.stack.imgur.com/K4xmr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K4xmr.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-28 16:04:44","3244","0","1","63639404","<p>I used below function and it worked</p>
<pre><code>regexReplace($$,' ','_')
</code></pre>
"
"63634415","ADF Mapping Data Flows - Reuse single running spark cluster for parallel execution of mapping data flows","<p>We have a complex ETL in ADF running multiple pipelines with data flow activities to load several tables in a data-warehouse based on table dependencies.</p>
<p>As a result of running multiple pipelines with inter-dependencies, several data flows are executed as a mix of some running sequentially and some running in parallel. It looks like each data flow running in parallel spins up a new spark cluster, which is causing our daily ETL run cost to skyrise!</p>
<p>Ideally we would like the spark cluster to be reused for all parallel data flow execution, if possible. Is there a way to specify an upper limit for the number of spark clusters that should be created for parallel data flow execution?</p>
<p>We already have TTL enabled for 10 mins.</p>
","<azure><apache-spark><azure-data-factory>","2020-08-28 13:08:34","819","1","1","63638474","<p>When you have TTL enabled, make sure to execute data flows using that Azure IR in sequence so that you don't spin-up multiple cluster pools.</p>
<p>To execute in parallel, use Azure IR without TTL.</p>
<p>We are working on the &quot;max concurrency&quot; feature you mention above, hope to land that soon.</p>
"
"63633959","Pivoting based on Row Number in Azure Data Factory - Mapping Data Flow","<p>I am new to Azure and am trying to see if the below result is achievable with data factory / mapping data flow without Databricks.</p>
<p>I have my csv file with this sample data :</p>
<p><a href=""https://i.stack.imgur.com/nDpFD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nDpFD.png"" alt=""enter image description here"" /></a></p>
<p>I have following data in my table :</p>
<p><a href=""https://i.stack.imgur.com/Tqyfq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tqyfq.png"" alt=""enter image description here"" /></a></p>
<p>My expected data/ result:</p>
<p><a href=""https://i.stack.imgur.com/nBIEz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nBIEz.png"" alt=""enter image description here"" /></a></p>
<p>Which transformations would be helpful to achieve this?</p>
<p>Thanks.</p>
","<azure><azure-data-factory>","2020-08-28 12:38:30","880","-1","1","63665544","<p>Now, you have the RowNumber column, you can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-pivot"" rel=""nofollow noreferrer"">pivot activity</a> to do row-column pivoting.<br />
I used your sample data to made a test as follows:</p>
<ol>
<li>My <strong>Projection</strong> tab is like this:
<a href=""https://i.stack.imgur.com/iDq0B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iDq0B.png"" alt=""enter image description here"" /></a></li>
<li>My <strong>DataPreview</strong> is like this:
<a href=""https://i.stack.imgur.com/EBg3F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EBg3F.png"" alt=""enter image description here"" /></a></li>
<li>In the <strong>Pivot1</strong> activity, we select <strong>Table_Name</strong> and <strong>Row_Number</strong> columns to group by. If you don't want <strong>Table_Name</strong> column, you can delete it here.
<a href=""https://i.stack.imgur.com/hcWHG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hcWHG.png"" alt=""enter image description here"" /></a></li>
<li>At <strong>Pivote key</strong> tab, we select <strong>Col_Name</strong> column.
<a href=""https://i.stack.imgur.com/kBb9M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kBb9M.png"" alt=""enter image description here"" /></a></li>
<li>At <strong>Pivoted columns</strong>, we must select a agrregate function to aggregate the <strong>Value</strong> column, here I use max().
<a href=""https://i.stack.imgur.com/GzKYF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GzKYF.png"" alt=""enter image description here"" /></a></li>
<li>The result shows:
<a href=""https://i.stack.imgur.com/tzeuQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tzeuQ.png"" alt=""enter image description here"" /></a></li>
</ol>
<p>Please correct me if I understand you wrong in the answer.</p>
<hr />
<p><strong>update:</strong></p>
<ol>
<li>The data source like this:
<a href=""https://i.stack.imgur.com/RDAIl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RDAIl.png"" alt=""enter image description here"" /></a></li>
<li>The result shows as you saied, ADF sorts the column alphabetically.It seems no way to customize sorting:
<a href=""https://i.stack.imgur.com/LRinR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LRinR.png"" alt=""enter image description here"" /></a></li>
<li>But when we done the sink activity, it will auto mapping into your sql result table.
<a href=""https://i.stack.imgur.com/2H5ui.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2H5ui.png"" alt=""enter image description here"" /></a></li>
</ol>
"
"63631933","ADF CDM Source Transformation not reading data","<p>I am having an issue with the inline data set for Common Data Model in Azure Data Factory.
Simply, everything in ADF appears to connect and read from my manifest file and entity definition - but when I click the &quot;Data preview&quot; button, I always get &quot;No output data&quot; - which I find bizarre, as the data can be read perfectly when using the CDM connector to the same files in PowerBI. What am I doing wrong to mean that the data is not read into the data preview and subsequent transformations in the mapping data flow?</p>
<p>My Manifest file looks as below (referring to an example entity):</p>
<pre><code>{
    &quot;$schema&quot;: &quot;CdmManifest.cdm.json&quot;,
    &quot;jsonSchemaSemanticVersion&quot;: &quot;1.0.0&quot;,
    &quot;imports&quot;: [
        {
            &quot;corpusPath&quot;: &quot;cdm:/foundations.cdm.json&quot;
        }
    ],
    &quot;manifestName&quot;: &quot;manifestExample&quot;,
    &quot;explanation&quot;: &quot;example&quot;,
    &quot;entities&quot;: [
        {
            &quot;type&quot;: &quot;LocalEntity&quot;,
            &quot;entityName&quot;: &quot;Entityname&quot;,
            &quot;entityPath&quot;: &quot;folder/EntityName.cdm.json/Entityname&quot;,
            &quot;dataPartitions&quot;: [
                {
                    &quot;location&quot;: &quot;folder/data/Entityname/Entityname.csv&quot;,
                    &quot;exhibitsTraits&quot;: [
                        {
                            &quot;traitReference&quot;: &quot;is.partition.format.CSV&quot;,
                            &quot;arguments&quot;: [
                                {
                                    &quot;name&quot;: &quot;columnHeaders&quot;,
                                    &quot;value&quot;: &quot;true&quot;
                                },
                                {
                                    &quot;name&quot;: &quot;delimiter&quot;,
                                    &quot;value&quot;: &quot;,&quot;
                                }
                            ]
                        }
                    ]
                }
            ]
        },
...
</code></pre>
","<azure-data-factory><common-data-service>","2020-08-28 10:23:29","524","1","1","63720096","<p>I am having exactly same output message &quot;No output data&quot;. I am using json not manifest. If i sink the source it moves no data but without error. My CDM originates from PowerBI dataflow. The PowerApps works fine but historization and privileges make it useless.</p>
<p>Edit:
On Microsofts info on preview feature we can find this
<a href=""https://i.stack.imgur.com/A8SGd.png"" rel=""nofollow noreferrer"">screen</a>. I will make a guess that CDM the ADS sources is not the same which orignates from Power BI.</p>
"
"63631032","Stripping Carriage Returns in ADF Data Wrangle not working","<p>I'm currently trying to build an ADF pipeline using the new Data Wrangling Data Flow, which is effectively the Power Query element of PowerBI as far as I can see (I'm more of a PBI developer!).</p>
<p>In the data flow, I am picking up a CSV file from an SFTP location and using the wrangle to transform the data and load into a SQL server database.</p>
<p>I am successfully picking up the file and loading it into a table, however the CSV contains carriage returns within the cells, which cause additional lines to be inserted into my table.</p>
<p>Using the wrangling data flow, I have added a step that removes the carriage return. I can visibly see the change has been applied in the post steps:</p>
<p>Pre Change: <a href=""https://i.stack.imgur.com/HublJ.png"" rel=""nofollow noreferrer"">Example of pre change</a></p>
<p>Post Change: <a href=""https://i.stack.imgur.com/10BLG.png"" rel=""nofollow noreferrer"">Example of post change</a></p>
<p>However, when I pass the data wrangling step into my pipeline, it seems to load the data ignoring the step to remove the #(CR)#(LF) - i.e. the carriage return inserts as new lines into my table. <a href=""https://i.stack.imgur.com/ecFUe.png"" rel=""nofollow noreferrer"">Example of Data Inserted to Table</a></p>
<p>So I guess my question here is does anyone have any experience of using a Data Wrangling data flow to strip out carriage returns and if so can they give me a bit of guidance as to how they made it work? As far as I can see, the carriage returns are taken into account before it goes through the data wrangle - which kinda defeats the objective of using it!</p>
<p>Thanks</p>
<p>Nick</p>
","<pipeline><azure-data-factory><carriage-return><data-wrangling>","2020-08-28 09:25:16","760","0","1","63701941","<p>It looks as if this is a limitation of the tool currently - given it is only in preview, this will likely be resolved going forwards, however at this time it does not appear as if the functionality to strip carriage returns is working</p>
"
"63629429","How to use dynamic connectors for source data in azure data factory pipeline","<p>I want to create a data pipeline using Azure Data Factory where the source data can come from ADLS, Oracle, S3 depending on how the user chooses to provide the data. What that means is I have to use three different connectors in the pipeline. Once the data is provided in any one of the sources, I want to do data transformation using databricks and again store the output corresponding to the source selected. For example, If I select S3 as a source then my output should also be stored in the same S3 bucket. If I choose the Oracle table as a source then the output should be stored in an oracle table. Basically trying to make a generic pipeline. Is it feasible to do it using ADF and Azure DevOps? Any pointers on how to get started would be really helpful.</p>
","<azure><azure-devops><azure-data-factory>","2020-08-28 07:36:58","114","0","1","63666769","<p>No, we can't. Data Factory connector use linked service to connect to the dataset. Different source need different drivers or settings(file path/type, schema and so on). It's impossible for now.</p>
"
"63628112","How to make a generic pipeline for data transformation using Azure Databricks and Data Factory","<p>I have a requirement to create a GUI to get some user input and also they can import a CSV file from GUI. Once the file is imported, I want to do data transformation on that file using Azure databricks(pyspark) and store the transformed data somewhere so that the user can download the transformed data. I would like to know how to make it a generic pipeline so that anyone in the organization can upload their file(it can have different columns and datatypes) and databricks does the transformation and stores the result. And for all these activities I want to leverage the Azure platform.</p>
","<azure><azure-data-factory><azure-web-app-service><azure-databricks>","2020-08-28 05:46:11","225","0","1","63628284","<p>Your questions is quite vague, but here are some pointers.</p>
<p>Build your UI to upload the file to a folder in ADLS Gen2 blob storage. <a href=""https://www.c-sharpcorner.com/article/upload-files-in-azure-blob-storage-using-asp-net-core/"" rel=""nofollow noreferrer"">Example here.</a>
Your ASP.NET application can then kick off a databricks notebook using the <a href=""https://docs.databricks.com/dev-tools/api/latest/jobs.html"" rel=""nofollow noreferrer"">Jobs API</a> to do the transformations. Alternatively you can use <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview"" rel=""nofollow noreferrer"">Event Grid</a> in Azure as an alternative to detect the new file and process it.
If there are features in ADF (Azure data factory) that you need in addition to databricks, you can <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">kick off an ADF job through an upload</a>. Your ADF can also call databricks using the databricks activity.</p>
<p>Since all of the above are asynchronous to your web application, you will need to notify your user of the file becoming available. You can have your UI detect the new file based on a convention and/or metadata, or call <a href=""https://sendgrid.com/docs/API_Reference/index.html"" rel=""nofollow noreferrer"">Sendgrid</a> at the end of the databricks job (or via Event Grid) to send a notification email.</p>
<p>So, there are a few options. Keep it simple :)</p>
"
"63625287","Azure data factory data flow silently NULLing date column","<p>I'm trying to use Azure Data Factory to upsert a CSV into an Azure SQL table. All seemed well until I checked the results. One of the columns is a nullable date. The CSV contains a value like so <code>1/2/2020 12:00:00 AM</code>. The data flow silently inserts a <code>NULL</code> instead of throwing an error because it didn't like the input. So how can I get my data flow to convert the string to a datetime properly, and then to error out on issues like this in the future? I really don't want silent failures and bad data.</p>
","<csv><azure-sql-database><azure-data-factory>","2020-08-27 23:07:56","3812","1","1","63626648","<p>The null value is due to incompatible date formats in ADF. You need to do date format conversion.</p>
<p>Is your source date format like this <strong>MM/dd/yyyy HH:mm:ss</strong>?<br />
If so, you can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived column</a> and add the expression <code>toString(toTimestamp(&lt;Your_Column_Name&gt;,'MM/dd/yyyy HH:mm:ss'),'yyyy-MM-dd HH:mm:SS')</code> to format this column to String. It solved the <code>NULL</code> value. Of course you can choose what the date format you want.</p>
<p>I made a test as follows:</p>
<ol>
<li><p>My data source is from a csv file and the <code>EmpDate</code> is a date type like yours and last row contains a null value.
<a href=""https://i.stack.imgur.com/hcWdq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hcWdq.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Then I add the expression <code>toString(toTimestamp(EmpDate,'MM/dd/yyyy HH:mm:ss'),'yyyy-MM-dd HH:mm:SS')</code> in the Derived column activity. Here you can choose the date format what you want.
<a href=""https://i.stack.imgur.com/cg2AZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cg2AZ.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>3.According to Mark Kromer's suggestion, I Add Conditional Split directly after the Derived Column and check for isNull(EmpDate). Here I use <code>not(isNull(EmpDate))</code> expression.
<a href=""https://i.stack.imgur.com/t0y9u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t0y9u.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>In the end, if the <code>EmpDate</code> contains null value, it will go to sink2 else go to sink1.
<a href=""https://i.stack.imgur.com/61v9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/61v9E.png"" alt=""enter image description here"" /></a>
The row contains null value:
<a href=""https://i.stack.imgur.com/QXrIC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QXrIC.png"" alt=""enter image description here"" /></a></li>
</ol>
"
"63620861","How to use the same pipeline in different environments with varying number of customers inside Azure Data Factory?","<p>I have a copy data pipeline in the Azure Data Factory. I need to deploy the same Data Factory instance in multiple environments like DEV, QA, PROD using Release Pipeline.</p>
<p>The pipeline transfer data from <strong>Customer Storage Account (Blob Container)</strong> to <strong>Centralized Data Lake</strong>. So, we can say - its a Many to One flow. <em>(Many customers &gt; One Data Lake)</em></p>
<p>Now, suppose I am in DEV environment &amp; I have 1 demo customer there. I have defined an ADF pipeline for Copy Data. But in prod environment, the number of customers will grow. So, I don't want to create multiple copies of the same pipeline in production Data Factory.</p>
<p>I am looking out for a solution so that I can keep one copy pipeline in Data Factory and deploy/promote the same Data Factory from one environment to the other environment. And this should work even if the number of customers is varying from one to another.</p>
<p>I am also doing CI/CD in Azure Data Factory using Git integration with Azure Repos.</p>
","<azure><etl><azure-data-factory>","2020-08-27 17:00:11","786","1","1","63623730","<p>You will have to create additional linked services and datasets which do not exist in a non-production environment to ensure any new &quot;customer&quot; storage account is mapped to the pipeline instance.</p>
<p>With CI/CD routines, you can deliver this in an incremental manner i.e. parameterize you release pipeline with variable groups and update the data factory instance with newer pipelines with new datasets/linked services.</p>
"
"63616825","Converting Fixed width file to delimited file in Azure Data Factory - Mapping Data Flow","<p>I have ten files (.txt) in ADLS. I have their metadata in Azure SQL db.
My Metadata looks like this :</p>
<p><a href=""https://i.stack.imgur.com/2MHDr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2MHDr.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to convert the fixed width file into delimited files with header using Mapping Data flow.
Only reference from Microsoft on this topic is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-fixed-width"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-fixed-width</a>.</p>
<p>But I have multiple files with varying number of columns.
Is there any way I can pass this metadata from table to Derived columns transformation.
I know it is easily achievable with Databricks. But I have to do this with Dataflow.</p>
<p>Any references or pointers will be really helpful.</p>
<p>Thank you.</p>
","<azure><azure-data-factory>","2020-08-27 13:14:23","1249","-1","1","63630464","<p>Yes, you can pass this metadata from table to Derived columns transformation.</p>
<p>Please follow my steps:</p>
<ol>
<li>I linked the <strong>DataSource</strong> to my ADLS's test file folder which contains T1.txt,T2.txt and T3.txt. It's important to give a name to the <strong>Column to store file name</strong>, so that we can get the file metadata(Where the data comes from).
<a href=""https://i.stack.imgur.com/wjxUg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wjxUg.png"" alt=""enter image description here"" /></a>
In the Data preview tab, we can see the info:
<a href=""https://i.stack.imgur.com/b1cCS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b1cCS.png"" alt=""enter image description here"" /></a></li>
<li>Then I use the expression <code>replace(replace(FileName,'/',''),'.txt','')</code> to replace the redundant strings.
<a href=""https://i.stack.imgur.com/IA1lU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IA1lU.png"" alt=""enter image description here"" /></a></li>
<li>In the <strong>SQLSource</strong>, I created a table following your example.
<a href=""https://i.stack.imgur.com/VTOm5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VTOm5.png"" alt=""enter image description here"" /></a></li>
<li>Then I joined the two sources.
<a href=""https://i.stack.imgur.com/fVpSV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fVpSV.png"" alt=""enter image description here"" /></a></li>
<li>I use the expression <code>substring(toString({_col0_}), startPosition, data_length)</code>  to split the string according to metadata.
<a href=""https://i.stack.imgur.com/b71TF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b71TF.png"" alt=""enter image description here"" /></a></li>
<li>The result shows:
<a href=""https://i.stack.imgur.com/57Mza.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/57Mza.png"" alt=""enter image description here"" /></a></li>
</ol>
<p>Hope my answer is helpful to you.</p>
"
"63615915","How to compare max min values of one column to another column in Azure Data Fatory","<p><a href=""https://i.stack.imgur.com/WWvjO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WWvjO.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/eJhqB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eJhqB.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/I3DBp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I3DBp.png"" alt=""enter image description here"" /></a></p>
<p>In the figure below as mentioned I have two data set and I have created a data flow in azure data factory.
I want to get the max and min date from dataset1 and compare it with dataset2.
As in the below example I have min(date_time) = 11-04-2020 01:17:40 and max(date_time) = 30-06-2020  22:00:00. So the dates which fall between this two max and min will be compared with Event_time column in dataset2 and all the matching dates should be printed as &quot;Y&quot; in Dataset1_Data_available column and non matching should be &quot;N&quot;.
Thanks!!</p>
","<azure><azure-data-factory>","2020-08-27 12:22:50","1242","0","1","63628915","<p>Please try this:</p>
<p>screenshot of my test data flow
<a href=""https://i.stack.imgur.com/PH7HQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PH7HQ.png"" alt=""enter image description here"" /></a></p>
<p><strong>1.source1 is your Dataset1,setting of 'Aggregate1':</strong>
<a href=""https://i.stack.imgur.com/VOEUD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VOEUD.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/okNpQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/okNpQ.png"" alt=""enter image description here"" /></a></p>
<p>Data preview of 'Aggregate1':
<a href=""https://i.stack.imgur.com/CWIzQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CWIzQ.png"" alt=""enter image description here"" /></a></p>
<p><strong>2.source2 is your Dataset2,setting of 'Lookup1'(In lookup conditions,please make sure left column value never equal right column value):</strong>
<a href=""https://i.stack.imgur.com/OLPyR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OLPyR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/7iAM5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7iAM5.png"" alt=""enter image description here"" /></a></p>
<p>Data preview of 'Lookup1':
<a href=""https://i.stack.imgur.com/S0Aa1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S0Aa1.png"" alt=""enter image description here"" /></a></p>
<p><strong>3.use 'DerivedColumn1' to change the value of 'Dataset1_Data_available'</strong></p>
<p>expression:<code>iif(greaterOrEqual(Event_time, minDateTime) &amp;&amp; lesserOrEqual(Event_time, maxDateTime),'Y','N')</code>
<a href=""https://i.stack.imgur.com/83tCB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/83tCB.png"" alt=""enter image description here"" /></a></p>
<p>Data preview of 'DerivedColumn1'
<a href=""https://i.stack.imgur.com/Ypj51.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ypj51.png"" alt=""enter image description here"" /></a></p>
<p><strong>4.You can use 'select' or 'mapping' in sink to delete columns you don't need.Then output the result.</strong></p>
<hr />
<p><strong>UPDATE</strong></p>
<p>I create some test sample data.
Data preview of <code>sorce1</code>(expected min Date should be '11-04-2020 01:17:40' and max Date should be '24-07-2020 08:09:02'):
<a href=""https://i.stack.imgur.com/dfvuw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dfvuw.png"" alt=""enter image description here"" /></a></p>
<p>When we use <code>min(date_time)</code> and <code>max(date_time)</code> in 'Aggregate1'(min Date:'07-06-2020 04:30:40' max Date:'30-04-2020 00:56:56'):
<a href=""https://i.stack.imgur.com/n1tXC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n1tXC.png"" alt=""enter image description here"" /></a></p>
<p>When use <code>min(toTimestamp(substring(date_time, 4, 2)+'-'+substring(date_time, 1,2) + substring(date_time,6,14),'MM-dd-yyyy HH:mm:ss'))</code> and <code>max(toTimestamp(substring(date_time, 4, 2)+'-'+substring(date_time, 1,2) + substring(date_time,6,14),'MM-dd-yyyy HH:mm:ss'))</code>(the same as we expected value):
<a href=""https://i.stack.imgur.com/9wlZ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9wlZ2.png"" alt=""enter image description here"" /></a></p>
"
"63613650","Azure Data Factory: Migration of pipelines from one data factory to another","<p>I have some pipelines which I want to move from one data factory to another. Is there any possible way to migrate them?</p>
","<azure><azure-data-factory>","2020-08-27 10:03:44","1777","1","2","63614903","<p>There is an import/export feature in the data factory canvas which supports this use case.</p>
<p>Moreover, this is the case where continuous deployment and integration proves very useful. More literature can be found - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a></p>
"
"63613650","Azure Data Factory: Migration of pipelines from one data factory to another","<p>I have some pipelines which I want to move from one data factory to another. Is there any possible way to migrate them?</p>
","<azure><azure-data-factory>","2020-08-27 10:03:44","1777","1","2","63620331","<p>The easiest way to do this is to just pull the git repo for the source factory down to your local file system and then just copy and paste the desired files into your destination factory folder structure.  That's it.</p>
<p>Alternatively, you can do this through the ADF editor by creating a shell of the pipeline in the target factory first, then go to the source factory and switch to the code view for that pipeline, copy and paste that code into the target pipeline shell you created, and then save from there.</p>
<p>A pipeline is just json.  You may need to copy the dependent objects also, but those are done the exact same way.</p>
"
"63610320","Azure Data Factory not able to parameterize Key Vault secretName inside Blob Storage linked service","<p>I'm trying to create some kind of <code>AzureBlobStorage</code> linked service which will retrieve it's <code>accountKey</code> from Key Vault by dynamically provided <code>secretName</code>. I'm trying to follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#account-key-authentication"" rel=""nofollow noreferrer"">this guide</a>.
So for this I've prepared this json:</p>
<pre><code>{
    &quot;name&quot;: &quot;AzureBlobStorageLinkedServiceWithDynamicStorageAccountKeysFromKeyVault&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;AzureBlobStorage&quot;,
        &quot;typeProperties&quot;: {
            &quot;connectionString&quot;: &quot;DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=@{linkedService().storageAccountName}&quot;,
            &quot;accountKey&quot;: {
              &quot;type&quot;: &quot;AzureKeyVaultSecret&quot;,
              &quot;store&quot;: {
                  &quot;referenceName&quot;: &quot;AzureKeyVaultWithStorageAccountsSecrets&quot;,
                  &quot;type&quot;: &quot;LinkedServiceReference&quot;
              },
              &quot;secretName&quot;: {
                &quot;type&quot;: &quot;Expression&quot;,
                &quot;value&quot;: &quot;@{linkedService().storageAccountName}&quot;
              }
            }
          
        },
        &quot;parameters&quot;: {
            &quot;storageAccountName&quot;: {
                &quot;type&quot;: &quot;String&quot;,
                &quot;defaultValue&quot;: &quot;fixmeaccountname&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>When I'm trying to test this I'm getting error</p>
<pre><code>Invalid storage connection string provided to 'UnknownLocation'. Check the storage connection string in configuration. Duplicate setting 'AccountName' found.
</code></pre>
<p>If I remove <code>AccountName=@{linkedService().storageAccountName}</code> from <code>connectionString</code> then I have this error</p>
<pre><code>Invalid storage connection string provided to 'UnknownLocation'. Check the storage connection string in configuration. Duplicate setting 'AccountKey' found.
</code></pre>
<p>Is it possible to have such linked service with parameterized key vault?</p>
","<azure><azure-data-factory><azure-keyvault><azure-blob-storage>","2020-08-27 06:26:39","411","0","1","63700223","<p>The secret on the KV is it connectionstring or just the key ? I think you are storing the connectionstring  as a secret , please try out with just the key and it should work .</p>
"
"63603605","Azure Data Factory v2 - OData - Copy Data - XmlError hexadecimal value 0x1F is an invalid character Line 1, position 1","<p>Here is a sample OData URL format used:</p>
<p><a href=""https://odata-my-company.net/api/v1/datalake/abcd1234321234ef9887492023/data_tablename/"" rel=""nofollow noreferrer"">https://odata-my-company.net/api/v1/datalake/abcd1234321234ef9887492023/data_tablename/</a></p>
<p>I have tried using Encoded URL as well substituting &quot;:&quot; as &quot;%3A&quot; and &quot;/&quot; as &quot;%2F&quot;</p>
<p>Also tried removing &quot;https://&quot; altogether.
Also tried using &quot;http://&quot; instead of &quot;https://&quot;</p>
<p>Nothing works.</p>
<p>Any help??? Thanks in advance</p>
<p>=== Error Message Below ===</p>
<p>Connection failed</p>
<p>Failed to create OData connection to RequestUrl
The metadata document could not be read from the message content.
XmlError : '', hexadecimal value 0x1F, is an invalid character. Line 1, position 1. : (1, 1)</p>
<p><a href=""https://i.stack.imgur.com/eh24B.png"" rel=""nofollow noreferrer"">Screenshot of same error in Azure Data Factory</a></p>
","<azure><azure-data-factory><azure-synapse>","2020-08-26 18:23:44","271","1","1","63606798","<p>Nothing wrong with the url . The error complains about the XML . I know that in ADF we have json all over the place . Just curious if the ODATA is returning XML and you have an issue there . To me it looks like you are looking at the wrong place .</p>
"
"63599875","Databricks or Functions with ADF?","<p>I'm using ADF to output some reports to pdf (at least that's the goal.)</p>
<p>I'm using ADF to output a csv to a storage blob and I would like to ingest that, do some formatting and stats work (with scipy and matplotlib in python) and export as a pdf to the same container. This would be run once a month, and I may do a few other things like this, but they are periodical reports at the most, no streaming or anything like that.</p>
<p>From an architectural stand point, would this be a good application for an Azure Function (which I have some experience), or Azure Databricks (which I will want some experience in).</p>
<p>My first thought is the Azure Functions, since they are serverless and pay-as-you-go. But I don't know too much about Databricks except that it's primarily used for big data and long running jobs.</p>
","<azure><azure-functions><azure-data-factory><azure-databricks>","2020-08-26 14:29:27","1503","1","1","63599911","<p>Databricks would be almost certainly an overkill for this. So yes, Azure Function for Python sounds like a perfect fit for your scenario.</p>
"
"63591378","Get datetime during deploy in YAML file","<p>I need to get DateTime during the deployment in a YAML file. The DateTime should be shown as</p>
<pre><code>&quot;startTime&quot;: &quot;2017-12-08T00:00:00&quot;
</code></pre>
<p>I found <a href=""https://stackoverflow.com/a/23529410/2688419"">this</a> help. but I need to follow the exact Datetime format. I wonder if anyone can help in this case?</p>
<p>-- Added --</p>
<p>I work on deploying Data Factory by YAML file. This StartTime will be DateTime for the trigger part of Data Factory pipeline</p>
<p>I have update my build pipeline with a variable, build.yml</p>
<pre><code>variables:
  deployDate: $(Get-Date -Format &quot;YYYYMMDDThhmmssZ&quot;)
</code></pre>
<p>and inside my deploy.yml file</p>
<pre><code> - task: AzureResourceGroupDeployment@2
      displayName: &quot;Deploy Azure Data Factory Content&quot;
          inputs:
            azureSubscription: ...
            action: ...
            resourceGroupName: ..
            location: ...
            templateLocation: ...
            csmFile: ...
            csmParametersFile: ...
            overrideParameters: &gt;-
              - ...
              -triggerStartTime &quot;$(deployDate)&quot;
            deploymentMode: 'Incremental'
</code></pre>
<p>and in adf.content.json, I added</p>
<pre><code>&quot;parameters&quot;: {
    &quot;triggerStartTime&quot;: {
    &quot;type&quot;: &quot;string&quot;
  }
}



&quot;name&quot;: &quot;[concat(parameters('factoryName'), '/Trigger')]&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/triggers&quot;,
    &quot;apiVersion&quot;: &quot;...&quot;,
    &quot;properties&quot;: {
      &quot;annotations&quot;: [],
      &quot;runtimeState&quot;: &quot;Started&quot;,
      &quot;pipeline&quot;: {
        &quot;pipelineReference&quot;: {
          &quot;referenceName&quot;: &quot;...&quot;,
          &quot;type&quot;: &quot;PipelineReference&quot;
        },
        &quot;parameters&quot;: {}
      },
      &quot;type&quot;: &quot;TumblingWindowTrigger&quot;,
      &quot;typeProperties&quot;: {
        &quot;frequency&quot;: &quot;Hour&quot;,
        &quot;interval&quot;: 1,
        &quot;startTime&quot;: &quot;[parameters('triggerStartTime')]&quot;,
        &quot;delay&quot;: &quot;00:00:00&quot;,
        &quot;maxConcurrency&quot;: 50,
        &quot;retryPolicy&quot;: {
          &quot;intervalInSeconds&quot;: 30
        },
        &quot;dependsOn&quot;: []
      }
    },
    &quot;dependsOn&quot;: [
      &quot;[concat(variables('factoryId'), '/pipelines/...')]&quot;
    ]
</code></pre>
","<deployment><azure-devops><yaml><azure-data-factory>","2020-08-26 06:03:24","9115","1","2","63593294","<p>There is an environment variable in the release stage named <code>RELEASE_DEPLOYMENT_STARTTIME</code> and we can use it in powershell via <code>$(Release.Deployment.StartTime)</code></p>
<p><a href=""https://i.stack.imgur.com/Rzpt6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rzpt6.png"" alt=""enter image description here"" /></a></p>
<p>In addition, we can custom the variable.</p>
<p><strong>Note</strong>: I use the date format as <code>yyyy-MM-dd HH:mm:ss</code> here, You can use other <a href=""https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/get-date?view=powershell-7"" rel=""nofollow noreferrer"">date formats</a></p>
<p>Define variables</p>
<pre><code>$date=$(Get-Date -Format &quot;yyyy-MM-dd HH:mm:ss&quot;);
Write-Host (&quot;##vso[task.setvariable variable=StartTime]$date&quot;)
</code></pre>
<p>Output the variable</p>
<pre><code>   Write-Host &quot;The value of StartTime is : $($env:StartTime)&quot;
</code></pre>
<p>Result:</p>
<p><a href=""https://i.stack.imgur.com/d90sB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d90sB.png"" alt=""enter image description here"" /></a></p>
<p><strong>Update1</strong></p>
<p>Please also check this <a href=""https://stackoverflow.com/questions/44738944/vsotfs-get-current-date-time-as-variable"">ticket</a></p>
"
"63591378","Get datetime during deploy in YAML file","<p>I need to get DateTime during the deployment in a YAML file. The DateTime should be shown as</p>
<pre><code>&quot;startTime&quot;: &quot;2017-12-08T00:00:00&quot;
</code></pre>
<p>I found <a href=""https://stackoverflow.com/a/23529410/2688419"">this</a> help. but I need to follow the exact Datetime format. I wonder if anyone can help in this case?</p>
<p>-- Added --</p>
<p>I work on deploying Data Factory by YAML file. This StartTime will be DateTime for the trigger part of Data Factory pipeline</p>
<p>I have update my build pipeline with a variable, build.yml</p>
<pre><code>variables:
  deployDate: $(Get-Date -Format &quot;YYYYMMDDThhmmssZ&quot;)
</code></pre>
<p>and inside my deploy.yml file</p>
<pre><code> - task: AzureResourceGroupDeployment@2
      displayName: &quot;Deploy Azure Data Factory Content&quot;
          inputs:
            azureSubscription: ...
            action: ...
            resourceGroupName: ..
            location: ...
            templateLocation: ...
            csmFile: ...
            csmParametersFile: ...
            overrideParameters: &gt;-
              - ...
              -triggerStartTime &quot;$(deployDate)&quot;
            deploymentMode: 'Incremental'
</code></pre>
<p>and in adf.content.json, I added</p>
<pre><code>&quot;parameters&quot;: {
    &quot;triggerStartTime&quot;: {
    &quot;type&quot;: &quot;string&quot;
  }
}



&quot;name&quot;: &quot;[concat(parameters('factoryName'), '/Trigger')]&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/triggers&quot;,
    &quot;apiVersion&quot;: &quot;...&quot;,
    &quot;properties&quot;: {
      &quot;annotations&quot;: [],
      &quot;runtimeState&quot;: &quot;Started&quot;,
      &quot;pipeline&quot;: {
        &quot;pipelineReference&quot;: {
          &quot;referenceName&quot;: &quot;...&quot;,
          &quot;type&quot;: &quot;PipelineReference&quot;
        },
        &quot;parameters&quot;: {}
      },
      &quot;type&quot;: &quot;TumblingWindowTrigger&quot;,
      &quot;typeProperties&quot;: {
        &quot;frequency&quot;: &quot;Hour&quot;,
        &quot;interval&quot;: 1,
        &quot;startTime&quot;: &quot;[parameters('triggerStartTime')]&quot;,
        &quot;delay&quot;: &quot;00:00:00&quot;,
        &quot;maxConcurrency&quot;: 50,
        &quot;retryPolicy&quot;: {
          &quot;intervalInSeconds&quot;: 30
        },
        &quot;dependsOn&quot;: []
      }
    },
    &quot;dependsOn&quot;: [
      &quot;[concat(variables('factoryId'), '/pipelines/...')]&quot;
    ]
</code></pre>
","<deployment><azure-devops><yaml><azure-data-factory>","2020-08-26 06:03:24","9115","1","2","63598563","<p>I managed to solve my problem. First of all, I removed all the changes I did in YAML files.
and then I updated the only adf.content.json</p>
<pre><code>&quot;parameters&quot;: {
   &quot;baseTime&quot;: {
       &quot;type&quot;: &quot;string&quot;,
       &quot;defaultValue&quot;: &quot;[utcNow('u')]&quot;,
       &quot;metadata&quot;: {
           &quot;description&quot;: &quot;Schedule will start one hour from this time.&quot;
     }
  }
}
</code></pre>
<p>and update the variable, I want to run 15min after the deploy</p>
<pre><code>  &quot;variables&quot;: {
    &quot;startTime&quot;: &quot;[dateTimeAdd(parameters('baseTime'), 'PT15M')]&quot;
  }
</code></pre>
"
"63586349","Data Factory: Multiple collection reference for XML copy to SQL","<p>I am attempting to set up a data factory pipeline which will ingest XML data to an Azure SQL Database. The XML follows this structure:</p>
<pre><code>&lt;schools&gt;
    &lt;school&gt;
        &lt;students&gt;
            &lt;student&gt;&lt;/student&gt;
            &lt;student&gt;&lt;/student&gt;
            &lt;student&gt;&lt;/student&gt;
        &lt;/students&gt;
    &lt;/school&gt;
    &lt;school&gt;
        &lt;students&gt;
            &lt;student&gt;&lt;/student&gt;
            &lt;student&gt;&lt;/student&gt;
            &lt;student&gt;&lt;/student&gt;
        &lt;/students&gt;
    &lt;/school&gt;
&lt;schools&gt;
</code></pre>
<p>I've set up multiple tables in SQL to accept this data. Put simply, there is a schools table which will take all the schools and a students table to accept all the students.</p>
<p>I set up the copy task in data factory and had to set the &quot;Collection reference&quot; to <code>&lt;school&gt;</code> in order to get it to iterate over the schools. If I don't do this it only loads in the first school and ignores the rest.</p>
<p>This works just fine for loading in schools. The problem is with the next copy task which looks at the same XML and tries to copy all the students from all the schools into the students table.</p>
<p>If I set the collection reference to <code>&lt;school&gt;</code> it will only copy in the first student from every school and ignores the rest of the students. If I set the collection reference to <code>&lt;student&gt;</code> it will copy all the students in the first school but ignore the rest of the schools and students.</p>
<p>I'd like to iterate over ALL schools AND students in order to load in all students from all schools but I don't see any easy way to do this. Is there some way to set multiple collection references to both schools and students?</p>
","<azure><azure-data-factory>","2020-08-25 20:06:40","935","1","1","63601202","<p>Where does the XML come from?  I would take more of an ELT approach and land the XML in a staging table, then use the built-in XML capabilities of Azure SQL Database such as the <code>nodes</code> and <code>value</code> method.  A simplified example:</p>
<p>Similar JSON pattern using Web Activity and Stored Proc activity in ADF:</p>
<p><a href=""https://i.stack.imgur.com/5r4Eu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5r4Eu.png"" alt=""enter image description here"" /></a></p>
<p>Sample SQL for importing XML:</p>
<pre><code>DROP TABLE IF EXISTS dbo.yourXMLStagingTable
DROP TABLE IF EXISTS dbo.student
DROP TABLE IF EXISTS dbo.school
DROP TABLE IF EXISTS #tmp
GO

CREATE TABLE dbo.yourXMLStagingTable (
    rowId       INT IDENTITY PRIMARY KEY,
    yourXML     XML NOT NULL,
    dateAdded   DATETIME NOT NULL DEFAULT GETDATE(),
    addedBy     VARCHAR(50) NOT NULL DEFAULT SUSER_NAME()
    )
GO


CREATE TABLE dbo.school (
    schoolId    INT IDENTITY PRIMARY KEY,
    schoolName  VARCHAR(50) UNIQUE NOT NULL,
    )
GO


CREATE TABLE dbo.student (
    studentId   INT IDENTITY(1000,1) PRIMARY KEY,
    schoolId    INT FOREIGN KEY REFERENCES dbo.school(schoolId),
    studentName VARCHAR(50) UNIQUE NOT NULL,
    )
GO


-- Use Data Factory to insert the data into a staging table
-- This is just to generate sample data
INSERT INTO dbo.yourXMLStagingTable ( yourXML )
SELECT '&lt;schools&gt;
    &lt;school schoolName = &quot;school1&quot;&gt;
        &lt;students&gt;
            &lt;student name = &quot;student11&quot;&gt;&lt;/student&gt;
            &lt;student name = &quot;student12&quot;&gt;&lt;/student&gt;
            &lt;student name = &quot;student13&quot;&gt;&lt;/student&gt;
        &lt;/students&gt;
    &lt;/school&gt;
    &lt;school schoolName = &quot;school2&quot;&gt;
        &lt;students&gt;
            &lt;student name = &quot;student21&quot;&gt;&lt;/student&gt;
            &lt;student name = &quot;student22&quot;&gt;&lt;/student&gt;
            &lt;student name = &quot;student23&quot;&gt;&lt;/student&gt;
        &lt;/students&gt;
    &lt;/school&gt;
&lt;/schools&gt;'
GO

-- Look at the dummy data
SELECT * FROM dbo.yourXMLStagingTable
GO

-- Dump into a staging table
-- Get the schools
SELECT 
    s.rowId,
    schools.c.value('@schoolName', 'VARCHAR(50)') AS schoolName,
    students.c.value('@name', 'VARCHAR(50)') AS studentName
INTO #tmp
FROM dbo.yourXMLStagingTable s
    CROSS APPLY s.yourXML.nodes('schools/school') schools(c)
        CROSS APPLY schools.c.nodes('students/student') students(c)


-- Look at the temp data
SELECT 'temp data' s, * FROM #tmp


-- Insert distinct schools data to schools table
INSERT INTO dbo.school ( schoolName )
SELECT DISTINCT schoolName
FROM #tmp


-- Insert distinct student data to student table, maintaining link to schools table
INSERT INTO dbo.student ( schoolId, studentName )
SELECT DISTINCT s.schoolId, t.studentName
FROM #tmp t
    INNER JOIN dbo.school s ON t.schoolName = s.schoolName
GO


-- End result
SELECT 'end result' s, *
FROM dbo.school school
    INNER JOIN dbo.student student ON school.schoolId = student.schoolId
</code></pre>
"
"63585645","ADF and OnPrem DevOps","<p>Does anyone know if we can use Azure Data Factory with On Prem DevOps instead of the Cloud DevOps? This is a government company and can't store the data in the Cloud, nor code.</p>
","<azure-data-factory>","2020-08-25 19:12:22","71","-1","1","63602065","<p>Just wanted to let you know that devops is there on azure goverment cloud also :
<a href=""https://devblogs.microsoft.com/azuregov/azure-devops-server-in-azure-government/"" rel=""nofollow noreferrer"">https://devblogs.microsoft.com/azuregov/azure-devops-server-in-azure-government/</a>
Thanks
Himanshu</p>
"
"63583945","How to dynamically pass ADLS gen2 folder/filename to Databricks Notebook in ADF or Databricks","<p>I am using a Databricks Notebook Activity in ADF to transform files in ADLS gen2 folder. This folder is dynamic and a new folder is created on daily basis with daynumber. So I want my Databricks Notebook Activity to pickup foldername dynamically for each day to process files in that folder.</p>
<p>Can we do this ADF or within Databricks Notebook Activity?</p>
","<azure><azure-data-factory><azure-databricks>","2020-08-25 17:13:08","835","0","1","63594895","<p>Have you tried to add dynamic content to the Notebook path?<br />
I've made a test according to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook#trigger-a-pipeline-run"" rel=""nofollow noreferrer"">tutorial</a> and it works well:</p>
<ol>
<li><p>First, I declare a parameter <code>name</code>.
<a href=""https://i.stack.imgur.com/q1D4b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q1D4b.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Switch to the Setting tab, I add the expression <code>@concat('/adftutorial/',string(dayOfMonth(utcnow())))</code> to the Notebook path text
<a href=""https://i.stack.imgur.com/f5hpN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f5hpN.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Then I run debug and enter <code>/path/filename</code> to the parameter
<a href=""https://i.stack.imgur.com/MSXD1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MSXD1.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>It will read files in the dynamic path I specified before.<br />
<a href=""https://i.stack.imgur.com/xVK5f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xVK5f.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>In my case, the file doesn't exist so it pops up an error.</p>
"
"63583347","Link Azure SQL Database to Data Factory using managed identity","<p>I have been trying to use Managed Identity to connect to Azure SQL Database from Azure Data factory.<br />
Steps are as follow:</p>
<ol>
<li>Created a Linked Service and selected Managed Identity as the Authentication Type</li>
<li>On SQL Server, added Managed Identity created for Azure Data Factory as Active Directory Admin</li>
</ol>
<p>The above steps let me do all data operations on the database. Actually that is the problem. I want to restrict the privileges given to Azure Data Factory on my SQL database.</p>
<p>First,  let me know whether I have followed the correct steps to set up the managed identity. Then, how to limit privileges because I don't want data factory to do any DDL on SQL database.</p>
","<azure><azure-sql-database><azure-data-factory><azure-managed-identity>","2020-08-25 16:35:07","4697","3","1","63589482","<p>As Raunak comments,you should change the role to db_datareader.</p>
<p>In you sql database,run this sql:</p>
<pre><code>CREATE USER [your Data Factory name] FROM EXTERNAL PROVIDER;
</code></pre>
<p>and this sql:</p>
<pre><code>ALTER ROLE db_datareader ADD MEMBER [your Data Factory name];
</code></pre>
<p>You can find '[your Data Factory name]' here
<a href=""https://i.stack.imgur.com/c8Xjc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/c8Xjc.png"" alt=""enter image description here"" /></a></p>
<p>Then you do any DDL operation in Data Factory,you will the error like this:</p>
<pre><code>&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed. Please search error to get more details.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=The INSERT permission was denied on the object
</code></pre>
<hr />
<p><strong>Update:</strong></p>
<p>1.Search for and select SQL server in azure portal
<a href=""https://i.stack.imgur.com/udlVH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/udlVH.png"" alt=""enter image description here"" /></a></p>
<p>2.select you and save as admin
<a href=""https://i.stack.imgur.com/t160D.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/t160D.png"" alt=""enter image description here"" /></a></p>
<p>3.click the button and run two sql in sql database.
<a href=""https://i.stack.imgur.com/2QHd9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2QHd9.png"" alt=""enter image description here"" /></a></p>
<p>More details,you can refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#managed-identity"" rel=""noreferrer"">documentation</a>.</p>
"
"63583281","how to stop the execution of a foreach loop in ADF when an activity fails","<p>I have a scenario where I need to fail the complete pipeline when an activity fails inside the ForEach loop container in ADF V2. Don't want the loop to continue further.</p>
<p>I am using ExecutePipeline to call the pipeline which contain ForEach loop.</p>
<p>Please advice me in doing this.</p>
<p>Thanks,
Nandini</p>
","<azure><azure-data-factory>","2020-08-25 16:31:17","4244","2","2","63593128","<p>In ADF, break loop in the ForEach activity is not supported now. You can add a If Condition activity in the ForEach activity to skip some steps in the loop.</p>
"
"63583281","how to stop the execution of a foreach loop in ADF when an activity fails","<p>I have a scenario where I need to fail the complete pipeline when an activity fails inside the ForEach loop container in ADF V2. Don't want the loop to continue further.</p>
<p>I am using ExecutePipeline to call the pipeline which contain ForEach loop.</p>
<p>Please advice me in doing this.</p>
<p>Thanks,
Nandini</p>
","<azure><azure-data-factory>","2020-08-25 16:31:17","4244","2","2","69456924","<p>For a situation where I wanted to break out of a loop and return to the parent flow when a Copy data Activity failed, I set the Copy data failure output path to execute two activities:</p>
<ul>
<li>Set a condition that would stop looping (in my case # records written less than expected).</li>
<li>Force a failure with an invalid 'Set variable' (Set a string value to integer or anything that would trigger a fail).</li>
</ul>
<p>When the Copy data activity fails, execution picks up in the parent flow on the Failure path of the loop.</p>
"
"63580474","Azure Data Factory - Copy files to a list of folders based on json config file","<p>I'm trying to use Azure Data Factory to read a json file and copy files based on the config it contains.</p>
<p>The json file:</p>
<pre><code>{
    &quot;FolderConfig&quot;: [
     {   
        &quot;Source&quot;: &quot;/pub/example&quot;,
        &quot;Destination&quot;: &quot;/FOL1&quot;
     },
     {
        &quot;Source&quot;: &quot;/pub/example&quot;,
        &quot;Destination&quot;: &quot;/FOL2&quot;
     }
    ]
}
</code></pre>
<p>The idea is to loop over the objects in FolderConfig and do a foreach on them.</p>
<p>The foreach would then copy files from source to destination</p>
<p>I tried using a Lookup activity with 'Source DataSet' set to the json file and this gives the following output:</p>
<pre><code>{
    &quot;count&quot;: 1,
    &quot;value&quot;: [
        {
            &quot;FolderConfig&quot;: [
                {
                    &quot;Source&quot;: &quot;/pub/example1&quot;,
                    &quot;Destination&quot;: &quot;/FOL1&quot;
                },
                {
                    &quot;Source&quot;: &quot;/pub/example2&quot;,
                    &quot;Destination&quot;: &quot;/FOL2&quot;
                }
            ]
        }
    ],
    &quot;effectiveIntegrationRuntime&quot;: &quot;integrationDebugRuntime1&quot;,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;SelfhostedIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 2
    }
}
</code></pre>
<p>I then use this output in the ForEach activity with 'Items' set to
'@activity('Lookup1').output.value[0].FolderConfig'</p>
<p>The ForEach iterates over the FolderConfig array and here I use a 'Copy Data' activity.</p>
<p>In the 'Copy Data' activity I am able to specify the source using dynamic content:
<a href=""https://i.stack.imgur.com/Y6jld.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y6jld.png"" alt=""enter image description here"" /></a></p>
<p>However for the destination sink I can't seem to use dynamic content from the ForEach item</p>
<p><a href=""https://i.stack.imgur.com/L8Gqa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L8Gqa.png"" alt=""enter image description here"" /></a></p>
<p>Does somebody have an idea on how to specify a variable destination?</p>
<p>Thanks for any help!</p>
","<azure><azure-data-factory>","2020-08-25 13:57:02","316","0","1","63591529","<p>Please try this:</p>
<p>First: 'items' in the ForEach activity should be like this:</p>
<pre><code>@activity('Lookup1').output.value[0].FolderConfig
</code></pre>
<p>Second:click 'open' button at sink(Azure blob storage)
<a href=""https://i.stack.imgur.com/yANlD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yANlD.png"" alt=""enter image description here"" /></a></p>
<p>Final:add this dynamic content</p>
<pre><code>@item().Destination
</code></pre>
<p><a href=""https://i.stack.imgur.com/vg5Hx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vg5Hx.png"" alt=""enter image description here"" /></a></p>
"
"63577687","How to add a date range in Azure Data Factory data flow","<h2>Working info</h2>
<p>I have two different sources of data set so I have created a dataflow in data factory in which for first data(A) set I am doing some transformation and loading into sink,in another data set(B) similarly am performing some transformation and loading into another sink.</p>
<h2>Issue</h2>
<p>Now I have some requirements in which there is date column DT_COLUMN_A(11-04-2020  01:17:40) in first data set(A)which needs to be compared with a date column DT_COLUMN_B(01-01-2020  16:32:00) in second data set (B) and store the compared output as a column in second dataset(B).</p>
<p>So I need the min and max(date range) of date column from dataset A ,apply it to min and max of date column to dataset B and find the dates which are matching in A and B and store it as YES if not matching NO.</p>
<h2>Code approach thought</h2>
<p>Logic needed:</p>
<p><code>if(min(DT_COLUMN_A) and max(DT_COLUMN_A) == min(DT_COLUMN_B) and max(DT_COLUMN_B) then YES else No.</code></p>
<p>I am trying to achieve this in ADF data flow but unable to do it.</p>
","<date><azure-functions><date-range><azure-data-factory>","2020-08-25 11:18:12","1275","0","2","63583234","<p>To get MIN and MAX of a dataset in ADF, you will need the Aggregate transformation. Create new columns called MinA, MinB, MaxA, MaxB from each of the relative streams in your data flow using Aggregate. Set the aggregate function to MIN and MAX appropriately for each. Then, you'll be able to set an iif() expression afterward, or use a Filter or Conditional Split transformation that uses those stored min &amp; max values.</p>
"
"63577687","How to add a date range in Azure Data Factory data flow","<h2>Working info</h2>
<p>I have two different sources of data set so I have created a dataflow in data factory in which for first data(A) set I am doing some transformation and loading into sink,in another data set(B) similarly am performing some transformation and loading into another sink.</p>
<h2>Issue</h2>
<p>Now I have some requirements in which there is date column DT_COLUMN_A(11-04-2020  01:17:40) in first data set(A)which needs to be compared with a date column DT_COLUMN_B(01-01-2020  16:32:00) in second data set (B) and store the compared output as a column in second dataset(B).</p>
<p>So I need the min and max(date range) of date column from dataset A ,apply it to min and max of date column to dataset B and find the dates which are matching in A and B and store it as YES if not matching NO.</p>
<h2>Code approach thought</h2>
<p>Logic needed:</p>
<p><code>if(min(DT_COLUMN_A) and max(DT_COLUMN_A) == min(DT_COLUMN_B) and max(DT_COLUMN_B) then YES else No.</code></p>
<p>I am trying to achieve this in ADF data flow but unable to do it.</p>
","<date><azure-functions><date-range><azure-data-factory>","2020-08-25 11:18:12","1275","0","2","73453379","<p>I managed to get something similar to work using using a mapLoop() expression to first build an array of dates in a derived column transformation followed by a flatten transformation</p>
<p><a href=""https://stackoverflow.com/a/73453351/12592985"">https://stackoverflow.com/a/73453351/12592985</a></p>
"
"63577294","execute Azure DataFactory pipeline from different datafactory","<p>I have 6 datafactories.
4 of them in the same region, 2 in a different region.
They are in different resourcegroups.</p>
<p>In one of them (let's call this one <code>adf-mgmt</code>) I created a pipeline (<code>sendmail</code>) that sends an email if an error occurs;
<a href=""https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification--part-1/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification--part-1/</a></p>
<p>Can I use the pipeline <code>sendmail</code> from the other datafactories or do I need to recreate that pipeline in each datafactory?</p>
","<azure><azure-data-factory>","2020-08-25 10:51:01","437","0","2","63608334","<p>Can I use the pipeline sendmail from the other datafactories or do I need to recreate that pipeline in each datafactory?</p>
<p>No, we can't. Data Factory doesn't support execute the pipeline from different pipeline.</p>
<p>Others have post the request in <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/36641200-execute-pipeline-task-in-a-different-datafactory"" rel=""nofollow noreferrer"">Data Factory feedback</a>, you could continue to vote up it to make the product team know:
<a href=""https://i.stack.imgur.com/GJrDC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GJrDC.png"" alt=""enter image description here"" /></a></p>
<p>And thanks for the good suggestion provided by @Joel Cochran:</p>
<p>&quot;you could use the REST API to execute external pipelines. A better option would be to place your sendmail action in an Azure Function that could be executed from virtually anywhere.&quot;</p>
<p>&quot;A Logic App could also work since both can be exposed as a REST endpoint. I would still encourage you to investigate Function Apps. LAs are workflows, where as FAs are encapsulated components. LAs and ADF both know how to leverage FAs, so they are very useful tools when building workflows.&quot;</p>
<p>You could follow the suggestion, or recreate that pipeline in each Data factory if you still could not figure it out with Azure Function or Login App.</p>
"
"63577294","execute Azure DataFactory pipeline from different datafactory","<p>I have 6 datafactories.
4 of them in the same region, 2 in a different region.
They are in different resourcegroups.</p>
<p>In one of them (let's call this one <code>adf-mgmt</code>) I created a pipeline (<code>sendmail</code>) that sends an email if an error occurs;
<a href=""https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification--part-1/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification--part-1/</a></p>
<p>Can I use the pipeline <code>sendmail</code> from the other datafactories or do I need to recreate that pipeline in each datafactory?</p>
","<azure><azure-data-factory>","2020-08-25 10:51:01","437","0","2","69294191","<p>It is possible via the Azure Data Factory REST API. &quot;Create Run&quot; would be your point of interest. Refer the below link.</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run</a></p>
"
"63574327","Similar product in AWS or GCP like Azure Data Factory?","<p>I am totaly new to the cloud in any way. I started some weeks ago with the Azure cloud and we setting up a project using many different products of Azure.
At the moment we think about setting up the project on a way that we are not trapped by Microsoft and are able to switch to GCP or AWS. For most products we use I have found similar ones in the other Clouds but I wonder if there is somthing like Azure Data Factory in AWS or CGP? I could not find something in my first google research.</p>
<p>Best and thanks for your help</p>
","<amazon-web-services><azure><google-cloud-platform><azure-data-factory>","2020-08-25 07:52:16","9056","4","3","63574496","<p>If you need a good comparison between different cloud (Azure, AWS, Google, Oracle, and Alibaba) use this site: <a href=""http://comparecloud.in/"" rel=""nofollow noreferrer"">http://comparecloud.in/</a></p>
<p>Example for your case with &quot;Azure Data Factory&quot;:</p>
<p><a href=""https://i.stack.imgur.com/eWIkb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eWIkb.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/yAq37.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yAq37.png"" alt=""enter image description here"" /></a></p>
"
"63574327","Similar product in AWS or GCP like Azure Data Factory?","<p>I am totaly new to the cloud in any way. I started some weeks ago with the Azure cloud and we setting up a project using many different products of Azure.
At the moment we think about setting up the project on a way that we are not trapped by Microsoft and are able to switch to GCP or AWS. For most products we use I have found similar ones in the other Clouds but I wonder if there is somthing like Azure Data Factory in AWS or CGP? I could not find something in my first google research.</p>
<p>Best and thanks for your help</p>
","<amazon-web-services><azure><google-cloud-platform><azure-data-factory>","2020-08-25 07:52:16","9056","4","3","63610385","<p>You could use a mix of those products:</p>
<ul>
<li>[Cloud Data Fusion]<a href=""https://cloud.google.com/composer"" rel=""nofollow noreferrer"">https://cloud.google.com/composer</a></li>
<li>Cloud DataPrep: This is a version of <a href=""https://trifacta.com"" rel=""nofollow noreferrer"">Trifacta</a>. Good for data cleaning.</li>
</ul>
<p>If you need to orchestrate workflows / etls, <a href=""https://cloud.google.com/composer"" rel=""nofollow noreferrer"">Cloud composer</a> will do it for you. It is a managed <a href=""https://airflow.apache.org/"" rel=""nofollow noreferrer"">Apache Airflow</a>. Which means it will handle complex dependencies.</p>
<p>If you just need to trigger a job on a daily basis, Cloud Scheduler is your friend.</p>
"
"63574327","Similar product in AWS or GCP like Azure Data Factory?","<p>I am totaly new to the cloud in any way. I started some weeks ago with the Azure cloud and we setting up a project using many different products of Azure.
At the moment we think about setting up the project on a way that we are not trapped by Microsoft and are able to switch to GCP or AWS. For most products we use I have found similar ones in the other Clouds but I wonder if there is somthing like Azure Data Factory in AWS or CGP? I could not find something in my first google research.</p>
<p>Best and thanks for your help</p>
","<amazon-web-services><azure><google-cloud-platform><azure-data-factory>","2020-08-25 07:52:16","9056","4","3","63941101","<p>You can check the link <a href=""http://osamaoracle.com/2020/06/19/services-mapping-aws-azure-gcp-oc-ibm-and-alibab-cloud/"" rel=""nofollow noreferrer"">here</a> which is cloud services mapping</p>
"
"63573350","Moving to Azure Synapse from Azure SQL Database","<p>We are currently ingesting data from application databases into an Azure SQL Database. The size is around 600 GB now (which mainly distributed to only 3 fact tables, the rest of the tables are master data which is quite small) and it's running on 40 vCores (we use it a lot for reporting, so need a high number of vCores).</p>
<p>Some difficulties I'm currently facing:</p>
<ol>
<li><p>Data copy from source to sink usually takes a really long time. The approach we are using is to <code>delete</code> all records for this month, and then <code>copy</code> this month's data from application db over. Writing to sink usually takes a lot of time as well (due to the indices on the fact table I believe).</p>
</li>
<li><p>High data I/O whenever someone pulls a big query.</p>
</li>
</ol>
<p>Here to hope someone can shed some lights on how to make the setup works faster.</p>
<p>Thanks!</p>
","<azure><azure-sql-database><azure-data-factory><azure-synapse>","2020-08-25 06:44:46","500","0","1","63601994","<p>Though i needed some more info , but i think i can share some pointers .
When you mentioned big query taking more time &quot; , have you checked the query and make sure that the indices are there on the required columns and they are updated regulary ? It appears that you have 3 tables with lot of data , is the query are slow on all three ? ( If I were you i will try the divide the problems into smaller issues  and investigate each one of them )</p>
<p>On the copy part , before you copy you will have to select the data and so we will have to improve the query perofiormance which I mentioned above . How are you copying data ? Since I see ADF tag , I am assuming its ADF . Are you copying data sequentially ? I mean copy data to BigTable1 then copy  BigTable2 then BigTable 3 ? You can explore the possiblilty of copying data in parallel . I am not sure how have you implmeneted the logic in ADF but three copy avtivity one below the other will do the trick .</p>
<p>In each copy activity you have the option to set parallelism and also the batchcount , I could suggest to take a look on that .</p>
<p>Performanace problem is very difficult to help with unless you have access to the data :) let me know how it goes .</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#parallel-copy"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#parallel-copy</a></p>
<p>Thanks</p>
<p>Himanshu</p>
"
"63571890","Changing a Date automatically every month in copy activity in Azure Data Factory to copy data from SAP HANA to Azure SQLDB","<p>I am working in <em><strong>azure data factory (ADF)</strong></em> to copy data from <strong><a href=""https://www.sap.com/products/hana.html"" rel=""nofollow noreferrer"">SAP HANA</a></strong> to <strong><a href=""https://azure.microsoft.com/pl-pl/services/sql-database/"" rel=""nofollow noreferrer"">Azure SQLDB</a></strong>. I am using &quot;copy activity&quot; in ADF to perform this operation using SQL query to query the table in HANA and sink it into SQLDB. The query contains <code>reporting_date</code> which I need to manually update every month to fetch the data.</p>
<p>I want to remove this redundant procedure of changing dates manually and then run it every month because i have 350+ pipelines to change dates every month and this task becomes very hectic and time consuming.
Is there any way to change the date at one place, it works as an input to the source query and query gets automatically updated with the date?
Please find attached screenshots for my requirement.</p>
<p><a href=""https://i.stack.imgur.com/M8QcV.png"" rel=""nofollow noreferrer"">ADF screenshot</a></p>
<p>Query to be updated:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT &quot;SAPINSTANCE&quot;, &quot;MCO&quot;, &quot;COUNTRY&quot;, &quot;BUKRS&quot;, &quot;EntityName&quot;, &quot;XBLNR&quot;, &quot;HKONT&quot;, &quot;TXT50&quot;, &quot;GSBER&quot;, &quot;BELNR&quot;, &quot;BLART&quot;, &quot;LTEXT&quot;, &quot;CC_BLDAT&quot;, &quot;CC_BUDAT&quot;, &quot;WAERS&quot;, &quot;MONAT&quot;, &quot;SGTXT&quot;, &quot;GJAHR&quot;, &quot;BKTXT&quot;, &quot;UMSKZ&quot;, &quot;S_LTEXT&quot;, &quot;AUFNR&quot;, &quot;PROJK&quot;, &quot;PRCTR&quot;, &quot;EBELN&quot;, &quot;KOART&quot;, &quot;AUGBL&quot;, &quot;CC_AUGDT&quot;, &quot;REBZG&quot;, &quot;LIFNR&quot;, &quot;NAME1&quot;, &quot;CC_SHKZG&quot;, &quot;KOSTL&quot;, &quot;BSCHL&quot;, &quot;BUZEI&quot;, &quot;EBELP&quot;, &quot;KTOKS&quot;, &quot;ZUONR&quot;, &quot;XINTB&quot;, &quot;XLOEB&quot;, &quot;XSPEB&quot;, &quot;XOPVW&quot;, &quot;XKRES&quot;, &quot;TYPE_WISE_CLASS&quot;, &quot;DESCRIPTION&quot;, &quot;MCLASS&quot;, &quot;CC_NUM_OF_DAYS_AGEING&quot;, &quot;CC_AGEING_BUCKET&quot;, &quot;CC_GROUP_CURRENCY&quot;, &quot;CC_REPORTING_DATE_VAR&quot;, &quot;MARKET_CLUSTER&quot;, &quot;HWAER&quot;, &quot;CURR_UNIT_TO_EURO_1&quot;, sum(&quot;CC_WRBTR&quot;) AS &quot;CC_WRBTR&quot;, sum(&quot;CC_GROUP_CURRENCY_VALUE&quot;) AS &quot;CC_GROUP_CURRENCY_VALUE&quot;, sum(&quot;CC_LOC_AMT_IN_MIN&quot;) AS &quot;CC_LOC_AMT_IN_MIN&quot;, sum(&quot;CC_AMT_IN_EUR_MIN&quot;) AS &quot;CC_AMT_IN_EUR_MIN&quot;, sum(&quot;CC_DMBTR_E&quot;) AS &quot;CC_DMBTR_E&quot; 
FROM &quot;_SYS_BIC&quot;.&quot;table&quot;('PLACEHOLDER' = ('$$IP_CLUSTER$$', 'Africa'), 'PLACEHOLDER' = ('$$IP_COUNTRY$$', '*'), 'PLACEHOLDER' = ('$$IP_FISCAL_YEAR$$', '*'), 'PLACEHOLDER' = '$$IP_REPORTING_DATE$$', '20200831'), 'PLACEHOLDER' = ('$$IP_AGEING_BUCKET$$', '''*'''), 'PLACEHOLDER' = ('$$IP_BUKRS$$', '*')) 
GROUP BY &quot;SAPINSTANCE&quot;, &quot;MCO&quot;, &quot;COUNTRY&quot;, &quot;BUKRS&quot;, &quot;EntityName&quot;, &quot;XBLNR&quot;, &quot;HKONT&quot;, &quot;TXT50&quot;, &quot;GSBER&quot;, &quot;BELNR&quot;, &quot;BLART&quot;, &quot;LTEXT&quot;, &quot;CC_BLDAT&quot;, &quot;CC_BUDAT&quot;, &quot;WAERS&quot;, &quot;MONAT&quot;, &quot;SGTXT&quot;, &quot;GJAHR&quot;, &quot;BKTXT&quot;, &quot;UMSKZ&quot;, &quot;S_LTEXT&quot;, &quot;AUFNR&quot;, &quot;PROJK&quot;, &quot;PRCTR&quot;, &quot;EBELN&quot;, &quot;KOART&quot;, &quot;AUGBL&quot;, &quot;CC_AUGDT&quot;, &quot;REBZG&quot;, &quot;LIFNR&quot;, &quot;NAME1&quot;, &quot;CC_SHKZG&quot;, &quot;KOSTL&quot;, &quot;BSCHL&quot;, &quot;BUZEI&quot;, &quot;EBELP&quot;, &quot;KTOKS&quot;, &quot;ZUONR&quot;, &quot;XINTB&quot;, &quot;XLOEB&quot;, &quot;XSPEB&quot;, &quot;XOPVW&quot;, &quot;XKRES&quot;, &quot;TYPE_WISE_CLASS&quot;, &quot;DESCRIPTION&quot;, &quot;MCLASS&quot;, &quot;CC_NUM_OF_DAYS_AGEING&quot;, &quot;CC_AGEING_BUCKET&quot;, &quot;CC_GROUP_CURRENCY&quot;, &quot;CC_REPORTING_DATE_VAR&quot;, &quot;MARKET_CLUSTER&quot;, &quot;HWAER&quot;, &quot;CURR_UNIT_TO_EURO_1&quot;
</code></pre>
<p>The above query acts as an input in the copy activity and needs to be updated with date every month manually. Please help!</p>
","<sql><date><sql-update><azure-data-factory>","2020-08-25 04:03:30","402","0","1","63575966","<p>I presume that you want to use &quot;current_date&quot; or similar dynamic value when triggering Pipeline execution using a scheduled event (for instance weekly, or monthly).</p>
<p>You can achieve that by dynamically concatinating your Query with current date, or perform more advanced date function operations if necessary:</p>
<pre><code>@concat('SELECT FROM schema.table t WHERE reporting_date =', formatDateTime(utcnow(), 'yyyy-MM-dd'))
</code></pre>
<p>You can find more detailed list of whats possible in the following url: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions</a></p>
"
"63566122","Azure Data Factory - Get Metadata inside for each activity","<p>folder structure: <br /><br />
raw</p>
<ul>
<li>test1
<ul>
<li>in.csv</li>
<li>out.csv</li>
</ul>
</li>
<li>test2
<ul>
<li>in.csv</li>
<li>out.csv</li>
</ul>
</li>
<li>test3
<ul>
<li>in.csv</li>
<li>out.csv <br /><br /></li>
</ul>
</li>
</ul>
<p>Here is what I want to do - use a get metadata activity to get a list of folders inside the raw folder. Then use a Foreach to go through the childitems of the get metadata activity and then inside the for each loop, use another get metadata activity that gets the metadata for every folder(all the test folders). This should work as new test folders are created (will have trigger to run pipeline), every folder will have the same structure and the same files inside but I need the get metadata to be able to work in the future for these folders that don't exist yet.<br />
The issue I'm facing is setting the dataset for the Get Metadata that is inside the for loop since I can't set the dataset to the multiple test folders, some of which dont exist yet. I don't want to have to update the datasets everytime as I want the pipeline to run automatically with a trigger for when a new test folder is created.
Thanks!</p>
","<foreach><azure-data-factory>","2020-08-24 17:44:30","1535","0","1","63571117","<p>Please try this:</p>
<p>The screenshot of my pipeline:
<a href=""https://i.stack.imgur.com/EqT2a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EqT2a.png"" alt=""enter image description here"" /></a></p>
<p>The dataset of inside Get Metadata Activity:
<a href=""https://i.stack.imgur.com/M5wPv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M5wPv.png"" alt=""enter image description here"" /></a></p>
<p>If you aren't sure folder(test1,test2,test3) or csv file(in.csv,out.csv) exists,
you can select 'Exists' in Get Metadata Activity like this:</p>
<p><a href=""https://i.stack.imgur.com/PihEq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PihEq.png"" alt=""enter image description here"" /></a></p>
<p>Then you can use this value in output to confirm whether it exists,so you can do something else without error.</p>
<p><a href=""https://i.stack.imgur.com/LFdWE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LFdWE.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you.</p>
"
"63564693","Azure Data Factory Copy Activity - Append to JSON File","<p>I am working on creating a data factory pipeline that copies data from a REST API endpoint to Azure Blob Storage. The API has a limitation of only returning 1000 records at a time, so I have built in a loop into my pipeline that will iterate through all of the pages. What I am wondering is - would it be possible to use the copy activity to append to the same file in the Azure Blob, rather than create a separate file for each page?</p>
<p>Below is what the API response looks like. The only value that I need from each response is the &quot;records&quot; list, so I was thinking if it is possible, I could get rid of the other stuff and just keep appending to the same file as the loop runs - although I do not know if the copy activity is capable of doing this. Would this be possible? Or is the only way to do this is to land all the responses as separate files in Blob Storage and then combine them after the fact?</p>
<p>Thank You</p>
<pre><code>{
    &quot;totalResults&quot;: 8483,
    &quot;pageResults&quot;: 3,
    &quot;timeStamp&quot;: &quot;2020/08/24 10:43:26&quot;,
    &quot;parameters&quot;: {
        &quot;page&quot;: 1,
        &quot;resultsPerPage&quot;: 3,
        &quot;filters&quot;: [],
        &quot;fields&quot;: [
            &quot;lastName&quot;,
            &quot;firstName&quot;,
            &quot;checklistItemsAssigned&quot;,
            &quot;checklistItemsStarted&quot;,
            &quot;checklistItemsCompleted&quot;,
            &quot;checklistItemsOverdue&quot;
        ],
        &quot;sort&quot;: {
            &quot;field&quot;: &quot;lastName&quot;,
            &quot;direction&quot;: &quot;asc&quot;
        }
    },
    &quot;records&quot;: [
        {
            &quot;checklistItemsAssigned&quot;: 10,
            &quot;lastName&quot;: &quot;One&quot;,
            &quot;firstName&quot;: &quot;Person&quot;,
            &quot;checklistItemsOverdue&quot;: 0,
            &quot;checklistItemsStarted&quot;: 10,
            &quot;checklistItemsCompleted&quot;: 10
        },
        {
            &quot;checklistItemsAssigned&quot;: 5,
            &quot;lastName&quot;: &quot;Two&quot;,
            &quot;firstName&quot;: &quot;Person&quot;,
            &quot;checklistItemsOverdue&quot;: 0,
            &quot;checklistItemsStarted&quot;: 5,
            &quot;checklistItemsCompleted&quot;: 5
        },
        {
            &quot;checklistItemsAssigned&quot;: 5,
            &quot;lastName&quot;: &quot;Three&quot;,
            &quot;firstName&quot;: &quot;Person&quot;,
            &quot;checklistItemsOverdue&quot;: 0,
            &quot;checklistItemsStarted&quot;: 5,
            &quot;checklistItemsCompleted&quot;: 5
        }
    ]
}
</code></pre>
","<azure-blob-storage><azure-data-factory>","2020-08-24 16:07:05","1565","1","1","63566216","<p>ADF's Copy activity supports copying blobs from block, append, or page type of blobs but copying data to <strong>only</strong> block blobs. Blobk blobs can only be overwritten.
You can probably create an append type of blob using Storage SDK, but it would be an overkill for most of the project. I would go with creating new blobs and merging them at the last stage.</p>
"
"63551489","Azure Data Factory - Update environment variable in continues integration pipeline","<p>I have recently started with ARM and Azure Data Factory, and now I have faced a problem when I try to deploy ADF into other environments. Because the source connection string are different in each environment and I can not keep the static value in adf.content.parameters.json file.
So I have created three YML files for each environment as Dev.yml, Test.yml, and Prod.yml. I have three files</p>
<ol>
<li>adf.content.json</li>
<li>adf.content.parameters.json</li>
<li>Dev.yml, test.yml and prod.yml</li>
</ol>
<p>In the file, adf.content.json, I have a connection string as my source. This value is changing in each environment. Here is the adf.content.parameters.json</p>
<pre><code>&quot;parameters&quot;: {
&quot;source_connectionString&quot;: {
  &quot;type&quot;: &quot;secureString&quot;,
  &quot;metadata&quot;: &quot;Secure string for 'connectionString' of 'source-db'&quot;
},
</code></pre>
<p>and I removed this parameter from adf.content.parameters.json but instead, I have added that to Dev.yml file which looks like this one (Test.yml and prod.yml are the same just different values)</p>
<pre><code>variables:
 - name:  source-connectionstring
   value: &lt;some value&gt;
</code></pre>
<p>I have a ci-build.yml and ci-deploy.yml files which will be used for the CI pipeline. In the ci-build.yml file, I use the same name in the adf.content.json</p>
<pre><code>variables:
 SourceConnectionString: $(source_connectionString:?You need to set the source_connectionString environment variable)


- stage: DevDeploy
  displayName: Deploy to development (D)
  variables:
   - template: ../.../.../dev.yml
   - group: ...
  dependsOn: Build
 jobs: 
   - template: cd_deploy.yml
    parameters:
      environment: dev
      azureServiceConnection: '...'
      containerRegistryDomain: ...
      apiResourceGroup: '...' 
      webAppName: '...
      azureSubscriptionName: ...
      apimResourceGroup: ...
      apimName:  ...
</code></pre>
<p>And in my ci-deply.yml file looks like this</p>
<pre><code>task: AzureResourceGroupDeployment@2
          displayName: &quot;Deploy Azure Data Factory Content&quot;
          inputs:
            azureSubscription: '...'
            action: '...'
            resourceGroupName: '...'
            location: '...'
            templateLocation: '...'
            csmFile: '$(System.ArtifactsDirectory)/.../arm/adf.content.json'
            csmParametersFile: '$(System.ArtifactsDirectory)/../arm/adf.content.parameters.json'
            overrideParameters: ' -source_connectionString$(SourceConnectionString) 
            deploymentMode: 'Incremental' 
</code></pre>
<p>But I get an error &quot; did not find expected key&quot; on <code>overrideParameters: ' -source_connectionString$(SourceConnectionString)</code></p>
<p>I do not know if this is the right approach for this? and if anyone can get why I can not get the key?</p>
","<azure-devops><azure-data-factory><azure-resource-manager>","2020-08-23 20:04:47","1443","0","1","63557474","<p>This error is usually caused by wrong syntax.</p>
<p>In <code>ci-deply.yml</code>, the script <code>overrideParameters: ' -source_connectionString$(SourceConnectionString)</code> loses a <code>'</code>.</p>
<p>So your task in <code>ci-deply.yml</code> should look like this:</p>
<pre><code>- task: AzureResourceGroupDeployment@2
  displayName: &quot;Deploy Azure Data Factory Content&quot;
  inputs:
    azureSubscription: '...'
    action: '...'
    resourceGroupName: '...'
    location: '...'
    templateLocation: '...'
    csmFile: '$(System.ArtifactsDirectory)/.../arm/adf.content.json'
    csmParametersFile: '$(System.ArtifactsDirectory)/../arm/adf.content.parameters.json'
    overrideParameters: ' -source_connectionString$(SourceConnectionString) '
    deploymentMode: 'Incremental'
</code></pre>
"
"63547018","Azure DataFactory in GCP","<p>I have a scenario where I need to synchronize multiple &quot;on premise&quot; Microsoft SQL server databases from business datacenters to cloud storage (let's call it blob storage).</p>
<p>In the past, I've used the Azure Data Factory on-premise client to bypass firewall considerations, not require a VPN, and delivery data directly to Azure Blob storage.</p>
<p>I need to do the same thing using Google tools (destination Google Cloud Storage). Is there an equivalent GCP tool that does not require a VPN? If not, any lowish-priced tool recommendations?</p>
","<azure><google-cloud-platform><azure-data-factory>","2020-08-23 12:35:30","1102","0","1","63550594","<p>To send file from on prem to Google Cloud Storage (GCS) your need 2 things</p>
<ul>
<li><a href=""https://cloud.google.com/sdk/install"" rel=""nofollow noreferrer"">gcloud SDK</a> installed on your local environment</li>
<li>a <a href=""https://cloud.google.com/iam/docs/creating-managing-service-account-keys"" rel=""nofollow noreferrer"">service account key file</a>. <em>Keep it secrets!!</em></li>
</ul>
<p>Then, follow these steps</p>
<ul>
<li>Authenticate your <a href=""https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"" rel=""nofollow noreferrer"">service account in gcloud sdk</a> <code>gcloud auth activate-service-account &lt;Service Account email&gt; --key-file=&lt;Service Account File&gt;</code></li>
<li>Extract your data locally</li>
<li>Use <a href=""https://cloud.google.com/storage/docs/gsutil"" rel=""nofollow noreferrer"">gsutil</a> to send the file to the cloud</li>
</ul>
<p>The connexion is authenticated and don't required VPN.</p>
"
"63535733","Altering CSV Rows in Azure Data Factory","<p>I've tried to use the 'Alter Rows' function within a Data Flow in Azure Data Factory to remove rows that match a condition from a CSV dataset.</p>
<p>The Data Preview shows that the rows matched will be deleted, however in the next step 'sink' it seems to ignore that and writes the original rows to the CSV file output.</p>
<p>Is it not possible to use alter rows on a CSV dataset and if not, is there a work around?</p>
","<azure><etl><azure-data-factory>","2020-08-22 11:38:51","717","0","1","63570525","<p>Firstly,use 'union' to migrate your csv files as source.</p>
<p>Then,use 'filter' to filter your data with date time stamps at source.</p>
"
"63533272","On-premises FTP server folders migration to Azure datalake","<p>I am trying to migrate on-premises zipped folders located at an FTP server to Azure datalake V2. These folders are created in the FTP server on a daily basis. Requirement is to incrementally migrate these folders onto Azure and unzip the folders (containing logs from different subsystems of a huge [around 4 times of a tennis court] medical device) while migrating. I used Data Factory to create a pipeline and a trigger to run the pipeline at a fixed time of the day, and delete the folder at FTP after migration. However, I got to know that the folder at FTP cannot be deleted because this feature is not supported by Data factory due to presence of zip file at source FTP. I am having the requirement to delete the Folders at FTP after migration because trigger will enable the pipeline to even migrate the historical folders which are already been migrated, that can take a long time for migration of these big files.</p>
","<azure><data-migration><azure-data-factory>","2020-08-22 06:34:15","124","0","1","63559572","<p><strong>1,</strong> First of all, you need to know the unzip files &amp; move them to azure datalake, move common files should be in two activity. It is impossible to process both compressed and uncompressed files in a single activity.</p>
<p>The settings to move ziped files:</p>
<p><a href=""https://i.stack.imgur.com/HeQhL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HeQhL.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/vUZQr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vUZQr.png"" alt=""enter image description here"" /></a></p>
<p><strong>2,</strong> Azure Datafactory does not have a built-in function to process file time from ftp server. Therefore, you need to reflect the creation time of the file through the file name, and then filter based on the file name before the file is moved.</p>
<p>Let me know whether this can answer your question.</p>
"
"63531799","Azure Data Factory - querying Cosmos DB using dynamic timestamps","<p>I want to create and maintain a snapshot of a collection in Cosmos DB.</p>
<p>Periodically, I want to retrieve only the delta (new or modified documents) from Cosmos and write them to the snapshot, which will be stored in a Azure Data Explorer cluster.</p>
<p>I wish to get the delta using the _ts member of the documents. In other words, I will fetch only records for which the _ts is between some range.</p>
<p>The range will be the range of a time window, which I get using a tumbling window trigger in the data factory.</p>
<p>The issue is that if I print the dynamic timestamps which I create in the query, and hard code them into the query, it works. But if I let the query generate them, I don't get any results.</p>
<p>For example:</p>
<p>I'm using those value to simulate the window range of the trigger.</p>
<p><a href=""https://i.stack.imgur.com/ykAc2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ykAc2.png"" alt=""enter image description here"" /></a></p>
<p>I use this query to create timestamps in unix time.</p>
<p><a href=""https://i.stack.imgur.com/Mq719.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mq719.png"" alt=""enter image description here"" /></a></p>
<p>and I see that the timestamps created are correct.</p>
<p><a href=""https://i.stack.imgur.com/G4HQ0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G4HQ0.png"" alt=""enter image description here"" /></a></p>
<p>And if I run my query using those hardcoded timestamps, I get results</p>
<p><a href=""https://i.stack.imgur.com/ZRrE0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZRrE0.png"" alt=""enter image description here"" /></a></p>
<p>But, if I run a query using the code that just create those timestamps, I get no results from the query</p>
<p><a href=""https://i.stack.imgur.com/8DS8i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8DS8i.png"" alt=""enter image description here"" /></a></p>
<p>This is the code to create the timestamps:</p>
<pre><code>select 
DateTimeToTimestamp('@{formatDateTime('2020-05-20T12:00:00.0000000Z','yyyy-MM
ddTHH:mm:ss.fffffffZ')}')/1000,
DateTimeToTimestamp('@{formatDateTime('2020-08-20T12:00:00.0000000Z','yyyy-MM
ddTHH:mm:ss.fffffffZ')}')/1000
</code></pre>
<p>Does anyone have a clue as to what might be the issue?</p>
<p>Any other way to achieve this is also welcome.</p>
<p>Thanks</p>
<p>EDIT: I managed to work around this by taking the other, simpler option:</p>
<pre><code>where TimestampToDateTime(c._ts*1000)&gt; &quot;@{formatDateTime(pipeline().parameters.windowStart,'yyyy-MM-ddTHH:mm:ss.fffffffZ')}&quot;
</code></pre>
","<azure-data-factory>","2020-08-22 01:37:37","587","0","1","63589611","<p>We are glad that you resolved this problem:</p>
<p>You managed to work around this by taking the other, simpler option:</p>
<pre><code>where TimestampToDateTime(c._ts*1000)&gt; &quot;@{formatDateTime(pipeline().parameters.windowStart,'yyyy-MM-ddTHH:mm:ss.fffffffZ')}&quot;
</code></pre>
<p>I think the error in first option is most caused by the different data type between <code>c.ts</code> and <code>DateTimeToTimestamp('@{formatDateTime('2020-05-20T12:00:00.0000000Z','yyyy-MM ddTHH:mm:ss.fffffffZ')}')/1000</code>.</p>
"
"63526538","Azure Data Flows","<p>I have a requirement to regularly update an existing set of 30+ CSV files with new data (append to the end). There is also a requirement to possibly remove the first X rows as Y rows are added to the end.</p>
<p>Am I using the correct services for this and in the correct manner?</p>
<ul>
<li><p>Azure Blob Storage to store the Existing and Update files.</p>
</li>
<li><p>Azure DataFactory with DataFlows. A PipeLine and DataFlow per CSV I want to transform that conducts a merge of datasets (existing + update), producing a
sink fileset that drops the new combined CSV back into Blob
storage.</p>
</li>
<li><p>A trigger on the Blob Storage Updates directory to trigger the pipeline when a new update file is uploaded.</p>
</li>
</ul>
<p>Questions:</p>
<ul>
<li>Is this the best approach for this problem, I need a solution with minimal input from users (I'll take care of Azure ops so long as all they have to do is upload a file and download the new one)</li>
<li>Do I need a pipeline and dataflow per CSV file? Or could I have one per transformation type (ie one for just appending, another for appending and removing first X rows)</li>
<li>I was going to create a directory in blob storage for each of the CSVs (30+ Dirs) and create a dataset for each directories existing and update files.</li>
<li>Then create a dataset for each output file into some new/ directory</li>
</ul>
","<azure><etl><azure-data-factory>","2020-08-21 16:21:06","66","0","1","63528481","<p>Depending on the size of your CSVs, you can either perform the append right inside of the data flow by taking both the new data as well as the existing CSV file as a source and then Union the 2 files together to make a new file.</p>
<p>Or, with larger files, use the Copy Activity &quot;merge files&quot; setting to merge the 2 files together.</p>
"
"63520965","Data Factory Help : CSV files in sharepoint","<p>We are currently using PowerBI to connect to CSV files on sharepoint using 'Source = SharePoint.Files('</p>
<p>we now need to bring these files into the datawarehouse but i can't find a file sharepoint connector only a sharepoint list connect.</p>
<p>Is there a way to grab files from sharepoint using data factory, i just need to load them into a Azure SQL database?</p>
<p>thanks</p>
","<sharepoint><azure-data-factory>","2020-08-21 10:18:16","159","0","1","63608489","<p>I have not found an easy way for this. The authentification towards Sharepoint is challenging and then the download functionality is als challenging. As Andrii wrote about the use of Logic Apps, we are considering a similar approach as an alternative. Due to lack of time we have not investigated this direction, yet.</p>
"
"63518117","How to test Azure Data factory linked service using API","<p>I am trying to create and use Azure data factory by Rest API but while creation of linked service connection has created successfully but when I checked connection it got failed so is there anything to do test connection by API or PowerShell command.</p>
","<azure><powershell><azure-data-factory><azure-management-api>","2020-08-21 07:10:17","1044","1","2","63520307","<p>There is no this method in Microsoft documentation.You can track this feature <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/38267635-test-connection-programmatically"" rel=""nofollow noreferrer"">here</a>.</p>
<p>But there is a <a href=""https://datathirst.net/blog/2018/9/23/adfv2-testing-linked-services"" rel=""nofollow noreferrer"">blog</a> about testing link service by PowerShell.Here is the <a href=""https://gist.github.com/simondmorias/e4d9e7265a20af8d7d17b92d483e3889"" rel=""nofollow noreferrer"">script</a> on github.</p>
<p>Hope this can help you.</p>
"
"63518117","How to test Azure Data factory linked service using API","<p>I am trying to create and use Azure data factory by Rest API but while creation of linked service connection has created successfully but when I checked connection it got failed so is there anything to do test connection by API or PowerShell command.</p>
","<azure><powershell><azure-data-factory><azure-management-api>","2020-08-21 07:10:17","1044","1","2","74468754","<p>Now, you can use one of cmdlet in <code>azure.datafactory.tools</code> PowerShell module:<br />
<a href=""https://github.com/SQLPlayer/azure.datafactory.tools#test-connection-of-linked-service-preview"" rel=""nofollow noreferrer"">Test connection of Linked Service (preview)</a></p>
<pre><code># Example 1
$LinkedServiceName = 'AzureSqlDatabase1'      
Test-AdfLinkedService @params -LinkedServiceName $LinkedServiceName
</code></pre>
<p>Alternatively, if you prefer, you can run such test as part of your CI/CD process in Azure DevOps installing #adftools extension, which uses the same PS module behind the scenes.<br />
More: <a href=""https://sqlplayer.net/adftools"" rel=""nofollow noreferrer"">https://sqlplayer.net/adftools</a></p>
<p><em>Disclaimer: I'm author of the tool.</em></p>
"
"63516749","Connecting to a Local SQL Server Instance with instance name from Azure Data Factory","<p>I am trying to access a local SQL Server from Data Factory.  I have installed the Integration Runtime on a local box, which also has a SQL Instance, but it's DEV, so not really a problem.</p>
<p>So VMs involved are:</p>
<p><strong>A</strong> - Local SQL Server and Integration Runtime installed (localhost)</p>
<p><strong>B</strong> - Local SQL Server without Instance name (ServerName.somesite.com)</p>
<p><strong>C</strong> - Local SQL Server with Instance name (ServerName\InstanceName.somesite.com)</p>
<p>I have successfully tested to make sure connections work to both the local box <strong>A</strong> and local box <strong>B</strong>, no problems.  However when trying to access the SQL Server with the instance name from Data Factory, I got the error: &quot;provider: SQL Network Interfaces, error: 26 - Error Locating Server/Instance Specified&quot;</p>
<p>I have checked the UDP Port 1434 issue, the port was opened on the local machine. Can Data Factory not connect to local servers with an instance name? If so, is there a workaround? (I am thinking an ODBC DSN on the local IR host or similar).</p>
<p>Even when trying to test this from the IR configuration manager, I get this:</p>
<p><a href=""https://i.stack.imgur.com/kxQjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kxQjA.png"" alt=""IRError"" /></a></p>
","<sql-server><azure-data-factory>","2020-08-21 04:58:06","770","0","1","63553476","<p>Thanks@Nick.McDermaid, as he said:</p>
<ol>
<li>To access a local SQL Server from Data Factory the syntanx is like this <code>host.something.something\instance</code> .</li>
<li>When the database instance is bound to another port number, we can use the syntax like this
<code>host.something.something\instance,port number</code></li>
</ol>
"
"63516550","Migration from Oracle to CDS using ADF","<p>I am trying to migrate data from Oracle to an entity in Common Data Service( CDS) through Azure Data Factory Copy Activity. As CDS comes with GUID as a primary key and Oracle doesnt have primary key, my pipeline always fails.</p>
<p>I tried to create an additional column in source data set with value as @guid() however it throws that column  must be of type guid</p>
<p>also tried
select REGEXP_REPLACE(SYS_GUID(), '(.{8})(.{4})(.{4})(.{4})(.{12})', '\1-\2-\3-\4-\5') MSSQL_GUID,c. * from table_name c;
guid is coming as string in the mapping</p>
<p>How do we automatically generate guid in this scenario</p>
","<azure-data-factory><cds>","2020-08-21 04:29:19","158","0","1","63641737","<p>Could you please try updating your additional column (@guid()) data type from &quot;type&quot;: &quot;String&quot; to &quot;type&quot;: &quot;Guid&quot; by editing the JSON payload of your pipeline (look for {} symbol at top right corner of your pipeline). See below GIF:</p>
<p><a href=""https://i.stack.imgur.com/MXLRr.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MXLRr.gif"" alt=""enter image description here"" /></a></p>
<p><strong>Update</strong>:</p>
<p>After further analysis by collaborating with product team, it (type coversion) is identified as an unsupported feature with dynamics sink, where UX disables type conversion for dynamics sink. UX hasn't supported it since the release of type conversion feature.</p>
<p>Product team has opened a work item as a feature improvement for type conversion with Dynamics sink . The ETA for this feature support is mid of September (<em>Note: This is tentative date</em>), but product team is actively working on it. I will closely monitor the work item and will update this post as soon as I have additional information.</p>
<p>As a workaround, please try to split pipeline into 2 copies (copy activities). Oracle -&gt; csv &amp; csv -&gt; dynamics. In first copy, add an additional column to write empty guid column in csv file. In second copy, change the type of guid column in csv to Guid and do the copy.</p>
<p>Please let us know how it goes.</p>
"
"63510913","Cannot find Azure Data Factory","<p>I was executing this command:</p>
<pre><code>$obj = Get-AzDataFactory -ResourceGroupName $rgName -Name $adfName
</code></pre>
<p>I get this error but the ADF exists.</p>
<pre><code>HTTP Status Code: NotFound Error Code: ResourceNotFound 
Error Message: The Resource 'Microsoft.DataFactory/dataFactories/*******' 
under resource group 'DEV*****RG' was not found.
</code></pre>
<p>When this command runs, the ADF is listed!</p>
<pre><code>Get-AzResource -ResourceGroupName &quot;DEV*****RG&quot;
</code></pre>
","<azure><azure-data-factory>","2020-08-20 18:07:50","294","2","1","63511087","<p>'Get-AzDataFactory<strong>V2</strong>' is the correct command</p>
"
"63504993","Azure Data Factory - How to drop a column in Mapping Data Flow","<p>I am using Excel as data source. These are my columns:</p>
<p><a href=""https://i.stack.imgur.com/OJTH5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OJTH5.png"" alt=""enter image description here"" /></a></p>
<p>I am dropping the Attribute name in my Select transformer.</p>
<p><a href=""https://i.stack.imgur.com/mLncy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mLncy.png"" alt=""enter image description here"" /></a></p>
<p>But the values are not dropped. Only the column names are dropped.</p>
<p><a href=""https://i.stack.imgur.com/mpzk9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mpzk9.png"" alt=""enter image description here"" /></a></p>
<p>Can you please assist how can I drop the Column Attribute Name.</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2020-08-20 12:21:30","2725","2","1","63515191","<p>This is one known issue in dataflow preview data with data misalignment, we had fixed it and wait for current deployment to take effective. It only impact data preview, so you can feel free to ignore the wrong data in preview and have dataflow pipeline debug/trigger run directly which can work well.</p>
"
"63504106","How can I add a key attribute column before storing it into Azure SQL DB?","<p>I am using Data Factory to shift data from a Data Lake Storage Gen2, transform the data and finally I want to store the data in an Azure SQL DB.</p>
<p>How can I add a column with an key attribute (bigint) based on the rowNumber()? I tried to use the functionality &quot;Derived Column&quot;, but I can't choose the function rowNumber() as it is only valid for window transformations.</p>
<p>Thanks</p>
","<azure-sql-database><azure-data-factory>","2020-08-20 11:31:10","36","0","1","63515618","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-surrogate-key"" rel=""nofollow noreferrer"">Surrogate key transformation in mapping data flow</a>.</p>
<p>Create the key based the row number of the data:</p>
<p><a href=""https://i.stack.imgur.com/mx2CF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mx2CF.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/j2USZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j2USZ.png"" alt=""enter image description here"" /></a></p>
"
"63502789","How to connect Web app to Azure Data Lake","<p>I am trying to understand the Azure stack, still learning a lot. But i can't understand the logic behind the Azure Data Lake (gen2 storage account). I want to save forms from a website (created in PHP) as a json file in Data Lake.</p>
<p>I got the JSON file ready to upload but how can i upload this to Azure Data Lake? Or would i have to take a other route, Data Factory for example? I am trying to understand how someone would connect a external web application (created in PHP) to a Data Lake storage.</p>
<p>Also interested if it would be better to gather al the form data of one day and then push it to Data Lake instead of pushing each form separately.</p>
<p>I want to accomplish this:</p>
<pre><code>Webpage --&gt; Form posted --&gt; Data Lake (Form data as Json file)
                        --&gt; Local MySQL database (working already)
</code></pre>
","<azure><azure-data-factory><azure-data-lake>","2020-08-20 10:06:36","1246","-1","1","63504397","<p>If there is and Data Lake Store SDK for your language available, you can simply use that to write files directly from your code. See <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-directory-file-acl-dotnet#upload-a-file-to-a-directory"" rel=""nofollow noreferrer"">here</a> for an example for C#.</p>
<p>Since there is currently no SDK for PHP, the alternative is to use the <a href=""https://github.com/azure/azure-storage-php"" rel=""nofollow noreferrer"">Blob Storage SDK for PHP</a>. Since it is still the same storage, you can upload the file using the blob interface and still access it using the Data Lake interface.</p>
"
"63498759","Azure Data Factory - No Output in Select Transformation - Mapping Data Flow","<p>I am reading an excel file and applying some transformations.
I am not able to find any data from Select transformer in Data Preview.</p>
<p><a href=""https://i.stack.imgur.com/ffpRu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ffpRu.png"" alt=""enter image description here"" /></a></p>
<p>But the previous &quot;FromNumericLen&quot; transformer outputs data and can find the same in Data Preview.</p>
<p>Thanks.</p>
","<azure><azure-data-factory>","2020-08-20 05:22:41","212","0","1","63510444","<p>Per the comment thread, the fix is to enable Allow Schema Drift in the Source transform.</p>
"
"63494121","Azure DataFactory Params - Newbie Question","<p>I'm working with ADF and trying to leverage parameters to make life easier and reduce the number of objects being created in the ADF itself. What I am trying to do, would appear on the surface to be extremely simple, bu in reality its driving me slowly crazy. Would greatly appreciate any assistance!</p>
<p>I am trying to set up a parameterised dataset to be used as a sink target. Inside that dataset I have added a param named &quot;filenames&quot; of type string. In the connection tab I have added that param to the file part of the path. The folder part point to my Azure Data Lake folder and the file part is set to: @dataset().filename which is the result of choosing 'dynamic content' then selecting the param.</p>
<p>So far so good.. my sink target is, as far as I am aware, ready to receive &quot;filenames&quot; to write out to.</p>
<p>This is where it all goes wrong.</p>
<p>I now create a new pipeline. I want to use a list or array of values inside that pipeline which represent the names of the files I want to process. I have been told that I'll need a Foreach to send each of the values one at a time to the COPY DATA task behind the Foreach. I am no stranger to Foreach type loops and behaviors.. but for the life of me I CANNOT see where to set up the list of filenames. I can create a param as a type &quot;array&quot; but how the heck do you populate it?</p>
<p>I have another use case which this problem is preventing me from completing. This use case is, I think, the same problem but perhaps serves to explain the situation more clearly. It goes like this:</p>
<p>I have a linked service to a remote database. I need to copy data from that database (around 12 tables) into the data lake. At the moment I have about 12 &quot;COPY DATA&quot; actions linked together - which is ridiculous. I want to use a Foreach loop to copy the data from source to data lake one after the other. Again, I can set up the sink dataset to be parameterised, just fine... but how the heck do I create the array/list of table names in the pipeline to pass to the sink dataset?</p>
<p>I add the Foreach and inside the foreach a &quot;COPY DATA&quot; but where do I add all the table names?</p>
<p>Would be very grateful for any assistance. THANK YOU.</p>
","<azure><azure-data-factory>","2020-08-19 19:56:31","52","1","1","63494811","<p>If you want to manually populate values of an array as a pipeline parameter, you create the parameter with Array type and set the value with syntax like:  [&quot;File1&quot;,&quot;File2&quot;,&quot;File3&quot;]</p>
<p><a href=""https://i.stack.imgur.com/OGc7t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OGc7t.png"" alt=""enter image description here"" /></a></p>
<p>You then iterate that array using a ForEach activity.</p>
<p><a href=""https://i.stack.imgur.com/ygWIU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ygWIU.png"" alt=""enter image description here"" /></a></p>
<p>Inside the ForEach, you reference @item() to get the current file name value the loop is on.</p>
<p><a href=""https://i.stack.imgur.com/vHGk7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vHGk7.png"" alt=""enter image description here"" /></a></p>
<p>You can also use a Lookup activity to get data from elsewhere and iterate over that using the ForEach.</p>
"
"63491065","How to Convert a column having one timestamp to another timestamp in Azure Data Factory","<p>I have column <strong>ABC</strong> where timestamp is of format dd/MM/yyyy HH:mm:SS (11/04/2020 1:17:40).I want to create another column <strong>ABC_NEW</strong> with same data as old column but with different timestamp 'yyyy-MM-dd HH:mm:SS'.I tried doing in azure data factory derived column using</p>
<p>toTimestamp(column_name,'yyyy-MM-dd HH:mm:SS') but it did not work it is coming as NULL. Can anyone help?</p>
","<azure><timestamp><azure-data-factory>","2020-08-19 16:14:23","3752","1","3","63496556","<p>It's a 2-step process. You first need to tell ADF what each field in your timestamp column represents, then you can use string conversions to manipulate that timestamp into the output string as you like:</p>
<p>toString(toTimestamp('11/04/2020 1:17:40','MM/dd/yyyy HH:mm:ss'),'yyyy-MM-dd HH:mm:SS')</p>
"
"63491065","How to Convert a column having one timestamp to another timestamp in Azure Data Factory","<p>I have column <strong>ABC</strong> where timestamp is of format dd/MM/yyyy HH:mm:SS (11/04/2020 1:17:40).I want to create another column <strong>ABC_NEW</strong> with same data as old column but with different timestamp 'yyyy-MM-dd HH:mm:SS'.I tried doing in azure data factory derived column using</p>
<p>toTimestamp(column_name,'yyyy-MM-dd HH:mm:SS') but it did not work it is coming as NULL. Can anyone help?</p>
","<azure><timestamp><azure-data-factory>","2020-08-19 16:14:23","3752","1","3","63501233","<p>Data Factory doesn't support date format 'dd/mm/yyyy', we can not convert it to 'YYYY-MM-DD' directly.<br />
I use <strong>DerivedColumn</strong> to generate a new column <strong>ABC_NEW</strong> from origin column <strong>DateTime</strong> and enter the expression bellow：</p>
<pre><code>toTimestamp(concat(split(substring(DateTime,1, 10), '/')[3], '-',split(substring(DateTime,1, 10), '/')[2],'-',split(substring(DateTime,1, 10), '/')[1],substring(DateTime,11, length(DateTime))))
</code></pre>
<p><a href=""https://i.stack.imgur.com/9aFxO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9aFxO.png"" alt=""enter image description here"" /></a></p>
<p>The result shows:</p>
<p><a href=""https://i.stack.imgur.com/gzDcv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gzDcv.png"" alt=""enter image description here"" /></a></p>
"
"63491065","How to Convert a column having one timestamp to another timestamp in Azure Data Factory","<p>I have column <strong>ABC</strong> where timestamp is of format dd/MM/yyyy HH:mm:SS (11/04/2020 1:17:40).I want to create another column <strong>ABC_NEW</strong> with same data as old column but with different timestamp 'yyyy-MM-dd HH:mm:SS'.I tried doing in azure data factory derived column using</p>
<p>toTimestamp(column_name,'yyyy-MM-dd HH:mm:SS') but it did not work it is coming as NULL. Can anyone help?</p>
","<azure><timestamp><azure-data-factory>","2020-08-19 16:14:23","3752","1","3","67966078","<p>This is a trick which was a blocker for me, but try this-</p>
<ol>
<li>Go to sink</li>
<li>Mapping</li>
<li>Click on output format</li>
<li>Select the data format or time format you prefer to store the data into the sink.</li>
</ol>
"
"63489742","JSON parsing error, unsupported encoding or multiline in Data Factory","<p>I am using Azure's Data Factory tool to read a text file with a JSON structure from a Data Lake Storage Gen2 to transform the data and pass it to an Azure SQL DB.</p>
<p>I can test the connection with &quot;Source settings&quot; and it works successfully.</p>
<p>Sadly, I am receiving the following error msg when trying to import a &quot;projection&quot;:</p>
<p>&quot;JSON parsing error, unsupported encoding or multiline&quot;</p>
<p>Under &quot;Source options&quot;-&gt;&quot;JSON settings&quot; I also tried it by activating the field &quot;Single document&quot;. Without any success.</p>
<p>[{&quot;_id&quot;:{&quot;$oid&quot;:&quot;xxxxxxx&quot;},&quot;uid&quot;:&quot;xxxxxxxxxx&quot;,&quot;test-uid&quot;:&quot;xxxxxxxxxxxx&quot;,&quot;url&quot;:&quot;https://test&quot;,&quot;info&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64)&quot;,&quot;test&quot;:&quot;&quot;,&quot;dimension&quot;:&quot;2560x1343&quot;,.....</p>
<p>The Data Preview, of course, shows the same error message.</p>
<p>I can't detect any JSON encoding errors. Any idea what I can do?
Any tutorial you can recommend for shifting JSON from Data Lake Gen2 into Azure SQL DB?</p>
<p>Update:</p>
<p>Due to the special character &quot;$&quot;, the file were encoded in UTF-8-BOM. Reading the data from inside a Data Factory from the Data Lake Storage Gen2 this leads to the parsing errors.</p>
","<mongodb><azure-sql-database><azure-data-factory><azure-data-lake>","2020-08-19 14:55:29","787","1","1","63572554","<p>Congratulations that you solved it:</p>
<p>&quot;I just remove the $ and everything works then&quot;.</p>
<p>Please see the question update.</p>
<p>I help you post it as answer and this can be beneficial to other community members.</p>
"
"63488899","Azure Data Factory - Degree of copy parallelism","<p>I'm running an Azure Data Factory that copies multiple tables from on prem SQL server to an Azure Data Lake.</p>
<p>So, I set many Copy activities through Az Data Factory Designer to execute parallel copies (each activity is carrying on the extract of one table).</p>
<p>For better resources optimization, I would like to know if there is a way to copy multiple tables with one Copy activity ?</p>
<p>I heard of &quot;degree of copy parallelism&quot;, but don't know how to use it ?</p>
<p>Rgds,</p>
<hr />
<p><strong>If the question helped, up-vote it. Thanks in advance.</strong></p>
","<azure-data-factory>","2020-08-19 14:11:45","4528","1","1","63495115","<p>To use one Copy activity for multiple tables, you'd need to wrap a single parameterized Copy activity in a ForEach activity.  The ForEach can scale to run multiple sources at one time by setting isSequential to false and setting the batchCount value to the number of threads you want.  The default batch count is 20 and the max is 50.  Copy Parallelism on a single Copy activity just uses more threads to concurrently copy partitions of data from the same data source.</p>
"
"63487186","Delete Multiple Azure Data Factory Pipeline","<p>I want to delete 50+ ADF pipelines linked to Azure Devops GIT. We can do it manually via Azure Front-end, but it's a tedious task. I have tried deleting it via Powershell but powershell can only delete the pipelines which present under DataFactory(PFA the screenshot) mode whereas it is not impacting pipelines linked to Azure DevOps GIT.</p>
<p><a href=""https://i.stack.imgur.com/jF7qz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jF7qz.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone suggest any better approach to do this activity ?</p>
","<azure><azure-pipelines><azure-powershell><azure-data-factory><azureportal>","2020-08-19 12:41:04","894","1","2","63510985","<p>If you just want them out of git, you can create a feature branch from the ADF editor. Then use Visual Studio or any git repo navigator to pull that branch to your local file system. Manually delete the files and then push back to git and do a pull request to merge back into your master.</p>
<p>If you really want to purge them completely, you can do the same manual deletion upon your ADF publish branch too.</p>
"
"63487186","Delete Multiple Azure Data Factory Pipeline","<p>I want to delete 50+ ADF pipelines linked to Azure Devops GIT. We can do it manually via Azure Front-end, but it's a tedious task. I have tried deleting it via Powershell but powershell can only delete the pipelines which present under DataFactory(PFA the screenshot) mode whereas it is not impacting pipelines linked to Azure DevOps GIT.</p>
<p><a href=""https://i.stack.imgur.com/jF7qz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jF7qz.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone suggest any better approach to do this activity ?</p>
","<azure><azure-pipelines><azure-powershell><azure-data-factory><azureportal>","2020-08-19 12:41:04","894","1","2","63561917","<p>I accomplished the task of deleting pipeline by :-</p>
<ol>
<li>Creating a clone of the Dev branch in my local Visual studio.</li>
<li>By deleting all .json file (corresponding to the pipeline I wanted to delete) from newly cloned local branch.</li>
<li>Commit, Sync and Push the change to Dev GIT branch.</li>
</ol>
<p>This is the easiest way I could find to delete 50+ pipelines in one shot.</p>
"
"63475491","DayOfWeek function dynamically in Azure Data Factory","<p>I have a column like this abc - 11/04/2020 1:17:40  date format I want to create a derived column where I need <code>dayOfWeek</code> dynamically for every row present in abc column.</p>
<p>I tried using a derived column in adf but it is not showing the day everything is coming as NULL.</p>
<p>Below is the image of the dataflow pipeline where I am trying to add a derive column with the expression. I tried passing the column abc there but no use it is not coming.</p>
<p>How can I solve this issue?</p>
<p><a href=""https://i.stack.imgur.com/vxWJ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vxWJ2.png"" alt=""Sample image of table"" /></a></p>
","<azure><azure-functions><azure-sql-database><azure-data-factory>","2020-08-18 19:32:57","3177","0","2","63478949","<p>You could using bellow expressions, add the date format:</p>
<pre><code>dayOfWeek(toDate(abc,'MM/dd/yyyy'))
</code></pre>
<p>I tested and it works well:
<a href=""https://i.stack.imgur.com/9sNYb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9sNYb.png"" alt=""enter image description here"" /></a></p>
<p>Update:</p>
<p>For your another question, we could using <code>case</code> to achieve that:</p>
<pre><code>case(dayOfWeek(toDate(abc,'MM/dd/yyyy'))==0,'Sunday',
dayOfWeek(toDate(abc,'MM/dd/yyyy'))==1,'Monday',
dayOfWeek(toDate(abc,'MM/dd/yyyy'))==2,'Tuesday',
dayOfWeek(toDate(abc,'MM/dd/yyyy'))==3,'Wednesday',
dayOfWeek(toDate(abc,'MM/dd/yyyy'))==4,'Thursday',
dayOfWeek(toDate(abc,'MM/dd/yyyy'))==5,'Fraiday',
dayOfWeek(toDate(abc,'MM/dd/yyyy'))==6,'Saturday'
 )
</code></pre>
<p>Scheenshot:</p>
<p><a href=""https://i.stack.imgur.com/uWmpk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uWmpk.png"" alt=""enter image description here"" /></a></p>
"
"63475491","DayOfWeek function dynamically in Azure Data Factory","<p>I have a column like this abc - 11/04/2020 1:17:40  date format I want to create a derived column where I need <code>dayOfWeek</code> dynamically for every row present in abc column.</p>
<p>I tried using a derived column in adf but it is not showing the day everything is coming as NULL.</p>
<p>Below is the image of the dataflow pipeline where I am trying to add a derive column with the expression. I tried passing the column abc there but no use it is not coming.</p>
<p>How can I solve this issue?</p>
<p><a href=""https://i.stack.imgur.com/vxWJ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vxWJ2.png"" alt=""Sample image of table"" /></a></p>
","<azure><azure-functions><azure-sql-database><azure-data-factory>","2020-08-18 19:32:57","3177","0","2","65434821","<p>Did you try this? A very simple one. Here you can use the variable 'abc' instead of UTC time.</p>
<pre><code>@formatDateTime(utcnow(),'dddd')
</code></pre>
<p><a href=""https://i.stack.imgur.com/xNs8z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xNs8z.png"" alt=""enter image description here"" /></a></p>
"
"63470265","""Failed to delete integration runtime."" Azure Data Factory","<p>I have two integration runtimes(both are self-hosted). When I try to delete one I get an error message.</p>
<pre><code>Error: Failed to delete integration runtime.
Detail: The document cannot be deleted since it is referenced by AzureSqlDatabaseContoso.
</code></pre>
<p>But this is not true. At the moment there is no such thing as &quot;AzureSqlDatabaseContoso&quot;. Perhaps it might have been there before. I did a search on source code as well, it is not present in the whole Git branch.
How can I delete it ?</p>
","<azure><azure-data-factory>","2020-08-18 13:58:55","3607","3","2","63510282","<p>This happened to me before.  I just recreated the phantom object with the same name, associated it with the IR to be deleted, and then deleted the newly-recreated object (AzureSqlDatabaseContoso, in this case).</p>
<p>After that, ADF let me delete the underlying IR. Weird, but it worked for me.</p>
"
"63470265","""Failed to delete integration runtime."" Azure Data Factory","<p>I have two integration runtimes(both are self-hosted). When I try to delete one I get an error message.</p>
<pre><code>Error: Failed to delete integration runtime.
Detail: The document cannot be deleted since it is referenced by AzureSqlDatabaseContoso.
</code></pre>
<p>But this is not true. At the moment there is no such thing as &quot;AzureSqlDatabaseContoso&quot;. Perhaps it might have been there before. I did a search on source code as well, it is not present in the whole Git branch.
How can I delete it ?</p>
","<azure><azure-data-factory>","2020-08-18 13:58:55","3607","3","2","63523702","<p>the answer is what JeffRamos posted. Another option is to rename the file and 'name' field in git source and reload the adf and delete it there.</p>
<pre><code>Source/linkedService/AzureKeyVault.json 
</code></pre>
<p>rename this to</p>
<pre><code>Source/linkedService/test.json 
</code></pre>
<p>json content</p>
<pre><code>{
    &quot;name&quot;: &quot;AzureKeyVault&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureKeyVault&quot;,
        &quot;typeProperties&quot;: {
            &quot;baseUrl&quot;: &quot;https://mykv.vault.azure.net/&quot;
        }
    }
}
</code></pre>
<p>Rename &quot;name&quot; field</p>
<pre><code>{
    &quot;name&quot;: &quot;test&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureKeyVault&quot;,
        &quot;typeProperties&quot;: {
            &quot;baseUrl&quot;: &quot;https://mykv.vault.azure.net/&quot;
        }
    }
}
</code></pre>
"
"63467597","How to use values passed in HTTP Request in Logic Apps / Assign Values to Logic App Parameters Dynamically","<p>I am trying to make a generic Logic App(LA) to do some processing on some files. Calling the Logic App from ADF and able to pass the correct File Names. However I am not able to use/assign values passed to the Logic App to the parameters defined in the LA.  What am I Missing ? Please see the screenshot.</p>
<p>-Thanks</p>
<p><a href=""https://i.stack.imgur.com/QRDI8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QRDI8.png"" alt=""enter image description here"" /></a></p>
<p>Sample Execution to show the names are passed properly.</p>
<p><a href=""https://i.stack.imgur.com/QOGak.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QOGak.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><azure-logic-apps>","2020-08-18 11:24:37","2025","1","1","63478696","<p>As far as I know, we can't assign <code>PRM_FileName</code> from the body of the request to one parameter. But we can use expression to get the value of <code>PRM_FileName</code>.</p>
<p>The expression should be <code>triggerBody()?['PRM_FileName']</code>. You can also assign <code>PRM_FileName</code> to a variable (for example named <code>var1</code>) and you can use the <code>var1</code> in your next actions but not use the expression(shown as below screenshot).
<a href=""https://i.stack.imgur.com/1nj2X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1nj2X.png"" alt=""enter image description here"" /></a></p>
<p>============================<strong>Update</strong>===========================</p>
<p>Below is my logic app:</p>
<p><a href=""https://i.stack.imgur.com/WdtaS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WdtaS.png"" alt=""enter image description here"" /></a></p>
<p>I did everything what you mentioned in your 3 steps except I put the <code>PRM_FileName</code> in the body of the request but not appending it at the end of url.</p>
<p><a href=""https://i.stack.imgur.com/dcxLA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dcxLA.png"" alt=""enter image description here"" /></a></p>
<p>============================<strong>Update 2</strong>===========================</p>
<p>Please use same schema with mine:</p>
<pre><code>{
    &quot;type&quot;: &quot;object&quot;,
    &quot;properties&quot;: {
        &quot;PRM_FileName&quot;: {
            &quot;type&quot;: &quot;string&quot;
        }
    }
}
</code></pre>
<p>And then select the <code>PRM_FileName</code> into the variable directly(shown as below screenshot).
<a href=""https://i.stack.imgur.com/h0nZ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h0nZ2.png"" alt=""enter image description here"" /></a></p>
<p>The expression should be <code>triggerBody()?['PRM_FileName']</code>, but in your screenshot the expression is <code>triggerOutputs()['queries']['PRM_FileName']</code>.</p>
"
"63466371","Read two columns alone from an excel in Azure Data Factory","<p>I have my data in excel file and I am able to read the whole sheet.
But I want to read only first column and third column. I have headers in the excel. So I need to read values from A2 and from C2.</p>
<p>I tried reading <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-excel"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-excel</a> . But unable to find a way. Any ways to read these two columns alone. Can any help me what values I have to specify in range. Many thanks!</p>
<p><a href=""https://i.stack.imgur.com/oDYfQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oDYfQ.png"" alt=""enter image description here"" /></a></p>
","<excel><azure><azure-data-factory>","2020-08-18 10:06:22","1006","0","2","63467057","<p>Source file with 4 column</p>
<p><img src=""https://i.stack.imgur.com/2O7YV.png"" alt=""1"" /></p>
<p>Select the column that you want from and Click om Insert Pivot table and grab the range</p>
<p><img src=""https://i.stack.imgur.com/ZbnqU.png"" alt=""2"" /></p>
<p>Then open the Source dataset and paste that range and preview the data.</p>
<p><img src=""https://i.stack.imgur.com/e3ceX.png"" alt=""3"" /></p>
<p>Configure the sink and publish your pipeline.
Hope, You got your answer.</p>
"
"63466371","Read two columns alone from an excel in Azure Data Factory","<p>I have my data in excel file and I am able to read the whole sheet.
But I want to read only first column and third column. I have headers in the excel. So I need to read values from A2 and from C2.</p>
<p>I tried reading <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-excel"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-excel</a> . But unable to find a way. Any ways to read these two columns alone. Can any help me what values I have to specify in range. Many thanks!</p>
<p><a href=""https://i.stack.imgur.com/oDYfQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oDYfQ.png"" alt=""enter image description here"" /></a></p>
","<excel><azure><azure-data-factory>","2020-08-18 10:06:22","1006","0","2","63482334","<p>Maybe this can be a workaround.</p>
<p>screenshot of dataflow:</p>
<p><a href=""https://i.stack.imgur.com/nXX7v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nXX7v.png"" alt=""enter image description here"" /></a></p>
<p>1.create two source in dataflow,range of first source:<code>A1:A5</code>,another:<code>C1:C5</code>(row num should be same)</p>
<p>2.create two SurrogateKey,give the key column,and start value should be same.</p>
<p>3.create join,setting like below:
<a href=""https://i.stack.imgur.com/DSqTa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DSqTa.png"" alt=""enter image description here"" /></a></p>
<p>4.create select,delete the column you don't need(sk),then you can do some transform or something else.</p>
<p>data preview:
<a href=""https://i.stack.imgur.com/KpHSz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KpHSz.png"" alt=""enter image description here"" /></a></p>
"
"63461961","Azure Blob CONTENT-MD is null for some files","<p>blob files have <code>CONTENT-MD5  -</code>. When I am trying to read it from properties, its giving value as <code>null</code>. It's happening with few files only, others having their md5. for me, It's not possible to re-upload these files as they are stored here by an ADF job. Is there any specific reason for this?</p>
","<azure><azure-blob-storage><azure-data-factory>","2020-08-18 04:38:06","608","1","1","63497121","<p>I have met this issue, I don't why and didn't find the reason.</p>
<p>Just as I know, the only solution is download these no CONTENT-MD5 files from the Blob storage, re-upload to Blob Storage and overwrite the old one. Then the files will have CONTENT-MD5. If you don't have the permission to access the blob, I'm afraid there's no good solution for this issue.</p>
<p>For example, I checked my storage and find a file (created by Data Factory) with no CONTENT-MD5 property:
<a href=""https://i.stack.imgur.com/WFKZz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WFKZz.png"" alt=""enter image description here"" /></a></p>
<p>I just download it and upload again:</p>
<p><a href=""https://i.stack.imgur.com/94hOR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/94hOR.png"" alt=""enter image description here"" /></a></p>
<p>Then the file will have the CONTENT-MD5 property:
<a href=""https://i.stack.imgur.com/XWIkN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XWIkN.png"" alt=""enter image description here"" /></a></p>
<p>During the uploading, I think Azure will re-create the CONTENT-MD5 for it.</p>
"
"63459334","Once I have saved data to a variable, how can I then use that variable to copy to a new file?","<p>I have a pipeline that is retrieving data to an array variable in a ForEach loop. I'd like to next use this array variable and save this array data into a new file (JSON or CSV is preferred).</p>
<p>How would I accomplish this?</p>
","<azure-data-factory>","2020-08-17 22:16:45","77","0","1","63466103","<p>In a ForEach loop and We can use the <code>item().&lt;KeyName&gt;</code> to access every item's value.</p>
<p>For exmaple :<br />
If the file is formatted as the following: {&quot;id&quot;:&quot;ba3cbfdd-c668-4dab-88c0-ac2f3af183cf&quot;} {&quot;id&quot;:&quot;e90f527b-efe0-43ee-b398-e9e6b7f2abb5&quot;}, then we can use <code>item().id</code> to access the value.</p>
"
"63457538","What are the API Permissions necessary to start Azure Data Factory pipeline?","<p>I would need to start an Azure Data Factory pipeline from REST API as per <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun#code-try-0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun#code-try-0</a></p>
<p>I have created an AAD app, and given it the <em>Azure Service Management</em> API Permission. However when the client through the OAuth2 Implicit flow receives an id_token and invokes that API to start a pipeline I get</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;InvalidAuthenticationToken&quot;,
        &quot;message&quot;: &quot;The access token is invalid.&quot;
    }
}
</code></pre>
<p>Am i using proper API pemrission? thanks.</p>
","<azure><oauth-2.0><active-directory><azure-data-factory>","2020-08-17 19:38:54","1623","2","2","63457644","<p>I cant test it right now but I would assume that having Data Factory Contributor should be enough for this.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#data-factory-contributor"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#data-factory-contributor</a></p>
<p><a href=""https://i.stack.imgur.com/ACbtC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ACbtC.png"" alt=""enter image description here"" /></a></p>
"
"63457538","What are the API Permissions necessary to start Azure Data Factory pipeline?","<p>I would need to start an Azure Data Factory pipeline from REST API as per <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun#code-try-0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun#code-try-0</a></p>
<p>I have created an AAD app, and given it the <em>Azure Service Management</em> API Permission. However when the client through the OAuth2 Implicit flow receives an id_token and invokes that API to start a pipeline I get</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;InvalidAuthenticationToken&quot;,
        &quot;message&quot;: &quot;The access token is invalid.&quot;
    }
}
</code></pre>
<p>Am i using proper API pemrission? thanks.</p>
","<azure><oauth-2.0><active-directory><azure-data-factory>","2020-08-17 19:38:54","1623","2","2","63462843","<p>If you just want to use OAuth2 flow to get the token to call the REST API, the <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow"" rel=""nofollow noreferrer"">client credentials flow</a> is more suitable than the Implicit flow in this case.</p>
<p>Please follow the steps below.</p>
<p>1.<a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal#get-tenant-and-app-id-values-for-signing-in"" rel=""nofollow noreferrer"">Get values for signing in</a> and <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal#create-a-new-application-secret"" rel=""nofollow noreferrer"">create a new application secret</a>.</p>
<p>2.Navigate to the data factory -&gt; <code>Access control (IAM)</code> -&gt; <code>Add</code> -&gt; add your AD App as an RBAC role e.g. <code>Contributor</code>, <code>Owner</code>, <code>Data Factory Contributor</code>, details follow <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/quickstart-assign-role-user-portal"" rel=""nofollow noreferrer"">this</a>.</p>
<p>3.In the postman, follow the screenshot below, fix the request body got from step 1, then use the token to call REST API, it will work fine.</p>
<pre><code>POST https://login.microsoftonline.com/&lt;tenant-id&gt;/oauth2/v2.0/token

client_id=&lt;client_id&gt;
&amp;scope=https://management.azure.com/.default
&amp;client_secret=&lt;client_secret&gt;
&amp;grant_type=client_credentials
</code></pre>
<p><a href=""https://i.stack.imgur.com/swdlI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/swdlI.png"" alt=""enter image description here"" /></a></p>
"
"63455354","Azure Data Factory send failed notifications to teams channel","<p>I need help on how do I send Azure Data Factory(ADF) Start, Success or Failure notification within ADF using Web activity.</p>
<p>I searched a lot but found articles relates to sending email notifications through ADF, but no article found which talks about sending these Start, Success or Failure notification to MS-Teams channel.</p>
<p>NOTE : I already have created MS-Teams channel and I have links associated with this channel.</p>
","<azure-pipelines><azure-data-factory><microsoft-teams>","2020-08-17 16:57:55","978","0","1","63462398","<p>Could you please take a look at <a href=""https://learn.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/what-are-webhooks-and-connectors"" rel=""nofollow noreferrer"">Webhooks and Connectors</a>?. Connectors allow you to post the notifications from an external service to the Teams channel. You can build your own connector by following <a href=""https://learn.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/connectors-creating"" rel=""nofollow noreferrer"">this</a> guide.</p>
"
"63454768","Azure Data Factory Exist Transformation","<p>Is there a way that after comparing two tables and then use the Case function?</p>
<p>I am trying to have a new column base on Exists transformation. In sql I do it like this:</p>
<pre><code>(isnull (select 'YES' from sales where salesperson = t1.salesperson group by salesperson), 'NO')) AS registeredSales

T1 is personal.
</code></pre>
<p>Or should I include the table into the stream of the joins and then use the case() function to compare the two columns?</p>
<p>If there's another way to work around to compare these two streams, I would be pleased to hear.</p>
<p>Thanks.</p>
","<azure-data-factory>","2020-08-17 16:18:18","993","0","1","63461093","<p>Flat files in a datalake can also be compared. We can use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">derived column</a> in dataflow to gernerate a new column.</p>
<ol>
<li>I create a dataflow demo cotains two sources:  <strong>CustomerSource</strong>(customer.csv stored in datalake2) and <strong>SalesSource</strong>(sales.csv stored in datalake2 and it contains only one column) as follows</li>
</ol>
<p><a href=""https://i.stack.imgur.com/a1Qf4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a1Qf4.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Then I join the two sources with the column <strong>CustomerId</strong></li>
</ol>
<p><a href=""https://i.stack.imgur.com/whGi8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/whGi8.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>Then I use <strong>Select</strong> activity to give an alias to the <strong>CustomerId</strong> from <strong>SalesSource</strong></li>
</ol>
<p><a href=""https://i.stack.imgur.com/Nf1X9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nf1X9.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li><p>In the <strong>DerivedColumn</strong>, I select the <strong>Add column</strong> and enter the expression <code>iifNull(SalesCustomerID, 'NO', 'YES')</code> to generate a new column named 'registeredSales' as follows:
<a href=""https://i.stack.imgur.com/DNNY4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DNNY4.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>The last column of the result shows:</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/TY1fK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TY1fK.png"" alt=""enter image description here"" /></a></p>
"
"63454386","How to copy files of specific size from one blob container to another blob container?","<p>Is there a way to copy files of Specific size(like files size greater than or less than 100 mb) from one blob to another blob container or any location using ADF or any other Azure resources which can help achieve it.</p>
","<azure><azure-functions><azure-blob-storage><azure-data-factory>","2020-08-17 15:54:34","89","-1","1","63460689","<p>I recommend you to use <code>logic app</code>. The following is the design process:</p>
<p><a href=""https://i.stack.imgur.com/XmMen.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XmMen.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/GoVgF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GoVgF.png"" alt=""enter image description here"" /></a></p>
<p>Note that the unit of <code>size</code> is bytes, you need to convert it.</p>
<p>Here are the test results:</p>
<p><strong>Source container</strong></p>
<p><a href=""https://i.stack.imgur.com/xiOhp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xiOhp.png"" alt=""enter image description here"" /></a></p>
<p><strong>Destination container</strong></p>
<p><a href=""https://i.stack.imgur.com/DfgWj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DfgWj.png"" alt=""enter image description here"" /></a></p>
<p>The test results look okay.</p>
"
"63453320","Cannot register Azure Integration Runtime","<p>I am trying to register a self hosted Azure integration runtime.
This used to work before, but suddenly the existing runtime could not connect anymore.</p>
<p>I already:</p>
<ul>
<li>Reinstalled the local runtime installation</li>
<li>Removed integration runtime in Azure and created a new one</li>
<li>Regenerated keys in Azure and tried key1 and key2</li>
<li>Tried to install the runtime on another server</li>
<li>Tried to use the Express setup from the 'Edit integration runtime' page</li>
</ul>
<p><a href=""https://i.stack.imgur.com/pOCMt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pOCMt.png"" alt=""enter image description here"" /></a></p>
<p>But when I try to register I always get the following error:
<a href=""https://i.stack.imgur.com/C2XwG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C2XwG.png"" alt=""enter image description here"" /></a></p>
<p>In the event log I get the following error:</p>
<pre><code>Failed to get service token from ADF service with key ****************** and time cost is: 0.1628846 seconds, the error code is: InvalidGatewayKey, activityId is: 445deaef-dcd5-44c9-9123-703b81b35f3f and detailed error message is Processed HTTP request failed.
.

   at Microsoft.DataTransfer.DIAgentHost.NamedPipeControllers.NamedPipeGatewayClientController.AttachGatewayNode(String[] args)
   at Microsoft.DataTransfer.DIAgentHost.NamedPipeServer.MapNamedPipeRequestToController(String requestTemplate, NamedPipeStreamString ss, NamedPipeDataContract`1 result, String temp)
   at Microsoft.DataTransfer.DIAgentHost.NamedPipeServer.PipeServerThread(Object data)
.
</code></pre>
<p>So here there is mention of 'InvalidGatewayKey' and 'Processed HTTP request failed'</p>
<p>I couldn't find any useful information on these errors, so any help is greatly appreciated!</p>
","<azure><azure-data-factory>","2020-08-17 14:50:42","1818","3","2","63507118","<p>Finally the only solution was to create a new data factory and delete the old one.
No idea why the initial suddenly stopped working.</p>
"
"63453320","Cannot register Azure Integration Runtime","<p>I am trying to register a self hosted Azure integration runtime.
This used to work before, but suddenly the existing runtime could not connect anymore.</p>
<p>I already:</p>
<ul>
<li>Reinstalled the local runtime installation</li>
<li>Removed integration runtime in Azure and created a new one</li>
<li>Regenerated keys in Azure and tried key1 and key2</li>
<li>Tried to install the runtime on another server</li>
<li>Tried to use the Express setup from the 'Edit integration runtime' page</li>
</ul>
<p><a href=""https://i.stack.imgur.com/pOCMt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pOCMt.png"" alt=""enter image description here"" /></a></p>
<p>But when I try to register I always get the following error:
<a href=""https://i.stack.imgur.com/C2XwG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C2XwG.png"" alt=""enter image description here"" /></a></p>
<p>In the event log I get the following error:</p>
<pre><code>Failed to get service token from ADF service with key ****************** and time cost is: 0.1628846 seconds, the error code is: InvalidGatewayKey, activityId is: 445deaef-dcd5-44c9-9123-703b81b35f3f and detailed error message is Processed HTTP request failed.
.

   at Microsoft.DataTransfer.DIAgentHost.NamedPipeControllers.NamedPipeGatewayClientController.AttachGatewayNode(String[] args)
   at Microsoft.DataTransfer.DIAgentHost.NamedPipeServer.MapNamedPipeRequestToController(String requestTemplate, NamedPipeStreamString ss, NamedPipeDataContract`1 result, String temp)
   at Microsoft.DataTransfer.DIAgentHost.NamedPipeServer.PipeServerThread(Object data)
.
</code></pre>
<p>So here there is mention of 'InvalidGatewayKey' and 'Processed HTTP request failed'</p>
<p>I couldn't find any useful information on these errors, so any help is greatly appreciated!</p>
","<azure><azure-data-factory>","2020-08-17 14:50:42","1818","3","2","75171796","<p>In case anyone else have this problem, it may be that your ADF is erroneously configured to use private endpoint instead of public endpoint.</p>
<p>The error reported here is similar to what is documented in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/security-and-access-control-troubleshoot-guide?tabs=data-factory#unable-to-register-ir-authentication-key-on-self-hosted-vms-due-to-private-link"" rel=""nofollow noreferrer"">Unable to register IR authentication key on Self-hosted VMs due to private link</a>.</p>
"
"63448774","Execute azure data factory foreach activity with start date and end date","<p>I have a json file and it contains the start date and end date. I need to iterate over this start date and end date with azure data factory foreach activity. As per my knowledge, the foreach expect the items (collection/array). But in my case, I have only two items which are start and end date. I want to run the data factory for process some historic data. I don't have the collection of dates, so how can I iterate this with start date and end date? If someone can help me to figure it out, it would be great.</p>
","<azure><azure-data-factory>","2020-08-17 10:12:45","3376","3","2","63449929","<p>The for-each loop has no intelligence built in to calculate the number of iterations it has to repeat for certain tasks added into the looping activity.</p>
<p>What can be done here is to use some sort of watermark inputs and run the data load using these watermarks. Tasks such as lookup activity will come handy to emulate this behavior.</p>
"
"63448774","Execute azure data factory foreach activity with start date and end date","<p>I have a json file and it contains the start date and end date. I need to iterate over this start date and end date with azure data factory foreach activity. As per my knowledge, the foreach expect the items (collection/array). But in my case, I have only two items which are start and end date. I want to run the data factory for process some historic data. I don't have the collection of dates, so how can I iterate this with start date and end date? If someone can help me to figure it out, it would be great.</p>
","<azure><azure-data-factory>","2020-08-17 10:12:45","3376","3","2","63481998","<p>My suggestion will be to use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-until-activity"" rel=""noreferrer"">UNTIL</a> activity to iterate from start date to end date.</p>
<p>Like FOR-EACH, UNTIL is also used for looping in ADF. While for-each iterates over a defined collection, Until iterates till a criteria is met. This is equivalent of while loop of programming languages.</p>
<p>You can assign both start and end dates to a variable and use a counter variable to increment dates one by one from start to end date.</p>
<p>You may use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#addDays"" rel=""noreferrer"">AddDays</a> expression function for incrementing a day and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#equals"" rel=""noreferrer"">equals</a> function for comparing dates</p>
"
"63447686","Parameterized authorization credentials in web activity ADF","<p>I try to parametrize Web activity authorization. I have created two parameters User_Name and Pass, type of String. Assign parameters value to auth. credential with this part of json code in adf pipeline.</p>
<p><a href=""https://i.stack.imgur.com/D8hce.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D8hce.png"" alt=""enter image description here"" /></a></p>
<p>Properties in the web activity settings tab look like this.</p>
<p><a href=""https://i.stack.imgur.com/FwEWy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FwEWy.png"" alt=""enter image description here"" /></a></p>
<p>I have got authentication error when trying to execute the pipeline, the user name property has got a correct value but the password was bad. This is error code message:  <em>&quot;errorCode&quot;: &quot;2108&quot;</em>
<em>Authentication_InvalidCredentials&quot;,&quot;message&quot;:&quot;The server has rejected the client credentials.</em></p>
<p>Any help will be appreciated</p>
","<azure><authentication><parameters><authorization><azure-data-factory>","2020-08-17 08:57:32","1326","2","2","63449972","<p>It is not recommended practice to parameterize passwords or secrets. Store all connection strings in Azure Key Vault instead, and parameterize the Secret Name.</p>
"
"63447686","Parameterized authorization credentials in web activity ADF","<p>I try to parametrize Web activity authorization. I have created two parameters User_Name and Pass, type of String. Assign parameters value to auth. credential with this part of json code in adf pipeline.</p>
<p><a href=""https://i.stack.imgur.com/D8hce.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D8hce.png"" alt=""enter image description here"" /></a></p>
<p>Properties in the web activity settings tab look like this.</p>
<p><a href=""https://i.stack.imgur.com/FwEWy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FwEWy.png"" alt=""enter image description here"" /></a></p>
<p>I have got authentication error when trying to execute the pipeline, the user name property has got a correct value but the password was bad. This is error code message:  <em>&quot;errorCode&quot;: &quot;2108&quot;</em>
<em>Authentication_InvalidCredentials&quot;,&quot;message&quot;:&quot;The server has rejected the client credentials.</em></p>
<p>Any help will be appreciated</p>
","<azure><authentication><parameters><authorization><azure-data-factory>","2020-08-17 08:57:32","1326","2","2","63482488","<p>As a rule of thumb, things that can have parameterized content are those fields which accept dynamic input.</p>
<p>These tend to have text <em>Add dynamic content [Alt+P]</em> text below them when you click on the input box.</p>
<p><a href=""https://i.stack.imgur.com/Taz5W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Taz5W.png"" alt=""enter image description here"" /></a></p>
<p>User name and password in basic authentication cannot be parameterized directly but there is a way.</p>
<p>Choose Authentication as None in the settings but provide authentication information in header whose value can be parameterized.
See the screenshot below.</p>
<p><a href=""https://i.stack.imgur.com/EDoct.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EDoct.png"" alt=""enter image description here"" /></a></p>
"
"63447066","Linked Service between two or more datafactory","<p>is possible to configure a linked service between 2 or more datafactory?</p>
<p>I red documentation but i didn't found it</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2020-08-17 08:16:29","114","-1","1","63460995","<p>Per my experience, we can't do that and never heard such configuration.</p>
<p>Just as I know, we only could share the Integration runtime between 2 or more Data Factory.</p>
<p>But we still need to create the linked service to connect to the same on-premise data source through shared shared self-hosted integration runtime</p>
<p>In one word, it's impossible to configure a linked service between 2 or more Data Factory.</p>
"
"63440300","Azure function returns 403 when triggered by Azure DataFactory","<p>I have problem with triggering Azure function which returns 403 (HTTP trigger).</p>
<p>Trigger is initiated using Azure Data Factory.</p>
<p>It is important to mention that function is exposed through Azure APIM (API management) - function internally has restriction set in a way that only APIM IP is allowed to trigger the function.</p>
<p>Funny thing is that if I send ‘manual’ trigger (postman, HTTP call through APIM from my local machine) everything works as expected - I’m able to trigger the function.</p>
<p>If the same request is sent via Azure DataFactory 403 is returned. Like the request never leaves Azure and IP that hits the function is not the APIM IP. Is this possible and if yes, how to solve this?</p>
<p>Function, APIM and DataFactory are in the same Azure subscription.</p>
<p>FYI - 403 is coming from the function itself, not the APIM. If I remove access restriction from the function, everything works as expected.</p>
<p>Also, to make sure that the request goes through APIM, for one of the tests I put wrong APIM subscription key and this time when ADF triggered the function I got 403, but from APIM which means that requests goes for sure through gateway.</p>
","<azure><azure-functions><azure-data-factory><http-status-code-403><azure-api-management>","2020-08-16 18:03:06","859","0","1","63463590","<blockquote>
<p>Is this possible and if yes, how to solve this?</p>
</blockquote>
<p>I don't think it's possible, I test it in my side and it works fine(both in postman and in Data Factory). I provide my configuration for your reference.</p>
<p><strong>1.</strong> I create a APIM and we can see its IP address. Then add my function into APIM.</p>
<p><a href=""https://i.stack.imgur.com/nnTwC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nnTwC.png"" alt=""enter image description here"" /></a></p>
<p><strong>2.</strong> In my function, I configure &quot;Access Restrictions&quot; as below:
<a href=""https://i.stack.imgur.com/vuu3T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vuu3T.png"" alt=""enter image description here"" /></a></p>
<p><strong>3.</strong> Then request the url through APIM in postman, success.
<a href=""https://i.stack.imgur.com/oXvDy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oXvDy.png"" alt=""enter image description here"" /></a></p>
<p>Request the url through APIM in Data Factory, also success.</p>
<p><a href=""https://i.stack.imgur.com/hKNEp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hKNEp.png"" alt=""enter image description here"" /></a></p>
<p><strong>4.</strong> So please check if there is anything different with my configuration(For example, use the function url like <code>https://funappname.azurewebsites.net/api/HttpTrigger1?clientId=apim-huryAPIM</code> but not url from APIM in your Data Factory request). And my function, APIM, Data Factory are in same region(East Asia).</p>
"
"63434224","Automation for ADF V2 Pipeline","<p>I need help with implementation for below requirement:</p>
<p>There is one ADF pipeline that runs every two hours (with Tumbling window trigger), now i need to create one more pipeline that will be used for performing maintenance job . This pipeline is scheduled to run once a month (with schedule trigger). Here is the requirement that i'm trying to implement:</p>
<ol>
<li>Now before running the second pipeline i need to make sure the first pipeline is not running (basically get the status and if its running wait for its completion) and then disable the trigger associated with it.</li>
<li>Run the second pipeline and after its completion , enable the trigger that is associated with first pipeline</li>
</ol>
<p>Please let me know if this can be achieved within ADF or some kind of custom scripting needed to achieve the result.</p>
","<azure-data-factory><azure-automation>","2020-08-16 07:21:49","586","0","1","63443777","<p>First, your idea is achievable.</p>
<p>Second, if you want to use built-in feature in Azure Datafactory, then there is no way.</p>
<p>Basically, you need to use azure function(simple httptrigger, dont give any input, then you can hit and execute it directly.) to achieve your requirement that ADF can't do. From your description, the executing of these two pipelines are mutually exclusive, so you can use sdk to check to status of another pipeline in azure function. If another pipeline is running, then wait a few seconds then re-check the status of another pipeline.(In short, put the main logic and code in the azure function.)</p>
<p>Simple azure function:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook-trigger?tabs=csharp"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook-trigger?tabs=csharp</a></p>
<p>Use SDK to monitor:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#net"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#net</a></p>
<p>(The link I give is C#, you can choose other supported language.)</p>
<p><a href=""https://i.stack.imgur.com/dGkJD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dGkJD.png"" alt=""enter image description here"" /></a></p>
"
"63430641","Best approach to migrate huge amounts of data from Oracle 9i to Azure Hyperscale","<p>We have a requirement to transfer huge amounts of data from On-premise Oracle 9i to Hyperscale. I am trying to find the best possible approach to get this transfer happen. At the moment, I am using Azure Data Factory and it is taking more than 24 hours for the transfer and we don't have that much of a window while transferring data.</p>
<p>Also, my On-premise IR is on 16GB RAM with 4 Cores and the DTU's on Copy Activty on ADF is set to Auto.Can anyone suggest if this is the best approach that I am following or is there a better way ?</p>
<p>Note : I've checked that AzureDatabase Migration Service does not support Oracle 9i so I don't think that can be my approach.</p>
<p>Thanks in advance</p>
","<azure><azure-sql-database><database-migration><azure-data-factory><oracle9i>","2020-08-15 20:43:29","205","0","1","63444048","<p>We don't know how large data your Oracle database have, you can reference this Data Factory copy performance table:
<a href=""https://i.stack.imgur.com/6nYH3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6nYH3.png"" alt=""enter image description here"" /></a></p>
<p>To get the highly performant, we could follow the this document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#performance-tuning-steps"" rel=""nofollow noreferrer"">Copy activity performance and scalability guide</a>:</p>
<p>You are copy data from On-premise Oracle to Hyperscale, we need the Self-host integration runtime. To achieve higher throughput, you can either scale up or scale out the Self-hosted IR. Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#self-hosted-integration-runtime-scalability"" rel=""nofollow noreferrer"">Self-hosted integration runtime scalability</a></p>
<p>Or using more DIUs for the copy active:
<a href=""https://i.stack.imgur.com/YDo9Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YDo9Q.png"" alt=""enter image description here"" /></a></p>
"
"63419644","Azure data factory to split a CSV file into multiple CSV files?","<p>I have a file in Azure blob container as below,</p>
<pre><code>[Header 1]
123,abc,456,def,..
[Header 2]
789,XYZ, 101,PQR,..
567,DEF,675,GEF,..
</code></pre>
<p>I am expecting Azure data factory to convert it into multiple CSV files in blob container as below,</p>
<pre><code>CSV1 file:
    123,abc,456,def,..
CSV2 file:
    789,XYZ, 101,PQR,..
    567,DEF,675,GEF,..
</code></pre>
<p>The number of headers is known and always constant. But the number of rows under each header can vary.</p>
","<azure><csv><lookup><azure-data-factory>","2020-08-14 20:30:25","1513","1","1","63500474","<p>Please try this:</p>
<p>create a dataflow,add two source,create select and delete column you don't need,finally output to a single file.</p>
<p>screenshot of dataflow:
<a href=""https://i.stack.imgur.com/hV8iw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hV8iw.png"" alt=""enter image description here"" /></a></p>
<p>setting of sink:
<a href=""https://i.stack.imgur.com/9xPhk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9xPhk.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you.</p>
"
"63416877","Use Azure Data Factory to copy files and place a csv of files copied","<p>I am trying to implement the following flow in an Azure Data Factory pipeline:</p>
<ol>
<li>Copy files from an SFTP to a local folder.</li>
<li>Create a comma separated file in the local folder with the list of files and their
sizes.</li>
</ol>
<p>The first step was easy enough, using a 'Copy Data' step with 'SFTP' as source and 'File System' as sink.</p>
<p>The files are being copied, but in the output of this step, I don't see any file information.</p>
<p>I also don't see an option to create a file using data from a previous step.</p>
<p>Maybe I'm using the wrong technology?
One of the reasons I'm using Azure Data Factory, is because of the integration runtime, which allows us to have a single fixed IP to connect to the external SFTP. (easier firewall configuration)</p>
<p>Is there a way to implement step 2?</p>
<p>Thanks for any insight!</p>
","<azure><azure-data-factory><azure-logic-apps>","2020-08-14 16:44:42","599","1","1","63446369","<p>There is no built-in feature to achieve this.</p>
<p>You need to use ADF with other service, I suppose you to first use azure function to check the files and then do copy.</p>
<p>The structure should be like this:</p>
<p><a href=""https://i.stack.imgur.com/mLfnq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mLfnq.png"" alt=""enter image description here"" /></a></p>
<p>You can get the size of the files and save them to the csv file:</p>
<p><strong>Get size of files(python):</strong></p>
<p><a href=""https://stackoverflow.com/questions/51298467/how-to-fetch-sizes-of-all-sftp-files-in-a-directory-through-paramiko"">How to fetch sizes of all SFTP files in a directory through Paramiko</a></p>
<p><strong>And use pandas to save the messages as csv(python):</strong></p>
<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html</a></p>
<p><a href=""https://stackoverflow.com/questions/16923281/writing-a-pandas-dataframe-to-csv-file"">Writing a pandas DataFrame to CSV file</a></p>
<p><strong>Simple http trigger of azure function(python):</strong></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook-trigger?tabs=python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook-trigger?tabs=python</a></p>
<p>(Put the processing logic in the body of the azure function. Basically, you can do anything you want in the body of the azure function except for the graphical interface and some unsupported things. You can choose the language you are familiar with, but in short, there is not a feature in ADF that satisfies your idea.)</p>
"
"63413972","Load XML Data Into SQL Table (Azure) via Azure Data Factory V2","<p>I am trying to load XML Data into Azure Table using ADF V2. There is a connector for XML as a Source but for some reason I am not able to get the data loaded as requirement.</p>
<p><a href=""https://i.stack.imgur.com/4uNfT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4uNfT.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/IBvAm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IBvAm.png"" alt=""enter image description here"" /></a></p>
<p>So I tried to load the Entire XML into a stage table Column (XML TYPE) so that I can use XQuery to get the desired Result but unable to.</p>
<p>Any Ideas to load and query data by any other means using Azure Stack?</p>
","<xml><azure><azure-sql-database><azure-data-factory><sqlxml>","2020-08-14 13:43:53","571","0","1","63593066","<p>I am not sure as to what is the error which you were , but i did uploaded the below dummy XML data on to SQL .</p>
<p>XML dummy</p>
<pre><code>&lt;note&gt;
&lt;record&gt;
&lt;to&gt;Tove&lt;/to&gt;
&lt;from&gt;Jani&lt;/from&gt;
&lt;heading&gt;Reminder&lt;/heading&gt;
&lt;body&gt;Don't forget me this weekend!&lt;/body&gt;
&lt;/record&gt;
&lt;record&gt;
&lt;to&gt;Tove1&lt;/to&gt;
&lt;from&gt;Jani&lt;/from&gt;
&lt;heading&gt;Reminder&lt;/heading&gt;
&lt;body&gt;Don't forget me this weekend!&lt;/body&gt;
&lt;/record&gt;
&lt;record&gt;
&lt;to&gt;Tove2&lt;/to&gt;
&lt;from&gt;Jani&lt;/from&gt;
&lt;heading&gt;Reminder&lt;/heading&gt;
&lt;body&gt;Don't forget me this weekend!&lt;/body&gt;
&lt;/record&gt;
&lt;/note&gt;
</code></pre>
<p>This is my output .</p>
<p><a href=""https://i.stack.imgur.com/8uLIH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8uLIH.png"" alt=""enter image description here"" /></a></p>
<p>Please make sure is done correctly , this is the thing which worked for me .
<a href=""https://i.stack.imgur.com/3dd2N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3dd2N.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know if you have any questions .</p>
<p>Thanks
Himanshu</p>
"
"63397919","How to create azure data factory pipeline and trigger it automatically whenever file arrive in SFTP?","<p>I'm building azure data factory pipeline where source is SFTP and target is azure blob storage.
The files can arrive at anytime and any number of files can come into the SFTP on the daily basis.
I have to copy the file from Sftp to blob storage whenever any file arrive in SFTP.
I know event trigger functionality in ADF but It's possible only if files are coming into the blob storage.
Is it possible to achieve same kind of functionality i.e copying files on arrival,when sources are different from blob storage.</p>
","<azure><azure-data-factory>","2020-08-13 15:02:33","1772","1","1","63405179","<p>Data Factory can't achieve that.</p>
<p>Some ideas is that you could achieve your purpose with <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview"" rel=""nofollow noreferrer"">logic app</a>:</p>
<ol>
<li>You could create a <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sftp#examples"" rel=""nofollow noreferrer"">SFTP server trigger: When a file is added or
modified</a></li>
<li>Add an action <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#get-a-pipeline-run"" rel=""nofollow noreferrer"">get a pipeline run</a> to execute the Data Factory
pipeline:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/IOBVL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IOBVL.png"" alt=""enter image description here"" /></a></p>
<p>Pass the new added filename to the pipeline and run the pipeline.</p>
"
"63397110","Azure Datafactory can't handle empty json array in blob","<p>in azure data factory dataset, using the copy activity to load json blob to sqldb, when the json blob is an empty array &quot;[]&quot; the copy activity gets stuck with error.</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=UserErrorTypeInSchemaTableNotSupported,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to get the type from schema table. This could be caused by missing Sql Server System CLR Types.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.InvalidCastException,Message=Unable to cast object of type 'System.DBNull' to type 'System.Type'.,Source=Microsoft.DataTransfer.ClientLibrary,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;BP_acctset_Blob2SQL&quot;,
    &quot;details&quot;: []
}
</code></pre>
","<azure><etl><azure-data-factory>","2020-08-13 14:13:35","1944","1","1","63407104","<ol>
<li><p>Use Get Metadata to get the file size.
<a href=""https://i.stack.imgur.com/B4MI1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B4MI1.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Use if condition to juge if the size is greater than 2. If true then exec copy activity.
<a href=""https://i.stack.imgur.com/sGrwu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sGrwu.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
"
"63396420","Schema compliance in Azure data factory","<p>I am trying to do schema compliance of an input file in ADF. I have tried the below.</p>
<ol>
<li>Get Metadata Activity</li>
<li>The schema validation that is available in source activity</li>
</ol>
<p>But the above seems to only check if a particular field is present or not in the specified position. Also Azure by default takes the datatype of all these fields as string since the input is flat file.</p>
<p>I want to check the position and datatype as well. for eg:-</p>
<p><strong>empid,name,salary</strong><br />
1,abc,10<br />
2,def,20<br />
3,ghi,50<br />
xyz,jkl,10</p>
<p>The row with empid as xyz needs to be rejected as it is not of number data type. Any help is appreciated.</p>
","<azure-data-factory>","2020-08-13 13:36:35","282","0","1","63407042","<p>You can use data flow and create a filter to achieve this.</p>
<p>Below is my test:</p>
<p>1.create a source
<a href=""https://i.stack.imgur.com/rHjIu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rHjIu.png"" alt=""enter image description here"" /></a></p>
<p>2.create a filter and use this expression:<code>regexMatch(empid,'(\\d+)')</code>
<a href=""https://i.stack.imgur.com/gnZOz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gnZOz.png"" alt=""enter image description here"" /></a></p>
<p>3.Output:
<a href=""https://i.stack.imgur.com/GFiYS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GFiYS.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you.</p>
"
"63388546","How can I migrate zipped folders located on-premises FTP server to Azure storage?","<p>I have a zipped folder which have several sub folders and each sub-folders have many files in it. Some files are binary files, some are csv and some are plain text. My requirement is to copy the zip folder, as it is without unzipping it at the on-premise FTP sever, to Microsoft Azure cloud. And this process is not one time but on a regular basis copy whenever a new zip files comes into FTP server.</p>
","<azure><migration><azure-data-factory><azure-data-lake><data-migration>","2020-08-13 05:16:33","236","0","1","63389834","<p>Please ref this tutorial: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp"" rel=""nofollow noreferrer"">Copy data from FTP server by using Azure Data Factory</a></p>
<p>For example, my zipped file has sub folders and each sub-folders have different files:
<a href=""https://i.stack.imgur.com/bgRRC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bgRRC.png"" alt=""enter image description here"" /></a></p>
<p>Using Binary format as Source dataset, choose the Commpression type: ZipDeflate:</p>
<p>Source dataset:</p>
<p><a href=""https://i.stack.imgur.com/2DNDW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2DNDW.png"" alt=""enter image description here"" /></a></p>
<p>Sink dataset:
<a href=""https://i.stack.imgur.com/9KsSS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9KsSS.png"" alt=""enter image description here"" /></a></p>
<p>This can achieve the zipped folder copy.</p>
<p>For you purpose, you want to trigger the pipeline when a new zip file is added to into FTP server. Just For Data Factory, the event trigger doesn't support FTP Server. We can't do that.</p>
<p>I think you could achieve that with a <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview"" rel=""nofollow noreferrer"">Logic app</a> through <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-ftp"" rel=""nofollow noreferrer"">FTP trigger</a>: When a file is added or modified (properties only) trigger.</p>
<p><a href=""https://i.stack.imgur.com/PgTBD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PgTBD.png"" alt=""enter image description here"" /></a></p>
<p>Pass the new add zip file name as Data Factor pipeline parameter, and run the pipeline.</p>
"
"63385916","Azure Data Factory Aggregate count()","<p>i have a query with multiple joins and there is a field that is created from a count, like this:</p>
<pre><code>select
t1.clientID,
t1.clientName,
t2.invoiceNo
(select count (*) from invoices where clientID = t1.clientID) AS clientswithSale
from customers t1
JOIN invoices t2 on t2.clientID = t1.clientID
</code></pre>
<p>how can i create that column using an aggregate transformation inside a data flow with multiple sources?</p>
<p>can i link a source into the count() function?</p>
<p>Thanks</p>
","<azure-data-factory>","2020-08-12 23:20:17","1874","0","2","63389689","<p>I made another test recently and the general process is as follows
Source (Invoices), Source (Customers), Join, Aggregate (count)</p>
<ol>
<li><p>I create two soucres and I will link them each other by the key <code>SalesOrderID</code> later. The <code>source1</code> analogy cutomer table and the <code>source2</code>  analogy invoice table.
<a href=""https://i.stack.imgur.com/qRlTf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qRlTf.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Then I aggregate the source2 , <strong>Group by</strong> <code>SalesOrderID</code> and <strong>Aggregates</strong> with the expression <code>count(1)</code> as follows:
<a href=""https://i.stack.imgur.com/XCITZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XCITZ.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/zuiKS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zuiKS.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>End, I join the two sources with the key <code>SalesOrderID</code> and this works well.
<a href=""https://i.stack.imgur.com/BM3O6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BM3O6.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
"
"63385916","Azure Data Factory Aggregate count()","<p>i have a query with multiple joins and there is a field that is created from a count, like this:</p>
<pre><code>select
t1.clientID,
t1.clientName,
t2.invoiceNo
(select count (*) from invoices where clientID = t1.clientID) AS clientswithSale
from customers t1
JOIN invoices t2 on t2.clientID = t1.clientID
</code></pre>
<p>how can i create that column using an aggregate transformation inside a data flow with multiple sources?</p>
<p>can i link a source into the count() function?</p>
<p>Thanks</p>
","<azure-data-factory>","2020-08-12 23:20:17","1874","0","2","63418636","<p>That SQL statement in ADF Data Flow would equate to:</p>
<p>Source (Invoices), Source (Customers), Inner Join, Aggregate (count)</p>
"
"63382696","HTTP request in Azure Data Factory","<p>In Azure Data Factory, I need to tap into a HTTP requests via URL using the HTTP connector. I was able to do this as well as setup the dataset. Where I'm having issues is on the pipeline. Here's what I need to do. What is the best way to accomplish this?</p>
<ol>
<li>Call out to the service base URL and retrieve the header returned of TotalPages.</li>
<li>Using the value for TotalPages, make subsequent requests to the URL with the parameter page (e.g., page=1, page=2, etc.) using the value from TotalPages to form those requests.</li>
</ol>
<p>Thanks.</p>
","<http-headers><httprequest><azure-data-factory>","2020-08-12 18:28:17","2072","0","2","63392556","<p>Ok. So the issue here is that you cannot nest control structures in Data Factory more than 1 time. The solution is to create two or more pipelines (aka Master and Child).</p>
<p>From the Master pipeline retrieve the number of tasks you will need to execute, and pass them to a for loop. Within the for loop launch for each activity pair a new Child pipeline which will then execute the second activity.</p>
<p>If the Activity is simple enough you can skip the Child Pipeline altogether and do it directly inside the first for loop.</p>
<p>As a Json representation of pipelines in question it should look along these lines:</p>
<pre><code>{
&quot;name&quot;: &quot;generic_master&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Web1&quot;,
            &quot;type&quot;: &quot;WebActivity&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;url&quot;: &quot;https://jsonplaceholder.typicode.com/posts/1&quot;,
                &quot;method&quot;: &quot;GET&quot;
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Web1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Web1').output&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Execute Pipeline1&quot;,
                        &quot;type&quot;: &quot;ExecutePipeline&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;pipeline&quot;: {
                                &quot;referenceName&quot;: &quot;generic_child&quot;,
                                &quot;type&quot;: &quot;PipelineReference&quot;
                            },
                            &quot;waitOnCompletion&quot;: true
                        }
                    }
                ]
            }
        }
    ],
    &quot;annotations&quot;: []
}
</code></pre>
<p>}</p>
<pre><code>{
&quot;name&quot;: &quot;generic_child&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Web1&quot;,
            &quot;type&quot;: &quot;WebActivity&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;url&quot;: &quot;https://jsonplaceholder.typicode.com/posts/1&quot;,
                &quot;method&quot;: &quot;POST&quot;
            }
        }
    ],
    &quot;annotations&quot;: []
}
</code></pre>
<p>}</p>
"
"63382696","HTTP request in Azure Data Factory","<p>In Azure Data Factory, I need to tap into a HTTP requests via URL using the HTTP connector. I was able to do this as well as setup the dataset. Where I'm having issues is on the pipeline. Here's what I need to do. What is the best way to accomplish this?</p>
<ol>
<li>Call out to the service base URL and retrieve the header returned of TotalPages.</li>
<li>Using the value for TotalPages, make subsequent requests to the URL with the parameter page (e.g., page=1, page=2, etc.) using the value from TotalPages to form those requests.</li>
</ol>
<p>Thanks.</p>
","<http-headers><httprequest><azure-data-factory>","2020-08-12 18:28:17","2072","0","2","67618015","<p>In order to read the TotalPages values from the HTTP Request's response, you can use a &quot;Lookup&quot; activity to submit the HTTP request and store the TotalPages value in a variable with the &quot;Set variable&quot; activity.
Actions:
Pipeline level:</p>
<ul>
<li>create a variable called TotalPages
Lookup activity:</li>
<li>tick the first row only box on the Settings tab</li>
<li>As a source dataset, use the data set defined for your HTTP request</li>
<li>Select the GET method.
Set variable activity:</li>
<li>Select the TotalPages variable on the Variables tab</li>
<li>In the value box, click on &quot;Add dynamic content&quot; and enter something like this: @{activity('GetTotalPages').output.firstRow.RegisterSearch['@TotalPages']}</li>
</ul>
<p>In my case, the lookup activity is called GetTotalPages, and my HTTP request returns the total number of pages in a RegisterSearch array, under a column name @TotalPages</p>
"
"63381947","How to use Azure Data Factory to copy files between Sharepoint 365 and OneDrive","<p>I have to build ADF pipelines that move files from Sharepoint document library folders into a single OneDrive which belongs to a 3rd party. I am unable to find good source of information on how to create Sharepoint and OneDrive datasets in ADF.</p>
<p>Any help on how to create the datasets would be appreciated.</p>
<p>Thank you!</p>
","<azure><sharepoint><azure-data-factory>","2020-08-12 17:37:23","1746","2","1","63460929","<p>Please ref this document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Azure Data Factory connector overview</a></p>
<p>They are not supported as the connector. We can not create the dataset.</p>
"
"63379086","Weekday of the month trigger/schedule in Azure Data Factory","<p>I would like to create trigger on 4th weekday of the month in ADF pipeline. I could only come closed to first Monday of the month as shown below. But that is not enough for us.</p>
<p><a href=""https://i.stack.imgur.com/VMy0U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VMy0U.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-12 14:48:58","2121","1","2","63387137","<p>I'm agree with @Joel Cochran.</p>
<p>I think the easiest way is that you could create 5 triggers for this pipeline:</p>
<p><a href=""https://i.stack.imgur.com/Iil4W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iil4W.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Tigger 1: Forth Monday;</li>
<li>Tigger 2: Forth Tuesday;</li>
<li>Tigger 3: Forth Wednesday;</li>
<li>Tigger 4: Forth Thursday;</li>
<li>Tigger 5: Forth Friday;</li>
</ul>
<p>If you want achieve your request feature in one trigger, I would suggest post this new <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">feedback</a> to Data Factory. Data Factory product team may see it and think about add the feature.</p>
"
"63379086","Weekday of the month trigger/schedule in Azure Data Factory","<p>I would like to create trigger on 4th weekday of the month in ADF pipeline. I could only come closed to first Monday of the month as shown below. But that is not enough for us.</p>
<p><a href=""https://i.stack.imgur.com/VMy0U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VMy0U.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-12 14:48:58","2121","1","2","63395117","<p>Thanks to the answers before.
Given below seems optimal. One trigger is enough. It will execute 3 days. And an azure function to check if 'today' is the 4th workday.</p>
<p><a href=""https://i.stack.imgur.com/JpUX5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JpUX5.png"" alt=""enter image description here"" /></a></p>
"
"63376930","Azure Data Factory's Web Hook Activity times out after 1 minute","<p>I have an Azure Data Factory V2 with a Web Hook Activity which is used to invoke an Azure Function with HTTP trigger, code as below.</p>
<pre><code>using namespace System.Net

# Input bindings are passed in via param block.
param($Request, $TriggerMetadata)

# Interact with query parameters or the body of the request.
$callBackUri = $Request.Body.callBackUri
Write-Output &quot;CallBack url is : $callBackUri&quot;

# Need to return Http 202 Accepted here 
# This is the issue, it does not actually return from this point at the moment
Push-OutputBinding -Name Response -Value ([HttpResponseContext]@{
    StatusCode = [HttpStatusCode]::Accepted
    Body = $body
}) -Clobber

# some long running processing
$seconds = 60
Write-Output &quot;Returned http 202. Sleeping for $seconds seconds..&quot;
Start-Sleep -Second $seconds
Write-Output &quot;Sleep complete.&quot;

# Invoke the callback
Invoke-WebRequest -Uri $callBackUri -Method POST -ContentType 'application/json' -Body &quot;This is a callback&quot;
</code></pre>
<p>The Function is supposed to receive the HTTP request, return an HTTP 202 Accepted response immediately and then continue processing. After the process is complete it the function needs to invoke a POST on the callBackUri to indicate to the Web Hook Activity that the processing is complete.</p>
<p>However, the function does not return a 202, instead it completes it's  long running process and then returns a 203. I do understand that initially output binding is set and it is returned only after the entire script is executed.</p>
<p>Is there a way around this? I am simply trying to implement this: <a href=""https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/"" rel=""nofollow noreferrer"">https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/</a></p>
","<azure-data-factory><azure-function-app><powershell-core>","2020-08-12 12:51:36","824","1","1","63388120","<p>I tried,  <em><strong>Start-Job</strong></em>, <em><strong>Start-ThreadJob</strong></em>, <em><strong>Invoke-Command</strong></em> with all variations to attempt to invoke an async REST request without waiting (fire and forget) from the Function App but failed.</p>
<p>This seems logical to me because, if this was allowed people would simply run everything on background threads without waiting for them to complete and that would defeat the purpose of Function app - serverless.</p>
<p>What I've settled with is to use the same <em><strong>Invoke-RestRequest</strong></em> with a 1 second timeout, inside a try catch to suppress the timeout exception. This caused the request to immediately timeout and the Function app would complete.</p>
<p>The code looks like this:</p>
<pre><code>$Body = @{
            callbackUri = $Request.Body.callBackUri;
        } | ConvertTo-Json
try{
    # This API will be responsible for issuing the call back after it has finished the long running process
    $output = Invoke-RestMethod -Method Post -Body $Body -Uri $funcapp2Url -ContentType 'application/json' -TimeoutSec 1
}
catch {
    Write-Output $_
}

# Return HTTP 202 immediately
Push-OutputBinding -Name Response -Value ([HttpResponseContext]@{
    StatusCode = [HttpStatusCode]::Accepted
    Body = &quot;Wait for callback&quot;
}) -Clobber
</code></pre>
"
"63375109","Pipeline Runs - Query By Factory order by RunEnd not working","<p>HI i am using Pipeline Runs - Query By Factory to get latest pipleline RunEnd(datetime).
body of my request looking like following.</p>
<pre><code>{
 &quot;lastUpdatedAfter&quot;:&quot;2020-07-05&quot;,
 &quot;lastUpdatedBefore&quot;:&quot;2020-11-16&quot;,
 &quot;Orderby&quot;:[{&quot;RunEnd&quot;}],
 &quot;filters&quot;:[{&quot;operand&quot;:&quot;PipelineName&quot;,&quot;operator&quot;:&quot;Equals&quot;,&quot;values&quot;:[&quot;abc&quot;]}]
}
</code></pre>
<p>When i added orderby then filter for pipelinName is not working(otherwise filter is working) and RunEnd sorted by default descending order but i need it in ascending order and dont have any syntax available in microsoft artical.</p>
<p>So i need</p>
<ol>
<li>syntax of order by acending and also working filter for piplelineName.</li>
</ol>
","<azure><azure-data-factory>","2020-08-12 10:58:11","336","0","1","63409712","<pre><code>I found the answer. 
{
  &quot;lastUpdatedAfter&quot;:&quot;@{adddays(formatDateTime(utcnow(),'yyyy-MM-dd'),-60)}&quot;,
  &quot;lastUpdatedBefore&quot;:&quot;@{formatDateTime(utcnow(),'yyyy-MM-dd')}&quot;,
  &quot;filters&quot;:[
             {&quot;operand&quot;:&quot;PipelineName&quot;,&quot;operator&quot;:&quot;Equals&quot;,&quot;values&quot;: 
              [&quot;insighthealth_npi_at_move_to_bronze&quot;]
             },
            ],
  &quot;orderBy&quot;:[{&quot;orderBy&quot;:&quot;RunEnd&quot;,&quot;order&quot;:&quot;DESC&quot;}]
 }
</code></pre>
"
"63370834","Webhook Activity runs forever in ADF V2","<p>I want to process my cube through runbook where I have written the code in Power Shell.I want to call this runbook from the webhook activity from ADF.But this activity is running until the timeout and could not complete however its processing the cube successfully by triggering the run book. Do I have to write some more code in order to make it successful or fail in ADF. or am I missing something else in my configuration.</p>
","<powershell><webhooks><azure-data-factory><azure-automation>","2020-08-12 06:16:00","622","0","1","63420122","<p>If you want for ADF to wait until the runbook completes than a Webhook activity (like you are doing) is right. But your runbook needs to take in the callbackUri parameter using the <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-webhooks?#parameters-used-when-the-webhook-starts-a-runbook"" rel=""nofollow noreferrer"">$webhookData parameter in the Azure Automation webhook documentation</a>. And the runbook needs to make a Web call back to the ADF callback API to let it know it’s done.</p>
<p>On the other hand if you don’t want ADF to wait or know whether the runbook failed then use a Web Activity rather than a Webhook activity.</p>
<p>If you proceed with an ADF webhook, then you will need to change your Azure Automation runbook to include the following parameter and parsing:</p>
<pre><code>param(
    [object] $WebhookData
)

$ErrorActionPreference = &quot;Stop&quot;;

try
{
    #parse the webhook parameters
    if (-Not $WebhookData.RequestBody)
    {
        #we're testing in the test pane
        $WebhookData = ConvertFrom-Json -InputObject $WebhookData
    }
    $WebhookBody = ConvertFrom-Json -InputObject $WebhookData.RequestBody
    [string] $callbackUri = $WebhookBody.callbackUri

    #complete your typical runbook work here...

    if ($callbackUri)
    {
        $null = Invoke-WebRequest -Method Post -Uri $callbackUri -ContentType &quot;application/json&quot; -Body '{&quot;StatusCode&quot;: &quot;200&quot;}' -UseBasicParsing
    }

}
catch 
{
    if ($callbackUri)
    {
        $null = Invoke-WebRequest -Method Post -Uri $callbackUri -ContentType &quot;application/json&quot; -Body '{&quot;StatusCode&quot;: &quot;500&quot;, &quot;Error&quot;: {&quot;ErrorCode&quot;:100,&quot;Message&quot;,&quot;$_&quot;}}' -UseBasicParsing
    }
    &quot;Failed: $_&quot;
    throw $_
}
</code></pre>
<p>That should solve your problem with ADF thinking the runbook runs forever.</p>
"
"63368261","ADF Create Pipeline Run - Parameters","<p>I need to trigger a ADF Pipeline via REST API and pass a parameter in order to execute the pipeline for the given ID (parameter).
With sparse documentation around this, I am unable to figure out how to pass parameters to the URL</p>
<p>Sample:</p>
<pre><code>https://management.azure.com/subscriptions/asdc57878-77fg-fb1e8-7b06-7b0698bfb1e8/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/df-datafactory-dev/pipelines/pl_StartProcessing/createRun?api-version=2018-06-01
</code></pre>
<p>I tried to send parmaters in the request body but I get the following message depending on how params are sent</p>
<pre><code>{
    &quot;message&quot;: &quot;The request entity's media type 'text/plain' is not supported for this resource.&quot;
}
</code></pre>
<p>I tried using python requests :</p>
<pre><code>import requests

url = &quot;https://management.azure.com/subscriptions/adsad-asdasd-adasd-adasda-adada/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/datafactory-dev/pipelines/pl_Processing/createRun?api-version=2018-06-01&quot;

payload = &quot; \&quot;parameters\&quot;: {\r\n     “stateID”: “78787878”\r\n}&quot;
headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer adsasdasdsad'
}

response = requests.request(&quot;POST&quot;, url, headers=headers, data = payload)

print(response.text.encode('utf8'))
</code></pre>
<p>I tried to put the parameter in the payload (body)</p>
","<azure-data-factory>","2020-08-12 00:55:11","1362","0","3","63400715","<p>Paramters can be passed within body</p>
<p>python sample:</p>
<pre><code>import requests

url = &quot;https://management.azure.com/subscriptions/adsad-asdasd-adasd-adasda-adada/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/datafactory-dev/pipelines/pl_Processing/createRun?api-version=2018-06-01&quot;

payload = &quot;{\&quot;stateID\&quot;:1200}&quot;
headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer adsasdasdsad'
}

response = requests.request(&quot;POST&quot;, url, headers=headers, data = payload)

print(response.text.encode('utf8'))
</code></pre>
"
"63368261","ADF Create Pipeline Run - Parameters","<p>I need to trigger a ADF Pipeline via REST API and pass a parameter in order to execute the pipeline for the given ID (parameter).
With sparse documentation around this, I am unable to figure out how to pass parameters to the URL</p>
<p>Sample:</p>
<pre><code>https://management.azure.com/subscriptions/asdc57878-77fg-fb1e8-7b06-7b0698bfb1e8/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/df-datafactory-dev/pipelines/pl_StartProcessing/createRun?api-version=2018-06-01
</code></pre>
<p>I tried to send parmaters in the request body but I get the following message depending on how params are sent</p>
<pre><code>{
    &quot;message&quot;: &quot;The request entity's media type 'text/plain' is not supported for this resource.&quot;
}
</code></pre>
<p>I tried using python requests :</p>
<pre><code>import requests

url = &quot;https://management.azure.com/subscriptions/adsad-asdasd-adasd-adasda-adada/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/datafactory-dev/pipelines/pl_Processing/createRun?api-version=2018-06-01&quot;

payload = &quot; \&quot;parameters\&quot;: {\r\n     “stateID”: “78787878”\r\n}&quot;
headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer adsasdasdsad'
}

response = requests.request(&quot;POST&quot;, url, headers=headers, data = payload)

print(response.text.encode('utf8'))
</code></pre>
<p>I tried to put the parameter in the payload (body)</p>
","<azure-data-factory>","2020-08-12 00:55:11","1362","0","3","70622656","<p>You have to use a parameter name as post</p>
<pre><code>url = &quot;https://management.azure.com/subscriptions/adsad-asdasd-adasd-adasda-adada/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/datafactory-dev/pipelines/pl_Processing/createRun?api-version=2018-06-01 -d '{&quot;stateID&quot;=&quot;78787878&quot;}'
</code></pre>
<p>microsoft docs for your reference :
<a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run</a></p>
"
"63368261","ADF Create Pipeline Run - Parameters","<p>I need to trigger a ADF Pipeline via REST API and pass a parameter in order to execute the pipeline for the given ID (parameter).
With sparse documentation around this, I am unable to figure out how to pass parameters to the URL</p>
<p>Sample:</p>
<pre><code>https://management.azure.com/subscriptions/asdc57878-77fg-fb1e8-7b06-7b0698bfb1e8/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/df-datafactory-dev/pipelines/pl_StartProcessing/createRun?api-version=2018-06-01
</code></pre>
<p>I tried to send parmaters in the request body but I get the following message depending on how params are sent</p>
<pre><code>{
    &quot;message&quot;: &quot;The request entity's media type 'text/plain' is not supported for this resource.&quot;
}
</code></pre>
<p>I tried using python requests :</p>
<pre><code>import requests

url = &quot;https://management.azure.com/subscriptions/adsad-asdasd-adasd-adasda-adada/resourceGroups/dev-rg/providers/Microsoft.DataFactory/factories/datafactory-dev/pipelines/pl_Processing/createRun?api-version=2018-06-01&quot;

payload = &quot; \&quot;parameters\&quot;: {\r\n     “stateID”: “78787878”\r\n}&quot;
headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer adsasdasdsad'
}

response = requests.request(&quot;POST&quot;, url, headers=headers, data = payload)

print(response.text.encode('utf8'))
</code></pre>
<p>I tried to put the parameter in the payload (body)</p>
","<azure-data-factory>","2020-08-12 00:55:11","1362","0","3","71161575","<p>You have to pass them as the POST body.<br />
To pass more than one parameter the body this looks like:</p>
<pre><code>{
</code></pre>
<p>&quot;param1&quot;: &quot;param1value&quot;
,&quot;param2&quot;:&quot;param2value&quot;
}</p>
"
"63361366","DataLake Power BI Design Pattern","<p>Our customer has a common problem of many distributed datastores with varying technology stacks. Then end game is to bring certain parts of data together to produce detailed reporting through Microsoft Power BI.</p>
<p>Is there a standard pattern for this approach? My initial thoughts are to</p>
<ol>
<li>Where possible, Azure Data Factory to migrate data into Azure Data Lake.</li>
<li>Where this isn't possible, automate the extract and dump of data into Azure Data Lake.</li>
<li>User Power BI Desktop to connect to the csv datasets to generate a model &amp; reports (whilst performing considerable transformation)</li>
<li>Publish to PowerBI Service to share amongst users</li>
</ol>
<p>Concerns...</p>
<ol>
<li>Should we be using DataFlows within Data Factory (or some other ETL) to do transformation OR continue using the Query Editor in PowerBI</li>
<li>Are there performance issues with connecting many datasets and performing considerable transformation activities within PowerBI Desktop?</li>
<li>In order to update the report with new data, is it just a case of overwriting the previous CSV file in the datalake and refreshing the report?</li>
</ol>
","<azure><powerbi><azure-data-factory><azure-data-lake-gen2>","2020-08-11 15:23:54","43","0","1","63361990","<p>In my experience:</p>
<ol>
<li>Use Query Editor. That's the popular tool. Tons of examples &amp; videos everywhere.</li>
<li>Not sure what &quot;many&quot; is, but you can load a spreadsheet file that's up to 1GB. A single worksheet can be up to 30MB. <a href=""https://learn.microsoft.com/en-us/power-bi/connect-data/reduce-the-size-of-an-excel-workbook"" rel=""nofollow noreferrer"">Click here for more on limits</a></li>
<li>Yes. And you can also setup automatic refreshes so you don't have to click anything.</li>
</ol>
"
"63356531","Bulk Load Multiple Worksheets of an Excel Workbook parallelly into SQL DB using Azure Data Factory V2","<p>Is there a way to Bulk Load worksheets into SQL tables using ADF V2 ?</p>
<p>I was able to load 5 Sheets of data by making 10 Data sets (Excel and SQL Tables) using 2 linked Service (Blob &amp; SQL DB). The performance wasn't good even when all of the data activities were executed in parallel.</p>
<p>If I add more sheets to load, then performance degrades significantly. I assume it's because the same excel workbook is consumed and it takes time to open and read considering it's size (50 MB) and becomes a bottleneck.</p>
<p>SSIS could do it only sequentially using the Foreach loop (depends how dynamically the destination table was set)</p>
<p>There must be a better way to load the data rather than creating N*2 number of Data sets (Worksheet Source &amp; Resp SQL Table).</p>
<p>-Thanks</p>
","<azure><azure-data-factory>","2020-08-11 10:44:06","1447","2","2","63356771","<p>You could do this in two steps:</p>
<ul>
<li>First export the excel spreadsheets to csv files, you could combined these to a single file.</li>
<li>Then use Bulk Insert to load the data into the database</li>
</ul>
<p>See: <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-ver15</a></p>
"
"63356531","Bulk Load Multiple Worksheets of an Excel Workbook parallelly into SQL DB using Azure Data Factory V2","<p>Is there a way to Bulk Load worksheets into SQL tables using ADF V2 ?</p>
<p>I was able to load 5 Sheets of data by making 10 Data sets (Excel and SQL Tables) using 2 linked Service (Blob &amp; SQL DB). The performance wasn't good even when all of the data activities were executed in parallel.</p>
<p>If I add more sheets to load, then performance degrades significantly. I assume it's because the same excel workbook is consumed and it takes time to open and read considering it's size (50 MB) and becomes a bottleneck.</p>
<p>SSIS could do it only sequentially using the Foreach loop (depends how dynamically the destination table was set)</p>
<p>There must be a better way to load the data rather than creating N*2 number of Data sets (Worksheet Source &amp; Resp SQL Table).</p>
<p>-Thanks</p>
","<azure><azure-data-factory>","2020-08-11 10:44:06","1447","2","2","63376220","<p>As @Mark has posted - &quot;You can click &quot;edit&quot; in the dataset &quot;sheets&quot; property and parameterize that property. Then you can use a pipeline &quot;foreach&quot; to loop through sheet names&quot;</p>
"
"63352157","Is there a provision to group a breaking sequence based on its continuity in Azure Mapping dataflow","<p>I wanted to group a breaking sequence based on its continuity using Azure ADF Mapping Dataflow.</p>
<p>Following is the blob file sample:</p>
<p><a href=""https://i.stack.imgur.com/p0HWw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p0HWw.png"" alt=""enter image description here"" /></a></p>
<p>As shown above, there is a primary key column and a column containing breaking sequences, I wanted to create a third column(Expected output) which groups a sequence.</p>
<p>As 1,2,3 is the 1st sequence, it is having value 1, and 9,10,11,12 is the next sequence which has value 2 and so on.</p>
","<azure-data-factory>","2020-08-11 05:44:43","121","2","1","63353621","<p>Use Windows transformation to compare current row with preview row. Then, use a 2nd Window transformation to increment the counter if there is a gap, i.e. current sequence - previous sequence &gt; 1.</p>
<p>If you add a Source called source1 to a new data flow, you can paste this code using the Script button to show the script behind. Add a new-line with Enter at the end and copy/paste:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>source1 derive(dummy = 1) ~&gt; CreateDummyValue
CreateDummyValue window(over(dummy),
asc(year, true),
startRowOffset: -1L,
endRowOffset: 0L,
year2 = first(year)) ~&gt; AddPreviousValue
AddPreviousValue window(over(dummy),
asc(year, true),
column1 = sum(iif(abs(year-year2)&lt;=1,0,1))) ~&gt; CompareAndBucket</code></pre>
</div>
</div>
</p>
<p>I'm using my own column names from movies data, so replace the column names with your data columns. For you, movie should be replaced with Sequences and Expected Output is my &quot;Column1&quot;.</p>
"
"63350497","How to run stored procedure in Azure Data Factory DataFlow SQL Server source","<p>When using an Azure SQL Server source, I use the Query option and specify a stored procedure to run.  When I paste in the same code in Management Studio, it works, but when executed from ADF source using <code>Query</code> option, it errors with the following condition.  How can I call a stored procedure using Query option?</p>
<pre><code>{&quot;message&quot;:&quot;at Source 'Source': com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'EXECUTE'.. Details:at Source 'Source': com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'EXECUTE'.&quot;,&quot;failureType&quot;:&quot;UserError&quot;,&quot;target&quot;:&quot;SyncData&quot;,&quot;errorCode&quot;:&quot;DFExecutorUserError&quot;}
</code></pre>
<p>Here is the query I'm passing that works when called from SSMS:</p>
<pre><code>EXECUTE [dbo].[sp_ReplicaGetChanges] @ReplicaVersion = 0, @FirstTimeFlag = 1, @SourceSchema = 'dbo', @SourceTable = 'Brand', @UpdateColumns = NULL
</code></pre>
<p><a href=""https://i.stack.imgur.com/I1b0S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I1b0S.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-11 02:05:58","2019","0","2","63350724","<p>Some analytics system does not like execute and dbo.
Try to change execute to exec and/or remove dbo. From sp
E.g.   Exec sp_ReplicaGetChanges</p>
"
"63350497","How to run stored procedure in Azure Data Factory DataFlow SQL Server source","<p>When using an Azure SQL Server source, I use the Query option and specify a stored procedure to run.  When I paste in the same code in Management Studio, it works, but when executed from ADF source using <code>Query</code> option, it errors with the following condition.  How can I call a stored procedure using Query option?</p>
<pre><code>{&quot;message&quot;:&quot;at Source 'Source': com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'EXECUTE'.. Details:at Source 'Source': com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'EXECUTE'.&quot;,&quot;failureType&quot;:&quot;UserError&quot;,&quot;target&quot;:&quot;SyncData&quot;,&quot;errorCode&quot;:&quot;DFExecutorUserError&quot;}
</code></pre>
<p>Here is the query I'm passing that works when called from SSMS:</p>
<pre><code>EXECUTE [dbo].[sp_ReplicaGetChanges] @ReplicaVersion = 0, @FirstTimeFlag = 1, @SourceSchema = 'dbo', @SourceTable = 'Brand', @UpdateColumns = NULL
</code></pre>
<p><a href=""https://i.stack.imgur.com/I1b0S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I1b0S.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-11 02:05:58","2019","0","2","63351217","<p>UDFs are supported in Data Flows, but not SPs</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#source-transformation"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#source-transformation</a></p>
"
"63349392","Azure data factory Dataflow Count distinct","<p>i have one dataflow with multiple sources and joins and want to create a new column based in the count distinct of column1 from the stream line vs another source, like comparing customerId in the customers_tables vS customerId in the invoice table. In SQL i do it like this:</p>
<p>(select count(DISTINCT customerID) from invoice) AS CustomersWithNoSales,</p>
<p>is there a way to make this comparative using an aggregate transformation in a dataflow?</p>
<p>thanks.</p>
","<adfs2.0><azure-data-factory>","2020-08-10 23:24:37","1046","0","1","63364766","<p>Have to add the other source and then join with the other stream. After that i was able to use an aggregate transformation with the function countDistinct between the two fields of each sources.</p>
"
"63348946","What are some ways to handle bad files in Azure Datafactory Copy Activity","<p>When using the default datafactory copy activity to load files from json blobs (source using modified timestamp) to sqldb table (destination). For the fault-tolerant settings, if the row is incompatible then it gets skipped, however if there is a bad file that is not a valid json format then the activity errors and retries instead of skipping the bad file.</p>
<p>What are some ways to identify/skip incompatible or corrupt files in ADF copy activity?</p>
<p>Thanks in advance</p>
<p><a href=""https://i.stack.imgur.com/TvwXn.png"" rel=""nofollow noreferrer"">failure log screenshot</a></p>
","<azure><azure-data-factory>","2020-08-10 22:37:30","813","0","1","63355750","<p>According to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#copying-tabular-data"" rel=""nofollow noreferrer"">Fault tolerance of copy activity in Azure Data Factory</a>, the fault tolerance of copy activity only support <strong>binary files</strong> and <strong>tabular data</strong>.<br />
Identify invalid json files is not supported yet.</p>
"
"63347956","In Azure Mapping Dataflows has anyone been able to successfully change the escape character of a Source Dataset?","<p>Has anyone tried this using the Mapping Dataflows?</p>
<p>Example input field is:</p>
<p>&quot;This is a sentence, it contains &quot;&quot;double quotes&quot;&quot; and a comma&quot;</p>
<p>The escape character is a &quot; and the quote character is a &quot;.</p>
<p>When I use a regular Copy activity this works without a problem, however
when using the same Dataset in a Mapping Dataflow it gets parsed into 2 fields instead of one. In fact changing the escape character makes no difference.</p>
<p><a href=""https://i.stack.imgur.com/qJyer.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qJyer.png"" alt=""Dataflow Dataset parameters"" /></a></p>
","<azure><csv><escaping><azure-data-factory><quotes>","2020-08-10 21:00:55","43","0","1","63348269","<p>Closing this issue. I've realised that output still has the \ as the escape character so when opening the output file in Excel it appears corrupted</p>
"
"63346813","Ping routine in MS PowerAutomate","<p>folks, I am trying to find a way to ping a list of IP addresses one at a time and capture either the &quot;from&quot; IP address, which = success or &quot;Request Timeout&quot; = Failure</p>
<p>Ideally being able to capture this information into a JSON file would be fantastic.</p>
<p>I'm using PowerApps and Data Factories to orchestrate the PINGing if Possible and then to consume the JSON results into a DB. the job would run once a day.</p>
<p>Looking for ideas or examples. Thank you!</p>
","<json><ip><azure-data-factory><ping><power-automate>","2020-08-10 19:31:16","1765","0","2","63370519","<p>For your requirement, I think PowerAutomate is not a good service for us to implement it. You can do it in <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/"" rel=""nofollow noreferrer"">azure function</a> by code.</p>
<p>You can <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-scheduled-function"" rel=""nofollow noreferrer"">create a timer trigger azure function</a> with the cron expression <code>0 0 0 * * *</code>, the function will triggered to run every day at 00:00:00. Then you can write code in it to ping addresses, please refer to this <a href=""https://stackoverflow.com/questions/57600698/ping-server-from-azure-function/57601677#57601677"">post</a> about how to do ping in azure function code.</p>
"
"63346813","Ping routine in MS PowerAutomate","<p>folks, I am trying to find a way to ping a list of IP addresses one at a time and capture either the &quot;from&quot; IP address, which = success or &quot;Request Timeout&quot; = Failure</p>
<p>Ideally being able to capture this information into a JSON file would be fantastic.</p>
<p>I'm using PowerApps and Data Factories to orchestrate the PINGing if Possible and then to consume the JSON results into a DB. the job would run once a day.</p>
<p>Looking for ideas or examples. Thank you!</p>
","<json><ip><azure-data-factory><ping><power-automate>","2020-08-10 19:31:16","1765","0","2","64698975","<p>If you have the premium HTTP, you could use a HTTP Head to check websites. I also use a local Powershell Ping, and then import it using FTP.</p>
"
"63345680","Azure DataFactory Run Activity in response to any one of several activities failing","<p>Suppose I do a &quot;setup&quot; task part way through my pipeline, and then a matching cleanup task at the end.
If something goes wrong in-between I want to do a different cleanup task.</p>
<p>How do I configure that?</p>
<p>I can't just declare an on-failure dependency on the final task, since failures earlier in the chain mean that latter activities aren't run, and thus neither succeed NOR fail.</p>
<p>I can't see anyway to configure an &quot;OR&quot; dependency, like I would have back in the days of Sql Server Maintenance tasks :)</p>
<p>At the moment it seems like my best option is to copy my failure activity 5 times and each on to the failure of EACH intermediary step?</p>
","<azure-data-factory>","2020-08-10 18:06:17","413","0","1","63345795","<p>The solution is the &quot;Skipped&quot; state.</p>
<p>Declare the 'failure cleanup' as happening if the setup Succeeded, and the 'success cleanup' was Skipped.</p>
<p>From MSDN on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-dependency"" rel=""nofollow noreferrer"">Activity Dependency States</a>:</p>
<blockquote>
<p><strong>Activity dependency</strong></p>
<p>Activity Dependency defines how subsequent activities depend on previous activities, determining the condition of whether to continue executing the next task. An activity can depend on one or multiple previous activities with different dependency conditions.</p>
<p>The different dependency conditions are: Succeeded, Failed, Skipped, Completed.</p>
<p>For example, if a pipeline has Activity A -&gt; Activity B, the different scenarios that can happen are:</p>
<ul>
<li>Activity B has dependency condition on Activity A with <em>succeeded</em>:
<ul>
<li>Activity B only runs if Activity A has a final status of <em>succeeded</em>.</li>
</ul>
</li>
<li>Activity B has dependency condition on Activity A with <em>failed</em>:
<ul>
<li>Activity B only runs if Activity A has a final status of <em>failed</em>.</li>
</ul>
</li>
<li>Activity B has dependency condition on Activity A with <em>completed</em>:
<ul>
<li>Activity B runs if Activity A has a final status of <em>succeeded</em> or <em>failed</em></li>
</ul>
</li>
<li>Activity B has a dependency condition on Activity A with <em>skipped</em>:
<ul>
<li>Activity B runs if Activity A has a final status of <em>skipped</em>.</li>
<li><em>Skipped</em> occurs in the scenario of Activity X -&gt; Activity Y -&gt; Activity Z, where each activity runs only if the previous activity <em>succeeds</em>. If Activity X <em>fails</em>, then Activity Y has a status of <em>Skipped</em> because it never executes. Similarly, Activity Z has a status of <em>Skipped</em> as well.</li>
</ul>
</li>
</ul>
</blockquote>
"
"63343084","Is there a way to ""wait"" for ""Azure Data Factory"" Execution task to complete before executing next steps of Azure Logic Apps","<p>Trying to Load some Excel data using ADF pipeline via Logic Apps. However when triggering through Logic Apps, the task triggers and then moves to the next step immediately. Looking for a solution where the next step waits for a &quot;Execute Data factory Pipeline&quot; to execute completely before proceeding.</p>
<p>Adding an image for clarity.</p>
<p>-Thanks</p>
<p><a href=""https://i.stack.imgur.com/oJbna.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oJbna.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-logic-apps><azure-data-factory>","2020-08-10 15:20:21","1920","2","1","63350563","<p>For this requirement, I provide a sample of my logic app below for your reference:</p>
<p><strong>1.</strong> Add a &quot;Create a pipeline run&quot; action and initialize a variable named <code>status</code>(set its value as &quot;<strong>InProgerss</strong>&quot;).</p>
<p><a href=""https://i.stack.imgur.com/mSOiM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mSOiM.png"" alt=""enter image description here"" /></a></p>
<p><strong>2.</strong> Then add a &quot;<a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-control-flow-loops#until-loop"" rel=""noreferrer"">Until</a>&quot; action, set the break condition as <code>status</code> is equal to &quot;<strong>Succeeded</strong>&quot;. Add a &quot;<strong>Get a pipeline run</strong>&quot; action and set the variable <code>status</code> as the value of <code>Status</code> comes from &quot;<strong>Get a pipeline run</strong>&quot; in the &quot;<strong>Until</strong>&quot; action. Shown as below screenshot:</p>
<p><a href=""https://i.stack.imgur.com/uWT32.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uWT32.png"" alt=""enter image description here"" /></a></p>
<p><strong>3.</strong> After that, run your logic app. The steps will run after the &quot;Until&quot; action(also after your pipeline complete).</p>
<p><strong>By the way:</strong></p>
<p>You can also do it in Data Factory, you can delete the data after completion. Please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#mapping-data-flow-properties"" rel=""noreferrer"">document</a>.</p>
<p><a href=""https://i.stack.imgur.com/HywAK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/HywAK.png"" alt=""enter image description here"" /></a></p>
"
"63341530","How to debug DataFlow exception","<p>I am receiving the following error condition in an Azure Data Factory data flow that looks like the picture below.</p>
<p><a href=""https://i.stack.imgur.com/Qj1xC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qj1xC.png"" alt=""enter image description here"" /></a></p>
<pre><code>{&quot;message&quot;:&quot;DSL stream  has parsing errors\nLine 11 Position 207: token recognition error at: '\\n'. Details:DSL stream  has parsing errors\nLine 11 Position 207: token recognition error at: '\\n'&quot;,&quot;failureType&quot;:&quot;UserError&quot;,&quot;target&quot;:&quot;df_ReplicaSync&quot;,&quot;errorCode&quot;:&quot;DFExecutorUserError&quot;}
</code></pre>
<p>How does one debug this?  It has no reference to any activity, operation, or parameter?  I tried looking at the line in the JSON code for each activity in the data flow but no help.</p>
<p>UPDATE:</p>
<p>There was a line feed in the expression builder window where you build the T-SQL.  It will not complain about it but it will add a new line character that will cause this issue at runtime.</p>
<p><a href=""https://i.stack.imgur.com/BdP6Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BdP6Q.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/97IGM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/97IGM.png"" alt=""enter image description here"" /></a></p>
<p>When viewing the script as Mark suggested</p>
<p><a href=""https://i.stack.imgur.com/VqgH6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VqgH6.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-10 13:50:35","2236","1","1","63345581","<p>DSL is referring to the data flow script (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-script"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-script</a>) that is behind the map. You can see it by clicking on the Script button on the top right. If you click on it for your data flow, you can find the location of the error via Line 11, Pos 207.</p>
"
"63337778","How to wait for Azure Data Factory pipeline to complete in Azure DevOps?","<p>In my Azure DevOps release, I need to trigger an Azure Data Factory pipeline and wait for the process to finish.</p>
<p>Is there any way to do this without any special trick in Az DevOps? Currently using <a href=""https://github.com/liprec/vsts-publish-adf"" rel=""nofollow noreferrer"">vsts-publish-adf</a> in my release.</p>
<p>Thanks</p>
","<azure><azure-devops><azure-data-factory>","2020-08-10 09:44:03","1997","1","2","63340655","<p>It is feasible, though I am unable to evaluate whether it is a good idea in your situation. Here's the practical answer however:</p>
<p>You could trigger and follow the pipeline run with a <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops"" rel=""nofollow noreferrer"">Azure CLI Task</a> that runs in your <strong>Release stage</strong>. Azure CLI has <a href=""https://learn.microsoft.com/en-us/cli/azure/ext/datafactory/datafactory?view=azure-cli-latest"" rel=""nofollow noreferrer"">Data Factory-specific commands</a> which begin with <code>az datafactory</code>, so you can use them in both cases.</p>
<ul>
<li>starting the run with <code>az datafactory pipeline-run</code></li>
<li>waiting for its completion in a loop, running <code>az datafactory pipeline-run show</code> e.g. once a minute</li>
</ul>
<p>Another solution could be using a REST API, such as in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api#monitor-pipeline"" rel=""nofollow noreferrer"">this example of monitoring the pipeline run</a></p>
"
"63337778","How to wait for Azure Data Factory pipeline to complete in Azure DevOps?","<p>In my Azure DevOps release, I need to trigger an Azure Data Factory pipeline and wait for the process to finish.</p>
<p>Is there any way to do this without any special trick in Az DevOps? Currently using <a href=""https://github.com/liprec/vsts-publish-adf"" rel=""nofollow noreferrer"">vsts-publish-adf</a> in my release.</p>
<p>Thanks</p>
","<azure><azure-devops><azure-data-factory>","2020-08-10 09:44:03","1997","1","2","63355573","<blockquote>
<p>Is there any way to do this without any special trick in Az DevOps?</p>
</blockquote>
<p>The direct answer is <strong>No</strong> cause the third-party task itself doesn't support this scenario <strong>by design</strong>.</p>
<p>According to <a href=""https://github.com/liprec/vsts-publish-adf/issues/38"" rel=""nofollow noreferrer"">comment from the Author <strong>liprec</strong></a>: At this moment the task only triggers a pipeline run and is not waiting for that run to complete. He has plans to add such a task to wait and poll the task run. So what you want could be possible in coming days, but <strong>for now it's not supported</strong>.</p>
<p>You have to use something like Powershell scripts to trigger ADF pipeline run via command-line like <strong>Mekki</strong> suggests above. Here's another similar <a href=""https://stackoverflow.com/questions/58077439/how-to-monitor-adf-pipeline-from-ci-cd-pipeline"">PS example</a>.</p>
"
"63337300","Azure ADF GetMetadata childItems if folder might not exist","<p>I have a path to DataLakeStorage, which may or may not exist.</p>
<p>I want to iterate over the contents of that folder, if it exists.</p>
<p>In C# I would arrange to have a <code>children</code> collection, that was empty if the folder didn't exist, and then iterate over that (possibly empty) collection.</p>
<p><strong>Can I do the same in ADF (v2)?</strong></p>
<p>If I do a <code>Get Metadata</code> activity returning both <code>exists</code> and <code>childItems</code>, then it <em>nearly</em> works:</p>
<ul>
<li>It works if the folder does exist</li>
<li>It doesn't error if the folder does NOT exist.</li>
<li>But the <code>childItems</code> property is not defined if the folder doesn't exist, so I don't get an empty array to iterate over.</li>
</ul>
<p>The first solution that comes to mind is to try to build <a href=""https://stackoverflow.com/questions/63337199/azure-adf-expression-that-returns-either-an-existing-array-or-an-empty-array-b"">Azure ADF expression that returns either an existing array, or an empty array, based on a bool</a>, which I've asked as a direct question. But if there's a nicer / more idiomatic approach, then I'm open to that too :)</p>
","<azure-data-factory>","2020-08-10 09:12:46","4097","1","1","63350576","<p>Please try something like this:</p>
<p>1.create a variable,type is array,value is empty,like this:
<a href=""https://i.stack.imgur.com/5nl4S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5nl4S.png"" alt=""enter image description here"" /></a></p>
<p>2.create a For Each activity which depends on your Get Metadata success.</p>
<p>Expression:
<code>@if( contains(activity('Get Metadata1').output,'childitems'), activity('Get Metadata1').output.childitems, variables('emptyArr'))</code></p>
<p>or
<code>@if( activity('Get Metadata1').output.exists, activity('Get Metadata1').output.childitems, variables('emptyArr'))</code></p>
<p><a href=""https://i.stack.imgur.com/xBgv6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xBgv6.png"" alt=""enter image description here"" /></a></p>
<p>Below is my test:</p>
<p><strong>Scenario one:path exists</strong>
<a href=""https://i.stack.imgur.com/LzEFW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LzEFW.png"" alt=""enter image description here"" /></a></p>
<p><strong>Scenario two:path not exists</strong>
<a href=""https://i.stack.imgur.com/xIe4I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xIe4I.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you:)</p>
"
"63337199","Azure ADF expression that returns either an existing array, or an empty array, based on a bool","<p>I have a boolean expression: <code>equals(myStringValue, targetStringValue)</code></p>
<p>I have an array expression which might or might not be valid, depending on the boolean condition <code>myArrayExpression</code>.</p>
<p>I want to write:</p>
<pre><code>if(
  equals(myStringValue, targetStringValue),
  myArrayExpression,
  ?????
)
</code></pre>
<p>where <code>?????</code> is an expression that returns an empty array.</p>
<hr>
<p>Naturally, this is an XY-problem.
I definitely want to know how to do this directly, because understanding how this language works well is important to me. But if you want to know about the XY problem, it's over here: <a href=""https://stackoverflow.com/questions/63337300/azure-adf-getmetadata-childitems-if-folder-might-not-exist"">Azure ADF GetMetadata childItems if folder might not exist</a></p>
","<logic><azure-data-factory>","2020-08-10 09:05:58","973","1","2","63337710","<p>Defining an array variable, with no default value, and then referencing that does work.</p>
<p>But seems very sad - now we've got an extra variable floating around for no reason :(</p>
"
"63337199","Azure ADF expression that returns either an existing array, or an empty array, based on a bool","<p>I have a boolean expression: <code>equals(myStringValue, targetStringValue)</code></p>
<p>I have an array expression which might or might not be valid, depending on the boolean condition <code>myArrayExpression</code>.</p>
<p>I want to write:</p>
<pre><code>if(
  equals(myStringValue, targetStringValue),
  myArrayExpression,
  ?????
)
</code></pre>
<p>where <code>?????</code> is an expression that returns an empty array.</p>
<hr>
<p>Naturally, this is an XY-problem.
I definitely want to know how to do this directly, because understanding how this language works well is important to me. But if you want to know about the XY problem, it's over here: <a href=""https://stackoverflow.com/questions/63337300/azure-adf-getmetadata-childitems-if-folder-might-not-exist"">Azure ADF GetMetadata childItems if folder might not exist</a></p>
","<logic><azure-data-factory>","2020-08-10 09:05:58","973","1","2","73474719","<p>You can use if (x, Y, skip(createArray(''), 1))</p>
"
"63334777","azure data factory - timeout sink side","<p>I try to transform big tables to azure SQL server.
while the small one are completed, the big ones aren't, and fall on timeout sink side.
the errors are attached.
while the sql server doesn’t has any timeout specified, it still wont work.</p>
<p>the sql db is 800 DTU.</p>
<p>how do i increase the timeout at sink side, if that is the problem.</p>
<p>isn't the data factory supposed to save the connection and retry if failed?</p>
<pre class=""lang-js prettyprint-override""><code>errors:
{
    &quot;dataRead&quot;: 1372864152,
    &quot;dataWritten&quot;: 1372864152,
    &quot;sourcePeakConnections&quot;: 1,
    &quot;sinkPeakConnections&quot;: 2,
    &quot;rowsRead&quot;: 2205634,
    &quot;rowsCopied&quot;: 2205634,
    &quot;copyDuration&quot;: 8010,
    &quot;throughput&quot;: 167.377,
    &quot;errors&quot;: [
        {
            &quot;Code&quot;: 11000,
            &quot;Message&quot;: &quot;Failure happened on 'Sink' side. 'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Timeouts in SQL write operation.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=Execution Timeout Expired.  The timeout period elapsed prior to completion of the operation or the server is not responding.,Source=.Net SqlClient Data Provider,SqlErrorNumber=-2,Class=11,ErrorCode=-2146232060,State=0,Errors=[{Class=11,Number=-2,State=0,Message=Execution Timeout Expired.  The timeout period elapsed prior to completion of the operation or the server is not responding.,},],''Type=System.ComponentModel.Win32Exception,Message=The wait operation timed out,Source=,'&quot;,
            &quot;EventType&quot;: 0,
            &quot;Category&quot;: 5,
            &quot;Data&quot;: {
                &quot;FailureInitiator&quot;: &quot;Sink&quot;
            },
            &quot;MsgId&quot;: null,
            &quot;ExceptionType&quot;: null,
            &quot;Source&quot;: null,
            &quot;StackTrace&quot;: null,
            &quot;InnerEventInfos&quot;: []
        }
    ],
    &quot;effectiveIntegrationRuntime&quot;: &quot;XXX&quot;,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;DataMovement&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;SelfhostedIR&quot;,
                &quot;duration&quot;: 2.0166666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    },
    &quot;usedParallelCopies&quot;: 1,
    &quot;executionDetails&quot;: [
        {
            &quot;source&quot;: {
                &quot;type&quot;: &quot;SqlServer&quot;
            },
            &quot;sink&quot;: {
                &quot;type&quot;: &quot;SqlServer&quot;
            },
            &quot;status&quot;: &quot;Failed&quot;,
            &quot;start&quot;: &quot;2020-08-03T17:16:58.8388528Z&quot;,
            &quot;duration&quot;: 8010,
            &quot;usedParallelCopies&quot;: 1,
            &quot;profile&quot;: {
                &quot;queue&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 810
                },
                &quot;preCopyScript&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 0
                },
                &quot;transfer&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 7200,
                    &quot;details&quot;: {
                        &quot;readingFromSource&quot;: {
                            &quot;type&quot;: &quot;SqlServer&quot;,
                            &quot;workingDuration&quot;: 7156,
                            &quot;timeToFirstByte&quot;: 0
                        },
                        &quot;writingToSink&quot;: {
                            &quot;type&quot;: &quot;SqlServer&quot;
                        }
                    }
                }
            },
            &quot;detailedDurations&quot;: {
                &quot;queuingDuration&quot;: 810,
                &quot;preCopyScriptDuration&quot;: 0,
                &quot;timeToFirstByte&quot;: 0,
                &quot;transferDuration&quot;: 7200
            }
        }
    ],
    &quot;dataConsistencyVerification&quot;: {
        &quot;VerificationResult&quot;: &quot;NotVerified&quot;
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 810
    }
}
</code></pre>
","<azure><timeout><azure-data-factory>","2020-08-10 05:50:04","5124","2","2","63369563","<p>Please try to set the Write batch timeout in sink side:</p>
<ol>
<li>The wait time for the batch insert operation to finish before it
times out. The allowed value is <strong>timespan</strong>. An example is “00:30:00”
(30 minutes).</li>
</ol>
<p><a href=""https://i.stack.imgur.com/72p8Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/72p8Y.png"" alt=""enter image description here"" /></a></p>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#azure-sql-database-as-the-sink"" rel=""nofollow noreferrer"">Azure SQL Database as the sink</a></p>
"
"63334777","azure data factory - timeout sink side","<p>I try to transform big tables to azure SQL server.
while the small one are completed, the big ones aren't, and fall on timeout sink side.
the errors are attached.
while the sql server doesn’t has any timeout specified, it still wont work.</p>
<p>the sql db is 800 DTU.</p>
<p>how do i increase the timeout at sink side, if that is the problem.</p>
<p>isn't the data factory supposed to save the connection and retry if failed?</p>
<pre class=""lang-js prettyprint-override""><code>errors:
{
    &quot;dataRead&quot;: 1372864152,
    &quot;dataWritten&quot;: 1372864152,
    &quot;sourcePeakConnections&quot;: 1,
    &quot;sinkPeakConnections&quot;: 2,
    &quot;rowsRead&quot;: 2205634,
    &quot;rowsCopied&quot;: 2205634,
    &quot;copyDuration&quot;: 8010,
    &quot;throughput&quot;: 167.377,
    &quot;errors&quot;: [
        {
            &quot;Code&quot;: 11000,
            &quot;Message&quot;: &quot;Failure happened on 'Sink' side. 'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Timeouts in SQL write operation.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=Execution Timeout Expired.  The timeout period elapsed prior to completion of the operation or the server is not responding.,Source=.Net SqlClient Data Provider,SqlErrorNumber=-2,Class=11,ErrorCode=-2146232060,State=0,Errors=[{Class=11,Number=-2,State=0,Message=Execution Timeout Expired.  The timeout period elapsed prior to completion of the operation or the server is not responding.,},],''Type=System.ComponentModel.Win32Exception,Message=The wait operation timed out,Source=,'&quot;,
            &quot;EventType&quot;: 0,
            &quot;Category&quot;: 5,
            &quot;Data&quot;: {
                &quot;FailureInitiator&quot;: &quot;Sink&quot;
            },
            &quot;MsgId&quot;: null,
            &quot;ExceptionType&quot;: null,
            &quot;Source&quot;: null,
            &quot;StackTrace&quot;: null,
            &quot;InnerEventInfos&quot;: []
        }
    ],
    &quot;effectiveIntegrationRuntime&quot;: &quot;XXX&quot;,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;DataMovement&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;SelfhostedIR&quot;,
                &quot;duration&quot;: 2.0166666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    },
    &quot;usedParallelCopies&quot;: 1,
    &quot;executionDetails&quot;: [
        {
            &quot;source&quot;: {
                &quot;type&quot;: &quot;SqlServer&quot;
            },
            &quot;sink&quot;: {
                &quot;type&quot;: &quot;SqlServer&quot;
            },
            &quot;status&quot;: &quot;Failed&quot;,
            &quot;start&quot;: &quot;2020-08-03T17:16:58.8388528Z&quot;,
            &quot;duration&quot;: 8010,
            &quot;usedParallelCopies&quot;: 1,
            &quot;profile&quot;: {
                &quot;queue&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 810
                },
                &quot;preCopyScript&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 0
                },
                &quot;transfer&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 7200,
                    &quot;details&quot;: {
                        &quot;readingFromSource&quot;: {
                            &quot;type&quot;: &quot;SqlServer&quot;,
                            &quot;workingDuration&quot;: 7156,
                            &quot;timeToFirstByte&quot;: 0
                        },
                        &quot;writingToSink&quot;: {
                            &quot;type&quot;: &quot;SqlServer&quot;
                        }
                    }
                }
            },
            &quot;detailedDurations&quot;: {
                &quot;queuingDuration&quot;: 810,
                &quot;preCopyScriptDuration&quot;: 0,
                &quot;timeToFirstByte&quot;: 0,
                &quot;transferDuration&quot;: 7200
            }
        }
    ],
    &quot;dataConsistencyVerification&quot;: {
        &quot;VerificationResult&quot;: &quot;NotVerified&quot;
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 810
    }
}
</code></pre>
","<azure><timeout><azure-data-factory>","2020-08-10 05:50:04","5124","2","2","68437314","<p>Would like to supplement my case: same error as OP but the exception is raised during executing dataflow.</p>
<p>Following the accepted answer's direction, I set the <strong>Batch size</strong> to some limit in sink of my dataflow will help to solve my issue.</p>
"
"63324387","how to fill down values using Azure Data Factory","<p>sorry for the basic question, I am coming from PowerQuery background, and started using ADF for a new Project. first I started wrangling data flows and fill down values is not supported, Now I am trying with mapping data flow and I can't find in the documentation how to fill down a value ?</p>
<p>see example I have the ID column and looking to add FILL_ID</p>
<p><a href=""https://i.stack.imgur.com/Bbiza.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bbiza.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-09 08:57:45","1226","1","2","63335112","<p>You can use DerivedColumn.</p>
<p>1.add a column or select a column exist in your source.
<a href=""https://i.stack.imgur.com/7f6Qv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7f6Qv.png"" alt=""enter image description here"" /></a></p>
<p>2.enter an expression,if value of your column is null(you can check this by using Data preview),you can use iifNull function.About expression in dataflow,you can refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions"" rel=""nofollow noreferrer"">this</a>.
<a href=""https://i.stack.imgur.com/Hd8Y1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hd8Y1.png"" alt=""enter image description here"" /></a></p>
"
"63324387","how to fill down values using Azure Data Factory","<p>sorry for the basic question, I am coming from PowerQuery background, and started using ADF for a new Project. first I started wrangling data flows and fill down values is not supported, Now I am trying with mapping data flow and I can't find in the documentation how to fill down a value ?</p>
<p>see example I have the ID column and looking to add FILL_ID</p>
<p><a href=""https://i.stack.imgur.com/Bbiza.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bbiza.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-09 08:57:45","1226","1","2","63335265","<p>This data flow script snippet will do the trick:</p>
<p>source1 derive(dummy = 1) ~&gt; DerivedColumn1
DerivedColumn1 window(over(dummy),
asc(movie, true),
startRowOffset: -1L,
endRowOffset: 0L,
Rating2 = first(coalesce(Rating))) ~&gt; Window1
Window1 derive(Rating = iif(isNull(Rating),Rating2,Rating)) ~&gt; DerivedColumn2</p>
<ol>
<li>Create a new data flow</li>
<li>Add a Source transformation that points to your text file</li>
<li>Click on the script behind button on top right of browser UI</li>
<li>Hit Enter to create newline at the bottom the script</li>
<li>Paste the above snippet and click OK</li>
</ol>
<p>You should now see a Derived Column, Window, and another Derived. Go into the Window and 2nd Derived Column to change my column names to yours for sort and the coalesce function. THen in the 2nd Derived Column, pick the names of your columns.</p>
<ul>
<li><p>The first derived creates a dummy var that you'll need because your use case is to pick the previous non-null value across the entire dataset.</p>
</li>
<li><p>The Window sorts the data because your use case requires it and the window column creates a new column that uses coalesce() to find first non-null.</p>
</li>
<li><p>The 2nd Derived Column swaps in the previous value is the current is NULL.</p>
</li>
</ul>
"
"63316284","How to create iteration scoped variables inside ForEach activities in Azure Data Factory","<p>I have a <code>ForEach</code> activity where inside each iteration, I need to set a few iteration specific variables.  I can achieve this by using variables defined for the pipeline (pipeline scope), but this forces me to run the loop in <code>Sequential</code> mode so that multiple iterations running in parallel will not update the same variable.  What I really need is the ability to define these variables within each iteration (iteration scope) so that I can run the <code>ForEach</code> activity in parallel mode.</p>
<p><a href=""https://i.stack.imgur.com/K40sL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/K40sL.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/civrj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/civrj.png"" alt=""enter image description here"" /></a></p>
<p>I've considered creating a SQL dataset where I could do a lookup for fake values (SELECT 1 AS var1, 2 AS var2) just to get a structure where I can set and use those values, but that seems really lame.  I've also considered using an array variable type with the AppendVariable option, but that introduces a lot of custom parsing.</p>
<p>Would be nice if I could just have an InMemory dataset that doesn't have to be tied to a data source where I could use it as a structure inside my ForEach iteration.  Does anyone have any other ideas about how to set iteration specific variables inside <code>ForEach</code> loop?</p>
","<azure-data-factory>","2020-08-08 14:04:49","5529","6","3","63384081","<p>About the best way to currently do this it to pull the values from an outer lookup or get metadata activity if you can. using the inner lookup wouldn't be as cost efficient or performance efficient. especially if you were iterating over 100's or thousands. Of course this is if you can determine the values for each iteration ahead of time. if you can't than. I would drably go for your lookup approach. or if you can get away from the variables entirely just set the values using an expression using dynamic properties.</p>
"
"63316284","How to create iteration scoped variables inside ForEach activities in Azure Data Factory","<p>I have a <code>ForEach</code> activity where inside each iteration, I need to set a few iteration specific variables.  I can achieve this by using variables defined for the pipeline (pipeline scope), but this forces me to run the loop in <code>Sequential</code> mode so that multiple iterations running in parallel will not update the same variable.  What I really need is the ability to define these variables within each iteration (iteration scope) so that I can run the <code>ForEach</code> activity in parallel mode.</p>
<p><a href=""https://i.stack.imgur.com/K40sL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/K40sL.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/civrj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/civrj.png"" alt=""enter image description here"" /></a></p>
<p>I've considered creating a SQL dataset where I could do a lookup for fake values (SELECT 1 AS var1, 2 AS var2) just to get a structure where I can set and use those values, but that seems really lame.  I've also considered using an array variable type with the AppendVariable option, but that introduces a lot of custom parsing.</p>
<p>Would be nice if I could just have an InMemory dataset that doesn't have to be tied to a data source where I could use it as a structure inside my ForEach iteration.  Does anyone have any other ideas about how to set iteration specific variables inside <code>ForEach</code> loop?</p>
","<azure-data-factory>","2020-08-08 14:04:49","5529","6","3","66605019","<p>I agree, this is very annoying and irritating.</p>
<p>If the first part of Jason's answer is vaiable for your situation, then that's definitely the way to go. (Define the variables outside the loop).</p>
<p>But assuming that the variables are dynamically calculated per-iteration, then the only solution that I know is to define the body of the Foreach loop as its own pipeline. Now you can define variable inside that inner pipeline, which are &quot;scoped&quot; to the separate executions of the inner-pipeline.</p>
<p>Quite a lot of ADF's pipeline limitations can be circumvented like this. Nested Ifs/Foreaches, Activity limits, etc.</p>
"
"63316284","How to create iteration scoped variables inside ForEach activities in Azure Data Factory","<p>I have a <code>ForEach</code> activity where inside each iteration, I need to set a few iteration specific variables.  I can achieve this by using variables defined for the pipeline (pipeline scope), but this forces me to run the loop in <code>Sequential</code> mode so that multiple iterations running in parallel will not update the same variable.  What I really need is the ability to define these variables within each iteration (iteration scope) so that I can run the <code>ForEach</code> activity in parallel mode.</p>
<p><a href=""https://i.stack.imgur.com/K40sL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/K40sL.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/civrj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/civrj.png"" alt=""enter image description here"" /></a></p>
<p>I've considered creating a SQL dataset where I could do a lookup for fake values (SELECT 1 AS var1, 2 AS var2) just to get a structure where I can set and use those values, but that seems really lame.  I've also considered using an array variable type with the AppendVariable option, but that introduces a lot of custom parsing.</p>
<p>Would be nice if I could just have an InMemory dataset that doesn't have to be tied to a data source where I could use it as a structure inside my ForEach iteration.  Does anyone have any other ideas about how to set iteration specific variables inside <code>ForEach</code> loop?</p>
","<azure-data-factory>","2020-08-08 14:04:49","5529","6","3","72652659","<p>Another workaround that worked for me was to use a Filter Activity. It's not super pretty but can help.</p>
<p>Say you want to assign <code>expr</code> to a variable. Just add a filter activity and configure it like this:</p>
<p><strong>Items:</strong> <code>@array(expr)</code></p>
<p><strong>Condition:</strong> <code>@equals(1, 1)</code></p>
<p>Then instead of using a variable simply use the Filter Activity output like this:</p>
<p><code>@first(activity('&lt;your filter activity name&gt;').output.Value)</code></p>
"
"63305664","How to read always encrypted data from Azure data factory","<p>I want to read always encrypted data stored in Azure SQL database using key vault and export to another Azure SQL database by decrypting created in the same server.
Is it possible in Azure data factory? If so what are the steps. What are the alternatives ?</p>
<p>Also I have read about using self-hosted IR on VM for this purpose. Is it possible to use Azure IR service in ADF Integration runtime setup? Since the resources are built on serverless architecture.</p>
<p>(PAAS) Azure SQL database A(Encrypted)  -&gt; (PAAS) Azure SQL database B(Decrypt)</p>
","<azure-sql-database><azure-data-factory>","2020-08-07 16:20:29","1566","-1","1","63310255","<p>You can use ADF to export Always Encrypted data from Azure SQL DB, but it requires a self-hosted integration runtime, and you have to use the ODBC linked service instead of the SQL Database linked service. You can use SQL auth or the ADF managed identity to connect to your database. You have to grant Key Vault access to ADF.</p>
<p>For more info, see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#using-always-encrypted"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#using-always-encrypted</a>.</p>
<p>You could also do this in SSIS (which could be run in an SSIS IR in ADF), but it is <a href=""http://thewindowsupdate.com/2020/07/23/load-data-from-always-encrypted-enabled-azure-sql-database-using-ssis-in-adf/"" rel=""nofollow noreferrer"">basically the same process</a> and uses the ODBC driver as well.</p>
"
"63304521","Data Factory Pipeline failed with various errors","<p>I set up an Azure Data Factory with a MongoDB as a source and a Delta Lake Storage Gen2.
All connections (to the source and the target destination) were successfully checked. But the execution of the pipeline failed with the following errors:</p>
<p>*ADLS Gen2 operation failed for: Operation returned an invalid status code 'Forbidden'. Account: 'xxxx'. FileSystem: 'marketing'. Path: 'output/users.txt'. ErrorCode: 'AuthorizationPermissionMismatch'</p>
<p>&quot;errorCode&quot;: &quot;2200&quot;, &quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=UserErrorTypeInSchemaTableNotSupported,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to get the type from schema table. This could be caused by missing Sql Server System CLR</p>
<p>&quot;errorCode&quot;: &quot;2200&quot;, &quot;message&quot;: &quot;Failure happened on 'Sink' side. ErrorCode=UserErrorSchemaMappingCannotInferSinkColumnType,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Data type of column 'xx' can't be inferred from 1st row of data, please specify its data type in mappings of copy activity or structure of DataSet
*</p>
<p>Any ideas how I can fix those errors?</p>
","<mongodb><azure><azure-data-factory><azure-data-lake-gen2>","2020-08-07 15:11:01","1722","0","1","63592501","<p>The error &quot;*ADLS Gen2 operation failed for: Operation returned an invalid status code 'Forbidden'. Account: 'xxxx'. FileSystem: 'marketing'. Path: 'output/users.txt'. ErrorCode: 'AuthorizationPermissionMismatch'&quot;</p>
<p>points to the fact the pipeline does not have the required permission on the ADLS Gen2 . Please go through this doc : <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control</a>.</p>
<p>Please beware that in GEN2 the permissions are more granular and so when to check connections on the ADF UI that mey not catch that .</p>
<p>Let me know how it goes .</p>
<p>Thanks
Himanshu</p>
"
"63303828","How to wait for data factory pipeline to complete the execution?","<p>net core project. I am calling some ADF(Azure data factory) pipeline using my .net core code as below.</p>
<pre><code> public async Task&lt;string&gt; RunADFPipeline(DataFactoryManagementClient dataFactoryManagementClient, Dictionary&lt;string,object&gt; keyValuePairs, ADFClient aDFClient, string pieplineName)
        {
            CreateRunResponse runResponse = dataFactoryManagementClient.Pipelines.CreateRunWithHttpMessagesAsync(aDFClient.ResourceGroupName, aDFClient.DataFactoryName, pieplineName, parameters: keyValuePairs).Result.Body;
            return runResponse.RunId;
        }
</code></pre>
<p>This pipeline will run for around five minutes and pipeline will write some data to azure sql Db. Now my requirement is to fetch data from sql db. I have couple of questions running around this. How my code will come to know when my pipeline finished excution? I have tried something below.</p>
<pre><code> public async Task&lt;object&gt; GetPipelineInfoAsync(DataFactoryManagementClient dataFactoryManagementClient, ADFClient aDFClient, string runId)
        {
            var info = await dataFactoryManagementClient.PipelineRuns.GetAsync(aDFClient.ResourceGroupName, aDFClient.DataFactoryName, runId);
            return new
            {
                RunId = info.RunId,
                PipelineName = info.PipelineName,
                InvokedBy = info.InvokedBy.Name,
                LastUpdated = info.LastUpdated,
                RunStart = info.RunStart,
                RunEnd = info.RunEnd,
                DurationInMs = info.DurationInMs,
                Status = info.Status,
                Message = info.Message
            };
        } 
</code></pre>
<p>By passing RunId received from first call to above method I can get the status of it. But I cannot wait till the execution completes. The intention is ADF pipeline will write some data to db and that data I need to send back to UI. But I cant wait for this in current call. I am planning to use Signal R for this. As soon as ADF pipeline finishes execution I can call GetPipelineInfoAsync method and if the status is success then I can go to db and fetch details. Only problem I am facing is I cannot block main thread till the adf pipeline finishes execution. Can someone help me how can I fix this problem? Any help would be greatly appreciated. Thanks</p>
","<c#><azure><asp.net-core><signalr><azure-data-factory>","2020-08-07 14:28:23","1206","0","1","65295278","<p>You could use the management client's <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.pipelinerunsoperationsextensions.querybyfactory?f1url=%3FappId%3DDev15IDEF1%26l%3DEN-US%26k%3Dk(Microsoft.Azure.Management.DataFactory.PipelineRunsOperationsExtensions.QueryByFactory);k(TargetFrameworkMoniker-.NETFramework,Version%253Dv4.6.1);k(DevLang-csharp)%26rd%3Dtrue&amp;view=azure-dotnet"" rel=""nofollow noreferrer"">query by factory</a> extension API, passing in filters for pipeline name and status (and say a time span):</p>
<pre><code>var filters = new PipelineRunFilterParameters
{
    Filters =
    {
        new PipelineRunQueryFilter
        {
            Operand = &quot;PipelineName&quot;,
            OperatorProperty = &quot;Equals&quot;,
            Values = new List&lt;string&gt; { &quot;my_pipeline&quot;, &quot;my_other_pipeline&quot; }
        },
        new PipelineRunQueryFilter
        {
            Operand = &quot;Status&quot;,
            OperatorProperty = &quot;Equals&quot;,
            Values = new List&lt;string&gt; { &quot;InProgress&quot;, &quot;Queued&quot; }
        }
    },
    LastUpdatedBefore = DateTime.Now,
    LastUpdatedAfter = (DateTime.Now.AddDays(-1))
};

while ((_adfClient.PipelineRuns.QueryByFactory(_resourceGroup, _dataFactoryName, filters).Value.Count) &gt;= _maxConcurrentPipelines)
{
    Task.Delay(1000).Wait();
}
</code></pre>
<p>You can also wait on a single pipeline using the client's response id when creating a run and this API:</p>
<pre><code>while ((pipelineRun = _adfClient.PipelineRuns.Get(_resourceGroup, _dataFactoryName, response.RunId)).Status == &quot;InProgress&quot; || pipelineRun.Status == &quot;Queued&quot;)
{
    Task.Delay(1000).Wait();
}
</code></pre>
"
"63302070","ADF Limitation : Number of COPY Data Activities in One Pipeline?","<p>I have about 25 CSV Files with different columns (BLOB storage) with around ~250 columns each and want to load it Azure SQL DB separate tables(Basic Tier).</p>
<p>Created a Pipeline with 10 COPY Data Activity (CDA) all parallel in One pipeline for a start and executed it. The ADF pipeline just keeps on running without performing any task.  When I reduce the CDA to 7, the pipeline works and loads the data in a mater of seconds. To check if there is any connections limitation with SQL database, executed 3 pipelines simultaneously with 7 CDA each and it worked.</p>
<p>Question here is --&gt; Is there any Restriction/Limitation to the number of CDA we can have in a pipeline. If Yes, what can be done to change it ?</p>
<p>-Thanks</p>
<p>--EDIT   Added Screenshot post applying solution provided to change property for parallel copies.</p>
<p><a href=""https://i.stack.imgur.com/MjxqS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MjxqS.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/eufvk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eufvk.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2020-08-07 12:39:03","1274","1","1","63305670","<p>Yes there are limitations.  Since you are going from a Blob file store to an Azure SQL DB, to increase the amount of parallel copies you will need to set the parallelCopies property.</p>
<pre><code>&quot;activities&quot;:[
    {
        &quot;name&quot;: &quot;Sample copy activity&quot;,
        &quot;type&quot;: &quot;Copy&quot;,
        &quot;inputs&quot;: [...],
        &quot;outputs&quot;: [...],
        &quot;typeProperties&quot;: {
            &quot;source&quot;: {
                &quot;type&quot;: &quot;BlobSource&quot;,
            },
            &quot;sink&quot;: {
                &quot;type&quot;: &quot;AzureSQLDBSink&quot;
            },
            &quot;parallelCopies&quot;: 32
        }
    }
]
</code></pre>
<p>From file store to non-file store   - When copying data into Azure SQL Database or Azure Cosmos DB, default parallel copy also depend on the sink tier (number of DTUs/RUs).</p>
<ul>
<li>When copying data into Azure Table, default parallel copy is 4.</li>
</ul>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features</a></p>
"
"63301805","Azure DataFactory find output structure of activity block without Debugging","<p>Currently, in order to use the various data-manipulation blocks of Azure DataFactory (v2) I need to run the pipeline in the Debugger, and view the actual outputs, in order findout what the JSON structure of their outputs is.</p>
<p><em>Some</em> of the Activities have sample outputs documented in MSDN (e.g. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#sample-output"" rel=""nofollow noreferrer""><code>Get Metadata</code> Activity</a>) but others don't (e.g. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity"" rel=""nofollow noreferrer""><code>Filter</code> Activity</a>).</p>
<p>The Author 'IDE' doesn't have any autocomplete :(</p>
<p>So is there any way that I could have known that I wanted the <code>.Values</code> property of the filter output, without having to <em>actually execute</em> the pipeline up to that point?</p>
","<azure-data-factory>","2020-08-07 12:22:59","113","0","1","63335273","<p>All the actives are running in the pipeline. We must first run(debug) the pipeline, then the actives can run.</p>
<p>There is no way that you get the output of the actives without executing the pipeline.</p>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities"" rel=""nofollow noreferrer"">Pipelines and activities in Azure Data Factory</a>.</p>
"
"63301721","Azure DataFactory Retain LastModified Date when copying file","<p>I want to (automatically, but as part of a pipeline) archive some existing files, by moving them to a new folder.</p>
<p>I've written a pipeline to do that, but since it's a &quot;Copy-and-delete-Original&quot; command, the new file has a new Timestamp.</p>
<p>Is there any way to retain the original timestamps, either by <em>actually</em> moving the file, or by explicitly setting the LastModified date? (there doesn't appear to be a setting on the copy data activity to retain the Timestamp :(</p>
","<azure-data-factory>","2020-08-07 12:18:24","594","0","1","63305554","<p>I don't think this is supported through ADF's web UI.  I could be wrong, but I haven't see a way to do it.</p>
<p>But you could call the REST API for Blob services and set the lastmodifieddate that way.  You could get the file's original lastmodifieddate using the getmetadata activity and then copying the file to the new location, and then call the REST API and reset the property.</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-properties</a></p>
"
"63300659","Does Azure DataFactory support defining a recursive loop or a loop that modifies it's exit condition?","<p>Does ADF (v2) allow for recursive activities, or loops which modify the collection they're looping over?</p>
<hr>
<p><em><strong>This question is NOT about getting all nested children of a folder, or about finding the min of a collection. Those are just particular examples of where you might use a control structure like this. Don't reply with solutions for the specific cases!</strong></em></p>
<ul>
<li>Given a folder, you can easily list the direct children of that folder, using the <code>Get-Metdata</code> Activity.</li>
<li>And you can iterate over each of those children.</li>
<li>And if one of the children is itself a folder, you could get <em>it's</em> direct children.</li>
</ul>
<p>...</p>
<p>But I can't see whether any ADF control-structures would allow you do going down that process until it ends, building a collection of all the files found on the way.
You could build a pipeline that goes an fixed number of levels down, but you can't be <em>actually</em> recursive?</p>
<p>Similarly, supposing you wanted to find the smallest element in a set, and you had a <code>.first()</code> and a <code>.filter()</code> but not a <code>.sort()</code> <em>(Hah! What sort of insane system would do <strong>that</strong>!)</em>. Then a modifiable loop condition, you could do something along the lines of:</p>
<pre><code>While(list.length &gt; 1) {
    list = filter(list, where listElement &lt;= first(list))
}
</code></pre>
","<loops><recursion><azure-data-factory>","2020-08-07 11:14:30","751","0","1","63305327","<p>If I am understanding you correctly you want to look through a recursive set of files in a N number of directories and then be able to do some filtering based on all the files iterated on?</p>
<p>If so, I would look at setting up a dataset that uses a wildcard for the folder path depending on how you have partitioned out your files.  You should be able to point to something like <code>2020/*/*/*.csv</code>.  That's easy enough.  But to filter on what's <em>IN</em> the files you would need to use a mapping Data Flow and use that dataset as your source and then build out a pipeline that filters your results.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-filter"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-filter</a></p>
"
"63299496","Azure Data Factory Capture Error output from Notifications tab","<p>I have a stored procedure that I use to log the progress of my ADF executions.</p>
<p>I can capture things like Data Factory Name (@pipeline().DataFactory) and RunId (@pipeline().RunId) and record these against the rows in the log table.</p>
<p>However, what I also want to capture is the error output from the notifications tab when executions fails.</p>
<p>For example</p>
<p><a href=""https://i.stack.imgur.com/4Wh8K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Wh8K.png"" alt=""Error output from notifications tab"" /></a></p>
<p>I tried this in the failure constraint (red arrow)</p>
<pre><code>@activity('Execute LandingTbls').output
</code></pre>
<p>but the output in the log table from this was (not much help here)</p>
<pre><code>System.Collections.Generic.Dictionary`2[System.String,System.Object]
</code></pre>
<p>How can this be done?</p>
","<error-handling><azure-data-factory>","2020-08-07 10:00:53","1330","0","1","63335169","<p>Basiclly, you can do like this:</p>
<p><a href=""https://i.stack.imgur.com/6dtdH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6dtdH.png"" alt=""enter image description here"" /></a></p>
<p>The expression is <code>@activity('Validation1').Error.Message</code>.</p>
<p>(On my side, the activity I want to check error message is Validation1, you can change it to the activity on your side.)</p>
<p><a href=""https://i.stack.imgur.com/YSxMG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YSxMG.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/1IyvZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1IyvZ.png"" alt=""enter image description here"" /></a></p>
"
"63292083","How to migrate data (folders) from a server on a local private network to azure data lake?","<p>I have a server located on a local secured network. I want to migrate the data periodically from this server to Azure data lake 2.0.</p>
<p>I tried using data factory, but was unsuccessful. Any recommendation would be appreciated.</p>
","<azure><azure-data-lake><data-migration><azure-data-factory>","2020-08-06 21:34:15","34","1","1","63310335","<p>Azure Data Factory should be fine for this, but you need to use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">self-hosted integration runtime</a>. You would set up a linked service of type file system that uses the self-hosted IR as your source linked service. Then set up a linked service for ALDS Gen 2 to use for your destination. You can copy multiple files at once and preserve the folder hierarchy by making sure the copy behavior is set to PreserveHierarchy. You can find more information, including how to set your datasets and copy activity in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system#supported-capabilities"" rel=""nofollow noreferrer"">Microsoft Docs</a>.</p>
"
"63288578","Unable to specify parameters to parameterized source data set in ADF data flow","<p>I have a data flow that has a parameter: <code>TableName</code>.  The dataset that is used as a source within the flow is parameterized for a <code>TableName</code> parameter (SQL Server dataset).  When selecting this dataset in source setting within the ADF dataflow, it does not allow me to set the <code>TableName</code> parameter as it does when setting the source within a standard CopyActivity.</p>
<p>So how does one use a parameterized dataset in a dataflow if it never allows you to set the parameters?</p>
<p>UPDATE: The settings are actually on the DataFlow activity itself.</p>
<p><a href=""https://i.stack.imgur.com/CKg6y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKg6y.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-06 17:08:47","1390","2","2","63292019","<p>In data flow, you will set the dataset parameter in Debug Settings when designing/debugging your data flow. You can then set the parameter at runtime in the data flow activity settings in the pipeline.</p>
"
"63288578","Unable to specify parameters to parameterized source data set in ADF data flow","<p>I have a data flow that has a parameter: <code>TableName</code>.  The dataset that is used as a source within the flow is parameterized for a <code>TableName</code> parameter (SQL Server dataset).  When selecting this dataset in source setting within the ADF dataflow, it does not allow me to set the <code>TableName</code> parameter as it does when setting the source within a standard CopyActivity.</p>
<p>So how does one use a parameterized dataset in a dataflow if it never allows you to set the parameters?</p>
<p>UPDATE: The settings are actually on the DataFlow activity itself.</p>
<p><a href=""https://i.stack.imgur.com/CKg6y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKg6y.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-06 17:08:47","1390","2","2","63297798","<p>As I understand, you mean that you can set the <code>TableName</code> in Copy Active and can't in Data Flow.</p>
<p>In Copy Active, we could set parameter like this:
<a href=""https://i.stack.imgur.com/BNStP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BNStP.png"" alt=""enter image description here"" /></a></p>
<p>But in Data Flow, the UI looks like:
<a href=""https://i.stack.imgur.com/YBCHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YBCHA.png"" alt=""enter image description here"" /></a></p>
<p>I have a workaround is that <strong>we could choose the table with <code>Query</code> in <code>Source operations</code></strong>:</p>
<pre><code>'select * from ' +$TableName
</code></pre>
<p><a href=""https://i.stack.imgur.com/DZurA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZurA.png"" alt=""enter image description here"" /></a></p>
<p>Pipeline parameter:</p>
<p><a href=""https://i.stack.imgur.com/Tdzhh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tdzhh.png"" alt=""enter image description here"" /></a></p>
<p>Data Flow parameter:
<a href=""https://i.stack.imgur.com/xxwxL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xxwxL.png"" alt=""enter image description here"" /></a></p>
<p>It works well.</p>
"
"63283203","Move a file on FTP","<p>I'm using Azure Data Factory and the integration runtime installed on an on-premise machine to connect to an FTP and copy files.</p>
<p>All this works, but after the successful copy, the requirement is to move the files on the source FTP to a different folder on that same FTP.</p>
<p>Is this at all possible?
Is it possible to run a script on the on-premise machine using a pipeline and the integration runtime?</p>
<p>Thanks for any insight!</p>
","<azure><azure-data-factory>","2020-08-06 12:03:27","993","0","2","63294693","<p>It is not possible by using Azure Data Factory, because the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Azure Data Factory doc</a> shows the FTP is not supported as sink:</p>
<p><a href=""https://i.stack.imgur.com/sC4pR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sC4pR.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/9JiDS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JiDS.png"" alt=""enter image description here"" /></a></p>
<p>So I suggest you to use <strong>FTP client</strong> or <strong>FTP cmd</strong> to move file manually.</p>
<p><strong>For example:</strong></p>
<p>I want to move source file <code>/public_html/upload/64/SomeMusic.mp3</code> to <code>/public_html/archive/64/SomeMusic.mp3</code></p>
<p>then in the <strong>FTP client</strong>:</p>
<p><code>rename /public_html/upload/64/SomeMusic.mp3 /public_html/archive/64/SomeMusic.mp3</code></p>
<p>or by <strong>FTP cmd</strong>:</p>
<pre><code>RNFR /public_html/upload/64/SomeMusic.mp3
RNTO /public_html/archive/64/SomeMusic.mp3
</code></pre>
"
"63283203","Move a file on FTP","<p>I'm using Azure Data Factory and the integration runtime installed on an on-premise machine to connect to an FTP and copy files.</p>
<p>All this works, but after the successful copy, the requirement is to move the files on the source FTP to a different folder on that same FTP.</p>
<p>Is this at all possible?
Is it possible to run a script on the on-premise machine using a pipeline and the integration runtime?</p>
<p>Thanks for any insight!</p>
","<azure><azure-data-factory>","2020-08-06 12:03:27","993","0","2","65405420","<p>How about use LogicApps?</p>
<p>LogicApps can do the action &quot;copy file (FTP)&quot; and &quot;delete file (FTP)&quot;.</p>
<p>Make a LogicApps that is http trigger and trigger it from ADF.</p>
"
"63282836","Unable to change Azure Data Factory subscription","<p>I'm trying to change/migrate a Data Factory resource from one subscription to another in Azure and I'm getting an odd JSON error which I'm not sure if it's an issue with the portal itself or whether there is a true validation error that is stopping it from being changed.</p>
<pre><code> {
  &quot;message&quot;: &quot;Cannot read property 'responseJSON' of undefined&quot;,
  &quot;name&quot;: &quot;TypeError&quot;,
  &quot;stack&quot;: &quot;TypeError: Cannot read property 'responseJSON' of undefined\n    at c (https://portal.azure.com/AzureHubs/Content/Dynamic/hUQmInOzYJjx.js:2:668)\n    at o (https://portal.azure.com/Content/Dynamic/ACdWT39XjnDc.js:4:72)\n    at https://portal.azure.com/Content/Dynamic/ACdWT39XjnDc.js:4:5711&quot;
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/UUkii.png"" rel=""nofollow noreferrer"">Screenshot</a></p>
","<azure><azure-data-factory>","2020-08-06 11:42:24","199","0","1","63467541","<p>You will receive this error message when you don’t have permissions on the destination subscription or Resource Group which you are trying to move the resources.</p>
<p>Make sure the account which you are trying to move the resources has permission in both the destinations Subscription and Resource Group.</p>
<p>Go to <strong>Access control (IAM)</strong> settings for both Subscription and Resource Group and the user is added as an <strong>owner</strong> or <strong>administrator</strong>.</p>
<p>Once you grant the owner permissions on your associates account, I’m able to move the resources without any issue.</p>
"
"63280629","ADF : Using MSA account with Integration Runtime for On-Premise File System","<p>We have a scenario to copy data from on-premise to Cloud Storage and we are using an Integration Runtime for this.</p>
<p>In the Linked Service to the on-premise file system, I see an option for providing username/password, or to retrieve the same from a key vault. However, want to know if I will be able to use a MSA/gMSA account to connect to the on-premise file system, so that I do not have the hassle of managing passwords.</p>
<p>Any quick pointers on this would be helpful.</p>
","<azure-data-factory>","2020-08-06 09:26:45","199","0","1","63297439","<p>If your MSA/gMSA account can access the local machine then you can.<br />
As the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system"" rel=""nofollow noreferrer"">Azure Data Factory doc</a> defines:</p>
<p><a href=""https://i.stack.imgur.com/ue5LN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ue5LN.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/6TLMN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6TLMN.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>Update:</strong><br />
You can choose to use Key Vault to store your password, so that you don't need to enter the password any more. You can manage the password centrally.
<a href=""https://i.stack.imgur.com/0ls1u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0ls1u.png"" alt=""enter image description here"" /></a></p>
"
"63280078","Azure data factory data flow json to SQL","<p>I have a source JSON file with hierarchical data, which I need to sink in two SQL tables(relational).
The JSON is as below</p>
<pre><code>{
&quot;orders&quot;:[
  {
    &quot;orderid&quot;:&quot;30933&quot;,
    &quot;ordername&quot;:&quot;abc&quot;,
    &quot;items&quot;:[
      {
        &quot;itemid&quot;:1,
        &quot;itemstatus&quot;:&quot;Failed&quot;
      },
      {
        &quot;itemid&quot;:2,
        &quot;itemstatus&quot;:&quot;Failed&quot;
      }
    ]
  },
  {
    &quot;orderid&quot;:&quot;308320&quot;,
    &quot;ordername&quot;:&quot;xyz&quot;,
    &quot;items&quot;:[
      {
        &quot;itemid&quot;:5,
        &quot;itemstatus&quot;:&quot;Succeeded&quot;
      }
    ]
  }
]
}
</code></pre>
<p>My SQL holding two tables Order and OrderItem with OrderID primary and foreign key.</p>
<p>Now I have an Azure data factory data flow with source as above JSON and I need to park all data relational in respective tables.</p>
<p>So here I need OrderId(30933,308320) and OrderName(abc,xyz) will go into Order table and respective items data go into OrderItem table(which reference OrderId from Order table). In this case Order table have 2 and OrderItem table have 3 entries.</p>
","<sql><json><azure-data-factory><dataflow>","2020-08-06 08:53:38","738","0","1","63294676","<p>We can not achieve that in one copy active.</p>
<p>We could using two copy actives in one pipeline, I tested and it succeed. You could follow my steps bellow:</p>
<ol>
<li>Copy active1: copy the data from <code>Orders</code>(orderid and ordername) to table <code>Orders</code>.</li>
<li>Copy active2: copy the data from <code>items</code>(itemid and itemstatus) to
table <code>OrderItems</code>.</li>
</ol>
<p><strong>Note:</strong></p>
<ol>
<li><p>Copy active 1 and 2 use the same json file as the source. The
differences are in the Mapping settings.</p>
</li>
<li><p>Copy active 1 sink is Azure SQL database table <code>Orders</code>, Copy active
2 sink is Azure SQL database table <code>OrderItems</code>.</p>
</li>
</ol>
<p>To make you understand it clearly, I made two GIF pictures.</p>
<p><strong>Mapping settings in Copy active 1:</strong>
<a href=""https://i.stack.imgur.com/fcach.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fcach.gif"" alt=""enter image description here"" /></a></p>
<p><strong>Mapping settings in Copy active 2:</strong>
<a href=""https://i.stack.imgur.com/oqchZ.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqchZ.gif"" alt=""enter image description here"" /></a></p>
<p>Run the pipeline:</p>
<p><a href=""https://i.stack.imgur.com/uDMHW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uDMHW.png"" alt=""enter image description here"" /></a></p>
<p>Check the data in table:</p>
<p><a href=""https://i.stack.imgur.com/INVlG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/INVlG.png"" alt=""enter image description here"" /></a></p>
<p>The limit is that we only could get the first element of <code>items</code>, we can not choose the collection reference both.</p>
<p><a href=""https://i.stack.imgur.com/wJL0S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wJL0S.png"" alt=""enter image description here"" /></a></p>
<p>Update:</p>
<p>Congratulations that you achieved it in another way: flatten/transpose the data using data flow and then maintain it to pour in a relational SQL table.</p>
"
"63278759","ADF Dynamic Parameters - Error : Failed to validate the signature because the content is tampered","<p>We have a scenario where the same kind of flow is applicable for multiple scenarios. Hence, instead of creating linked services / datasets for every scenario, I am trying to have a generic linked service/dataset so that they can be reused by passing different parameters.</p>
<p>I have a Blob linked service where I parameterized the storage account name. Tested the linked service with a parameter and it is working fine. Now, I create a dataset with this linked service and provide a parameter for the storage account name in the dataset. When I try to test the dataset and provide the same input parameter, I get the below error :</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=UserErrorInvalidCredential,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
Message=**Failed to validate the signature because the content is tampered**, 
the expect context is '{\&quot;DefaultEndpointsProtocol\&quot;:\&quot;https\&quot;,\&quot;AccountName\&quot;:\&quot;xxxx\&quot;}' 
and the runtime context is '{\&quot;DefaultEndpointsProtocol\&quot;:\&quot;https\&quot;,\&quot;AccountName\&quot;:\&quot;@body('Generic Passthrough CopyComposeRuntimeVariables')?.GenericBlobDatasetxxxx.DatasetStorageAccountName\&quot;}'.,Source=diawp,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Generic Passthrough Copy&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>Any pointers to solve the above issue would be helpful.</p>
","<azure-data-factory>","2020-08-06 07:27:36","574","0","1","63299553","<p>The customized dynamic content of the linked server only works at the first time. When you create a dataset with this linked service, it means that the linked server will be published again, then the dynamic content will be overwritten by the default content.</p>
<p>You can check the changes in linked service settings on Portal:</p>
<p><a href=""https://i.stack.imgur.com/q1fjl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q1fjl.png"" alt=""enter image description here"" /></a></p>
"
"63278426","Delta Lake Gen2 for a MongoDB migration","<p>Which Azure pipeline and Data Storage you would prefer for a MongoDB migration?</p>
<p>I know there exists the functionality of an Azure Migration Service where you can shift MongoDB data directly to an Azure CosmosDB. Azure Migration Services seems to be available only for specific licenses. Using Cosmos DB it is also necessary to take care of costs.
Another possibility is to use Stitch to shift MongoDB directly into Azure.</p>
<p>Since we don't want to use an additional tool, we want to use Azure Data Factory to shift the MongoDB data into an Azure Data Storage. We want to use the Data Lake Storage Gen2, as it combines the advantages of the Blob Storage and the Data Lake Storage Gen1.</p>
<p>Which pipeline you would prefer? Any experiences with storing the MongoDB data in Azure Data Lake Storage Gen2?</p>
","<azure><azure-cosmosdb><azure-data-factory><azure-data-lake-gen2><stitch>","2020-08-06 07:04:18","370","0","1","63513894","<p>Please see the following Azure Data Factory <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities"" rel=""nofollow noreferrer"">document</a> pertaining to <em>Pipelines</em> and <em>Activities</em>, which details the source and target data endpoints that are currently supported.</p>
<p><a href=""https://i.stack.imgur.com/he9OO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/he9OO.png"" alt=""Azure ADF Azure Storage Data Movement Support "" /></a></p>
<ul>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store"" rel=""nofollow noreferrer"">Copy data to or from Azure Data Lake Storage Gen1 using Azure Data Factory</a></p>
</li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage"" rel=""nofollow noreferrer"">Copy and transform data in Azure Data Lake Storage Gen2 using Azure Data Factory</a></p>
</li>
</ul>
<p><a href=""https://i.stack.imgur.com/ouRAa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ouRAa.png"" alt=""Azure ADF NoSQL Data Movement Support"" /></a></p>
<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb"" rel=""nofollow noreferrer"">Copy data from MongoDB using Azure Data Factory</a></li>
</ul>
<p>Using the MongoDB connector as a source and Azure Data Lake Storage Gen2 as a sink, you can then perform any transformation and finally, migrate the data to Azure Cosmos DB...if desired.</p>
<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db"" rel=""nofollow noreferrer"">Copy and transform data in Azure Cosmos DB (SQL API) by using Azure Data Factory</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db-mongodb-api"" rel=""nofollow noreferrer"">Copy data to or from Azure Cosmos DB's API for MongoDB by using Azure Data Factory</a></li>
</ul>
<p>If you experience any issues with the migration of data to Azure Cosmos DB, if that is the goal of the migration, then consider the following direct migration paths: <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/cosmosdb-migrationchoices"" rel=""nofollow noreferrer"">Options to migrate your on-premises or cloud data to Azure Cosmos DB</a></p>
"
"63272566","ADF DataFlows Last Entry in a column","<p>Apologies if this has been asked previously, as I've been searching and still haven't found anything helpful. If it is, please point me in the right direction.</p>
<p>Okay so the issue is, I have a table with the columns, <strong>EquipmentId</strong> and <strong>OperatingHours</strong>. The data is something like this,</p>
<pre><code>| EquipmentId | OperatingHours |
|    1020     |      3577      |
|    1020     |      3577      |
|    1020     |      3580      |
|    1020     |      3581      |
|    1020     |      3585      |
</code></pre>
<p>I want to get the last Entry based on the <strong>EquipmentId</strong> so that I could get the value <strong>3585</strong>, I've tried using <strong>Aggregate</strong> using <strong>Group By EquipmentId</strong> and under <strong>Aggregations</strong> something like this,</p>
<p><a href=""https://i.stack.imgur.com/kPMsk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kPMsk.png"" alt=""enter image description here"" /></a></p>
<p>But I am still getting <strong>3577</strong> in the output and not the last entry.</p>
<p>My question is, How can I get the last entry value only using DataFlows, is there anything else I could use instead of Aggregate to sort this out.</p>
<p>Apologies if I've done something wrong, it's my first day working with DataFlows so just trying to follow different tutorials available online.</p>
<p>Thank you and looking forward to hearing from any of you.</p>
","<azure><azure-sql-database><dataflow><azure-data-factory>","2020-08-05 19:49:54","48","0","1","63277959","<p>I tried the same operation, but I get the value '3585'.</p>
<p>Please ref my steps bellow:</p>
<p>Source settings and data preview:</p>
<p><a href=""https://i.stack.imgur.com/sqYSF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sqYSF.png"" alt=""enter image description here"" /></a></p>
<p>Aggregate settings and data preview:
<a href=""https://i.stack.imgur.com/z3keL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z3keL.png"" alt=""enter image description here"" /></a></p>
<p>I think we have the same operations. Please correct me if I missed anything.
Maybe you can try again.</p>
"
"63271278","How to pass parameters to an ADF pipeline from Schedule type trigger?","<p>Is there a way to pass parameters to an ADF pipeline from Schedule type trigger?</p>
","<azure><azure-pipelines><azure-data-factory>","2020-08-05 18:19:31","4105","1","1","63276173","<p>Yes, you can.</p>
<p>First, add parameter to your pipeline:</p>
<p><a href=""https://i.stack.imgur.com/3A9ii.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3A9ii.png"" alt=""enter image description here"" /></a></p>
<p>And then you can edit the json file of your trigger:</p>
<pre><code>{
    &quot;name&quot;: &quot;trigger2&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;runtimeState&quot;: &quot;Started&quot;,
        &quot;pipelines&quot;: [
            {
                &quot;pipelineReference&quot;: {
                    &quot;referenceName&quot;: &quot;pipeline3&quot;,
                    &quot;type&quot;: &quot;PipelineReference&quot;
                },
                &quot;parameters&quot;: {
                    &quot;test1&quot;: &quot;111&quot;
                }
            }
        ],
        &quot;type&quot;: &quot;ScheduleTrigger&quot;,
        &quot;typeProperties&quot;: {
            &quot;recurrence&quot;: {
                &quot;frequency&quot;: &quot;Minute&quot;,
                &quot;interval&quot;: 1,
                &quot;startTime&quot;: &quot;2020-08-06T02:18:00.000Z&quot;,
                &quot;timeZone&quot;: &quot;UTC&quot;
            }
        }
    }
}
</code></pre>
<p>This is the doc:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger-definition"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger-definition</a></p>
<p>On my side it is no problem, please have a try on your side.</p>
"
"63271154","Recognize 'lookup' ID's (CRM) according to values in CosmosDB and set it into a Logic App","<p><strong>Background:</strong></p>
<p>I have data coming from CosmosDB that needs to be pushed into CRM. I am using a LogicApp for this.</p>
<p><strong>Dilemma:</strong></p>
<p>There are multiple IDs that have different attributes associated with them:
For instance the 'ems_featurecategoryid'. Each has a different name that goes along with its unique ID. (This the raw data that is within CRM)</p>
<p><strong>Figure A</strong>
<a href=""https://i.stack.imgur.com/uZtkw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uZtkw.png"" alt=""Figure A"" /></a></p>
<p>Here is the CRM interface and the lookup field (feature category) in which I wish to push the CosmosDB data into:</p>
<p><strong>Figure B</strong>
<a href=""https://i.stack.imgur.com/ZgJAe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZgJAe.png"" alt=""enter image description here"" /></a></p>
<p>Here is the CosmosDB data that hold this FeatureCategory attribute:</p>
<p><strong>Figure C</strong></p>
<p><a href=""https://i.stack.imgur.com/mEy1V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mEy1V.png"" alt=""enter image description here"" /></a></p>
<p>There are over 3000 of these. How would I map the IDs that are within CRM (Figure A) with CosmosDB (Figure C), so that when a value input is provided to the Feature Category lookup in CRM, this is recognized according to the value in CosmosDB?</p>
<p>This is what I am thinking:</p>
<p>I provided the Logic App with one of the GUIDs from Figure A. I am not sure if this is the right thing to do. The value is pushed through, but how do I account for all of the others?</p>
<p>Any help or suggestions are appreciated.</p>
<p><a href=""https://i.stack.imgur.com/s0WLY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s0WLY.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rFbA0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rFbA0.png"" alt=""enter image description here"" /></a></p>
","<dynamics-crm><azure-cosmosdb><azure-data-factory><azure-logic-apps><power-automate>","2020-08-05 18:11:31","172","0","1","63300228","<p>If I understand your question correct, You have Id's and it's respective name coming from CosmoDB.</p>
<p>In turn these Id's and unique name are feature category records(unique), I am not sure what entity name it has.
Now you wish for your Entity Features update record with it's field Feature Category(Lookup field) with the data coming from Cosmo DB.</p>
<p>You cannot directly update lookup field of a record with it's ID.</p>
<p><a href=""https://community.dynamics.com/365/b/linn-s-power-platform-notebook/posts/power-automate-how-to-set-lookup-field-value-in-common-data-service-current-environment-flow-connector"" rel=""nofollow noreferrer"">Follow the link here to set lookup</a></p>
<p>For a contact lookup you do it as</p>
<pre><code>contacts(&lt;ContactID&gt;)
</code></pre>
<p>So how do you set Feature Category field, As mentioned in link above you do it as</p>
<pre><code>pluralSchemaname(YourGuidFromCosmoDB)
</code></pre>
<p>If this does not work for you, then I would say, use Get Record Action of power automate/logic app,</p>
<p>So get your Feature entity record as below, Record Identifier would be a Guid you will get from Cosmo DB.</p>
<p>Now you get Feature lookup record and you can use this record to update field Feature in your entity Features.</p>
<p><a href=""https://i.stack.imgur.com/nwu0r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nwu0r.png"" alt=""enter image description here"" /></a></p>
"
"63269309","View the full dataset within ADF","<p>How do I view the full dataset that is within ADF and not just a preview?</p>
<p>Within ADF, (Azure Data Factory) there is an option to 'preview the data'. How would I view this data in full?</p>
<p><a href=""https://i.stack.imgur.com/2Wi7w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Wi7w.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/H83U0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H83U0.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2020-08-05 16:14:45","221","0","1","63275555","<p>Per my experience, I'm afraid we can see the full data of the dataset. Some time the source dataset may be very large.</p>
<p>I'm glad that you figured that: &quot;make a copy of it and send it to another database like Cosmosdb etc&quot;.</p>
"
"63267251","How to extract a substring from a filename (which is the date) when reading a file in Azure Data Factory v2?","<p>I have this Pipeline where I'm trying to process a CSV file with client data. This file is located in an Azure Data Lake Storage Gen1, and it consists of client data from a certain period of time (i.e. from January 2019 to July 2019). Therefore, the file name would be something like <em>&quot;Clients_20190101_20190731.csv&quot;</em>.</p>
<p>From my Data Factory v2, I would like to read the file name and the file content to validate that the content (or a date column specifically) actually matches the range of dates of the file name.</p>
<p>So the question is: how can I read the file name, extract the dates from the name, and use them to validate the range of dates inside the file?</p>
","<date><substring><filenames><azure-data-factory>","2020-08-05 14:23:04","1445","0","1","63287841","<p>I haven't tested this, but you should be able to use the get metadata activity to get the filename. Then you can access the outputs of the metadata activity and build an expression to split out the file name.  If you want to validate data in the file based on the metadata output (filename expression you built) your option would be to use Mapping Data Flows or to pass in the expression to a Databricks Notebook. Mapping Data Flows uses Databricks under the hood. ADF natively does not have transformation tools that you could accomplish this. You can't look at the data in the file except to move it (COPY activity).  With the exception of the lookup activity which has a 5000 record limit.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>
"
"63265606","Copy data 'Storage Connection' forces to use blob storage in ADF instead of ADLS Gen2","<p>I have a data factory with input from ADLS Gen2 (only this is compliant in our company). It works fine.
The pic given below is the settings of 'copy data' activity. As given in the pic for storing logs (missed rows data), we are forced to use blob storage or gen 1 datalake. How can we use ADLS Gen2 for this? Looks like a bottleneck. We will have complacency issues if such data is stored outside Gen2</p>
<p><a href=""https://i.stack.imgur.com/dTBS7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dTBS7.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-data-lake-gen2>","2020-08-05 12:49:24","65","0","1","63267472","<p>On my side it is no problem, please try to directly edit the definition json of your activity:</p>
<p>This is my json:</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline3&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;BinarySource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;BinaryReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;BinarySink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;logStorageSettings&quot;: {
                        &quot;linkedServiceName&quot;: {
                            &quot;referenceName&quot;: &quot;AzureDataLakeStorage1&quot;,
                            &quot;type&quot;: &quot;LinkedServiceReference&quot;
                        }
                    },
                    &quot;validateDataConsistency&quot;: false
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;Binary1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;Binary2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/XyaaW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XyaaW.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/lurf1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lurf1.png"" alt=""enter image description here"" /></a></p>
"
"63263261","FactorySizeInGbUnits and MaxAllowedFactorySizeInGbUnits in Azure Data Factory Metrics","<p>Recently configured Data factory for the first time and wanted to understand the metrics being collected.
The two metrics that I am not able to understand are:</p>
<ul>
<li><strong>FactorySizeInGbUnits</strong> - Is this the Gigabytes of data that Data Factory is transferring this very second or has transferred in totality so far?</li>
<li><strong>MaxAllowedFactorySizeInGbUnits</strong> - Is this a value we can set? For example, Maximum 100 Gigabytes of data can be transferred in a second?</li>
</ul>
<p>I did look at the <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/metrics-supported#microsoftdatafactoryfactories"" rel=""nofollow noreferrer"">Microsoft documentation</a> but the description isn't very clear (as shown below)
<a href=""https://i.stack.imgur.com/vupDC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vupDC.png"" alt=""enter image description here"" /></a></p>
<p>I have <strong>configured the data factory in my lab, created pipeline, and performed a copy activity</strong> and this is how the log looks:
<a href=""https://i.stack.imgur.com/970P1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/970P1.png"" alt=""enter image description here"" /></a></p>
<p>These may be trivial metrics for experienced Data factory folks but is really confusing for a newbie like me. Please assist as I could not find any documentation that could explain the two in full depth with an example.</p>
<p>Appreciate your help.</p>
<p>Thanks!!</p>
","<azure><azure-data-factory>","2020-08-05 10:31:02","132","0","1","67541229","<p>FactorySizeInGbUnits -&gt; Total size of the factory (pipelines + datasets + .....) rounded to next GB.</p>
<p>MaxAllowedFactorySizeInGbUnits -&gt; Max allowed factory size.</p>
"
"63263029","Azure Data Factory - dynamically copy data","<p>I'm looking to use azure data factory to run against a bunch of sql queries. The scenario I'm targeting is this:</p>
<ol>
<li>A preprocessor generates many (100+) sql files with HANA queries for extraction. These can be run against SAP HANA, each producing output that is saved. I have an application that does this, and it works.</li>
<li>I'm looking to move this to Azure Data Factory, as it needs to go through an Azure Self Hosted Integration Runtime if running from Azure. In other words, if running on the cloud, the only option is the ADF SAP HANA connector (which supports the Copy Data activity).</li>
<li>Ideally, I'd be able to point <em>something</em> to a list of sql files, execute each one, and store the results separately. If I could somehow give the connection to a notebook, or custom activity, that'd be sufficient. Is this possible?</li>
</ol>
<p>What would be the best way to achieve this? I guess I could programmatically generate 100+ copy data activities, but that seems heavy handed, and if more files are required later, I'd be changing the pipeline definition.</p>
","<azure><hana><azure-data-factory>","2020-08-05 10:16:27","318","0","1","63280565","<p>In that case I would recommend to use sqoop. I did use Apache Sqoop around three years ago to move SAP HANA data to Azure Blob and it worked like a mint.</p>
"
"63260908","How to run .Net spark jobs on Databricks from Azure Data Factory?","<p>In Azure data factory, you have a Databricks Acvitiy. This activity supports running python, jar and notebooks. And These notebooks may be written in scala, python, java, and R but not c#/.net.</p>
<p>Is there inherent or direct support where I can write my .NET spark code and run it on Databricks from Data Factory?</p>
<p>Can I use .NET spark in Azure Databricks to its full extent?</p>
","<c#><azure-data-factory><azure-databricks><.net-spark>","2020-08-05 08:06:59","480","3","1","63811374","<p>You specify that you want to launch a JAR file. The .NET for Apache Spark project uses a JAR file to start a listener which the .NET code then connects to.</p>
<p>The JAR is microsoft-spark-2.4.x-0.12.1.jar (depends on the version of spark and the .NET NuGet version). The Class to run is org.apache.spark.deploy.dotnet.DotnetRunner which you need to pass the correct parameters to, so that it starts your .NET application.</p>
<p>Ed</p>
"
"63250368","Reading/writing error messages from data factory pipelines","<p>Going to try asking this here.</p>
<p>I'm trying to write an error message from an Azure Data Factory pipeline to a table in SQL server. It needs to capture the error message from a Databricks Python job. I can't find any official documentation and the method I have found from <a href=""https://www.tech-findings.com/2019/06/ADF-Insert-Pipeline-details-in-Custom-Monitoring-Table.html"" rel=""nofollow noreferrer"">this source</a>:</p>
<p>@{activity('Proc source').error.message}</p>
<p>..doesn't write anything to the table. Just a blank string with no explanation.</p>
<p>Why data factory doesn't just have an area where you can view the details of errors instead of just saying &quot;Failed&quot; is beyond me. Or if it does, it's hidden away.</p>
<p>Does anyone have any ideas?</p>
","<python><azure><pipeline><azure-data-factory>","2020-08-04 15:40:45","1766","0","2","63260032","<p>You can see the details of error at here.</p>
<p><a href=""https://i.stack.imgur.com/7HXa4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7HXa4.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>@{activity('Proc source').error.message}</p>
</blockquote>
<p>This expression works.</p>
<p><a href=""https://i.stack.imgur.com/yRd5z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yRd5z.png"" alt=""enter image description here"" /></a></p>
<p>Is errorCode saved to your table?Make sure your activity name is correct.</p>
"
"63250368","Reading/writing error messages from data factory pipelines","<p>Going to try asking this here.</p>
<p>I'm trying to write an error message from an Azure Data Factory pipeline to a table in SQL server. It needs to capture the error message from a Databricks Python job. I can't find any official documentation and the method I have found from <a href=""https://www.tech-findings.com/2019/06/ADF-Insert-Pipeline-details-in-Custom-Monitoring-Table.html"" rel=""nofollow noreferrer"">this source</a>:</p>
<p>@{activity('Proc source').error.message}</p>
<p>..doesn't write anything to the table. Just a blank string with no explanation.</p>
<p>Why data factory doesn't just have an area where you can view the details of errors instead of just saying &quot;Failed&quot; is beyond me. Or if it does, it's hidden away.</p>
<p>Does anyone have any ideas?</p>
","<python><azure><pipeline><azure-data-factory>","2020-08-04 15:40:45","1766","0","2","63264052","<p>SOLVED:</p>
<p>For anyone else reading this, the problem turned out to be something very stupid. I'd included a space in the python file name.
This returned an error code of 3202 and a blank error message.</p>
<p>I had another problem and resolution after this that I'd like to share, again for anyone frantically googling a solution to the problem. Once I'd removed spaces from the python filename, I logged an error code of '3204'
and the error message:  &quot;Databricks execution failed with error message: . Run page url: (link to page showing clusters)&quot;. In the Databricks workspace launchable through the Azure portal, selecting a cluster through the 'clusters' sidebar then heading to 'Driver Logs' will show errors, in the 'Standard error' window that comes up.</p>
<p>I had already installed the libraries I needed on an existing cluster, but I'd forgotten to change a setting in the Databricks linked service. 'Select cluster' was set to 'new job cluster' when I needed 'Existing interactive cluster'. Thus, it wasn't pointing to the cluster I had expected.</p>
<p>These are all fairly small errors as it turns out, but again I hope that someone else dealing with the same issues will be able to find this post and save themselves some hassle!</p>
"
"63249868","Dynamic filename in Data Factory dataflow source","<p>I’m working with a pipeline that loads table data from onpremise SQL to a datalake csv file dynamically, sinking a .csv file for each table that I already set to load in a versionControl table in a AzureSQL using Foreach.</p>
<p>So, after load the data, i want to update the versionControl table with the lastUpdate date, based on the MAX(lastUpdate) field of each .csv file loaded. To accomplish that, i know that i need to add a dataflow after the copy activity, so i can use the aggregate transformation, but don’t know how to pass the filename to the source of the dataflow dynamically in a parameter.</p>
<p>Thanks!</p>
","<azure-data-factory>","2020-08-04 15:12:29","1940","1","1","63250896","<p>2 options:</p>
<ol>
<li><p>Parameterized dataset. Use a source dataset in the dataflow that has a parameter for the file name. You can then pass in that filename as a pipeline parameter.</p>
</li>
<li><p>Parameterized Source wildcard. You can also use a source dataset in the dataflow that points just to a folder in your container. You can then parameterize the wildcard property in the Source and send in the filename there as a pipeline parameter.</p>
</li>
</ol>
"
"63242401","SSIS on Azure - how to execute a process task on Azure?","<p>I'm running SSIS in an Azure datafactory / SQL managed instance environment and want to lift and shift my existing ETL to Azure from an on-premise environment.</p>
<p>Currently I have an executable that performs a number of tasks and is run as part of the SSIS package as a process task - no trouble at all to run in an on-premise environment, I simply refer to the executable (.net console application) in the process task using it's local path.  How do I go about moving that to the cloud and running the exe in Azure?</p>
<p>Do I store the .exe in blob storage?  How do I call it from the SSIS package?  MS appears to suggest that Azure supports running process tasks here: <a href=""https://learn.microsoft.com/en-us/sql/integration-services/control-flow/execute-process-task?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/integration-services/control-flow/execute-process-task?view=sql-server-ver15</a></p>
<p>Is it possible, or should I start looking at an alternative way of running this task?</p>
","<azure><ssis><azure-data-factory>","2020-08-04 07:52:36","516","-1","1","63244593","<p>You can try to put the executables in Azure Files. Locate the executable like \YourAzureStorageAccountName.file.core.windows.net\YourFolderName\test.exe, and provide Authentication with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/ssis-azure-connect-with-windows-auth"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/ssis-azure-connect-with-windows-auth</a> for Azure Files</p>
"
"63239767","How to pass data from Data flow activity to other activity in Azure data factory v2","<p>I have extracted a max date from the data in a Data flow activity, however, I am not able to pass it or set it to a variable so that other activities could use the same value. Please suggest how could it be done.</p>
<p><a href=""https://i.stack.imgur.com/nolKi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nolKi.png"" alt=""Max date - data flow activity"" /></a></p>
","<azure><azure-data-factory>","2020-08-04 03:21:29","794","0","2","63250833","<p>You have to persist it somewhere. Every data flow requires a Sink. Just drop it into a CSV file in Blob/ADLS. You don't even need a header or any other columns. Just store &quot;12&quot; or whatever your result it in that file. The next activity in your pipeline will then be a Lookup activity to get that value and use it in your pipeline.</p>
"
"63239767","How to pass data from Data flow activity to other activity in Azure data factory v2","<p>I have extracted a max date from the data in a Data flow activity, however, I am not able to pass it or set it to a variable so that other activities could use the same value. Please suggest how could it be done.</p>
<p><a href=""https://i.stack.imgur.com/nolKi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nolKi.png"" alt=""Max date - data flow activity"" /></a></p>
","<azure><azure-data-factory>","2020-08-04 03:21:29","794","0","2","68249710","<p>I believe you're looking for <a href=""https://www.youtube.com/watch?v=HqCn42FaRJs"" rel=""nofollow noreferrer"">cached lookups</a>, a feature added ~3 months or so after you asked this question.</p>
"
"63238117","ADF: Dynamic Content in parameters","<p>I am trying to pass text with dynamic content as a parameter into a pipeline (execute pipeline activity).</p>
<p>As a super simple example, I want the input to my pipeline to be a timestamp, utcnow(). Here are my results:</p>
<p>I've noticed:<br />
If I put @utcnow() in a set variable activity and set the execute pipeline parameter to that variable it works.</p>
<p>If I put @utcnow() (or @{utcnow()}) in the main parameter and set the execute pipeline parameter to that parameter it does not work. I get that string &quot;utcnow()&quot; as the result.</p>
<p>Is there anything that I am missing here? I definitely feel like I've done this successfully before.</p>
","<azure-data-factory>","2020-08-03 23:09:04","5429","1","1","63257207","<p>If I understand your question correctly, the issue is caused by the main parameter(pipeline parameter) doesn't support expression or functions.</p>
<p>For example,  we could pass the value from variable to pipeline active parameter, and it works well, because variable support expression/functions:
<a href=""https://i.stack.imgur.com/oHzUu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oHzUu.png"" alt=""enter image description here"" /></a></p>
<p>When the main pipeline only contains an Execute Pipeline active, we pass the value from main parameter(pipeline parameter) to the Execute Pipeline parameter:
<a href=""https://i.stack.imgur.com/tW6Fg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tW6Fg.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/OFl8v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OFl8v.png"" alt=""enter image description here"" /></a></p>
<p>When we debug the pipeline, we need pass the value of main parameter:
<a href=""https://i.stack.imgur.com/tMZJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tMZJS.png"" alt=""enter image description here"" /></a></p>
<p>The value of pipeline parameter only support the String value, then function <code>utcNow()</code> or <code>@{utcnow()</code> will considered as the String.</p>
"
"63237495","Azure Data Factory blob dataset using gzip compression started throwing validation error","<p>I have parquet blob datasets defined in my Data Factory using gzip compression. The files have a '.parquet' extension. Copy activities and data flows have been working fine with these parquet blobs for the last 6 months.</p>
<p>Today I started getting following validation error when editing my pipeline:</p>
<pre><code>Gzip compression in data flow only supports files with &quot;.gz&quot;
extension. Please update the file name configuration or choose another
compression type.
</code></pre>
<p>Has something changed in the ADF service?</p>
<p>Where can I read more about this breaking change and when it was introduced?</p>
","<azure><gzip><azure-blob-storage><azure-data-factory>","2020-08-03 21:57:59","642","0","2","63268454","<p>We encountered this same problem on 8/3/20, and opened a support ticket with Microsoft on 8/4/20. The response was that it is a known problem, is on track to be fixed this week.</p>
"
"63237495","Azure Data Factory blob dataset using gzip compression started throwing validation error","<p>I have parquet blob datasets defined in my Data Factory using gzip compression. The files have a '.parquet' extension. Copy activities and data flows have been working fine with these parquet blobs for the last 6 months.</p>
<p>Today I started getting following validation error when editing my pipeline:</p>
<pre><code>Gzip compression in data flow only supports files with &quot;.gz&quot;
extension. Please update the file name configuration or choose another
compression type.
</code></pre>
<p>Has something changed in the ADF service?</p>
<p>Where can I read more about this breaking change and when it was introduced?</p>
","<azure><gzip><azure-blob-storage><azure-data-factory>","2020-08-03 21:57:59","642","0","2","63276107","<p>this problem is been solved. Please have a check, This is not a major update, it is a bug and it has now been fixed:</p>
<p><a href=""https://i.stack.imgur.com/VXaCV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VXaCV.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/pFQch.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pFQch.png"" alt=""enter image description here"" /></a></p>
<p>You can check the update here:</p>
<p><a href=""https://i.stack.imgur.com/AUwqR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AUwqR.png"" alt=""enter image description here"" /></a></p>
"
"63235861","Blob-Blob ADF Copy Activity","<p>I have, what I assume to be a very basic question, that I cannot find an answer for.</p>
<p>I would like to move files from blob1 to blob2 (external to archival). I have an orchestration pipeline that would run this pipeline to basically &quot;truncate&quot; blob1 (copy from external blob to archival blob and then delete from external blob).</p>
<p>I've created linked services and datasets for both blobs and two parameters (input and output) and added two test csv files. My first step is working within the Copy Activity under the source tab:
<a href=""https://i.stack.imgur.com/OCWoo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OCWoo.png"" alt="""" /></a></p>
<p>Below I believe I am requesting to copy data in the &quot;input_container_name&quot;/anything/anything and to place it in:
<a href=""https://i.stack.imgur.com/EYcUx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EYcUx.png"" alt=""enter image description here"" /></a></p>
<p>When I run this, it completes the task, but no files are being moved, throughput is 0.</p>
<p>Is this the best way to tackle this task? Or should I try to get a list of all blobs in the container and then copy each blob in a for loop?</p>
","<azure-data-factory>","2020-08-03 19:34:51","163","0","1","63240699","<p>I tried the same operation with you but I get the error when I debug the pipeline:
<a href=""https://i.stack.imgur.com/maypF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/maypF.png"" alt=""enter image description here"" /></a></p>
<p>If we don't set the container name from parameter then the pipeline works well.</p>
<p>For example, I move the blob (folder and csv files) from container <code>test</code> to <code>test2</code>:
<a href=""https://i.stack.imgur.com/gl7c4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gl7c4.png"" alt=""enter image description here"" /></a></p>
<p>Sink dataset:</p>
<p><a href=""https://i.stack.imgur.com/B9rT6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B9rT6.png"" alt=""enter image description here"" /></a></p>
<p>Sink:</p>
<p><a href=""https://i.stack.imgur.com/Q2Ety.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q2Ety.png"" alt=""enter image description here"" /></a></p>
<p>This will help you move all the folder and csv files to another container.</p>
"
"63235620","Dynamically changing Linked Services/Datasets in Azure Data Factory","<p>We require the ability the to perform a lookup into a config table that will let us know the Server and Database and allow us to change it for subsequent lookups.   I can set it up so it up dynamically where it prompts me for the Server and Database – but that process wouldn’t work for us as this process will be running unattended.</p>
<p>For the purposes of this discussion, assume we have a table, Config, which looks like:</p>
<p>ID  Server  Database
1   A       POC1
2   B       POC2
3   V       POC3</p>
<p>And I use a Lookup function in ADF which contains the query:
select Server, Database from Config where id = 1</p>
<p>I then will get the results from this query to set the Server and Database in the Linked Service and Data Set so that I can issue another lookup such as</p>
<p>Select bank_name, bank_etl, bank_rules from bank_table (in the server/database I just looked up in the Config Table) - and continue along with the rest of the ADF process</p>
","<azure-data-factory>","2020-08-03 19:17:33","853","0","1","63243540","<p>You can try to use Switch activity.According to the Server Database in the Lookup activity's output,execute different activity.</p>
<p>Below is my test pipeline.</p>
<p><a href=""https://i.stack.imgur.com/hMzgn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hMzgn.png"" alt=""enter image description here"" /></a></p>
<p>Setting of Switch:</p>
<p><a href=""https://i.stack.imgur.com/fQMFb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fQMFb.png"" alt=""enter image description here"" /></a></p>
<p>You need to create source dataset before so that the second Lookup activity can use it as source.</p>
"
"63234686","When to use Data Factory (copy) over direct pull in SQL synapse","<p>I am just going through some Microsoft Document and doing handOn for Data engineering related things.
I have couple of queries for a scenrerio - &quot;copy CSV file(s) from Blob storage to Synapse analytics (stage table(s)):</p>
<p>I read that we can do direct data pull in Synapse with the process of creating external tables. (<a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/load-data-wideworldimportersdw"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/load-data-wideworldimportersdw</a>)</p>
<p>If above is possible, then in what cases we do use Azure Data factory Copy or data flow method?
While working with Azure data factory, is it a good idea to use Polybase, because it will use Blob storage again as staging in this scenrerio (i.e. I am copying file from Blob only and again using blob for staging)?</p>
<p>I searched for answers to my queries but haven't found any satisfactory answer yet.</p>
","<azure><azure-data-factory><data-warehouse><polybase><azure-synapse>","2020-08-03 18:06:09","178","0","1","63238468","<p>If you're just straight loading data from CSV into DW, use Copy. Polybase is recommended, but not always needed for small files.</p>
<p>If you need to transform that data or perform updates, then use data flows.</p>
"
"63232111","Logic App not able to deserialized Azure data factory pipeline error message","<p>I am facing the below issue with Azure Data Factory using Logic App.</p>
<p>I am using the Azure Data Factory pipeline for migration and Logic App for sending &quot;Success &amp; Failure&quot; notification to the technical team.
Now success is working fine as the message is hardcoded, but failure is not as the Logic App web activity is not able to parse data factory pipeline error.</p>
<p>Here is the input that is going to Logic App web activity
Input</p>
<pre><code>{
&quot;url&quot;: &quot;https://xxxxxxxxxxxxxxxxx&quot;,
&quot;method&quot;: &quot;POST&quot;,
&quot;headers&quot;: {},
&quot;body&quot;: &quot;{\n \&quot;title\&quot;: \&quot;PIPELINE RUN FAILED\&quot;,\n \&quot;message\&quot;:\&quot;Operation on target Migration Validation failed: Execution fail against sql server. Sql error number: 50000. Error Message: The DELETE statement conflicted with the REFERENCE constraint \&quot;FK_cmclientapprovedproducts_cmlinkclientchannel\&quot;. The conflict occurred in database \&quot;Core7\&quot;, table \&quot;dbo.cmClientApprovedProducts\&quot;, column 'linkclientchannelid'.\&quot;,\n \&quot;color\&quot;: \&quot;Red\&quot;,\n \&quot;dataFactoryName\&quot;: \&quot;LFC-TO-MCP-ADF\&quot;,\n \&quot;pipelineName\&quot;: \&quot;LFC TO MCP MIGRATION\&quot;,\n \&quot;pipelineRunId\&quot;: \&quot;f4f84365-58f0-4da1-aa00-64c3a4daa9e1\&quot;,\n \&quot;time\&quot;: \&quot;2020-07-31T22:44:01.6477435Z\&quot;\n}&quot;
}
</code></pre>
<p>Here is the error logic app is throwing</p>
<pre><code>failures
    {
    &quot;errorCode&quot;: &quot;2108&quot;,
    &quot;message&quot;: &quot;{\&quot;error\&quot;:{\&quot;code\&quot;:\&quot;InvalidRequestContent\&quot;,\&quot;message\&quot;:\&quot;The request content is not valid and could not be deserialized: 'After parsing a value an unexpected character was encountered: F. Path 'message', line 3, position 202.'.\&quot;}}&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Send Failed Notification&quot;,
    &quot;details&quot;: []
    }
</code></pre>
<p>I have tried various options, like set variable and convert by using various existing methods (string, json, replace etc), but no luck
e.g <code>@string(activity('LOS migration').Error.Message)</code>
Struggling almost all day this...please suggest if anyone faced a similar issue...</p>
<p>Below is the data flow activity
<img src=""https://i.stack.imgur.com/tbK0c.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/fAu0J.png"" alt=""enter image description here"" /></p>
","<azure-logic-apps><azure-data-factory>","2020-08-03 15:13:44","2004","1","3","63242154","<p>For the failure case, pass the error output use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-control-flow"" rel=""nofollow noreferrer""><code>@{activity('LOS migration').error.message</code></a>.</p>
<p>For sending email, it doesn't know if it's going to send a failure or success email. We have to adapt the body so the activity can use parameters, which we'll define later:</p>
<pre><code>{
   &quot;DataFactoryName&quot;: &quot;@{pipeline().DataFactory}&quot;,
   &quot;PipelineName&quot;: &quot;@{pipeline().Pipeline}&quot;,
   &quot;Subject&quot;: &quot;@{pipeline().parameters.Subject}&quot;,
   &quot;ErrorMessage&quot;: &quot;@{pipeline().parameters.ErrorMessage}&quot;,
   &quot;EmailTo&quot;: &quot;@pipeline().parameters.EmailTo&quot;
}
</code></pre>
<p>We can reference this variables in the body by using the following format: <code>@pipeline().parameters.parametername</code>. For more details, you could refer to this <a href=""https://www.mssqltips.com/sqlservertip/5962/send-notifications-from-an-azure-data-factory-pipeline--part-2/"" rel=""nofollow noreferrer"">article</a>.</p>
"
"63232111","Logic App not able to deserialized Azure data factory pipeline error message","<p>I am facing the below issue with Azure Data Factory using Logic App.</p>
<p>I am using the Azure Data Factory pipeline for migration and Logic App for sending &quot;Success &amp; Failure&quot; notification to the technical team.
Now success is working fine as the message is hardcoded, but failure is not as the Logic App web activity is not able to parse data factory pipeline error.</p>
<p>Here is the input that is going to Logic App web activity
Input</p>
<pre><code>{
&quot;url&quot;: &quot;https://xxxxxxxxxxxxxxxxx&quot;,
&quot;method&quot;: &quot;POST&quot;,
&quot;headers&quot;: {},
&quot;body&quot;: &quot;{\n \&quot;title\&quot;: \&quot;PIPELINE RUN FAILED\&quot;,\n \&quot;message\&quot;:\&quot;Operation on target Migration Validation failed: Execution fail against sql server. Sql error number: 50000. Error Message: The DELETE statement conflicted with the REFERENCE constraint \&quot;FK_cmclientapprovedproducts_cmlinkclientchannel\&quot;. The conflict occurred in database \&quot;Core7\&quot;, table \&quot;dbo.cmClientApprovedProducts\&quot;, column 'linkclientchannelid'.\&quot;,\n \&quot;color\&quot;: \&quot;Red\&quot;,\n \&quot;dataFactoryName\&quot;: \&quot;LFC-TO-MCP-ADF\&quot;,\n \&quot;pipelineName\&quot;: \&quot;LFC TO MCP MIGRATION\&quot;,\n \&quot;pipelineRunId\&quot;: \&quot;f4f84365-58f0-4da1-aa00-64c3a4daa9e1\&quot;,\n \&quot;time\&quot;: \&quot;2020-07-31T22:44:01.6477435Z\&quot;\n}&quot;
}
</code></pre>
<p>Here is the error logic app is throwing</p>
<pre><code>failures
    {
    &quot;errorCode&quot;: &quot;2108&quot;,
    &quot;message&quot;: &quot;{\&quot;error\&quot;:{\&quot;code\&quot;:\&quot;InvalidRequestContent\&quot;,\&quot;message\&quot;:\&quot;The request content is not valid and could not be deserialized: 'After parsing a value an unexpected character was encountered: F. Path 'message', line 3, position 202.'.\&quot;}}&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Send Failed Notification&quot;,
    &quot;details&quot;: []
    }
</code></pre>
<p>I have tried various options, like set variable and convert by using various existing methods (string, json, replace etc), but no luck
e.g <code>@string(activity('LOS migration').Error.Message)</code>
Struggling almost all day this...please suggest if anyone faced a similar issue...</p>
<p>Below is the data flow activity
<img src=""https://i.stack.imgur.com/tbK0c.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/fAu0J.png"" alt=""enter image description here"" /></p>
","<azure-logic-apps><azure-data-factory>","2020-08-03 15:13:44","2004","1","3","63256255","<p>now it is working...
Pasting body content into the body text field box WITHOUT clicking on 'Add Dynamic Content' in web activity calling Logic App.</p>
"
"63232111","Logic App not able to deserialized Azure data factory pipeline error message","<p>I am facing the below issue with Azure Data Factory using Logic App.</p>
<p>I am using the Azure Data Factory pipeline for migration and Logic App for sending &quot;Success &amp; Failure&quot; notification to the technical team.
Now success is working fine as the message is hardcoded, but failure is not as the Logic App web activity is not able to parse data factory pipeline error.</p>
<p>Here is the input that is going to Logic App web activity
Input</p>
<pre><code>{
&quot;url&quot;: &quot;https://xxxxxxxxxxxxxxxxx&quot;,
&quot;method&quot;: &quot;POST&quot;,
&quot;headers&quot;: {},
&quot;body&quot;: &quot;{\n \&quot;title\&quot;: \&quot;PIPELINE RUN FAILED\&quot;,\n \&quot;message\&quot;:\&quot;Operation on target Migration Validation failed: Execution fail against sql server. Sql error number: 50000. Error Message: The DELETE statement conflicted with the REFERENCE constraint \&quot;FK_cmclientapprovedproducts_cmlinkclientchannel\&quot;. The conflict occurred in database \&quot;Core7\&quot;, table \&quot;dbo.cmClientApprovedProducts\&quot;, column 'linkclientchannelid'.\&quot;,\n \&quot;color\&quot;: \&quot;Red\&quot;,\n \&quot;dataFactoryName\&quot;: \&quot;LFC-TO-MCP-ADF\&quot;,\n \&quot;pipelineName\&quot;: \&quot;LFC TO MCP MIGRATION\&quot;,\n \&quot;pipelineRunId\&quot;: \&quot;f4f84365-58f0-4da1-aa00-64c3a4daa9e1\&quot;,\n \&quot;time\&quot;: \&quot;2020-07-31T22:44:01.6477435Z\&quot;\n}&quot;
}
</code></pre>
<p>Here is the error logic app is throwing</p>
<pre><code>failures
    {
    &quot;errorCode&quot;: &quot;2108&quot;,
    &quot;message&quot;: &quot;{\&quot;error\&quot;:{\&quot;code\&quot;:\&quot;InvalidRequestContent\&quot;,\&quot;message\&quot;:\&quot;The request content is not valid and could not be deserialized: 'After parsing a value an unexpected character was encountered: F. Path 'message', line 3, position 202.'.\&quot;}}&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Send Failed Notification&quot;,
    &quot;details&quot;: []
    }
</code></pre>
<p>I have tried various options, like set variable and convert by using various existing methods (string, json, replace etc), but no luck
e.g <code>@string(activity('LOS migration').Error.Message)</code>
Struggling almost all day this...please suggest if anyone faced a similar issue...</p>
<p>Below is the data flow activity
<img src=""https://i.stack.imgur.com/tbK0c.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/fAu0J.png"" alt=""enter image description here"" /></p>
","<azure-logic-apps><azure-data-factory>","2020-08-03 15:13:44","2004","1","3","68539009","<p>If you want to use the direct error message of the data factory activity as an input to the logic app email expression, you could try.</p>
<pre><code>&quot;ErrorMessage&quot;: &quot;@{string(replace(activity('activity_name').Error.Message, '&quot;',''''))}&quot;
</code></pre>
<p>Replace 'activity_name' with your failing activity name.</p>
"
"63229984","Pipeline Runs - Query By Factory is not working with Webhook","<p>I am using azure datafactory WebHook to get detial of Pipeline Runs - Query By Factory using following link.
POST <a href=""https://management.azure.com/subscriptions/%7BsubscriptionId%7D/resourceGroups/%7BresourceGroupName%7D/providers/Microsoft.DataFactory/factories/%7BfactoryName%7D/queryPipelineRuns?api-version=2018-06-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/queryPipelineRuns?api-version=2018-06-01</a>
I filled up all the required variable eg subcriptionId etc i works on postman using my credentials. but i need to run this using MSI and from datafactory (currently using webhook).</p>
<p>Using MSI authentication. my datafactory have owner role.
<a href=""https://i.stack.imgur.com/7qFAZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7qFAZ.png"" alt=""enter image description here"" /></a></p>
<p>Then made a post request from datafactory using webhook. tried both option by selecting callback and without call back.
<a href=""https://i.stack.imgur.com/39a7Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/39a7Y.png"" alt=""enter image description here"" /></a></p>
<p>When i run that its take long time (10 min ) and status shows that its timeout.</p>
<p><a href=""https://i.stack.imgur.com/X4mTE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X4mTE.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-managed-identity><azure-data-factory>","2020-08-03 13:03:54","215","0","1","63341116","<p>Hi after spending much time to fix it i found that MSI authentication is not available in WebHook. Instead Web will do that business.
So web Activity is the right guy.</p>
"
"63226766","Iterate through files in Data factory","<p>I have a Datalake gen 1 with folder structure /Test/{currentyear}/<strong>{Files}</strong></p>
<p><strong>{Files}</strong> Example format
2020-07-29.csv
2020-07-30.csv
2020-07-31.csv</p>
<p>Every day one new file gets added to the folder.</p>
<p>I need to create ADF to load the files in the SQL server.
COnditions</p>
<ol>
<li>When my ADF runs for the first time it needs to iterate all files and load into sql server</li>
<li>When ADF executing starting from second time( daily once) it needs to pick up only todays file and load into SQL server</li>
</ol>
<p>Can anyone tell me how to design ADF with above conditions</p>
","<azure><azure-data-factory>","2020-08-03 09:18:43","639","1","1","63257856","<p>This should be designed as two part.</p>
<blockquote>
<p>When my ADF runs for the first time it needs to iterate all files and
load into sql server</p>
</blockquote>
<p>You should create a temporary pipeline to acheieve this.(I think you know how to do this, so this part I will not talk about.)</p>
<blockquote>
<p>When ADF executing starting from second time( daily once) it needs to
pick up only todays file and load into SQL server</p>
</blockquote>
<p>So this needs you to create another pipeline which is continuous running.</p>
<p>Two points to acheive this:</p>
<p><strong>First,</strong> trigger this pipeline by event trigger.(When the file is upload, trigger this pipeline.).</p>
<p><a href=""https://i.stack.imgur.com/nIs7k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nIs7k.png"" alt=""enter image description here"" /></a></p>
<p><strong>Second,</strong> filter the file by specific format:</p>
<p><a href=""https://i.stack.imgur.com/HBjxF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HBjxF.png"" alt=""enter image description here"" /></a></p>
<p>For your requirement, the expression should be <code>@{formatDateTime(utcnow(),'yyyy-MM-dd')}</code>.</p>
<p>On my side, I can do that successful. Please have a try on your side.</p>
"
"63222855","Azure Data Factory - Order of actions inside the Copy Activity","<p>I have a tricky question about the &quot;Copy Activity&quot; in ADF. Assume the following scenario:</p>
<ol>
<li>Source: an external API or an non-Azure database using hosted integration runtime.</li>
<li>Sink: an Azure SQL Server database.</li>
<li>The &quot;pre-copy Script&quot; field has a command to delete some data from the sink table (why deleting is out of scope of the discussion).</li>
</ol>
<p>When the pipeline runs, the connection with the source fails (due to a time-out, network issue, authentication, etc.)</p>
<p>The question is: will the pre-copy script run in this case? Or the script only runs after ADF successfully connected the source data store? I couldn't find any reference about it.</p>
<p>I can just try to simulate it and see what happens, but I'm hopping someone can save my time. :)</p>
<p>Thanks in advance!</p>
","<azure><azure-data-factory>","2020-08-03 02:09:36","304","0","1","63238792","<p>Per my experience about Data Factory, the pre-copy script won't run.</p>
<p>As I understand, we can consider it as a workflow, connect to source--&gt; get data from source--&gt;connect to sink--&gt;run the pre-copy script--&gt;write data to sink. No mater which step failed, data factory will stop run.</p>
"
"63211262","Create Dynamic Json on the go in expression functions inside mapping data flow (Azure Data Factory)","<p>I am trying to convert a csv file into json schema based collection. For json conversion, the only way I found was creating subcolumns for a column if you want hierarchy, Or you can use expressions like @(key1=value, @(key2=value2)) for json structures. What I want to do is Either  pick key1 or key2 as parameters from dataflow/pipeline Or use it in a loop over an array such as map(array,@(#item=#item+2)).
But It restricts me from doing that. However, I am able to traverse over values, But keys are something that needs to be only hardcoded. Please suggest a workaround if any.</p>
","<json><mapping><azure-data-factory><dataflow>","2020-08-02 00:25:34","395","0","1","63223186","<p>In my opinion:</p>
<blockquote>
<p>1: You can import the csv file into Azure SQL Database by using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal"" rel=""nofollow noreferrer"">copy activity</a><br />
2: Then you can <a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/json-features"" rel=""nofollow noreferrer"">work with JSON features in Azure SQL Database</a></p>
</blockquote>
"
"63209453","Create JSON file from a query with FOR JSON clause result in ADF","<p>I need to create a JSON file from azure SQL database and store the file in Azure blob storage.
In ADF, I created a simple pipeline with one Copy Data activity to achieve this.
I used t-sql query with FOR JSON clause to get data from the database.</p>
<pre><code>SELECT * FROM stage.Employee FOR JSON AUTO, ROOT ('main_root')
</code></pre>
<p>Here is my source:
<a href=""https://i.stack.imgur.com/3NaN5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3NaN5.png"" alt=""enter image description here"" /></a></p>
<p>And this is a sink:
<a href=""https://i.stack.imgur.com/F235b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F235b.png"" alt=""enter image description here"" /></a></p>
<p>After execute pipeline, the created file looks like this
<a href=""https://i.stack.imgur.com/8fd6U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8fd6U.png"" alt=""enter image description here"" /></a></p>
<p>I want to get a normal JSON file with the structure</p>
<pre><code>{
&quot;main_root&quot;: [
    {
        &quot;Employee_No&quot;: &quot;1000&quot;,
        &quot;Status&quot;: &quot;Employee&quot;,
        &quot;...&quot; &quot;...&quot;,
        &quot;...&quot;
    },
    {
        &quot;Employee_No&quot;: &quot;1000&quot;,
        &quot;Status&quot;: &quot;Employee&quot;,
        &quot;...&quot; &quot;...&quot;,
        &quot;...&quot;
    },
    {
        &quot;Employee_No&quot;: &quot;1000&quot;,
        &quot;Status&quot;: &quot;Employee&quot;,
        &quot;...&quot; 
      &quot;...&quot;,
        &quot;...&quot;
</code></pre>
<p>Any help I will appreciate.</p>
","<json><sql-server><azure><azure-data-factory>","2020-08-01 19:51:36","1453","0","1","63212097","<p>You are building a hierarchical structure from a relation source, so you'll want to build your R2H logic in data flows to accommodate this data transformation.</p>
<p>Set the SQL DB table as your source and then build your hierarchical structure in a Derived Column with sub-columns for hierarchies and collect data into arrays using Aggregate with the collect() function.</p>
"
"63207259","Azure Datalake Analytics U-SQL with Azure Datalake Storage Gen 2","<blockquote>
<p>Question : what is the path forward for using ADLA (U-SQL) with ADLS(Gen2) ?</p>
</blockquote>
<p>I have been running Azure Data lake Analytics (U-SQL) jobs via Azure Data factory (ADF v2) with Azure Data lake Store Generation 1 for quite a while now in East US2</p>
<p>I was planning to have another instance deployed to cater Canadian clients and wanted to setup Azure Data lake Store Generation 1</p>
<h2>What I tried :</h2>
<ul>
<li>I was not able to create an Azure Datalake Storage Gen 1 account in Central Canada (or any Canadian region for that matter)</li>
<li>I tried to move to Azure Datalake Storage Gen2 but then ran into an issue where Azure Data Factory - U-SQL activity could not be linked with Gen2 Storage linked service to pick up U-SQL script</li>
</ul>
<p>I stumbled upon multiple links about this topic :</p>
<ul>
<li><a href=""https://feedback.azure.com/forums/327234-data-lake/suggestions/36445702-add-support-for-adls-gen2-to-adla"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/327234-data-lake/suggestions/36445702-add-support-for-adls-gen2-to-adla</a></li>
<li><a href=""https://social.msdn.microsoft.com/Forums/en-US/5ce97eef-8940-4591-a19c-934f71825e7d/connect-data-lake-analytics-to-adls-gen-2"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/5ce97eef-8940-4591-a19c-934f71825e7d/connect-data-lake-analytics-to-adls-gen-2</a></li>
</ul>
<p>which essentially say that U-SQL / ADLA won't be supporting ADLS Gen2</p>
<p>I am a bit confused since there is no official documentation on ADLA's direction</p>
","<azure-data-lake><azure-data-factory><u-sql><azure-data-lake-gen2>","2020-08-01 16:03:28","499","1","1","63226733","<p><strong>Update:</strong></p>
<p>This is the structure of my u-sql activity. It can work and process successfully:(You can try to create a new json of u-sql activity to replace your u-sql activity.)</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline4&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;U-SQL1&quot;,
                &quot;type&quot;: &quot;DataLakeAnalyticsU-SQL&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;scriptPath&quot;: &quot;test1/u-sql.txt&quot;,
                    &quot;scriptLinkedService&quot;: {
                        &quot;referenceName&quot;: &quot;LinkTo0730&quot;,
                        &quot;type&quot;: &quot;LinkedServiceReference&quot;
                    }
                },
                &quot;linkedServiceName&quot;: {
                    &quot;referenceName&quot;: &quot;AzureDataLakeAnalytics1&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                }
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>Original Answer:</strong></p>
<blockquote>
<p>I was not able to create an Azure Datalake Storage Gen 1 account in
Central Canada (or any Canadian region for that matter)</p>
</blockquote>
<p>On my side, I also cannot create datalake gen1 on region Central Canada. This is the limit of my subscription. But you can have a check of the resource manager on your side, maybe you can.(Azure data lake gen1 is 'Microsoft.DataLakeStore')</p>
<p><img src=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/media/resource-providers-and-types/show-locations.png"" alt=""1"" /></p>
<p><strong>Resource Manager is supported in all regions, but the resources you deploy might not be supported in all regions. In addition, there may be limitations on your subscription that prevent you from using some regions that support the resource. The resource explorer displays valid locations for the resource type.</strong></p>
<p>Please have a check of this document:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-providers-and-types"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-providers-and-types</a></p>
<blockquote>
<p>I tried to move to Azure Datalake Storage Gen2 but then ran into an
issue where Azure Data Factory - U-SQL activity could not be linked
with Gen2 Storage linked service to pick up U-SQL script</p>
</blockquote>
<p>On my side, seems it is reading the u-sql script in gen2, did you get some error?</p>
"
"63204428","How to call Oracle stored procedure from azure data factory v2","<p>My requirement is copy data from Oracle to SQL Server. Before copying from Oracle database, I need to update the Oracle table using procedure which has some logic.</p>
<p>How do I execute Oracle stored procedure from Azure datafactory?</p>
<p>I referred <a href=""https://stackoverflow.com/questions/51591123/oracle-store-procedure-in-azure-data-factory-v2"">to this thread</a></p>
<p>if I use <code>EXECUTE PROC_NAME (PARAM);</code> in <code>preCopy</code> script it's failing with following error</p>
<pre><code>Failure happened on 'Source' side. 
ErrorCode=UserErrorOdbcOperationFailed,
Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException
Message=ERROR [42000] [Microsoft][ODBC Oracle Wire Protocol driver]
[Oracle]ORA-00900: invalid SQL statement
Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,
Type=System.Data.Odbc.OdbcException
Message=ERROR [42000] [Microsoft][ODBC Oracle Wire Protocol driver]
[Oracle]ORA-00900: invalid SQL statement,Source=msora28.dll
</code></pre>
<p>Could anyone help on this?</p>
<p>Note: I am using self-hosted runtime environment for data factory
thanks!!</p>
","<oracle><stored-procedures><self-hosting><azure-data-factory>","2020-08-01 10:56:50","3421","1","3","63221938","<p>In Oracle, <code>EXECUTE X(Y)</code> is a SQL*Plus-specific command shortcut for the PL/SQL statement <code>BEGIN X(Y); END;</code>. Since you are not using SQL*Plus, try the BEGIN/END syntax.</p>
"
"63204428","How to call Oracle stored procedure from azure data factory v2","<p>My requirement is copy data from Oracle to SQL Server. Before copying from Oracle database, I need to update the Oracle table using procedure which has some logic.</p>
<p>How do I execute Oracle stored procedure from Azure datafactory?</p>
<p>I referred <a href=""https://stackoverflow.com/questions/51591123/oracle-store-procedure-in-azure-data-factory-v2"">to this thread</a></p>
<p>if I use <code>EXECUTE PROC_NAME (PARAM);</code> in <code>preCopy</code> script it's failing with following error</p>
<pre><code>Failure happened on 'Source' side. 
ErrorCode=UserErrorOdbcOperationFailed,
Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException
Message=ERROR [42000] [Microsoft][ODBC Oracle Wire Protocol driver]
[Oracle]ORA-00900: invalid SQL statement
Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,
Type=System.Data.Odbc.OdbcException
Message=ERROR [42000] [Microsoft][ODBC Oracle Wire Protocol driver]
[Oracle]ORA-00900: invalid SQL statement,Source=msora28.dll
</code></pre>
<p>Could anyone help on this?</p>
<p>Note: I am using self-hosted runtime environment for data factory
thanks!!</p>
","<oracle><stored-procedures><self-hosting><azure-data-factory>","2020-08-01 10:56:50","3421","1","3","65622176","<p>I used a Lookup Activity and a SELECT statement of DUAL TABLE. Due to the stored procedures can not be call from a statement SELECT. I created an oracle function and the function calls the stored procedure. The function returns a value and this value is received by the lookup activity.
When you define the function, you have to add the statement PRAGMA AUTONOMOUS_TRANSACTION. This is because Oracle does not allow to execute DML instructions with a SELECT statement by default. Then, you need to define that DML instructions in the Stored Procedure will be an autonomous transaction.</p>
<pre><code>--Tabla
CREATE TABLE empleados(
   emp_id NUMBER(9),
   nombre VARCHAR2(100),
   CONSTRAINT empleados_pk PRIMARY KEY(emp_id),
);

create or replace procedure insert_empleado (numero in NUMBER, nombre in VARCHAR2) is
 begin
    INSERT INTO empleados (emp_id, nombre)
    Values(numero, nombre);
COMMIT;
end;

create or replace function funcinsert_empleado (numero in NUMBER, nombre in VARCHAR2)
return VARCHAR2 
is
PRAGMA AUTONOMOUS_TRANSACTION;
begin
  insert_empleado (numero, nombre);
  return 'done';
end;
--statement in query of lookup
SELECT  funcinsert_empleado ('1', 'Roger Federer') 
FROM DUAL;
</code></pre>
<p><a href=""https://i.stack.imgur.com/K1qPC.png"" rel=""nofollow noreferrer"">Example lookup</a></p>
<p>This is example in Spanish. <a href=""https://dev.to/maritzag/ejecutar-un-stored-procedure-de-oracle-desde-data-factory-2jcp"" rel=""nofollow noreferrer"">https://dev.to/maritzag/ejecutar-un-stored-procedure-de-oracle-desde-data-factory-2jcp</a></p>
"
"63204428","How to call Oracle stored procedure from azure data factory v2","<p>My requirement is copy data from Oracle to SQL Server. Before copying from Oracle database, I need to update the Oracle table using procedure which has some logic.</p>
<p>How do I execute Oracle stored procedure from Azure datafactory?</p>
<p>I referred <a href=""https://stackoverflow.com/questions/51591123/oracle-store-procedure-in-azure-data-factory-v2"">to this thread</a></p>
<p>if I use <code>EXECUTE PROC_NAME (PARAM);</code> in <code>preCopy</code> script it's failing with following error</p>
<pre><code>Failure happened on 'Source' side. 
ErrorCode=UserErrorOdbcOperationFailed,
Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException
Message=ERROR [42000] [Microsoft][ODBC Oracle Wire Protocol driver]
[Oracle]ORA-00900: invalid SQL statement
Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,
Type=System.Data.Odbc.OdbcException
Message=ERROR [42000] [Microsoft][ODBC Oracle Wire Protocol driver]
[Oracle]ORA-00900: invalid SQL statement,Source=msora28.dll
</code></pre>
<p>Could anyone help on this?</p>
<p>Note: I am using self-hosted runtime environment for data factory
thanks!!</p>
","<oracle><stored-procedures><self-hosting><azure-data-factory>","2020-08-01 10:56:50","3421","1","3","68621522","<p><strong>In case you only want to execute the DML query using the Azure Data Factory without procedure on oracle database  :-</strong></p>
<p>I have another solution where you can use the copy activity with the pre-copy feature of sink in-spite of lookup activity.</p>
<p>For this approach just follow the below steps :-</p>
<ol>
<li>Keep both the source table and sink table as same ( Let say table A ) using the same linked service.</li>
<li>In sink use the pre-copy script feature and keep the DML (Insert/Update/Delete ) query that you want to perform over the table B.( This table is not necessary to be same as table A )</li>
<li>In case you want to avoid the copy of data to same table you can select query option in the source part and provide a where clause which is not going to satisfy and hence no copy of data will happen .
or you can create a table temp with one column and one row .
I have tested both the options and it works ... good part of above solution is you can avoid the procedure or function creation and maintenance .</li>
</ol>
"
"63203479","Not able to set Status of imported products to ""Active"" in Dynamics 365 sales","<p>I am trying to import product table data using Azure data factory to Dynamics 365 sales and it is being imported successfully. However all the products are in draft state, even though I had set the statecode  (Status) value to 0 as in the documentation, 0 is supposed to be the value for Active.</p>
","<dynamics-crm><azure-data-factory><dynamics-365><dynamics-crm-365><dynamics-365-sales>","2020-08-01 08:58:08","530","2","2","63206144","<p>You can try to update both &quot;statecode&quot; and &quot;statuscode&quot; with valid values as they are related to each other, maybe &quot;statuscode&quot; holds non valid value that prevents you from updating the &quot;statecode.</p>
<p>check the below URL for more info:
<a href=""https://powerobjects.com/2020/03/11/how-to-programmatically-set-the-status-code-field-of-a-dynamic-entity/"" rel=""nofollow noreferrer"">https://powerobjects.com/2020/03/11/how-to-programmatically-set-the-status-code-field-of-a-dynamic-entity/</a></p>
"
"63203479","Not able to set Status of imported products to ""Active"" in Dynamics 365 sales","<p>I am trying to import product table data using Azure data factory to Dynamics 365 sales and it is being imported successfully. However all the products are in draft state, even though I had set the statecode  (Status) value to 0 as in the documentation, 0 is supposed to be the value for Active.</p>
","<dynamics-crm><azure-data-factory><dynamics-365><dynamics-crm-365><dynamics-365-sales>","2020-08-01 08:58:08","530","2","2","63211441","<p>Dynamics CRM OOB Product entity is little different. You have to publish the created products in draft by default.</p>
<p>You can change the system wide setting though.</p>
<blockquote>
<p><a href=""https://i.stack.imgur.com/UFPpJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UFPpJ.jpg"" alt=""enter image description here"" /></a></p>
</blockquote>
"
"63200198","Get credentials for DataFactoryManagementClient","<p>I have code that generates credentials to pass to a DataFactoryManagementClient. It uses an app registration to obtain credentials, as I had started it in a console app.</p>
<pre><code>    public DataFactoryClient(string tenantId, string subscriptionId, string applicationId, string clientSecret)
    {
        AuthenticationContext context = new AuthenticationContext(&quot;https://login.windows.net/&quot; + tenantId);
        ClientCredential clientCredential = new ClientCredential(applicationId, clientSecret);
        AuthenticationResult result = context.AcquireTokenAsync(
            &quot;https://management.azure.com/&quot;, clientCredential).Result;
        ServiceClientCredentials credentials = new TokenCredentials(result.AccessToken);
        client = new DataFactoryManagementClient(credentials)
        {
            SubscriptionId = subscriptionId
        };
    }
</code></pre>
<p>I moved the code to an Azure Function, now I want to get rid of the App Registration and just use the Managed Identity assigned automatically (I have enabled that)  I can't figure out how though... DataFactoryManagementClient expects a ServiceClientCredentials object but I have a ManagedIdentityCredential object.</p>
<p>Is there any way to do that?</p>
","<azure-functions><azure-data-factory><azure-managed-identity>","2020-07-31 23:16:21","1683","0","1","63222940","<p>If you want to use the system-assigned identity of the azure function with <code>DataFactoryManagementClient</code>, you could use <a href=""https://learn.microsoft.com/en-us/azure/key-vault/general/service-to-service-authentication#using-the-library"" rel=""nofollow noreferrer""><code>AzureServiceTokenProvider</code></a>, when you publish your code to the azure function, it will use the system-assigned identity of your function automatically.</p>
<p><strong>Note:</strong> When you use the code, please create a connection string <a href=""https://learn.microsoft.com/en-us/azure/key-vault/general/service-to-service-authentication#connection-string-support"" rel=""nofollow noreferrer""><code>AzureServicesAuthConnectionString</code></a> for your function app first, to use system-assigned identity, its value should be <code>RunAs=App</code>.</p>
<p>Sample(I test it in local to get an ADF, it automatically uses the VS sign-in account to auth, in azure function, it uses the system-assigned identity):</p>
<pre><code>using Microsoft.Azure.Management.DataFactory;
using Microsoft.Azure.Services.AppAuthentication;
using Microsoft.Rest;

namespace ConsoleApp11
{
    class Program
    {
        static void Main(string[] args)
        {
            AzureServiceTokenProvider azureServiceTokenProvider = new AzureServiceTokenProvider();
            string accessToken = azureServiceTokenProvider.GetAccessTokenAsync(&quot;https://management.azure.com/&quot;).Result;
            string subscriptionId = &quot;xxxxxxxx&quot;;
            ServiceClientCredentials credentials = new TokenCredentials(accessToken);
            var client = new DataFactoryManagementClient(credentials)
            {
                SubscriptionId = subscriptionId
            };
            var a = client.Factories.Get(&quot;group-name&quot;,&quot;joyfactory&quot;);
            System.Console.WriteLine(a);
        }
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/8tHxy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8tHxy.png"" alt=""enter image description here"" /></a></p>
"
"63199449","cannot be converted from original type 'System.Int64' to target type 'Microsoft.Xrm.Sdk.OptionSetValue'","<p>When importing data of product table to dynamics 365 using data factory, I am getting this error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Sink' side. ErrorCode=UserErrorTypeConversionFail,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The attribute 'statecode' with value '0' cannot be converted from original type 'System.Int64' to target type 'Microsoft.Xrm.Sdk.OptionSetValue'.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,''Type=System.InvalidCastException,Message=Specified cast is not valid.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy data1&quot;,
    &quot;details&quot;: [] }
</code></pre>
<p>This is for the field 'statecode' of which all values are set to 0 so that the status of the products being imported is active.
I have not been able to find a fix for this error.</p>
","<dynamics-crm><azure-data-factory><dynamics-365><dynamics-365-sales>","2020-07-31 21:42:24","279","0","1","63203434","<p>The issue got resolved. It was because I was importing data to Produkt (German name for Product) and my user interface language was English. I changed the language to German which is also the base language and changed the table name to Product. This solved my problem.</p>
"
"63199370","Lookup Activity and ForEach in Data Factory - Processing Nested JSON Values","<p>I am trying to understand if it's possible to read the nested JSON values in a Data Factory. I have a look up activity followed by a for each to process the values from the lookup. I can read the values in the for each using the '@activity('lu_config_readentities').output.value' in case of simple json object.</p>
<p>With the below example JSON value, I can read the Query value using @item().Query but @item.Query.Term fails with an error.</p>
<p>Is this a known limitation? Is there anyway I could read the values?</p>
<pre><code>JSON Sample: 

{
    &quot;count&quot;: 1,
    &quot;value&quot;: [
        {
            &quot;Id&quot;: 63,
            &quot;Source&quot;: &quot;xxx&quot;,
            &quot;EntityName&quot;: &quot;test&quot;,
            &quot;Query&quot;: {
                        &quot;Term&quot;:&quot;science&quot;,
                        &quot;Database&quot;:&quot;nih&quot;
                     }
        }
    ],
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (Central US)&quot;,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;DIUHours&quot;
            }
        ]
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 11
    }
}

Error output: 

  &quot;message&quot;: &quot;The expression 'item().Query.Term' cannot be evaluated because property 'Term' cannot be selected. Property selection is not supported on values of type 'String'.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Set variable1&quot;

</code></pre>
","<azure-data-factory>","2020-07-31 21:35:18","581","1","1","63224429","<p>You can follow below steps, I can success get the value in nested json.</p>
<p>This is my Lookup activity:</p>
<p><a href=""https://i.stack.imgur.com/JFLZV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JFLZV.png"" alt=""enter image description here"" /></a></p>
<p>Then you can use name to get the value directly:</p>
<p><a href=""https://i.stack.imgur.com/dCpHh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dCpHh.png"" alt=""enter image description here"" /></a></p>
<p>This is your json:</p>
<pre><code>{
            &quot;Id&quot;: 63,
            &quot;Source&quot;: &quot;xxx&quot;,
            &quot;EntityName&quot;: &quot;test&quot;,
            &quot;Query&quot;: {
                        &quot;Term&quot;:&quot;science&quot;,
                        &quot;Database&quot;:&quot;nih&quot;
                     }
}
</code></pre>
<p>This is the output of the Lookup activity on my side:</p>
<pre><code>{
    &quot;firstRow&quot;: {
        &quot;Id&quot;: 63,
        &quot;Source&quot;: &quot;xxx&quot;,
        &quot;EntityName&quot;: &quot;test&quot;,
        &quot;Query&quot;: {
            &quot;Term&quot;: &quot;science&quot;,
            &quot;Database&quot;: &quot;nih&quot;
        }
    },
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (Central US)&quot;,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;DIUHours&quot;
            }
        ]
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 10
    }
}
</code></pre>
<p>And this is the expression should be(On my side, lookup activity is named Lookup1):</p>
<pre><code>activity('Lookup1').output.firstRow.Query.Term
</code></pre>
<p>On my side I can get the nested value success, please have a try on your side.</p>
"
"63197465","How to process excel in azure data flow","<p>I have multiple excel files in a blob storage and sheet name in those files are different while loading in the files in azure sql database in azure data flow how the sheet name to be handled.
Note : I have to do it in azure data flow,i tried doing in azure pipeline its working</p>
","<azure><azure-data-factory>","2020-07-31 18:44:05","760","0","1","63222627","<p>Data factory supports Excel connector now, but still couldn't support dynamic choose the sheet index for now.</p>
<p>Others have added comment in <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/19807720-add-excel-as-source"" rel=""nofollow noreferrer"">Data Factory feedback</a> but didn't get reply:
<a href=""https://i.stack.imgur.com/ukG6U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ukG6U.png"" alt=""enter image description here"" /></a></p>
<p>I think you also could post a <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">new feedback</a>, we could help you vote up it and Data Factory Product Team may see it.</p>
"
"63192880","Azure Data Factory - Tumbling Window Trigger - Limit hours it is running","<p>With an Azure Data Factory &quot;Tumbling Window&quot; trigger, is it possible to limit the hours of each day that it triggers during (adding a window you might say)?</p>
<p>For example I have a Tumbling Window trigger that runs a pipeline every 15 minutes. This is currently running 24/7 but I'd like it to only run during business hours (0700-1900) to reduce costs.</p>
<p><strong>Edit:</strong></p>
<p>I played around with this, and found another option which isn't ideal from a monitoring perspective, but it appears to work:</p>
<p>Create a new pipeline with a single &quot;If Condition&quot; step with a dynamic Expression like this:</p>
<p><code>@and(greater(int(formatDateTime(utcnow(),'HH')),6),less(int(formatDateTime(utcnow(),'HH')),20))</code></p>
<ul>
<li>In the true case activity, add an Execute Pipeline step executing your original pipeline (with &quot;Wait on completion&quot; ticked)</li>
<li>In the false case activity, add a wait step which sleeps for X minutes</li>
</ul>
<p>The longer you sleep for, the longer you can possibly encroach on your window, so adjust that to match.</p>
<p>I need to give it a couple of days before I check the billing on the portal to see if it has reduced costs. At the moment I'm assuming a job which just sleeps for 15 minutes won't incur the costs that one running and processing data would.</p>
","<azure><azure-data-factory>","2020-07-31 13:33:28","699","1","1","63204539","<p>there is no easy way but you can create two deployment pipelines for the same job in Azure devops and as soon as your winodw 0700 to 1900 expires you replace that job with a dummy job using azure dev ops pipeline.</p>
"
"63182755","Azure Data Factory deployed via ARM Template","<p>I deploy successfully. The pipeline has a Copy step copying a csv file from a linked storage account to a linked Azure SQL database.
The deployed version fails with &quot;Error 2012,Value cannot be null. Parameter name: dictionary&quot;.</p>
<p>I can manually rebuild the Copy and it works fine but I cannot determine how to fix my template. I've tried exporting the successful run and looking at the code via {}, but so far no luck.</p>
","<azure><azure-data-factory><azure-rm-template>","2020-07-30 23:22:36","797","0","1","63339296","<p>In the Linked AzureSqlDatabase service, typeProperties contained a Connection String and &quot;type&quot;: &quot;SecureString&quot;. I'm not sure why that was ever inserted. However, the fact that the LinkedService connection tested successfully, made me ignore the connection string.</p>
<p>Once I detected its presence and removed it, all worked properly.</p>
"
"63180287","Azure Data Factory - REST to Azure SQL with filter","<p>What is the best approach to perform the following task within the Azure Data Factory:</p>
<ol>
<li>Call Rest-API and get json as a response</li>
<li>Parse json and copy some of the values to one Azure SQL-table (according to some filter) and the other values to another SQL-table.</li>
</ol>
<p>I guess, it is not possible to perform this in one step by using the copy-activity.
So a kind of staging-step might be needed.
What could it be?</p>
","<json><api><rest><azure-sql-database><azure-data-factory>","2020-07-30 19:43:16","268","0","1","63184521","<p>Please try this:</p>
<p>1.create a copy activity,source：REST sink:blob,copy REST response to blob storage json file.</p>
<p>2.create a dataflow,source: json file in blob storage</p>
<p>then create a <code>conditional split</code>, according to filter to different sink.</p>
<p>Hope this can help you.</p>
"
"63179625","Copy data from azure blob storage to adls gen 2","<p>I have around 2 million json files in azure blob storage.Each file contains one record.I need to move all those json files to adls gen 2 whose createdate is greater than 2019-01-01.</p>
<p>Note:createdate is one of the field inside json.</p>
<p>Is it possible to achieve this through azure data factory</p>
","<azure><azure-data-factory><azure-databricks><azure-data-lake-gen2>","2020-07-30 18:56:40","762","0","1","63186557","<p>Yes, you need to use query to do this. First all of please put the data into some space that can do data analytics.</p>
<p>First, create a <strong>copy activity1</strong>(copy data from azure blob storage to sql database), then create a <strong>copy activity2</strong>(copy data from sql database to adls gen2.)</p>
<p><a href=""https://i.stack.imgur.com/O792R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O792R.png"" alt=""enter image description here"" /></a></p>
<p>In the source of the copy activity2, please do like this:</p>
<p><a href=""https://i.stack.imgur.com/1U2g1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1U2g1.png"" alt=""enter image description here"" /></a></p>
<pre><code>select *  
from dbo.test1 A 
where A.createdate &gt;= '2019-01-01'
</code></pre>
<p>I have already do a test and it works fine. Please have a try.</p>
"
"63179058","Data factory SQL connection String with keyvault","<p>I exported arm template from Datafactory V2, when Importing the template it is asking me to manually enter SQL database connection string. To minimize the human interaction I made the following changes.</p>
<pre><code>        {
            &quot;name&quot;: &quot;[concat(parameters('factoryName'), '/myFactory')]&quot;,
            &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedServices&quot;,
            &quot;apiVersion&quot;: &quot;2018-06-01&quot;,
            &quot;properties&quot;: {
                &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                &quot;typeProperties&quot;: {
                    &quot;connectionString&quot;: &quot;[concat('Server=tcp:',parameters('sqlServerName'),'.database.windows.net,1433;Initial Catalog=', parameters('sqlDatabaseName'), ';Persist Security Info=False;User ID=',parameters('sqlServerUserName'),';Password=(password)',';MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30')]&quot;,
                    &quot;password&quot;: {
                        &quot;type&quot;: &quot;AzureKeyVaultSecret&quot;,
                        &quot;store&quot;: {
                            &quot;referenceName&quot;: &quot;AzureKeyVault1&quot;,
                            &quot;type&quot;: &quot;LinkedServiceReference&quot;
                        },
                        &quot;secretName&quot;: &quot;sql-password&quot;
                    }
                }
            },
            &quot;dependsOn&quot;: [
                &quot;[concat(variables('factoryId'), '/linkedServices/AzureKeyVault1')]&quot;
            ]
        },
</code></pre>
<p>So currently when deploying to Datafactory V2 and test connection to this SQL server, I got</p>
<pre><code>Cannot connect to SQL Database: 'tcp:mysqlserver.database.windows.net,1433', 
Database: 'mydatabase', User: 'admin'. Check the linked service configuration
is correct, and make sure the SQL Database firewall allows the integration runtime to access.
Login failed for user 'admin'., SqlErrorNumber=18456,
</code></pre>
<p>If I manually input all the connections in the portal UI, I can easily connect to the database and test successfully so it is not a firewall issue.</p>
<p>Then I think there could be 2 issue:</p>
<p>1.how the password from keyvault is consumed in the connectionstring. I didn't find much information about it online.</p>
<ol start=""2"">
<li>When I open the created Sql Linked service, I notice the Fully qualified domain name is missing,If i manually add it in then the connection works.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/iK57t.png"" rel=""nofollow noreferrer"">The SQL connection UI</a></p>
","<azure><azure-data-factory><azure-rm-template>","2020-07-30 18:18:37","1413","0","1","63179452","<p>Throwing this as an alternative answer/approach.</p>
<p>Store the connection string in it's entirety in Key Vault. If doing this then the reference would look like:</p>
<pre><code>{
  &quot;name&quot;: &quot;[concat(parameters('factoryName'), '/',parameters('connectionNameAdventureWorks'))]&quot;,
  &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedServices&quot;,
  &quot;apiVersion&quot;: &quot;2018-06-01&quot;,
  &quot;properties&quot;: {
    &quot;annotations&quot;: [],
    &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
    &quot;typeProperties&quot;: {
      &quot;connectionString&quot;: {
        &quot;type&quot;: &quot;AzureKeyVaultSecret&quot;,
        &quot;store&quot;: {
          &quot;referenceName&quot;: &quot;[variables('azkDataAnalyticsReferenceName')]&quot;,
          &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;secretName&quot;: &quot;[variables('azkAdventureWorksSecretName')]&quot;
      }
    }
  },
  &quot;dependsOn&quot;: [
     &quot;[concat(variables('factoryId'), '/linkedServices/',variables('azkDataAnalyticsReferenceName'))]&quot; 
  ]
}
</code></pre>
<p>And even more secure approach would be to add Data Factory as a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">Managed Identity</a> and then <a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-sql"" rel=""nofollow noreferrer"">run a sql script to add the user</a> If doing this then all there is no need for any credentials to be passed at all.</p>
<p>One downside is if the DataFactory is deleted and recreated then the managed identity permissions would need to be reassigned to the sql database.</p>
"
"63176552","Az Data Factory: The function 'int' was invoked with a parameter that is not valid","<p>How to convert a <code>string</code> to a <code>int</code> data type in Azure Data Factory <code>Data Flow</code> activity to set a parameter?</p>
<p>I have been trying to get a value from a <code>json</code> file in a gen2 data lake:</p>
<pre><code>{
    &quot;firstRow&quot;: {
        &quot;schema_name&quot;: &quot;my_schema&quot;,
        &quot;table_name&quot;: &quot;my_table&quot;,
        &quot;reserved_space_MB&quot;: 138.808,
        &quot;unused_space_MB&quot;: 1.392,
        &quot;data_space_MB&quot;: 136.848,
        &quot;index_space_MB&quot;: 0.568,
        &quot;row_count&quot;: 916300
    },
    ...
}
</code></pre>
<p>But got this error in the last activty:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;InvalidTemplate&quot;,
    &quot;message&quot;: &quot;The function 'int' was invoked with a parameter that is not valid. The value cannot be converted to the target type&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;_split_file_from_table&quot;,
    &quot;details&quot;: &quot;&quot;
}
</code></pre>
<p>I have been following the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">documentation</a> and also these stack overflow questions:<br />
<a href=""https://stackoverflow.com/questions/62944366/azure-data-factory-split-file-by-file-size/62955643#62955643"">Azure Data Factory split file by file size</a><br />
<a href=""https://stackoverflow.com/questions/61067826/convert-row-count-to-int-in-azure-data-factory"">Convert Row Count to INT in Azure Data Factory</a><br />
But I'm getting the same error no matter what I do.</p>
<p><strong>How to reproduce</strong></p>
<ol>
<li>Use a <code>Lookup</code> activity in a pipeline:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/Kz05E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kz05E.png"" alt="""" /></a></p>
<ol start=""2"">
<li>start variable <code>table_size_var</code> as a string data type</li>
</ol>
<p><a href=""https://i.stack.imgur.com/IeKXr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IeKXr.png"" alt="""" /></a></p>
<ol start=""3"">
<li><code>Set Variable</code> activity to get the <code>data_space_MB</code> value:<br />
<code>@string(activity('read_json').output.firstRow.data_space_MB)</code></li>
</ol>
<p><a href=""https://i.stack.imgur.com/A8ukh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A8ukh.png"" alt="""" /></a></p>
<ol start=""4"">
<li>Set parameter <code>table_size_mb</code> in a <code>Data Flow</code>:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/PSKPf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PSKPf.png"" alt="""" /></a></p>
<ol start=""5"">
<li>Set the value from the variable <code>table_size_var</code> to the parameter <code>table_size_mb</code>:<br />
<code>@int(variables('table_size_var'))</code></li>
</ol>
<p><a href=""https://i.stack.imgur.com/4YA0X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4YA0X.png"" alt="""" /></a></p>
<ol start=""6"">
<li>Run the pipeline - results:</li>
</ol>
<p><strong>lookup</strong>
<a href=""https://i.stack.imgur.com/1Co7i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Co7i.png"" alt="""" /></a></p>
<p><strong>Set Variable</strong>
<a href=""https://i.stack.imgur.com/T9eu1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T9eu1.png"" alt="""" /></a></p>
<p><strong>Data Flow</strong>
<a href=""https://i.stack.imgur.com/nK21G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nK21G.png"" alt="""" /></a></p>
","<azure><azure-data-factory>","2020-07-30 15:33:13","4459","0","1","63185039","<p>I tried and get the same error.</p>
<p>The <code>int()</code> function only works for covert int string to integer, the parameter must be an int String!
For example string <code>'100'</code> to integer <code>100</code>. It can not convert decimal string <code>'136.848'</code> to integer <code>136.848</code>.</p>
<p>I'm using <code>int()</code> and <code>split()</code> function to get integer <code>138</code> , then the pipeline works well.
Expression:</p>
<pre><code>@int(split(variables('num'),'.')[1])
</code></pre>
<ol>
<li>The variable will split into an array, <code>'138'</code> and <code>'848'</code>, using <code>[1]</code> to get the first element.</li>
<li>Then using <code>int()</code> to convert <code>'138'</code> to integer <code>138</code>.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/Zb4xi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zb4xi.png"" alt=""enter image description here"" /></a></p>
"
"63172386","Connect Azure Data Factory to Logic App with credentials","<p>I have created an Azure Data Factory that calls Logic App via &quot;Web&quot; action.
Logic app is activated by HTTP request. My issue is, I have not idea, how can I secure logic app with API service (all manuals show JWT token) and then use authentication options from the web action</p>
<p>Please, do you have an idea, what is the easiest secured way to implement this?</p>
<p><a href=""https://i.stack.imgur.com/7GbIe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7GbIe.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-logic-apps>","2020-07-30 11:38:10","273","1","1","63186290","<p>You can specify the IP address to access your logic app. Azure data factory now <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/20565967-static-ip-ranges-for-data-factory-and-add-adf-to-l"" rel=""nofollow noreferrer"">supports static IP address ranges</a>, so you can <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-azure-integration-runtime"" rel=""nofollow noreferrer"">create</a> an integration runtime. Then allow the IP address to request your logic app by setting as below:</p>
<p><a href=""https://i.stack.imgur.com/DkZGS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DkZGS.png"" alt=""enter image description here"" /></a></p>
"
"63169005","String function on filenames and adding as additional column","<p>I have CSV files in blob storage with underscore delimited filenames such as <code>100001_1036_1595841882.csv</code>. I want to push these CSVs into Azure Synapse but with columns added for each delimited field in the file name.</p>
<p>I've tried using the new &quot;Additional columns&quot; feature in the Copy activity, but somehow I can't use string functions with <code>$$FILEPATH</code> (see the image below). It's fine with just <code>$$FILEPATH</code> alone.</p>
<p><a href=""https://i.stack.imgur.com/tkKNI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tkKNI.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-07-30 08:18:47","831","1","1","63170857","<p>You can do this using the copy activity but you have to take a different approach then using the $$FILEPATH variable which is not possible to manipulate using the &quot;Add dynamic content' formula builder.</p>
<p>If you instead were to pass in the file name as a parameter to the pipeline you can manipulate it as you normally do. E.g if you have an event based trigger you can pull the filename from there.</p>
<p><a href=""https://i.stack.imgur.com/jbDtD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jbDtD.png"" alt=""enter image description here"" /></a></p>
<p>In the screenshot above you can see how I pulled out the different part of the filename using the <code>@split(pipeline().parameters.fileName, '_')[0]</code> function. I do that for each part and also add the <code>$$FILEPATH</code> to get the full filename (you could of course use the value from <code>@pipeline().parameters.fileName</code> instead). For the last part I also removed the file extension.</p>
<p>When you do the mapping between source and sink the additional columns will show up as any column in the source dataset.</p>
<p>Once the pipeline execution complete the SQL table contains all relevant data</p>
<p><a href=""https://i.stack.imgur.com/btV3j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/btV3j.png"" alt=""Screenshot of data once written to sink DB"" /></a></p>
"
"63161477","How to copy IoT Hub stored blobs to an Azure SQL using Data Factory","<p>We are using the IoT Hub routing feature to store messages into an Azure Blob container. By default it stores the messages in a hierarchical manner - creating a folder structure for year, month, day and so on. Within the folder for each day, it creates multiple block blob binary files. Each file may contain multiple JSON objects, each representing a unique IoT telemetry message.</p>
<p>How can I use Azure Data Factory to copy each of these messages into an Azure SQL database?</p>
<p><a href=""https://i.stack.imgur.com/953Xt.png"" rel=""nofollow noreferrer"">Screenshot from Azure Storage Explorer</a></p>
<p><a href=""https://i.stack.imgur.com/IrhoQ.png"" rel=""nofollow noreferrer"">A sample blob file containing multiple messages</a></p>
","<azure-sql-database><azure-data-factory><azure-iot-hub><azure-blob-storage>","2020-07-29 19:54:46","418","0","1","63168940","<p>It seams that all the files have the same json schema. Then you could follow my steps.</p>
<p>I created an folder <code>csv</code> in my container and have several csv files with json data:
<a href=""https://i.stack.imgur.com/R17Qu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R17Qu.png"" alt=""enter image description here"" /></a></p>
<p><strong>Source Dataset:</strong> the data in csv file is json format, so I choose the json format file.</p>
<ol>
<li>choose the container: test</li>
<li>import the schema(.json)</li>
</ol>
<p><a href=""https://i.stack.imgur.com/tiPBs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tiPBs.png"" alt=""enter image description here"" /></a></p>
<p><strong>Source setting:</strong> using wildcard file path to choose all the folder and file in the container.</p>
<p><a href=""https://i.stack.imgur.com/rG687.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rG687.png"" alt=""enter image description here"" /></a></p>
<p><strong>Sink setting:</strong>
<a href=""https://i.stack.imgur.com/St1OZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/St1OZ.png"" alt=""enter image description here"" /></a></p>
<p><strong>Mapping:</strong>
<a href=""https://i.stack.imgur.com/ESWmR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ESWmR.png"" alt=""enter image description here"" /></a></p>
<p>Run the pipeline and check the result in sink table:
<a href=""https://i.stack.imgur.com/kzFkc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kzFkc.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/fVApK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fVApK.png"" alt=""enter image description here"" /></a></p>
"
"63157740","Data Factory can't download CSV file from web API with Basic Auth","<p>I'm trying to download a CSV file from a website in Data Factory using the HTTP connector as my source linked service in a copy activity. It's basically a web call to a url that looks like <a href=""https://www.mywebsite.org/api/entityname.csv?fields=:all&amp;paging=false"" rel=""nofollow noreferrer"">https://www.mywebsite.org/api/entityname.csv?fields=:all&amp;paging=false</a>.</p>
<p>The website uses basic authentication. I have manually tested by using the url in a browser and entering the credentials, and everything works fine. I have used the REST connector in a copy activity to download the data as a JSON file (same url, just without the &quot;.csv&quot; in there), and that works fine. But there is something about the authentication in the HTTP connector that is different and causing issues. When I try to execute my copy activity, it downloads a csv file that contains the HTML for the login page on the source website.</p>
<p>While searching, I did come across this <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/14741#issuecomment-444074466"" rel=""nofollow noreferrer"">Github issue on the docs</a> that suggests that the basic auth header is not initially sent and that may be causing an issue.</p>
<p>As I have it now, the authentication is defined in the linked service. I'm hoping that maybe I can add something to the Additional Headers or Request Body properties of the source in my copy activity to make this work, but I haven't found the right thing yet.</p>
<p>Suggestions of things to try or code samples of a working copy activity using the HTTP connector and <strong>basic auth</strong> would be much appreciated.</p>
","<azure-data-factory>","2020-07-29 16:02:46","798","1","2","63171337","<p>I suggest you try to use the REST connector instead of the HTTP one. It supports Basic as authentication type and I have verified it using a test endpoint on HTTPbin.org</p>
<p><a href=""https://i.stack.imgur.com/vLyH3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vLyH3.png"" alt=""Screenshot of linked service configuration"" /></a></p>
<p>Above is the configuration for the REST linked service. Once you have created a dataset connected to this linked service you can include it in you copy activity.</p>
<p><a href=""https://i.stack.imgur.com/YsrMg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YsrMg.png"" alt=""Screenshot of including REST dataset as source in copy activity"" /></a></p>
<p>Once the pipeline executes the content of the REST response will be saved in the specified file.</p>
<p><a href=""https://i.stack.imgur.com/NE9B0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NE9B0.png"" alt=""Screenshot of data written to sink file"" /></a></p>
"
"63157740","Data Factory can't download CSV file from web API with Basic Auth","<p>I'm trying to download a CSV file from a website in Data Factory using the HTTP connector as my source linked service in a copy activity. It's basically a web call to a url that looks like <a href=""https://www.mywebsite.org/api/entityname.csv?fields=:all&amp;paging=false"" rel=""nofollow noreferrer"">https://www.mywebsite.org/api/entityname.csv?fields=:all&amp;paging=false</a>.</p>
<p>The website uses basic authentication. I have manually tested by using the url in a browser and entering the credentials, and everything works fine. I have used the REST connector in a copy activity to download the data as a JSON file (same url, just without the &quot;.csv&quot; in there), and that works fine. But there is something about the authentication in the HTTP connector that is different and causing issues. When I try to execute my copy activity, it downloads a csv file that contains the HTML for the login page on the source website.</p>
<p>While searching, I did come across this <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/14741#issuecomment-444074466"" rel=""nofollow noreferrer"">Github issue on the docs</a> that suggests that the basic auth header is not initially sent and that may be causing an issue.</p>
<p>As I have it now, the authentication is defined in the linked service. I'm hoping that maybe I can add something to the Additional Headers or Request Body properties of the source in my copy activity to make this work, but I haven't found the right thing yet.</p>
<p>Suggestions of things to try or code samples of a working copy activity using the HTTP connector and <strong>basic auth</strong> would be much appreciated.</p>
","<azure-data-factory>","2020-07-29 16:02:46","798","1","2","63332548","<p>The HTTP connector expects the API to return a 401 Unauthorized response after the initial request. It then responds with the basic auth credentials. If the API doesn't do this, it won't use the credentials provided in the HTTP linked service.</p>
<p>If that is the case, go to the copy activity source, and in the additional headers property add Authorization: Basic followed by the base64 encoded string of username:password. It should look something like this (where the string at the end is the encoded username:password):</p>
<pre><code>Authorization: Basic ZxN0b2njFasdfkVEH1fU2GM=`
</code></pre>
<p>It's best if that isn't hard coded into the copy activity but is retrieved from Key Vault and passed as secure input to the copy activity.</p>
"
"63152144","FIle format in azure data factory","<p>I have multiple format files in a blob storage ,how to check format of  only csv files and move that into another folder in azure data factory</p>
","<azure><azure-data-factory>","2020-07-29 11:02:27","437","0","1","63155712","<p>Use the GetMetadata activity to list the files. Be sure to add the &quot;ChildItems&quot; property:
<a href=""https://i.stack.imgur.com/jkh0G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jkh0G.png"" alt=""enter image description here"" /></a></p>
<p>Next use a Filter activity to only process the items that match your criteria:
<a href=""https://i.stack.imgur.com/sQKc6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sQKc6.png"" alt=""enter image description here"" /></a></p>
<p>NOTE: this example only processes a) files [ignores subfolders] that b) name starts with &quot;scoring-&quot; and c) ends in &quot;.csv&quot;.</p>
<p>Now you can perform the move inside the a ForEach over the filtered list:
<a href=""https://i.stack.imgur.com/cJZN1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cJZN1.png"" alt=""enter image description here"" /></a></p>
"
"63151818","Is it possible to copy files from blob storage / file share into an FTP within an ADF?","<p>I have a couple of pipelines in ADF, which in the end will produce some files. These files are currently being stored in file shares and blob storages. However, I'd like to move them inside a FTP server.</p>
<p>As of now, I have create a linked service to that FTP and a dataset that points to the FTP and the folder that I want to use to upload the files. However, when I use the activity &quot;Copy Data&quot; and use this dataset, I get the error &quot;the linked service in sink dataset does not support sink&quot;.</p>
<p>As far as I understand, this is only possible with a SFTP, which is not valid for me, it must be an FTP (technical limitations).</p>
<p>Can you provide me some guidance here?</p>
<p>Best regards!</p>
","<azure><ftp><azure-pipelines><azure-data-factory><azure-blob-storage>","2020-07-29 10:43:19","1000","2","1","63152654","<p>You can call Azure Logic App activity which contains SFTP and FTP connectors + Azure Storage connectors:</p>
<p><a href=""http://microsoft-bitools.blogspot.com/2018/06/execute-logic-apps-in-azure-data.html"" rel=""nofollow noreferrer"">http://microsoft-bitools.blogspot.com/2018/06/execute-logic-apps-in-azure-data.html</a></p>
"
"63138165","Azure Data Factory DYNAMICALLY partition a csv/txt file based on rowcount","<p>I am using azure dataflow to transform delimited files (csv/txt) to json. But I want to separate the files dynamically based on a max row count of 5,000 because I will not know the row count every time. So if I have a csv file with 10,000 rows the pipeline will output two equal json files, file1.json and file2.json. What is the best way to actually get the row count of my sources and the correct n number of partitions based on that row count within Azure Data Factory?</p>
","<azure><dynamic><partitioning><azure-data-factory>","2020-07-28 15:55:59","3109","0","2","63145722","<p>We can't specify the row number to split the csv file. The closest workaround is specify the partition of the sink.</p>
<p>For example, I have a csv file contains 700 rows data. I successfully copy to two equal json files.</p>
<p>My source csv data in Blob storage:
<a href=""https://i.stack.imgur.com/VDo2v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VDo2v.png"" alt=""enter image description here"" /></a></p>
<p><strong>Sink settings:</strong> each partition output a new file:  <code>json1.json</code> and <code>json2.json</code>:</p>
<p><a href=""https://i.stack.imgur.com/e0PJC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e0PJC.png"" alt=""enter image description here"" /></a></p>
<p><strong>Optimize:</strong></p>
<ol>
<li>Partition operation: <code>Set partition</code></li>
<li>Partition type: <code>Dynamic partition</code></li>
<li>Number of partitions: <code>2</code> (means split the csv data to 2 partitions)</li>
<li>Stored ranges in columns: <code>id</code>(split based on the <code>id</code> column)</li>
</ol>
<p><a href=""https://i.stack.imgur.com/Tqn9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tqn9E.png"" alt=""enter image description here"" /></a></p>
<p>Run the Data flow and the csv file will split to two json files which each contains 350 rows data.</p>
<p>For your situation, the csv file with 10,000 rows the pipeline will output two equal json files(each contains 5,000 row data).</p>
"
"63138165","Azure Data Factory DYNAMICALLY partition a csv/txt file based on rowcount","<p>I am using azure dataflow to transform delimited files (csv/txt) to json. But I want to separate the files dynamically based on a max row count of 5,000 because I will not know the row count every time. So if I have a csv file with 10,000 rows the pipeline will output two equal json files, file1.json and file2.json. What is the best way to actually get the row count of my sources and the correct n number of partitions based on that row count within Azure Data Factory?</p>
","<azure><dynamic><partitioning><azure-data-factory>","2020-07-28 15:55:59","3109","0","2","64977561","<p>One way to achieve this is to use the mod or % operator.</p>
<ol>
<li>To start with set a surrogate key on the CSV file or use any sequential key in the data.</li>
<li><a href=""https://i.stack.imgur.com/G5FLH.png"" rel=""nofollow noreferrer"">Add a aggregate step with a group by clause that is your key % row count</a></li>
<li>Set the Aggregates function to collect()</li>
</ol>
<p>Your output should now be a array of rows with the expected count in each.</p>
"
"63136990","In a nested set of logic apps,Is it possible to bubble errors up?","<p>We have a parent logic app that calls child logic apps, which in turn can call other child logic apps depending on the scenario.
The parent logic app is really just for orchestration.</p>
<p>Occasionally, one nested/child logic app will fail, but the parent logic app will show as succeeded.
Is it possible to bubble errors up to the parent logic app?</p>
","<azure-logic-apps><azure-data-factory>","2020-07-28 14:53:21","78","0","1","63214673","<p>The best solution I can think of is to build you Child logic apps with a HTTP Request trigger. The parent calls it and expects a result. When you Child has an issue, return the error and a response code, say 500 (forcefully fail the child), then the Parent can parse the response to check what the response code is. If it's 500, then you can do whatever you want with that error then forcefully fail the Parent.</p>
"
"63130560","XML File Using Azure Data Factory","<p>I have used the latest xml file system support feature available in the Azure data factory to convert a  2 GB xml file to csv using copy activity, but it ended up in a memory exception. But if I use a lesser sized files , eg: 500 mb one, the file gets processed within 1 hour without any parallelism a or DIU. The 2 GB file doesnt get converted even if I use all the permutation combinations of parallelism, DIU, or Block size? Am I doing something wrong. If anyone could suggest a solution, it would be really helpful.</p>
<p>Regards,
Sandeep</p>
","<xml><azure><azure-data-factory>","2020-07-28 09:01:24","284","1","1","63140714","<p>I would recommend you to try the XML to CSV conversion using Data Flow instead if you are experiencing issues with the standard Copy activity. I would also suggest you provision a memory optimized integration runtime to execute you data flow activity.</p>
"
"63120384","Dynamic sheet name in source dataset: (Excel (Blob storage)) on Azure Data Factory. - Error: Please select a work sheet for your dataset","<p>I have multiple .xlsx-files in my blob storage and I need to copy them to my Azure SQL Database using Azure data Factory.
I want to keep one Source Dataset (Blob Storage -Excel).
So I added two parameters in the dataset.</p>
<p>File   (string): blabla.xlsx</p>
<p>Sheet  (string): blabla (name of the sheet in excel).
<a href=""https://i.stack.imgur.com/6EyKM.png"" rel=""nofollow noreferrer"">Source Dataset</a></p>
<p>If I go to copy data and details are already filled in, I got the following error:
'Please select a work sheet for your dataset'
<a href=""https://i.stack.imgur.com/bHprB.png"" rel=""nofollow noreferrer"">Copy data</a></p>
<p>If I change the sheet name in hardcode: blabla. It will works, but then I cannot make use of a dynamic sheet name.<br />
Does someone know how I can fix this?</p>
","<excel><azure-functions><azure-sql-database><azure-blob-storage><azure-data-factory>","2020-07-27 17:21:29","3286","1","1","63121517","<p>If you want to pass the sheet name as dynamic to the dataset, then you will have to have dataset parameter and a pipeline parameter and then pass sheet name value from pipeline parameter to dataset parameter as below:</p>
<p><a href=""https://i.stack.imgur.com/buefm.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/buefm.gif"" alt=""enter image description here"" /></a></p>
"
"63119029","Sending a JSON file in Data Factory in Azure","<p>I have looked all over and can't find exactly what i am trying to do.  I am trying to send a json file to an external client.  The json file is a blob storage and is called kimblejob.json  I have it pointing to a Web activity that has the authenticaton already set as basic.  I have the json file under Datasets.  Now, every time I put something in the body, that is the what gets posted and not the file.  How do I get the file to post and not what is in the body?  What do I put in the body?  How do I direct it to send the json file and not the content of the body?
Thanks</p>
<p><img src=""https://i.stack.imgur.com/Fr1yJ.png"" alt=""enter image description here"" /></p>
","<json><azure><azure-data-factory>","2020-07-27 15:59:37","1048","1","3","63141095","<p>According to docs it is not possible to copy data <strong>to</strong> a REST endpoint, only from. You can read more about that on this page <code>https://learn.microsoft.com/en-us/azure/data-factory/connector-rest</code> on MS Docs.</p>
<p>You can also read about all supported data stores and formats on this link <code>https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats</code>. If you scroll down to <code>Generic protocol</code> you will see that HTTP and REST is not supported as a sink.</p>
<p><a href=""https://i.stack.imgur.com/K20y6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K20y6.png"" alt=""Screenshot showing the HTTP and REST is not supported as sinks in ADF"" /></a></p>
<p>Putting a dataset reference in a Web activity will only pass the metadata of the dataset and the linked service in your http POST - it will be up to the receiving API to know what to do with it.</p>
<p>In the past scenarios I've had where we need to upload data to a REST API I have landed the data temporarily in a storage account and then called a Azure Function to handle the upload to the REST endpoint.</p>
"
"63119029","Sending a JSON file in Data Factory in Azure","<p>I have looked all over and can't find exactly what i am trying to do.  I am trying to send a json file to an external client.  The json file is a blob storage and is called kimblejob.json  I have it pointing to a Web activity that has the authenticaton already set as basic.  I have the json file under Datasets.  Now, every time I put something in the body, that is the what gets posted and not the file.  How do I get the file to post and not what is in the body?  What do I put in the body?  How do I direct it to send the json file and not the content of the body?
Thanks</p>
<p><img src=""https://i.stack.imgur.com/Fr1yJ.png"" alt=""enter image description here"" /></p>
","<json><azure><azure-data-factory>","2020-07-27 15:59:37","1048","1","3","67321583","<p>You can use lookup activitiy to look to that file. It will have 4 mb limit though . Output from lookup you can use it to provide body of web activitiy.</p>
"
"63119029","Sending a JSON file in Data Factory in Azure","<p>I have looked all over and can't find exactly what i am trying to do.  I am trying to send a json file to an external client.  The json file is a blob storage and is called kimblejob.json  I have it pointing to a Web activity that has the authenticaton already set as basic.  I have the json file under Datasets.  Now, every time I put something in the body, that is the what gets posted and not the file.  How do I get the file to post and not what is in the body?  What do I put in the body?  How do I direct it to send the json file and not the content of the body?
Thanks</p>
<p><img src=""https://i.stack.imgur.com/Fr1yJ.png"" alt=""enter image description here"" /></p>
","<json><azure><azure-data-factory>","2020-07-27 15:59:37","1048","1","3","68503719","<p>You can pass an output (JSON) from a Web Activity into the body of another Web Activity without having to use Copy data and blob storage.</p>
"
"63118220","Issue with Azure Data Factory - Long file path names","<p>I am using copy activity to copy the files from on premise to Azure Storage Account . The job is failing as there is a long file path name at the source.
can someone help me how can i fix this issue
I am getting the following error</p>
<pre><code> &quot;errorCode&quot;: &quot;2200&quot;,
 &quot;message&quot;: &quot;ErrorCode=UserErrorFileNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot find the file specified. Folder path: '&lt;&lt;long file path&gt;&gt;''.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.IO.FileNotFoundException,Message=Could not find file '&lt;&lt;long file path&gt;&gt;'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;CopyToBlob&quot;,
    &quot;details&quot;: []
</code></pre>
","<azure-data-factory>","2020-07-27 15:10:13","273","0","1","63271009","<p>This looks like a limitation on the file path maximum length which is already documented in below documentation :</p>
<p><a href=""https://learn.microsoft.com/en-us/windows/win32/fileio/naming-a-file#maximum-path-length-limitation"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/windows/win32/fileio/naming-a-file#maximum-path-length-limitation</a></p>
<p>Could you please follow these steps to enable long path in you SHIR hosted VM and see if that helps to resolve the issue (This workaround is suggested for the OS of the VM where SHIR installed is at-least Windows Server 2016, Windows 10):</p>
<ol>
<li><p>Logon to SHIR hosted VM.</p>
</li>
<li><p>Open &quot;Local Group Policy Editor&quot;, in the left-handed pane, drill down to computer configuration &gt; Administrative Templates &gt; system &gt; Filesystem.
On the right, find the &quot;Enable win32 long paths&quot; item and double-check it.</p>
</li>
<li><p>In the properties window that opens, select the &quot;Enabled&quot; option and then click &quot;OK&quot;.</p>
<p><a href=""https://i.stack.imgur.com/pohxl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pohxl.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>You can now exit the Local Group Policy Editor and restart your computer (or sign out and back in) to allow the changes to finish.</p>
</li>
</ol>
"
"63117632","How do we pass value or parameter from azure data factory to runbook and excute it","<p>could you help me on how would we pass variable from azure datafactory to runbook.</p>
<p>for example i would pass the conatiner name for the run book to generate a sas key using the container name.</p>
","<azure><azure-data-factory><azure-runbook>","2020-07-27 14:37:07","81","-1","1","63133948","<p>To accomplish the requirement of generating an SAS token for an Azure storage container, you would have to use cmdlet <a href=""https://learn.microsoft.com/en-us/powershell/module/azure.storage/new-azurestoragecontainersastoken?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">New-AzureStorageContainerSASToken</a> (that belongs to <a href=""https://learn.microsoft.com/en-us/powershell/module/azure.storage/?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">Azure.Storage</a> module) / cmdlet <a href=""https://learn.microsoft.com/en-us/powershell/module/az.storage/new-azstoragecontainersastoken?view=azps-4.4.0"" rel=""nofollow noreferrer"">New-AzStorageContainerSASToken</a> (that belongs to <a href=""https://learn.microsoft.com/en-us/powershell/module/az.storage/?view=azps-4.4.0"" rel=""nofollow noreferrer"">Az.Storage</a> module).</p>
<p>As instructed <a href=""https://learn.microsoft.com/en-us/powershell/azure/new-azureps-module-az?view=azps-4.4.0"" rel=""nofollow noreferrer"">here</a>, starting in December 2018, the Azure PowerShell Az module is in general release and is now the intended PowerShell module for interacting with Azure. So Az modules are latest recommended ones to use and AzureRM modules are the older ones. So AzureRM modules are the older ones and Az modules are latest recommended ones to use.</p>
<p>To make the cmdlet <a href=""https://learn.microsoft.com/en-us/powershell/module/az.storage/new-azstoragecontainersastoken?view=azps-4.4.0"" rel=""nofollow noreferrer"">New-AzStorageContainerSASToken</a> (that belongs to <a href=""https://learn.microsoft.com/en-us/powershell/module/az.storage/?view=azps-4.4.0"" rel=""nofollow noreferrer"">Az.Storage</a> module) work in your runbook, make sure you have <a href=""https://learn.microsoft.com/en-us/powershell/module/az.storage/?view=azps-4.4.0"" rel=""nofollow noreferrer"">Az.Storage</a> module imported / available in your Azure Automation account.</p>
<p><a href=""https://i.stack.imgur.com/nyVTX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nyVTX.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/r8llc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r8llc.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/J6fJe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J6fJe.png"" alt=""enter image description here"" /></a></p>
<p>Similarly, if your other requirements are related to Azure DataFactory then leverage <a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/?view=azps-4.4.0"" rel=""nofollow noreferrer"">Az.DataFactory</a> module cmdlets as appropriate. Note that, if necessary, you would have to <a href=""https://learn.microsoft.com/en-us/azure/automation/shared-resources/modules"" rel=""nofollow noreferrer"">import required Azure modules</a> or <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-update-azure-modules"" rel=""nofollow noreferrer"">update modules</a> based on the cmdlets that you use.</p>
"
"63117563","How to copydata from RestAPI using datafactory and save it in Datalake?","<p>I'm trying to fetch data from REST API and save the json string it into DataLake and I'm getting an error. I've followed the steps mentioned here
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest</a> &amp; <a href=""https://www.alexvolok.com/2019/adfv2-rest-api-part1-oauth2/"" rel=""nofollow noreferrer"">https://www.alexvolok.com/2019/adfv2-rest-api-part1-oauth2/</a></p>
<p>The API which I'm trying to connect uses OAuth2 so I need to first get the access token and then do a get request to get actual data.</p>
<p>Below are the steps which I'm following</p>
<ol>
<li><p>Creating a Web HTTP request in the pipeline and passing the client_ID, client secret, username, password and grant type in the body of the request. When I debug the pipline I do get the Access_token which I need in step 2.</p>
</li>
<li><p>In Step two I have a copy activity which uses the output(access_token) from web to authenticate the second REST GET request but this is where I'm facing a lot of issues. The code which I'm using is &quot;@concat('Bearer ', activity('GetAccessToken').output.access_token)&quot;</p>
</li>
<li><p>In step 3 I have two datasets and 2 Linked services, Dataset 1 is a REST dataset which has the base url and relative url which is linked to the REST linked service and secondly the sink dataset is connected to AZURE datalake storage.</p>
</li>
<li><p>In Source Dataset I'm passing additional header Authorization = @concat('Bearer ', activity('GetAccessToken').output.access_token) and ideally since the API which I want to call will return empty if no parameters are send so I pass in the parameters inside the &quot;Request Body&quot; is that even correct? the Request body would look something like this &quot;start_date=2020/07/17&amp;end_date=2020/07/18&quot;.</p>
</li>
<li><p>The sink is a simple Json dataset stored in DataLake.</p>
</li>
</ol>
<p>When I try to debug I get the error as below</p>
<p>But I'm getting the below error</p>
<pre><code>{
  &quot;errorCode&quot;: &quot;2200&quot;,
  &quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=UserErrorHttpStatusCodeIndicatingFailure,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The HttpStatusCode 401 indicates failure. { \&quot;Error\&quot;: { \&quot;Message\&quot;:\&quot;Authentication failed: Invalid headers\&quot;, \&quot;Server-Time\&quot;:\&quot;2020-07-27T06:59:24\&quot;, \&quot;Id\&quot;:\&quot;6AAF87BC-5634-4C28-8626-810A19B86BFF\&quot; } },Source=Microsoft.DataTransfer.ClientLibrary,'&quot;,
  &quot;failureType&quot;: &quot;UserError&quot;,
  &quot;target&quot;: &quot;CopyDataFromAPI&quot;,
  &quot;details&quot;: []
}
</code></pre>
<p>Please advise if I'm doing anything wrong.</p>
","<azure><azure-data-factory>","2020-07-27 14:33:08","668","0","1","63149117","<p>I Knew this was a simple issue,</p>
<p>So for people who are looking for answers.</p>
<p>Please make sure the REST Source URL starts with HTTPS:// instead of HTTP:// I Guess Azure does not pass headers to url which starts with HTTP:// which is strange because POSTMAN and python script has no problem sending the headers.</p>
"
"63117036","How to set parameters on a CI/CD pipeline for Azure Data Factory?","<p>I am currently building multiple pipelines that share the same parameters.
One parent pipeline sends the parameters to the others when it executes them.</p>
<p>When I am setting the file: arm-template-parameters-definition.json, how can I specify that I want to change the parameters only for the parent pipeline?</p>
<p>He is currently detection all parameters with same name and creates more than 250 parameters.</p>
<p>Thank you</p>
","<azure-data-factory>","2020-07-27 14:04:20","782","0","3","63156523","<p>One option is to set a different parameter name for the parent pipeline, something prefixing with 'Parent_ParameterName'</p>
<p>In ADF UI, go to Manage <em>menu</em> -&gt; <em>Paremeterization Template</em> -&gt; provide the variable name in the section   <strong>&quot;Microsoft.DataFactory/factories/pipelines&quot;</strong>:</p>
<pre><code>variables&quot;: {
        &quot;Parent_ParameterName&quot;: {
          &quot;defaultValue&quot;: &quot;=&quot;
        },
</code></pre>
<p>publishing the parameterization template will create the parameter only for the parent pipleine i.e. the parameters that matches the 'parent_parameterName'</p>
"
"63117036","How to set parameters on a CI/CD pipeline for Azure Data Factory?","<p>I am currently building multiple pipelines that share the same parameters.
One parent pipeline sends the parameters to the others when it executes them.</p>
<p>When I am setting the file: arm-template-parameters-definition.json, how can I specify that I want to change the parameters only for the parent pipeline?</p>
<p>He is currently detection all parameters with same name and creates more than 250 parameters.</p>
<p>Thank you</p>
","<azure-data-factory>","2020-07-27 14:04:20","782","0","3","63744728","<p>There's this new Global Parameters feature available since beginning of August.
It can come in handy!</p>
"
"63117036","How to set parameters on a CI/CD pipeline for Azure Data Factory?","<p>I am currently building multiple pipelines that share the same parameters.
One parent pipeline sends the parameters to the others when it executes them.</p>
<p>When I am setting the file: arm-template-parameters-definition.json, how can I specify that I want to change the parameters only for the parent pipeline?</p>
<p>He is currently detection all parameters with same name and creates more than 250 parameters.</p>
<p>Thank you</p>
","<azure-data-factory>","2020-07-27 14:04:20","782","0","3","72547095","<p>Not sure if this is exactly what you are looking for, but what I typically do is add global parameters and then reference those everywhere (in your case from the pipeline parameters).  Then in your build you can override the global parameters with the values that you want.</p>
<p>So from your 'child' pipelines, you would create a pipeline parameter just like you do today, but the value of that parameter would be the global parameter name like this:</p>
<p><a href=""https://i.stack.imgur.com/e36qO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e36qO.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/jZs9B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jZs9B.png"" alt=""enter image description here"" /></a></p>
<p>Here are some links to references that I used to get this working:</p>
<ul>
<li><a href=""https://towardsdatascience.com/azure-data-factory-ci-cd-made-simple-building-and-deploying-your-arm-templates-with-azure-devops-30c30595afa5"" rel=""nofollow noreferrer"">Azure Data Factory CI-CD made simple: Building and deploying ARM templates with Azure DevOps YAML Pipelines</a></li>
<li><a href=""https://microsoft-bitools.blogspot.com/2021/11/adf-snack-set-global-params-during.html"" rel=""nofollow noreferrer"">ADF Release - Set global params during deployment</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/author-global-parameters"" rel=""nofollow noreferrer"">Global parameters in Azure Data Factory</a></li>
</ul>
"
"63114436","Azure Data Factory CI/CD - automatic publish after merge to master branch","<p>we are building CI/CD solution over our Azure Data Factory, and what I'm missing and what I was not able to get answer for after some searching is if there is possibility to automate publishing from collaboration branch to our <strong>adf_publish</strong> branch. So desired workflow should be:</p>
<ol>
<li>Create feature branch</li>
<li>When you are satisfied with changes create pull request</li>
<li>Pull request is approved and merged to collaboration(master) branch</li>
<li>After merge, pipeline <strong>will automatically publish changes to development Azure Data Factory service</strong> and reflect the changes in <strong>adf_publish</strong> branch. Without someone <strong>pushing Publish button</strong> in UI.</li>
<li>If necessary release pipelines are triggered from adf_publish branch to other environments</li>
</ol>
","<git><azure><continuous-integration><azure-data-factory>","2020-07-27 11:33:19","936","0","2","63156632","<p>This feature is currently unavailable in adf, kindly vote for the adf feedback link below:
<a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/40556329-please-allow-users-to-automate-publish"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/40556329-please-allow-users-to-automate-publish</a></p>
"
"63114436","Azure Data Factory CI/CD - automatic publish after merge to master branch","<p>we are building CI/CD solution over our Azure Data Factory, and what I'm missing and what I was not able to get answer for after some searching is if there is possibility to automate publishing from collaboration branch to our <strong>adf_publish</strong> branch. So desired workflow should be:</p>
<ol>
<li>Create feature branch</li>
<li>When you are satisfied with changes create pull request</li>
<li>Pull request is approved and merged to collaboration(master) branch</li>
<li>After merge, pipeline <strong>will automatically publish changes to development Azure Data Factory service</strong> and reflect the changes in <strong>adf_publish</strong> branch. Without someone <strong>pushing Publish button</strong> in UI.</li>
<li>If necessary release pipelines are triggered from adf_publish branch to other environments</li>
</ol>
","<git><azure><continuous-integration><azure-data-factory>","2020-07-27 11:33:19","936","0","2","66175606","<p>This has been added to Azure Data Factory and Azure DevOps Pipelines as of early 2021.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment-improvements#the-new-cicd-flow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment-improvements#the-new-cicd-flow</a></p>
"
"63103603","Stored procedure activity ADF V2","<p>I'm using a stored procedure activity for ADF v2 pipeline. Now issue here is whenever the pipeline fails at the stored procedure activity I'm not getting the complete error details. Below is the JSON output of that stored procedure activity:</p>
<pre><code>{
    &quot;effectiveIntegrationRuntime&quot;: &quot;DefaultIntegrationRuntime (West Europe)&quot;,
    &quot;executionDuration&quot;: 416,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
},
&quot;billingReference&quot;: {
    &quot;activityType&quot;: &quot;ExternalActivity&quot;,
    &quot;billableDuration&quot;: [
        {
            &quot;meterType&quot;: &quot;AzureIR&quot;,
            &quot;duration&quot;: 0.11666666666666667,
            &quot;unit&quot;: &quot;Hours&quot;
        }
    ]
}
}
</code></pre>
<p>Please let me know how do I get the error details for the stored procedure activity for ADF v2 pipeline?</p>
","<azure><azure-data-factory>","2020-07-26 17:42:32","247","0","1","63103640","<p>You should throw the exception in your stored procedure code:</p>
<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/language-elements/throw-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/language-elements/throw-transact-sql?view=sql-server-ver15</a></p>
"
"63084603","addHours, addMinutes function not working in azure data Factory","<p>I am trying for a function in order to add some hours to my present existing time zones in order to handle time stamp of various countries.</p>
<p>Would be glad if some one can help me in knowing if any other way of adding hours or minutes to a timestamp.</p>
<p>This question specific to Azure Data Factory</p>
<p><a href=""https://i.stack.imgur.com/g1xWn.png"" rel=""nofollow noreferrer"">Function is not Found</a></p>
","<timestamp><azure-data-factory>","2020-07-25 05:10:48","2712","1","1","63107314","<p>Data Factory supports many date functions in expression:
<a href=""https://i.stack.imgur.com/V4T5h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V4T5h.png"" alt=""enter image description here"" /></a></p>
<p>Please ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">Date functions</a></p>
<p>We can use this functions in dynamic content.</p>
<p>This example adds 10 hours to the specified timestamp:</p>
<pre><code>addHours('2018-03-15T00:00:00Z', 10)
</code></pre>
<p>And returns this result: <code>&quot;2018-03-15T10:00:0000000Z&quot;</code></p>
<p>If you want to add some hours to my present existing time zones, the expression should be:</p>
<pre><code>addHours(utcNow(), 10)
</code></pre>
<p>For more details, please ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">Expressions and functions in Azure Data Factory</a></p>
<p><strong>Update:</strong></p>
<p>It looks like you are using Data flow. Then you could using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#add"" rel=""nofollow noreferrer"">add</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#hours"" rel=""nofollow noreferrer"">hours</a>:
<a href=""https://i.stack.imgur.com/NTMW0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NTMW0.png"" alt=""enter image description here"" /></a></p>
<p>Your expression should be something like:</p>
<pre><code>currentTimestamp()+hours(2)
</code></pre>
"
"63077489","Is it possible to download a million files in parallel from Rest API endpoint using Azure Data Factory into Blob?","<p>I am fairly new to Azure and I have a task in hand to make use of any Azure Service (or group of azure services in integration together for that matter) to o download a million files in parallel from a third party Rest API endpoint, that returns one file at a time, using Azure Data Factory into Blob Storage?</p>
<p><strong>WHAT I RESEARCHED :</strong></p>
<p>From what I researched my task had three requirements in a nutshell :</p>
<ul>
<li>Parallel runs in millions  - For this I deduced Azure Batch would be a good option as it lets run a large number of tasks in parallel on VMs ( it uses that concept for graphic rendering processes or Machine Learning Tasks)</li>
<li>Save response from Rest API to Blob Storage : I found that Azure Data Factory is able to handle such ETL type of operation from a Source/Sink style, where I could set the REST API as source and target as blob.</li>
</ul>
<p><strong>WHAT I HAVE TRIED:</strong></p>
<p>Here are some things to note:</p>
<ul>
<li>I added the REST API and Blob as linked services.</li>
<li>The API endpoint takes in a query string param named : <strong>fileName</strong></li>
<li>I am passing the whole URL with the query string</li>
<li>The Rest API is protected by Bearer Token, which I am trying to pass using additional headers.</li>
</ul>
<p><strong>THE MAIN PROBLEM:</strong></p>
<ul>
<li>I get an error message on publishing pipeline that <strong>model is not appropriate</strong>, just that one line, and it gives no insight what's wrong</li>
</ul>
<p><strong>OTHER QUERIES:</strong></p>
<ul>
<li>It is possible to pass query string values dynamically from a sql table such that each filename can be picked a single row/column item from single columned rows of data from stored procedure/inline query?</li>
<li>Is it possible to make this pipeline run in parallel using Azure Batch somehow? How can we integrate this process ?</li>
<li>Is it possible to achieve the million parallel without data factory just using Batch ?</li>
</ul>
","<azure><azure-blob-storage><azure-data-factory>","2020-07-24 16:14:42","501","1","1","63142296","<p>Hard to help with you main problem - you need to provide more examples of your code</p>
<p>In relation to your other queries:</p>
<ul>
<li><p>You can use a &quot;Lookup activity&quot; to fetch a list of files from a database (with either sproc or inline query). The next step would be a ForEach activity that iterates over the array and copies the file from the REST endpoint to the storage account. You can adjust the parallelism on the ForEach activity to match your requirement but around 20 concurrent executions is what you normally see.</p>
</li>
<li><p>Using Azure Batch to just download a file seems a bit overkill as it should be a fairly quick operation. If you want to see an example of a Azure Batch job written in C# I can recommend this example =&gt; `https://github.com/Azure-Samples/batch-dotnet-quickstart/blob/master/BatchDotnetQuickstart. In terms of parallelism I think you will manage to achieve a higher degree on Azure Batch compared to Azure Data Factory.</p>
</li>
<li><p>In you need to actually download 1M files in parallel I don't think you have any other option then Azure Batch to get close to such numbers. But you most have a pretty beefy API if it can handle 1M requests within a second or two.</p>
</li>
</ul>
"
"63074929","Looking for Implementing Role Playing dimension in Azure Analysis Services (Tabular)","<p>As of now in my fact table i have around 6 Date Keys which needs to be linked to one Date Dimension.</p>
<p>Possible solution on table with me is:</p>
<p>Creating multiple date dimension and linking it with Fact Table. As this involves duplicating the data. I wanted to avoid this solution as i have around 10 Date Columns in my fact Table.</p>
<p>I am looking for a solution in order to reduce the redundancy of number of Tables in my Model.</p>
<p>Something like User-Role</p>
","<sql><azure><azure-data-factory><azure-analysis-services>","2020-07-24 13:48:59","80","0","1","63136627","<p>Without increasing the number of tables,  we can use the relationship which is relevant to the measures. This can be achieved by enforcing the data model to activate the relationship we need. The relationship can be moved from inactive to active in DAX using USERELATIONSHIP function. Below are the snaps showing one active and 2 inactive relationships and usage of &quot;USERELATIONSHIP&quot; while implementing the measures and results later.</p>
<p><a href=""https://i.stack.imgur.com/fWNKg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fWNKg.png"" alt=""ActiveRelationship"" /></a>
<a href=""https://i.stack.imgur.com/8JW7i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8JW7i.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/6Mc8E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Mc8E.png"" alt=""enter image description here"" /></a></p>
"
"63070576","How to impersonate logged in user to manage other Azure service","<p>I have webapp to make changes in ADF (kinda deployment of objects in ADF). I want to authenticate user against ADF. I am able to detect who is accessing web app. I get Azure AD Id like user1@company.com. I want to validate if the same user has access to ADF and if yes, generate bearer token to make changes in ADF (using rest api) or using SDK.</p>
","<asp.net><azure><azure-data-factory><azure-sdk-.net>","2020-07-24 09:16:29","1009","0","1","63126085","<p>You can use the way below to get the access token when the user login to the web app, no need to validate if the same user has access to ADF, because if he does not have the access, the token will not be able to call the rest api, he will get the 401 unauthorized error.</p>
<p>1.First, make sure you have <a href=""https://learn.microsoft.com/en-us/azure/app-service/configure-authentication-provider-aad"" rel=""nofollow noreferrer"">configured your web app to use Azure AD login</a>, then navigate to the <a href=""https://resources.azure.com/"" rel=""nofollow noreferrer"">resource explorer</a> -&gt; find your web app -&gt; add <code>[&quot;resource=https://management.azure.com&quot;]</code> to <code>additionalLoginParams</code> like below -&gt; <code>PUT</code>.</p>
<p><a href=""https://i.stack.imgur.com/EpOmn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EpOmn.png"" alt=""enter image description here"" /></a></p>
<p>2.Navigate to the <code>Azure Active Directory</code> in the portal -&gt; <code>App registrations </code> -&gt; find the AD App corresponding your web app -&gt; <code>API permissions</code> -&gt; add the permission <code>user_impersonation</code> of <code>Azure Service Management</code> like below.</p>
<p><a href=""https://i.stack.imgur.com/T8wzw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T8wzw.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/SD1Cd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SD1Cd.png"" alt=""enter image description here"" /></a></p>
<p>3.Then when the user login the web app, after he consents the permissions,  you can get the token with endpoint <code>https://webappname.azurewebsites.net/.auth/me</code>, and use the token to call the <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/v2"" rel=""nofollow noreferrer"">data factory rest api</a>.</p>
<p><a href=""https://i.stack.imgur.com/Iap0k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iap0k.png"" alt=""enter image description here"" /></a></p>
<p>4.Make sure the user has an RBAC role e.g. <code>Contributor</code> in your subscription/ADF, then the token will be able to call the rest api successfully.</p>
<p>For example, I test with <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/listbyfactory"" rel=""nofollow noreferrer""><code>Pipelines - List By Factory</code></a> api, it works fine.</p>
<p><a href=""https://i.stack.imgur.com/vItK5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vItK5.png"" alt=""enter image description here"" /></a></p>
"
"63067580","Trying to move data from one Azure Blob Storage to another using a Python script","<p>I have data that exists in a zipped format in container A that I need to transform using a Python script and am trying to schedule this to occur within Azure, but when writing the output to a new storage container (container B), it simply outputs a csv with the name of the file inside rather than the data.</p>
<p>I've followed the tutorial given on the microsoft site exactly, but I can't get it to work - what am I missing?</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/batch/tutorial-run-python-batch-azure-data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/batch/tutorial-run-python-batch-azure-data-factory</a></p>
<pre><code>file_n='iris.csv'

# Load iris dataset from the task node
df = pd.read_csv(file_n)

# Subset records
df = df[df['Species'] == &quot;setosa&quot;]

# Save the subset of the iris dataframe locally in task node
df.to_csv(&quot;iris_setosa.csv&quot;, index = False, encoding=&quot;utf-8&quot;)

# Upload iris dataset
blobService.create_blob_from_text(containerName, &quot;iris_setosa.csv&quot;, &quot;iris_setosa.csv&quot;)
</code></pre>
<p>Specifically, the final line seems to be just giving me the output of a csv called &quot;iris_setosa.csv&quot; with a contents of &quot;iris_setosa.csv&quot; in cell A1 rather than the actual data that it reads in.</p>
","<python><azure><azure-data-factory>","2020-07-24 05:48:46","252","0","1","63070217","<p><strong>Update:</strong></p>
<p>replace <code>create_blob_from_text</code> with <code>create_blob_from_path</code>.</p>
<p><code>create_blob_from_text</code> creates a new blob from str/unicode, or updates the content of an existing blob. So you will find text <em>iris_setosa.csv</em> in the content of the new blob.</p>
<p><code>create_blob_from_path</code> creates a new blob from a file path, or updates the content of an existing blob. It is what you want.</p>
<hr />
<p>This workaround uses <code>copy_blob</code> and <code>delete_blob</code> to <strong>move</strong> Azure Blob from one container to another.</p>
<pre><code>from azure.storage.blob import BlobService

def copy_azure_files(self):

        blob_service = BlobService(account_name='account_name', account_key='account_key')
        blob_name = 'iris_setosa.csv'
        copy_from_container = 'test-container'
        copy_to_container = 'demo-container'

        blob_url = blob_service.make_blob_url(copy_from_container, blob_name)
        # blob_url:https://demostorage.blob.core.windows.net/test-container/iris_setosa.csv

        blob_service.copy_blob(copy_to_container, blob_name, blob_url)

        #for move the file use this line
        blob_service.delete_blob(copy_from_container, blob_name)
</code></pre>
"
"63067139","ADF -pass SQL query in source with single quotes in date column","<p>I am trying to pass parameters in dynamic pipeline in source query inside data flow. Tablename and Column name is dynamic and will be passed through parameters. But i got an error while executing the below ADF query:</p>
<p><strong>ADF Query</strong>: concat('SELECT ', $ValidationColumn, ' FROM ', $ValidationTable,' WHERE audit_datetime &gt;= ', replace($StartAuditDateTime,'T',''),' AND audit_datetime &lt;= ' ,replace($EndAuditDateTime,'T','') )</p>
<p><strong>Expected SQL query</strong>: Select ID from Master where audit_datetime &gt;= '2020-07-23 18:47:20.5666' AND audit_datetime &lt;= '2020-07-24 01:47:20.5456'</p>
","<azure-sql-database><azure-data-factory>","2020-07-24 05:00:59","1738","2","3","63072633","<p>I am assuming you want to escape single quote to get '{yourDateValue}' format,
you can do that like this <code>''''</code></p>
<p>example:</p>
<pre><code>@{concat('select * from [yourtable] WHERE [yourColumn] &gt;=','''',{yourexpression},'''')}
</code></pre>
"
"63067139","ADF -pass SQL query in source with single quotes in date column","<p>I am trying to pass parameters in dynamic pipeline in source query inside data flow. Tablename and Column name is dynamic and will be passed through parameters. But i got an error while executing the below ADF query:</p>
<p><strong>ADF Query</strong>: concat('SELECT ', $ValidationColumn, ' FROM ', $ValidationTable,' WHERE audit_datetime &gt;= ', replace($StartAuditDateTime,'T',''),' AND audit_datetime &lt;= ' ,replace($EndAuditDateTime,'T','') )</p>
<p><strong>Expected SQL query</strong>: Select ID from Master where audit_datetime &gt;= '2020-07-23 18:47:20.5666' AND audit_datetime &lt;= '2020-07-24 01:47:20.5456'</p>
","<azure-sql-database><azure-data-factory>","2020-07-24 05:00:59","1738","2","3","63124146","<p>Please try below expression to form your query in your ADF Mapping dataflow expression builder.</p>
<pre><code>&quot;SELECT '{$ValidationColumn}' FROM '{$ValidationTable}' WHERE audit_datetime &gt;= '{$StartAuditDateTime}' AND audit_datetime &lt;= '{$EndAuditDateTime}'&quot;
</code></pre>
<p>This will result in below QUERY formation:</p>
<pre><code>SELECT ID FROM Master WHERE audit_datetime &gt;= '2020-07-23 18:47:20.5666' AND audit_datetime &lt;= '2020-07-24 01:47:20.5456'
</code></pre>
"
"63067139","ADF -pass SQL query in source with single quotes in date column","<p>I am trying to pass parameters in dynamic pipeline in source query inside data flow. Tablename and Column name is dynamic and will be passed through parameters. But i got an error while executing the below ADF query:</p>
<p><strong>ADF Query</strong>: concat('SELECT ', $ValidationColumn, ' FROM ', $ValidationTable,' WHERE audit_datetime &gt;= ', replace($StartAuditDateTime,'T',''),' AND audit_datetime &lt;= ' ,replace($EndAuditDateTime,'T','') )</p>
<p><strong>Expected SQL query</strong>: Select ID from Master where audit_datetime &gt;= '2020-07-23 18:47:20.5666' AND audit_datetime &lt;= '2020-07-24 01:47:20.5456'</p>
","<azure-sql-database><azure-data-factory>","2020-07-24 05:00:59","1738","2","3","70788092","<p>Use this SQL syntax in Expression builder in ADF Data Flow where you need to use singe quotes</p>
<pre><code>&quot;SELECT * FROM myTable WHERE someColumn = 'Y'&quot;
</code></pre>
"
"63060856","Invalid data stored in Delimited Text after running ADF Data Flow","<p>I'm trying to store data from an input to csv file in blob storage via ADF data flow. The pipeline ran successfully. However on checking the csv file, I see some invalid data included. Here are the settings of Delimited Text and Sink. Please let me know what I am missing?</p>
<p><a href=""https://i.stack.imgur.com/zGlrF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zGlrF.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/Q2mp1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q2mp1.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/LyCmH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LyCmH.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-07-23 18:28:51","178","0","1","63065868","<p>I tested and repeat the error.</p>
<p>The error is caused by that all the csv files in the <code>csv/test</code> folder have different schema.</p>
<p>Even if the pipeline runs with no error, but the data in to the single file will has the error.</p>
<p>In Data Factory, when we try to merge more files to one, or copy data from more files to single, the files in the folder must have the same schema.</p>
<p>Note that please using <code>wildcard paths</code> to filter all the csv files:</p>
<p><a href=""https://i.stack.imgur.com/5Wq0C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Wq0C.png"" alt=""enter image description here"" /></a></p>
<p>For example, I have two csv files which have same schema in the container:
<a href=""https://i.stack.imgur.com/GvuS7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GvuS7.png"" alt=""enter image description here"" /></a></p>
<p>Source dataset preview:</p>
<p><a href=""https://i.stack.imgur.com/puyXR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/puyXR.png"" alt=""enter image description here"" /></a></p>
<p>Only if the source dataset preview is correct, the output file also will be correct.</p>
"
"63058890","Not able to get logs related to azure data factory mapping data flows from log analytics","<p>We are working on implementing a custom logging solution. Most of the information what we need is already present in log analytics from data factory analytics solution but for getting log info on data flows,  there is a challenge. When querying, we get this error in output. &quot;Too large to parse&quot;. </p>
<p>Since data flows are complex and critical piece in a pipeline, we are in desperate need to get data like rows copied, skipped, read etc of each activities with in data flow. can you pls help how to get those info?</p>
","<azure-data-factory><azure-log-analytics>","2020-07-23 16:26:55","535","0","1","63142696","<p>You can get the same information shown in the ADF portal UI by making a POST request to the below REST endpoint. You can find more information and read about authentication on the following link <code>https://learn.microsoft.com/en-us/rest/api/datafactory/pipelineruns/querybyfactory</code></p>
<p>You can choose to query by factory or for a specific pipeline run id depending on your needs.</p>
<pre><code>https://management.azure.com/subscriptions/&lt;subscription id&gt;/resourcegroups/&lt;resource group name&gt;/providers/Microsoft.DataFactory/factories/&lt;ADF resource Name&gt;/pipelineruns/&lt;pipeline run id&gt;/queryactivityruns?api-version=2018-06-01
</code></pre>
<p>Below is an example of the data you can get from one stage:</p>
<pre><code>{
  &quot;stage&quot;: 7,
  &quot;partitionTimes&quot;: [
    950
  ],
  &quot;lastUpdateTime&quot;: &quot;2020-07-28 18:24:55.604&quot;,
  &quot;bytesWritten&quot;: 0,
  &quot;bytesRead&quot;: 544785954,
  &quot;streams&quot;: {
    &quot;CleanData&quot;: {
      &quot;type&quot;: &quot;select&quot;,
      &quot;count&quot;: 241231,
      &quot;partitionCounts&quot;: [
        950
      ],
      &quot;cached&quot;: false
    },
    &quot;ProductData&quot;: {
      &quot;type&quot;: &quot;source&quot;,
      &quot;count&quot;: 241231,
      &quot;partitionCounts&quot;: [
        950
      ],
      &quot;cached&quot;: false
    }
  },
  &quot;target&quot;: &quot;MergeWithDeltaLakeTable&quot;,
  &quot;time&quot;: 67589,
  &quot;progressState&quot;: &quot;Completed&quot;
}
</code></pre>
"
"63056195","How to remove milliseconds from utcnow() result in Azure data factory","<p>I want to pass a value for parameter usertime, the value should be like 2020-07-23T13:19:31Z , which will be used in my source connection url.</p>
<p>For this i supplied utcnow() function in the value tab. But i realized utcnow() will return the value as &quot;2018-04-15T13:00:00.0000000Z&quot;</p>
<p>To remove the millisecond part i have used the expression substring(utcnow(),1,20).
and also used expression formatDateTime('utcnow()', 'yyyy-MM-ddTHH:mm:ss').</p>
<p>Both my trails are useless where my expression returning error ass invalid parameter.</p>
<p>Could you please help me how can i supply the value 2020-07-23T13:19:31Z in Azure data factory datasource parameters.</p>
","<azure><azure-data-factory>","2020-07-23 14:05:37","1662","0","1","63064193","<p>You don't want utcNow inside quotes, here is an example from one of my pipelines using your format:</p>
<pre><code>@formatDateTime(utcnow(), 'yyyy-MM-ddTHH:mm:ss')
</code></pre>
<p>which gives this result, setting a variable named x:</p>
<pre><code>{
    &quot;name&quot;: &quot;x&quot;,
    &quot;value&quot;: &quot;2020-07-24T13:44:42Z&quot;
}
</code></pre>
<p>Build it in 'Add dynamic content' as you can pick the functions and it will format properly if you aren't familiar.</p>
<p>Your substring won't work because it requires a string for the first parameter and utcnow is a timestamp.</p>
"
"63055604","Does Azure Data Factory supports regular expression matching?","<p>I have this requirement where in my Azure Data Factory pipeline I have a filter activity where I need to check whether name of a file is matching a specific pattern.</p>
<p>eg: &gt;the file should be a csv file and name should starts with 'D'.
&gt;the name of the file must contain a specific word.</p>
<p>These values will come from a config file.
I want the implementation be something like this:
My config file will have an attribute called &quot;filePatternt&quot; which will be a regex.
In the filter activity, I will extract this attribute and will do something like <em><strong>regex.match(@myregular expression from file pattern attribute, @filename).</strong></em></p>
<p>But I am not finding any relevant article or regex function that I can use for the above.</p>
<p>Please let me know for any clue or solution or link you have.</p>
<p>Regards,
Subrat</p>
","<regex><azure-data-factory>","2020-07-23 13:35:55","8191","1","1","63068640","<p>When we copy data from blob to other, Data factory support using some expressions to filter the blobs in wildcard operations, like:</p>
<ol>
<li><code>*</code>: If you want to copy all blobs from a container or folder, additionally specify wildcardFileName as <code>*</code>.</li>
<li><code>*.csv</code>: choose all the csv files from a container or folder.</li>
<li><code>Start*</code>: copy all blobs from a container or folder which name start
with 'Start'.</li>
</ol>
<p>Ref:</p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#copy-activity-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#copy-activity-properties</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#folder-and-file-filter-examples"" rel=""nofollow noreferrer"">Folder and file filter examples</a></li>
</ol>
<p>It may looks like the regex expression, but very simple.
<a href=""https://i.stack.imgur.com/CbS5T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CbS5T.png"" alt=""enter image description here"" /></a></p>
<p>Usually, Data Factory doesn't mention about regex expression, we will consider that it's not supported.</p>
<p>For you purpose &quot;the file should be a csv file and name should starts with 'D'. &gt;the name of the file must contain a specific word.&quot;, You could using bellow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">Data Factory Expression</a> in <code>Add dynamic content</code> help you achieve that:</p>
<pre><code>startsWith()
contains()
endsWith()
and()
</code></pre>
<p>For example, I just create an example pipeline with Get Metadata and a If Condition. Using bellow condition expression to filter the csv file which name start with 'D' and contains a specific word:</p>
<ol>
<li>Get metadata to get the file name.</li>
<li>If condition to filter the file.</li>
</ol>
<p>I prefer to use If condition, the expression is same with Filter conditon:</p>
<pre><code>@and(and(startswith(activity('Get Metadata3').output.itemName,'D'),contains(activity('Get Metadata3').output.itemName,'specifcWords')),endswith(activity('Get Metadata3').output.itemName,'.csv'))
</code></pre>
<p><a href=""https://i.stack.imgur.com/mWYS2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mWYS2.png"" alt=""enter image description here"" /></a></p>
<p>If condition is true(the csv file which name start with 'D' and contains a specific word), we could add the case active.</p>
"
"63038681","Azure Data Factory import multiple valued lookup fields","<p>I need to import data from Sharepoint online list using Azure Data Factory in SQL Azure DB.
The share point online list contained a fields of Lookup type, with options &quot;Allow Multiple Values&quot; ON.</p>
<p>I use Azure Data Factory as tool to copy data from SP to Azure SQL DB.
If field is a normal lookup field, i have no problem and i import his value.
If field is a lookup field with options &quot;Allow Multiple Values&quot; ON, i have problem and i don't see the field in mapping, so i can't import.
I search in documentation but i don't find nothing to highlight this case.</p>
<p>Is there a way to import SP lookup field with options &quot;Allow Multiple Values&quot; ON with Azure Data Factory?</p>
<p>Thanks a lot,
p.</p>
","<azure><azure-data-factory><sharepoint-list><multiple-value><lookupfield>","2020-07-22 16:14:50","725","1","1","63184364","<p>The fields with “Allow Multiple Values” will be treated as complex type which are not supported currently.</p>
<p>We would suggest you to add the feedback in <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">Azure data factory user voice forum</a>. All the feedback shared in this forum will be monitored and reviewed by Azure data factory Microsoft engineering team.</p>
<p>Once you have added the feedback , please share that here , so that other community members can up-vote and/or comment on that</p>
"
"63038174","Inserting an If Condition in the middle of an existing Pipeline","<p>in the latest ADF UI, there doesn't appear to be a way to add an IF Condition Block and reference another Block in the pipeline upon Success.  The Old UI worked seamlessly, but the new one just allows you to add a new block - but not one from the existing Pipeline.</p>
","<azure-data-factory>","2020-07-22 15:46:25","156","-1","1","63045674","<p>We could add the new IF Condition block to the exist pipeline and reference it to other Block easily.</p>
<p>As we know in the Data factory, the default green arrow means Success:
<a href=""https://i.stack.imgur.com/TlVbf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TlVbf.png"" alt=""enter image description here"" /></a></p>
<p>When we add the If Condition block and reference to other block, we could click the green arrow and it will be bold. We can enter 'Delete' key to delete it：</p>
<p><a href=""https://i.stack.imgur.com/51ywD.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/51ywD.gif"" alt=""enter image description here"" /></a></p>
<p>Now we can reference the block to other block, please see bellow gif:
<a href=""https://i.stack.imgur.com/jODpx.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jODpx.gif"" alt=""enter image description here"" /></a></p>
<p>I think the new Data Factory UI also is simple and easy to use.</p>
"
"63017023","Azure data factory copy activity csv to cosmosdb fails","<p><strong>Azure Data factory Copy Activity</strong></p>
<p><strong>Source</strong>: csv file
<strong>Sink</strong>: cosmos db
<strong>Operation</strong>: upsert</p>
<p>Copy activity fails with code '2200', some issue with id field, It was working find before few weeks</p>
<p>My csv file has a number column that I am using as id for cosmos documents, so i can update existing ones</p>
<p><strong>Error details</strong></p>
<pre><code>{
'errorCode': '2200',
'message': 'ErrorCode=UserErrorDocumentDBWriteError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Documents failed to import due to invalid documents which violate some of Cosmos DB constraints: 1) Document size shouldn't exceeds 2MB; 2) Document's 'id' property must be string if any, and must not include the following charaters: '/', '\\\\', '?', '#'; 3) Document's 'ttl' property must not be non-digital type if any.,Source=Microsoft.DataTransfer.DocumentDbManagement,'',
'failureType': 'UserError',
'target': 'Copy_ToCosmosDB',
'details': []
}
</code></pre>
","<azure><azure-cosmosdb><azure-data-factory>","2020-07-21 14:41:33","550","0","1","63028073","<p>When you upsert items in cosmos db,don't change your Partition key.Because the Partition key in cosmos db can't be changed.More Detail refer to this <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/partitioning-overview"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>For example:
My container's Partition key is <code>/name</code>,here is an item like this:</p>
<pre><code>{
    &quot;id&quot;: &quot;2&quot;,
    &quot;no&quot;: 2,
    &quot;name&quot;: &quot;Monica&quot;,
    &quot;createTime&quot;: &quot;2020-06-22T00:00:00.000Z&quot;,
    
}
</code></pre>
<p>My csv file is something like this(update Monica to Monican):</p>
<pre><code>id,no,name,createTime
2,2,Monican,2020-06-22T00:00:00.000Z
</code></pre>
<p>When I run the pipeline,I will get the same error with you.</p>
"
"63009742","Is there any solution for this azure datafactory error?","<p>I am facing below datafactory error while lodaing data from sql server to blob by using data factory.Can anyone give some inputs on this issue?</p>
<p>{
&quot;errorCode&quot;: &quot;2100&quot;,
&quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Reference to database and/or server name in 'demo.INFORMATION_SCHEMA.TABLES' is not supported in this version of SQL Server.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Reference to database and/or server name in 'demo.INFORMATION_SCHEMA.TABLES' is not supported in this version of SQL Server.,Source=.Net SqlClient Data Provider,SqlErrorNumber=40515,Class=15,ErrorCode=-2146232060,State=1,Errors=[{Class=15,Number=40515,State=1,Message=Reference to database and/or server name in 'demo.INFORMATION_SCHEMA.TABLES' is not supported in this version of SQL Server.,},],'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;ListTablesIn sql&quot;,
&quot;details&quot;: []
}</p>
","<azure><azure-sql-database><azure-data-factory>","2020-07-21 07:42:25","999","0","1","63025443","<p>Please use <strong>Query</strong> operation to get the data from <code>INFORMATION_SCHEMA.TABLES</code>:
<a href=""https://i.stack.imgur.com/jR8Ir.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jR8Ir.png"" alt=""enter image description here"" /></a></p>
<p>Or set the schema with INFORMATION_SCHEMA and table name: TABLES in Source dataset:
<a href=""https://i.stack.imgur.com/jZmDv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jZmDv.png"" alt=""enter image description here"" /></a></p>
<p>I tested and it all works well in my Azure SQL database.</p>
<p><strong>Update:</strong></p>
<p>Congratulations that you resolved the issue by choose database in linked service.</p>
"
"63001180","how to pass existing activities to for-each activity in Azure Data Factory","<p>my overall tasks exists of 2 activities.</p>
<ol>
<li>Copy from REST to SQL staging</li>
<li>MAP from SQL staging table to original Table</li>
</ol>
<p>at pipeline level i have 2 variables.</p>
<p>payload to pass jsonPayload and then i want to run the above mentioned 2 activities in a loop against different urls. for that i have a second pipeline level parameter called 'relativeURLs' which is and array of urls.</p>
<p>in a foreach loop i want to call my activity 1 and 2 but when i click foreach it looks like i have to define my activities 1,2 from scratch. cant i simply refer the existing ones?</p>
<p><a href=""https://i.stack.imgur.com/SPLvF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SPLvF.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2020-07-20 17:52:30","191","0","1","63005449","<p>try having those 2 activities in other pipeline which accepts parameters and call this new child pipeline from &quot;Execute Pipeline&quot; avtivity inside for each loop</p>
"
"62998996","ADF Merge-Copying JSON files in Copy Data Activity creates error for Mapping Data Flow","<p>I am trying to do some optimization in ADF. Setup is a third-party tool copies one JSON file per object to a BLOB storage container. These feed to a Mapping Data Flow. The individual files written by the third party tool work great. If I copy these files to a different BLOB folder using an Azure Copy Data activity, the MDF can no longer parse the files and gives an error: &quot;JSON parsing error, unsupported encoding or multiline.&quot; I started this with a Merge Files, but outcome is same regardless of copy behavior I choose.</p>
<p><strong>2ND EDIT:</strong> After another day's work, I have found that the Copy Activity Merge File from JSON to JSON definitely adds an EOL character to each single JSON object as it gets imported to the Merge file. I have also found that the MDF fails definitely with those EOL characters in the Merge file. If I remove all EOL characters from the Merge file, the same MDF will work. For me, this is a bug. The copy activity is adding a character that breaks the MDF. There seems to be a second issue in some of my data that doesn't fail as an individual file but does when concatenated that breaks the MDF when I try to pull all the files together, but I have tested the basic behavior on 1-5000 files and been able to repeat the fail/success tests.</p>
<p>I took the original file, and the copied file, ran them through all of sorts of test, what I eventually found when I dump into Notepad++:
Copied file:</p>
<pre><code>{&quot;CustomerMasterData&quot;:{&quot;Customer&quot;:[{&quot;ID&quot;:&quot;123456&quot;,&quot;name&quot;:&quot;Customer Name&quot;,}]}}\r\n
</code></pre>
<p>Original file:</p>
<pre><code>{&quot;CustomerMasterData&quot;:{&quot;Customer&quot;:[{&quot;ID&quot;:&quot;123456&quot;,&quot;name&quot;:&quot;Customer Name&quot;,}]}}\n
</code></pre>
<p>If I change the copied file from ending with \r\n to \n, the MDF can read the file again. What is going on here? And how do I change the file write behavior or the MDF settings so that I can concatenate or copy files without the CRLF?</p>
<p><strong>EDIT: NEW INFORMATION --  It seems on further review like maybe the minification/whitespace removal is the culprit. If I download the file created by the ADF copy and format it using a JSON formatter, it works. Maybe the CRLF -&gt; LF masked something else. I'm not sure what to do at this point, but its super frustrating.</strong>
Other possibly relevant information:</p>
<ul>
<li>Both the source and sink JSON datasets are set to use UTF-8 (not default(UTF-8), although I tried that). Would a different encoding fix this?</li>
<li>I have tried remapping schemas, creating new data sets, creating new Mapping Data Flows, still get the same error.</li>
</ul>
<p><strong>EDITED for clarity based on comments:</strong>
In the case of a single JSON element in a file, I can get this to work -- data preview returns same success or failure as pipeline when run
<a href=""https://i.stack.imgur.com/hVXB3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hVXB3.jpg"" alt=""single JSON case"" /></a></p>
<p>In the case of multiple documents merged by ADF I get the below instead. It seems on further review like maybe the minification/whitespace removal is the culprit. If I download the file created by the ADF copy and format it using a JSON formatter, it works. Maybe the CRLF -&gt; LF masked something else. I'm not sure what to do at this point, but its super frustrating.
<a href=""https://i.stack.imgur.com/kwqPb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kwqPb.jpg"" alt=""multiple JSON case"" /></a></p>
<p>Repro: Create any valid JSON as a single file, put it in blob storage, use it as a source in a mapping data flow, to do any sink operation. Create a second file with same schema, get them both to run in same flow using wildcard paths. Use a Copy Activity with Merge Files as the Sink Copy Activity and Array of Objects as the File pattern. Try to make your MDF use this new file. If it fails, download the file created by ADF, run it through a formatter (I have used both VS Code -&gt; &quot;Format Document&quot; from standard VS Code JSON extension, and VS 2019 &quot;Unminify&quot; command) and reupload... It should work now.</p>
","<json><azure-data-factory>","2020-07-20 15:38:21","1880","0","2","63008458","<p>According to my test:</p>
<p>1.copy data activity can't change unix(LF) to windows(CRLF).</p>
<p>2.MDF can also parse unix(LF) file and windows(CRLF) file.</p>
<p>Maybe there is something else wrong.
By the way,I see there is a comma after <code>&quot;name&quot;:&quot;Customer Name&quot;</code> in your Original file,I delete it before my test.</p>
"
"62998996","ADF Merge-Copying JSON files in Copy Data Activity creates error for Mapping Data Flow","<p>I am trying to do some optimization in ADF. Setup is a third-party tool copies one JSON file per object to a BLOB storage container. These feed to a Mapping Data Flow. The individual files written by the third party tool work great. If I copy these files to a different BLOB folder using an Azure Copy Data activity, the MDF can no longer parse the files and gives an error: &quot;JSON parsing error, unsupported encoding or multiline.&quot; I started this with a Merge Files, but outcome is same regardless of copy behavior I choose.</p>
<p><strong>2ND EDIT:</strong> After another day's work, I have found that the Copy Activity Merge File from JSON to JSON definitely adds an EOL character to each single JSON object as it gets imported to the Merge file. I have also found that the MDF fails definitely with those EOL characters in the Merge file. If I remove all EOL characters from the Merge file, the same MDF will work. For me, this is a bug. The copy activity is adding a character that breaks the MDF. There seems to be a second issue in some of my data that doesn't fail as an individual file but does when concatenated that breaks the MDF when I try to pull all the files together, but I have tested the basic behavior on 1-5000 files and been able to repeat the fail/success tests.</p>
<p>I took the original file, and the copied file, ran them through all of sorts of test, what I eventually found when I dump into Notepad++:
Copied file:</p>
<pre><code>{&quot;CustomerMasterData&quot;:{&quot;Customer&quot;:[{&quot;ID&quot;:&quot;123456&quot;,&quot;name&quot;:&quot;Customer Name&quot;,}]}}\r\n
</code></pre>
<p>Original file:</p>
<pre><code>{&quot;CustomerMasterData&quot;:{&quot;Customer&quot;:[{&quot;ID&quot;:&quot;123456&quot;,&quot;name&quot;:&quot;Customer Name&quot;,}]}}\n
</code></pre>
<p>If I change the copied file from ending with \r\n to \n, the MDF can read the file again. What is going on here? And how do I change the file write behavior or the MDF settings so that I can concatenate or copy files without the CRLF?</p>
<p><strong>EDIT: NEW INFORMATION --  It seems on further review like maybe the minification/whitespace removal is the culprit. If I download the file created by the ADF copy and format it using a JSON formatter, it works. Maybe the CRLF -&gt; LF masked something else. I'm not sure what to do at this point, but its super frustrating.</strong>
Other possibly relevant information:</p>
<ul>
<li>Both the source and sink JSON datasets are set to use UTF-8 (not default(UTF-8), although I tried that). Would a different encoding fix this?</li>
<li>I have tried remapping schemas, creating new data sets, creating new Mapping Data Flows, still get the same error.</li>
</ul>
<p><strong>EDITED for clarity based on comments:</strong>
In the case of a single JSON element in a file, I can get this to work -- data preview returns same success or failure as pipeline when run
<a href=""https://i.stack.imgur.com/hVXB3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hVXB3.jpg"" alt=""single JSON case"" /></a></p>
<p>In the case of multiple documents merged by ADF I get the below instead. It seems on further review like maybe the minification/whitespace removal is the culprit. If I download the file created by the ADF copy and format it using a JSON formatter, it works. Maybe the CRLF -&gt; LF masked something else. I'm not sure what to do at this point, but its super frustrating.
<a href=""https://i.stack.imgur.com/kwqPb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kwqPb.jpg"" alt=""multiple JSON case"" /></a></p>
<p>Repro: Create any valid JSON as a single file, put it in blob storage, use it as a source in a mapping data flow, to do any sink operation. Create a second file with same schema, get them both to run in same flow using wildcard paths. Use a Copy Activity with Merge Files as the Sink Copy Activity and Array of Objects as the File pattern. Try to make your MDF use this new file. If it fails, download the file created by ADF, run it through a formatter (I have used both VS Code -&gt; &quot;Format Document&quot; from standard VS Code JSON extension, and VS 2019 &quot;Unminify&quot; command) and reupload... It should work now.</p>
","<json><azure-data-factory>","2020-07-20 15:38:21","1880","0","2","67057222","<p>don't know if you already solved the problem: I came across the exact same problem 3 days ago and after several tries I found a solution:</p>
<ol>
<li>in the copy data activity under sink settings, use &quot;set of objects&quot; (instead of &quot;array of objects&quot;) under File Pattern, so that the merged big JSON has the value of the original small JSON files written per line</li>
<li>in the MDF after setting up the wildcard paths with the *.json pattern, under JSON Settings select: Document per line as the Document form.<br />
After that you should be good to go, as least it solved my problem. The automatic written CRLF in &quot;array of objects&quot; setting in the copy data activity should be a default setting and MSFT should provide the option to omit it in the settings in the future.</li>
</ol>
"
"62997613","DateTime format error, Copy Activity. Azure data factory","<p>I am copying data from CSV to Azure MySQL. I've a date field  with format MM/DD/YYYY in source. It is throwing an error while copying data to MySQL. I tried changing the datatype in field mapping, pipeline ran succesfulbut data is not loaded. I need to convert the format to YYYY-MM-DD.</p>
<p>&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;'Type=MySql.Data.MySqlClient.MySqlException,Message=Incorrect date value: '12/06/2010' for column 'xxx_dt' at row 49,Source=MySqlConnector,''Type=MySql.Data.MySqlClient.MySqlException,Message=Incorrect date value: '12/06/2010' for column 'XXX_DT' at row 49,Source=MySqlConnector,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,</p>
<p>Please suggect.</p>
<p>thanks.</p>
","<azure-data-factory>","2020-07-20 14:26:57","1026","0","1","63006208","<p>Data Factory can not convert date format from 'MM/DD/YYYY' to 'YYYY-MM-DD' directly.</p>
<p>If you want convert the date format from 'MM/DD/YYYY' to 'YYYY-MM-DD', please using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data Flow</a> with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived Column</a>.</p>
<p>For example, I have csv file with the column date format with &quot;MM/DD/YYYY&quot;:
<a href=""https://i.stack.imgur.com/bDsBb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bDsBb.png"" alt=""enter image description here"" /></a></p>
<p>I use Data Flow with bellow Derived Column expression:</p>
<pre><code>toDate(concat(split({ born},'/')[3],'-',split({ born},'/')[1],'-',split({ born},'/')[2]))
</code></pre>
<p><a href=""https://i.stack.imgur.com/Ql8Vv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ql8Vv.png"" alt=""enter image description here"" /></a></p>
<p>The column <code>born</code> is convert to 'yyyy-mm-dd' format:
<a href=""https://i.stack.imgur.com/qbnMs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qbnMs.png"" alt=""enter image description here"" /></a></p>
<p>You could follow my steps.</p>
"
"62997058","ADF - How to copy data from an excel sheet referring to the part of the file name","<p>What I'm trying to do is copy data from an excel doc in a blob to a db. I want to access the file using 'CodeMapping' only since it's original name is like this 'CodeMapping-acbcb08e-gca6-457a-8g07-273941021w5z'
How should I do it? Can someone help me?</p>
<p>Copy activity - This is what I tried. But this is wrong
<a href=""https://i.stack.imgur.com/br1Ua.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/br1Ua.png"" alt=""enter image description here"" /></a></p>
<p>Dataset</p>
<p><a href=""https://i.stack.imgur.com/QQMxQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QQMxQ.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-07-20 13:59:57","150","0","1","63005855","<p>Pleas use:</p>
<pre><code>CodeMapping*.xlsx
</code></pre>
<p><a href=""https://i.stack.imgur.com/8nOCG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8nOCG.png"" alt=""enter image description here"" /></a></p>
<p>This expression mean access the .xlsx file which filename start with &quot;CodeMapping&quot;.</p>
<p>Ref:</p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#blob-storage-as-a-source-type"" rel=""nofollow noreferrer"">Blob storage as a source type</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#source-transformation"" rel=""nofollow noreferrer"">Source transformation</a></li>
</ol>
"
"62996295","List all Azure Data factories in the subscription using Python","<p>I am trying to list all Azure Datafactories in a subscription using below code. I know the &quot;list&quot; from factories operation class is not properly defined but I am not getting much info about its usage sample from documentation, if any one please advise. #newtopython</p>
<pre><code>     from azure.mgmt.resource import ResourceManagementClient
     from azure.mgmt.datafactory import DataFactoryManagementClient
     from DataFactoryManagement import list
     from azure.mgmt.datafactory.models import *
     from datetime import datetime, timedelta
     import time
     credentials = ServicePrincipalCredentials(
         client_id='#####################',
         secret='###########',
         tenant='#############################'
     )
     subscription_id = '################'
     client = DataFactoryManagementClient(credentials, subscription_id)
     adf = client.factories:list()
     print (adf)
</code></pre>
","<python><azure><azure-data-factory>","2020-07-20 13:18:51","255","1","1","63006342","<p>If you want to list all the ADFs in the subscription, you need to use the <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.operations.factoriesoperations?view=azure-python#list-custom-headers-none--raw-false----operation-config-"" rel=""nofollow noreferrer""><code>list</code></a> method, it works fine on my side.</p>
<p><em>Sample:</em></p>
<pre><code>from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.datafactory import DataFactoryManagementClient

subscription_id = 'xxxxx'
credentials = ServicePrincipalCredentials(client_id='xxxxx', secret='xxxxx', tenant='xxxxx')
adf_client = DataFactoryManagementClient(credentials, subscription_id)

Factories = adf_client.factories.list()
for factory in Factories:
    print(factory)
</code></pre>
<p><a href=""https://i.stack.imgur.com/OvqNo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OvqNo.png"" alt=""enter image description here"" /></a></p>
"
"62993387","Truncate (NOT ROUND) decimal numbers in Azure Data Factory Mapping Data Flow","<p>Do you know a way of truncating decimal numbers (not round) in <strong>mapping data flow</strong> in Azure Data Factory?</p>
<p>Example:</p>
<p><strong>Input:</strong> 123.456</p>
<p><strong>Output:</strong> 123.45 (not 123.46)</p>
","<azure><azure-data-factory><dataflow>","2020-07-20 10:26:42","948","2","1","62994652","<p>The answer is <strong>round(123.456,2,4)</strong> --&gt; 2 is the post characters, 4 is the round option - <strong>FLOOR</strong></p>
"
"62991155","How to uncompress rar files using Azure DataFactory","<p>We have a new client, while landing the project we gave them a blob storage for them to leave files so we could later automate and process the information.</p>
<p>The idea is to use Azure Datafactory but we find no way of dealing with .rar files, and even .zip, being it files from windows, are giving us trouble. And since it is the clien giving the .rar format, we wanted to make absolutely sure there is no way to process before asking them to change it, or deploying a databricks or similar service just for the purpose of transforming the file.</p>
<p>Is there any way to get a .rar file from a blob storage, uncompress it, then process it?</p>
<p>I have been looking in posts like <a href=""https://stackoverflow.com/a/58441630"">this</a> and related official documentation and closest we have come is using ZipDeflate, but it does not seem to fill our requirement.</p>
<p>Thanks in advance!</p>
","<azure><compression><azure-data-factory>","2020-07-20 08:09:34","638","0","2","62992051","<p>Data factory compression only supported types are GZip, Deflate, BZip2, and ZipDeflate.</p>
<p>For the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs-legacy#compression-support"" rel=""nofollow noreferrer"">Unsupported file types and compression formats</a>, Data Factory provides some workarounds for us:</p>
<p>You can use the extensibility features of Azure Data Factory to transform files that aren't supported. Two options include Azure Functions and custom tasks by using Azure Batch.</p>
<p>You can see a sample that uses an Azure function to <a href=""https://github.com/Azure/Azure-DataFactory/tree/master/SamplesV2/UntarAzureFilesWithAzureFunction"" rel=""nofollow noreferrer"">extract the contents of a tar file</a>. For more information, see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Functions activity</a>.</p>
<p>You can also build this functionality using a custom dotnet activity. Further information is available <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Next way, you may need to figure out how to using Azure function to extract the contents of a rar file.</p>
"
"62991155","How to uncompress rar files using Azure DataFactory","<p>We have a new client, while landing the project we gave them a blob storage for them to leave files so we could later automate and process the information.</p>
<p>The idea is to use Azure Datafactory but we find no way of dealing with .rar files, and even .zip, being it files from windows, are giving us trouble. And since it is the clien giving the .rar format, we wanted to make absolutely sure there is no way to process before asking them to change it, or deploying a databricks or similar service just for the purpose of transforming the file.</p>
<p>Is there any way to get a .rar file from a blob storage, uncompress it, then process it?</p>
<p>I have been looking in posts like <a href=""https://stackoverflow.com/a/58441630"">this</a> and related official documentation and closest we have come is using ZipDeflate, but it does not seem to fill our requirement.</p>
<p>Thanks in advance!</p>
","<azure><compression><azure-data-factory>","2020-07-20 08:09:34","638","0","2","63005521","<p>you can use logic apps
you can use webhook activity calling a runbook</p>
<p>both are easiee than using a custom activity</p>
"
"62966722","Azure Data Factory copy activity JSON data type conversion issue","<p>I have an azure data factory pipeline for fetch the data from a third party API and store the data to the data-lake as .json format. When i click the import schema, it shows the correct datatype format.
<a href=""https://i.stack.imgur.com/SdCWg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SdCWg.png"" alt=""enter image description here"" /></a></p>
<p>When I set the above mentioned data-lake as a source of data flow activity, the Int64 data type convert to boolean. I have checked the Microsoft documents and knew if the value is 0 or 1, it automatically convert to boolean. How can I avoid this data type conversion?</p>
<p><a href=""https://i.stack.imgur.com/EsBPH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EsBPH.png"" alt=""enter image description here"" /></a></p>
","<json><azure><azure-data-lake><azure-data-factory>","2020-07-18 09:10:20","2923","0","2","62984312","<p>First, verify if you have checked 'Infer drifted column types' to true under Source Settings.</p>
<p>Data Factory detects the data type as boolean if the values in the source column are only 1 or 0. This could be a potential bug.</p>
<p>One way around is, since you are using Data Flow, Add derivations for the columns using a Case statement and derive 1 &amp; 0 in output based on boolean value.</p>
"
"62966722","Azure Data Factory copy activity JSON data type conversion issue","<p>I have an azure data factory pipeline for fetch the data from a third party API and store the data to the data-lake as .json format. When i click the import schema, it shows the correct datatype format.
<a href=""https://i.stack.imgur.com/SdCWg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SdCWg.png"" alt=""enter image description here"" /></a></p>
<p>When I set the above mentioned data-lake as a source of data flow activity, the Int64 data type convert to boolean. I have checked the Microsoft documents and knew if the value is 0 or 1, it automatically convert to boolean. How can I avoid this data type conversion?</p>
<p><a href=""https://i.stack.imgur.com/EsBPH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EsBPH.png"" alt=""enter image description here"" /></a></p>
","<json><azure><azure-data-lake><azure-data-factory>","2020-07-18 09:10:20","2923","0","2","62988004","<p>The easiest way is that just reset the all schema to String, that means don't convert the data type in Source dataset.</p>
<p>For example, this my source dataset schema and data, all the values in <code>setNum</code> are 1 or 0:
<a href=""https://i.stack.imgur.com/ZsBsc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZsBsc.png"" alt=""enter image description here"" /></a></p>
<p>Data Flow Source Projection, the data type of setNum first considered as Boolean.</p>
<p><a href=""https://i.stack.imgur.com/cmuOs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cmuOs.png"" alt=""enter image description here"" /></a></p>
<p>Reset schema: all the data type will be string.</p>
<p><a href=""https://i.stack.imgur.com/xBD23.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xBD23.png"" alt=""enter image description here"" /></a></p>
<p>Then data factory will convert the data type in Sink level. It is similar with copy data from csv file.</p>
<p>Update:</p>
<p>You can first reset the schema to String.</p>
<p>Then using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived Column</a> to change/convert the data type as you want.</p>
<p>Using bellow expressions:</p>
<pre><code>toShort()
toString()
toShort()
</code></pre>
<p><a href=""https://i.stack.imgur.com/sjfQA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sjfQA.png"" alt=""enter image description here"" /></a></p>
<p>This will solve the problem.</p>
"
"62958162","How to pass date value in Rest API parameter using Azure Data factory","<p>I have a API Url which having one of the parameter with TIMESTAMP.The Url will be like</p>
<p>Actual Url: <a href=""https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=2020-07-17T11%3A09%3A25Z&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D"" rel=""nofollow noreferrer"">https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=2020-07-17T11%3A09%3A25Z&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D</a></p>
<p>and time stamp value is 2020-07-17T11%3A09%3A25Z.
I want to avoid timestamp value to become expire and for that i created a parameter TS with value utcNow().  And using that parametr in urllike</p>
<p><a href=""https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=dataset().TS&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D"" rel=""nofollow noreferrer"">https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=dataset().TS&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D</a></p>
<p>am using this Url to connect rest api service as source in ADF. I am trying to replace Timestampvalue with a parameter value utcNow(). But i am failing with error Expected EOF, found ':' at 6.</p>
<p>Can you help me how can i replace timestamp value from 2020-07-1217T11%3A09%3A25Z to system to avoid timevalue expiration.</p>
","<json><azure><azure-data-factory><rest>","2020-07-17 16:29:26","970","0","2","62963829","<p>Store these values in variables and then concatenate.</p>
"
"62958162","How to pass date value in Rest API parameter using Azure Data factory","<p>I have a API Url which having one of the parameter with TIMESTAMP.The Url will be like</p>
<p>Actual Url: <a href=""https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=2020-07-17T11%3A09%3A25Z&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D"" rel=""nofollow noreferrer"">https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=2020-07-17T11%3A09%3A25Z&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D</a></p>
<p>and time stamp value is 2020-07-17T11%3A09%3A25Z.
I want to avoid timestamp value to become expire and for that i created a parameter TS with value utcNow().  And using that parametr in urllike</p>
<p><a href=""https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=dataset().TS&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D"" rel=""nofollow noreferrer"">https://mtplace.api.onoc.com/V2/products/00vkxX?app_id=adm12&amp;access_mdm=computer&amp;TIMESTAMP=dataset().TS&amp;hash_code=O2cnasnajsaBz4qMdKDV3xw2mniiiRQasCuEv65auvjCs%3D</a></p>
<p>am using this Url to connect rest api service as source in ADF. I am trying to replace Timestampvalue with a parameter value utcNow(). But i am failing with error Expected EOF, found ':' at 6.</p>
<p>Can you help me how can i replace timestamp value from 2020-07-1217T11%3A09%3A25Z to system to avoid timevalue expiration.</p>
","<json><azure><azure-data-factory><rest>","2020-07-17 16:29:26","970","0","2","62985108","<p>Timestamp parameter needs URL encoding. I could not find any inbuilt function for encoding. So alternative way is to use a dataflow and column derivation with replace function ( replace ':' with '%3A' and space with 'T'). You may need some more transformations.</p>
<p>Then, timestamp '2020-07-19 19:50:40.851' can be passed as '2020-07-19T19%3A50%3A40Z'</p>
"
"62955426","Azure Data Factory access issue","<p>New to Azure and I was trying to play a bit with Data Factory v2.</p>
<p>Got a free subscription and created a data factory within that subscription, and assigned my user the <code>Data Factory Contributor</code> role on the subscription.</p>
<p>My problem is that whenever I try to author a pipeline and I navigate to the data factory web page I keep getting an error &quot;Azure Data Factory  is not accessible&quot;.</p>
<p>Any idea why this happens?</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2020-07-17 13:51:07","220","0","2","62989265","<p>It was a browser compatibility issues. The Azure Data Factory Web app is supported on Chrome, Edge and IE 11, not supported on brave and safari, because of the difference of browser rendering core and js engine.</p>
"
"62955426","Azure Data Factory access issue","<p>New to Azure and I was trying to play a bit with Data Factory v2.</p>
<p>Got a free subscription and created a data factory within that subscription, and assigned my user the <code>Data Factory Contributor</code> role on the subscription.</p>
<p>My problem is that whenever I try to author a pipeline and I navigate to the data factory web page I keep getting an error &quot;Azure Data Factory  is not accessible&quot;.</p>
<p>Any idea why this happens?</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2020-07-17 13:51:07","220","0","2","70976739","<p>Maybe your data factory is in an invalid state. When you click in your Data factory it opens the overview page, check if Status is &quot;Succeeded&quot;.</p>
"
"62946748","Collect exp. Function with distinct Values in Mapping Data Flow ADF Aggregate Transformation","<p>I want to use collect(column1) function that collects all row values for a group By column2 in Agg. Transformation. But since that column1 has duplicates values, I get duplicates in my returned array. I want a function that collects all distinct values.</p>
<p><a href=""https://i.stack.imgur.com/YApLE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YApLE.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><dataflow><collect>","2020-07-17 03:43:52","1115","0","2","62951150","<p>There is no collectDistinct() function so you cant achieve this by function in dataflow.</p>
<p>You can try this:
create two Aggregate Transformation.</p>
<p>First,group by base model number and modelDocId,then add a column(DModelDocId) and expression is <code>first(modelDocId)</code>.</p>
<p>Second,group by base model number，then add a column(modelDocIds) and expression is <code>collect(DModelDocId)</code>.</p>
<p>Hope this can help you.</p>
"
"62946748","Collect exp. Function with distinct Values in Mapping Data Flow ADF Aggregate Transformation","<p>I want to use collect(column1) function that collects all row values for a group By column2 in Agg. Transformation. But since that column1 has duplicates values, I get duplicates in my returned array. I want a function that collects all distinct values.</p>
<p><a href=""https://i.stack.imgur.com/YApLE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YApLE.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><dataflow><collect>","2020-07-17 03:43:52","1115","0","2","65798647","<p>This has been answered, but there is easier way to collect unique values in aggregate situations like this, without needing another group by. In our scenario, we had the data as follows</p>
<p>| ORDER_NO | BRAND          |</p>
<p>| -------- | -------------- |</p>
<p>| ORD001   | ExampleBrand1  |</p>
<p>| ORD001   | ExampleBrand1  |</p>
<p>| ORD001   | ExampleBrand2  |</p>
<p>We wanted to get this to one row per ORDER_NO, with unique brands listed against it. Sample output as follows</p>
<p>| ORDER_NO | BRANDS |</p>
<p>| -------- | -------------- |</p>
<p>| ORD001   | ExampleBrand1,ExampleBrand2 |</p>
<p>So we did add an aggregate transformation (group by) on ORDER_NO, and used collect(BRAND) to collect all BRAND values in an array named BRANDS_ARR.</p>
<p>After this, we added a Derived Column transformation, and converted the array of BRAND strings to unique string BRANDS as follows</p>
<p>column name: BRANDS</p>
<p>Expression:
<code>toString(reduce(BRANDS_ARR, '', iif((instr(#acc, #item) &gt; 0), #acc, #acc + #item + ',') , #result)) </code></p>
<p>Note that we use reduce function with iif and instr functions to decide if the next value should be accumulated or not. This way duplicates get filtered and you get a &quot;Set&quot; like functionality.</p>
<p>Nit: Above snippet gets an extra comma in the end, which can easily be stripped off using replace function.</p>
"
"62944366","Azure Data Factory split file by file size","<p>out of my two weeks of Azure experience. I want to split files based on a size. For example there is a table with 200k rows I would like to set a parameter to split that table into multiple files with a limit of 100Mb per file (if that makes sense). It will return <code>N</code> number of files depending of the table size. something like:</p>
<p><code>my_file_1ofN.csv</code></p>
<p>I was walking through the documentation, blogs and videos and could do some POC with Azure Functions, Azure Batch, and Databricks with a python script in my personal account. The problem is the company doesn't let me use any of these approaches.</p>
<p>So I split the file using the number of partitions but these files are with a different sizes depending on the table and the partition.</p>
<p>Is there a way to accomplish this? I'm experimenting with <code>lookups</code> and <code>foreach</code> activities in the pipeline now but with not good results.</p>
<p>Any idea or clue will be welcome. Thanks!!</p>
","<azure><azure-data-factory>","2020-07-16 22:22:25","3167","2","1","62955643","<p>I haven't been able to figure this out by size, but if you can get a total row count, you can use DataFlow to output a rough approximation based on row count.</p>
<p><strong>IN THE PIPELINE</strong>:</p>
<p>In this example, I am reading data out of an Azure Synapse SQL Pool, so I'm running a Lookup to calculate the number of &quot;Partitions&quot; based on 8,000,000 rows per partition:</p>
<p><a href=""https://i.stack.imgur.com/ho5Ih.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ho5Ih.png"" alt=""enter image description here"" /></a></p>
<p>I then capture the result as a variable:</p>
<p><a href=""https://i.stack.imgur.com/rmSfb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rmSfb.png"" alt=""enter image description here"" /></a></p>
<p>Next, pass the variable to the DataFlow:</p>
<p><a href=""https://i.stack.imgur.com/fX7nD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fX7nD.png"" alt=""enter image description here"" /></a></p>
<p>NOTE: the <em>@int</em> cast is because DataFlow supports int but pipeline's do not, so in the pipeline the data is stored in a string variable.</p>
<p><strong>IN THE DATAFLOW</strong>:</p>
<p>Create an <em>int</em> parameter for &quot;partitionCount&quot;, which is passed in from the pipeline:</p>
<p><a href=""https://i.stack.imgur.com/oJwIB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oJwIB.png"" alt=""enter image description here"" /></a></p>
<p><strong>SOURCE</strong>:</p>
<p>In the Optimize tab you can control how the source the data is partitioned on read. For this purpose, switch to &quot;Set Partitioning&quot; and select Round Robin based on the partitionCount variable:</p>
<p><a href=""https://i.stack.imgur.com/gD7ws.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gD7ws.png"" alt=""enter image description here"" /></a></p>
<p>This will split the incoming data into X number of buckets based on the parameter.</p>
<p><strong>SINK</strong>:</p>
<p>Under the Settings tab, experiment with the &quot;File name option&quot; settings to control the output name. The options are a bit limited, so you may have trouble getting exactly what you want:</p>
<p><a href=""https://i.stack.imgur.com/FqMVW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FqMVW.png"" alt=""enter image description here"" /></a></p>
<p>Since you have already partitioned the data, just use the default Source Optimization settings:</p>
<p><a href=""https://i.stack.imgur.com/5nGG7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5nGG7.png"" alt=""enter image description here"" /></a></p>
<p><strong>RESULT</strong>:</p>
<p>This will produce X number of files with a numbered naming scheme and consistent file size:</p>
<p><a href=""https://i.stack.imgur.com/TyAmr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TyAmr.png"" alt=""enter image description here"" /></a></p>
"
"62942016","Type Conversion Error in Copy activity: Azure data factory","<p>I am trying to load multiple csv files from blob to Azure MySQL. I am using getMetadata activity and foreach activity, within forEach activity one copy activity to copy data from blob to sink. Pipeline is failing with error</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;'Type=Microsoft.Azure.Data.Governance.Plugins.Core.TypeConversionException,Message=Exception occurred when converting value '' for column name 'DATASTORE_ID' from type 'String' (precision:, scale:) to type 'Int32' (precision:0, scale:0). Additional info: Input string was not in a correct format.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.FormatException,Message=Input string was not in a correct format.,Source=mscorlib,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy data1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>I am not importing any schema in Mapping tab.
please suggest the solution.</p>
<p>Thanks in Advance.</p>
<p>Geetha.</p>
","<azure-data-factory>","2020-07-16 19:15:11","1159","0","1","62985967","<p>Replace 'Copy Data' with 'Data Flow' with straight field to field mapping and the copy from blob to sink works fine.</p>
"
"62942001","Test multiple If condition in Azure Data factory and string equality test for the activities","<p>I want to test a string value in Azure data factory.<br />
Based on this String value my pipeline would get trigger.</p>
<p>I am collecting this value as a parameter from automation runbook.<br />
If the parameter value is &quot;Nike&quot; then Nike pipeline will trigger and else some other pipeline.
The If condition having two options-</p>
<ol>
<li>True Activity</li>
<li>False Activity</li>
</ol>
<p>so if the parameter does not match with a specified value then my false block would get trigger.<br />
But I have multiple values hence True/False condition will not suffice my requirement.</p>
<p><strong>So my first question is-</strong> Can we test more than one condition in  ADF <code>If</code> condition activity so that whichever String value it matches that specific pipeline would get trigger, if not so then which activity should I use.</p>
<p><strong>My second question is-</strong> How do I test the equality of parameter value so that I can trigger the specified pipeline.<br />
below code I have tried but thrown the error.
<code>@equals(pipeline().parameters.clientName,'Nike')</code><br />
Please note- clientName is the parameter name</p>
","<azure><azure-data-factory>","2020-07-16 19:14:11","7466","0","1","62947706","<p><img src=""https://i.stack.imgur.com/lmK8k.pngstrong"" alt=""1"" /></p>
<p>Did you try &quot;Switch&quot; activity ?</p>
<p>See below link:
<a href=""https://www.sqlservercentral.com/blogs/switch-activity-in-azure-data-factory-container-with-many-ifs"" rel=""nofollow noreferrer"">https://www.sqlservercentral.com/blogs/switch-activity-in-azure-data-factory-container-with-many-ifs</a></p>
"
"62941784","Azure Data Factory Trigger","<p>We have an external source which will copy a file on to our Azure File Server. The size of the file is around 10 GB. I want to copy this file to Azure Blob Storage as soon as the file copy is complete on Azure file server using Azure Data Factory.
The vendor cannot copy this file to Blob container.
Can someone help me what type of trigger i can configure for this. I was able to do the copy manually but i am looking if we can automate it. I cannot even schedule this activity as the file copy from external source is at random time.</p>
<p>Thanks</p>
","<azure-data-factory>","2020-07-16 18:59:16","94","0","1","62964558","<p>Maybe.. you can have a logic app triggered on an event on storage which will trigger data factory pipeline ..copy activity can do the job for you inside ADF..</p>
"
"62940780","POST data to REST API using Azure Data Factory","<p>I am trying to setup Web activity to POST data from Azure Data Lake Gen 1 to REST API service, followed similar setup performed in this <a href=""https://stackoverflow.com/questions/62322109/how-can-i-pass-file-as-parameter-in-post-request-web-activity-azure-data-facto/62336108#62336108"">link</a> but couldn't succeed due to error 'Missing file'.</p>
<p>Sample CURL request - Successful when attempted with POSTMAN</p>
<pre><code>curl --location --request POST 'https://xxxx/files' \
--header 'X-API-TOKEN: xxxx' \
--form 'file=xxxxx'
</code></pre>
<p>Dataset settings:
<a href=""https://i.stack.imgur.com/CtuvM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CtuvM.jpg"" alt=""enter image description here"" /></a></p>
<p>Web activity settings:</p>
<p><a href=""https://i.stack.imgur.com/SRPuU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SRPuU.jpg"" alt=""enter image description here"" /></a></p>
<p>Run Error:
<a href=""https://i.stack.imgur.com/9aHNT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9aHNT.jpg"" alt=""enter image description here"" /></a></p>
","<azure><api><azure-data-factory>","2020-07-16 17:56:31","4411","1","2","63397329","<p>Raised ticket on this to Microsoft and they stated this feature is yet to be implemented for Data Factory.</p>
<p>But for now I have achieved this through Logic Apps.</p>
<p>Logic Apps supports multiform data, where the data is read from Data lake and named as a file before sending it as attachment to target server.</p>
<p><a href=""https://i.stack.imgur.com/hlYbD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hlYbD.jpg"" alt=""enter image description here"" /></a></p>
<pre><code>{
  &quot;$content-type&quot;: &quot;multipart/form-data&quot;,
  &quot;$multipart&quot;: [
    {
      &quot;body&quot;: &quot;@{body('Read_File_from_Data_Lake')}&quot;,
      &quot;headers&quot;: {
        &quot;Content-Disposition&quot;: &quot;form-data; name=file; filename=Sample.csv&quot;
      }
    }
  ]
}
</code></pre>
"
"62940780","POST data to REST API using Azure Data Factory","<p>I am trying to setup Web activity to POST data from Azure Data Lake Gen 1 to REST API service, followed similar setup performed in this <a href=""https://stackoverflow.com/questions/62322109/how-can-i-pass-file-as-parameter-in-post-request-web-activity-azure-data-facto/62336108#62336108"">link</a> but couldn't succeed due to error 'Missing file'.</p>
<p>Sample CURL request - Successful when attempted with POSTMAN</p>
<pre><code>curl --location --request POST 'https://xxxx/files' \
--header 'X-API-TOKEN: xxxx' \
--form 'file=xxxxx'
</code></pre>
<p>Dataset settings:
<a href=""https://i.stack.imgur.com/CtuvM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CtuvM.jpg"" alt=""enter image description here"" /></a></p>
<p>Web activity settings:</p>
<p><a href=""https://i.stack.imgur.com/SRPuU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SRPuU.jpg"" alt=""enter image description here"" /></a></p>
<p>Run Error:
<a href=""https://i.stack.imgur.com/9aHNT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9aHNT.jpg"" alt=""enter image description here"" /></a></p>
","<azure><api><azure-data-factory>","2020-07-16 17:56:31","4411","1","2","75748994","<p>It is now supported I have done this today, you need to select POST on the REST activity</p>
<p><a href=""https://i.stack.imgur.com/GK1cU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GK1cU.png"" alt=""POST Example in Data Flow"" /></a></p>
<p>and the body is created from the mapping fields. Each mapped field represents a JSON element in the body object.</p>
<p><a href=""https://i.stack.imgur.com/KvVBc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KvVBc.png"" alt=""enter image description here"" /></a></p>
<p>This will result in the body of the request being:</p>
<pre><code>{
    &quot;jsonElement1&quot; : &quot;&lt;my_row_status_value&gt;&quot;,
    &quot;jsonElement2&quot; : &quot;&lt;my_row_ProductType_value&gt;&quot;
}
</code></pre>
"
"62939759","How can we call rest api using ssl certificate in azure data factory?","<p>I have been trying to configure/call a rest full api through azure data factory where it gives response in xml format.</p>
<ol>
<li><p>Using REST Linked Service: it doesn't have the certificate authentication type. So cannot go with this.</p>
</li>
<li><p>Using HTTP Linked Service: it has the certificate authentication and able to create it successfully but when try to create a dataset it doesn't have the xml format to choose.
I have even read the supported file formats in azure data factory and mentioned the same.</p>
</li>
</ol>
<p>Is there any other posbilities where im missing in azure data factory.
Could anyone help on this please.</p>
<p>Else i will go with Azure Logic app or Azure Databricks.
Still i need to know how can we configure in above two referred azure resources but i will try it later on.</p>
","<azure><azure-data-factory>","2020-07-16 16:57:26","984","1","1","62948814","<p>XML format is supported for the following connectors: Amazon S3, Azure Blob, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2, Azure File Storage, File System, FTP, Google Cloud Storage, HDFS, HTTP, and SFTP. It is supported as source but not sink.</p>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-xml"" rel=""nofollow noreferrer"">XML format in Azure Data Factory</a></p>
<p>When we create dataset in Web active, we can choose the XML format:</p>
<p>For example, click New--&gt;Blog Storage--&gt;XML:
<a href=""https://i.stack.imgur.com/67VZU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/67VZU.png"" alt=""enter image description here"" /></a></p>
<p>Please check if your source supports the XML format file.</p>
"
"62939724","Error with No Escape Character in Mapping Data Flow in Data Factory","<p>TLDR: Why does Azure Data Factory Data Flow not allow you to have no escape character?
<br>
<br></p>
<p>We have bad source data from a source that is unlikely to update it on their end (this is the nicest way I could phrase this). They have multiple columns where the value in the column is 01F\ or 8239\ and the backslash is written in their spec to be part of the value, not to be considered an escape character like it is standardized to be in the entire world.</p>
<p>The overall set up of the files are that they are comma delimited, each column's contents is in &quot; &quot;, and we have all of the normal new line characters. It's just the backslash that is not complying with standards. E.g.</p>
<pre><code>&quot;Column 1&quot;,&quot;Column 2&quot;,&quot;Column 3&quot;,&quot;Column 4&quot;
&quot;John&quot;,&quot;01F\&quot;,&quot;34&quot;,&quot;NY&quot;
&quot;Jane&quot;,&quot;3K&quot;,&quot;8239\&quot;,&quot;CA&quot;

|---------------------|------------------|------------------|------------------|
|      Column 1       |     Column 2     |     Column 3     |     Column 4     |
|---------------------|------------------|------------------|------------------|
|        &quot;John&quot;       |       &quot;01F\&quot;     |       &quot;34&quot;       |         &quot;NY&quot;     |
|---------------------|------------------|------------------|------------------|
|        &quot;Jane&quot;       |        &quot;3K&quot;      |      &quot;8239\&quot;     |         &quot;CA&quot;     |
|---------------------|------------------|------------------|------------------|
</code></pre>
<br>
<br>
In Azure Data Factory we are trying to see if we can make it ignore the \ as being an escape character. (FYI when we leave it as being treated as an escape character, it pulls the column right after the column with the backslash into one column). We can see in the data set where to set it so that there is no escape character.
<p><a href=""https://i.stack.imgur.com/Wa14K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wa14K.png"" alt=""enter image description here"" /></a></p>
<p>However when we then add that data set to our data flow and attempt to preview the data there, we get an error that we can't have no escape character in the data flow, and that quote character should be no quote character when we have no escape character.</p>
<p><a href=""https://i.stack.imgur.com/lSF1V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lSF1V.png"" alt=""enter image description here"" /></a></p>
<p>If we try to go back and also set no quote characters (which we don't actually want to do, just to test if that'll let it work), we get an error that the data flow can't have no escape or quote characters.</p>
<p><a href=""https://i.stack.imgur.com/UpNr4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UpNr4.png"" alt=""enter image description here"" /></a></p>
<br>
<br>
What is the purpose of having those two options available if they do not work in Azure Data Factory? Or is there somewhere else we need to update additional settings to make this work?
<br>
<br>
Thank you!!
<br>
<br>
Edit: I forgot to mention that we also attempted to see if we could replace the backslashes in the data flow column mapping section. We tried using the replace() function, but could not get it to work (kept giving syntax errors).
<pre><code>ORIGINAL working code for column: trim(toString($$))
ATTEMPTED WORKAROUNDS:
replace(trim(toString($$)),'\','-')
trim(replace(toString($$),'\','-')
trim(toString(replace($$),'\','-'))
</code></pre>
","<azure><azure-data-factory>","2020-07-16 16:55:21","907","1","2","62948960","<p>For the 'Delimited Text' source, it does not allow you to select Quote character with 'No escape character'. You can try the workaround options</p>
<ol>
<li>If your destination is Azure Synapse, try with Polybase load. (It will load the data with escape sequence and Quote character. After data load you can do a cleansing.)</li>
<li>If possible convert the source data format from 'Delimited Text' to 'parquet' or 'Json'</li>
</ol>
"
"62939724","Error with No Escape Character in Mapping Data Flow in Data Factory","<p>TLDR: Why does Azure Data Factory Data Flow not allow you to have no escape character?
<br>
<br></p>
<p>We have bad source data from a source that is unlikely to update it on their end (this is the nicest way I could phrase this). They have multiple columns where the value in the column is 01F\ or 8239\ and the backslash is written in their spec to be part of the value, not to be considered an escape character like it is standardized to be in the entire world.</p>
<p>The overall set up of the files are that they are comma delimited, each column's contents is in &quot; &quot;, and we have all of the normal new line characters. It's just the backslash that is not complying with standards. E.g.</p>
<pre><code>&quot;Column 1&quot;,&quot;Column 2&quot;,&quot;Column 3&quot;,&quot;Column 4&quot;
&quot;John&quot;,&quot;01F\&quot;,&quot;34&quot;,&quot;NY&quot;
&quot;Jane&quot;,&quot;3K&quot;,&quot;8239\&quot;,&quot;CA&quot;

|---------------------|------------------|------------------|------------------|
|      Column 1       |     Column 2     |     Column 3     |     Column 4     |
|---------------------|------------------|------------------|------------------|
|        &quot;John&quot;       |       &quot;01F\&quot;     |       &quot;34&quot;       |         &quot;NY&quot;     |
|---------------------|------------------|------------------|------------------|
|        &quot;Jane&quot;       |        &quot;3K&quot;      |      &quot;8239\&quot;     |         &quot;CA&quot;     |
|---------------------|------------------|------------------|------------------|
</code></pre>
<br>
<br>
In Azure Data Factory we are trying to see if we can make it ignore the \ as being an escape character. (FYI when we leave it as being treated as an escape character, it pulls the column right after the column with the backslash into one column). We can see in the data set where to set it so that there is no escape character.
<p><a href=""https://i.stack.imgur.com/Wa14K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wa14K.png"" alt=""enter image description here"" /></a></p>
<p>However when we then add that data set to our data flow and attempt to preview the data there, we get an error that we can't have no escape character in the data flow, and that quote character should be no quote character when we have no escape character.</p>
<p><a href=""https://i.stack.imgur.com/lSF1V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lSF1V.png"" alt=""enter image description here"" /></a></p>
<p>If we try to go back and also set no quote characters (which we don't actually want to do, just to test if that'll let it work), we get an error that the data flow can't have no escape or quote characters.</p>
<p><a href=""https://i.stack.imgur.com/UpNr4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UpNr4.png"" alt=""enter image description here"" /></a></p>
<br>
<br>
What is the purpose of having those two options available if they do not work in Azure Data Factory? Or is there somewhere else we need to update additional settings to make this work?
<br>
<br>
Thank you!!
<br>
<br>
Edit: I forgot to mention that we also attempted to see if we could replace the backslashes in the data flow column mapping section. We tried using the replace() function, but could not get it to work (kept giving syntax errors).
<pre><code>ORIGINAL working code for column: trim(toString($$))
ATTEMPTED WORKAROUNDS:
replace(trim(toString($$)),'\','-')
trim(replace(toString($$),'\','-')
trim(toString(replace($$),'\','-'))
</code></pre>
","<azure><azure-data-factory>","2020-07-16 16:55:21","907","1","2","63178092","<p>I just wanted to share that a user on the Microsoft forums provided an answer that ended up working.</p>
<p>We changed the escape character from \ to ^ in the data set settings (only did this after confirming that the ^ character is not used in any fashion anywhere). We did not apply the replace function in the mapping of the column because unfortunately we needed to keep the \ character in those columns. But it worked and our data is now flowing through the way we need it to (even though these are not best practices for data management).</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/48595/error-with-no-escape-character-in-mapping-data-flo.html"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/48595/error-with-no-escape-character-in-mapping-data-flo.html</a></p>
"
"62936945","SSIS: convert generated string into datetime: DT_WSTR to DT_DBDATE","<p>I have this expression <code>(DT_WSTR, 10)(year(getutcdate())-2)+&quot;-01-01&quot;</code> that results in the string <code>2018-01-01</code>.
NOw I want that string converted to a datetype but it complains that I cannot convert from DT_WSTR to DT_DBDATE (or any other datetype)</p>
<p>Curiously, it keeps complaining about WSTR if I try to use (DT_STR, 10,1252)(year(getutcdate())-2)+&quot;-01-01&quot;</p>
<p>How do I convert that generated string into a datetype DATE?</p>
<p>The ultimate goals is to get the 1st of january 2 years back at 12:00:00 AM
Since I am working on systems with different datesettings I prefer to avoid strings</p>
","<date><ssis><azure-data-factory>","2020-07-16 14:26:29","366","1","1","62937081","<p>Got it!</p>
<pre><code>(DT_DATE)(DT_DBDATE)DATEADD(&quot;M&quot;,-MONTH(GETDATE())+1,DATEADD(&quot;D&quot;,-DAY(GETDATE())+1,DATEADD(&quot;YEAR&quot;,-2,GETUTCDATE())))
</code></pre>
"
"62935804","Azure Data Factory, Passing REST GET response to stored procedure in Azure SQL Database","<p>I'm trying to build a (I think) very simple pipeline:</p>
<ol>
<li>Get the textual body of a GET operation.</li>
<li>Pass the (json) output as-is (= no transformations needed in ADF) to a &quot;Json&quot; parameter of a stored procedure in an Azure SQL Server database. The stored procedure handles the (complex) parsing/mapping.</li>
</ol>
<p>I thought that this can be done with just 1 Copy activity, but now I think I'm wrong.</p>
<p>In de Copy activity the Sink configuration looks like this:</p>
<pre><code>&quot;sink&quot;: {
  &quot;type&quot;: &quot;AzureSqlSink&quot;,
  &quot;sqlWriterStoredProcedureName&quot;: &quot;[dbo].[spParseJson]&quot;,
  &quot;sqlWriterTableType&quot;: &quot;What to enter here?&quot;,
  &quot;storedProcedureTableTypeParameterName&quot;: &quot;What to enter here?&quot;,
  &quot;storedProcedureParameters&quot;: {
  &quot;Json&quot;: {
     &quot;type&quot;: &quot;String&quot;,
     &quot;value&quot;: &quot;&lt;output of Source&gt;&quot;
     }
   }
 }
</code></pre>
<p>I really tried to read and understand the documentation, but imho the documentation doesn't explain much or in a bad vague way.</p>
<p>The &quot;output of Source&quot; should be the output from Source. But what function or variable to use for that?</p>
<p>What should I enter for &quot;sqlWriterTableType&quot; / &quot;storedProcedureTableTypeParameterName&quot;? After some digging I understand that ADF will create temp tables and such, but that isn't what I want.</p>
<p>I've also tried an other approach:</p>
<ol>
<li>Use the Web activity to just download the Json.</li>
<li>Execute SP with the input: @Activity(&quot;WebactivityName&quot;).output.</li>
</ol>
<p>But then I found out that the Web activity is limited to 1MB. The Json is about 1,5 MB. If the limit wouldn't be there, then I would have a solution. Argh.</p>
<p>FYI:
The content of the Json has a dynamically changing schema and is not well structured, so there's really no way that I can use the standard mapping capabilities in ADF.</p>
<p>Any help or guidance is appreciated. If you know of some documentation that is informative then that would also help.</p>
","<azure><azure-sql-database><azure-data-factory>","2020-07-16 13:26:02","2235","3","1","63304437","<p>I have updated this answer to split it into 2 parts. The first part deals with a simple implementation, which limits the Json response size to ~1MB. The second part deals with more complex implementation that does not impose this limit on Json response size.</p>
<p><strong>Part 1</strong></p>
<p>What you want to do is chain Web activity with Stored procedure Activity.
<a href=""https://i.stack.imgur.com/fMRVt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fMRVt.png"" alt=""Chain output of Activity 1 to 2"" /></a></p>
<p>This will allow you to pass the output from GetJson Web Activity onto the Stored procedure Activity.</p>
<p>Next you will want to add a parameter to your Stored procedure Activity so it can dynamically receive the chained output from the first step.</p>
<p><a href=""https://i.stack.imgur.com/f5RE1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f5RE1.png"" alt=""Add parameter to stored procedure"" /></a></p>
<p>This should enable you to pass the information successfully.</p>
<p>Here is a Json representation of the Pipeline in question:</p>
<pre><code>{
&quot;name&quot;: &quot;get-request-output-to-mssql-stored-procedure&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;GetJson&quot;,
            &quot;type&quot;: &quot;WebActivity&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;url&quot;: &quot;https://jsonplaceholder.typicode.com/posts/1&quot;,
                &quot;method&quot;: &quot;GET&quot;
            }
        },
        {
            &quot;name&quot;: &quot;Exec stored proc&quot;,
            &quot;type&quot;: &quot;SqlServerStoredProcedure&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;GetJson&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;storedProcedureParameters&quot;: {
                    &quot;Json&quot;: {
                        &quot;value&quot;: {
                            &quot;value&quot;: &quot;@activity('GetJson').output&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        },
                        &quot;type&quot;: &quot;String&quot;
                    }
                }
            },
            &quot;linkedServiceName&quot;: {
                &quot;referenceName&quot;: &quot;your-server-def&quot;,
                &quot;type&quot;: &quot;LinkedServiceReference&quot;
            }
        }
    ],
    &quot;annotations&quot;: []
}
</code></pre>
<p>}</p>
<p><strong>Part 2</strong></p>
<p>Instead of creating a Web Activity, we will use Lookup activity in combination with a Linked Service Definition and a Json Dataset definition.</p>
<p>You will need to create a Linked Service of type HTTP.</p>
<p><a href=""https://i.stack.imgur.com/iDKJa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iDKJa.png"" alt=""Create Linked Service of Type HTTP"" /></a></p>
<p>And configure it to use the URL you would like to get the Json response from:</p>
<p><a href=""https://i.stack.imgur.com/DEA7L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DEA7L.png"" alt=""Configure Linked Service"" /></a></p>
<p>You can then create a new Dataset (of type HTTP) which will use this Linked Service to get data.</p>
<p><a href=""https://i.stack.imgur.com/P66DP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P66DP.png"" alt=""Create a Dataset of type HTTP"" /></a></p>
<p>And choose Json as its Format type:</p>
<p><a href=""https://i.stack.imgur.com/1DQU0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1DQU0.png"" alt=""Select Json"" /></a></p>
<p>You can then set the request URL, and set the schema (unless you need it) to None.</p>
<p>You can then create a Lookup activity which uses the Json dataset as the Source dataset, and set the Request method to GET.</p>
<p><a href=""https://i.stack.imgur.com/ciyEJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ciyEJ.png"" alt=""Use Lookup to get Json response"" /></a></p>
<p>Here is a Json representation of the Pipeline in question:</p>
<pre><code>{
&quot;name&quot;: &quot;get-request-output-to-mssql-stored-procedure-2&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Exec stored proc&quot;,
            &quot;type&quot;: &quot;SqlServerStoredProcedure&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;RetrieveJson&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;storedProcedureParameters&quot;: {
                    &quot;Json&quot;: {
                        &quot;value&quot;: {
                            &quot;value&quot;: &quot;@activity('RetrieveJson').output&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        },
                        &quot;type&quot;: &quot;String&quot;
                    }
                }
            },
            &quot;linkedServiceName&quot;: {
                &quot;referenceName&quot;: &quot;ASD_SINGLE&quot;,
                &quot;type&quot;: &quot;LinkedServiceReference&quot;
            }
        },
        {
            &quot;name&quot;: &quot;RetrieveJson&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;JsonSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;HttpReadSettings&quot;,
                        &quot;requestMethod&quot;: &quot;GET&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;JsonDataset1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        }
    ],
    &quot;annotations&quot;: []
}
</code></pre>
<p>}</p>
"
"62931931","How to connect to a network drive from Azure Data Factory Pipeline","<p>I was successfully able to connect to my local machine using File System Linked Service. But now, when I am trying to access a folder from a network drive using a similar approach, I am unable to proceed. Please can anyone tell me where am I going wrong ?</p>
<p>The below is the working version when connected from my local machine.</p>
<pre><code>{
    &quot;name&quot;: &quot;Test_FileSystem&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;FileServer&quot;,
        &quot;typeProperties&quot;: {
            &quot;host&quot;: &quot;D:\\AZURE_FS&quot;,
            &quot;userId&quot;: &quot;&lt;xxx@org.com&gt;&quot;,
            &quot;encryptedCredential&quot;: &quot;eyJDcmVkZW50aWFsSWQiOiI0ZDYwMWI1Yi02YmI3LTRlN2YtOTBmYi0xNmIzZjI1MzQ3ZjciLC==&quot;
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;IR-FS-DEV&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}
</code></pre>
<p>However, when I use a similar approach and try connecting to a network drive,my connection is failing.</p>
<pre><code>{
    &quot;name&quot;: &quot;NetworkDrive_LS&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;FileServer&quot;,
        &quot;typeProperties&quot;: {
            &quot;host&quot;: &quot;\\\\&lt;host&gt;:&lt;port&gt;\\SD\\AZURE_FS&quot;,
            &quot;userId&quot;: &quot;&lt;userid&gt;&quot;,
            &quot;encryptedCredential&quot;: &quot;eyJDcmVkZW50aWFsSW==&quot;
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;ServerIR-VM&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    } }
</code></pre>
<p>Also,I am using a Self Hosted IR. Is there something I am doing wrong? My error is as below :</p>
<p>File path <code>\\&lt;host&gt;:&lt;port&gt;\SD\AZURE_FS</code> is not supported. Check the configuration to make sure the path is valid. The given path's format is not supported.</p>
","<azure><azure-data-factory>","2020-07-16 09:48:16","2405","0","1","62932852","<p>Looks like I found my mistake -</p>
<p>As per the example from the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system"" rel=""nofollow noreferrer"">documentation</a></p>
<pre><code>{
    &quot;name&quot;: &quot;FileLinkedService&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;FileServer&quot;,
        &quot;typeProperties&quot;: {
            &quot;host&quot;: &quot;&lt;host&gt;&quot;,
            &quot;userid&quot;: &quot;&lt;domain&gt;\\&lt;user&gt;&quot;,
            &quot;password&quot;: {
                &quot;type&quot;: &quot;SecureString&quot;,
                &quot;value&quot;: &quot;&lt;password&gt;&quot;
            }
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;&lt;name of Integration Runtime&gt;&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}
</code></pre>
<p>I changed to add the domain for the userid and also removed the port number. That solved.</p>
<pre><code>{
    &quot;name&quot;: &quot;NetworkDrive_LS&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;FileServer&quot;,
        &quot;typeProperties&quot;: {
            &quot;host&quot;: &quot;\\\\&lt;host&gt;\\SD\\AZURE_FS&quot;,
            &quot;userId&quot;: &quot;&lt;domain&gt;\\&lt;user&gt;&quot;,
            &quot;encryptedCredential&quot;: &quot;eyJDcmVkZW50aWFsSWQiOiIwY==&quot;
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;ServerIR-VM&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}


 
</code></pre>
"
"62930097","Datafactory activity output field has hyphen","<p>I am trying to get ouput data from activity, but the field I am interesting in has an hyphen 'x-ms-approximate-messages-count':</p>
<pre><code>        &quot;Response&quot;: &quot;&quot;,
    &quot;ADFWebActivityResponseHeaders&quot;: {
        &quot;x-ms-request-id&quot;: &quot;2cae82dd-0003-005e-4344-5b880f000000&quot;,
        &quot;x-ms-version&quot;: &quot;2019-10-10&quot;,
        &quot;x-ms-approximate-messages-count&quot;: &quot;2&quot;,
        &quot;Cache-Control&quot;: &quot;no-cache&quot;,
        &quot;Date&quot;: &quot;Thu, 16 Jul 2020 07:39:31 GMT&quot;,
        &quot;Server&quot;: &quot;Windows-Azure-Queue/1.0;Microsoft-HTTPAPI/2.0&quot;
    },
   ...
}
</code></pre>
<p>So my &quot;If condition&quot; activity is not good:</p>
<pre><code>@equals( activity('size').output.x-ms-approximate-messages-count,0)
</code></pre>
<p>Error in Datafactory is:</p>
<pre><code>  &quot;code&quot;: &quot;BadRequest&quot;,
  &quot;message&quot;: &quot;ErrorCode=InvalidTemplate, ErrorMessage=The expression 'equals( activity('QueueSize').output.x-ms-approximate-messages-count,0)' is not valid: the string character 'm' at position '39' is not expected.\&quot;&quot;,
</code></pre>
<p>So I understand character '-' was a problem for accessing that field.</p>
<p>How can I access that field?</p>
","<azure><azure-data-factory>","2020-07-16 07:56:23","170","0","1","62930249","<p>Please try like this:</p>
<pre><code>@equals( activity('size').output['x-ms-approximate-messages-count'],0)
</code></pre>
<p>Hope this can help you:).</p>
"
"62929329","Add dynamic content to annotations in Azure Data Factory","<p>Is it possible to add dynamic content like pipeline parameters to Azure Data Factory annotations, so that i can filter on them and group by them in the monitor?</p>
","<azure><azure-data-factory>","2020-07-16 07:07:22","1673","4","1","62929330","<p>It is possible, but very limited.
You cannot add dynamic content to annotations in trigger or datasets. If you do, the elements will validate, but they will throw an error on execution.</p>
<p>But it is possible to add dynamic content in pipeline annotations. As annotations have been disappearing from the UI, you need to open the code of the pipeline (either by cloning the repo to your local machine and open the json file, or by clicking on the {} sign in the top right corner of your pipeline view). On the very bottom you will find</p>
<pre><code>&quot;annotations&quot;: []
</code></pre>
<p>Here you can add for example dynamic pipeline parameters like this:</p>
<pre><code>&quot;annotations&quot;: [
    &quot;@pipeline().parameters.&lt;parametername&gt;&quot;
]
</code></pre>
<p>For every run of the pipeline an annotation will be added with the parameter set for the current run.</p>
"
"62925566","Error when using Alternate Key in CDS connector","<p>I'm using the <em>Common Data Service for Apps</em> connector in Azure Data Factory to load data into Dynamics 365</p>
<p>I've done this successfully before using the entity key. See this question: <a href=""https://stackoverflow.com/questions/62054515/loading-records-into-dynamics-365-through-adf"">Loading records into Dynamics 365 through ADF</a></p>
<p>Now I'm trying to use an alternate key to Upsert records into the <code>account</code> entity. (In this case insert)</p>
<h3>In Dynamics</h3>
<p>I've created two custom attributes fields in <code>account</code>:</p>
<pre><code>Field name        Data Type    Field Type    Max Length
=======================================================
xyz_srcsystem     Single Line  Simple        50
xyz_srccode       Single Line  Simple        50
</code></pre>
<p>Then I created a Key on <code>account</code> which contains these fields:</p>
<p><code>xyz_alternatekeyaccount</code></p>
<h3>In ADF</h3>
<p>Then I used a <em>Copy Data</em> activity in ADF to copy data from a SQL view into the account entity, using the CDS connector as a target.</p>
<p>This my source SQL statement:</p>
<pre><code>SELECT 
CAST(NULL as uniqueidentifier) as accountid,
'ADFTest1' as accountnumber, 'ADF Test 1' as [description],  
'nmcdermaid@xyz.com.au' as emailaddress1,
CAST('TST' AS NVARCHAR(50)) as xyz_srcsystem,
CAST('1' AS NVARCHAR(50)) as xyz_srccode
</code></pre>
<p>In the target, in the <em>Alternate key name</em> field I entered the alternate key name: <code>xyz_alternatekeyaccount</code></p>
<p>The error I get when I run the pipeline is</p>
<blockquote>
<p>Invalid type for entity id value</p>
</blockquote>
<p>Some test to rule out edge cases:</p>
<ul>
<li>if I put a dummy alternate key in, I get <em>Cannot retrieve key information of alternate key 'xyz_alternatekeyaccountx' for entity 'account'</em>. This implies it is finding the alternate key correctly</li>
<li>If I remove the alternate key from the connector, it drops back to the other usual set of errors that I see</li>
<li>When I pull the entity into SQL using the CDM connector, the custom attributes arrive as <code>NVARCHAR(MAX)</code></li>
<li>I've tried casting to these data types: <code>NVARCHAR(MAX)</code> <code>NVARCHAR(50)</code> <code>VARCHAR(MAX)</code> <code>VARCHAR(50)</code></li>
<li>If I use the normal key (not an alternate key), and get the datatype wrong (anything other than GUID), I'll get the same error</li>
</ul>
<p>Also see this Doco GitHub I raised:</p>
<p><a href=""https://github.com/MicrosoftDocs/azure-docs/issues/59028"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/59028</a></p>
","<dynamics-crm><azure-data-factory><common-data-service><xrmtoolbox>","2020-07-16 00:14:01","526","1","1","62927344","<p>When I changed the source SQL to this, it worked:</p>
<pre><code>SELECT 
'ADFTest1' as accountnumber, 'ADF Test 1' as [description],  
'nmcdermaid@xyz.com.au' as emailaddress1,
CAST('TST' AS NVARCHAR(50)) as xyz_srcsystem,
CAST('1' AS NVARCHAR(50)) as xyz_srccode
</code></pre>
<p>Note: the difference is I did not include the true primary key in the source dataset.</p>
<p>Not that if you want to UPSERT a new record (INSERT) and this isn't based on an alternate key, you have to include a NULL primary key</p>
"
"62921774","how to convert currency to decimal in Azure Data Factory's copy data activity mapping","<p>In Azure Data Factory i have a pipe line and pipeline has one copy data activity that has a source a REST api and a destination a SQL DB Table.</p>
<p>In the mapping of this copy activity i am telling that which columns from REST dataset (on left) will be mapped to which columns on SQL dataset (onright)</p>
<p><a href=""https://i.stack.imgur.com/j8tfA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j8tfA.png"" alt=""enter image description here"" /></a></p>
<p>there is a json property in Rest &quot;totalBalance&quot; that is supposed to be mapped to &quot;Balance&quot; field in DB Tables.</p>
<p>Json has &quot;totalBalance&quot; as string for example &quot;$36,970,267.07&quot; so how to convert this into decimal so that i can map it to DataBase table?</p>
<p>do i need to some how use mapping activity instead of copy activity ? or just the copy activity can do that ?</p>
","<azure><azure-data-factory>","2020-07-15 18:44:40","1496","0","2","62928468","<p>The copy activity can not do that directly.</p>
<p>There are two way I think can do this:</p>
<p>First:change decimal to varchar in DB Tables.</p>
<p>Second:add a lookup activity before copy activity and remove the '$' in 'totalBalance' column,then add an additional column like this:</p>
<p><a href=""https://i.stack.imgur.com/DHohe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DHohe.png"" alt=""enter image description here"" /></a></p>
<p>Finally,use this additional column map to 'Balance' column.</p>
<p>Hope this can help you.</p>
"
"62921774","how to convert currency to decimal in Azure Data Factory's copy data activity mapping","<p>In Azure Data Factory i have a pipe line and pipeline has one copy data activity that has a source a REST api and a destination a SQL DB Table.</p>
<p>In the mapping of this copy activity i am telling that which columns from REST dataset (on left) will be mapped to which columns on SQL dataset (onright)</p>
<p><a href=""https://i.stack.imgur.com/j8tfA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j8tfA.png"" alt=""enter image description here"" /></a></p>
<p>there is a json property in Rest &quot;totalBalance&quot; that is supposed to be mapped to &quot;Balance&quot; field in DB Tables.</p>
<p>Json has &quot;totalBalance&quot; as string for example &quot;$36,970,267.07&quot; so how to convert this into decimal so that i can map it to DataBase table?</p>
<p>do i need to some how use mapping activity instead of copy activity ? or just the copy activity can do that ?</p>
","<azure><azure-data-factory>","2020-07-15 18:44:40","1496","0","2","62946822","<p>finally what worked for me was having a copy activity and a mapping activity.</p>
<p>Copy activity copies data from REST to SQLtable where all the columns are VARCHAR type and from that table a mapping activity sinks data from SQL(allString) tables to actual destination SQLTable.</p>
<p>But between mapping and sink i added &quot;Derived Column&quot; for each source property i want to convert and in expression of that derived column i am using expression like this</p>
<p><strong>toDecimal(replace(replace(totalAccountReceivable, '$', ''),',',''))</strong></p>
<p><a href=""https://i.stack.imgur.com/wKybm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wKybm.png"" alt=""enter image description here"" /></a></p>
"
"62915615","Index Out of Range Error When Creating SnowFlake Linked Service in Azure Data Factory","<p>I am passing the credentials and parameters required but I get the error</p>
<blockquote>
<p>The value of the property 'index' is invalid: 'Index was out of range.
Must be non-negative and less than the size of the collection.
Parameter name: index'. Index was out of range. Must be non-negative
and less than the size of the collection. Parameter name: index
Activity ID: 36a4265d-3607-4472-8641-332f5656661d.</p>
</blockquote>
","<azure><snowflake-cloud-data-platform><azure-data-factory>","2020-07-15 13:07:38","1174","1","3","63286826","<p>Seems the UI doesn't generate the linked service correctly. Using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-snowflake"" rel=""nofollow noreferrer"">Microsoft Docs Example JSON</a> I received the same index error when attempting to create the linked service. If I remove the password from the connection string and add it as a separate property I am able to successfully generate the linked service.</p>
<p><strong>Microsoft Docs Example (Doesn't Work)</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;SnowflakeLinkedService&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;Snowflake&quot;,
        &quot;typeProperties&quot;: {
            &quot;connectionString&quot;: &quot;jdbc:snowflake://&lt;accountname&gt;.snowflakecomputing.com/?user=&lt;username&gt;&amp;password=&lt;password&gt;&amp;db=&lt;database&gt;&amp;warehouse=&lt;warehouse&gt;&amp;role=&lt;myRole&gt;&quot;
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;&lt;name of Integration Runtime&gt;&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}
</code></pre>
<p><strong>Working Example</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;SnowflakeLinkedService&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;Snowflake&quot;,
        &quot;typeProperties&quot;: {
            &quot;connectionString&quot;: &quot;jdbc:snowflake://&lt;accountname&gt;.snowflakecomputing.com/?user=&lt;username&gt;&amp;db=&lt;database&gt;&amp;warehouse=&lt;warehouse&gt;&quot;,
            &quot;password&quot;: {
                &quot;type&quot;: &quot;SecureString&quot;,
                &quot;value&quot;: &quot;&lt;password&gt;&quot;
            }
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;&lt;name of Integration Runtime&gt;&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}
</code></pre>
"
"62915615","Index Out of Range Error When Creating SnowFlake Linked Service in Azure Data Factory","<p>I am passing the credentials and parameters required but I get the error</p>
<blockquote>
<p>The value of the property 'index' is invalid: 'Index was out of range.
Must be non-negative and less than the size of the collection.
Parameter name: index'. Index was out of range. Must be non-negative
and less than the size of the collection. Parameter name: index
Activity ID: 36a4265d-3607-4472-8641-332f5656661d.</p>
</blockquote>
","<azure><snowflake-cloud-data-platform><azure-data-factory>","2020-07-15 13:07:38","1174","1","3","66800479","<p>I had the same issue, the password contained a ' and that's causing the trouble. Changed the password with no symbols and it works like a charm</p>
"
"62915615","Index Out of Range Error When Creating SnowFlake Linked Service in Azure Data Factory","<p>I am passing the credentials and parameters required but I get the error</p>
<blockquote>
<p>The value of the property 'index' is invalid: 'Index was out of range.
Must be non-negative and less than the size of the collection.
Parameter name: index'. Index was out of range. Must be non-negative
and less than the size of the collection. Parameter name: index
Activity ID: 36a4265d-3607-4472-8641-332f5656661d.</p>
</blockquote>
","<azure><snowflake-cloud-data-platform><azure-data-factory>","2020-07-15 13:07:38","1174","1","3","75523858","<p>We hit this same issue today, it was because our password had an ampersand (&amp;) at the end. This seemed to mess up the connection string as it contained this:</p>
<pre><code>&amp;password=abc123&amp;&amp;role=MyRole
</code></pre>
<p>Changing the password to not include an ampersand fixed it</p>
"
"62913652","Azure Data Factory V2: workaround to not being able to have a forEach activity inside a If Condition activity","<p>I want to have a forEach activity to be run if it meets some condition (inside a If Condition activity). But I get the following error:</p>
<p><em>ForEach activity ('') is not allowed under a Switch Activity.</em></p>
<p><a href=""https://i.stack.imgur.com/dRR5O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dRR5O.png"" alt=""enter image description here"" /></a></p>
<p>Is there any way of looping through items only if a condition is met in ADF?</p>
","<azure><azure-data-factory>","2020-07-15 11:16:52","5210","0","1","62916908","<p>You will need to architect your solution around the nesting restrictions like this. This is typically solved by placing the conditional workloads in other pipelines and using the Execute Pipeline activity inside the parent pipeline. You may need several child pipelines based on the complexity of the workloads. Use parameters to pass dependent values and the &quot;Wait on completion&quot; action to control concurrency.</p>
"
"62900297","how to pass variables to Azure Data Factory REST url's query stirng","<p>i am new to Azure data factory. I have a source dataset that is linked to a REST api. The url of this API has a query string. I have an activity that copy data from REST to Database. But i have to pass different values in query string and run the same activity against different values. How can this be done in Azure Data Factory?</p>
<p><a href=""https://i.stack.imgur.com/CO949.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CO949.png"" alt=""enter image description here"" /></a></p>
<p>i kind of reached here. but how to pass the value of this &quot;HospitalCode&quot; ?</p>
","<azure><azure-data-factory>","2020-07-14 17:02:03","4875","4","1","62908223","<p>Please try something like this:</p>
<p>1.create a pipeline and set a variable(your HospitalCode):</p>
<pre><code>Name:CodeArray Type:Array DEFAULT VALUE:[&quot;01&quot;,&quot;02&quot;]
</code></pre>
<p>2.create a ForEach Activity:</p>
<pre><code>Items:@variables('CodeArray')
</code></pre>
<p>3.create a Parameter name is <code>code</code>,type is <code>String</code>.</p>
<p>Setting of Linked service like this:
<a href=""https://i.stack.imgur.com/4mFtL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4mFtL.png"" alt=""enter image description here"" /></a>
dynamic content:<code>@concat('pjjs/pls?HospitalCode=',dataset().code)</code></p>
<p>4.Setting of Copy Activity's Source
<a href=""https://i.stack.imgur.com/TQKwM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TQKwM.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you.</p>
"
"62899097","Adding and elements to Dataflow in ADF .NET SDK","<p>I am wondering if it is possible to create Data Flows using Azure Data Factory .NET SDK? I am able to create pipelines, and add a dataflow as ExecuteDataFlowActivity, however I am not able to add parameters, sources or transformations. Any documentation or samples are much appreciated:</p>
<pre><code>        DataFactoryManagementClient client = ...
        DataFlowResource dataflow = new DataFlowResource { 
            Properties = new DataFlow
            {
                Description = dataFactoryName                    
            }                
        };
        
        client.DataFlows.CreateOrUpdate(resourceGroup, dataFactoryName, dataFactoryName, dataflow);
</code></pre>
","<c#><azure><azure-data-factory>","2020-07-14 15:54:24","230","0","1","62899847","<p>It is possible, but sometimes just go with the UI if you can. I would recommend to take look at on how ADF generates ARM templates for Data Flows and maybe you will change your mind=)</p>
<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.mappingdataflow?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.mappingdataflow?view=azure-dotnet</a></p>
<p>Review docs, this object inherits DataFlow, so it should work in similar way.</p>
"
"62888607","Is it possible to iterate over 2 different inputs inside a foreach in azure data factory?","<p>So what I am trying to do is identify if a file is present in a folder, if not copy it. The current structure uses a GetMetadata activity from the source, passes that list to a foreach activity, I am trying to add another GetMetadata activity inside the foreach for the destination then do a NOT filter based on the 'contains' expression to check if the item from the metadata activity for source is present in the childitems from the metadata activity in the destination, question is how do I distinguish these to item() when running in the filter activity, is it possible to access these as different objects?</p>
","<azure><azure-data-factory>","2020-07-14 05:29:28","383","0","1","62926866","<p>You could follow my steps:</p>
<p><strong>1. Create Source/Sink dataset with parameter 'SourceFile'/SinkFile:</strong></p>
<p>Source dataset:</p>
<p><a href=""https://i.stack.imgur.com/F2BH6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F2BH6.png"" alt=""enter image description here"" /></a></p>
<p>Sink dataset:
<a href=""https://i.stack.imgur.com/gRhFA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gRhFA.png"" alt=""enter image description here"" /></a></p>
<p><strong>2. Set pipeline parameter 'filename':</strong>
<a href=""https://i.stack.imgur.com/QKo3A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QKo3A.png"" alt=""enter image description here"" /></a></p>
<p><strong>3. Get metadata settings:</strong></p>
<p>Get metadata 1:</p>
<p><a href=""https://i.stack.imgur.com/BePJF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BePJF.png"" alt=""enter image description here"" /></a></p>
<p>Get metadata 1:</p>
<p><a href=""https://i.stack.imgur.com/rVYSx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rVYSx.png"" alt=""enter image description here"" /></a></p>
<p><strong>4. If condition settings:</strong></p>
<p>Using bellow expression to filter: if the file is exist in Source and not in Sink:</p>
<pre><code>@and(equals(activity('Get Metadata1').output.exists,false),equals(activity('Get Metadata2').output.exists,true))
</code></pre>
<p><a href=""https://i.stack.imgur.com/i6jMX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i6jMX.png"" alt=""enter image description here"" /></a></p>
<p><strong>5. Set the true active:</strong></p>
<p>If the file is exist in Source and not in Sink, copy the file to Sink:</p>
<p><a href=""https://i.stack.imgur.com/A6lPf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A6lPf.png"" alt=""enter image description here"" /></a></p>
"
"62881034","Getting HTTP Status Code: BadRequest when creating Azure Data Factory Trigger using PowerShell with AzureRm module","<p>I am deploying Azure ADF trigger using PowerShell, all triggers deploying successfully except one trigger. I am getting the following error</p>
<pre><code>[ERROR] Set-AzureRmDataFactoryV2Trigger : HTTP Status Code: BadRequest
[ERROR] Error Code: BadRequest
[ERROR] Error Message: The document creation or update failed because of invalid 
[ERROR] reference 'pipeline1'
[ERROR] Request Id: 895u9iuy-4j34-227f-63d3-948jd8djw86e
[ERROR] Timestamp (Utc):07/13/2020 16:52:28
[ERROR] At C:\Users\user1\source\repos\MyProject\PowerShell\Deploy-AdFTr
[ERROR] iggersWithDLS.ps1:290 char:21
[ERROR] + ...             Set-AzureRmDataFactoryV2Trigger -ResourceGroupName $stora ...
[ERROR] +                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[ERROR]     + CategoryInfo          : CloseError: (:) [Set-AzureRmDataFactoryV2Trigger 
[ERROR]    ], CloudException
[ERROR]     + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.SetAzureD 
[ERROR]    ataFactoryTriggerCommand
[ERROR]  
</code></pre>
<p>My PowerShell code is below.</p>
<pre><code>$isTriggerExist = Get-AzureRmDataFactoryV2Trigger -ResourceGroupName $storageAccountRG -DataFactoryName $dataFactoryName -TriggerName $triggerObject.Name -ErrorAction:SilentlyContinue
if($isTriggerExist -eq $null)
{
    Set-AzureRmDataFactoryV2Trigger -ResourceGroupName $storageAccountRG -DataFactoryName $dataFactoryName -Name $triggerObject.Name -DefinitionFile $file -Force
    if($runTimeState[$g] -eq &quot;Started&quot;)
    {
        Start-AzureRmDataFactoryV2Trigger -ResourceGroupName $storageAccountRG -DataFactoryName $dataFactoryName -TriggerName $triggerObject.name -Force
    }
}
</code></pre>
<p>I am sharing the trigger's JSON code.</p>
<pre><code>{
&quot;name&quot;: &quot;trg_pipline1&quot;,
&quot;properties&quot;: {
    &quot;annotations&quot;: [],
    &quot;runtimeState&quot;: &quot;Started&quot;,
    &quot;pipelines&quot;: [
        {
            &quot;pipelineReference&quot;: {
                &quot;referenceName&quot;: &quot;pipeline1&quot;,
                &quot;type&quot;: &quot;PipelineReference&quot;
            }
        }
    ],
    &quot;type&quot;: &quot;ScheduleTrigger&quot;,
    &quot;typeProperties&quot;: {
        &quot;recurrence&quot;: {
            &quot;frequency&quot;: &quot;week&quot;,
            &quot;interval&quot;: 1,
            &quot;startTime&quot;: &quot;2019-10-10T00:00:00Z&quot;,
            &quot;endTime&quot;: &quot;2022-12-14T01:00:00Z&quot;,
            &quot;timeZone&quot;: &quot;UTC&quot;,
            &quot;schedule&quot;: {
                &quot;hours&quot;: [
                    11,
                    13,
                    15,
                    17,
                    19,
                    21,
                    23
                ],
                &quot;weekDays&quot;: [
                    &quot;Monday&quot;,
                    &quot;Tuesday&quot;,
                    &quot;Wednesday&quot;,
                    &quot;Thursday&quot;,
                    &quot;Friday&quot;
                ]
            }
        }
    }
}
</code></pre>
<p>}</p>
","<powershell><triggers><azure-data-factory><azure-powershell>","2020-07-13 17:23:38","1369","0","1","62909920","<p>This error looks similar to <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/3c841641-3548-4b8f-9080-6cd851105b88/setazurermdatafactoryv2dataset-error-message-the-document-creation-or-update-failed-because-of?forum=AzureDataFactory"" rel=""nofollow noreferrer"">this</a>.</p>
<ul>
<li>Could you please ensure that <code>pipeline1</code> is deployed completely before you deploy <code>trg_pipline1</code>?</li>
<li>Also, can you verify if the steps given <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger#azure-powershell"" rel=""nofollow noreferrer"">here</a> are followed to create the Trigger? (Try removing <code>&quot;runtimeState&quot;: &quot;Started&quot;</code>)</li>
</ul>
"
"62880666","DFExecutorUserError java.lang.String cannot be cast to scala.collection.immutable.Map","<p>I'm collecting data from an SQL database then I fill an archive in the blob with extension &quot;Avro&quot;, then I get this Avro and insert into my stage table in DW. It was working perfectly a couple days ago, but now I'm receiving this error when Datafactory reads the blob archive to sink at the DW stage.</p>
<p>Can somebody help?</p>
<pre><code>{&quot;message&quot;:&quot;java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Map. Details:java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Map&quot;,&quot;failureType&quot;:&quot;SystemError&quot;,&quot;target&quot;:&quot;stage table name&quot;,&quot;errorCode&quot;:&quot;DFExecutorUserError&quot;}
</code></pre>
","<azure><azure-data-factory>","2020-07-13 16:58:19","230","-2","1","62884079","<p>I just call to Microsoft and figureout thats a bug with azure dataflow, basicaly the data flow is broke when you call a dataflow using Azure KeyVault to autenticate in repos,
i change my authentication to manually and worked...
the Microsoft is working to fix this issue.</p>
"
"62880542","AzureDataFactory: Downloading a Excel file from URL with HTTP Connector gives The excel format '' is not supported issue","<p>I'm trying to copy an Excel file from an URL to my Azure Blob Storage. For the same purpose, I have a HTTP Linking Service that has the base url. I'm using this Linking Service in the DataSet (Excel file Format) for the Copy Activity. The URL is publicly accessible and I'm not using any authentication.</p>
<p><em>URL</em>: <a href=""https://go.microsoft.com/fwlink/?LinkID=521962"" rel=""nofollow noreferrer"">https://go.microsoft.com/fwlink/?LinkID=521962</a></p>
<p><em>Sheet name</em>: <strong>Sheet1</strong></p>
<p>I always get</p>
<blockquote>
<p>The excel format '' is not supported while only supporting '.xls' and
'.xlsx'</p>
</blockquote>
<p>But the file is in xlsx format. Any help will be much appreciated . Thank you.</p>
","<azure><azure-data-factory><httpconnection>","2020-07-13 16:51:03","892","0","1","62887304","<p>Please follow this:</p>
<p>1.Source dataset format: select Binary format.</p>
<p>2.Setting of Http link service<br />
Base URL:https://go.microsoft.com/fwlink/?LinkID=521962,
Authentication type select Anonymous.</p>
<p>3.Sink dataset format:select Binary format.And file path like below image:
<a href=""https://i.stack.imgur.com/0H4AO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0H4AO.png"" alt=""enter image description here"" /></a></p>
<p>4.download the <code>Financial Sample.xlsx</code> and you can see the data.</p>
<p>Hope this can help you.</p>
"
"62879815","""The length of execution output is over limit"" Error in ADF Web Activity","<p>I am getting the following error in my ADF Web Activity. Anyone know how to fix this?
Thanks!</p>
<pre><code> activity failures
{
    &quot;errorCode&quot;: &quot;2001&quot;,
    &quot;message&quot;: &quot;The length of execution output is over limit (around 1M currently). &quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;ADFGetEmployees&quot;,
    &quot;details&quot;: []
}
</code></pre>
","<azure><rest><azure-data-factory>","2020-07-13 16:07:03","520","-1","1","62886667","<p>Web activity has times out limitation for 1 minute. Also, based on the above error <code>The length of execution ouput is over limit (around 1M currently)</code>., web activity also has output size limitation for 1 MB.</p>
<p>You could find the limitation rules from <a href=""https://learn.microsoft.com/en-us/azure/azure-subscription-service-limits#data-factory-limits"" rel=""nofollow noreferrer"">Data Factory limits</a> and some of the them could be adjusted if you ask for <a href=""https://azure.microsoft.com/en-us/blog/azure-limits-quotas-increase-requests/"" rel=""nofollow noreferrer"">Contact Support</a>.</p>
<p>According the Data Factory limits and your data size, some workaround is that choose the right Azure Data Factory component. Such as: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">HTTP connector</a>, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">REST connector</a> and the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-web-table"" rel=""nofollow noreferrer"">Web table connector</a>:
<a href=""https://i.stack.imgur.com/5sGEZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5sGEZ.png"" alt=""enter image description here"" /></a></p>
<p>Ref: <a href=""https://stackoverflow.com/questions/52843154/web-activity-throws-overlimit-error-when-calling-rest-api"">Web activity throws overlimit error when calling rest api</a></p>
"
"62877410","Azure Data Factory CI/CD for Schedule Triggers Not Working","<p>For triggers it looks like only <code>pipelines</code> <code>pipeline</code> and <code>typeProperties</code> blocks can be overriden based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#default-parameterization-template"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>What I want to achieve is with my CI/CD process and overriding parameters <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">functionality</a>, to have a schedule trigger disabled in the target ADF, unlike my source ADF.</p>
<p>If I inspect the JSON of a trigger that looks like the following field could do the trick <code>&quot;runtimeState&quot;: &quot;Started&quot;</code>.</p>
<pre><code>{
    &quot;name&quot;: &quot;name_daily&quot;,
    &quot;properties&quot;: {
        &quot;description&quot;: &quot; &quot;,
        &quot;annotations&quot;: [],
        &quot;runtimeState&quot;: &quot;Started&quot;,
        &quot;pipelines&quot;: [
            {
                &quot;pipelineReference&quot;: {
                    &quot;referenceName&quot;: &quot;name&quot;,
                    &quot;type&quot;: &quot;PipelineReference&quot;
                }
            }
        ],
        &quot;type&quot;: &quot;ScheduleTrigger&quot;,
        &quot;typeProperties&quot;: {
            &quot;recurrence&quot;: {
                &quot;frequency&quot;: &quot;Day&quot;,
                &quot;interval&quot;: 1,
                &quot;startTime&quot;: &quot;2020-05-05T13:01:00.000Z&quot;,
                &quot;timeZone&quot;: &quot;UTC&quot;,
                &quot;schedule&quot;: {
                    &quot;minutes&quot;: [
                        1
                    ],
                    &quot;hours&quot;: [
                        13
                    ]
                }
            }
        }
    }
}
</code></pre>
<p>But if I attempt to add it in the JSON file like this:</p>
<pre><code>&quot;Microsoft.DataFactory/factories/triggers&quot;: {
        &quot;properties&quot;: {
            &quot;runtimeState&quot;: &quot;-&quot;,
            &quot;typeProperties&quot;: {
                &quot;recurrence&quot;: {
                    &quot;interval&quot;: &quot;=&quot;,
                    &quot;frequency&quot;: &quot;=&quot;
                }
            }
        }
    }
</code></pre>
<p>it never shows up in the Override section in Azure Pipeline Releases.</p>
<p>Does this ADF CI/CD functionality exist for triggers? How can I achieve my target here?</p>
","<azure><azure-pipelines><azure-data-factory><azure-pipelines-release-pipeline>","2020-07-13 13:55:49","1090","0","1","62894057","<p>Turns out <code>runtimeState</code> for triggers is not obeyed in the <code>arm-template-parameters-definition.json</code>.</p>
<p>The path is clearer after some more research - I can achieve what I want with either editing the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#script"" rel=""nofollow noreferrer"">Powershell script Microsoft has provided</a> or use an <a href=""https://social.technet.microsoft.com/wiki/contents/articles/53153.azuredevops-cicd-for-azure-data-factory.aspx#Starting_all_the_triggers"" rel=""nofollow noreferrer"">ADF custom task</a> from the Azure Devops marketplace.</p>
"
"62873941","Using parameters to locate file during trigger creation in Azure Data Factory","<p>I am trying to create a trigger that I will use for starting a pipeline in ADF:</p>
<p><a href=""https://i.stack.imgur.com/wTuvA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wTuvA.png"" alt=""enter image description here"" /></a></p>
<p>The folder I want to set my trigger on can have different paths:</p>
<ul>
<li>202001/Test/TriggerFolder</li>
<li>202002/Test/TriggerFolder</li>
<li>202003/Test/TriggerFolder</li>
</ul>
<p>etc..</p>
<p>Therefore in my <strong>Blob path begins with</strong> I would like to use a parameter (that I will set somewhere else through another pipeline) that tells the trigger where to look for instead of having a static name file.</p>
<p>Unfortunately it doesn't seem to give me the chance to add dynamic content as (for example) in a DataSet.
If there is really no chance, because maybe I may think the trigger is something instantiated once, is there a way to create a trigger as a step during a pipeline?</p>
<p>Thank you!</p>
","<azure><azure-data-factory><azure-data-lake><azure-triggers>","2020-07-13 10:29:39","377","1","1","62896372","<p>There is possibility to pass parameter from &quot;ARM Template&quot; of the Azure Data Factory. During the deployment of pipelines, this parameter can be passed with necessary value. Below is example code for it.</p>
<p>Sample Code:</p>
<pre><code>   {
        &quot;name&quot;: &quot;[concat(parameters('factoryName'), '/trigger1')]&quot;,
        &quot;type&quot;: &quot;Microsoft.DataFactory/factories/triggers&quot;,
        &quot;apiVersion&quot;: &quot;2018-06-01&quot;,
        &quot;properties&quot;: {
            &quot;annotations&quot;: [],
            &quot;runtimeState&quot;: &quot;Stopped&quot;,
            &quot;pipelines&quot;: [],
            &quot;type&quot;: &quot;BlobEventsTrigger&quot;,
          &quot;typeProperties&quot;: {
            &quot;blobPathBeginsWith&quot;: &quot;[parameters('trigger1_properties_typeProperties_blobPathBeginsWith')]&quot;,
            &quot;ignoreEmptyBlobs&quot;: true,
            &quot;scope&quot;: &quot;[parameters('trigger1_properties_typeProperties_scope')]&quot;,
            &quot;events&quot;: [
              &quot;Microsoft.Storage.BlobCreated&quot;
            ]
          }
        },
</code></pre>
"
"62854623","Copying CSV data to a JSON array object in Azure Data Factory","<p>I've been going round in circles trying to get what I thought would be a relatively trivial pipeline working in Azure Data Factory. I have a CSV file with a schema like this:</p>
<pre><code>Id, Name, Color
1, Apple, Green
2, Lemon, Yellow
</code></pre>
<p>I need to transform the CSV into a JSON file that looks like this:</p>
<pre><code>{&quot;fruits&quot;:[{&quot;Id&quot;:&quot;1&quot;,&quot;Name&quot;:&quot;Apple&quot;,&quot;Color&quot;:&quot;Green&quot;},{&quot;Id&quot;:&quot;2&quot;,&quot;Name&quot;:&quot;Lemon&quot;,&quot;Color&quot;:&quot;Yellow&quot;}]
</code></pre>
<p>I can't find a simple example that helps me understand how to do this in ADF. I've tried a Copy activity,   and a data flow, but the furthest I've got is a json object like this:</p>
<pre><code>{&quot;fruits&quot;:{&quot;Id&quot;:&quot;1&quot;,&quot;Name&quot;:&quot;Apple&quot;,&quot;Color&quot;:&quot;Green&quot;}}
{&quot;fruits&quot;:{&quot;Id&quot;:&quot;2&quot;,&quot;Name&quot;:&quot;Lemon&quot;,&quot;Color&quot;:&quot;Yellow&quot;}}
</code></pre>
<p>Surely this is simple to achieve. I'd be very grateful if anyone has any suggestions. Thanks!</p>
","<azure><azure-data-factory>","2020-07-11 21:08:11","1279","0","1","62987851","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#tabularhierarchical-source-to-hierarchical-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#tabularhierarchical-source-to-hierarchical-sink</a></p>
<p>&quot;When copying data from tabular source to hierarchical sink, writing to array inside object is not supported&quot;</p>
<p>But, if we put file pattern under Sink properties as 'Array of Objects', you can achieve somewhere till here:</p>
<pre><code>    [{&quot;Id&quot;:&quot;1&quot;,&quot;Name&quot;:&quot; Apple&quot;,&quot;Color&quot;:&quot; Green&quot;}
     ,{&quot;Id&quot;:&quot;2&quot;,&quot;Name&quot;:&quot; Lemon&quot;,&quot;Color&quot;:&quot; Yellow&quot;}
    ]
</code></pre>
"
"62833304","Until Activity in Azure Data Factory executes even when the condition doesn't match","<p>I have used an until activity to copy files in a sequential order. For this I have created a variable and assigned the value 0 and compared it against the number of files available in the data lake. The expression used in the until activity checks if the variable values is greater than the number of files, till the variable value becomes greater than the count of files, the loop executes. If no files found in the data lake then the variable value would be set to 1 and the the count of files would be 0.
But what happens is , even if the value of variable is 1 and count of files is zero the until activity activity tries to execute the inner activities.  Any solution for this?
The condition activity used to set the variable value:
@equals(activity(lookup).output.firstrow.NumberOfFiles,0) then set variable var1 =1 else 0
the expression used in until activity : @greater(int(variables('var1')),activity(lookup).output.firstrow.NumberOfFiles)
Inside the until activity:
Set the variable to increment the var1 value by 1</p>
<p>If any suggestions, would be really helpful</p>
<p>Regards,
Sandeep</p>
","<azure><until-loop><azure-data-factory>","2020-07-10 11:26:30","3859","0","2","62868254","<p>According to Microsoft documentation,the Until activity provides the same functionality that a do-until looping structure provides in programming languages.</p>
<p>It will first execute inner activities one time ,and then according to the result of expression to decide to loop or break.</p>
<p>So in your situation,your inner activities will execute once and then get the <code>true</code> value and break the until activity.</p>
<p>If your don't want to execute once,you can use another <code>If Condition activity</code> and let <code>var1 = 1</code> don't execute the util activity.</p>
"
"62833304","Until Activity in Azure Data Factory executes even when the condition doesn't match","<p>I have used an until activity to copy files in a sequential order. For this I have created a variable and assigned the value 0 and compared it against the number of files available in the data lake. The expression used in the until activity checks if the variable values is greater than the number of files, till the variable value becomes greater than the count of files, the loop executes. If no files found in the data lake then the variable value would be set to 1 and the the count of files would be 0.
But what happens is , even if the value of variable is 1 and count of files is zero the until activity activity tries to execute the inner activities.  Any solution for this?
The condition activity used to set the variable value:
@equals(activity(lookup).output.firstrow.NumberOfFiles,0) then set variable var1 =1 else 0
the expression used in until activity : @greater(int(variables('var1')),activity(lookup).output.firstrow.NumberOfFiles)
Inside the until activity:
Set the variable to increment the var1 value by 1</p>
<p>If any suggestions, would be really helpful</p>
<p>Regards,
Sandeep</p>
","<azure><until-loop><azure-data-factory>","2020-07-10 11:26:30","3859","0","2","63509943","<blockquote>
<p>If no files found in the data lake then the variable value would be
set to 1 and the the count of files would be 0.</p>
</blockquote>
<blockquote>
<p>the expression used in until activity :
@greater(int(variables('var1')),activity(lookup).output.firstrow.NumberOfFiles)</p>
</blockquote>
<p>The Until activity is working as intended.</p>
<blockquote>
<p>The expression used in the until activity checks if the variable
values is greater than the number of files, till the variable value
becomes greater than the count of files, the loop executes.</p>
</blockquote>
<p>Unfortunately, your assumption about how it works is incorrect.  <strong>The Until loop will always run at least once.  At the end of each iteration it will then test its condition to see if it should run again.</strong>  It does not check the condition first, as you described.</p>
<p>The fix is to use a ForEach activity instead of an Until activity.  A ForEach will always run the number of times you need, and it saves you from having to track the loop condition with variable manipulation.</p>
"
"62826514","Move Dynamics 365 Sales data to on-premise application via Azure?","<p>I've been digging and watching videos for days and am no closer to a solution. I'm trying to put together an integration where upon sales order creation in Dynamics 365 the record is sent to an on-premise ERP via Azure.</p>
<p>Specifically, I'm using Logic Apps and the CDS connector as a trigger to initiate the transfer but am lost as to how to proceed.</p>
<p>Onward from Logic Apps, do I connect to the on-premise machine via Azure Data Factory/Data Management Gateway? Or are those two separate things?</p>
<p>To communicate with the ERP, I need to use WCF. I've been told I'll need to use Azure Relay to do that. However, is there then a need for the Data Factory/Data Management Gateway components?</p>
","<azure><azure-data-factory><azure-logic-apps>","2020-07-10 02:37:30","51","0","1","62834642","<p>You can use <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-connection"" rel=""nofollow noreferrer"">Azure Data Gateway</a> to connect to an on-premise system from Logic Apps.</p>
<p>This blog post walks through the process of connecting to an on-premise web service from a Logic App with a <a href=""https://learn.microsoft.com/en-us/connectors/custom-connectors/"" rel=""nofollow noreferrer"">custom connector</a>:</p>
<p><a href=""https://azureintegrations.com/2019/01/01/azure-connecting-to-an-onpremise-webservice-and-file-system-from-a-logic-app/"" rel=""nofollow noreferrer"">https://azureintegrations.com/2019/01/01/azure-connecting-to-an-onpremise-webservice-and-file-system-from-a-logic-app/</a></p>
<p>If instead of triggering an on-premise web service you could save data to the file system or to a DB then the process would be simpler as you could use standard connectors instead of creating custom ones.</p>
"
"62826251","Azure Data Factory: Copy activity - Writing to sink is showing '00:00:00'","<p>I build a pipeline in which the Copy activity is loading a flat file into a SQL table.</p>
<p>When I view the activity details, the <strong>Writing to sink</strong> is always showing '00:00:00'.</p>
<p><a href=""https://i.stack.imgur.com/U2CAE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U2CAE.png"" alt=""enter image description here"" /></a></p>
<p>Has anyone ever seen the same issue or it is just me?</p>
","<azure><azure-data-factory>","2020-07-10 01:58:07","234","0","1","63189068","<p><a href=""https://i.stack.imgur.com/kUBhq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kUBhq.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Ofi62.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ofi62.png"" alt=""enter image description here"" /></a></p>
<p>I think this is because your data volume is not big enough, and you just do a copy without data analytis. So that is why is takes less than one second.</p>
"
"62823486","Azure Data Factory - Grab timeout message","<p>After a copy activity fails, I'd like to have a specific set of activities to be run if it failed due to a timeout. I can see there is an error message but it's not included within the output json of the copy activity. Is there any way to retrieve this error message and grab the errorCode programatically?</p>
<p><a href=""https://i.stack.imgur.com/dpSjv.png"" rel=""nofollow noreferrer"">data factory timeout message</a></p>
<p>I've been trying to grab it via the output of the copy activity but the output below does not have any errors.</p>
<p><a href=""https://i.stack.imgur.com/ljQe8.png"" rel=""nofollow noreferrer"">output</a></p>
<p>Is there anyway to use the dynamic content to grab that first errorCode?</p>
","<azure><azure-data-factory>","2020-07-09 20:49:37","73","0","1","62826693","<p>Please try something like this:</p>
<p><a href=""https://i.stack.imgur.com/VYJcX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VYJcX.png"" alt=""enter image description here"" /></a></p>
<p><strong>Variables of <code>Set variable2</code></strong></p>
<p>dynamic content:<code>@activity('Copy data1').Error.errorCode</code></p>
<p>You can also use this dynamic content:<code>@activity('Copy data1').STATUS</code></p>
<p><a href=""https://i.stack.imgur.com/85SVU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/85SVU.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output of <code>Set variable2</code></strong>(<code>@activity('Copy data1').Error.errorCode</code>):</p>
<p><a href=""https://i.stack.imgur.com/9IYvm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9IYvm.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output of <code>Set variable2</code></strong>(<code>@activity('Copy data1').STATUS</code>):</p>
<p><a href=""https://i.stack.imgur.com/8pISA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8pISA.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you:).</p>
"
"62820642","How to pass a parameter from Azure Automation Runbook to Azure Datafactory pipeline","<p>I have written below code powershell script to pass the get the value from the logic apps and pass it to Azure data factory.</p>
<pre><code>#I am sharing a part of the code which is should pass the parameter to the data factory
# get the clientName value from the logic apps.
Param (
Param (
[Parameter (Mandatory = $true)]
[string]
$clientName
)

$parameters = @{
    &quot;clientName&quot; = $clientName
}

#
$Pipeline1 = Invoke-AzureRmDataFactoryV2Pipeline -ResourceGroupName $rg1 -DataFactoryName $adf1 -PipelineName $pn1 -Parameter $parameters

</code></pre>
<p>But do not know how does my datafactory would get this value.</p>
","<azure><powershell><azure-data-factory><azure-automation>","2020-07-09 17:41:10","486","1","1","62822284","<p>Created a pipeline parameter with the name as same you have used in the declared in automation runbook</p>
<pre><code>clientName
</code></pre>
<p>then use this variable in notebook activities at base parameter area</p>
<p><a href=""https://i.stack.imgur.com/K6uw0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K6uw0.png"" alt=""enter image description here"" /></a></p>
"
"62818257","Add a condition to skip a transformation in ADF Data Flow","<p>I have an ADF Data Flow that includes a filter as shown below:</p>
<p><a href=""https://i.stack.imgur.com/0lFRd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0lFRd.png"" alt=""enter image description here"" /></a></p>
<p>I need to set a parameter and set a condition before filter and based on that condition it should or should not perform filter step.</p>
<p>For example, parameter = environment</p>
<p>If environment = non prod, do filter transformation</p>
<p>Else If environment = prod, skip filter transformation.</p>
<p>Is there a way to add this parameter and condition via ADF Data Flow?</p>
<p><strong>UPDATE:</strong></p>
<p>I added a parameter as follows:</p>
<p><a href=""https://i.stack.imgur.com/RpiEF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RpiEF.png"" alt=""enter image description here"" /></a>
I added a conditional split as follows:</p>
<p><a href=""https://i.stack.imgur.com/2XOPf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2XOPf.png"" alt=""enter image description here"" /></a></p>
<p>This doesn't skip Filter transformation when environment = prod. Am I missing something?</p>
","<azure><azure-data-factory>","2020-07-09 15:21:18","541","0","1","62820060","<p>Use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split"" rel=""nofollow noreferrer"">Conditional Split</a> transformation for conditional logic</p>
"
"62815723","How to edit a *.csv file in the azure blob storage with an azure function?","<p>I am working with Azure at the moment and I am unhappy with the predefined functions in the DataFactory because they start a Cluster in the background and this is absolutely not necessary for my problem.</p>
<p>I receive a csv file in a predefined folder and want to pick a set of columns and store them in a certain order in a csv file.</p>
<p>At the moment my file looks as follows:</p>
<p>The JSON file:</p>
<pre><code>  &quot;bindings&quot;: [
    {
      &quot;name&quot;: &quot;myblob&quot;,
      &quot;type&quot;: &quot;blobTrigger&quot;,
      &quot;path&quot;: &quot;input-raw&quot;,
      &quot;connection&quot;: &quot;AzureWebJobsStorage&quot;,
      &quot;direction&quot;: &quot;in&quot;
    },
    {
      &quot;name&quot;: &quot;outputblob&quot;,
      &quot;type&quot;: &quot;blob&quot;,
      &quot;path&quot;: &quot;{blobTrigger}-copy&quot;,  
      &quot;connection&quot;: &quot;AzureWebJobsStorage&quot;,
      &quot;direction&quot;: &quot;out&quot;
    }
  ],
  &quot;disabled&quot;: false,
  &quot;scriptFile&quot;: &quot;__init__.py&quot;
}

</code></pre>
<p>The <strong>init</strong>.py:</p>
<pre><code>import logging
import azure.functions as func

def main(myblob: func.InputStream, outputblob: func.Out[func.InputStream]):
    logging.info(f&quot;Python blob trigger function processed blob \n&quot;
                 f&quot;Name: {myblob.name}\n&quot;
                 f&quot;Blob Size: {myblob.length} bytes&quot;)
    outputblob.set(myblob)
</code></pre>
<p>My function picks a file in the folder and copies it with a '-copy' in the end in the same folder.
Is there an easy way to access the data and to edit it with python?</p>
<p>Toll now I tried the packages 'csv', 'io' and 'fileinput' to read the information but I could not manage till now to edit or even see the data within my VisualStudioCode.</p>
<p>If you need more information please let me know.</p>
<p>Best
P</p>
","<python><azure><visual-studio-code><azure-functions><azure-data-factory>","2020-07-09 13:14:44","1232","2","1","62826735","<p>In fact there is no way to 'edit' the .csv file. But you can download the .csv file and change it then upload to override the .csv file on azure.</p>
<p>By the way, if i read right, your function have a big problem. When the azure function be triggered, there will be endless 'xx-Copy' file in your container. I mean the output file will be the trigger condition of your function and the function will be endless.</p>
<p>This is my function, It use InputStream in func to read the blob data:</p>
<pre><code>import logging

import azure.functions as func


def main(myblob: func.InputStream):
    
    logging.info(myblob.read().decode(&quot;utf-8&quot;) );
    logging.info(f&quot;Python blob trigger function processed blob \n&quot;
                 f&quot;Name: {myblob.name}\n&quot;
                 f&quot;Blob Size: {myblob.length} bytes&quot;)
</code></pre>
<hr />
<pre><code>{
  &quot;scriptFile&quot;: &quot;__init__.py&quot;,
  &quot;bindings&quot;: [
    {
      &quot;name&quot;: &quot;myblob&quot;,
      &quot;type&quot;: &quot;blobTrigger&quot;,
      &quot;direction&quot;: &quot;in&quot;,
      &quot;path&quot;: &quot;samples-workitems&quot;,
      &quot;connection&quot;: &quot;AzureWebJobsStorage&quot;
    }
  ]
}
</code></pre>
<p>In my situation, I first read the blob data to bytes, and then I convert it to string. Let me know whether this can solved your question.:)</p>
"
"62808877","Custom .NET Activity: The request was aborted: Could not create SSL/TLS secure channel","<p>Under the Batch Service option, we are running a custom dot net activity, and that .net tool downloads data from some APIs. It was working fine without any issues until yesterday. Now we are facing the below error while downloading data from the APIs. I don't know why we are facing such an issue suddenly.</p>
<blockquote>
<p>Error: The request was aborted: Could not create SSL/TLS secure
channel. StackTrace:    at System.Net.WebClient.OpenRead(Uri address)
at Import.DownloadSite(String url, String type)</p>
</blockquote>
<p>The same .Net tool works fine in our local desktop machines(Windows 10 OS). Getting the error only in the batch account machine. So I think the problem is in the batch account machine.</p>
<p>Using the below C# code for downloading data from the APIs:</p>
<pre><code>WebClient wc = new WebClient();
ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls12 | SecurityProtocolType.Tls11 | SecurityProtocolType.Tls | SecurityProtocolType.Ssl3;
return wc.DownloadString(string.Format(ApiUrl, type));
</code></pre>
<p>I tried most of the solutions suggested in the below link, and nothing works for me.
<a href=""https://stackoverflow.com/questions/2859790/the-request-was-aborted-could-not-create-ssl-tls-secure-channel"">The request was aborted: Could not create SSL/TLS secure channel</a></p>
<p>More information:</p>
<ul>
<li>.NET Framework: v4.5</li>
<li>Batch machine: Windows Server 2012 R2 (x64)</li>
</ul>
","<azure><azure-data-factory><azure-batch>","2020-07-09 06:37:25","185","0","1","63184426","<p>You need to Add HTTP Header &quot;Expect&quot; with value &quot;100-continue&quot;. In your case, please add these lines of code before creating the request :</p>
<pre><code> ServicePointManager.Expect100Continue = true;
 ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls
        | SecurityProtocolType.Tls11
        | SecurityProtocolType.Tls12
        | SecurityProtocolType.Ssl3;
</code></pre>
"
"62808248","How to reuse the Azure Data Factory pipeline for multiple users","<p>I have a Azure data factory pipeline that is calling a Databricks notebook.
I have parameterized the pipeline and via this pipeline I am passing the product name to the databricks notebook.</p>
<p>Based on the parameter the Databricks will push the processed data into the specific ADLS directory.
Now the problem is- How do I make my pipeline aware that which parameter need to pass to the Databricks.</p>
<p>Example: If I pass the <strong>Nike</strong> via the adf to the databricks then my data would get pushed into Nike directory or If I pass <strong>Adidas</strong> then data would get pushed into Adidas directory.</p>
<p>Please note that I am triggering the ADF from the automation account.</p>
","<azure><azure-pipelines><azure-data-factory><azure-databricks><azure-automation>","2020-07-09 05:49:30","525","0","1","62831498","<p>As I understood, you are using product_name = dbutils.widgets.get('product_name') statement in the databricks notebook to get the param and based on that param you process the data. The question is how to pass different params to the notebook? You create one adf pipeline and you can pass different params to the triggers that execute the adf pipeline.
Create ADF pipeline
<a href=""https://i.stack.imgur.com/VD2B2.png"" rel=""nofollow noreferrer"">Adf pipeline</a>
create trigger that will pass params to the ADF pipeline
<a href=""https://i.stack.imgur.com/qRYqM.png"" rel=""nofollow noreferrer"">triggers</a>
This way you will have 1 ADF pipeline with multiple instances of it with different params like Adidas, Nike etc.</p>
"
"62808223","structure in getMetadata activity for csv file dataset show string datatypes for integer column in azure data factory","<p>I want to do validation as first step before proceeding further in pipeline execute.
I am fetching metadata activity for my dataset and then checking it against a predefined schema in if condition.
Metadata for csv files show column type string even for integer which is breaking the validation.</p>
","<azure><azure-data-factory>","2020-07-09 05:46:23","639","0","1","62929778","<p>Get Metadata doesn't support it, all the data type is considered as string in csv files.</p>
<p>You have posted a question on Microsoft forums here: <a href=""https://learn.microsoft.com/en-us/answers/questions/44635/structure-in-getmetadata-activity-for-csv-file-dat.html"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/44635/structure-in-getmetadata-activity-for-csv-file-dat.html</a>, but Microsoft MSFT confirmed that: Using getMetadata on a csv file will give all strings.</p>
<p>The link he provided doesn't work for the column type.</p>
<p>I think that's a by-design problem and has no workaround now. And per my experience, the structure only works well for database.</p>
<p>The best way for you it that ask <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">Azure Support</a> for more details. Or post a new Data Factory feedback here: <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory</a>. Hope the Data Factory Product team will see it and give us some guides.</p>
"
"62805966","Extract the data from Azure Devops Services (WIT Analytics) to Azure SQL Database using Azure Data Factory","<p>We have been working on large migration project which contains almost 50K Work Items including requirements, documents, Use Cases, Test Cases, Bugs, Issues etc. in our Azure Devops Services. As a BI Developer, we wanted to create some generic dashboards for all the teams across the organisation for the visibility and insights to take decisions. Initially, we used Power BI Advanced Functions(M Query etc.) to extract the data from WIT Analytics model and created datasets on Power BI Premium Service. Now, we want to push all our data directly to Azure SQL Database for data retaining and snapshots. There are different ways we can do that.</p>
<ol>
<li>Power BI Advanced Functions, ODATA Feed(Not supported in ADF), REST API(Limitations on Number of Records)</li>
</ol>
<p>Unfortunately, every way has it's disadvantages. What could be the best possible way to extract the data on hourly/daily basis from Azure Devops Service to Azure SQL Databases?</p>
<p>Any help or pointers would be appreciated.</p>
","<azure-devops><powerbi><azure-sql-database><azure-data-factory>","2020-07-09 01:24:34","1269","1","1","62831059","<p>You can use ODATA source in Azure data factory. See the tutorial in below document:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-odata"" rel=""nofollow noreferrer"">Copy data from an OData source by using Azure Data Factory</a></p>
"
"62804374","Return count in ADF Data Flow","<p>I have an ADF Data Flow that outputs 2 sets of values (Name, Location) as shown below:</p>
<p><a href=""https://i.stack.imgur.com/ifYfP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ifYfP.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/TBBQl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TBBQl.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way to output the count of Names in each Location via ADF Data Flow?</p>
","<azure><azure-data-factory>","2020-07-08 22:08:20","1906","0","1","62805232","<p>You can do it with <code>Aggregate</code> action. I tested it with your data.</p>
<p><a href=""https://i.stack.imgur.com/pLGoT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pLGoT.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>Start with <code>Aggregate</code> action's <code>Group by</code> section, add <code>location</code> as group by columns.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/YwbnE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YwbnE.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Mention aggregated column name in the Columns and <code>count(name)</code> as aggregate expression.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/a9AMy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a9AMy.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>Verify the aggregate's result in Aggregate's <code>Data preview</code></li>
</ol>
<p><a href=""https://i.stack.imgur.com/qfSsN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfSsN.png"" alt=""enter image description here"" /></a></p>
"
"62802280","How can I add dynamic content to ""First Row As Header"" condition in Azure Data Factory Dataset?","<p>In a dataset, I can see that I can add dynamic content in the First Row as Header box:</p>
<p><a href=""https://i.stack.imgur.com/kP061.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kP061.png"" alt=""enter image description here"" /></a></p>
<p>My question is can I use dynamic content in a way that if a column header is empty in the csv then I can add a custom name. If all the column names are there, it would take the first row as is?</p>
<p>Asking because I have some files with 1/2 empty column names.</p>
<p>Thanks!</p>
","<azure><azure-data-factory>","2020-07-08 19:26:36","4907","1","1","62808925","<blockquote>
<p>My question is can I use dynamic content in a way that if a column header is empty in the csv then I can add a custom name. If all the column names are there, it would take the first row as is?</p>
</blockquote>
<p>No,because dynamic content must return boolean value,you can't replace empty column name with your custom name.</p>
<p>As a workaround,you can use data flow.</p>
<p>Below is my test sample:</p>
<p><em>My data in csv file:</em></p>
<pre><code>fieldA,,fieldB,,fieldC
1,2,3,4,5
</code></pre>
<p><strong>Setting of source of dataset:</strong></p>
<p><a href=""https://i.stack.imgur.com/u3B1d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u3B1d.png"" alt=""enter image description here"" /></a></p>
<p><strong>ADF will auto generate column name when your column name is empty,like _c1</strong></p>
<p><a href=""https://i.stack.imgur.com/EZlsz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EZlsz.png"" alt=""enter image description here"" /></a></p>
<p><strong>Then you can use <code>DerivedColumn</code>:</strong></p>
<p><a href=""https://i.stack.imgur.com/st11W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/st11W.png"" alt=""enter image description here"" /></a></p>
<p><strong>Finally:you can use <code>select</code> or <code>sink mapping</code>,delete columns which are generated by ADF.</strong></p>
"
"62801807","Connecting to Azure SQL Database through Azure Data Factory: assigning a static IP to integration runtime?","<p>I have an Azure Data Factory copy activity that will copy some table data from a CSV file to an Azure SQL Database. I'm having trouble getting Data Factory to connect with Azure SQL DB, and I don't want to turn on &quot;Allow Azure services and resources to access this server&quot; since I don't want some services to access the Azure SQL server.</p>
<p>I've tried setting up firewall IP rules to allow access to only the IP Data Factory uses, but I've discovered that it uses a different IP every time it tries to connect to my Azure SQL DB. I've also tried searching for a list of ranges that Data Factory pulls from to add to the firewall rules, but there doesn't seem to be such a list anywhere.</p>
<p>Is there any work around for this? Or is there any way to assign a static IP to Data Factory or it's integration runtime so I can just assign that IP to a firewall rule?</p>
","<azure><azure-sql-database><azure-data-factory>","2020-07-08 18:57:13","1340","-1","1","62805887","<p>Yes, data factory now support static IP address range now. We can add these IP to the firewall to allow the access to the server.</p>
<p>With the <a href=""https://learn.microsoft.com/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""nofollow noreferrer"">introduction of Static IP address range</a>, you can now whitelist IP ranges for the particular Azure integration runtime region to ensure you don’t have to allow all Azure IP addresses in your cloud data stores. This way, you can restrict the IP addresses that are permitted to access the data stores.</p>
<p><a href=""https://i.stack.imgur.com/o0Ohz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o0Ohz.png"" alt=""enter image description here"" /></a></p>
<p>For more details, please ref this Azure official document:</p>
<ol>
<li><a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/azure-data-factory-now-supports-static-ip-address-ranges/ba-p/1117508"" rel=""nofollow noreferrer"">Azure Data Factory now supports Static IP address ranges</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/virtual-network/service-tags-overview#discover-service-tags-by-using-downloadable-json-files"" rel=""nofollow noreferrer"">service tags IP range download link</a></li>
</ol>
<p>Feedback: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/20565967-static-ip-ranges-for-data-factory-and-add-adf-to-l"" rel=""nofollow noreferrer"">Static IP ranges for Data Factory and add ADF to list of Trusted Azure Services</a></p>
<p>For example, bellow is the my Data factory AustraliaEast static IP range:
<a href=""https://i.stack.imgur.com/zXqIs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zXqIs.png"" alt=""enter image description here"" /></a></p>
<p>But the static IP lists are updated and published weekly.</p>
<p>Hope this helps.</p>
"
"62799230","Use pipeline parameters in azure data factory copy pipeline","<p>I have a copy pipeline that reads data from a Netezza table. I use a query to load the rows based on a timestamp, something like</p>
<p><code>select * from myTable where creationTs &gt; '2019-10-10T00:00:00'</code></p>
<p>Right now the query contains a hardcoded value for the timestamp, I would like to be able to read that value from the pipeline parameters, so I can re-run for different dates without changing the query.
Is it even possible? I tried all sorts of syntaxes, to no avail.</p>
<p>Thank you in advance</p>
","<azure><azure-data-factory>","2020-07-08 16:23:39","173","0","1","62799539","<p>Yes, It is possible with pipeline parameters.</p>
<ul>
<li>Create a new <code>pipeline parameter</code></li>
</ul>
<p><a href=""https://i.stack.imgur.com/8P9HK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8P9HK.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Access the parameter value with expression like <code>@{pipeline().parameters.arg1}</code></li>
</ul>
<p><a href=""https://i.stack.imgur.com/zDDra.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zDDra.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><p>Every time you hit run/debug, you will be prompted to enter the value
for parameters.</p>
</li>
<li><p>When automating it with triggers, you may pass the parameter value as part of the triggers.</p>
</li>
</ul>
"
"62796975","Multiple TeraData cutsom queries to individual blob CSV files usinf Azure Data Factory","<p>I am very much new to Azure Data Factory.
I would like to know how to copy data from Teradata multiple queries(around 6 custom queries) to .csv files in Blob container using ADF.</p>
<p>I have referred documents, but I can copy data from tables , not from multiple custom queries.
If you could give me step by step instructions, that would help me a lot to learn.</p>
<p>Thanks!!</p>
","<azure><azure-data-factory>","2020-07-08 14:21:42","43","0","1","62806344","<p>When Teradata as source, it has the <strong>query</strong> property:
<a href=""https://i.stack.imgur.com/lIRlC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lIRlC.png"" alt=""enter image description here"" /></a></p>
<p>Ref：https://learn.microsoft.com/en-us/azure/data-factory/connector-teradata#teradata-as-source</p>
<p>We use the query property to run your custom queries. Click the <strong>Query</strong> operation and put query in the box. I just guess your query across more tables.</p>
<p><a href=""https://i.stack.imgur.com/GuLjH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GuLjH.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps.</p>
"
"62796468","CICD for ADF: how to restrict access to the adf_publish branch without breaking the CICD process?","<p>as part of our project we are linking an azure datafactory instance to azure git using the steps documented by microsoft. (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/source-control</a>) We are hoping to restrict access as much possible to avoid user error. In keeping with that goal we are trying to determine if there is a way to configure the branch policies in azure git so that only the adf instance can update the adf_publish branch after a successful publish. <strong>No User</strong> should be able to update the adf_publish branch from the repo itself. In the past we configured the branch policy to disable any pushes to the adf_publish branch. However also this prevented the adf instance from pushing its changes. So we trying to determine if there is a possible solution to for this goal.</p>
","<continuous-integration><azure-data-factory>","2020-07-08 13:55:50","626","4","1","62803845","<p>I think this question is more about branch security rather than Azure Data Factory itself.<br />
If you want to control or limit control for users - simply select a particular branch (in this case it's <strong>adf_publish</strong>) in Azure DevOps and go to <strong>Branch security</strong>:</p>
<p><a href=""https://i.stack.imgur.com/abEFc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/abEFc.png"" alt=""enter image description here"" /></a></p>
<p>Then you can set up required permissions per group or per user, like <em>Contribute</em> or <em>Force Push</em>:
<a href=""https://i.stack.imgur.com/YQ8NA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YQ8NA.png"" alt=""enter image description here"" /></a></p>
"
"62795095","Azure Data Factory: Can self-hosted Integration Runtime be configured to use Sql Integrated Security","<p>We are using Azure Data Factory (ADFv2) to move data from on-prem Sql Server to Azure cloud, using a self-hosted Integration Runtime (IR).</p>
<p>Saving sql credentials in keyVault works great, but I wanted to know if self-hosted IR could be configured to use Windows Integrated Security to connect to on-prem Sql Server sources. Can the IR service be configured to run as a windows user that has permissions to connect to sql?</p>
<p>I am going to try it out myself, but wanted to post a question here in case someone has already tried it. I could not find any ADF documentation on this.</p>
","<sql-server><azure><azure-data-factory><integrated-security>","2020-07-08 12:43:28","787","1","1","62795603","<blockquote>
<p>Can the IR service be configured to run as a windows user that has permissions to connect to sql?</p>
</blockquote>
<p>No.  And this is intentional, because if that were allowed:</p>
<ol>
<li>The IR service account would accumulate privileges</li>
</ol>
<p>and</p>
<ol start=""2"">
<li>The users of any linked data factory would be able to use those privileges</li>
</ol>
<p>So the IR only handles network connectivity, and data factory users must bring their own credentials for accessing source systems.</p>
"
"62785998","Compare two tables to find missing rows dynamically in Azure Data Factory","<p>I have a Mapping table that stores Source and destination table name. I want to compare source table data with destination table data and find out the missing rows. Table names should be passed dynamically in data flow mapping.</p>
","<azure><azure-data-factory>","2020-07-08 00:58:48","2662","1","2","62787186","<p>Use the Exists transformation in ADF data flows with the &quot;Does not exists&quot; option.</p>
<p><a href=""https://i.stack.imgur.com/1hDjZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1hDjZ.png"" alt=""enter image description here"" /></a></p>
"
"62785998","Compare two tables to find missing rows dynamically in Azure Data Factory","<p>I have a Mapping table that stores Source and destination table name. I want to compare source table data with destination table data and find out the missing rows. Table names should be passed dynamically in data flow mapping.</p>
","<azure><azure-data-factory>","2020-07-08 00:58:48","2662","1","2","66342960","<p>Do you have an example syntax when using the same dynamic column name $ColumnName for both the streams? Because all I get is true because I assume the $ColumnName seems to be evaluated as a string and then &quot;string = string&quot; evaluates to true.</p>
<p>In some of the documentation it indicates inputStream@column == inputStream@column so I tried eg sourceStaging@$ColumnName == sourceWarhouse@$ColumnName... but that doesn't work either.</p>
"
"62776392","Azure Storage V2 blob event - Not triggering ADF","<p>I have an Azure Data Factory V2 (X), with a blob event trigger. The blob event trigger works fine and triggers the Data Facotry (X) when I manually upload a file using the storage explorer. But when I use another data factory (Y) to write the file to the same Blob Storage instead of manual write,  the event doesn't trigger my Data Factory (X).</p>
<p>I have verified the following:</p>
<ol>
<li>There are no multiple events under the 'Events' blade section of the Blob Storage.</li>
<li>There is a System Topic and a Subscription created with the correct filters. I have the 'BeginsWith' with my container name and  'EndsWith' with the file extension 'parquet'.</li>
<li>I have verified the related questions on Stack Overflow but this seems to be different.</li>
</ol>
<p>Any clues what could be wrong or is this a known issue?</p>
<p>EDIT:
I checked the logs of the Event Topic &amp; Subscription, when the file is written by the ADF (Y) there is no event generated but with the manual upload/write the event gets triggered.</p>
","<azure-data-factory><azure-eventgrid>","2020-07-07 13:27:14","1040","2","1","63149966","<p>If your event of trigger is blob created, then the blob event trigger is basically depends on the new Etag. When the new etag is created, then your pipeline will be triggered.</p>
<p>This is the trigger on my side:</p>
<p><a href=""https://i.stack.imgur.com/xJAQ7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xJAQ7.png"" alt=""enter image description here"" /></a></p>
<p>On my side, I create 3 containers: test1,test2,test3. Pipeline on Datafactory Y send files from test1 to test2, and pipeline on Datafactory X have above trigger. If triggered, it will send files from test2 to test3. When file is writed by the pipeline on Datafactory Y, pipeline on Datafactory X will be triggered. And files will be send to test3 with no problem.</p>
<p>Basiclly, the principle of the 'blob created' is based on the new etag of your container. When your Datafactory send files to target container, then there is no doubt that a new etag will appear.</p>
<p>I notice you mentioned the BeginsWith and EndsWith, so I think this is the problem comes from. Please have a check of that.</p>
"
"62772934","Azure Data Factory V2 logical OR","<p>Is it possible to create a logical OR for Azure Data Factory V2 pipeline activities? For example, we have two copy activities and on fails of any of its the Data Factory should call an API.</p>
<p><a href=""https://i.stack.imgur.com/APWCs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/APWCs.png"" alt=""enter image description here"" /></a></p>
","<azure><api><azure-data-factory>","2020-07-07 10:13:04","1163","1","2","62783474","<p>Not that I know about. When an activity has multiple conditions, they are treated as an AND condition, meaning that both copy activities have to fail to execute the web activity.</p>
<p>One common pattern to do this is create a pipeline that encapsulates the logic to report errors, and call it from each activity's failurem, capturing error output to a pipeline parameter.</p>
<p>Hope this helped!</p>
"
"62772934","Azure Data Factory V2 logical OR","<p>Is it possible to create a logical OR for Azure Data Factory V2 pipeline activities? For example, we have two copy activities and on fails of any of its the Data Factory should call an API.</p>
<p><a href=""https://i.stack.imgur.com/APWCs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/APWCs.png"" alt=""enter image description here"" /></a></p>
","<azure><api><azure-data-factory>","2020-07-07 10:13:04","1163","1","2","62788570","<p>Yes, we can.</p>
<p>You could add a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">If Condition</a> to create the logical pipeline. If the two copy active output status contains 'failed', then  call the API.</p>
<p>As we know, each copy active has the output has the bellow details:</p>
<pre><code>&quot;executionDetails&quot;: [
        {
            &quot;source&quot;: {
                &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                &quot;region&quot;: &quot;East US&quot;
            },
            &quot;sink&quot;: {
                &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                &quot;region&quot;: &quot;East US&quot;
            },
            &quot;status&quot;: &quot;Succeeded&quot;,
            &quot;start&quot;: &quot;2020-07-08T05:37:33.9079553Z&quot;,
            &quot;duration&quot;: 4,
            &quot;usedDataIntegrationUnits&quot;: 4,
            &quot;usedParallelCopies&quot;: 1,
            &quot;profile&quot;: {
                &quot;queue&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 2
                },
                &quot;transfer&quot;: {
                    &quot;status&quot;: &quot;Completed&quot;,
                    &quot;duration&quot;: 1,
                    &quot;details&quot;: {
                        &quot;readingFromSource&quot;: {
                            &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                            &quot;workingDuration&quot;: 0,
                            &quot;timeToFirstByte&quot;: 0
                        },
                        &quot;writingToSink&quot;: {
                            &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                            &quot;workingDuration&quot;: 0
                        }
                    }
                }
            },
</code></pre>
<p>We can set the <code>executionDetails:{&quot;status&quot;: &quot;Succeeded&quot;}</code> to the if-condition active:
<a href=""https://i.stack.imgur.com/azdCM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/azdCM.png"" alt=""enter image description here"" /></a></p>
<p>Expression: No matter which actives failed, all the API:</p>
<pre><code>@or(equals(activity('Copy data1').output.executionDetails.status,'Failed'),equals(activity('Copy data2').output.executionDetails.status,'Failed') )
</code></pre>
<p>If condition true, add the true active to call the API:
<a href=""https://i.stack.imgur.com/VkYB3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VkYB3.png"" alt=""enter image description here"" /></a></p>
<p>If all the copy actives are &quot;succeeded&quot;(condition false) and we don't want to do anything, we don't need to add the false active.</p>
<p>Hope this helps.</p>
"
"62770462","Checking file count returned by getmetadata activity in Azure data factory","<p>How to get how many file names/folder names are returned by <code>getmetadata</code> activity in Azure data factory?</p>
<p>I want to get the number of files/folders returned by <code>getmetadata</code> activity and on the basis of this count decide which activities will execute..</p>
<p>Anyone have any idea about how can we get this count?</p>
","<azure><azure-data-factory>","2020-07-07 07:49:48","3720","-1","3","62771680","<p>Please try something like this:</p>
<p>The screenshots of pipeline:
<a href=""https://i.stack.imgur.com/BdbT7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BdbT7.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><p><strong>Setting of Get Metadata1</strong>
<a href=""https://i.stack.imgur.com/fKavR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fKavR.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p><strong>Setting of If Condition1</strong></p>
</li>
</ul>
<blockquote>
<p>Expression:@greater(length(activity('Get
Metadata1').output.childItems),100)</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/z8y7P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z8y7P.png"" alt=""enter image description here"" /></a></p>
<p>Hope this can help you:).</p>
"
"62770462","Checking file count returned by getmetadata activity in Azure data factory","<p>How to get how many file names/folder names are returned by <code>getmetadata</code> activity in Azure data factory?</p>
<p>I want to get the number of files/folders returned by <code>getmetadata</code> activity and on the basis of this count decide which activities will execute..</p>
<p>Anyone have any idea about how can we get this count?</p>
","<azure><azure-data-factory>","2020-07-07 07:49:48","3720","-1","3","62828843","<p>Thanks to those who commented and special thanks to Steve Zhao.</p>
<p>Actually i wanted to check if files present i.e length is greater than 0 then go to flow A else Choose flow B.</p>
<p>I've tried standalone greater and length function in my expression (greater(length(activity('GETFILENAMESFROMBLOB').output.childitems), 0)) to calculate length but when child items are 0 but when i tried this previous activity i.e GETMETADATA doesn't contains childitems array in it's output so my If condition activity gives error that property childitems doesn't exist.</p>
<p>I also tried empty function but the main issue was when there are no input files for getmetadata then we should not expect a childitems array in it's output.</p>
<p>So here is how i have solved the problem, at first i checked if childitems array is present in output of my getmetadata activity, then we'll get the actual count using length function else expression will have to return 0. Below is the expression used for if condition Activity. Please check.</p>
<p><strong>Expression:</strong></p>
<p><em><strong>@if( contains(activity('GETFILENAMESFROMBLOB').output,'childitems'), length(activity('GETFILENAMESFROMBLOB').output.childitems), equals(2,3))</strong></em></p>
<p>Hope this may help you!</p>
"
"62770462","Checking file count returned by getmetadata activity in Azure data factory","<p>How to get how many file names/folder names are returned by <code>getmetadata</code> activity in Azure data factory?</p>
<p>I want to get the number of files/folders returned by <code>getmetadata</code> activity and on the basis of this count decide which activities will execute..</p>
<p>Anyone have any idea about how can we get this count?</p>
","<azure><azure-data-factory>","2020-07-07 07:49:48","3720","-1","3","64465263","<p>Below Code in <em>IF condition Activities expression</em> should be suffice for <em><strong>filecount &gt; 0</strong></em> logic.
<strong>@greater(string(length(activity('Get Metadata1').output.childitems)),'0')</strong>
<a href=""https://i.stack.imgur.com/cKsGx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cKsGx.png"" alt=""enter image description here"" /></a></p>
"
"62770141","Cannot get storage account key. Cannot read property 'slice' of null in azure data factory","<p>When i am trying to create any linked service in azure data factory then i am getting the error message.
&quot;Cannot get storage account key. Cannot read property 'slice' of null&quot;</p>
<p>Request you to please help resolve</p>
","<azure><azure-storage><azure-data-factory>","2020-07-07 07:29:43","646","0","1","63147391","<p>This question needs more details.</p>
<p>I can only tell about how the linked service can create success.</p>
<p>It seems you want to create a linked service for azure storage account, so just follow the below steps:</p>
<p>2, Go to the manage tab and create a new linked service:</p>
<p><a href=""https://i.stack.imgur.com/BQlDF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BQlDF.png"" alt=""enter image description here"" /></a></p>
<p>2, choose a type of linked service.</p>
<p>3, give the linked service your storage key, it is getting from this place:</p>
<p><a href=""https://i.stack.imgur.com/S8S0w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S8S0w.png"" alt=""enter image description here"" /></a></p>
<p>4, then you can create the linked service success:</p>
<p><a href=""https://i.stack.imgur.com/GEF5j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GEF5j.png"" alt=""enter image description here"" /></a></p>
"
"62766636","What permissions are needed to run an Azure Data Factory (ADF) debug session?","<p>What are the Least Privilege permissions that are needed to run a debug session?
We have removed Data Factory Contributor because we want to disallow publishing to the dev instance by all developers, as is mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control#permissions"" rel=""nofollow noreferrer"">Best Practices</a>.</p>
<blockquote>
<p>Only a select set of people should be allowed to publish to the
factory. To do so, they must have the Data Factory contributor role on
the resource group the factory is in.</p>
</blockquote>
<p>However, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/iterative-development-debugging"" rel=""nofollow noreferrer"">this page</a> does not say what permissions are required to run Debug and we are getting the following error when trying to Debug a pipeline:
<a href=""https://i.stack.imgur.com/sKawZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sKawZ.png"" alt=""enter image description here"" /></a></p>
<p>Related Documentation Issue: <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/58517"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/58517</a></p>
","<azure-data-factory>","2020-07-07 01:06:24","7515","1","1","62767134","<p>To create and manage child resources for Data Factory - including datasets, linked services, pipelines, triggers, and integration runtimes - the following requirements are applicable:</p>
<ul>
<li>To create and manage child resources in the Azure portal, you must
belong to the <strong>Data Factory Contributor</strong> role at the resource group
level or above.</li>
<li>To create and manage child resources with PowerShell or the SDK, the
<strong>contributor</strong> role at the resource level or above is sufficient.</li>
</ul>
<p>Debug actually runs the pipeline, it also means manage the pipeline runs. As I understand, we need the <strong>Data Factory Contributor</strong> or <strong>contributor</strong> permission.</p>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-roles-permissions#roles-and-requirements"" rel=""nofollow noreferrer"">Roles and requirements</a></p>
<p>As the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control#permissions"" rel=""nofollow noreferrer"">Best Practices</a> document said:</p>
<ul>
<li>&quot;Only a select set of people should be allowed to publish to the
factory. To do so, they must have the Data Factory contributor role
on the resource group the factory is in.&quot;</li>
</ul>
<p>If you want to control the data factory permission of the developers, you could follow bellow steps:</p>
<ol>
<li><p>Create AAD user group, and add the selected developers to
the group.</p>
</li>
<li><p>Add the <strong>Data Factory Contributor</strong> or <strong>contributor</strong> role to the
group. Then all the users in the group will have the permission.</p>
<p><a href=""https://i.stack.imgur.com/g1LJq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g1LJq.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-groups-create-azure-portal"" rel=""nofollow noreferrer"">Create a basic group and add members using Azure Active Directory</a></p>
<p>Hope this helps.</p>
"
"62756152","Using azure custom activity to run c# executable which converts DBF file into CSV","<p>Is there any way to query a dbf file stored in azure blob storage and then put the results into a data table? So far I was downloading the dbf file locally and then I was using OleDb connection to query the dbf file and load the results into a data table. Now I am moving my executable into Azure Data Factory as a custom activity and therefore there is no local path to download the dbf file. Is there any other way to query a dbf file stored in azure blob storage without having to download it? My executable is written in c#. Can you point me to the right direction?</p>
<p>Any help will be much appreciated!</p>
","<c#><azure><azure-data-factory><dbf><custom-activity>","2020-07-06 12:30:39","399","0","1","62758147","<p>If you use Azure Data Lake Gen2 rather than Azure Storage, you can query your files using USQL. (PS: dbf is not a valid format in this case)</p>
<p>Your executable can be replaced by an Azure Function, which can be an activity in your Azure Data Factory pipeline:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity#:%7E:text=%20Azure%20Function%20activity%20in%20Azure%20Data%20Factory,Activity%20supports%20routing.%20For%20example%2C%20if...%20More%20"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity#:~:text=%20Azure%20Function%20activity%20in%20Azure%20Data%20Factory,Activity%20supports%20routing.%20For%20example%2C%20if...%20More%20</a></p>
"
"62752505","Give contributor accesss to a service principal id to my storage account","<p>In data factory v2, I have created a linked service to O365 using a Tenant ID, Client ID and Access key.
But, O365 dataset only supports blob as sink with service principal authentication.
I have tried adding the client ID I had used for O365 connector a contributor role to my storage account. But, it is not found in the search. I am thinking client id is the service principal.
How can I add the same service principal I had used to create o365 connector in ADF to my storage account?
Thanks</p>
","<azure-active-directory><azure-data-factory>","2020-07-06 08:55:27","167","0","1","62752864","<p>Client id is not supported in this case.</p>
<p>We need to search for a security principal by entering a string to search for name or email address.</p>
<p>So here you should search for the name of the service principal rather than client id.
<a href=""https://i.stack.imgur.com/zRleX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zRleX.png"" alt=""enter image description here"" /></a></p>
"
"62726597","Databricks- Can we variablize the mount_point name during creation by passing the value from SQL lookup table","<p>Thanks  in Advance this site is of greathelp!!!!</p>
<p>Question:</p>
<p>Can we get variabliize the mount_point name and the filename while creating the data frame.</p>
<p>Mount name: select company from comaytable(pass the comapy name as mountpoint variable)</p>
<p>source = &quot;wasbs://uidbddnnps@dmoddddssa.blob.core.windows.net&quot;,
mount_point = &quot;/mnt/&quot;<strong>VARIABLIZENAME</strong>&quot;,
extra_configs = {&quot;fs.azure.sas.uiasasps.dmodssdsdgarea.blob.core.windows.net&quot;:dbutils.secrets.get(scope = &quot;AIdsT&quot;, key = &quot;keydmodslaarea&quot;)})
print(&quot;=&gt; Succeeded&quot;)</p>
<p>File Name Variablzie:</p>
<p>df = spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;,&quot;).options(header= &quot;true&quot;, inferschema='true').option('escape','&quot;').load(&quot;/mnt/AT/VARIABLIZE.csv&quot;)</p>
<p>Can we pass this values from datafactory also i can make use of it if necessary</p>
","<python><sql><databricks><azure-data-factory><azure-databricks>","2020-07-04 07:40:46","677","1","2","62754989","<p>You may checkout the steps mentioned below:</p>
<p><strong>Step1:</strong> Declaring the variables:</p>
<pre><code>mountname = 'test'
csvname = 'original.csv'
path = &quot;dbfs:/mnt/{0}/{1}&quot;.format(mountname,csvname)
</code></pre>
<p><strong>Step2:</strong> Mounting the storage account</p>
<pre><code>dbutils.fs.mount(
  source = &quot;wasbs://test@chepra.blob.core.windows.net/&quot;,
  mount_point = &quot;/mnt/{0}&quot;.format(mountname),
  extra_configs = {&quot;fs.azure.sas.test.chepra.blob.core.windows.net&quot;:&quot;gv7nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXlOiA==&quot;})
print(&quot;=&gt; Succeeded&quot;) 
</code></pre>
<p><strong>Step3:</strong> Creating the Spark Dataframe</p>
<pre><code>df = spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;,&quot;).options(header= &quot;true&quot;, inferschema='true').option('escape','&quot;').load(&quot;{0}&quot;.format(path))
</code></pre>
<p><a href=""https://i.stack.imgur.com/xQfnt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xQfnt.png"" alt=""enter image description here"" /></a></p>
"
"62726597","Databricks- Can we variablize the mount_point name during creation by passing the value from SQL lookup table","<p>Thanks  in Advance this site is of greathelp!!!!</p>
<p>Question:</p>
<p>Can we get variabliize the mount_point name and the filename while creating the data frame.</p>
<p>Mount name: select company from comaytable(pass the comapy name as mountpoint variable)</p>
<p>source = &quot;wasbs://uidbddnnps@dmoddddssa.blob.core.windows.net&quot;,
mount_point = &quot;/mnt/&quot;<strong>VARIABLIZENAME</strong>&quot;,
extra_configs = {&quot;fs.azure.sas.uiasasps.dmodssdsdgarea.blob.core.windows.net&quot;:dbutils.secrets.get(scope = &quot;AIdsT&quot;, key = &quot;keydmodslaarea&quot;)})
print(&quot;=&gt; Succeeded&quot;)</p>
<p>File Name Variablzie:</p>
<p>df = spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;,&quot;).options(header= &quot;true&quot;, inferschema='true').option('escape','&quot;').load(&quot;/mnt/AT/VARIABLIZE.csv&quot;)</p>
<p>Can we pass this values from datafactory also i can make use of it if necessary</p>
","<python><sql><databricks><azure-data-factory><azure-databricks>","2020-07-04 07:40:46","677","1","2","62819874","<p>Just to understand you have a ADF where you are calling a lookup ( running a SQL query ) and the intent is that you want to pass the value from the Lookup to a notebook .</p>
<p>If thats the case we can achieve this by implementing a Lookup acitivity and  a foreach ( to loop in the all the records . Inside the foreach please use a Notebook activity , point this to the notebook which you want to run and pass value of the company ( something as @item() etc ) <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a> .</p>
<p>On the notebook  you can use the widget and get the value as incoming parameter .</p>
<p>CompanyName = dbutils.widgets.get(&quot;CompanyName&quot;)</p>
<p>Please let me know if you have any questions .</p>
"
"62720957","Transform data type in Azure Data Factory between tables","<p>I want to copy the contents of a certain table to another, both stored in the Azure database, but I'm wondering whether I can change the data type of a certain field, i.e: if there's a field in table 1 whose type is a char, and I want to copy it to table 2, but change its type to byte, is it possible?</p>
","<azure><azure-data-factory>","2020-07-03 18:33:06","556","1","1","62729059","<p>If these two tables are in the same database then the easiest way to do this would be to use T-SQL, eg</p>
<pre><code>INSERT INTO yourTargetTable ( col1, col2 ...
SELECT
    CAST( col1 AS INT ),
    CAST( col2 AS CHAR(1) ),
    ...
FROM yourTable
</code></pre>
<p>Byte is not a SQL Server data type, so please post some sample data and expected results.</p>
<p>If the tables are in different databases then you may have a case for using Data Factory - please post back with further information.</p>
"
"62719685","Is there a way to schedule a Data Factory trigger that runs multiple times everyday BUT ONLY DURING A SPECIFIC TIME FRAME?","<p>I've created a pipeline with a web activity that uses REST API calls to my Synapse instance and pauses it at the end of every day. I'm trying to schedule it in Data Factory but the problem is I can't schedule it in the way I need. I want the pipeline to be triggered every hour but only between 6 PM and 10 PM everyday.</p>
<p>But ADF doesn't seem to provide this facility where a pipeline could be run only between a fixed timeframe  every single day. I don't want my Synapse instance to be paused every hour during the day time.</p>
<p>Is there a way to bypass this limitation of Azure Data Factory scheduling?</p>
","<azure-data-factory>","2020-07-03 16:53:09","3165","2","4","62748684","<p>You can using Azure logic app <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-workflow-actions-triggers#recurrence-trigger"" rel=""nofollow noreferrer"">Recurrence trigger</a> to achieve that:
<a href=""https://i.stack.imgur.com/F591F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F591F.png"" alt=""enter image description here"" /></a></p>
<p>Create a Recurrence trigger to call the data factory pipeline run with <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#get-a-pipeline-run"" rel=""nofollow noreferrer"">Get a pipeline run</a> action.</p>
<p>Trigger the pipeline runs in [18,19,20,21,22](6 PM~10 PM) clock every day.</p>
<p>Hope this helps.</p>
"
"62719685","Is there a way to schedule a Data Factory trigger that runs multiple times everyday BUT ONLY DURING A SPECIFIC TIME FRAME?","<p>I've created a pipeline with a web activity that uses REST API calls to my Synapse instance and pauses it at the end of every day. I'm trying to schedule it in Data Factory but the problem is I can't schedule it in the way I need. I want the pipeline to be triggered every hour but only between 6 PM and 10 PM everyday.</p>
<p>But ADF doesn't seem to provide this facility where a pipeline could be run only between a fixed timeframe  every single day. I don't want my Synapse instance to be paused every hour during the day time.</p>
<p>Is there a way to bypass this limitation of Azure Data Factory scheduling?</p>
","<azure-data-factory>","2020-07-03 16:53:09","3165","2","4","67996403","<p>Do not use Logic App or anything else if you want to be good with Data Factory. Can use does not mean should use. just put 18 into Hours box then click anywhere blank outside the box, repeat this with 19 and any other hours will give your Schedule execution times 08:00,19:00,20:00 ...</p>
"
"62719685","Is there a way to schedule a Data Factory trigger that runs multiple times everyday BUT ONLY DURING A SPECIFIC TIME FRAME?","<p>I've created a pipeline with a web activity that uses REST API calls to my Synapse instance and pauses it at the end of every day. I'm trying to schedule it in Data Factory but the problem is I can't schedule it in the way I need. I want the pipeline to be triggered every hour but only between 6 PM and 10 PM everyday.</p>
<p>But ADF doesn't seem to provide this facility where a pipeline could be run only between a fixed timeframe  every single day. I don't want my Synapse instance to be paused every hour during the day time.</p>
<p>Is there a way to bypass this limitation of Azure Data Factory scheduling?</p>
","<azure-data-factory>","2020-07-03 16:53:09","3165","2","4","69661921","<p>I had a similar problem. I let the trigger run hourly and added an If-statement to a new pipeline that checks if the time is within the right timeframe, if so, trigger the original pipeline, else, do nothing.</p>
"
"62719685","Is there a way to schedule a Data Factory trigger that runs multiple times everyday BUT ONLY DURING A SPECIFIC TIME FRAME?","<p>I've created a pipeline with a web activity that uses REST API calls to my Synapse instance and pauses it at the end of every day. I'm trying to schedule it in Data Factory but the problem is I can't schedule it in the way I need. I want the pipeline to be triggered every hour but only between 6 PM and 10 PM everyday.</p>
<p>But ADF doesn't seem to provide this facility where a pipeline could be run only between a fixed timeframe  every single day. I don't want my Synapse instance to be paused every hour during the day time.</p>
<p>Is there a way to bypass this limitation of Azure Data Factory scheduling?</p>
","<azure-data-factory>","2020-07-03 16:53:09","3165","2","4","75865956","<p>We can have trigger at any time by specifying in Hours tab while creating a trigger:</p>
<p><img src=""https://i.stack.imgur.com/dWWyd.png"" alt=""Attached Sample Image"" /></p>
"
"62714788","Azure Data Factory: Flattening/normalizing a cloumn from CSV file using Azure Data Factory activity","<p>I have pulled a csv file from one of our source using ADF and there is one column called &quot;attributes&quot; which contains multiple fields (in the form of key value pairs). Now I want to expand that column into different fields (columns). Below is the sample of that:</p>
<pre><code>leadId  activityDate    activityTypeId  campaignId  primaryAttributeValue   attributes
1234    2020-06-22T00:00:44Z    46  33686   Mail    {&quot;Description&quot;:&quot;Clicked: https://stepuptostepout.com/&quot;,&quot;Source&quot;:&quot;Lead action&quot;,&quot;Date&quot;:&quot;2020-06-21 19:00:44&quot;}
5678    2020-06-22T00:01:54Z    13  33128   SMS {&quot;Reason&quot;:&quot;Changed&quot;,&quot;New Value&quot;:110,&quot;Old Value&quot;:null,&quot;Source&quot;:&quot;Marketo Flow Action&quot;}
</code></pre>
<p>Here the attributes column have different Key-value pairs and I want them in different column so that I can store them in Azure SQL Database:</p>
<p>attributes
{&quot;Reason&quot;:&quot;Changed&quot;,&quot;New Value&quot;:110,&quot;Old Value&quot;:null,&quot;Source&quot;:&quot;Marketo&quot;}</p>
<p>I want them as:</p>
<pre><code>Reason     New Value    Old Value   Source
Changed    110          null        Marketo
</code></pre>
<p>I am using Azure Data Factory. Please help!</p>
<p>Updating this:
One more thing I have noticed in my data is that the keys are not uniform, also if there is one key (say 'Source') present for one lead ID it might not be present/missing in the other leadId, making this more complicated. Hence having a separate column for each Attribute Key might not be a good idea.
Thus, we can have a separate table for 'attribute' field with lead ID, AttributeKey, AttributeValue as columns (we can join this with our main table using LeadID). The Attribute table will look like:</p>
<pre><code>LeadID AttributeKey AttributeValue
5678 Reason Changed
5678 New Value 110
5678 Old Value null
5678 Source Marketo
</code></pre>
<p>Can you help me I can I achieve this using ADF?</p>
","<azure-data-factory>","2020-07-03 11:48:13","501","0","1","62769884","<p>You can use data flow to do this thing.Below is my test sample.</p>
<p><a href=""https://i.stack.imgur.com/m8dtR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m8dtR.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><strong>Setting of source1</strong>
<a href=""https://i.stack.imgur.com/li4eY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/li4eY.png"" alt=""enter image description here"" /></a></li>
<li><strong>Setting of Filter1</strong></li>
</ul>
<blockquote>
<p>instr(attributes,'Reason') != 0</p>
</blockquote>
<ul>
<li><p><strong>Setting of DerivedColumn1</strong></p>
<p><a href=""https://i.stack.imgur.com/r6GrJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r6GrJ.png"" alt=""enter image description here"" /></a></p>
</li>
</ul>
<p>Here is my expression and it's complex.</p>
<blockquote>
<p>@(Reason=translate(split(split(attributes,',')[1],':')[2],'&quot;',''),
NewValue=translate(split(split(attributes,',')[2],':')[2],'&quot;',''),
OldValue=translate(split(split(attributes,',')[3],':')[2],'&quot;',''),
Source=translate(translate(split(split(attributes,',')[4],':')[2],'&quot;',''),'}',''))</p>
</blockquote>
<ul>
<li><strong>Setting of Select1</strong>
<a href=""https://i.stack.imgur.com/fDvn3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fDvn3.png"" alt=""enter image description here"" /></a></li>
</ul>
<p>Here is the result:
<a href=""https://i.stack.imgur.com/UYMUH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UYMUH.png"" alt=""enter image description here"" /></a></p>
<p>By the way,if your file is json,may be simple to do this than csv.
Hope this can help you:).</p>
"
"62712207","create more than one derived column in one component ADF","<p>I have started using ADF. When creating new derived columns, I use the derived column functionality. Is it possible to use one instance of this component to create more than one new column or do I need to nest components sequentially, one per each new column?</p>
<p>Thank you.</p>
<p><a href=""https://i.stack.imgur.com/ttj0r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ttj0r.png"" alt=""enter image description here"" /></a></p>
","<etl><azure-data-factory>","2020-07-03 09:12:26","60","0","2","62724752","<p>You can create multiple columns inside of a single Derived Column. You will find a + button to add additional columns.</p>
"
"62712207","create more than one derived column in one component ADF","<p>I have started using ADF. When creating new derived columns, I use the derived column functionality. Is it possible to use one instance of this component to create more than one new column or do I need to nest components sequentially, one per each new column?</p>
<p>Thank you.</p>
<p><a href=""https://i.stack.imgur.com/ttj0r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ttj0r.png"" alt=""enter image description here"" /></a></p>
","<etl><azure-data-factory>","2020-07-03 09:12:26","60","0","2","62752992","<p><strong>Is it possible to use one instance of this component to create more than one new column?</strong></p>
<p>Yes, it is. @Mark Kromer is correct. You can use one <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">DerivedColumn</a> to create more than one new columns:</p>
<p><a href=""https://i.stack.imgur.com/Vz5Ou.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vz5Ou.png"" alt=""enter image description here"" /></a></p>
<p>You could click the <strong>Inspect</strong> to see the input(source) schema and output schema.</p>
<p>Input(source) schema:</p>
<p><a href=""https://i.stack.imgur.com/SQgHS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SQgHS.png"" alt=""enter image description here"" /></a></p>
<p>Output schema:
<a href=""https://i.stack.imgur.com/uuv7u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uuv7u.png"" alt=""enter image description here"" /></a></p>
<p><strong>Do I need to nest components sequentially, one per each new column?</strong></p>
<p>To sum up, no, you don't need to to that. If you have any other concerns, please let us know.</p>
<p>Hope this helps.</p>
"
"62709845","From ADF connect to Azure SQL using AAD with Allow Azure services and resources access disabled","<p>I am working on ADF for ETL/ELT project, currently I am using Azure SQL database to load source data. To connect to Azure SQL database from ADF pipeline I have to enable &quot;Allow Azure services and resources to access this server&quot; whereas this has security threat anyone can connect to this server in Azure environment.</p>
<p>Hence I wanted to disable this option and but also I would like to connect to Azure SQL database from ADF pipeline.
Is Azure AAD user may help in this case? I know about whitelisting some IP's but ADF's IP's are dynamic for everyrun hence it is very hard for me to whitelist few of them.</p>
","<azure><azure-active-directory><azure-sql-database><azure-data-factory>","2020-07-03 06:43:52","675","0","1","62711829","<p>Azure we know, if we don't set enable &quot;Allow Azure services and resources to access this server&quot; in, the only way to access the SQL database is that we must set the Data factory client IP to database firewall.</p>
<p>Or we will get the error like bellow:</p>
<p><a href=""https://i.stack.imgur.com/xjenG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xjenG.png"" alt=""enter image description here"" /></a></p>
<p>As you said &quot;ADF's IP's are dynamic for everyrun hence it is very hard for me to whitelist few of them&quot;,  a good news is that Azure Data Factory support static IP range as the firewall role.</p>
<p>Data Factory product team provides the workaround for us:</p>
<p>Great news – static IP range for Azure Integration Runtime is now available in all ADF regions! You can whitelist specific IP ranges for ADF as part of firewall rules. The IPs are documented here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses#azure-integration-runtime-ip-addresses-specific-regions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses#azure-integration-runtime-ip-addresses-specific-regions</a>. Static IP ranges for gov cloud and China cloud will be published soon!</p>
<p>Please refer to this blog post on how you can use various mechanisms including trusted Azure service and static IP to secure data access through ADF:
<a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/azure-data-factory-now-supports-static-ip-address-ranges/ba-p/1117508"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/azure-data-factory-now-supports-static-ip-address-ranges/ba-p/1117508</a></p>
<p>Service tag support will be made available in next few weeks. Please stay tuned!</p>
<p>If your network security requirement calls for ADF support for VNet and cannot be met using Trusted Azure service (released in Oct 2019), static IP range (released in Jan 2020), or service tag (upcoming), please vote for VNet feature here: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/37105363-data-factory-should-be-able-to-use-vnet-without-re"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/37105363-data-factory-should-be-able-to-use-vnet-without-re</a></p>
<p>Please reference this feedback: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/20565967-static-ip-ranges-for-data-factory-and-add-adf-to-l"" rel=""nofollow noreferrer"">Static IP ranges for Data Factory and add ADF to list of Trusted Azure Services</a></p>
<p>You could get the data factory static IP ranges and add the list to Azure SQL database firewall roles.</p>
<p>Data Factory static IP list example:</p>
<pre><code>{
      &quot;name&quot;: &quot;DataFactory&quot;,
      &quot;id&quot;: &quot;DataFactory&quot;,
      &quot;properties&quot;: {
        &quot;changeNumber&quot;: 6,
        &quot;region&quot;: &quot;&quot;,
        &quot;platform&quot;: &quot;Azure&quot;,
        &quot;systemService&quot;: &quot;DataFactory&quot;,
        &quot;addressPrefixes&quot;: [
          &quot;13.66.143.128/28&quot;,
          &quot;13.67.10.208/28&quot;,        
          ...,
          ...
               
        ]
      }
    }
</code></pre>
<p>Hope this helps.</p>
"
"62705052","ADF Copy Data activity - reference Source column from dynamic expression for Sink stored procedure parameter value","<p>I have Azure Data Factory Pipeline that has a Copy Data activity with Stored Procedure Sink. The SP takes as an input a table type parameter. Everything works fine so far. But now that SP has changed and I need to add another parameter that should be Max of one of the columns of the Source for my Copy Data activity. I cannot do this inside that SP since it is re-used by other components and takes it as input. Of course I could wrap it into another SP that would calculate that Max and then call the original SP, bu I thought better way would be if I could do that directly form ADF Pipeline. So I thought I could add a new parameter in my Sink SP and somehow get that Max using dynamic content, but I can't figure out a way to reference Source of the Copy Data activity.</p>
<p>Lets say the Source of Copy Data has column <code>Id</code> and I need to pass the Max value of that column to the SP Sink. Is there a way to do something like <code>max(@Source.Id)</code> in the SP's parameter value field?</p>
","<stored-procedures><azure-data-factory>","2020-07-02 21:09:01","615","0","1","62749791","<p>As far as I'm aware,it's impossible to reference the Source directly.So I think use <code>lookup activity</code> is an alternative to do such thing.</p>
<p>Hope this can help you.</p>
"
"62685744","How to execute a databricks notebook when multiple files loaded to ADLS","<p>I'm looking for a light way of executing a databricks notebook that depends on multiple files having been loaded to Azure Data Lake Storage.</p>
<p>Multiple different ADF packages are loading different files into ADLS and then processed by databricks notebooks. Some of the notebooks depend on multiple files from different packages.</p>
<p>A single file is simple enough with an event trigger. Can this be generalised to more than one file without something like Airflow handling dependencies?</p>
","<azure-data-factory><azure-databricks>","2020-07-01 21:40:10","705","0","1","64246613","<p>This isn't exactly light since you'll have to provision a Azure SQL table, but this is what I'll do:</p>
<ol>
<li>I would create and store a JSON file in ADLS which details each notebook/pipeline and the file name dependencies.</li>
<li>I'll then provision an Azure SQL Table to store the metadata of each of these files. Essentially, this table will have 3 columns:</li>
</ol>
<ul>
<li>General File Name (which matches the file name dependencies in step #1 (e.g.: FileName)</li>
<li>Real File Name (e.g.:FileName_20201007.csv)</li>
<li>Timestamp</li>
<li>Flag (boolean) if file is present</li>
<li>Flag (boolean) if file is processed (i.e.: it's dependent Databricks
notebook has run)</li>
</ul>
<ol start=""3"">
<li><p>To populate the table in Step#2, I'd use a Azure Logic App which will look for when a blob that meets your criteria is created and then subsequently update/create a new entry on the Azure SQL Table.
See:
<a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage</a> &amp;
<a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sqlazure"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sqlazure</a></p>
</li>
<li><p>You'll need to ensure that at the end of the Azure pipeline/Databricks Notebook that is ran, you update the Azure SQL flag of the respective dependencies to indicate these versions of the file is processed. Your Azure SQL Table will function as a 'watermark' table.</p>
</li>
</ol>
<p>Before your pipeline triggers the Azure databricks notebook, your pipeline will look up the JSON file in ADLS, identify the dependencies for each Notebook, check if all the dependencies are available AND not processed by the Databricks notebook, and subsequently continue to run the Databricks notebook once all this criteria is met.</p>
<p>In terms of triggering your pipeline, you could either use an Azure LogicApp to do this or leverage a tumbling window on ADF.</p>
"
"62682936","File filtering in data factory based on last successful run date","<p>I have files in SFTP that get uploaded every week. The files have a name with the date appended to it. I want to copy only the newest files every week into adls. Whenever the job is completed successfully the date is stored in a sql table so I can look up the last successful run date.</p>
<p>In my mind I think I need something like this: filter files whose name contains a date greater than or equal to last successful run date.... or filter files whose last modified date is greater or equal to last successful run date?</p>
<p>This way if the job failed on the last run it will grab the previous weeks and the current weeks files.</p>
<p>Currently my pipeline has a lookup that can show me the last successful run, get meta data that shows all the files in the sftp folder, filter, and for each(copy). I know the copy works because I set the filter for a specific file and it worked. The area I need help with is the filter.</p>
","<azure><azure-pipelines><azure-data-factory>","2020-07-01 18:10:28","1625","0","2","62688027","<p>I create a sample to test and hope this can help you:</p>
<p><a href=""https://i.stack.imgur.com/UGM83.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UGM83.png"" alt=""total"" /></a></p>
<p>I set a pipeline variable <code>LastRunDate</code> type is <code>string</code>,value is &quot;2020-07-01&quot; which like your output of <code>lookup activity</code>.</p>
<p>Here is the output of my <code>GetMetaData Activity</code>:</p>
<pre><code>{
    &quot;childItems&quot;: [
        {
            &quot;name&quot;: &quot;polo_2020-06-30.csv&quot;,
            &quot;type&quot;: &quot;File&quot;
        },
        {
            &quot;name&quot;: &quot;polo_2020-07-01.csv&quot;,
            &quot;type&quot;: &quot;File&quot;
        },
        {
            &quot;name&quot;: &quot;polo_2020-07-02.csv&quot;,
            &quot;type&quot;: &quot;File&quot;
        }
    ]
}
</code></pre>
<p>The setting of <code>Filter Activity</code>:</p>
<p>&quot;items&quot;:<code>@activity('Get Metadata1').output.childItems</code></p>
<p>&quot;Condition&quot;:<code>@greater(split(split(item().name,'_')[1],'.')[0],variables('LastRunDate'))</code></p>
<p>Here is the output of <code>Filter Activity</code>:</p>
<pre><code>{
    &quot;ItemsCount&quot;: 3,
    &quot;FilteredItemsCount&quot;: 1,
    &quot;Value&quot;: [
        {
            &quot;name&quot;: &quot;polo_2020-07-02.csv&quot;,
            &quot;type&quot;: &quot;File&quot;
        }
    ]
}
</code></pre>
"
"62682936","File filtering in data factory based on last successful run date","<p>I have files in SFTP that get uploaded every week. The files have a name with the date appended to it. I want to copy only the newest files every week into adls. Whenever the job is completed successfully the date is stored in a sql table so I can look up the last successful run date.</p>
<p>In my mind I think I need something like this: filter files whose name contains a date greater than or equal to last successful run date.... or filter files whose last modified date is greater or equal to last successful run date?</p>
<p>This way if the job failed on the last run it will grab the previous weeks and the current weeks files.</p>
<p>Currently my pipeline has a lookup that can show me the last successful run, get meta data that shows all the files in the sftp folder, filter, and for each(copy). I know the copy works because I set the filter for a specific file and it worked. The area I need help with is the filter.</p>
","<azure><azure-pipelines><azure-data-factory>","2020-07-01 18:10:28","1625","0","2","62703140","<p>I ended up solving my problem by using using a tumbling window trigger to plug in the last successful run date and current date variables into the last modified parameters in the get metadata activity. I used <a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"" rel=""nofollow noreferrer"">this</a> as a guide</p>
"
"62682926","Possible to use Data Factory to extract all Azure Active Directory users?","<p>Microsoft has the tutorial showing how to use Data Factory to extract Office 365 data, but that seems to only extract Outlook email information?</p>
<p>Is there a way to use Data Factory (and a tutorial hopefully) to connect to Azure AD and extract all the Active Directory users?  Microsoft Graph API has the commands to do that, but I wasn't clear if that was the only way or if Data Factory can connect to it directly (like the O365 connector it has)?</p>
","<azure-active-directory><azure-data-factory>","2020-07-01 18:10:00","5216","1","2","62691578","<blockquote>
<p>Microsoft has the tutorial showing how to use Data Factory to extract Office 365 data, but that seems to only extract Outlook email information?</p>
</blockquote>
<p>According to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-office-365"" rel=""nofollow noreferrer"">Microsoft documentation</a>,not only email information,there are many other information such as address book contacts, calendar events, user information, mailbox settings, and so on.</p>
<p>So you can get user information which contains <code>aboutMe</code>,<code>companyName</code>,etc. when you choose <code>BasicDataSet_v0.User_v1</code> in dataset.(All properties you can get,please refer to <a href=""https://learn.microsoft.com/en-us/graph/api/resources/user?view=graph-rest-1.0"" rel=""nofollow noreferrer"">this documentation</a>)</p>
<p>By the way,your tenant admin need to opt-in to Microsoft Graph data connect if you do this.And there is no AAD connector.</p>
<p><a href=""https://i.stack.imgur.com/dcWfV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dcWfV.png"" alt=""officedataset"" /></a></p>
<p>Hope this can help you.</p>
"
"62682926","Possible to use Data Factory to extract all Azure Active Directory users?","<p>Microsoft has the tutorial showing how to use Data Factory to extract Office 365 data, but that seems to only extract Outlook email information?</p>
<p>Is there a way to use Data Factory (and a tutorial hopefully) to connect to Azure AD and extract all the Active Directory users?  Microsoft Graph API has the commands to do that, but I wasn't clear if that was the only way or if Data Factory can connect to it directly (like the O365 connector it has)?</p>
","<azure-active-directory><azure-data-factory>","2020-07-01 18:10:00","5216","1","2","62724353","<p>I used Azure Logic Apps and got through Azure AD group (get members) then used Office 365 Connector to Get Manager there is also another option get direct reports. If you are using Logic apps please make sure to turn the pagination on so you get all users.</p>
<p>This is were I started but once I get going I was able to figure it out myself
<a href=""https://eax360.com/query-azure-active-directory-using-graph-api-logic-apps/"" rel=""nofollow noreferrer"">https://eax360.com/query-azure-active-directory-using-graph-api-logic-apps/</a></p>
"
"62681146","TTL configuration and billing in Azure Integration Runtime","<p>Doing some tests, I could see that having an Azure Integration Runtime (AIR) allowed us to reduce considerably the amount of time required to finish a pipeline.</p>
<p>To fully understand the use of this configuration and its billing as well, I've got these questions. Let's assume I've got two independent pipelines, all of their Data Flow activities use the same AIR with a TTL = 10 minutes.</p>
<p>The first pipeline takes 7 minutes to finish. The billing will be (if I understand well):</p>
<p>billing: time to acquire cluster + job execution time + TTL  (7 + 10)</p>
<p>Five minutes later, I trigger the second pipeline.  It will take only 3 minutes to finish (I understand it will also use the same pool as the first one). After it concludes, the TTL is  setting up to 10 minutes again or is equal to 2 minutes
10 - 5 -3 (original TTL - start time second pipe - runtime second pipe), in this case, what will happen if I trigger a third pipeline that could take more than 2 minutes?
What about the billing, how is it going to be calculated?</p>
<p>Regards.</p>
","<azure><azure-data-factory>","2020-07-01 16:20:41","385","0","1","62681920","<p>Look at the ADF pipeline monitoring view and find all of your data flow activity executions.</p>
<p>Add up that total data flow activity execution time.</p>
<p>Now add the TTL value for that Azure IR you were using to that total.</p>
<p>That is the total time you will be billed.</p>
"
"62677621","Set-AzDataFactoryV2 Invalid parameter","<p>I want to create a datafactory and link it to an azure devops repository with the command line Set-AzDataFactoryV2.</p>
<p>So I installed the following module: <strong>Az.DataFactory</strong> (1.8.2)</p>
<p>As the microsoft documentation mentionned it (<a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/Set-AzDataFactoryV2?view=azps-4.3.0"" rel=""nofollow noreferrer"">click here</a>), it should possible with the following commands:</p>
<pre><code>Get-AzDataFactoryV2 -ResourceGroupName &quot;ADF&quot; -Name &quot;WikiADF&quot; | Set-AzDataFactoryV2 -AccountName msdata -RepositoryName ADFRepo -CollaborationBranch master -RootFolder / -ProjectName &quot;Azure Data Factory&quot;
</code></pre>
<p>OR</p>
<pre><code>New-AzDataFactoryV2 -ResourceGroupName &quot;ADF&quot; -Name &quot;WikiADF&quot; -Location 'EastUS' -HostName 'https://github.com' -AccountName msdata -RepositoryName ADFRepo -CollaborationBranch master -RootFolder /
</code></pre>
<p>But the result is</p>
<pre><code>+ CategoryInfo          : InvalidArgument : (:) [Set-AzDataFactoryV2], ParameterBindingException
    + FullyQualifiedErrorId : NamedParameterNotFound,Microsoft.Azure.Commands.DataFactoryV2.SetAzureDataFactoryCommand
</code></pre>
<p>What is the problem ?
Is it possible to create a datafactory and link it to a repository with a command line with other ways (without API REST) ?</p>
","<azure><azure-powershell><azure-data-factory>","2020-07-01 13:09:58","241","0","1","62708028","<p>I have tested the two commands you shared in this post with module: Az.DataFactory (1.8.2) and both of them work fine. I only modify the resource group name and data factory name.</p>
<p>Based on the error message you provided, it means one the the parameters is invalid.</p>
<p>For example, if I modify the parameter <code>-Name</code> to <code>-Namettt</code>:</p>
<pre><code>New-AzDataFactoryV2 -ResourceGroupName &quot;ADF&quot; -Namettt &quot;WikiADF&quot; -Location 'EastUS' -HostName 'https://github.com' -AccountName msdata -RepositoryName ADFRepo -CollaborationBranch master -RootFolder /
</code></pre>
<p>I will get the same error as yours.</p>
<p>Please check if you have all the parameters corrected.</p>
"
"62673501","Azure Data Factory Scheduled Triggers Not Working","<p>I tried to setup scheduled triggers within Azure Data Factory, but they are not getting executed and are not logged either. They used to work but since a couple of days nothing happens anymore, even when I delete them and setup new ones.</p>
<p>When I hit &quot;trigger now&quot;, everything is fine.
So basically I setup scheduled triggers (activate them), deploy them and wait for the automatic call but nothing happens at all.</p>
<p>Do you have an idea?</p>
<p><a href=""https://i.stack.imgur.com/lsRtM.png"" rel=""nofollow noreferrer"">lot of configured triggers</a>
<a href=""https://i.stack.imgur.com/k2S95.png"" rel=""nofollow noreferrer"">emtpy trigger execution logging</a></p>
<p>eg:</p>
<pre><code>{
&quot;name&quot;: &quot;Trigger_Hourly3&quot;,
&quot;properties&quot;: {
    &quot;annotations&quot;: [],
    &quot;runtimeState&quot;: &quot;Started&quot;,
    &quot;pipelines&quot;: [
        {
            &quot;pipelineReference&quot;: {
                &quot;referenceName&quot;: &quot;Pipe_02_xy&quot;,
                &quot;type&quot;: &quot;PipelineReference&quot;
            }
        }
    ],
    &quot;type&quot;: &quot;ScheduleTrigger&quot;,
    &quot;typeProperties&quot;: {
        &quot;recurrence&quot;: {
            &quot;frequency&quot;: &quot;Hour&quot;,
            &quot;interval&quot;: 1,
            &quot;startTime&quot;: &quot;2020-06-30T13:43:00.000Z&quot;,
            &quot;timeZone&quot;: &quot;UTC&quot;
        }
    }
}
</code></pre>
<p>}</p>
<p>Thanks, Lukas</p>
","<triggers><scheduler><azure-data-factory>","2020-07-01 09:12:31","2573","3","1","62727242","<p>Resolution: Fixed an error that was raised during Azure DevOps deployment that was not raised by ADF validation and not sent back as error to ADF UI. Then the triggers were activated again in the following task and everything runs smoothly again.</p>
"
"62669749","Getting error exceeded the maximum length of 128 while copying some text files from azure blob to synapse","<p>I am getting the following error upon trying to copy text files from Azure Blob to Synapse:</p>
<pre><code>*{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Source' side. 'Type=System.Data.SqlClient.SqlException,Message=Parse Error: Identifier 'ARCHIVED_AT,ID,DISPENSATION_ID,ETL_RUN_TIMESTAMP' exceeded the maximum length of 128.,Source=.Net SqlClient Data Provider,SqlErrorNumber=104307,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=104307,State=1,Message=Parse Error: Identifier 'ARCHIVED_AT,ID,DISPENSATION_ID,ETL_RUN_TIMESTAMP' exceeded the maximum length of 128.,},],'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy Blob to Synapse&quot;,
    &quot;details&quot;: []
}*
</code></pre>
<p>While I have been able to successfully copy most text files from Azure blob to Azure Synapse using ADF pipeline, I am not sure what is going wrong with this one.</p>
<p>Can anyone help me figure what I need to do to resolve this?</p>
","<azure-data-factory>","2020-07-01 04:32:24","1021","0","2","62709289","<p>I agree with @Joel Cochran. It seams that the header <code>ARCHIVED_AT,ID,DISPENSATION_ID,ETL_RUN_TIMESTAMP</code> is recognized as one column.</p>
<p>I guess your txt file should has four columns like bellow format:</p>
<pre><code>ARCHIVED_AT, ID, DISPENSATION_ID, ETL_RUN_TIMESTAMP
data1,1,100,2020-07-02 05:51:32.910
data2,2,102,2020-07-03 05:51:32.910
···
···
</code></pre>
<p>Please check the row delimiter of you source txt file that if it is <code>comma(,)</code> :
<a href=""https://i.stack.imgur.com/GWYkD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GWYkD.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps.</p>
"
"62669749","Getting error exceeded the maximum length of 128 while copying some text files from azure blob to synapse","<p>I am getting the following error upon trying to copy text files from Azure Blob to Synapse:</p>
<pre><code>*{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Source' side. 'Type=System.Data.SqlClient.SqlException,Message=Parse Error: Identifier 'ARCHIVED_AT,ID,DISPENSATION_ID,ETL_RUN_TIMESTAMP' exceeded the maximum length of 128.,Source=.Net SqlClient Data Provider,SqlErrorNumber=104307,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=104307,State=1,Message=Parse Error: Identifier 'ARCHIVED_AT,ID,DISPENSATION_ID,ETL_RUN_TIMESTAMP' exceeded the maximum length of 128.,},],'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy Blob to Synapse&quot;,
    &quot;details&quot;: []
}*
</code></pre>
<p>While I have been able to successfully copy most text files from Azure blob to Azure Synapse using ADF pipeline, I am not sure what is going wrong with this one.</p>
<p>Can anyone help me figure what I need to do to resolve this?</p>
","<azure-data-factory>","2020-07-01 04:32:24","1021","0","2","62735147","<p>I was using a txt file with pipe delimiter and not a csv file.
There were double quotes in multiple columns of my table as well as hidden characters which was causing the issue. It got resolved after I got them replaced at source.</p>
"
"62668525","Azure Data Factory copy nested JSON to SQL table","<p>Does anyone have an easy way to convert nested JSON to a flat SQL table?  Just want to repeat the higher level data on each of the lower level detail.  It looks like it can be done in mapping, I have tried as per the MS documentation but got a table full of NULL. Here is what I have tried and the result.
<a href=""https://i.stack.imgur.com/M8e2i.png"" rel=""nofollow noreferrer"">json</a></p>
<p>Option 1
Result:  Only returns the first record of the ‘assignedLicences’
<a href=""https://i.stack.imgur.com/rlGI3.png"" rel=""nofollow noreferrer"">Option1</a></p>
<p>Option 2:
Returns multiple ‘assignedLicenses’ for each user, but only returns the first user id in each page.
<a href=""https://i.stack.imgur.com/Ie7dJ.png"" rel=""nofollow noreferrer"">Option2</a></p>
<p>Option3: as per the MS documentation
Result: returns all NULL values
<a href=""https://i.stack.imgur.com/35Y9g.png"" rel=""nofollow noreferrer"">Option3</a></p>
","<json><sql-server><azure-data-factory>","2020-07-01 01:54:38","5189","1","1","62673041","<p>You can have a try:</p>
<p>1.click <code>import schemas</code> button</p>
<p>2.if you have a JsonArray,select it.</p>
<p>3.you can directly see and edit the fields' JSON paths by opening <code>Advanced editor </code>.</p>
<p><a href=""https://i.stack.imgur.com/BqIQM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BqIQM.png"" alt=""mapping"" /></a></p>
<p>Here is a Microsoft documentation about it.Please refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#tabularhierarchical-source-to-hierarchical-sink"" rel=""nofollow noreferrer"">this</a>.</p>
<p>Hope this can help you.</p>
"
"62665505","Parameterize Integration Runtime in linked services of Azure data factory","<p>I have configured CI/CD pipelines for Azure Data Factory. I need to have a separate Integration Runtime for some linked services in Azure data factory for QA environment. When I deploy using the ARM templates of DEV Azure Data Factory from adf_publish branch, I am able to provide values for the parameter for only sql server name, key vault not IR. Is there any way I would be able to provide value of Integration Runtime in the linked service.
Thanks in advance</p>
<p><a href=""https://i.stack.imgur.com/qdaoH.jpg"" rel=""nofollow noreferrer"">Please click here to see the screenshot</a></p>
","<devops><azure-data-factory>","2020-06-30 20:16:10","2869","2","3","62672181","<p>You can go to the 'Manage' of your ADF instance and then go to 'Integration runtimes' to create a new Azure IR for your ADF.</p>
<p><a href=""https://i.stack.imgur.com/efznV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/efznV.png"" alt=""enter image description here"" /></a></p>
<p>You can create an azure IR based on azure or self hosted.</p>
<p>For more information, please have a look of these document:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime</a></p>
"
"62665505","Parameterize Integration Runtime in linked services of Azure data factory","<p>I have configured CI/CD pipelines for Azure Data Factory. I need to have a separate Integration Runtime for some linked services in Azure data factory for QA environment. When I deploy using the ARM templates of DEV Azure Data Factory from adf_publish branch, I am able to provide values for the parameter for only sql server name, key vault not IR. Is there any way I would be able to provide value of Integration Runtime in the linked service.
Thanks in advance</p>
<p><a href=""https://i.stack.imgur.com/qdaoH.jpg"" rel=""nofollow noreferrer"">Please click here to see the screenshot</a></p>
","<devops><azure-data-factory>","2020-06-30 20:16:10","2869","2","3","62756983","<p>This can be achieved by following the below link from Microsoft  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a></p>
<p>A new name can be added under the integration runtime arm template.</p>
"
"62665505","Parameterize Integration Runtime in linked services of Azure data factory","<p>I have configured CI/CD pipelines for Azure Data Factory. I need to have a separate Integration Runtime for some linked services in Azure data factory for QA environment. When I deploy using the ARM templates of DEV Azure Data Factory from adf_publish branch, I am able to provide values for the parameter for only sql server name, key vault not IR. Is there any way I would be able to provide value of Integration Runtime in the linked service.
Thanks in advance</p>
<p><a href=""https://i.stack.imgur.com/qdaoH.jpg"" rel=""nofollow noreferrer"">Please click here to see the screenshot</a></p>
","<devops><azure-data-factory>","2020-06-30 20:16:10","2869","2","3","66015839","<p>Use connectVia property of linkedServices in the arm-template-paramerters-definition.json to change integration runtime from CI/CD</p>
<pre><code>&quot;Microsoft.DataFactory/factories/linkedServices&quot;: {
        &quot;*&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;baseUrl&quot;: &quot;-::string&quot;
                                  },
                &quot;connectVia&quot;: {
                    &quot;*&quot;: &quot;=&quot;
                              }
                         }
             }
   }
</code></pre>
"
"62662241","Can I split a column text as array using data factory data flow?","<p>Inside my data flow pipeline I would like to add a derived column and its datatype is array. I would like to split the existing column with 1000 characters without breaking words. I think we can use regexSplit,</p>
<pre><code>regexSplit(&lt;string to split&gt; : string, &lt;regex expression&gt; : string) =&gt; array
</code></pre>
<p>But I do not know which regular expression I can use for split the existing column without breaking words.
Please help me to figure it out.</p>
","<regex><azure><azure-data-factory>","2020-06-30 16:42:33","3524","3","2","62748465","<p>I wouldn't use a regex for this, but a truncating function like this one, <a href=""https://stackoverflow.com/a/1614090/119129"">courtesy of TimS</a>:</p>
<pre class=""lang-cs prettyprint-override""><code>public static string TruncateAtWord(this string input, int length)
{
    if (input == null || input.Length &lt; length)
        return input;
    int iNextSpace = input.LastIndexOf(&quot; &quot;, length, StringComparison.Ordinal);
    return string.Format(&quot;{0}…&quot;, input.Substring(0, (iNextSpace &gt; 0) ? iNextSpace : length).Trim());
}
</code></pre>
<p>Translated into expression functions it would look something* like this.</p>
<pre><code>substring(Input, 1, iif(locate(Input, ' ', 1000) &gt; 0, locate(Input, ' ', 1000) , length(Input)) )
</code></pre>
<p>Since you don't have a <code>lastIndexOf</code> available as an expression function, you would have to default to <code>locate</code>, which means that this expression truncates the string at the first space <strong>after</strong> the 1000th character.</p>
<p>*I don't have an environment where I can test this.</p>
"
"62662241","Can I split a column text as array using data factory data flow?","<p>Inside my data flow pipeline I would like to add a derived column and its datatype is array. I would like to split the existing column with 1000 characters without breaking words. I think we can use regexSplit,</p>
<pre><code>regexSplit(&lt;string to split&gt; : string, &lt;regex expression&gt; : string) =&gt; array
</code></pre>
<p>But I do not know which regular expression I can use for split the existing column without breaking words.
Please help me to figure it out.</p>
","<regex><azure><azure-data-factory>","2020-06-30 16:42:33","3524","3","2","62780464","<p>I created a workaround for this and it works fine for me.</p>
<pre><code>filter(split(regexReplace(regexReplace(text, `[\t\n\r]`, ``), `(.{1,1000})(?:\s|$)`, `$1~~`), '~~'), #item !=&quot;&quot;)
</code></pre>
<p>I think, we have a better solution than this.</p>
"
"62653168","Avoid duplicates during parallel job run in Azure Synapse","<p>Need your suggestions in developing code in Azure Synapse.</p>
<p>We have a requirement where our jobs will run in parallel at same time and insert data to the same table.
During this insert there are changes that duplicate entries will be inserted to the same table.
For Example: If Job A and Job B run at same time both with same values then &quot;not exists&quot; or &quot;not in&quot; will fail to work. In this case I will get duplicates from both the job. Primary key or Unique constraint allows duplicates in Azure synapse. Is there any best way to lock tables during data insert. Like if Job A is running then JOB B should not insert the data to same table. Please pour your suggestions as I am new to this. Note: We use stored Procedure to load the data through ADF V2</p>
<p>Thanks,
Nandini</p>
","<azure><azure-data-factory><azure-synapse>","2020-06-30 08:22:24","1225","2","2","63236049","<p>Duplicates must be handled within jobs before inserting data into Azure Synapse. If the duplicates exists between two jobs, then do it after completion of both jobs. It depends really how you are loading data. You can easily manage by creating a temp table instead of directly loading data to final table. Please make sure the structure of temp table should be same as final table (Distribution, Partition, constraints, nullability of the columns)  You can use SQL BCP/INSERT TO/CTAS/CTAS with partition switching with stage table to final table.</p>
<p>If you can share specific scenario, it will be helpful to give suggestions relevant to your use case.</p>
"
"62653168","Avoid duplicates during parallel job run in Azure Synapse","<p>Need your suggestions in developing code in Azure Synapse.</p>
<p>We have a requirement where our jobs will run in parallel at same time and insert data to the same table.
During this insert there are changes that duplicate entries will be inserted to the same table.
For Example: If Job A and Job B run at same time both with same values then &quot;not exists&quot; or &quot;not in&quot; will fail to work. In this case I will get duplicates from both the job. Primary key or Unique constraint allows duplicates in Azure synapse. Is there any best way to lock tables during data insert. Like if Job A is running then JOB B should not insert the data to same table. Please pour your suggestions as I am new to this. Note: We use stored Procedure to load the data through ADF V2</p>
<p>Thanks,
Nandini</p>
","<azure><azure-data-factory><azure-synapse>","2020-06-30 08:22:24","1225","2","2","69562446","<p>I just got the same case and I solved it with <strong>Pipeline Runs - Query By Factory</strong></p>
<ol>
<li>Use a <code>Until</code> activity before the <code>DataFlow</code> activity that writes the values in the table with this expression <code>@equals(activity('pingPL').output.value[0].runId, pipeline().RunId)</code> as follow:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/B7CXF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B7CXF.png"" alt="""" /></a></p>
<ol start=""2"">
<li>Into the <code>Until</code> activities put a web activity and a wait time:</li>
</ol>
<p><a href=""https://i.stack.imgur.com/nRFr1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nRFr1.png"" alt="""" /></a></p>
<p>a. <code>Web</code> activity body - follow <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipeline-runs/query-by-factory#runqueryfilteroperand"" rel=""nofollow noreferrer"">docs</a>:</p>
<pre><code>{
  &quot;lastUpdatedAfter&quot;: &quot;@{addminutes(utcnow(), -30)}&quot;,
  &quot;lastUpdatedBefore&quot;: &quot;@{utcnow()}&quot;,
  &quot;filters&quot;: [
    {
      &quot;operand&quot;: &quot;PipelineName&quot;,
      &quot;operator&quot;: &quot;Equals&quot;,
      &quot;values&quot;: [
        &quot;pipeline_name_where_writeInSynapse_is_located&quot;
      ]
    },
    {
      &quot;operand&quot;: &quot;Status&quot;,
      &quot;operator&quot;: &quot;Equals&quot;,
      &quot;values&quot;: [
        &quot;InProgress&quot;
      ]
    }
  ]
}
</code></pre>
<p>b. <code>Wait</code> activity 30 sec or whatever make sense</p>
<p>What is happening is, if you trigger several times the same pipeline in parallel the web activity is going to filter each PL status <code>InProgress</code>. It will look like this:</p>
<pre><code>{
    &quot;value&quot;: [
        {
            &quot;id&quot;: &quot;...&quot;,
            &quot;runId&quot;: &quot;52004775-5ef5-493b-8a44-ee3fff6bff7b&quot;,
            &quot;debugRunId&quot;: null,
            &quot;runGroupId&quot;: &quot;52004775-5ef5-493b-8a44-ee3fff6bff7b&quot;,
            &quot;pipelineName&quot;: &quot;synapse_writting&quot;,
            &quot;parameters&quot;: {
                &quot;region&quot;: &quot;NW&quot;,
                &quot;unique_item&quot;: &quot;a&quot;
            },
            &quot;invokedBy&quot;: {
                &quot;id&quot;: &quot;80efce4dbda74636878bc99472978ccf&quot;,
                &quot;name&quot;: &quot;Manual&quot;,
                &quot;invokedByType&quot;: &quot;Manual&quot;
            },
            &quot;runStart&quot;: &quot;2021-10-13T17:24:01.0210945Z&quot;,
            &quot;runEnd&quot;: &quot;2021-10-13T17:25:06.9692394Z&quot;,
            &quot;durationInMs&quot;: 65948,
            &quot;status&quot;: &quot;InProgress&quot;,
            &quot;message&quot;: &quot;&quot;,
            &quot;output&quot;: null,
            &quot;lastUpdated&quot;: &quot;2021-10-13T17:25:06.9704432Z&quot;,
            &quot;annotations&quot;: [],
            &quot;runDimension&quot;: {},
            &quot;isLatest&quot;: true
        },
        {
            &quot;id&quot;: &quot;...&quot;,
            &quot;runId&quot;: &quot;cf3f5038-ba10-44c3-b8f5-df8ad4c85819&quot;,
            &quot;debugRunId&quot;: null,
            &quot;runGroupId&quot;: &quot;cf3f5038-ba10-44c3-b8f5-df8ad4c85819&quot;,
            &quot;pipelineName&quot;: &quot;synapse_writting&quot;,
            &quot;parameters&quot;: {
                &quot;region&quot;: &quot;NW&quot;,
                &quot;unique_item&quot;: &quot;a&quot;
            },
            &quot;invokedBy&quot;: {
                &quot;id&quot;: &quot;08205e0eda0b41f6b5a90a8dda06a7f6&quot;,
                &quot;name&quot;: &quot;Manual&quot;,
                &quot;invokedByType&quot;: &quot;Manual&quot;
            },
            &quot;runStart&quot;: &quot;2021-10-13T17:28:58.219611Z&quot;,
            &quot;runEnd&quot;: null,
            &quot;durationInMs&quot;: null,
            &quot;status&quot;: &quot;InProgress&quot;,
            &quot;message&quot;: &quot;&quot;,
            &quot;output&quot;: null,
            &quot;lastUpdated&quot;: &quot;2021-10-13T17:29:00.9860175Z&quot;,
            &quot;annotations&quot;: [],
            &quot;runDimension&quot;: {},
            &quot;isLatest&quot;: true
        }
    ],
    &quot;ADFWebActivityResponseHeaders&quot;: {
        &quot;Pragma&quot;: &quot;no-cache&quot;,
        &quot;Strict-Transport-Security&quot;: &quot;max-age=31536000; includeSubDomains&quot;,
        &quot;X-Content-Type-Options&quot;: &quot;nosniff&quot;,
        &quot;x-ms-ratelimit-remaining-subscription-reads&quot;: &quot;11999&quot;,
        &quot;x-ms-request-id&quot;: &quot;188508ef-8897-4c21-8c37-ccdd4adc6d81&quot;,
        &quot;x-ms-correlation-request-id&quot;: &quot;188508ef-8897-4c21-8c37-ccdd4adc6d81&quot;,
        &quot;x-ms-routing-request-id&quot;: &quot;WESTUS2:20211013T172902Z:188508ef-8897-4c21-8c37-ccdd4adc6d81&quot;,
        &quot;Cache-Control&quot;: &quot;no-cache&quot;,
        &quot;Date&quot;: &quot;Wed, 13 Oct 2021 17:29:02 GMT&quot;,
        &quot;Server&quot;: &quot;Microsoft-IIS/10.0&quot;,
        &quot;X-Powered-By&quot;: &quot;ASP.NET&quot;,
        &quot;Content-Length&quot;: &quot;1492&quot;,
        &quot;Content-Type&quot;: &quot;application/json; charset=utf-8&quot;,
        &quot;Expires&quot;: &quot;-1&quot;
    },
    &quot;effectiveIntegrationRuntime&quot;: &quot;NCAP-Simple-DataMovement (West US 2)&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;ExternalActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p>Then the <code>Until</code> expression will evaluate if the first <code>value[0]</code> has <code>runId == pipeline_runid</code> to stop the until activity and run the <code>dataflow</code> that writes in Synapse. Once the PL ends the status will be <code>Succeeded</code> and the <code>Web</code> activity in another job will get the next <code>value[0]</code> with the status <code>InProgress</code> and continue with the next write. This creates a dependency to the parallel jobs to wait until the dataflow validates and writes in table if need it.</p>
"
"62650855","Azure data factory V2 copy data issue - error code: 2200 An item with the same key has already been added","<p>I am trying to copy data from a csv file into the Azure sql but I am getting an unique error only during the deployment of pipeline. I am using a normal copy data</p>
<blockquote>
<p>{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The
value of the property '' is invalid: 'An item with the same key has
already been
added.'.,Source=,''Type=System.ArgumentException,Message=An item with
the same key has already been added.,Source=mscorlib,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;Copy data1&quot;,
&quot;details&quot;: [] }</p>
</blockquote>
<p>Kindly help to solve</p>
","<sql><sql-server><azure><azure-data-factory>","2020-06-30 05:31:09","1498","1","1","64474867","<p>The above error is due to a duplicate column/header in the csv file. The exception message states <em>An item with the same key has already been added</em> that is the duplicate column might be getting added to a dictionary and the exception has been thrown.</p>
"
"62650634","Azure Data Factory: Wrong and unexpected Datatype conversion error during import from csv to sql server via pipeline","<p>I am trying to load data from a csv to a SQL server database using an Azure pipeline copy data operator.It throws following error during trigger the pipeline.
In CSV file i have one date column (StatusDate) with null values when i am importing data in MS-SQL Data base table is having StatusDate is a date time column.</p>
<p>Error msg[![Source CSV File<a href=""https://i.stack.imgur.com/xoNr6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xoNr6.png"" alt=""][1][1]"" /></a>: &quot;Operation on target Copy data1 failed: Failure happened on 'Source' side. 'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed. Please search error to get more details.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=Microsoft.Azure.Data.Governance.Plugins.Core.TypeConversionException,Message=exception occurred when converting data from column name 'StatusDate', type 'String', value 'null' to type 'DateTime': The string was not recognized as a valid DateTime. There is an unknown word starting at index 0.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.FormatException,Message=The string was not recognized as a valid DateTime. There is an unknown word starting at index 0.,Source=mscorlib,'&quot;</p>
<p>Destination Table Structure:</p>
<pre><code>create table RepsNoLongerInDiscovery(
[CRD]   [varchar](11)   ,
[First] [varchar](50)   ,
[Middle]    [varchar](50)   ,
[Last]  [varchar](50)   ,
[Suffix]    [varchar](50)   ,
[Status]    [varchar](20)   ,
[StatusDate]    [datetime]  ) 
</code></pre>
","<azure><azure-active-directory><azure-sql-database><azure-data-factory>","2020-06-30 05:07:30","961","0","1","62669040","<p>I tested with copy active and it works well.</p>
<p>My dataset:</p>
<p><a href=""https://i.stack.imgur.com/SEzwJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SEzwJ.png"" alt=""enter image description here"" /></a></p>
<p>Bellow is my Source:</p>
<p><a href=""https://i.stack.imgur.com/JBhMU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JBhMU.png"" alt=""enter image description here"" /></a></p>
<p><strong>Note: we must specify the column data type as <code>DateTime</code> in Mapping settings</strong>:
<a href=""https://i.stack.imgur.com/2oMd2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2oMd2.png"" alt=""enter image description here"" /></a></p>
<p>Then run the pipeline and check the data in Sink table:
<a href=""https://i.stack.imgur.com/ffjg6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ffjg6.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps.</p>
"
"62647202","Capture output in Azure Data Factory WebActivity","<p>We have a Pipeline that invokes an API endpoint using WebActivity to start a long-running job.
As per specification, the API starts the job in the background and returns <code>HTTP Status Code 202</code> along with a json object like this:</p>
<pre><code>{
    &quot;TransactionID&quot;: &quot;534b1867-a5ac-41b3-9a57-xxx&quot;,
    &quot;TransactionStatus&quot;: &quot;Processing&quot;
}
</code></pre>
<p>Then the Pipeline waits until the status of that <code>TransactionID</code> changed to <code>Completed</code> using a combo of another <code>WebActivity</code> and <code>WaitActivity</code> in a <code>Loop</code> activity.</p>
<p>This pipeline <strong>was working correctly until today (June, 29th 2020)</strong> but now the json output of the WebActivity is not captured anymore by ADF.
<em>Before anyone asks: we have not changed anything in the Pipeline or in the API itself, it just stopped working.</em></p>
<p>My question is: why Azure Data Factory is not capturing anymore the output of web activities when the status code is not <code>200</code>? Were we doing somethign wrong and now Azure fixed it or has Azure changed the behaviour of WebActivity without notice?</p>
","<azure-data-factory>","2020-06-29 21:49:25","141","0","1","62665522","<p>Not surprisingly, Azure Data Factory changed behavior today (June 30) and all pipelines started working again.</p>
<p>If you came here to see a solution, there isn't any. It solved by itself.</p>
<p>I will keep this post in case it happens again so we can discuss and update accordingly.</p>
"
"62643980","MashupValueException when creating a Web Table Linked Service on ADF?","<p>I'm experiencing a MashupValueException while creating a Web Table Linked Service and I wonder if someone here have come across the same issue.</p>
<p><img src=""https://i.stack.imgur.com/TqzsG.png"" alt=""A print of the error message can be found in this pic"" /></p>
<p>The config of the Linked Service is as follows:</p>
<pre><code>{
    &quot;name&quot;: &quot;WikipediaLS&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;Web&quot;,
        &quot;typeProperties&quot;: {
            &quot;url&quot;: &quot;https://www.the-numbers.com/&quot;,
            &quot;authenticationType&quot;: &quot;Anonymous&quot;
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;MySelfHostedIR&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}
</code></pre>
<p>The error happens after filling the info on the form and clicking on &quot;Test Connection&quot;. Then the following error pops up:</p>
<pre><code>Connection failed
'Type=Microsoft.Data.Mashup.MashupValueException,Message=The supplied URL must be a valid 'http:' or 'https:' URL.,Source=Microsoft.Data.Mashup.ProviderCommon,' Activity ID: c8d924b9-cc81-49c1-85c8-70abfa85c859.
</code></pre>
<p>I have tested the extraction of the web table using Office 2016 and it works all right.</p>
<p>The aim is to extract the index 0 table from &quot;https://www.the-numbers.com/movie/budgets/all&quot;.</p>
","<azure-data-factory>","2020-06-29 17:53:43","132","0","1","62671627","<p>This is what you are facing:</p>
<p><a href=""https://i.stack.imgur.com/xeM8I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xeM8I.png"" alt=""enter image description here"" /></a></p>
<p>You can try to use below url, it can connect to the web successfully on my side:</p>
<pre><code>https://www.the-numbers.com/movie/budgets/all
</code></pre>
<p><a href=""https://i.stack.imgur.com/RfQZv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RfQZv.png"" alt=""enter image description here"" /></a></p>
<p>The website you give seems not stable. Sometimes you cannot visit the database.</p>
"
"62642801","What is the best way to load data from SOAP APIs in Azure Data Factory?","<p>Have to load data from soap wsdl and store in Azure Blob storage. How can we do in Azure Data Factory</p>
","<azure><azure-functions><azure-storage><azure-data-factory>","2020-06-29 16:40:24","1799","1","1","62649166","<p>SOAP is based on HTTP protocol. so you can use HTTP as the source dataset in Azure Datafactory Copy Activity to acheive what you want.</p>
<p>Please check this document for specific steps:</p>
<p><a href=""https://medium.com/@gabrielsribe/consuming-a-soap-service-using-azure-data-factory-copy-data-activity-a4a3332cc4c"" rel=""nofollow noreferrer"">https://medium.com/@gabrielsribe/consuming-a-soap-service-using-azure-data-factory-copy-data-activity-a4a3332cc4c</a></p>
"
"62639824","ADF copy task field type boolean as lowercase","<p>In ADF i have a copy task that copies data from JSON to Delimited text, i get the result as</p>
<pre><code>A | B | C
&quot;name&quot;|False|&quot;description&quot;
</code></pre>
<p>Json record is like</p>
<pre><code>{&quot;A&quot;:&quot;name&quot;,&quot;B&quot;:&quot;false&quot;,&quot;C&quot;:&quot;description&quot;}
</code></pre>
<p>Excepted result is as below</p>
<pre><code>A | B | C
&quot;name&quot;|false|&quot;description&quot;
</code></pre>
<p><a href=""https://i.stack.imgur.com/hWLyo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hWLyo.png"" alt=""enter image description here"" /></a></p>
<p>The bool value have to be in lowercase in the resulting Delimited text file, what am i missing?</p>
","<azure-data-factory>","2020-06-29 14:00:58","1301","1","1","62641899","<p>I can reproduce this.  The reason is you are converting the string to the ADF dataytpe &quot;Boolean&quot; which for some reason renders the values in Proper case.</p>
<p>Do you really have a receiving process which is case-sensitive?  If you need to maintain the case of the source value simply remove the mapping, ie</p>
<p><a href=""https://i.stack.imgur.com/l7MOD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l7MOD.png"" alt=""ADF designer"" /></a></p>
<p>If you do need some kind of custom mapping, then simply change the mapping data type to <code>String</code> and not <code>Boolean</code>.</p>
<p><strong>UPDATE after new JSON provided</strong></p>
<p>OK, so your first json sample has the &quot;false&quot; value in quotes so is treated as a string. In your second example, your json &quot;true&quot; is not in quotes so is a genuine json boolean value. ADF is auto-detecting this at run time and it seems like it can not be over-ridden as far as I can tell. Happy to be corrected. As an alternative, consider altering your original json to a string, as per you original example OR copying the file to Blob Store or Azure Data Lake, runniing some transform on it (eg Databricks) and then outputting the file. Alternately consider Mapping Data Flows.</p>
"
"62638738","Retrieve key vault in web activity in Azure Data Factory","<p>I am new to Azure Data Factory and I am now stuck at last step to finish my current task.</p>
<p>There are few REST API's that I need to call from ADF using Web Activity but before I make to REST API POST call, in this POST call I need to pass user credentials to be fetched from key vault and passing into BODY section. Once this POST request is issued I need to get authentication token from that server, hence as it is working in hard coded fashion.
But now I want to fetch userId and password from Azure Key Vault and I then need to pass it to Web Activity.</p>
<p>NOTE : I do not want to use MSI option.</p>
<p>Could you please help on retrieving userId and password from Azure Key Vault for Web Activity to pass this as part of authentication?</p>
","<azure><azure-data-factory><azure-keyvault>","2020-06-29 13:01:06","3161","1","1","62651728","<p>You can follow these steps to achieve what you want:</p>
<ol>
<li>Save the userid and password in the azure keyvault. And then give your azure datafactory authority to access the keyvault:</li>
</ol>
<p>I think you know how to add keyvault value, so I just show how to give the access authority.
<a href=""https://i.stack.imgur.com/XlBvo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XlBvo.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/IAXtp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IAXtp.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/lKwWf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lKwWf.png"" alt=""enter image description here"" /></a></p>
<p>And choose 'Add'. And dont foeget to save the edit.</p>
<ol start=""2"">
<li>The second step is about the web activity in ADF. You should create a web activity first.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/ebCBB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ebCBB.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/QVO4Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QVO4Z.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/8LeMD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8LeMD.png"" alt=""enter image description here"" /></a></p>
<p>Then, you can use a Set variable activity to get the value.</p>
<p><a href=""https://i.stack.imgur.com/OsFsy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OsFsy.png"" alt=""enter image description here"" /></a></p>
<p>The value of the activity is <code>@activity('yourwebactivityname').output.value</code></p>
<p>This is my pipeline:</p>
<p><a href=""https://i.stack.imgur.com/qjXAm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qjXAm.png"" alt=""enter image description here"" /></a></p>
<p>And I can get the value:</p>
<p><a href=""https://i.stack.imgur.com/lJES3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lJES3.png"" alt=""enter image description here"" /></a></p>
<p>For more information, please have a look of the below document:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-use-azure-key-vault-secrets-pipeline-activities"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-use-azure-key-vault-secrets-pipeline-activities</a></p>
"
"62608553","Trigger Azure data factory pipeline - Blob upload ADLS Gen2 (programmatically)","<p>We are uploading files into Azure data lake storage using Azure SDK for java. After uploading a file, Azure data factory needs to be triggered. BLOB CREATED trigger is added in a pipeline.
Main problem is after each file upload it gets triggered twice.</p>
<p>To upload a file into ADLS gen2, azure provides different SDK than conventional Blobstorage.</p>
<p>SDK uses package - <code>azure-storage-file-datalake</code>.</p>
<p><code>DataLakeFileSystemClient</code> - to get container</p>
<p><code>DataLakeDirectoryClient.createFile</code> - to create a file. //this call may be raising blob created event</p>
<p><code>DataLakeFileClient.uploadFromFile</code> - to upload file //this call may also be raising blob created event</p>
<p>I think ADF trigger is not upgraded to capture Blob created event appropriately from ADLSGen2.</p>
<p>Any option to achieve this? There are restrictions in my org not to use Azure functions, otherwise Azure functions can be triggered based on Storage Queue message or Service bus message and ADF pipeline can be started using data factory REST API.</p>
","<azure><azure-data-factory><azure-data-lake-gen2>","2020-06-27 10:02:56","1266","0","1","62630308","<p>You could try <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview"" rel=""nofollow noreferrer"">Azure Logic Apps</a> with a blob trigger and a data factory action:
<a href=""https://i.stack.imgur.com/bC0qS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bC0qS.png"" alt=""enter image description here"" /></a></p>
<p>Trigger: <a href=""https://learn.microsoft.com/en-us/connectors/azureblob/#when-a-blob-is-added-or-modified-(properties-only)"" rel=""nofollow noreferrer"">When a blob is added or modified (properties only)</a>:</p>
<ul>
<li>This operation triggers a flow when one or more blobs are added or
modified in a container. This trigger will only fetch the file
metadata. To get the file content, you can use the &quot;Get file content&quot;
operation. The trigger does not fire if a file is added/updated in a
subfolder. If it is required to trigger on subfolders, multiple
triggers should be created.</li>
</ul>
<p>Action: <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#get-a-pipeline-run"" rel=""nofollow noreferrer"">Get a pipeline run</a></p>
<ul>
<li>Get a particular pipeline run execution</li>
</ul>
<p>Hope this helps.</p>
"
"62595754","To use the output of a lookup activity to query the db and write to a csv file in storage account usin ADF","<p>My requirement is to use ADF to read data (columnA) from an xlx/csv file which is in the storage account and use that (columnA) to query my db and the output of my query which includes (columnA) should be written to a file in storage account.</p>
<p>I was able to read the data from the storage account but getting it as table. I Need to use it as a individual entry like select * from table where id=columnA.</p>
<p>Then the next task if I'm able to read each data, how to write it to a file</p>
<p>I used lookup activity to read data from excel, the below is the sample output, I need to use only the sku number for my query next, not able to proceed with this. Kindly suggest a solution</p>
<p><a href=""https://i.stack.imgur.com/YVvML.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YVvML.png"" alt=""enter image description here"" /></a></p>
<p>I set a variable as the output of the lookup as suggested here <a href=""https://www.mssqltips.com/sqlservertip/6185/azure-data-factory-lookup-activity-example/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/6185/azure-data-factory-lookup-activity-example/</a> and tried to use that variable in my query, but I'm getting exception when I trigger it, bad template error.</p>
","<azure><azure-storage><etl><azure-data-factory>","2020-06-26 13:26:35","3061","0","1","62648884","<p>Please try this:
I create a sample like yours and there is no need to use <code>set variable</code>.</p>
<p><a href=""https://i.stack.imgur.com/wuMUp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wuMUp.png"" alt=""enter image description here"" /></a></p>
<p>Details:</p>
<p>Below is <code>lookup</code> output:</p>
<pre><code>{
    &quot;count&quot;: 3,
    &quot;value&quot;: [
        {
            &quot;SKU&quot;: &quot;aaaa&quot;
        },
        {
            &quot;SKU&quot;: &quot;bbbb&quot;
        },
        {
            &quot;SKU&quot;: &quot;ccc&quot;
        }
    ]
}
</code></pre>
<p>Setting of <code>copy data activity</code>:</p>
<p><a href=""https://i.stack.imgur.com/IY75n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IY75n.png"" alt=""enter image description here"" /></a></p>
<p>Query sql:</p>
<pre><code>select * from data_source_table where Name = '@{activity('Lookup1').output.value[0].SKU}'
</code></pre>
<p>You can also use this sql,if you need:</p>
<pre><code>select * from data_source_table where Name in('@{activity('Lookup1').output.value[0].SKU}','@{activity('Lookup1').output.value[1].SKU}','@{activity('Lookup1').output.value[2].SKU}') 
</code></pre>
<p>This is my test data in my <code>SQL DataBase</code>:</p>
<p><a href=""https://i.stack.imgur.com/NAx7p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NAx7p.png"" alt=""enter image description here"" /></a></p>
<p>Here is the result:</p>
<pre><code>1,&quot;aaaa&quot;,0,2017-09-01 00:56:00.0000000
2,&quot;bbbb&quot;,0,2017-09-02 05:23:00.0000000
</code></pre>
<p>Hope this can help you.</p>
<p><strong>Update:</strong></p>
<p>You can try to use DataFlow.</p>
<p><a href=""https://i.stack.imgur.com/2veWe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2veWe.png"" alt=""dataflow"" /></a></p>
<p>source1 is your <code>csv</code> file,source2 is <code>SQL DataBase</code>.</p>
<p>This is setting of <code>lookup</code></p>
<p><a href=""https://i.stack.imgur.com/RWFQX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RWFQX.png"" alt=""lookup"" /></a></p>
<p>Filter condition:<code>!isNull(PersonID)</code>(One column in your SQL DataBase.)</p>
<p>Then,use <code>select</code> delete the SKU column.</p>
<p>Finally,Output to single file.</p>
"
"62590792","Data Factory trigger on Blob in different resource group is failing","<p>I have a requirement where I have a Data Factory in one resource group and a blob storage in another storage group. I need to create a trigger on Data factory pipeline when a blob created in blob storage. I am getting this error while my pipeline is invoked.</p>
<pre><code>The Microsoft.EventGrid resource provider is not registered in subscription ***** . Register the provider in the subscription and retry the operation. 
Activity id:6566a4c8-0a1c-4a9e-8940-ce62e43264e3, timestamp: 6/26/2020 8:02:56 AM (UTC)
</code></pre>
<p>As I understood from error, its permission issue but I am not sure how I can enable this connectivity. Can anyone help.</p>
","<permissions><azure-blob-storage><azure-data-factory><azure-resource-group>","2020-06-26 08:21:26","835","2","1","62631980","<p>So from the error, I think you should follow these steps:</p>
<p>Go to azure portal, and go to your subscription:</p>
<p><a href=""https://i.stack.imgur.com/zjzLe.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zjzLe.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/5INxR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5INxR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/733Mo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/733Mo.png"" alt=""enter image description here"" /></a></p>
<p>Search <code>Microsoft.EventGrid</code> in resource provider and then register <code>Microsoft.EventGrid</code>.</p>
<p>Let me know whether you can solve this problem.:)</p>
"
"62581071","Add to the same string variable inside a ForEach Acitivity in Azure Data Factory","<p>I want a concatenated string from a Get Metadata Activity. I am passing an array of column names and data type to a ForEach Activity and I would like get a concatenated string of the column names. Is it possible with a Set Variable inside the ForEach? When I try to concatenate it is not possible to call the same variable I am concatenating to. Is there a work around?</p>
<p>Pipeline:</p>
<p><a href=""https://i.stack.imgur.com/Isfzv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Isfzv.png"" alt=""enter image description here"" /></a></p>
<p>Output of Get Metadata1:</p>
<p><a href=""https://i.stack.imgur.com/3Owgv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Owgv.png"" alt=""enter image description here"" /></a></p>
<p>Inside ForEach:</p>
<p><a href=""https://i.stack.imgur.com/u2tip.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u2tip.png"" alt=""enter image description here"" /></a></p>
<p>Inside Set Variable:</p>
<p><a href=""https://i.stack.imgur.com/5QTZy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5QTZy.png"" alt=""enter image description here"" /></a></p>
<p>I am new to Azure. Would like to know if there is a better way. Thanks!</p>
","<azure><azure-data-factory>","2020-06-25 17:30:01","2067","1","1","62687759","<ol>
<li>Create an dst-text variable ex. <em>result</em></li>
<li>Create an array variable ex. <em>mylist</em></li>
<li>Inside Foreach, add an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-append-variable-activity"" rel=""nofollow noreferrer"">Append Variable</a> with variable: <em>mylist</em> value: item()</li>
<li>After Foreach, Set Variable: <em>result</em> value: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#join"" rel=""nofollow noreferrer"">join</a>(<em>mylist</em>,'')</li>
</ol>
"
"62579020","Rest API services in azure","<p>I am new to REST API services.
We got a requirement to extract the data from one of the on-premise environment. On-Premise team told us that they are sending HTTP Messages using POST method to URI.</p>
<p>On-Premise team asking us to share the URI in azure which accepts the HTTP Post Messages.</p>
<p>We are using Azure Data Factory where we can configure HTTP Data set and linked service but we cannot give any URI to on-premise team, in reverse ADF need HTTPS URI to extract the data. so this option is rolled out for us</p>
<p>Please let us know if blob can accept https post messages from on-premise if so how we can get the URI to share with on-premise team.</p>
<p>Is there any other way to implement this in azure ?</p>
<p>Regards,
Srinivas</p>
","<azure><http-post><azure-blob-storage><azure-data-factory><rest>","2020-06-25 15:33:20","122","0","1","62579102","<p>You can check the following link with specs about how to interact with Storage Account using REST API:</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/put-blob"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/storageservices/put-blob</a></p>
<p>PS: in the left menu, you'll find the related API spec for each service</p>
"
"62571191","Azure ADF quote all disabled","<p>in Azure copy , this option is disabled</p>
<p>Please see image
[1]: <a href=""https://i.stack.imgur.com/5crBb.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/5crBb.png</a></p>
<p>Any pointers</p>
<p>Regards
Ravikiran</p>
","<azure><azure-data-factory>","2020-06-25 08:35:49","895","1","2","62586738","<p>Quota all text only enabled in Data Flow Sinks settings:
<a href=""https://i.stack.imgur.com/z4L1J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z4L1J.png"" alt=""enter image description here"" /></a></p>
<p>Ref:</p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#sink-properties"" rel=""nofollow noreferrer"">Sink properties</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delimited-text#sink-example"" rel=""nofollow noreferrer"">Sink example</a></li>
</ol>
<p>In Copy active, the the option is disabled, but we could set it in dataset quote char:
<a href=""https://i.stack.imgur.com/bALBx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bALBx.png"" alt=""enter image description here"" /></a></p>
<p>But as you said &quot;it used to work before and suddenly it stopped&quot;, If you want get the root cause about the feature, the best way is ask Azure support for help.
Ref: How to <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">create an Azure support request</a>.</p>
<p>Hope this helps.</p>
"
"62571191","Azure ADF quote all disabled","<p>in Azure copy , this option is disabled</p>
<p>Please see image
[1]: <a href=""https://i.stack.imgur.com/5crBb.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/5crBb.png</a></p>
<p>Any pointers</p>
<p>Regards
Ravikiran</p>
","<azure><azure-data-factory>","2020-06-25 08:35:49","895","1","2","62587566","<p>as a work around I added text qualifiers to sink mapping after importing the schema
<a href=""https://i.stack.imgur.com/28yx8.png"" rel=""nofollow noreferrer"">Image of the workaround</a></p>
"
"62571096","Getting error while create Dynamic CRM Onpremises Linked service in Azure data Factory using IFD authentication","<p>I am trying to create a Dynamics 365(On premises) linked services in Azure data factory. I have entered following information.</p>
<ol>
<li>Deployment Type : OnPremisesWithIfd</li>
<li>Organization Name : Enter valid name</li>
<li>Host : Enter valid host</li>
<li>Port : 443 (by default)</li>
<li>Authentication type : IFD</li>
<li>Enter valid username and password</li>
</ol>
<p>But, I am getting following error:</p>
<blockquote>
<p>Unable to Login to Dynamics CRM: Unable to login to Dynamics CRM, Error was : Metadata contains a reference that cannot be resolved: 'https://https//crm-edu.abc.com/CRMDEV:443/XRMServices/2011/Discovery.svc?wsdl&amp;sdkversion=9'</p>
</blockquote>
","<azure><dynamics-crm><azure-data-factory>","2020-06-25 08:29:57","340","1","1","62594075","<blockquote>
<p>https://https//crm-edu.abc.com/CRMDEV:443/XRMServices/2011/Discovery.svc?wsdl&amp;sdkversion=9</p>
</blockquote>
<p>There is doubled https and also port in wrong part of url. Try entering just yours host part: crm-edu.abc.com into host input field.</p>
<p>Example of configuration: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365#dynamics-365-and-dynamics-crm-on-premises-with-ifd"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365#dynamics-365-and-dynamics-crm-on-premises-with-ifd</a></p>
"
"62567963","Azure data factory Dataset","<p>I have a DelimitedText ADF dataset. It is pipe delimited. When I use this as a source in copy data activity in a pipeline and write the file data to a SQL database table, blank values are loaded as NULL.</p>
<p>How can I avoid this? I want blank values to read as blank values and write into database table as blank values.</p>
<p>I tried keeping NULL value as blank and &quot;treatEmptyAsNull&quot;: false in dataset json; both didnt work.</p>
<p>Any suggestions?</p>
","<azure-data-factory>","2020-06-25 04:37:50","348","0","1","62569322","<p>Tested and set NULL value property with <code>''</code>, it works:
<a href=""https://i.stack.imgur.com/o56ex.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o56ex.png"" alt=""enter image description here"" /></a></p>
<p>Result comparation:</p>
<p><a href=""https://i.stack.imgur.com/F4AJe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F4AJe.png"" alt=""enter image description here"" /></a></p>
<p>I tested expression <code>concat('')</code>,  it also works.</p>
<p>Hope this helps.</p>
"
"62563618","Is it possible to create build pipeline in Azure DevOps for Azure Data factory","<p>I need to automate Azure Data Factory deployment via Azure DevOps. As part of that, I have created a build pipeline but for enabling continuous trigger, which branch should I select? Is it the adf_publish branch? If this is the branch, then I believe a new build should be published manually in the Dev environment to trigger a change in the adf_publish branch, which is a manual process.
Thanks in advance!!</p>
","<azure><azure-devops><azure-data-factory>","2020-06-24 20:32:40","404","0","2","62571819","<p>You should choose <code>adf_publish</code> branch when enabling continuous trigger. So that whenever changes are published to adf_publish branch, your pipeline will be triggered.</p>
<blockquote>
<p>By default, data factory generates the Resource Manager templates of the published factory and saves them into a branch called adf_publish.</p>
</blockquote>
<p>To trigger a change in the adf_publish branch, you need to click <strong>Publish</strong> to manually <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control#publish-code-changes"" rel=""nofollow noreferrer"">publish your code changes</a> in the collaboration branch to the Data Factory service, after you have merged changes to the collaboration branch (master is the default). So the the changes will be updated to adf_publish branch.</p>
<p>You can refer to the examples in below blogs:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">Continuous integration and delivery in Azure Data Factory</a></p>
<p><a href=""http://datanrg.blogspot.com/2019/02/continuous-integration-and-delivery.html"" rel=""nofollow noreferrer"">Continuous integration and delivery (CI/CD) in Azure Data Factory using DevOps and GitHub</a></p>
<p><a href=""https://medium.com/@cprosenjit/azure-devops-pipeline-setup-for-azure-data-factory-v2-8e957cd5141"" rel=""nofollow noreferrer"">Azure DevOps Pipeline Setup for Azure Data Factory (v2)</a></p>
"
"62563618","Is it possible to create build pipeline in Azure DevOps for Azure Data factory","<p>I need to automate Azure Data Factory deployment via Azure DevOps. As part of that, I have created a build pipeline but for enabling continuous trigger, which branch should I select? Is it the adf_publish branch? If this is the branch, then I believe a new build should be published manually in the Dev environment to trigger a change in the adf_publish branch, which is a manual process.
Thanks in advance!!</p>
","<azure><azure-devops><azure-data-factory>","2020-06-24 20:32:40","404","0","2","62757348","<p>The above mentioned issue where dev values are hardcoded can be changed by adding parametrization template in the Azure DataFactory. Using the below link from Microsoft custom parameters can be added.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a></p>
"
"62552949","In Azure Data Factory, how do i create a string expressions made up of a Paramater, string literal and function, in the web UI","<p>I would like to have an expression equal to MyParmater + '_' + utcnow()</p>
<p>My current attempt is: @{pipeline().parameters.Col}_{utcnow()}
but iy fails</p>
","<azure-data-factory>","2020-06-24 10:28:35","53","-1","1","62555997","<p>Use the <em><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#concat"" rel=""nofollow noreferrer"">concat</a></em> function:</p>
<pre><code>@concat(pipeline().parameters.Col,'_',utcNow())
</code></pre>
"
"62550631","Programatically Create New ADF pipeline using Existing Pipeline with in same ADF","<p>I am trying to create a new pipeline by doing a clone of existing pipeline. From portal, I can simply use clone option and do further modifications. By I need to automate the task of cloning the pipeline using Python. It will be really useful if someone give me pointer.</p>
<p>Steps I already did:</p>
<ol>
<li>open Code section of my existing pipeline and copy JSON file. I am trying to programmatically alter JSON like pipeline name and variable value. When I send back the request, it give invalid dataset links. I followed MS documentation for this. Since in MS documentation, they are creating all from scratch, it may work but I am here interested to reuse the existing datasets and linked services.</li>
<li>I don't want to create ADF pipeline from scratch as I already have the reference pipeline which is bit complex.</li>
</ol>
<p>Any suggestion would be useful.</p>
","<azure><azure-data-factory>","2020-06-24 08:17:42","227","0","1","66864004","<p>Throwing this out there as you did ask for &quot;any suggestion&quot;.</p>
<p>You might be able to do this by setting up source control then programmatically duplicating/updating the relevant ARM templates. The key would be to work in the Collaboration branch and not the publish branch.</p>
"
"62543165","Azure Data Factory runtime looks different when running Dataflow","<p>I am trying Azure Data Factory V2. I run a DB extract and sink output into Blob. When I use DataFlow the resulting files are split in parts like in Spark output style. This is because a Spark cluster is the runtime (i assume). However when I run a &quot;normal&quot; pipeline (no Dataflow) then it seems output is different e.g. output can even be a single file. So my question is: is there runtime difference between pipeline and dataflow? thanks</p>
","<azure><azure-data-factory>","2020-06-23 20:23:57","72","0","1","62545218","<p>The difference you are seeing is the Copy Activity infrastructure vs. the Data Flow activity infrastructure.</p>
<p>Copy is built for fast data movement while Data Flows is built for scale-out data transformation.</p>
<p>The diagram in the data flow overview document may help: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview</a></p>
"
"62536101","Cannot loop through files using Azure Data Factory wildcards correctly","<p>In my source folder (Azure Blob Storage) I have, among others, two csv files named:
retails.csv
retailsitems.csv</p>
<p>My copy activity is set to get the file name as <code>pCollection*.csv</code>, as I use some json conf files where pCollection is defined for those files as 'retails' and 'retailsitems' respectively.</p>
<p>It looks alright and works for all other files smoothly. However, for these two specifically it gets only the 'retailsitems.csv' file.</p>
<p>Why?</p>
<p>And how can I fix that without messing the other files?</p>
","<parameters><wildcard><azure-data-factory>","2020-06-23 13:38:16","53","0","1","62590375","<p>&quot;put the wild card inside the json parameter file. It worked without interfering on one another.&quot;</p>
<p>I help you post it as answer and this can be beneficial to other community members.</p>
"
"62529301","Azure DataFactory cleanup: delete all objects","<p>I have DEV and PROD ADF instances,</p>
<p>I export ARM template from DEV, change params and import ARM to PROD as a &quot;release&quot; workflow,</p>
<p>The problem is that ARM &quot;merges&quot; stuff instead of complete replacement, so if I delete pipeline in DEV it's still present in PROD after release</p>
<p>Is there a way to clean the whole ADF instance (delete all objects) so I could perform cleanup before release?</p>
","<azure-data-factory>","2020-06-23 07:15:48","1292","0","1","62594252","<p>I would make a Azure Powershell task and do it like this:</p>
<pre><code>param([string] $dataFactoryName, [string]$resourceGroupName)

    Write-Output &quot;Data factory  : $dataFactoryName&quot;
    Write-Output &quot;Resource group: $resourceGroupName&quot;
    Write-Output &quot;========================================&quot;

    Write-Output &quot;Collecting resource locks&quot;
    $locks = Get-AzResourceLock -ResourceName $dataFactoryName -ResourceType &quot;Microsoft.DataFactory/factories&quot; -ResourceGroupName $resourceGroupName
    # Set-AzResourceLock -LockLevel CanNotDelete -ResourceName $dataFactoryName -ResourceType &quot;Microsoft.DataFactory/factories&quot; -ResourceGroupName $resourceGroupName
    Write-Output &quot;Found $($locks.Count) locks&quot;
    $locks | ForEach-Object -process {
        Write-Output &quot;Removing Lock Id: $($lock.LockId)&quot;
        Remove-AzResourceLock -LockId $_.LockId -Force
    }

    Write-Output &quot;Collecting triggers&quot;
    $triggers = Get-AzDataFactoryV2Trigger -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName

    Write-Output &quot;Found $($triggers.Count) triggers&quot;
    $triggers | ForEach-Object -process { 
        Write-Output &quot;Stopping trigger $($_.name)&quot;
        Stop-AzDataFactoryV2Trigger -name $_.name -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Force 
        Write-Output &quot;Deleting trigger $($_.name)&quot;
        Remove-AzDataFactoryV2Trigger -name $_.name -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Force 
    }

    Write-Output &quot;========================================&quot; 
    Write-Output &quot;Collecting pipelines&quot;
    $pipelines = Get-AzDataFactoryV2Pipeline -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName | Sort-Object -Property name -Descending
    Write-Output &quot;Found $($pipelines.Count) pipelines&quot;
    $pipelines | ForEach-Object -process { 
        Write-Output &quot;Deleting pipeline $($_.name)&quot;
        Remove-AzDataFactoryV2Pipeline -name $_.name -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Force 
    }

    Write-Output &quot;========================================&quot; 
    Write-Output &quot;Collecting dataflows&quot;
    $dataFlows = Get-AzDataFactoryV2DataFlow -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName

    Write-Output &quot;Found $($dataFlows.Count) data flows&quot;
    $dataFlows | ForEach-Object -process {
        Write-Output &quot;Removing DataFlow $($_.name)&quot;
        Remove-AzDataFactoryV2DataFlow -name $_.name -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Force 
    }

    Write-Output &quot;========================================&quot;
    Write-Output &quot;Collecting datasets&quot;
    $datasets = Get-AzDataFactoryV2Dataset -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName
    Write-Output &quot;Found $($datasets.Count) datasets&quot;
    $datasets | ForEach-Object -process { 
        Write-Output &quot;Deleting dataset $($_.name)&quot;
        Remove-AzDataFactoryV2Dataset -name $_.name -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Force 
    }
    Write-Output &quot;========================================&quot;
    Write-Output &quot;Collecting Linked services&quot;
    $lservices = Get-AzDataFactoryV2LinkedService -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName | Sort-Object -Property name -Descending
    Write-Output &quot;Found $($lservices.Count) linked services&quot;

    $lservices | ForEach-Object -process { 
        Write-Output &quot;Deleting linked service $($_.name)&quot;
        Remove-AzDataFactoryV2LinkedService -name $_.name -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Force 
    }

    Write-Output &quot;========================================&quot;
    Write-Output &quot;Collecting Integration runtimes &quot;
    $runtimes = Get-AzDataFactoryV2IntegrationRuntime -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName
    Write-Output &quot;Found $($runtimes.Count) Integration Runtimes&quot;
    $runtimes | ForEach-Object -process { 
        Write-Output &quot;Deleting Integration runtime $($_.name)&quot;
        Remove-AzDataFactoryV2IntegrationRuntime -name $_.name -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Force 
    }

    Write-Output &quot;========================================&quot;
    Write-Output &quot;ReApply lock&quot;

    Set-AzResourceLock -LockName &quot;$dataFactoryName-lock&quot; -ResourceName $dataFactoryName -ResourceGroupName $resourceGroupName -ResourceType &quot;Microsoft.DataFactory/factories&quot; -LockLevel CanNotDelete -Force

    Write-Output &quot;========================================&quot;
</code></pre>
"
"62525120","Regular expression with if condition activity in Azure","<p>I want to check if a file name contains a date pattern (<code>dd-mmm-yyy</code>) in it using <code>if condition</code> activity in Azure Data Factory. For example: The file name I have is like somestring_23-Apr-1984.csv which has a date pattern in it.</p>
<p>I get the file name using <code>Get Metadata</code> activity and passing it to the <code>if condition</code> activity where I want to check if the file name has the date pattern in it and based on the result I would like to perform different tasks. The only way I know to do this is by using regex to check if the pattern is available in the file name string but, Azure does not have a regex solution mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>Is there any other way to achieve my requirement in ADF? Your help is much appreciated.</p>
","<azure><azure-data-factory>","2020-06-22 23:38:34","2457","0","1","62529260","<p>Yes,there is no regex in expression.There is another way to do this,but it is very complex.</p>
<p>First,get the date string(23-Apr-1984) from the output of <code>Get Metadata</code>.</p>
<p>Then,split the date string and determine whether each part match date pattern.</p>
<p>Below is my test pipeline:</p>
<p><a href=""https://i.stack.imgur.com/1y5s7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1y5s7.png"" alt=""total"" /></a></p>
<p>First <code>Set variable</code>:</p>
<pre><code>name: fileName
value: @split(split(activity('MyGetMetadataActivity').output.itemName,'_')[1],'.csv')[0]
</code></pre>
<p>Second <code>Set variable</code>:</p>
<pre><code>name: fileArray
value: @split(variables('fileName'),'-')
</code></pre>
<p><code>If Condition</code>:</p>
<pre><code>Expression:@and(contains(variables('DateArray'),variables('fileArray')[0]),contains(variables('MonthArray'),variables('fileArray')[1]))
</code></pre>
<p>By the way,I want to compare date with 0 and 30 originally,but <code>greaterOrEquals()</code> doesn't support nested properties.So I use <code>contains()</code>.</p>
<p>Hope this can help you.</p>
"
"62520886","Running stored procedure immediatelly","<p>I'm currently in need to execute a stored procedure just before I load data into a table.</p>
<p>I've tried with a stored procedure activity but it still has a time (around 10 secs) to start the copy and it will interfere with other processes we have running.</p>
<p>Is there a faster way? I also looked at Azure functions but I dont think it should be that complicated.</p>
","<postgresql><azure-data-factory>","2020-06-22 18:14:26","149","2","1","62524682","<p>The only way I can think of running it immediately before doing the actual copy is at the pre-copy script on the sink tab of the copy activity.</p>
<p>Any query you write there will be run before inserting the data, so if your database is a postgres (as you tagged the question) you may write:</p>
<pre><code>Call functionName()
</code></pre>
<p>If it was a sql server:</p>
<pre><code>exec functionName
</code></pre>
<p>Hope this helped!!</p>
<p><a href=""https://i.stack.imgur.com/GI7rc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GI7rc.png"" alt=""enter image description here"" /></a></p>
"
"62518244","multiple expressions in SSIS constriant","<p>I have two variables: user::runnumber and user::recordcount.</p>
<p>If the <code>runnumber = 1</code> OR <code>recordcount =0</code>  task A should execute.
Task A is an 'execute package task' that otherwise is functioning fine.
In any other scenario task A should be skipped.</p>
<p>I thought setting the task to disabled using the expression <code>(@[User::runnumber]==1 ||  @[User::record_count]!=0)</code> would do the trick but it doesn't.
Any idea on how to tackle this?</p>
<pre><code>runnumber = 1 recordcount = 0 --&gt; task A
runnumber = 1 recordcount = 100 --&gt; task A
runnumber = 4 recordcount = 0--&gt; task A
runnumber = 4 recordcount = 4--&gt; skip task A
</code></pre>
<p><a href=""https://i.stack.imgur.com/aWKik.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aWKik.png"" alt=""The flow"" /></a></p>
<p><a href=""https://i.stack.imgur.com/vvDqo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vvDqo.png"" alt=""current settings"" /></a></p>
","<ssis><azure-data-factory>","2020-06-22 15:40:32","813","1","2","62520688","<p>Use a precedence constraint to control the execution of Task A.  So if your control flow looked something like this:</p>
<p><a href=""https://i.stack.imgur.com/nDpKi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nDpKi.png"" alt=""enter image description here"" /></a></p>
<p>Double click the constraint arrow in between the two tasks, set the Evaluation Option to &quot;Expression&quot; and add the following expression:
<code>@[User::runnumber] == 1 ||  @[User::recordcount] == 0</code></p>
<p>EDIT:
Adding configuration of precedence constraint:
<a href=""https://i.stack.imgur.com/9p1Oq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9p1Oq.png"" alt="""" /></a></p>
"
"62518244","multiple expressions in SSIS constriant","<p>I have two variables: user::runnumber and user::recordcount.</p>
<p>If the <code>runnumber = 1</code> OR <code>recordcount =0</code>  task A should execute.
Task A is an 'execute package task' that otherwise is functioning fine.
In any other scenario task A should be skipped.</p>
<p>I thought setting the task to disabled using the expression <code>(@[User::runnumber]==1 ||  @[User::record_count]!=0)</code> would do the trick but it doesn't.
Any idea on how to tackle this?</p>
<pre><code>runnumber = 1 recordcount = 0 --&gt; task A
runnumber = 1 recordcount = 100 --&gt; task A
runnumber = 4 recordcount = 0--&gt; task A
runnumber = 4 recordcount = 4--&gt; skip task A
</code></pre>
<p><a href=""https://i.stack.imgur.com/aWKik.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aWKik.png"" alt=""The flow"" /></a></p>
<p><a href=""https://i.stack.imgur.com/vvDqo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vvDqo.png"" alt=""current settings"" /></a></p>
","<ssis><azure-data-factory>","2020-06-22 15:40:32","813","1","2","62523052","<p>There are two ways of going about this: Precedence Constraints and the Disabled property.</p>
<p>If there are no paths leaving a Task, then they are equivalent approaches. If there is a &quot;downstream&quot; task that should run regardless of whether the Execute Package Task is fired, then the Disabled property likely makes the most sense. Turning a precedent constraint off disables the entire branch of logic whereas setting Disabled just skips that task for execution.</p>
<p>For the supplied control flow, I'd just have the PC be Success and use the Disabled property.</p>
<p>The property of Disabled is always confusing for me to think about because of the negativity.</p>
<p>Regardless of which approach you take, I would suggest creating boolean variables to track whether the rules of &quot;run execute package task&quot; are met. You can test that independently of the logic for toggling whether the task is run.</p>
<p>I think your core issue might be <code>@[User::record_count]!=0</code>  Your business rule is when record_count is zero, then it should be a yes but this is the inversion of that.</p>
<p>For simplicity, I created 3 SSIS Variable, DisableExecutePackageTask of type Boolean and have them defined via Expressions</p>
<ul>
<li>EnableEPTRuleRunNumber  <code>@[User::runnumber]==1</code></li>
<li>EnableEPTRuleRecordCount <code>@[User::record_count]==0</code></li>
<li>EnableEPT <code>@[User::EnableEPTRuleRunNumber] || @[User::EnableEPTRuleRecordCount]</code></li>
</ul>
<p>I also defined two variables that I called labels and set the Name property of two Containers to be them so I could see when I ran my package what happened.</p>
<p>-- LabelRunNumber <code>&quot;RunNumber &quot; + (DT_WSTR, 5) @[User::runnumber] + &quot; rule is &quot; + (DT_WSTR, 10) @[User::EnableEPTRuleRunNumber]</code>
-- LabelRecordCount <code>&quot;RecordCount &quot; + (DT_WSTR, 5) @[User::record_count] + &quot; rule is &quot; + (DT_WSTR, 10) @[User::EnableEPTRuleRecordCount]</code></p>
<p>Then I would change the value of my variables runnumber and record_count to evaluate the scenarios.</p>
<p><a href=""https://i.stack.imgur.com/DkyOy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DkyOy.png"" alt=""number 1 count 0"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Jgt9o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jgt9o.png"" alt=""number 4 count 0"" /></a></p>
<p><a href=""https://i.stack.imgur.com/6TNrM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6TNrM.png"" alt=""number 4 count 4"" /></a></p>
<p>And since I love <a href=""http://www.bimlscript.com/"" rel=""nofollow noreferrer"">Biml</a></p>
<pre><code>&lt;Biml xmlns=&quot;http://schemas.varigence.com/biml.xsd&quot;&gt;
    &lt;Packages&gt;
&lt;!-- 
EPT should be ON when 
record count is non zero
or run number is 1

It should be OFF when
record count is zero
and run number is not 1 
--&gt;
        &lt;Package Name=&quot;SO_62518244&quot;&gt;
            &lt;Variables&gt;
                &lt;Variable Name=&quot;runnumber&quot; DataType=&quot;Int32&quot;&gt;0&lt;/Variable&gt;
                &lt;Variable Name=&quot;record_count&quot; DataType=&quot;Int32&quot;&gt;0&lt;/Variable&gt;
                &lt;Variable Name=&quot;EnableEPTRuleRunNumber&quot; DataType=&quot;Boolean&quot; EvaluateAsExpression=&quot;true&quot;&gt;@[User::runnumber]==1&lt;/Variable&gt;
                &lt;Variable Name=&quot;EnableEPTRuleRecordCount&quot; DataType=&quot;Boolean&quot; EvaluateAsExpression=&quot;true&quot;&gt;@[User::record_count]==0&lt;/Variable&gt;
                &lt;Variable Name=&quot;EnableEPT&quot; DataType=&quot;Boolean&quot; EvaluateAsExpression=&quot;true&quot;&gt;@[User::EnableEPTRuleRunNumber] || @[User::EnableEPTRuleRecordCount]&lt;/Variable&gt;
                &lt;Variable Name=&quot;LabelRunNumber&quot; DataType=&quot;String&quot; EvaluateAsExpression=&quot;true&quot;&gt;&quot;RunNumber &quot; + (DT_WSTR, 5) @[User::runnumber] + &quot; rule is &quot; + (DT_WSTR, 10) @[User::EnableEPTRuleRunNumber]&lt;/Variable&gt;
                &lt;Variable Name=&quot;LabelRecordCount&quot; DataType=&quot;String&quot; EvaluateAsExpression=&quot;true&quot;&gt;&quot;RecordCount &quot; + (DT_WSTR, 5) @[User::record_count] + &quot; rule is &quot; + (DT_WSTR, 10) @[User::EnableEPTRuleRecordCount]&lt;/Variable&gt;
            &lt;/Variables&gt;
            &lt;Tasks&gt;
                &lt;Container Name=&quot;Before&quot;/&gt;
                &lt;Container Name=&quot;EPT Placeholder&quot;&gt;
                    &lt;Expressions&gt;
                        &lt;Expression ExternalProperty=&quot;Disable&quot;&gt;!@[User::EnableEPT]&lt;/Expression&gt;
                    &lt;/Expressions&gt;
                    &lt;PrecedenceConstraints&gt;
                        &lt;Inputs&gt;
                            &lt;Input OutputPathName=&quot;Before.Output&quot; /&gt;
                        &lt;/Inputs&gt;
                    &lt;/PrecedenceConstraints&gt;
                &lt;/Container&gt;
                &lt;Container Name=&quot;After&quot;&gt;
                    &lt;PrecedenceConstraints&gt;
                        &lt;Inputs&gt;
                            &lt;Input OutputPathName=&quot;EPT Placeholder.Output&quot; /&gt;
                        &lt;/Inputs&gt;
                    &lt;/PrecedenceConstraints&gt;
                &lt;/Container&gt;
                &lt;Container Name=&quot;Debug RecordCount&quot;&gt;
                    &lt;Expressions&gt;
                        &lt;Expression PropertyName=&quot;Name&quot;&gt;@[User::LabelRecordCount]&lt;/Expression&gt;
                    &lt;/Expressions&gt;
                &lt;/Container&gt;
                &lt;Container Name=&quot;Debug RunNumber&quot;&gt;
                    &lt;Expressions&gt;
                        &lt;Expression PropertyName=&quot;Name&quot;&gt;@[User::LabelRunNumber]&lt;/Expression&gt;
                    &lt;/Expressions&gt;
                &lt;/Container&gt;
            &lt;/Tasks&gt;
        &lt;/Package&gt;
    &lt;/Packages&gt;
&lt;/Biml&gt;
</code></pre>
<p>The names of the tasks don't always update until you close and re-open the package as they don't get the pulse to re-evaulate themselves as the variables change but it does work.</p>
"
"62514821","Azure Wrangling data flow doesn't appearing","<p>I want to use the <code>Wrangling data flow</code> in <code>Azure Data Factory v2</code>, but this data flow doesn't appearing for me.</p>
<p>I followed this tutorial <a href=""https://learn.microsoft.com/en-us/azure/data-factory/wrangling-data-flow-tutorial"" rel=""nofollow noreferrer"">Prepare data with wrangling data flow</a></p>
<p>We have this image to create the wrangler:
<a href=""https://i.stack.imgur.com/DbEOg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DbEOg.png"" alt=""enter image description here"" /></a></p>
<p>But, in my subscription these options doesn't appearing for me.
<a href=""https://i.stack.imgur.com/Dl6Cq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dl6Cq.png"" alt=""enter image description here"" /></a></p>
<p>I searched in many websites, tutorials and I didn't find anything about this.</p>
","<azure><azure-data-factory>","2020-06-22 12:48:08","95","0","2","62570901","<p>If you have already create the Wrangling data flow, I think you can select it.</p>
<p>In your screenshot, it seems you dont select the 'Use existing data flow':</p>
<p><a href=""https://i.stack.imgur.com/D1J78.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D1J78.png"" alt=""enter image description here"" /></a></p>
"
"62514821","Azure Wrangling data flow doesn't appearing","<p>I want to use the <code>Wrangling data flow</code> in <code>Azure Data Factory v2</code>, but this data flow doesn't appearing for me.</p>
<p>I followed this tutorial <a href=""https://learn.microsoft.com/en-us/azure/data-factory/wrangling-data-flow-tutorial"" rel=""nofollow noreferrer"">Prepare data with wrangling data flow</a></p>
<p>We have this image to create the wrangler:
<a href=""https://i.stack.imgur.com/DbEOg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DbEOg.png"" alt=""enter image description here"" /></a></p>
<p>But, in my subscription these options doesn't appearing for me.
<a href=""https://i.stack.imgur.com/Dl6Cq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dl6Cq.png"" alt=""enter image description here"" /></a></p>
<p>I searched in many websites, tutorials and I didn't find anything about this.</p>
","<azure><azure-data-factory>","2020-06-22 12:48:08","95","0","2","70255945","<p>The Azure data wrangling in data flow is actually moved below as &quot;<strong>Power Query</strong>&quot;.</p>
<p>for more details watch the official docs video:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/wrangling-overview#use-cases"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/wrangling-overview#use-cases</a></p>
"
"62511940","Azure Data Factory - Salesforce connection problem","<p>I have some issue with my ADF Salesforce connetor. Did anybody have it?</p>
<p>Operation on target Copy data1 failed: Failure happened on 'Sink' side. ErrorCode=UserErrorSalesforceOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=[BatchFailure]JobId:7503B000005V9v5QAC, BatchId:7513B000006IF3pQAG, Message:InvalidBatch : Field name not found : ConnectionReceivedId,Source=Microsoft.DataTransfer.Runtime.SalesforceConnector,'</p>
","<salesforce><azure-data-factory>","2020-06-22 10:04:56","938","0","1","62548255","<p>Congratulations that you figured it out:</p>
<p>&quot; I figured it out! Some of fields were not allowed to add. After deletion in mapping section everything works fine&quot;</p>
<p>I help you post it as answer and this can be beneficial to other community members.</p>
"
"62510783","Is there any correlation between PipelineId and CorelationId in Azure Data Factory v2","<p>Is there any relationship between pipelinerunid and correlationId in ADF v2?</p>
<p>We have enabled LogAnalytics in ADF v2 and now we need to backtrack a Pipeline run for which I have its PipelineRunid.</p>
<p>So how should I filter out the log data based on PipelineRunId?</p>
<p>There are some <a href=""https://stackoverflow.com/questions/54034373/adf-log-analytics-how-to-correlate-multiple-pipelines"">posts</a> wherein they have mentioned that CorrelationId and PipelineId are same :</p>
<p>But that is not the case.</p>
<p>So wanted the help on the same</p>
<p>Thanks,
Pratik</p>
","<azure><azure-data-factory><azure-log-analytics>","2020-06-22 08:57:55","560","0","2","62526542","<p><strong>Is there any relationship between pipelinerunid and correlationId in ADF v2?</strong></p>
<p>PipelineID is the ID of the pipeline run.</p>
<p>CorrelationId is the unique ID for tracking a particular request.</p>
<p><a href=""https://i.stack.imgur.com/GEEyC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GEEyC.png"" alt=""enter image description here"" /></a></p>
<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">Monitor and Alert Data Factory by using Azure Monitor</a></p>
<p><strong>How should I filter out the log data based on PipelineRunId?</strong></p>
<p>When we open the logs, we query the log tables <code>ADFPipelineRun</code> and <code>ADFActivityRun</code> by column <code>PipelineRunId</code>/<code>RunID</code>.</p>
<p>For example:</p>
<p><a href=""https://i.stack.imgur.com/DftfH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DftfH.png"" alt=""enter image description here"" /></a></p>
<p>For more details, you could ref: <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-portal"" rel=""nofollow noreferrer"">Tutorial: Get started with Log Analytics queries</a>.</p>
<p>Hope this helps.</p>
"
"62510783","Is there any correlation between PipelineId and CorelationId in Azure Data Factory v2","<p>Is there any relationship between pipelinerunid and correlationId in ADF v2?</p>
<p>We have enabled LogAnalytics in ADF v2 and now we need to backtrack a Pipeline run for which I have its PipelineRunid.</p>
<p>So how should I filter out the log data based on PipelineRunId?</p>
<p>There are some <a href=""https://stackoverflow.com/questions/54034373/adf-log-analytics-how-to-correlate-multiple-pipelines"">posts</a> wherein they have mentioned that CorrelationId and PipelineId are same :</p>
<p>But that is not the case.</p>
<p>So wanted the help on the same</p>
<p>Thanks,
Pratik</p>
","<azure><azure-data-factory><azure-log-analytics>","2020-06-22 08:57:55","560","0","2","62526732","<p>Thank you for your reply :)
But the use case is to fetch the username who has triggered the pipeline manually.
In case if we use ADF pipeline run , we do not get the eventinitiatedby details.</p>
<p>But when we query the activitylogs based on the resourceuri , we got all details including corelationid of that execution except pipelinerunid.</p>
<p>So how should we link up the activitylog with the pipelinerunid?</p>
"
"62495368","Azure Data Factory processing end when validation fails","<p>I have developed a process to run a Stored process, only if there is a certain file. I used Validation to achieve this. The validation checks whether the file exists, if so, it starts the process. However, the issue is, if the file not exists it creates an error (Status comes as Error and the message is &quot;Operation on target Check SRManifest Exists failed:&quot;), even after i have added a Wait. What I wanted to happen is, if the file does not exist the process needs to be stopped. Appreciate if someone can help me to achieve. Below is the existing processes.<a href=""https://i.stack.imgur.com/fpqa3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fpqa3.png"" alt=""enter image description here"" /></a></p>
<p>Validation Settings as below:<a href=""https://i.stack.imgur.com/OVwYs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OVwYs.png"" alt=""enter image description here"" /></a> Also, The properties are as below <a href=""https://i.stack.imgur.com/nOESl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nOESl.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2020-06-21 06:52:45","1214","2","1","62513057","<p>It is no problem with your design.</p>
<p>I put a blob path that is not exist in the validation activity, and I also get this error. But if you go to see the monitor of the pipeline, you will found the wait activity is success:</p>
<p><a href=""https://i.stack.imgur.com/vk9jv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vk9jv.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>if the file does not exist the process needs to be stopped.</p>
</blockquote>
<p>Your pipeline is finish. So the process is already stop.</p>
<p>There is no problem with your piping design. But ADF will definitely throw an error for a validation activity that encounters a timeout. Unless you do not reach the timeout 30 seconds that you setted, this error is expected, it is not your problem.</p>
"
"62478507","How to post json to REST from ADF","<p>I am building a pipeline in Azure Data Factory.  In this pipeline, we are retrieving data from a API in json format and transform into another JSON format and want to post to other API.  As per my understanding ADF Data Copy activity doesn't support REST API as output/sink service.</p>

<p>Just wondering what are the other ways to implement this pipeline without much coding.</p>
","<azure><azure-data-factory><azure-logic-apps>","2020-06-19 21:03:32","858","1","2","62484260","<p>If the transformations are fairly straightforward then your pipeline may be as simple as:</p>
<ol>
<li>Web Activity with method=GET</li>
<li>Web Activity with method=POST with body set to an expression which transforms the output of the first Web Activity in some way.</li>
</ol>
<p>You get the output of an activity with this expression:</p>
<pre><code>activity('ActivityNameHere').output
</code></pre>
"
"62478507","How to post json to REST from ADF","<p>I am building a pipeline in Azure Data Factory.  In this pipeline, we are retrieving data from a API in json format and transform into another JSON format and want to post to other API.  As per my understanding ADF Data Copy activity doesn't support REST API as output/sink service.</p>

<p>Just wondering what are the other ways to implement this pipeline without much coding.</p>
","<azure><azure-data-factory><azure-logic-apps>","2020-06-19 21:03:32","858","1","2","73268970","<p>You could also save the output json to a cache sink and try passing that to another data flow source</p>
"
"62476319","How to check what is the Virtual Network of Azure Data Factory Integration Runtime?","<p>I need to check what is the <code>Virtual Network</code> of the <code>Integration Runtime</code> from <code>Azure Data Factory</code> with PowerShell. I know there is cmdlet:</p>
<pre><code>Get-AzureRmDataFactoryV2IntegrationRuntime -ResourceGroupName $rgName -DataFactoryName $dfName
</code></pre>
<p>which brings me a list of IRs from my ADF, but I find it a bit limited. Is there any generic way to get the subnets and virtual networks of the IRs?</p>
<p>Update 1:
For my case it's a <code>Self Hosted Integration Runtime</code>.</p>
","<azure><powershell><azure-virtual-network><azure-data-factory>","2020-06-19 18:17:13","554","-1","1","62508182","<p>Per my test, the command just works with the Integration Runtime whose type is <code>Managed(Azure-SSIS)</code>, if it is <code>SelfHosted</code>, the command will not return the information related to the vnent and subnet.</p>
<p><strong>Note</strong>: I test with the new <a href=""https://learn.microsoft.com/en-us/powershell/azure/new-azureps-module-az?view=azps-4.2.0"" rel=""nofollow noreferrer""><code>Az</code></a> command <code>Get-AzDataFactoryV2IntegrationRuntime</code>, what you used is the old <code>AzureRm</code>, it was deprecated and will not be updated anymore, I recommend you to use the new one, for the old <code>AzureRm</code> command <code>Get-AzureRmDataFactoryV2IntegrationRuntime</code>, it is basically the same result, you can have a try.</p>
<p><em>Managed Integration Runtime:</em></p>
<p><a href=""https://i.stack.imgur.com/sc2ZS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sc2ZS.png"" alt=""enter image description here"" /></a></p>
<p><em>SelfHosted Integration Runtime:</em></p>
<p><a href=""https://i.stack.imgur.com/Y6zfY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y6zfY.png"" alt=""enter image description here"" /></a></p>
"
"62473237","Azure Data Factory Salesforce to Salesforce","<p>Does anybody work with Azure Data Factory V2 and with Salesforce? I need some protips with data migration between two orgs in Salesforce. Thank you for help! :) </p>
","<salesforce><azure-data-factory>","2020-06-19 15:10:19","108","-1","1","62508627","<p>You could follow the Data Factory document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce"" rel=""nofollow noreferrer"">Copy data from and to Salesforce by using Azure Data Factory</a>.</p>
<p>This article outlines how to use Copy Activity in Azure Data Factory to copy data from and to Salesforce. It builds on the Copy Activity overview article that presents a general overview of the copy activity.</p>
<p>Hope this helps.</p>
"
"62468686","BadRequest: The document creation or update failed because of invalid reference","<p>I'm trying to deploy an ADF pipeline using CICD (Azure DevOps Deployment) using a release pipeline.
Here I'm trying to merge stuff from my Collaboration branch to master (usig GIT).
I'm also using:</p>

<p>Azure Deployment: 
Create Or Update Resource Group action on SiteOpsConsolidatedProd (Agent Job) as a part of the Continuous Deployment Process</p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>[error]At least one resource deployment operation failed. Please list deployment operations for details. Please see <a href=""https://aka.ms/DeployOperations"" rel=""nofollow noreferrer"">https://aka.ms/DeployOperations</a> for usage details.
   [error]Details:
   ##[error]BadRequest: The document creation or update failed because of invalid reference 'irslinked'.
   ##[error]Check out the troubleshooting guide to see if your issue is addressed: <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting</a>
   ##[error]Task failed while creating or updating the template deployment.</p>
</blockquote>

<ol>
<li>irslinked is the name of a self-hosted IR (Linked)</li>
<li>this IR: irslinked is also included in the overwrite template parameters list. </li>
</ol>
","<azure><azure-data-factory><azure-resource-manager><azure-deployment><azure-rm-template>","2020-06-19 10:53:06","5433","1","4","63139230","<p>As suggested by Microsoft in their ADF CICD best practices, the self-hosted IR should be hosted on a dedicated ADF instance and should be of type shared and linked to the dev/test/prod instances.
Link to the documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#best-practices-for-cicd"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#best-practices-for-cicd</a></p>
<p>Please respond if it works.</p>
"
"62468686","BadRequest: The document creation or update failed because of invalid reference","<p>I'm trying to deploy an ADF pipeline using CICD (Azure DevOps Deployment) using a release pipeline.
Here I'm trying to merge stuff from my Collaboration branch to master (usig GIT).
I'm also using:</p>

<p>Azure Deployment: 
Create Or Update Resource Group action on SiteOpsConsolidatedProd (Agent Job) as a part of the Continuous Deployment Process</p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>[error]At least one resource deployment operation failed. Please list deployment operations for details. Please see <a href=""https://aka.ms/DeployOperations"" rel=""nofollow noreferrer"">https://aka.ms/DeployOperations</a> for usage details.
   [error]Details:
   ##[error]BadRequest: The document creation or update failed because of invalid reference 'irslinked'.
   ##[error]Check out the troubleshooting guide to see if your issue is addressed: <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting</a>
   ##[error]Task failed while creating or updating the template deployment.</p>
</blockquote>

<ol>
<li>irslinked is the name of a self-hosted IR (Linked)</li>
<li>this IR: irslinked is also included in the overwrite template parameters list. </li>
</ol>
","<azure><azure-data-factory><azure-resource-manager><azure-deployment><azure-rm-template>","2020-06-19 10:53:06","5433","1","4","66582546","<p>This may be helpful for someone. I had a similar invalid reference error. Actually, my ADF was imported from git. I have reconfigured the git with &quot;<strong>import existing resource to repository</strong>&quot; option.</p>
<p>Azure documentation <a href=""https://learn.microsoft.com/en-us/azure/data-factory/ci-cd-github-troubleshoot-guide#recover-from-a-deleted-data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/ci-cd-github-troubleshoot-guide#recover-from-a-deleted-data-factory</a></p>
<blockquote>
<p>If customer had a Self-hosted Integration Runtime in deleted ADF, they
will have to create a new instance in new ADF, also uninstall and
reinstall the instance on their On-prem machine/VM with the new key
obtained. After setup of IR is completed, customer will have to change
the Linked Service to point to new IR and test the connection or it
will fail with error invalid reference.</p>
</blockquote>
"
"62468686","BadRequest: The document creation or update failed because of invalid reference","<p>I'm trying to deploy an ADF pipeline using CICD (Azure DevOps Deployment) using a release pipeline.
Here I'm trying to merge stuff from my Collaboration branch to master (usig GIT).
I'm also using:</p>

<p>Azure Deployment: 
Create Or Update Resource Group action on SiteOpsConsolidatedProd (Agent Job) as a part of the Continuous Deployment Process</p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>[error]At least one resource deployment operation failed. Please list deployment operations for details. Please see <a href=""https://aka.ms/DeployOperations"" rel=""nofollow noreferrer"">https://aka.ms/DeployOperations</a> for usage details.
   [error]Details:
   ##[error]BadRequest: The document creation or update failed because of invalid reference 'irslinked'.
   ##[error]Check out the troubleshooting guide to see if your issue is addressed: <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting</a>
   ##[error]Task failed while creating or updating the template deployment.</p>
</blockquote>

<ol>
<li>irslinked is the name of a self-hosted IR (Linked)</li>
<li>this IR: irslinked is also included in the overwrite template parameters list. </li>
</ol>
","<azure><azure-data-factory><azure-resource-manager><azure-deployment><azure-rm-template>","2020-06-19 10:53:06","5433","1","4","66583833","<p>I had this error message but it referenced a resource that didn't even exist.</p>
<p>I was able to solve it using steps from here:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control#troubleshooting-git-integration"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/source-control#troubleshooting-git-integration</a></p>
<p>Notably:</p>
<ul>
<li>Disconnect from existing git repo.</li>
<li>Reconnect back to the same repo, but select &quot;import existing resources&quot; into a &quot;Create new git branch&quot;.</li>
<li>Then use git to create more commits on top of that branch, which remove all the extra crud that got generated, until the codebase is back to the desired state.</li>
</ul>
<p>Note that I had to have sequential commits deleting stuff in dependency order, before it would work. i.e. Pipelines, then dataflows, then datasets, then linked services, then Int.Runtimes.</p>
"
"62468686","BadRequest: The document creation or update failed because of invalid reference","<p>I'm trying to deploy an ADF pipeline using CICD (Azure DevOps Deployment) using a release pipeline.
Here I'm trying to merge stuff from my Collaboration branch to master (usig GIT).
I'm also using:</p>

<p>Azure Deployment: 
Create Or Update Resource Group action on SiteOpsConsolidatedProd (Agent Job) as a part of the Continuous Deployment Process</p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>[error]At least one resource deployment operation failed. Please list deployment operations for details. Please see <a href=""https://aka.ms/DeployOperations"" rel=""nofollow noreferrer"">https://aka.ms/DeployOperations</a> for usage details.
   [error]Details:
   ##[error]BadRequest: The document creation or update failed because of invalid reference 'irslinked'.
   ##[error]Check out the troubleshooting guide to see if your issue is addressed: <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops#troubleshooting</a>
   ##[error]Task failed while creating or updating the template deployment.</p>
</blockquote>

<ol>
<li>irslinked is the name of a self-hosted IR (Linked)</li>
<li>this IR: irslinked is also included in the overwrite template parameters list. </li>
</ol>
","<azure><azure-data-factory><azure-resource-manager><azure-deployment><azure-rm-template>","2020-06-19 10:53:06","5433","1","4","76042720","<p>Reconnect with your GIT repo, worked for me in Synapse.</p>
"
"62464431","How to set an alert for Azure Data factory when Pipeline takes more than N minutes to complete","<p>I need to setup an alert system if my Azure Datafactory pipeline runs for more than 20 minutes. The alert should come while the pipeline is running and the duration passes 20mins, not after the completion of pipeline. How can I do this? I think this can be done using Azure function but I am not familiar with it so I'm in search for a script for the same. </p>
","<azure><azure-data-factory>","2020-06-19 06:21:08","873","1","1","62508879","<p>Yes, azure function is a solution to acheive your requirement.</p>
<p>For example, if you are using Python. You need an azure function that runs periodically to monitor the status of the pipeline. The key is the duration time of the pipeline. pipeline is based on activities. You can monitor every activity.</p>
<p>In Python, This is how to get the activity you want:</p>
<p><a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.operations.activityrunsoperations?view=azure-python#query-by-pipeline-run-resource-group-name--factory-name--run-id--filter-parameters--custom-headers-none--raw-false----operation-config-"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.operations.activityrunsoperations?view=azure-python#query-by-pipeline-run-resource-group-name--factory-name--run-id--filter-parameters--custom-headers-none--raw-false----operation-config-</a></p>
<p>The below is to get the duration time of azure datafactory activity:</p>
<p><a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.activityrun?view=azure-python#variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.activityrun?view=azure-python#variables</a></p>
<p>(There is a variable named duration_in_ms, you can use this to get the duration time of the activity run.)</p>
<p>This is use Python to monitor pipeline:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#python</a></p>
<p>You can create a azure function app with a timetrigger to monitor the azure datafactory activity. This is the document of azure function timetrigger:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-timer?tabs=python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-timer?tabs=python</a></p>
<p>The basic idea is put the code that monitor the pipeline whether run more than N minutes in the logic body of azure function timetrigger. And then use the status of the azure function to reflect whether the pipeline running time of azure datafactory exceeds N hours.</p>
<p>Then use the alarm event of the azure function. The alarm events supported by azure for the azure function are as follows: (You can set an output binding of your azure function.)</p>
<p><a href=""https://i.stack.imgur.com/wcjfO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wcjfO.png"" alt=""enter image description here"" /></a></p>
<p>In azure portal, you can find the alert in this place:</p>
<p><a href=""https://i.stack.imgur.com/WB3k9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WB3k9.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Tlvc1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tlvc1.png"" alt=""enter image description here"" /></a></p>
<p>(Select Email/SMS message as the action type and give it your email address.)</p>
"
"62457704","Azure Data Flow creating / managing keys for identity relationships","<p>Curious to find out what the best way is to generate relationship identities through ADF.</p>

<p>Right now, I'm consuming JSON data that does not have any identity information. This data is then transformed into multiple database sink tables with relationships (1..n, etc.). Due to FK constraints on some of the destination sink tables, these relationships need to be ""built up"" one at a time.</p>

<p>This approach seems a bit kludgy, so I'm looking to see if there are other options that I'm not aware of.</p>

<p>Note that I need to include the Surrogate key generation for each insert. If I do not do this, based on output database schema, I'll get a 'cannot insert PK null' error.</p>

<p>Also note that I turn <code>IDENTITY_INSERT</code> ON/OFF for each sink.</p>

<p><a href=""https://i.stack.imgur.com/5C3Mf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5C3Mf.png"" alt=""enter image description here""></a></p>
","<azure><azure-devops><azure-sql-database><azure-data-factory>","2020-06-18 19:13:13","1016","1","1","62460836","<p>I would tend to take more of an ELT approach and use the native JSON abilites in Azure SQL DB, ie <code>OPENJSON</code>.  You could land the JSON in a table in Azure SQL DB using ADF (eg a Stored Proc activity) and then call another stored proc to process the JSON, something like this:</p>

<pre><code>-- Setup
DROP TABLE IF EXISTS #tmp
DROP TABLE IF EXISTS import.City;
DROP TABLE IF EXISTS import.Region;
DROP TABLE IF EXISTS import.Country;
GO

DROP SCHEMA IF EXISTS import 
GO

CREATE SCHEMA import
    CREATE TABLE Country ( CountryKey INT IDENTITY PRIMARY KEY, CountryName VARCHAR(50) NOT NULL UNIQUE )
    CREATE TABLE Region ( RegionKey INT IDENTITY PRIMARY KEY, CountryKey INT NOT NULL FOREIGN KEY REFERENCES import.Country, RegionName VARCHAR(50) NOT NULL UNIQUE )
    CREATE TABLE City ( CityKey INT IDENTITY(100,1) PRIMARY KEY, RegionKey INT NOT NULL FOREIGN KEY REFERENCES import.Region, CityName VARCHAR(50) NOT NULL UNIQUE )
GO


DECLARE @json NVARCHAR(MAX) = '{
   ""Cities"": [
      {
         ""Country"": ""England"",
         ""Region"": ""Greater London"",
         ""City"": ""London""
      },
      {
         ""Country"": ""England"",
         ""Region"": ""West Midlands"",
         ""City"": ""Birmingham""
      },
      {
         ""Country"": ""England"",
         ""Region"": ""Greater Manchester"",
         ""City"": ""Manchester""
      },
      {
         ""Country"": ""Scotland"",
         ""Region"": ""Lothian"",
         ""City"": ""Edinburgh""
      }
   ]
}'


SELECT *
INTO #tmp
FROM OPENJSON( @json, '$.Cities' )
WITH
(
    Country     VARCHAR(50),
    Region      VARCHAR(50),
    City        VARCHAR(50)
)
GO


-- Add the Country first (has no foreign keys)
INSERT INTO import.Country ( CountryName )
SELECT DISTINCT Country
FROM #tmp s
WHERE NOT EXISTS ( SELECT * FROM import.Country t WHERE s.Country = t.CountryName )


-- Add the Region next including Country FK
INSERT INTO import.Region ( CountryKey, RegionName )
SELECT t.CountryKey, s.Region
FROM #tmp s
    INNER JOIN import.Country t ON s.Country = t.CountryName


-- Now add the City with FKs
INSERT INTO import.City ( RegionKey, CityName )
SELECT r.RegionKey, s.City
FROM #tmp s
    INNER JOIN import.Country c ON s.Country = c.CountryName
    INNER JOIN import.Region r ON s.Region = r.RegionName
        AND c.CountryKey = r.CountryKey


SELECT * FROM import.City;
SELECT * FROM import.Region;
SELECT * FROM import.Country;
</code></pre>

<p>This is a simple test script designed to show the idea and should run end-to-end but it is not production code.</p>
"
"62448349","Sending Office 365 Connector Card to MS Teams from Azure Data Factory v2","<p>I'm trying to use ADFv2's web hook functionality to send a message to Teams using the Office 365 connector card.</p>

<p>I have setup a webhook in Teams and can successfully send simple JSON payloads from ADF</p>

<p>for example </p>

<pre><code>{""text"":""Hello World!""}
</code></pre>

<p>But if I try the Office 365 Connector message example from the documentation ADFv2 does not recognize the payload as being valid JSON. </p>

<p>Anyone have any idea how I can structure the json message below in a way that ADFv2 will accept?</p>

<p>I've tried
- building the string dynamically and wrapping in @json function - which complains that the passed string is not valid json
- removing the @ symbol - which gives a bad request error</p>

<pre><code>{
    ""@type"": ""MessageCard"",
    ""@context"": ""http://schema.org/extensions"",
    ""themeColor"": ""0076D7"",
    ""summary"": ""Larry Bryant created a new task"",
    ""sections"": [{
        ""activityTitle"": ""![TestImage](https://47a92947.ngrok.io/Content/Images/default.png)Larry Bryant created a new task"",
        ""activitySubtitle"": ""On Project Tango"",
        ""activityImage"": ""https://teamsnodesample.azurewebsites.net/static/img/image5.png"",
        ""facts"": [{
            ""name"": ""Assigned to"",
            ""value"": ""Unassigned""
        }, {
            ""name"": ""Due date"",
            ""value"": ""Mon May 01 2017 17:07:18 GMT-0700 (Pacific Daylight Time)""
        }, {
            ""name"": ""Status"",
            ""value"": ""Not started""
        }],
        ""markdown"": true
    }],
    ""potentialAction"": [{
        ""@type"": ""ActionCard"",
        ""name"": ""Add a comment"",
        ""inputs"": [{
            ""@type"": ""TextInput"",
            ""id"": ""comment"",
            ""isMultiline"": false,
            ""title"": ""Add a comment here for this task""
        }],
        ""actions"": [{
            ""@type"": ""HttpPOST"",
            ""name"": ""Add comment"",
            ""target"": ""http://...""
        }]
    }, {
        ""@type"": ""ActionCard"",
        ""name"": ""Set due date"",
        ""inputs"": [{
            ""@type"": ""DateInput"",
            ""id"": ""dueDate"",
            ""title"": ""Enter a due date for this task""
        }],
        ""actions"": [{
            ""@type"": ""HttpPOST"",
            ""name"": ""Save"",
            ""target"": ""http://...""
        }]
    }, {
        ""@type"": ""ActionCard"",
        ""name"": ""Change status"",
        ""inputs"": [{
            ""@type"": ""MultichoiceInput"",
            ""id"": ""list"",
            ""title"": ""Select a status"",
            ""isMultiSelect"": ""false"",
            ""choices"": [{
                ""display"": ""In Progress"",
                ""value"": ""1""
            }, {
                ""display"": ""Active"",
                ""value"": ""2""
            }, {
                ""display"": ""Closed"",
                ""value"": ""3""
            }]
        }],
        ""actions"": [{
            ""@type"": ""HttpPOST"",
            ""name"": ""Save"",
            ""target"": ""http://...""
        }]
    }]
}
</code></pre>
","<office365><azure-data-factory><microsoft-teams>","2020-06-18 10:54:56","300","0","1","62685794","<p>As Trinetra-MSFT answered :-</p>
<p>&quot;ADFv2 don't support Message Card, they support simple JSON. Message card supposed to work for Incoming webhook created within Teams or outlook.&quot;</p>
"
"62446067","Azure Data Factory Lookup Source Data and Mail Notification","<p>I am trying my best to solve following scenario.
I am using PowerShell scripts to collect some information about my server environments and saving like .csv files. 
There are information about Hardware, Running Services etc. in the .csv files.
I am sending these .csv files into Blob Storage and using Azure Data Factory V2 Pipelines to write these information into Azure SQL. I have succesfully configured mail notification via Azure Logic Apps that is informing me the Pipeline Run was succesfull/unsuccesfull.
Now I am trying to lookup into source data to find concrete column. In my scenario it is column with the name of Windows Service - for example - Column: PrintSpooler - Row: Running.
So I need to lookup for concretely column and also send a mail notification if the service is running or it is stopped.
Is there any way how to do that ? 
In ideal way I want to receive a mail only in case the Service in my Source Data is stopped.
Thank you for any ideas.</p>
","<azure><azure-pipelines><azure-data-factory><azure-logic-apps>","2020-06-18 08:52:33","180","0","1","62532120","<p>Do you update the .csv file or upload a new .csv file?</p>
<p>If you upload a new .csv, then you can use azure function blob trigger.</p>
<p>This trigger will collect the new upload blob and you can do process on this blob. You can get the data in the .csv file and create an alert to your email.</p>
<p>This is the offcial document of azure function timetrigger:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-scheduled-function"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-scheduled-function</a></p>
<p>In the blobtrigger, you can search whether there is a value in the .csv file and then you can set an output binding.</p>
<p>And then, go to this place:</p>
<p><a href=""https://i.stack.imgur.com/WB3k9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WB3k9.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Tlvc1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tlvc1.png"" alt=""enter image description here"" /></a></p>
<p>Then you will get the alert in your email when the data in csv file is meet your requirement.</p>
"
"62440239","Azure Data Factory - Manual Trigger Execution Time","<p>I've been checking some of the results provided by ADF. I'm new with this tool, and I would like to understand more about the execution time that I see after running the ETL with the trigger or the debug option.</p>

<p><a href=""https://i.stack.imgur.com/ur7AW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ur7AW.png"" alt=""enter image description here""></a></p>

<p>My question is, why the difference at the end of the process? Debug is almost 25% of the other. What happens when I execute an ETL with the trigger option that increases the processing time comparing with the debug.</p>
","<debugging><triggers><azure-data-factory><execution-time>","2020-06-18 00:00:58","1088","1","1","62529332","<p>Yes, if you put the dataflow in a pipeline and start data flow debug in preview, then you will see something like this:</p>
<p><a href=""https://i.stack.imgur.com/DsmWR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DsmWR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ykPnK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ykPnK.png"" alt=""enter image description here"" /></a></p>
<p>Trigger pipeline seems take more times than debug it, but this does not mean that debug has better performance. You just dont count the start time of data flow debug module:</p>
<p><a href=""https://i.stack.imgur.com/HPJWv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HPJWv.png"" alt=""enter image description here"" /></a></p>
<p>The time that the trigger seems take more time than debug is because the dataflow debug module. Please have a look of this doc:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#monitoring-data-flow-performance"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#monitoring-data-flow-performance</a></p>
<p>When you trigger a pipeline, you need to initialize the Integration Runtime. This will take a lot of time. Why you debug the pipeline seems take less time is because you have already start the dataflow debug module. You can try to close this debug module, and then you will find debug also take about 5~6 minutes:</p>
<p><a href=""https://i.stack.imgur.com/3IP0k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3IP0k.png"" alt=""enter image description here"" /></a></p>
"
"62436275","ADF DataFlow toFloat function","<p>I have try to use ADF dataflow to convert a column with data like '630.180004119873' to float data type using toFlaot() function, however when output i can see the data been converted to '630.18'.</p>

<p>Is there anyone have idea how to prevent ADF DataFlow toFloat function to keep the result as '630.180004119873' instead of converted to '630.18'? </p>

<p>The code are as below:
<a href=""https://i.stack.imgur.com/Wgkh6.png"" rel=""nofollow noreferrer"">ADF Derived Column Settings</a></p>
","<azure><azure-data-lake><azure-data-factory>","2020-06-17 18:50:36","1207","0","1","62441049","<p>Actually, we can't use <code>toFloat</code> do the conversion. You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#todouble"" rel=""nofollow noreferrer"">toDouble</a> 
 or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#todecimal"" rel=""nofollow noreferrer"">toDecimal</a>:
<a href=""https://i.stack.imgur.com/S2LBp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2LBp.png"" alt=""enter image description here""></a></p>

<p>For example:</p>

<p><strong>toDecimal:</strong> you can customize the precision and scale.
<a href=""https://i.stack.imgur.com/wNreC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wNreC.png"" alt=""enter image description here""></a></p>

<p><strong>toDouble:</strong>
<a href=""https://i.stack.imgur.com/PlBKU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PlBKU.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"62432943","Is there any emulator for Azure data factory ,azure sql db and Synapse DB","<p>In our project, we are migrating data from data lakes (for which we have emulator) to azure sql db and azure synapse using data factory. 
Do we have any emulator for Azure data factory ,azure sql db  and Synapse DB?
We don't want to spend money on Dev effort. Is there any way we can develop it locally?</p>

<p>Similarly for Azure data bricks.</p>

<p>Regards,
Vikas</p>
","<azure-sql-database><azure-data-factory><azure-databricks><azure-synapse>","2020-06-17 15:42:00","1018","0","1","62484999","<p>I know that there is an emulator for CosmosDB, but I have never heard about its counterpart for ADF, SQLDB or Synapse (former Azure SQL DW).<br />
To be honest - for DEV environment - <strong>ADF</strong> should be pretty cheap as you will not pay for instance, only for activity.<br />
When it comes to <strong>Azure SQL DB</strong> - you can scale up (when using) and scale down once you finished working with it. Or, alternatively, you can choose SQL serverless, which is much cheaper as you're paying for usage only.<br />
In terms of <strong>Azure Synapse</strong> - the most expensive part is the computing layer, so just remember to stop/start service when needed.</p>
"
"62425912","Azure-data-Factory Copy data If a certain file exists","<p>I have many files in a blob container. However I wanted to run a Stored procedure only IF a certain file (e.g. SRManifest.csv) exists on the blob Container. I used Get metadata and IF Condition on Data Factory. Can you please help me with the dynamic script for this. I tried this <code>@bool(startswith(
activity('Get Metadata1').output.childitems.ItemName,
'SRManifest.csv'))</code>. It doesnt work.</p>

<p>Then I thought, what if i used <code>@greaterOREquals(activity('Get Metadata1').output.LastModified,adddays(utcnow(),-2))</code>But this checks the last modified within 2 days of the Bloob not the file exist. Thank you.</p>

<p>Please see below my diagram<a href=""https://i.stack.imgur.com/7ANYs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7ANYs.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-06-17 09:35:42","12000","2","2","62436120","<p>Based on your diagram, since you are looping over all the blob names already, you can add a Boolean variable to the pipeline and set its default value to <em>false</em>:</p>

<p><a href=""https://i.stack.imgur.com/RUj2j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RUj2j.png"" alt=""enter image description here""></a></p>

<p>Inside the ForEach activity, you only want to attempt to set the variable if the value is still <em>false</em>, and if the blob name is found, set it to <em>true</em>. Since Set Variable cannot be self-referential, do this inside the False branch of an If activity:</p>

<p><a href=""https://i.stack.imgur.com/uJQ5X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uJQ5X.png"" alt=""enter image description here""></a></p>

<p>This will only attempt to process if the value is <em>false</em> (so the file name has not been found yet), and will do nothing if the value is <em>true</em>. Now set the variable based on your file name:</p>

<p><a href=""https://i.stack.imgur.com/tyNCY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tyNCY.png"" alt=""enter image description here""></a></p>

<p>[<strong>NOTE</strong>: This value can be hard coded, parameterized, or based on a variable]</p>

<p>When you execute the pipeline, you'll see the Set Variable stops attempting once the value is set to <em>true</em>:</p>

<p><a href=""https://i.stack.imgur.com/Y2NtJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y2NtJ.png"" alt=""enter image description here""></a></p>

<p>In the main pipeline, after the ForEach activity has completed, you can use the variable to set the condition of your final If activity. If the blob is never found, it will still be <em>false</em>, so put the Stored Procedure activity inside the True branch.</p>
"
"62425912","Azure-data-Factory Copy data If a certain file exists","<p>I have many files in a blob container. However I wanted to run a Stored procedure only IF a certain file (e.g. SRManifest.csv) exists on the blob Container. I used Get metadata and IF Condition on Data Factory. Can you please help me with the dynamic script for this. I tried this <code>@bool(startswith(
activity('Get Metadata1').output.childitems.ItemName,
'SRManifest.csv'))</code>. It doesnt work.</p>

<p>Then I thought, what if i used <code>@greaterOREquals(activity('Get Metadata1').output.LastModified,adddays(utcnow(),-2))</code>But this checks the last modified within 2 days of the Bloob not the file exist. Thank you.</p>

<p>Please see below my diagram<a href=""https://i.stack.imgur.com/7ANYs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7ANYs.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-06-17 09:35:42","12000","2","2","62445558","<p>I have understood your requirement differently I think.</p>

<p><em>I wanted to run a Stored procedure only IF a certain file (e.g. SRManifest.csv) exists on the blob Container</em></p>

<p>1 Change your metadata activity to look for existence of sentinel file (SRManifest.csv)</p>

<p><a href=""https://i.stack.imgur.com/LunTq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LunTq.png"" alt=""enter image description here""></a></p>

<p>2 Follow with an IF activity, use this condition:</p>

<p><a href=""https://i.stack.imgur.com/3qKza.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3qKza.png"" alt=""enter image description here""></a></p>

<p>3 Put your sp in the True part of the IF activity</p>

<p>If you also needed the file list passed to the sp then you'll need the GetMetadata with childitems option inside the IF-True activity</p>
"
"62424805","Terraform: Blob linked service in Data Factory","<p>Can anyone help me on how to create azure blob linked service in data factory using terraform script? I can see there are linked services of keyvault, postgresql, sql server, mysql, datalake but not storage.</p>
","<terraform><azure-data-factory><terraform-provider-azure>","2020-06-17 08:36:16","703","-1","1","62462079","<p>It seems that there is no built-in terraform block for creating azure blob linked service in data factory. As a workaround, you can create linked services by using one of these tools or SDKs: .NET API, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services#create-linked-services"" rel=""nofollow noreferrer"">PowerShell</a>, REST API, Azure Resource Manager Template, and Azure portal.</p>

<p>In this case, with terraform, you can use <a href=""https://www.terraform.io/docs/provisioners/local-exec.html"" rel=""nofollow noreferrer"">local-exec Provisioner</a> to invoke a local executable after a resource is created.</p>

<p>If you are interested in looking at terraform resource <a href=""https://www.terraform.io/docs/providers/azurerm/r/data_factory_linked_service_data_lake_storage_gen2.html"" rel=""nofollow noreferrer"">azurerm_data_factory_linked_service_data_lake_storage_gen2</a>. You can link to
Azure Data Lake Storage Gen2.</p>

<blockquote>
  <p>url - (Required) The endpoint for the Azure Data Lake Storage Gen2
  service.</p>
</blockquote>
"
"62424710","Azure Data Factory Analytics pricing","<p>Due to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#monitor-data-factory-metrics"" rel=""nofollow noreferrer"">official doc</a>, in order to get activity-grain log events about ADF pipelines, I have to configure <a href=""https://azuremarketplace.microsoft.com/en-us/marketplace/apps/Microsoft.AzureDataFactoryAnalytics?src=azserv&amp;tab=Reviews"" rel=""nofollow noreferrer"">Azure Data Factory Analytics</a>:</p>

<p><a href=""https://i.stack.imgur.com/MD5wY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MD5wY.png"" alt=""enter image description here""></a></p>

<p>Without ADF Analytics I can get events only of pipeline grain, which doesn't solve my audit requirements.</p>

<p><em>Is ADF Analytics free to use? If no - what is the pricing? Shall I be aware about any billing specific of Azure Marketplace?</em> Currently I can't find any pricing details. </p>
","<azure><azure-data-factory>","2020-06-17 08:30:37","534","0","1","62441243","<p>ADF Analytics is charged. The pricing is <code>0.25$ per 50,000 entities</code>. You can see the pricing via <a href=""https://azure.microsoft.com/en-us/pricing/calculator/?service=data-factory"" rel=""nofollow noreferrer"">Azure Pricing Calculator</a>.</p>

<p>Add a screenshot for it:</p>

<p><a href=""https://i.stack.imgur.com/isoiD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/isoiD.jpg"" alt=""enter image description here""></a></p>
"
"62418857","Azure data factory - convert YYYYDDMMmmHHss to DDMMYYYYHHmmSS","<p>how to convert  YYYYDDMMmmHHss to DDMMYYYYmmHHSS  in Azure data factory file name output</p>

<p>Regards
Ravi</p>
","<azure><azure-data-factory>","2020-06-16 22:58:43","80","0","1","62421517","<p>I tried this and my source and sink are both csv.
This is my file name: <code>20201706170905.csv</code>.</p>

<p>First:create a getMetaData activity like this</p>

<p><a href=""https://i.stack.imgur.com/ov88J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ov88J.png"" alt=""getMetaData""></a></p>

<p>Then:create a copy activity,and the sink dataset filename set like this:</p>

<p><a href=""https://i.stack.imgur.com/X0ifE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X0ifE.png"" alt=""copy""></a></p>

<p><a href=""https://i.stack.imgur.com/ebEDJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ebEDJ.png"" alt=""dataset""></a></p>

<p>The expression in file name(you can use concat and substring to convert what you want):</p>

<pre><code>@concat(substring(activity('Get Metadata1').output.itemName,4,4),substring(activity('Get Metadata1').output.itemName,0,4),substring(activity('Get Metadata1').output.itemName,10,2),substring(activity('Get Metadata1').output.itemName,8,2),substring(activity('Get Metadata1').output.itemName,12,6))
</code></pre>

<p>Finally: run the pipeline(if you don't need the Original file,you can use delete activity delete it).</p>

<p>Result:</p>

<p><a href=""https://i.stack.imgur.com/xi8CK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xi8CK.png"" alt=""result""></a></p>

<p>Hope this can help you.</p>
"
"62415030","Azure Data Factory difference between duration time","<p>I'm brand new with Azure Data Factory. Previously I've been working with SSIS and Pentaho. Recently I have started using this tool to create some ETL, and I've noticed some differences between the time values provided at the end of the process. So I wonder what they mean (Duration - Processing Time - Time), and especially why the big difference between Duration and Processing Time, is this difference a standard preparation time for the tool or something like that?</p>

<p><a href=""https://i.stack.imgur.com/aW9dR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aW9dR.png"" alt=""enter image description here""></a></p>

<p>Regards.</p>
","<etl><azure-data-factory><execution-time>","2020-06-16 18:12:01","334","0","1","62416498","<p>When you read the ""Duration"" time from the top of your screenshot, that it is end-to-end for the pipeline activity. That takes into account all factors like marshaling of your data flow script from ADF to the Spark cluster, cluster acquisition time, job execution, and I/O write time.</p>

<p>The bottom section of your screenshot is the amount of time Spark spent in that stage of your transformation logic, which is all in-memory data frames.</p>

<p>The write time is shown in the data flow execution plan in the Sink transformation and the cluster acquisition time is shown at the top.</p>
"
"62410485","Azure Data Factory merge parquet files by folder","<p>I have an ADLS Gen2 Account(HNS enabled) with parquet files in this format:</p>

<pre><code>-MainFolder
 -SubFolder 1
 -SubFolder 2
   -Year
   -Month
   -Day
     -Parquet file 01
     -Parquet file 02
     -...
</code></pre>

<p>I want to use Azure Data Factory to combine the parquet files on the lowest level into one file, final structure should look like this.</p>

<pre><code>-MainFolder
 -SubFolder 1
 -SubFolder 2
   -Year
   -Month
   -Day
     -Merged Parquet File
</code></pre>

<p>If I use ""Copy Data"" Activiety I can only choose between ""Merge Files"" and ""Preserve Hirachie"".
Is there away to do this?
Thank you for your help!</p>
","<azure><azure-data-factory>","2020-06-16 14:06:51","3739","0","1","62416563","<p>If Merge Files doesn't work for you in Copy Activity, you can use Data Flow and the Union transformation can combine multiple files into a single file output.</p>
"
"62405173","How to execute SQL scripts (SQL Dacpac file) in Azure Data Factory v2?","<p>I have a requirement to create SQL tables in the Azure SQL Database and then load the data to those tables from CSV files in the Blob storage. The tables needs to created and then dropped after the completion of process. </p>

<p>We usually use the dacpac to deploy the database objects from Azure Devops pipeline.
Is there a way to execute these dacpac from ADF? 
or Is there any activity in the Azure Data Factory v2 to execute the scripts, if I maintain these scripts in the storage?</p>

<p>Any suggestions are much appreciated!</p>
","<azure><azure-sql-database><azure-data-factory>","2020-06-16 09:20:41","426","0","1","62420551","<p>As I know about Data Factory, we can not import the DACPAC file to SQL database directly. </p>

<p>Data Factory will consider the dacpac as a file, not a SQL script.</p>

<p>And Azure Data Factory only supports the following file formats. Refer to each article for format-based settings.:
<a href=""https://i.stack.imgur.com/lXkPo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lXkPo.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"62404320","ADF remove line break from column","<p>I'm trying to do a copy activity in Azure Data Factory, here is how the data looks</p>

<p>Source: Dynamics 365 entity</p>

<pre><code>A(GUID) | B(boolean) | C(string)
04741b89-3d51-ea11-a811-000d3af427b4| False | ""some text \n\n next line \n new line""
</code></pre>

<p>Sink: Azure Blob delimited text file</p>

<pre><code>A(string) | B(string) | C(string)
04741b89-3d51-ea11-a811-000d3af427b4| False | ""some text

next line
new line""
</code></pre>

<p>I have set the sink mapping in copy activity to string. But the result does not add quotes to the values of A and B field. </p>

<p>Expected result:</p>

<pre><code>""04741b89-3d51-ea11-a811-000d3af427b4""| ""False"" | ""some text next line new line""
</code></pre>

<p>How do i remove the line breaks?</p>
","<azure><azure-data-factory>","2020-06-16 08:35:38","4643","0","1","62406936","<p>I solved it by adding a dataflow activity before copy activity
In dataflow activity i added a derived column transform
<a href=""https://i.stack.imgur.com/jXGPy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jXGPy.png"" alt=""enter image description here""></a></p>

<p>In derived column settings mention the column to transform. I used a string replace expression to replace \n with empty string.</p>

<p><a href=""https://i.stack.imgur.com/jI7xg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jI7xg.png"" alt=""enter image description here""></a></p>
"
"62404140","How to Update Table in Snowflake using Azure Data Factory","<p>I have two tables in snowflake named table1 and table2. Table1 is the source table which contains incremental data and table2 is the target table.</p>

<p>So my usecase is I have to take data from table1 and update the data into table2 but this process has to be done using Azure Data Factory.</p>

<p>I tried to create a data flow in ADF but it didn't allowed me to connect with the snowflake directly as it is not in the supported sources list. The native snowflake connector only supports the Copy Data Activity. So as a work around I first created a copy activity which copy the data from snowflake to azure blob. Then used the Azure Blob as source for Data Flow to create my scd1 implementation and saved the output in csv files.</p>

<p>Now My question is how should I update the data in target table2. Because If I directly use the copy activity to copy the csv files into snowflake then it will result in the duplicate records at snowflake side. For instance lets say table2 contains a row</p>

<pre><code>id,name,age,data
1234,kristopher,24,somedata
</code></pre>

<p>and table1 contains</p>

<pre><code>id,name,age,data
1234,kristopher,24,some-new-data
</code></pre>

<p>So now I have table1 data in csv which has to be loaded in snowflake. If I am loading directly then the resultant looks something like this.</p>

<pre><code>id,name,age,data
1234,kristopher,24,somedata
1234,kristopher,24,some-new-data
</code></pre>

<p>But I only need</p>

<pre><code>1234,kristopher,24,some-new-data
</code></pre>

<p>Let me know if some more explanation is required. I am new to Azure Data Factory and Snowflake as well.</p>

<p>Thanks</p>
","<snowflake-cloud-data-platform><azure-data-factory><snowflake-schema><scd>","2020-06-16 08:26:03","818","0","2","62410620","<p>As you have observed, the ADF Data Flows currently don't support Snowflake datasets as a source. </p>

<p>You could theoretically follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-powershell"" rel=""nofollow noreferrer"">this</a> design pattern but it seems like alot of work for the requirement you have described. An alternative would be to go down the Azure Function route, but again I would trade off the requirement vs. effort required. </p>

<p>If it didn't have to be in ADF, then a quick approach would be to use a <a href=""https://docs.snowflake.com/en/sql-reference/sql/create-task.html"" rel=""nofollow noreferrer"">Snowflake Task</a> to schedule some SQL to manage the SCD behavior for you.</p>

<p>I hope this helps.</p>

<p>Best regards, 
Dan. </p>
"
"62404140","How to Update Table in Snowflake using Azure Data Factory","<p>I have two tables in snowflake named table1 and table2. Table1 is the source table which contains incremental data and table2 is the target table.</p>

<p>So my usecase is I have to take data from table1 and update the data into table2 but this process has to be done using Azure Data Factory.</p>

<p>I tried to create a data flow in ADF but it didn't allowed me to connect with the snowflake directly as it is not in the supported sources list. The native snowflake connector only supports the Copy Data Activity. So as a work around I first created a copy activity which copy the data from snowflake to azure blob. Then used the Azure Blob as source for Data Flow to create my scd1 implementation and saved the output in csv files.</p>

<p>Now My question is how should I update the data in target table2. Because If I directly use the copy activity to copy the csv files into snowflake then it will result in the duplicate records at snowflake side. For instance lets say table2 contains a row</p>

<pre><code>id,name,age,data
1234,kristopher,24,somedata
</code></pre>

<p>and table1 contains</p>

<pre><code>id,name,age,data
1234,kristopher,24,some-new-data
</code></pre>

<p>So now I have table1 data in csv which has to be loaded in snowflake. If I am loading directly then the resultant looks something like this.</p>

<pre><code>id,name,age,data
1234,kristopher,24,somedata
1234,kristopher,24,some-new-data
</code></pre>

<p>But I only need</p>

<pre><code>1234,kristopher,24,some-new-data
</code></pre>

<p>Let me know if some more explanation is required. I am new to Azure Data Factory and Snowflake as well.</p>

<p>Thanks</p>
","<snowflake-cloud-data-platform><azure-data-factory><snowflake-schema><scd>","2020-06-16 08:26:03","818","0","2","62685762","<p>you can put your login in a snowflake stored procedure, then execute your stored proc in ADF</p>
"
"62401539","ADF v2: Define keys for upsert behavior","<p>I'm trying to create a copy activity that copies from SQL to Dynamics CRM. I'm using the ADF v2 default connector. The Dynamics connectors as a sink have the upsert behavior but I don't know how to define the keys for comparing in the upsert. If I complete the configuration and the mapping, removing the unnecessary columns and I run the activity, I have the following error: <code>Input DataSet must contain key column(s) in Upsert/Update scenario. Missing key column(s):</code> Is there any way to set the keys in the input dataset? My mapping looks like: </p>

<p><a href=""https://i.stack.imgur.com/WY5kB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WY5kB.png"" alt=""enter image description here""></a></p>

<p>This field is always empty: 
<a href=""https://i.stack.imgur.com/DQtjF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DQtjF.png"" alt=""enter image description here""></a></p>

<p>Thanks!</p>
","<dynamics-crm><etl><azure-data-factory><upsert>","2020-06-16 05:32:50","1017","1","2","70840706","<p>If your source is a SQL query, you can add &quot;NewID() as ID&quot; to your query.   Then map ID to unique identifier column.</p>
<p>NewId() is specific to Azure SQL and Microsoft SQL Server.</p>
"
"62401539","ADF v2: Define keys for upsert behavior","<p>I'm trying to create a copy activity that copies from SQL to Dynamics CRM. I'm using the ADF v2 default connector. The Dynamics connectors as a sink have the upsert behavior but I don't know how to define the keys for comparing in the upsert. If I complete the configuration and the mapping, removing the unnecessary columns and I run the activity, I have the following error: <code>Input DataSet must contain key column(s) in Upsert/Update scenario. Missing key column(s):</code> Is there any way to set the keys in the input dataset? My mapping looks like: </p>

<p><a href=""https://i.stack.imgur.com/WY5kB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WY5kB.png"" alt=""enter image description here""></a></p>

<p>This field is always empty: 
<a href=""https://i.stack.imgur.com/DQtjF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DQtjF.png"" alt=""enter image description here""></a></p>

<p>Thanks!</p>
","<dynamics-crm><etl><azure-data-factory><upsert>","2020-06-16 05:32:50","1017","1","2","73548772","<p>I received <a href=""https://learn.microsoft.com/en-us/answers/questions/36657/adf-v2-define-keys-for-upsert-behavior.html"" rel=""nofollow noreferrer"">answer</a> in Microsoft Q&amp;A.</p>
<p>The idea is that we need to define an alternate key on the Dynamics side. Once this is done the key will be showed on the options.</p>
<p>Thanks</p>
"
"62401304","Throttle Bandwidth for a copy activity in ADF","<p>I need to limit a download speed of one self hosted IR in Azure to an on premise server to prevent the network to get clogged up.
What are my options here? Is it possible to set this is ADF directly or in the IR or do I have to set this in the network?</p>
","<azure><azure-data-factory><bandwidth-throttling><azure-vm>","2020-06-16 05:09:39","922","1","1","67055107","<p>According to this <a href=""https://social.msdn.microsoft.com/Forums/en-US/500ae957-7221-4b35-b063-2b861105c7cd/throttle-data-factory-bandwith"" rel=""nofollow noreferrer"">MSDN thread</a> it's not possible to throttle bandwidth natively in Azure Data Factory.</p>
<p>However, if you are using an Azure Data Factory Self-Hosted Integration Runtime you could probably throttle the bandwidth at the VM level. For example, for Windows VMs you could try a <a href=""http://woshub.com/limit-network-file-transfer-speed-windows/"" rel=""nofollow noreferrer"">QoS Group Policy</a> to throttle the network for all applications or just the Integration Runtime executable.</p>
"
"62389182","ADF add quotes to all fields","<p>I'm trying to do a copy activity in Azure Data Factory, here is how the data looks</p>

<p>Source: Dynamics 365 entity</p>

<pre><code>A(GUID) | B(boolean) | C(string)
04741b89-3d51-ea11-a811-000d3af427b4| False | ""some text""
</code></pre>

<p>Sink: Azure Blob delimited text file</p>

<pre><code>A(string) | B(string) | C(string)
04741b89-3d51-ea11-a811-000d3af427b4| False | ""some text""
</code></pre>

<p>I have set the sink mapping in copy activity to string. But the result does not add quotes to the values of A and B field. </p>

<p>Expected result:</p>

<pre><code>""04741b89-3d51-ea11-a811-000d3af427b4""| ""False"" | ""some text""
</code></pre>

<p>How can i add quotes to field A and B?
<a href=""https://i.stack.imgur.com/ILbpA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ILbpA.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-06-15 13:24:56","1472","0","2","62402869","<p><strong>How can I add quotes to field A and B?</strong></p>

<p>Since you already have double quote in column data, I would suggest you using single quote:</p>

<p>Sink dataset settings:
<a href=""https://i.stack.imgur.com/prP69.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/prP69.png"" alt=""enter image description here""></a></p>

<p>Sink:
<a href=""https://i.stack.imgur.com/jPI4z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jPI4z.png"" alt=""enter image description here""></a></p>

<p>Then the output file would be like:
<a href=""https://i.stack.imgur.com/oXKOZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oXKOZ.png"" alt=""enter image description here""></a></p>

<p>This will add quote to all the fields, include the first row.</p>

<p>Hope this helps.</p>
"
"62389182","ADF add quotes to all fields","<p>I'm trying to do a copy activity in Azure Data Factory, here is how the data looks</p>

<p>Source: Dynamics 365 entity</p>

<pre><code>A(GUID) | B(boolean) | C(string)
04741b89-3d51-ea11-a811-000d3af427b4| False | ""some text""
</code></pre>

<p>Sink: Azure Blob delimited text file</p>

<pre><code>A(string) | B(string) | C(string)
04741b89-3d51-ea11-a811-000d3af427b4| False | ""some text""
</code></pre>

<p>I have set the sink mapping in copy activity to string. But the result does not add quotes to the values of A and B field. </p>

<p>Expected result:</p>

<pre><code>""04741b89-3d51-ea11-a811-000d3af427b4""| ""False"" | ""some text""
</code></pre>

<p>How can i add quotes to field A and B?
<a href=""https://i.stack.imgur.com/ILbpA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ILbpA.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-06-15 13:24:56","1472","0","2","62404399","<p>I was able to solve this by adding a copy activity to json file first and then using the json file to copy to delimited text.</p>

<p><a href=""https://i.stack.imgur.com/FhqMa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FhqMa.png"" alt=""enter image description here""></a></p>
"
"62381934","Azure Data Factory - Content type JSON but response in XML","<p>Apologies if my question is too basic but I'm learning the ropes of Azure, so fairly new ... 
I need help with the following questions,</p>

<p>I've written a WebActivity which makes a GET request with the following two header values,</p>

<ul>
<li><p>Content-Type: application/JSON</p></li>
<li><p>Authorization: Bearer </p></li>
</ul>

<p>I'm expecting a response in JSON but unfortunately it's coming in an XML 
 format.  . Please refer to image attached,</p>

<p><strong>Request</strong></p>

<p><a href=""https://i.stack.imgur.com/9w8U1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9w8U1.jpg"" alt=""GET Request""></a></p>

<p><strong>Request RAW Format</strong></p>

<pre><code>{
""url"": ""https://someurl.com/data"",
""method"": ""GET"",
""headers"": {
    ""Content-Type"": ""application/json"",
    ""Authorization"": ""Bearer &lt;token&gt;""
}
</code></pre>

<p>}</p>

<p><strong>Response Received</strong>
Adding some part of response Received,</p>

<pre><code>{
""Response"": ""&lt;Fleet version=\""1\"" snapshotTime=\""2020-06-11T14:46:52Z\"" xmlns=\""http://standards.iso.org/iso/15143/-3\""&gt;&lt;Links&gt;&lt;rel&gt;self&lt;/rel&gt;&lt;href&gt;https://someurl.com/data&lt;/href&gt;&lt;/Links&gt;&lt;Links&gt;&lt;rel&gt;first&lt;/rel&gt;&lt;href&gt;https://someurl.com/data&lt;/href&gt;&lt;/Links&gt;&lt;Links&gt;&lt;rel&gt;last&lt;/rel&gt;&lt;href&gt;https://someurl.com/data&lt;/href&gt;&lt;/Links&gt;&lt;Equipment&gt;&lt;EquipmentHeader&gt;&lt;OemName&gt;ENIGMA&lt;/OemName&gt;&lt;EquipmentId&gt;1021&lt;/EquipmentId&gt;&lt;/EquipmentHeader&gt;&lt;Location datetime=\""2020-06-11T12:08:59Z\""&gt;&lt;Latitude&gt;52.3292240&lt;/Latitude&gt;&lt;Longitude&gt;-0.1976080&lt;/Longitude&gt;&lt;/Location&gt;&lt;CumulativeOperatingHours datetime=\""2020-06-11T15:02:30Z\""&gt;&lt;Hour&gt;5119.03&lt;/Hour&gt;&lt;/CumulativeOperatingHours&gt;&lt;/Equipment&gt;&lt;Equipment&gt;&lt;EquipmentHeader&gt;&lt;OemName&gt;ENIGMA&lt;/OemName&gt;&lt;EquipmentId&gt;1025&lt;/EquipmentId&gt;&lt;/EquipmentHeader&gt;&lt;Location datetime=\""2020-06-11T13:31:57Z\""&gt;&lt;Latitude&gt;52.3170160&lt;/Latitude&gt;&lt;Longitude&gt;- ...
</code></pre>

<p>Can someone help me in understanding if I'm doing something wrong here, using <strong>Postman</strong>, it works perfectly fine .. I've also tried Azure Logic Apps and get same XML response</p>

<p>Thank you in advance</p>
","<json><azure><azure-data-factory><azure-logic-apps>","2020-06-15 05:36:02","360","0","1","62381988","<p>The <code>Content-Type: application/json</code> means the request body is json type, but the response body doesn't have to be json type. You need to set another property in the header:</p>

<pre><code>Accept: application/json
</code></pre>

<p>Hope it helps~</p>
"
"62372543","Mapping Data Flows Error The stream is either not connected or column is unavailable","<p>I have a meta-data driven pipeline and a mapping data flow to load my data. When I try to run this pipeline, I get the following error.</p>

<pre><code>{""message"":""at Derive 'TargetSATKey'(Line 42/Col 26): Column 'PersonVID' not found. The stream is either not connected or column is unavailable. Details:at Derive 'TargetSATKey'(Line 42/Col 26): Column 'PersonVID' not found. The stream is either not connected or column is unavailable"",""failureType"":""UserError"",""target"":""Data Vault Load"",""errorCode"":""DFExecutorUserError""}
</code></pre>

<p>When I debug the mapping data flow, all the components in the data flow work as intended.</p>

<p>I guess that my source connection parameters aren't flowing through properly. Below is an email of my source connection</p>

<p><a href=""https://i.stack.imgur.com/SMhKI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SMhKI.png"" alt=""Source connection""></a></p>

<p><a href=""https://i.stack.imgur.com/hc1Cm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hc1Cm.png"" alt=""Pipeline error""></a></p>

<p>Please let me know if you have any thoughts and questions</p>
","<azure-data-factory>","2020-06-14 12:30:34","1008","0","1","63025929","<p>I found a resolution to my problem. The error was the data being passed in was a string but when the variable was unpacked my variable value didn't have a quote around it. Putting in the quotes fixed it.</p>
<p>For example</p>
<pre><code>'BusinessEntityID'
</code></pre>
<p>Please let me know if there are any questions</p>
"
"62371991","Azure Logic App and Data Flow - Remove ? and @ from JSON (XML)","<p>I have created a Logic App that converts XML to JSON</p>

<p><a href=""https://i.stack.imgur.com/0KkmA.png"" rel=""nofollow noreferrer"">LogicAppProcess</a></p>

<p>This work 'great', no issues in that part of the process - however, this retains some ? and @ symbols within some of the column names. This is then not working downstream in an Azure Data Flow as the @ symbol is being picked up a parameter..</p>

<p><a href=""https://i.stack.imgur.com/vSM58.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<pre><code>{""code"":""BadRequest"",""message"":""ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'version'"",""target"":""pipeline/PL_XXXX/runid/XXXX"",""details"":null,""error"":null}
</code></pre>

<p>Any ideas as to how i can either replace the ? and @ symbols at the first point in the process (Logic App) or accommodate for these in the Data Flow?</p>

<p>This is how the Data Flow code sees the error in the Script element - the ?xml is not throwing an error but the first @ is, and shows @version as the error</p>

<pre><code>""script"": ""source(output(\n\t\t{?xml} as ({@version} as string, {@encoding} as string),\n\t\trss as ({@version} as string, {@xmlns:cisAbstract} as string...
</code></pre>
","<azure-logic-apps><azure-data-factory>","2020-06-14 11:41:06","459","1","1","62381910","<p>You can use the <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/workflow-definition-language-functions-reference#replace"" rel=""nofollow noreferrer"">replace()</a> function in your logic app to replace the <code>?</code> and <code>@</code> with null string <code>""""</code>. If your data is not string type(such as json object or array or any other type), you can use <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/workflow-definition-language-functions-reference#string"" rel=""nofollow noreferrer"">string()</a> function to convert them to string first and then do the replace() function. Shown as below expression:</p>

<pre><code>replace(replace(string(&lt;your data&gt;), '@', ''), '?', '')
</code></pre>
"
"62359747","Create SQL target tables automatically in ADF","<p>I am new to Data Factory.
I would like to copy data from REST API into SQL Server tables.
Is there a way to automatically create SQL tables with data types based on the API calls? I don't want to do this manually.
Thanks!</p>
","<azure-data-factory>","2020-06-13 12:27:08","232","0","1","62379984","<p>We can't auto create SQL table with Rest API Source:
<a href=""https://i.stack.imgur.com/MZ2SG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MZ2SG.png"" alt=""enter image description here""></a></p>

<p>We will get the bellow error:</p>

<pre><code>Table option is not applicable when source dataset type is RestResource.
</code></pre>

<p>It's not supported for now.</p>

<p>Hope this helps</p>
"
"62356952","LeaseAlreadyPresent Error in Azure Data Factory V2","<p>I am getting the following error in a pipeline that has <strong>Copy activity with Rest API</strong> as source and <strong>Azure Data Lake Storage Gen 2</strong> as Sink.</p>
<blockquote>
<p>&quot;message&quot;: &quot;Failure happened on 'Sink' side. ErrorCode=AdlsGen2OperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ADLS Gen2 operation failed for: Operation returned an invalid status code 'Conflict'. Account: '{Storage Account Name}'. FileSystem: '{Container Name}'. Path: 'foodics_v2/Burgerizzr/transactional/_567a2g7a/2018-02-09/raw/inventory-transactions.json'. ErrorCode: 'LeaseAlreadyPresent'. Message: 'There is already a lease present.'. RequestId: 'd27f1a3d-d01f-0003-28fb-400303000000'..,Source=Microsoft.DataTransfer.ClientLibrary,''Type=Microsoft.Azure.Storage.Data.Models.ErrorSchemaException,Message=Operation returned an invalid status code 'Conflict',Source=Microsoft.DataTransfer.ClientLibrary,'&quot;,</p>
</blockquote>
<p>The pipeline runs in a for loop with <code>Batch size = 5</code>. When I make it sequential, the error goes away, but I need to run it in parallel.</p>
","<azure-data-lake><azure-data-factory><azure-blob-storage>","2020-06-13 07:47:58","1471","4","1","75239896","<p>This is known issue with adf limitation variable thread parallel running.
You probably trying to rename filename using variable.
Your option is to run another child looping after each variable execution.
i.e. variable  -&gt; Execute Pipeline</p>
<p><a href=""https://i.stack.imgur.com/HHg6f.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>or
remove those variable, hard coded those variable expression in azure activity.
<a href=""https://i.stack.imgur.com/Sm6o9.png"" rel=""nofollow noreferrer"">enter image description here</a>
Hope this helps</p>
"
"62345084","Conditional Execution of Piepline in Azure DataFactory","<p>I need perform an ""Execute Azure pipeline activity"" based on a Condition. The condition is if a table in sql server return any rows i should execute the pipeline ,other wise i should skip the execution.How can get the row count of table  in Azure pipeline.Which activity i should use in order to get the Row Count of a table. Please let me know if can use any other options to do the same.</p>
","<azure><azure-sql-database><azure-pipelines><azure-data-factory>","2020-06-12 13:16:07","79","0","1","62347361","<p>You can use the Lookup activity to determine whether the table has any rows.</p>

<ol>
<li>Create a DataSet with no table and no schema:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/eptpG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eptpG.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/S4O02.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S4O02.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>Add a Lookup activity with that DataSet as the Source. Specify a Query to get the row count, and check ""First row only"":</li>
</ol>

<p><a href=""https://i.stack.imgur.com/FCqyA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FCqyA.png"" alt=""enter image description here""></a></p>

<ol start=""3"">
<li>Capture the output in a boolean variable (or reference it directly in the IF activity):</li>
</ol>

<p><a href=""https://i.stack.imgur.com/VMdW8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VMdW8.png"" alt=""enter image description here""></a></p>

<p>Note the syntax is <em>output.firstRow.<strong></em>{sql_column_name}</strong></p>
"
"62340084","Azure Data Factory- Copy data from ADLS -to Cosmos DB - getting error while creating connection for ADLS as source","<p>Getting below error while creating source connection for ADLS in Azure Data Factory</p>

<p>Access to <a href=""https://example.azuredatalakestore.net/webhdfs/v1/"" rel=""nofollow noreferrer"">https://example.azuredatalakestore.net/webhdfs/v1/</a> is denied. Make sure the ACL and firewall rule is correctly configured in the Azure Data Lake Store account. Service request id: <code>e9508d22-3de7-4924-bc16-b759ac6cb320</code></p>

<p>Response details: </p>

<pre><code>{
  ""RemoteException"":
  {
    ""exception"": ""AccessControlException"",
    ""message"": ""Permission denied on / [e9508d22-3de7-4924-bc16-b759ac6cb320][2020-06-12T00:33:05.7388123-07:00]"",
    ""javaClassName"": ""org.apache.hadoop.security.AccessControlException""
  }
}
</code></pre>

<p>The remote server returned an error:</p>

<blockquote>
  <p>(403) Forbidden. Activity ID: 411d2e50-42b7-4b57-aa43-33a8832dbbc2.</p>
</blockquote>

<p>My Azure account is a trial account. I am using the organization id to create an account.
Could you please help me resolve this?</p>
","<azure><azure-data-factory><azure-data-lake>","2020-06-12 08:06:14","191","0","1","62346867","<p>You should add your <code>Managed identity object ID</code> at your ADLS.Please refer to <a href=""https://stackoverflow.com/questions/62237694/data-factory-new-linked-service-connection-failure-acl-and-firewall-rule/62253524#62253524"">this link</a>.</p>
"
"62339660","Azure Data Factory - Inner Activity Failed In For Each","<p>I have used a look up activity to pass the value to the for each iteration activity. The output values from Lookup is generated from a SQL table. Once the iteration starts if one of the activity inside the for each fails, the for each iterator tries to run it for the number of times, the lookup output value is available. How do I come out of the loop? I have removed the records from the SQL table, to come out of the loop, but the loop continues to run. How can I clear the For Each Items set when an inner activity fails?</p>

<p>REgards,
Sandeep</p>
","<azure-data-factory>","2020-06-12 07:34:50","2843","0","2","62380221","<p><strong>How can I clear the For Each Items set when an inner activity fails?</strong></p>

<p>No, we can't. For Each active doesn't support break for now even if the internal active failed.</p>

<p>Many users have post same questions in stack overflow and <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">Data Factory feedback</a>:
<a href=""https://i.stack.imgur.com/CMvmu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CMvmu.png"" alt=""enter image description here""></a></p>

<p>It' voted up 31 times but still with no respond of the Data Factory Product Team.</p>

<p>Ref: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/39673909-foreach-activity-allow-break"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/39673909-foreach-activity-allow-break</a></p>

<p><strong>Update:</strong></p>

<p>Congratulations that you have found a solution for you scenario:</p>

<p>""Now used an until activity by comparing the variable values and count of files out put from a lookup activity to resolve the issue.""</p>

<p>I post it in answer and this can be beneficial to other community members.</p>

<p>Hope this helps.</p>
"
"62339660","Azure Data Factory - Inner Activity Failed In For Each","<p>I have used a look up activity to pass the value to the for each iteration activity. The output values from Lookup is generated from a SQL table. Once the iteration starts if one of the activity inside the for each fails, the for each iterator tries to run it for the number of times, the lookup output value is available. How do I come out of the loop? I have removed the records from the SQL table, to come out of the loop, but the loop continues to run. How can I clear the For Each Items set when an inner activity fails?</p>

<p>REgards,
Sandeep</p>
","<azure-data-factory>","2020-06-12 07:34:50","2843","0","2","62400960","<p>I have replaced the for each loop with the until activity. The input for the until activity was a SQL query which returns the count of records from the table where the file names are copied and a variable value. Used the @greater expression with Variable value, and lookup activity value. Inside the loop created logic to increment the value of the variable using a temp variable and add expression. If an expression fails, marked the variable value greater than the lookup output value. </p>
"
"62328746","How to convert ADF Pipeline Run Id (string) to GUID?","<p>I have a table in a SQL database with a UNIQUEIDENTIFIER data type</p>

<p><a href=""https://i.stack.imgur.com/pTgYa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pTgYa.png"" alt=""enter image description here""></a></p>

<p>I would like to populate this table with the Pipeline Run Id parameter in Azure Data Factory</p>

<p><a href=""https://i.stack.imgur.com/IUtk9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IUtk9.png"" alt=""enter image description here""></a></p>

<p>When I pass this parameter, it is passed as a string value, my table in the SQL database expects a value GUID() value. Is there a way to convert the parameter to GUID() type. Or should I consider changing the datatype in the target table?</p>

<p>Thanks in advance :)</p>
","<azure-sql-database><azure-data-factory>","2020-06-11 16:13:40","2691","0","1","62330117","<p>You'll probably be better served by changing the SQL data type to a varchar if that's possible. Pipelines don't have a variable type for guid. They also do not have a conversion function for string to guid. The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#guid"" rel=""nofollow noreferrer"">guid() function</a> generates a new guid value but returns a string. The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions"" rel=""nofollow noreferrer"">Data Flow expression language</a> doesn't even contain a reference to guid. All in all, my conclusion is if you can just treat them as strings you should.</p>
"
"62327263","Azure DevOps CI/CD not deploying my pipeline triggers in an Active state","<p>My dev ADF has pipeline triggers that are Active and set to run every hour. Using devops repos &amp; CI/CD, I deployed these triggers from our dev to test ADF environments. The pipeline triggers were successfully created in my test environment, but they are inactive. Why are these not coming over in a 'Started' status. The ARM template shows the trigger as Started</p>

<p>These need to be turned on automatically after deployment as I don't have rights to turn these on manually.  I can't tell if this is a bug or if I'm missing something to turn these on within the deployment.  Please see screenshots:</p>

<p><a href=""https://i.stack.imgur.com/vpjzC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vpjzC.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ufvF9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ufvF9.png"" alt=""enter image description here""></a></p>
","<azure><azure-devops><continuous-integration><azure-data-factory><continuous-deployment>","2020-06-11 14:56:29","417","2","1","62339254","<p>We have the same issue and are using a powershell step in Azure Devops to enable the trigger:</p>

<pre class=""lang-bsh prettyprint-override""><code>Start-AzureRmDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryResourceName -Name $PipelineTriggerName -Force
</code></pre>
"
"62322109","How can i pass file as parameter in post request (Web Activity) Azure Data Factory?","<p>I need to send request to API over Azure Data Factory. But input parameter is file. How can i do this?</p>
","<azure><api><azure-data-factory>","2020-06-11 10:19:24","1918","1","1","62336108","<p>Data Factory <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Active</a> can help you achieve that. It depends on where the file location is. </p>

<p>For example, if your parameter file is stored in Blob Storage.</p>

<p>We can set the <code>filename</code> as dataset parameter:
<a href=""https://i.stack.imgur.com/3ocMA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3ocMA.png"" alt=""enter image description here""></a></p>

<p>Then set pipeline parameter <code>filename</code>:
<a href=""https://i.stack.imgur.com/KVNKG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KVNKG.png"" alt=""enter image description here""></a></p>

<p>Web active settings:</p>

<p>Set dataset filename = <code>@pipeline().parameters.filename</code>
<a href=""https://i.stack.imgur.com/CxB9x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CxB9x.png"" alt=""enter image description here""></a></p>

<p>We can not pass a file as parameter directly, but we could parameter the filename to set the file in source dataset.</p>

<p>Hope this helps you.</p>
"
"62318347","How to add ""text qualifier"" in Azure data factory sink","<p>Hi the requirement is to add text qualifiers for output file via Azure data factory</p>

<p>example </p>

<p>""text1"",""text2"",""text3""  -----> this is header</p>

<p>""a"",""b"",""c"",""d""</p>

<p>Regards
Rk</p>
","<azure><azure-data-factory>","2020-06-11 06:28:36","388","0","1","62335083","<p>Set the Quote All option in the Sink settings: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delimited-text#sink-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-delimited-text#sink-properties</a></p>
"
"62316162","What is the most appropriate way to copy data from SQL server to Azure synapses analytics","<p>After reading the web I have two options for this </p>

<ol>
<li>Copy data using ADF from SQL server to Azure synapses analytics.</li>
<li>Copy data to blob storage and from there copy data using external tables to the Azure synapses analytics. </li>
</ol>

<p>What is the best way out of these two?  </p>

<p>Is  ""Copy data to blob storage from there copy data using ADF to the Azure synapses analytics"" can be considered as another option? </p>
","<azure><polybase><azure-synapse><azure-data-factory>","2020-06-11 02:38:03","107","0","1","62317239","<p>Copy data from SQL server to Azure synapses analytics need configure Staging storage account:
<a href=""https://i.stack.imgur.com/cTt8v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cTt8v.png"" alt=""enter image description here""></a></p>

<p>I don't think you need options 2.</p>

<p>Hope this helps.</p>
"
"62312579","How to define multiple main methods in a Jar class (in Scala) and call it from Azure Data Factory?","<p>I am looking to create Jar file class which contains multiple main class methods(in Scala) so Azure data factory activity can call jar activity. Azure data factory requires main class name and parameters.</p>

<p>Regards
Rajaniesh</p>
","<scala><azure><jar><azure-data-factory>","2020-06-10 20:30:28","224","0","1","62340689","<p>In Azure Data Factory, Jar files can contain only one Main-Class attribute in the manifest, which means a jar can contain only one mainClassName.</p>

<p><a href=""https://i.stack.imgur.com/M451G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M451G.png"" alt=""enter image description here""></a></p>

<p>Here is the sample JSON definition of a Databricks Jar Activity:</p>

<pre><code>{
    ""name"": ""ADB-Jar-Activity"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Jar1"",
                ""type"": ""DatabricksSparkJar"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""mainClassName"": ""org.apache.spark.examples.SparkPi"",
                    ""parameters"": [
                        ""10"",
                        ""20""
                    ],
                    ""libraries"": [
                        {
                            ""jar"": ""dbfs:/docs/sparkpi.jar""
                        }
                    ]
                },
                ""linkedServiceName"": {
                    ""referenceName"": ""ADB"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        ],
        ""annotations"": []
    }
}
</code></pre>

<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-jar"" rel=""nofollow noreferrer"">Transform data by running a Jar activity in Azure Databricks</a>.</p>
"
"62312216","Azure Data Transfer for High Frequency / Small Size","<p>I am looking at an Azure Architecture for a client.</p>

<p>Requirement: Client runs a call centre. Their call centre management software tools are on-prem. The databases (MSSQL,MySQL,FileBased) for their software is also on-prem. They would like to transfer the call data and client interaction data from on-prem to Azure SQL DB. Now this Azure SQL DB will be designed as an Operational Data Store so once the data has landed in staging tables, a SQL Stored Procedure will be run to process the data to datamart tables.</p>

<p>The data in the ODS datamart tables will be used in PowerBI that is displayed in their call centres on large screens indicating up-to-date stats like number of calls made, target calls for the day, number of leads, ring time etc.</p>

<p><a href=""https://i.stack.imgur.com/fbNFr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fbNFr.png"" alt=""enter image description here""></a></p>

<p>Now they require a rather high frequency of data transfer to happen. Ranging from every 30seconds for  call data or every 5 minutes for leads data. But the amount of data will range from 10-100 records per transfer. The reason for this is because they want near-realtime dashboards displayed.</p>

<p>Now having looked at Azure Data Factory, I found that using it for near realtime data transfering can become very expensive. A single copy data pipeline can cost them around $3200 per month. And there wil be several of these pipelines. This is not possible with the client's budget for opertional cost. :(</p>

<p><strong>I need suggestions for an alternative approach</strong>. </p>

<p>Where small batches of data can be transered to Azure SQL Database at high frequency from on-prem datasources.</p>

<p>What I have considered thus far is developing an on-prem Windows Service (with .Net) that push data to Azure SQL, but this will take alot of development and require after sales support.</p>
","<azure><rest><azure-data-factory>","2020-06-10 20:03:49","140","0","1","62320861","<p>So after removing my head from my backside, i realised that the biggest issue was in the calculation of the cost.</p>

<p><a href=""https://i.pinimg.com/originals/f3/fd/7d/f3fd7d0dd751e7af5a547f81d8278e04.gif"" rel=""nofollow noreferrer""><img src=""https://i.pinimg.com/originals/f3/fd/7d/f3fd7d0dd751e7af5a547f81d8278e04.gif"" alt=""enter image description here""></a></p>

<p>So after watching some very insightfull videos (<a href=""https://www.youtube.com/watch?v=c958SwJ8lpg"" rel=""nofollow noreferrer"">Understanding Azure Data Factory Pricing</a>) I figured out how to determine the value of the variables used in the azure pricing calculator.</p>

<p>So it brought the monthly bill down to about $50.</p>
"
"62304031","Data ingestion to snowflake from Azure data factory","<p>Question : Can anyone help me to find a solution for ingesting data from Azure Data factory to snowflake table without using azure blob storage.</p>

<p>Requirements : We got a set of customer IDs stored in snowflake table right now.We want to iterate through each of the customer id and fetch all customer details from Amazon S3 using WebAPI and write it back to snowflake table. The current system uses Azure Databricks(PySpark) to POST customer id and GET related json data from S3 using WebAPI,parse json to extract our required info and write it back to snowflake. But this process takes at least 3 seconds for a single record and we cannot afford to spend that much time for data ingestion as we have large data volume to process and running ADB cluster for long time cost more. The solution we think is like instead of using python Web API,we can use azure data factory to get data from s3 bucket and ingest it to snowflake table. Since the data is customer data ,we are not suppose to store that in azure blob storage before writing it to snowflake due to privacy rules.Do we have any other method that can be used to write it to snowflake table directly from s3 or through ADF without using blob storage.</p>
","<azure><amazon-s3><azure-data-factory><snowflake-cloud-data-platform><azure-databricks>","2020-06-10 12:51:00","419","0","2","62304145","<p>You can create a databricks notebook and read all data from s3 and for temp purpose store the data on dbfs which will be destroyed as soon as the cluster terminates.</p>

<pre><code>ADF -&gt; Databricks Notebook

Databricks
Read from s3 -&gt; create a pyspark dataframe -&gt; filter the data based on your condition -&gt; write to snowflake
</code></pre>
"
"62304031","Data ingestion to snowflake from Azure data factory","<p>Question : Can anyone help me to find a solution for ingesting data from Azure Data factory to snowflake table without using azure blob storage.</p>

<p>Requirements : We got a set of customer IDs stored in snowflake table right now.We want to iterate through each of the customer id and fetch all customer details from Amazon S3 using WebAPI and write it back to snowflake table. The current system uses Azure Databricks(PySpark) to POST customer id and GET related json data from S3 using WebAPI,parse json to extract our required info and write it back to snowflake. But this process takes at least 3 seconds for a single record and we cannot afford to spend that much time for data ingestion as we have large data volume to process and running ADB cluster for long time cost more. The solution we think is like instead of using python Web API,we can use azure data factory to get data from s3 bucket and ingest it to snowflake table. Since the data is customer data ,we are not suppose to store that in azure blob storage before writing it to snowflake due to privacy rules.Do we have any other method that can be used to write it to snowflake table directly from s3 or through ADF without using blob storage.</p>
","<azure><amazon-s3><azure-data-factory><snowflake-cloud-data-platform><azure-databricks>","2020-06-10 12:51:00","419","0","2","62305820","<p>Well, if your data is already on S3 you can just use the <code>COPY INTO</code> command.  <a href=""https://docs.snowflake.com/en/user-guide/data-load-s3.html"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/user-guide/data-load-s3.html</a></p>
"
"62302124","Azure Data Factory V2 Using Params in SQL","<p>New to ADF in general and hitting a speed bump with the following scenario, would really appreciate any guidance on how to achieve this.</p>

<p>In my pipeline I want to use simple SQL to control how much data I process, so using:</p>

<pre><code>SELECT * FROM DPA.USER_REGISTRATIONS 
WHERE REG_DATE BETWEEN DATE '2020-01-01' AND DATE '2020-06-09'
</code></pre>

<p>Using the 'preview data' button/tool in ADF UI this returns a sample of exactly what I am looking for. Great, but I need the second DATE value to always be the current date. I'm sure this must be very simple but I can't seem to find the right way to form the params/expressions to get it to work.</p>

<p>I am trying:</p>

<pre><code>SELECT * FROM DPA.USER_REGISTRATIONS 
WHERE REG_DATE BETWEEN DATE '2020-01-01' AND DATE @pipeline.parameters.today;
</code></pre>

<p>Where I have a pipeline level parameter defined with name: today and value: utcnow()
But that doesn't work.</p>

<p>Can someone please advise on how to create and reference a suitable param?
Thanks!</p>
","<azure><azure-data-factory>","2020-06-10 11:10:32","36","0","1","62305332","<p>To do this based on dynamic values, you will need to use the Pipeline Expression Language (PEL) (and most likely variables) to build the query string. <a href=""https://stackoverflow.com/questions/62213623/azure-data-factory-use-variables-in-query/62217164#62217164"">This answer</a> may prove useful to you.</p>
"
"62301382","Copy Data from Blob to SQL via Azure data factory","<p>I have two sample files in blob as sample1.csv and sample2.csv as below</p>

<p><a href=""https://i.stack.imgur.com/jT0JM.png"" rel=""nofollow noreferrer"">data sample</a></p>

<p>SQL table name sample2, with column Name,id,last name,amount</p>

<p>Created a ADF flow without schema, it results as below</p>

<p><a href=""https://i.stack.imgur.com/NMlPO.png"" rel=""nofollow noreferrer"">preview data</a></p>

<p>source settings are allow schema drift checked.
sink setting are auto mapping turned on. allow insert checked. table action none.</p>

<p>I have also tried setting a define schema in dataset, its result are same.</p>

<p>any help here?</p>

<p>my expected outcome would be data in sample1 will inserted null into the column ""last name""</p>
","<blob><dataflow><azure-data-factory>","2020-06-10 10:31:36","517","0","2","62310406","<p>You cannot mix schemas in the same source in the same data flow execution.</p>

<p>Schema Drift will handle changes to the schema on an execution-per-execution basis.</p>

<p>But if you are reading multiple different schemas from a folder, you will get non-deterministic results.</p>

<p>Instead, if you loop through those files in a pipeline ForEach one-by-one, data flow will be able to handle the evolving schema.</p>
"
"62301382","Copy Data from Blob to SQL via Azure data factory","<p>I have two sample files in blob as sample1.csv and sample2.csv as below</p>

<p><a href=""https://i.stack.imgur.com/jT0JM.png"" rel=""nofollow noreferrer"">data sample</a></p>

<p>SQL table name sample2, with column Name,id,last name,amount</p>

<p>Created a ADF flow without schema, it results as below</p>

<p><a href=""https://i.stack.imgur.com/NMlPO.png"" rel=""nofollow noreferrer"">preview data</a></p>

<p>source settings are allow schema drift checked.
sink setting are auto mapping turned on. allow insert checked. table action none.</p>

<p>I have also tried setting a define schema in dataset, its result are same.</p>

<p>any help here?</p>

<p>my expected outcome would be data in sample1 will inserted null into the column ""last name""</p>
","<blob><dataflow><azure-data-factory>","2020-06-10 10:31:36","517","0","2","62381548","<p>If I understand correctly, you said: ""my expected outcome would be data in sample1 will inserted null into the column <code>last name</code>"", you only need to add a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">derived column</a> to you sample1.csv file.</p>

<p>You could follow my steps:</p>

<ol>
<li><p>I create a sample1.csv file in Blob Storage and a sample2 table in my SQL database:
<a href=""https://i.stack.imgur.com/fjWyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fjWyX.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/fg1KU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fg1KU.png"" alt=""enter image description here""></a></p></li>
<li><p>Using DerivedColumn to create new column <code>last name</code> with <code>null</code> value:</p></li>
</ol>

<p>expression: <code>toString(null())</code>
<a href=""https://i.stack.imgur.com/xBSmO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xBSmO.png"" alt=""enter image description here""></a></p>

<ol start=""3"">
<li><p>Sink settings:
<a href=""https://i.stack.imgur.com/7QXYo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7QXYo.png"" alt=""enter image description here""></a></p></li>
<li><p>Run the pipeline and check the data in table:
<a href=""https://i.stack.imgur.com/zsZXc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zsZXc.png"" alt=""enter image description here""></a></p></li>
</ol>

<p>Hope this helps.</p>
"
"62301169","How to implement scd2 in snowflake tables using Azure Data Factory","<p>I want to implement the scd2 in the snowflake tables. My source and target tables are present in snowflake only. The entire process has to be done using Azure Data Factory. 
I went through the documentation given by azure for implementing the scd2 using data flows but when I tried to create a dataset for snowflake connection its showing as disabled. </p>

<p>Is there any way or any documentation where I can see the steps to create SCD2 in adf with snowflake tables. </p>

<p>Thanks
vipendra</p>
","<azure-data-factory><scd2><snowflake-cloud-data-platform>","2020-06-10 10:20:20","1252","0","2","62310333","<p>SCD2 in ADF can be built and managed graphically via data flows. The Snowflake connector for ADF today does not work directly with data flows, yet. So for now, you will need to use the Copy Activity in an ADF pipeline and stage the dimension data in Blob or ADLS, then build your SCD2 logic in data flows using the staged data.</p>

<p>Your pipeline will look something like this:</p>

<p>[Copy Activity Snowflake-to-Blob] -> [Data Flow SCD2 logic Blob-to-Blob] -> [Copy Activity Blob-to-Snowkflake]</p>

<p>We are working on direct connectivity to Snowflake from data flows and hope to land that soon.</p>
"
"62301169","How to implement scd2 in snowflake tables using Azure Data Factory","<p>I want to implement the scd2 in the snowflake tables. My source and target tables are present in snowflake only. The entire process has to be done using Azure Data Factory. 
I went through the documentation given by azure for implementing the scd2 using data flows but when I tried to create a dataset for snowflake connection its showing as disabled. </p>

<p>Is there any way or any documentation where I can see the steps to create SCD2 in adf with snowflake tables. </p>

<p>Thanks
vipendra</p>
","<azure-data-factory><scd2><snowflake-cloud-data-platform>","2020-06-10 10:20:20","1252","0","2","62316924","<p>If your source and target tables are both in Snowflake, you could use Snowflake Streams to do this. There's a blog post covering this in more detail at <a href=""https://community.snowflake.com/s/article/Building-a-Type-2-Slowly-Changing-Dimension-in-Snowflake-Using-Streams-and-Tasks-Part-1"" rel=""nofollow noreferrer"">https://community.snowflake.com/s/article/Building-a-Type-2-Slowly-Changing-Dimension-in-Snowflake-Using-Streams-and-Tasks-Part-1</a></p>

<p>However, in short, if you have a source table <code>source</code>, you can put a stream on it like so:</p>

<pre><code>create or replace stream source_changes on table source;
</code></pre>

<p>This will capture all the changes that are made to the source table. You can then build a view on that stream that establishes how you want to feed those changes into the SCD table. (The blog post uses case statements to put start and end dates in on each row in the view). </p>

<p>From there, you can use a Snowflake Task to automate the process of loading from the stream into the SCD only when the Stream actually has changes.</p>
"
"62298123","Azure data factory - collapse row to one column","<p>I have below input </p>

<pre><code>A  C

1  X

1  Y

1  Z

2  D

2   E

2   F
</code></pre>

<p>where  A &amp; C are header, I want to collapse C into one column say ""B""</p>

<p>like below </p>

<pre><code>A  B
1  x,y,z
2  D,E,F 
</code></pre>

<p>how can we achieve in Azure data factory</p>
","<sql><azure><azure-data-factory>","2020-06-10 07:34:50","105","0","1","62298271","<p>Try with <code>string_agg</code></p>

<pre><code>select
    A,
    string_agg(C, ', ') as B
from myTable
group by
    A
</code></pre>
"
"62297772","Unable to file multiple BLOB to Synapse using data factory","<p>I want to copy bulk data from BLOB storage to Azure Synapse with the following structure:</p>

<p><strong>BLOB STORAGE:-</strong></p>

<pre><code>devpresented (Storage account)
        processedout (Container)
                Merge_user (folder)
                    &gt; part-00000-tid-89051a4e7ca02.csv
                Sales_data (folder)
                    &gt; part-00000-tid-5579282100a02.csv
</code></pre>

<p><strong>SYNAPSE SQLDW:</strong></p>

<pre><code>SCHEMA:- PIPELINEDEMO
TABLE: Merge_user, Sales_data
</code></pre>

<p>Using data factory I want to copy BLOB data to Synapse database as below:</p>

<pre><code>BLOB        &gt;&gt;  SQLDW
Merge_user  &gt;&gt;  PIPELINEDEMO.Merge_user
Sales_data  &gt;&gt;  PIPELINEDEMO.Sales_data
</code></pre>

<p>The following doc in mentioned for SQL DB to SQL DW:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal</a></p>

<p>However, I didn't find anything for BLOB source in data facory.</p>

<p>Can anyone please suggest, how can I move multiple BLOB files to different tables.</p>
","<azure><azure-blob-storage><azure-data-factory><azure-synapse>","2020-06-10 07:11:35","203","0","1","62320558","<p>If you need to copy the contents of different csv files to different tables in Azure Synapse, then you can enable multiple copy activities within a pipeline.</p>

<p>I create a .csv file, this is the content:</p>

<pre><code>111,222,333,
444,555,666,
777,888,999,
</code></pre>

<p>I upload this to my storage account, Then I set this .csv file as the source of the copy activity.</p>

<p><a href=""https://i.stack.imgur.com/cwVdO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cwVdO.png"" alt=""enter image description here""></a></p>

<p>After that, I create a table in azure synapse, and set this as the sink of the copy activity:</p>

<pre><code>create table testbowman4(Prop_0 int, Prop_1 int, Prop_2 int)
</code></pre>

<p><a href=""https://i.stack.imgur.com/XtJXo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XtJXo.png"" alt=""enter image description here""></a></p>

<p>At last, trigger this pipeline, you will find the data is in the table:</p>

<p><a href=""https://i.stack.imgur.com/xsidz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xsidz.png"" alt=""enter image description here""></a></p>

<p>You can create multiple similar copy activities, and each copy activity performs a copy action from blob to azure synapse.</p>
"
"62272576","Convert data in table storage from string to other formats in .NET SDK","<p>Is there a way to convert data in table storage from string format to int/timestamp format in .NET SDK?</p>

<p>I used an ADF pipeline to copy data from data lake to blob storage and then from blob storage to table storage. After running pipeline, I see all columns in table storage in string format.</p>

<p>I need to use the following query to filter data in table:</p>

<pre><code>https://myaccount.table.core.windows.net/Customers()?$filter=Age%20gt%2030 
</code></pre>

<p>Currently this doesn't work as the data in Age column is in string format.</p>

<p>Is it possible to first convert the data in 'Age' column to integer format and use the above query?</p>
","<c#><azure-table-storage><azure-data-factory>","2020-06-08 23:07:47","125","1","1","62277726","<p>I don’t know why you are obsessed with this question. As you asked in another post, Leon’s answer clearly tells you that it is not feasible.</p>

<p>Now I complete the whole operation by reading the data in the original table, creating a new table, and inserting the data, I hope it helps you. You can download <a href=""https://github.com/Jason446620/cosmos-tablecopy"" rel=""nofollow noreferrer"">my demo</a>, just need to replace your <code>StorageConnectionString</code> in Settings.json.</p>

<p>You need create table Customer.[More details you can see source code.]</p>

<p>Then run, you can see new table created from Storage Explorer, and data in there.</p>

<p><a href=""https://i.stack.imgur.com/AgrYJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AgrYJ.png"" alt=""enter image description here""></a></p>

<p>If you want <a href=""https://stackoverflow.com/questions/61925442/console-app-to-use-azure-storage-tableapi/61929744#61929744"">filter data</a>, you can do it.</p>

<p><a href=""https://i.stack.imgur.com/TlTCI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TlTCI.png"" alt=""enter image description here""></a></p>
"
"62270761","Azure Data Factory development with multiple users without using Source Control","<p>While working on a single Azure Data Factory solution with no Source Control. Is it possible to work parallelly for a team of 3 or more developers, without corrupting the main JSON?
Scenario: 
All developers are accessing the same ADF and working on different pipelines at the same time. One of the developer publishes his/her updates, does it somehow overwrites or ignores the changes other developers are publishing?</p>
","<azure-data-factory>","2020-06-08 20:35:48","860","1","1","62297539","<p>I tested and found that:</p>

<ol>
<li>Multiple users can access the same Data factory and working with
different pipelines in same time.</li>
<li>Publish only affect the current user and the current pipeline which
user is developing and editing. It won't overwrites other pipelines.</li>
</ol>

<p>For you question:</p>

<p><strong>Is it possible to work parallelly for a team of 3 or more developers, without corrupting the main JSON?</strong></p>

<p>Yes, it's possible.</p>

<p><strong>One of the developer publishes his/her updates, does it somehow overwrites or ignores the changes other developers are publishing?</strong></p>

<p>No, it doesn't. For example, user A only develop with pipeline A, then publish again. The Publish only affect the current pipeline, won't overwrite or affection other pipelines.</p>

<p>You could test and prove it.</p>

<p><strong>Update:</strong></p>

<p>Thanks @V_Singh for share us the Microsoft suggestion:</p>

<p>Microsoft suggested to use CI/CD only, otherwise there will be some disparity in code. </p>

<p>Reply from Microsoft: </p>

<p>""<strong>In Live Mode can hit unexpected errors if you try to publish because you may have not the latest version ( For Example user A publish, user B is using old version and depends on an old resource and try to publish) not possible. Suggested to please use Git, since it is intended for collaborative scenarios.</strong>""</p>

<p>Hope this helps.</p>
"
"62267807","Lookup Azure AD from Azure Data Factory (ADF)","<p>I am getting an ID from a flat file(which I believe is domainname\loginname). Before processing it to the database, I need to lookup on the Azure AD to get more details like Name, Email etc. </p>

<p>I need to first determine what attribute in Azure AD am I getting in my file. 
Then, I need to lookup Azure AD to get remaining additional info of that user using the ID I am getting in my flat file source.</p>

<p>How do I get a list of all attributes I can get from Azure AD? When I am using Get-AzureADUser powershell cmdlet on Azure portal, it is returning only object_id, email and display_name. </p>

<p>I am moving the file to Azure SQL DB using an ADF pipeline. I am looking for a way to lookup Azure AD from Azure Data Factory pipeline. If that is not possible ,is there any way to achieve the same using LogicApps/powershell/O365?</p>
","<azure><azure-active-directory><azure-logic-apps><azure-powershell><azure-data-factory>","2020-06-08 17:24:07","1504","0","1","62273283","<p>I believe if you are using the azuread powershell module, then it actually does return on all the attributes. you just have to list them. eg. </p>

<pre><code>get-azureaduser | fl 
</code></pre>

<p>you should see a full list of all the attributes. </p>

<p>you can create a pipeline to query the o365 graph api, this should be able to get you all the data. You do this by creating an office365 connector in the pipelines then get the data. </p>
"
"62266718","Azure datafactory - pipeline, Dataset, container, linkservices","<p>Does anyone know how I should configure the ""Dataset"" to read everything the Blob container has and to be able to perform the copy activity of each container?</p>

<p>I'm thinking of using a ""GetMetada"", but I don't know how to configure it since the ""LinkServices"" configuration leaves me inside the container and I get an error because in the ""Dataset"" I must configure the container and I don't know what to put there.</p>
","<azure><dataset><pipeline><azure-data-factory>","2020-06-08 16:24:49","71","0","1","62273859","<p>The first thing you must understand is that Copy Activity is at least container-based. There is no one Copy Activity that can perform copy activities for all containers.</p>

<p>For reading everything in the container, just set like this:</p>

<p><a href=""https://i.stack.imgur.com/0w6Mg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0w6Mg.png"" alt=""enter image description here""></a></p>

<p>(This can set all of the data in container 'test1' as source data.)</p>

<p>If you want to copy data from multiple containers, then you need multiple Copy Activities, like this:</p>

<p><a href=""https://i.stack.imgur.com/EliJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EliJ4.png"" alt=""enter image description here""></a></p>

<p>Can my answer answer your question? Any doubts please let me know.:)</p>
"
"62264367","Azure Data Factory v1 pipeline not starting","<p>I have created an Azure Data Factory which has the following activity JSON as viewed in the portal (excerpt)</p>

<pre><code>   ""start"": ""2018-07-27T00:00:00Z"",
    ""end"": ""2099-12-30T13:00:00Z"",
    ""isPaused"": false,
    ""runtimeInfo"": {
        ""deploymentTime"": ""2020-06-08T12:42:21.2801494Z"",
        ""activePeriodSetTime"": ""2020-06-08T12:23:16.2436361Z"",
        ""pipelineState"": ""Running"",
        ""activityPeriods"": {
            ""copyXZActivity"": {
                ""start"": ""2017-06-27T00:00:00Z"",
                ""end"": ""2099-12-30T13:00:00Z""
            }
        }
    },
    ""id"": ""ef896997-2046-4b2e-7074-ecb5f58dd489"",
    ""provisioningState"": ""Succeeded"",
    ""hubName"": ""sxdb_hub"",
    ""pipelineMode"": ""Scheduled""
</code></pre>

<p>My AzureSQLTable inputs and outputs have the following JSON config:</p>

<pre><code>""availability"": {
      ""frequency"": ""Minute"",
      ""interval"": 15
    },
</code></pre>

<p>I would expect it to run immediately, every 15 minutes, but the activity window is empty. The next scheduled run at 5/3/2020, 4:30 PM UTC according to the activity window, which seems to be a random date in the past. </p>

<p>How do I get the activity to run, as expected, every 15 minutes?</p>
","<azure-data-factory>","2020-06-08 14:28:49","50","0","1","62302445","<p>The problem seems to have been this line, which caused execution to begin on 3 May 2020:</p>

<pre><code>""copyXZActivity"": {
                ""start"": ""2017-06-27T00:00:00Z"",
</code></pre>

<p>Changing it to this fixed the issue:</p>

<pre><code> ""copyXZActivity"": {
                    ""start"": ""2020-06-10T00:00:00Z"",
</code></pre>

<p>Changing the start date to the current date got the activity running. It appears like Data Factory 1, when given a start date in the distant past, chooses another date a few months ago, and starts executing the activity from that day, continuously, until it ""catches up"" and then follows the interval pattern (although I wasn't able to navigate via the monitoring UI to that date). </p>

<p>It exhibits this ""catch up"" behavior with any start date/time in the past. </p>
"
"62245161","Moving Messages from Azure Queues to Database","<p>I'm a newbie to Azure . So far been successful in developing a couple of functions that has been able to do the following </p>

<ol>
<li>Trigger a function when a message arrives in Queue</li>
<li>Response received by function stored in a Queue</li>
</ol>

<p>Now I know that I can read these messages from the queue with a programming SDK like with Python or .net . However I'd like to figure out if it is possible to do an ETL with Azure Data Factory to directly consume the messages instead of another function scanning through the queue ?  </p>
","<azure><azure-data-factory><azure-queues>","2020-06-07 12:14:17","1384","0","1","62253866","<p>Data Factory doesn't support Queue(Storage queue or Service Bus Queue) as Data store for now. Please ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Azure Data Factory connector overview</a> </p>

<p>That means that we can not copy the Queue message directly with Data Factory.</p>

<p>Hope this helps.</p>
"
"62237694","Data Factory New Linked Service connection failure ACL and firewall rule","<p>I'm trying to move data from a datalake stored in Azure Data Lake Storage Gen1 to a table in an Azure SQL database.  In Data Factory ""new Linked Service"" when I test the connection I get a ""connection failed"" error message, ""Access denied...make sure ACL and firewall rule is correctly configured in the Azure Data Lake Store account.  I tried numerous times to correct using related Stack overflow comments and plethora of fragmented Azure documentation to no avail.  Am I using the correct approach and if so how do I fix the issue?</p>
","<sql><connection><firewall><acl><azure-data-factory>","2020-06-06 20:32:08","1137","0","1","62253524","<p>Please follow me:</p>

<p>First:</p>

<p>Go to ADF and new Linked service in ADF,then copy <code>Managed identity object ID</code>.
<a href=""https://i.stack.imgur.com/sPnLW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sPnLW.png"" alt=""storage""></a></p>

<p>Second:Go to Azure Data Lake Storage Gen1,navigate to Data Explorer -> Access -> click select in the 'Select User or group' field.</p>

<p>Finally:paste your <code>Managed identity object ID</code> and then test your connection in ADF.
<a href=""https://i.stack.imgur.com/L7wY1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L7wY1.png"" alt=""access""></a></p>
"
"62228202","Create ADF schedule running from 6:00 to 20:00","<p>I have built a package for ADF IR. Cool. Proud of it.
Now I want to schedule it for every hour between 06:00 and 20:00.
I can see no way in ADF to schedule it as such.
What I did now is schedule the package for hourly execution.
The IR is started at 06:00 and stopped at 20:00, effectively running hourly between 06:00 and 20:00.</p>

<p>However, in the near future I will be running other packages at night so I will be needing my runtime at night. And then it will run my package that should only run between 06:00 and 20:00.</p>

<p>Am I missing something in the schedule options?
How can I run my package run hourly between 06:00 and 20:00 without having to start and stop my runtime?</p>
","<azure><ssis><azure-data-factory><azure-scheduler>","2020-06-06 06:33:00","51","0","1","62264761","<p>ADF's recurrence pattern is not that robust, but you can leverage Logic Apps to manage the trigger. Here is <a href=""https://stackoverflow.com/questions/62134686/can-i-trigger-my-azure-data-factory-pipeline-in-5-working-day-between-9-am-to-6/62137667#62137667"">another answer</a> where I discussed this recently.</p>
"
"62220636","Azure Data Factory copy data - cannot convert 'System.String' to target type 'Microsoft.Xrm.Sdk.EntityReference'","<p>I am trying to set a lookup field in the copy data action in Azure Data Factory. </p>

<p>The error I'm getting is </p>

<pre><code>Failure happened on 'Sink' side. ErrorCode=UserErrorTypeConversionFail,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
Message=,Source=,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The 
attribute 'ks_supervisor' with value 'E3BEFA28-C7A6-EA11-A812-000D3A1BB8EF' cannot be converted from 
original type 'System.String' to target type 'Microsoft.Xrm.Sdk.EntityReference'.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,''Type=Sy
stem.InvalidCastException,Message=Specified cast is not 
valid.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,''Type=Microsoft.DataTransfer.Common.Sh
ared.HybridDeliveryException,Message=The attribute 'ks_supervisor' with value 'E3BEFA28-C7A6-EA11-A812-
000D3A1BB8EF' cannot be converted from original type 'System.String' to target type 
'Microsoft.Xrm.Sdk.EntityReference'.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,''Type=Sy
stem.InvalidCastException,Message=Specified cast is not 
valid.,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,
</code></pre>

<p>The key piece is <code>value 'E3BEFA28-C7A6-EA11-A812-000D3A1BB8EF' cannot be converted from 
original type 'System.String' to target type 'Microsoft.Xrm.Sdk.EntityReference'</code>. </p>

<p><strong>My question is how do I set a lookup field value in D365/CDS/CRM if it is expecting a value of type EntityReference?</strong> </p>

<p>I have tried the CDS connector, Dynamics 365, and the CRM connector, but all result in an identical error. I have also looked for ways to try converting the source value from a string to a GUID, but I can't find a way and I don't know if that will help since it is a GUID not EntityReference. The source column is from a SQL table of type ""uniqueidentifier"", but it is projecting automatically to a string. </p>
","<azure-data-factory>","2020-06-05 17:06:06","1083","0","1","62315902","<p>I had to add ""@EntityReference"" to the destination column name in the column mappings. See the answer provided by MSFT here: <a href=""https://learn.microsoft.com/en-us/answers/questions/34069/azure-data-factory-copy-data-to-cds-cannot-convert.html?childToView=34732#comment-34732"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/34069/azure-data-factory-copy-data-to-cds-cannot-convert.html?childToView=34732#comment-34732</a></p>

<p>See the documentation for setting lookup column values here. It also shows how to set a customer column value: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365#writing-data-to-lookup-field"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365#writing-data-to-lookup-field</a></p>
"
"62219325","Copy data from one blob storage to another blob storage","<p>My requiremnt is like that I have two storage account sa01 and sa02. Let say Sa01 having 10 files and Sa02 also having 10 files at time 01:00 AM. Now I have uploaded 4 more files at 1:15AM in sa01 and my copy activity wil automatically runs beacause I am implemented the event trigger. So It will insert the 4 files to sa02. </p>

<p>Question - It will insert the 4 files and also updating the previous (10) files also, so I am getting 14 files at time 01:15 AM,and requriment say that if 10 files uploaded already at 01:00 AM and 4 files which is latest can inserted in sa02. </p>

<p>See the timings in image I have just uploaded one file all the files time is modified.</p>

<p><a href=""https://i.stack.imgur.com/yJfGg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yJfGg.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/ebcTA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ebcTA.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Yu0T5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yu0T5.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/3Qzbr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Qzbr.png"" alt=""enter image description here""></a></p>
","<azure-storage><azure-data-factory><azure-blob-storage><eventtrigger>","2020-06-05 15:51:02","510","0","2","62231644","<p><a href=""https://azure.microsoft.com/en-us/services/data-share/"" rel=""nofollow noreferrer"">Azure Data Share</a> is one good way to accomplish this. It is typically used to sync storage with a partner company. But you can sync in your own subscription. There is no code to write. There is a UI and a sync schedule.</p>
"
"62219325","Copy data from one blob storage to another blob storage","<p>My requiremnt is like that I have two storage account sa01 and sa02. Let say Sa01 having 10 files and Sa02 also having 10 files at time 01:00 AM. Now I have uploaded 4 more files at 1:15AM in sa01 and my copy activity wil automatically runs beacause I am implemented the event trigger. So It will insert the 4 files to sa02. </p>

<p>Question - It will insert the 4 files and also updating the previous (10) files also, so I am getting 14 files at time 01:15 AM,and requriment say that if 10 files uploaded already at 01:00 AM and 4 files which is latest can inserted in sa02. </p>

<p>See the timings in image I have just uploaded one file all the files time is modified.</p>

<p><a href=""https://i.stack.imgur.com/yJfGg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yJfGg.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/ebcTA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ebcTA.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Yu0T5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yu0T5.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/3Qzbr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Qzbr.png"" alt=""enter image description here""></a></p>
","<azure-storage><azure-data-factory><azure-blob-storage><eventtrigger>","2020-06-05 15:51:02","510","0","2","62237963","<p>You can use a Metadata activity to get the lastModified of the destination folder.</p>

<ul>
<li>In your Copy activity, put dynamic content in the
<em>Filter by last modified: start time field</em>.  Choose the lastModified field output from the    Metadata activity.</li>
</ul>

<p>Only files in the source newer than the destination's lastModified will be copied.</p>

<p>Metadata activity is tiny fractions of a penny in cost.</p>
"
"62213623","azure data factory: use variables in query","<p>I have created a copy activity that copies data from an on premise database to a Azure SQL Database.
I need to modify dynamiccaly the query, so it takes a range of dates, with these two variables:<br></p>
<ul>
<li><code>inidate</code></li>
<li><code>enddate</code></li>
</ul>
<p>Which I want to use inside the <code>where</code> clause, but I don't know how to reference the variables. I tried this but it doesn't work:</p>
<pre><code>  SELECT * FROM tableOnPrem 
  WHERE dateOnPrem BETWEEN '@variable('inidate')' AND '@variable('enddate')'
</code></pre>
<p>How can this be achieved?</p>
","<azure-data-factory>","2020-06-05 10:49:54","10685","5","3","62217164","<p>In a Pipeline (like for Copy activity), you will need to build the query string dynamically using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">Pipeline Expression Language</a> (PEL). The best way to do this is to use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#concat"" rel=""nofollow noreferrer"">concat</a> function to piece together the query:</p>
<pre><code>@concat('SELECT * FROM tableOnPrem WHERE dateOnPrem BETWEEN ''',variables('inidate'),''' AND ''',variables('enddate'),'''')
</code></pre>
<p>This can get complex rather quickly, so you'll need to pay extra attention to commas, parentheses, ''' and ''''.</p>
<p>Note that the '@' symbol only appears once, at the beginning of the expression. Also, to reference a pipeline variable you are calling the &quot;variable<strong>s</strong>&quot; function.</p>
"
"62213623","azure data factory: use variables in query","<p>I have created a copy activity that copies data from an on premise database to a Azure SQL Database.
I need to modify dynamiccaly the query, so it takes a range of dates, with these two variables:<br></p>
<ul>
<li><code>inidate</code></li>
<li><code>enddate</code></li>
</ul>
<p>Which I want to use inside the <code>where</code> clause, but I don't know how to reference the variables. I tried this but it doesn't work:</p>
<pre><code>  SELECT * FROM tableOnPrem 
  WHERE dateOnPrem BETWEEN '@variable('inidate')' AND '@variable('enddate')'
</code></pre>
<p>How can this be achieved?</p>
","<azure-data-factory>","2020-06-05 10:49:54","10685","5","3","64803116","<p>Another option to handle is define them as pipeline parameters <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameters-data-flow"" rel=""nofollow noreferrer"">pipeline-prameters</a>. Say for example if you have parameters defined as</p>
<ul>
<li>start_date</li>
<li>end_date</li>
</ul>
<p>you can have query written like below</p>
<pre><code>SELECT * FROM tableOnPrem WHERE dateOnPrem BETWEEN @{pipeline().parameters.start_date} AND @{pipeline().parameters.end_date}
</code></pre>
"
"62213623","azure data factory: use variables in query","<p>I have created a copy activity that copies data from an on premise database to a Azure SQL Database.
I need to modify dynamiccaly the query, so it takes a range of dates, with these two variables:<br></p>
<ul>
<li><code>inidate</code></li>
<li><code>enddate</code></li>
</ul>
<p>Which I want to use inside the <code>where</code> clause, but I don't know how to reference the variables. I tried this but it doesn't work:</p>
<pre><code>  SELECT * FROM tableOnPrem 
  WHERE dateOnPrem BETWEEN '@variable('inidate')' AND '@variable('enddate')'
</code></pre>
<p>How can this be achieved?</p>
","<azure-data-factory>","2020-06-05 10:49:54","10685","5","3","69389935","<p>I found that adding curly braces was enough to get this working (and like <a href=""https://stackoverflow.com/a/62217164/6722583"">Joel Cochran</a> stated using the <em>plural</em> variable<strong>s</strong> keyword):</p>
<pre><code>SELECT * FROM tableOnPrem 
WHERE dateOnPrem 
BETWEEN '@{variables('inidate')}' AND '@{variables('enddate')}'
</code></pre>
<p>However, it still reports an error on the Source &quot;Preview data&quot; option, but despite this it runs correctly in the actual pipeline run.</p>
"
"62209741","How to store data either in Parquet or Jason files in ""Azure Datalake Gen2 blob storage"" before purge data in Azure SQL table","<p>How to store data either in Parquet or Jason files in ""Azure Datalake Gen2 blob storage"" before purge data in Azure SQL table. What are the steps &amp; services required to achieve this. </p>
","<azure-sql-database><azure-data-factory><databricks><azure-databricks>","2020-06-05 06:53:55","105","0","1","62328966","<p>You can use Copy activity in Azure Data Factory to store data from SQL table as parquet files. Rather simplified, but here are the steps just to give you an idea.</p>

<ol>
<li>If source sql table is in-house, install Azure Integration Runtime on in-house server.</li>
<li>Connect to source table using linked service and dataset using IR from above step.</li>
<li>Connect to Azure Gen 2 stortage from ADF, again using linked service.</li>
<li>Configure copy activity with source and sink.</li>
</ol>

<p>Read more details <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">here</a></p>
"
"62209545","GET method for an Azure function within Azure Data Factory fails","<p>I am trying to invoke an  <code>HTTP</code> triggered <code>Azure function</code> built on with a <code>GET</code> request. I setup the linked service as per the recommended steps and the function itself works with a query string through POSTMAN or internet browser, but fails when I try to invoke through Data factory.</p>

<pre><code>{
    ""errorCode"": ""3608"",
    ""message"": ""Call to provided Azure function '' failed with status-'NotFound' and message - 'Invoking Azure function failed with HttpStatusCode - NotFound.'."",
    ""failureType"": ""UserError"",
    ""target"": ""Azure Function1"",
    ""details"": []
}
</code></pre>

<p>I came across another stackoverflow post <a href=""https://stackoverflow.com/a/54497119/4212430"">https://stackoverflow.com/a/54497119/4212430</a> where there was a mention of a <code>JSON</code> response for ADF. </p>

<p>I have since changed my python code to provide an <code>HTTP</code> response as a <code>JSON</code> object as below</p>

<pre><code>def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    statename = req.params.get('statename')
    if not statename:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            statename = req_body.get('statename')

    if statename:
        initiate_main(statename)
        host.close()
        function_message = {""Response"":""Successfully trasnferred BOM files""}
        return func.HttpResponse(
            json.dumps(function_message),
            mimetype=""application/json"", 
            status_code=200)
    else:
        function_message = {""Response"":""Error in transferring files""}
        return func.HttpResponse(
            json.dumps(function_message), 
            mimetype=""application/json"", 
            status_code=400)
</code></pre>

<p>But that hasn't helped either.</p>
","<json><python-3.x><azure><azure-functions><azure-data-factory>","2020-06-05 06:41:25","550","0","1","62239582","<p>It turns out that I was using the wrong URI with an <code>api</code> added at the end while I should have just been giving the plain function name</p>
"
"62208862","Calling API through Azure Data Factory","<p>I have to get the values of parameter by calling REST API. I need to fetch the parameters from database for which I need to get the value.
How can I pass the parameters from database.</p>
","<azure-data-factory>","2020-06-05 05:40:48","609","-1","1","62248599","<p>You could use a Lookup activity to first query your database and get whatever value you want to get. You will be able to access this returned value in the lookup in your next steps. Parameterize your REST API linked service/dataset. You can pass in the value returned by the Lookup activity to this dataset parameter.</p>

<p>First, check if you are using firstRowOnly or your lookup is returning multiple rows. If it is returning multiple rows, you need to keep your next step in a forEach loop.</p>

<p>If your lookup activity is returning two rows, you get the output like shown below.</p>

<pre><code>{
    ""count"": 2,
    ""value"": [
        {
            ""enrollment_number"": ""123445""
        },
        {
        ""enrollment_number"": ""345678""
        }
    ]

}
</code></pre>

<p>ADF Expression for URL would be this:</p>

<p>RemainingUrl/@{activity(activity_name).output.value.enrollment_number}</p>
"
"62207736","Staging or landing on Azure","<p>I am performing ETL in Azure Data Factory and I just wanted to confirm my understanding of it before going further. Please find the image attached below.</p>

<p><a href=""https://i.stack.imgur.com/xStbP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xStbP.png"" alt=""enter image description here""></a></p>

<p>I am collecting data from multiple source and storing in Azure Blob Storage then perform Transformation and Loading. What I am confused about is that whether Azure Blob Storage is a landing or staging area here in my case. Some people use these terms interchangeably and couldn't understand the fine line between these two terms. </p>

<p>Also, can anyone explain me which part is Extract, Transform and Load is. In my understating, collecting the data from multiple source and store into Azure Blob Storage is Extracting, Azure Data Factory is Transformation and copying the transformed data into Azure Database is Loading. Am i correct or is there something I am misunderstanding here? </p>
","<azure><etl><azure-blob-storage><azure-data-factory>","2020-06-05 03:39:27","952","1","1","62259756","<blockquote>
  <p>What I am confused about is that whether Azure Blob Storage is a
  landing or staging area here in my case.</p>
</blockquote>

<p>In your case, Azure Blob Storage is both landing area and staging area. Landing area means a area collecting data from different places. Staing area means it only save data for a little time, staging data should be deleted during ETL process.</p>

<blockquote>
  <p>Also, can anyone explain me which part is Extract, Transform and Load
  is.</p>
</blockquote>

<p>Copy Activity is a typical technology based on ETL. If only talking about the Copy Activity of Azure Data Factory, after you specify the copy source, the ADF will perform copy activities based on this, this is 'extract'. The part of the ADF that transfers data to the specified Sink according to your settings, this is 'Load', and the details of the copy behavior is 'Transform'. If you look at your entire process, you collect data to blob storage is also 'Extract'.</p>
"
"62199256","Passing a dataset parameter in a data mapping acitivity in a data flow in Azure Data factory","<p>I have a parametrized dataset, that I used for a copy data activty and it worked fine.
I am trying to replicate that using a mapping dataflow but I cant find where to input the value for the dataset parameter...</p>
","<azure-data-factory>","2020-06-04 16:09:09","2071","1","2","62202562","<p>Parameterized Data Sets work exactly the same way in a Data Flow activity. If a Data Set used in the Data Flow has parameters, they will have configuration points on the Settings tab:</p>

<p><a href=""https://i.stack.imgur.com/iPvDi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iPvDi.png"" alt=""enter image description here""></a></p>
"
"62199256","Passing a dataset parameter in a data mapping acitivity in a data flow in Azure Data factory","<p>I have a parametrized dataset, that I used for a copy data activty and it worked fine.
I am trying to replicate that using a mapping dataflow but I cant find where to input the value for the dataset parameter...</p>
","<azure-data-factory>","2020-06-04 16:09:09","2071","1","2","66767339","<p>To clarify Joel's answer - you cannot assign parameter values to a Dataset from within the Data Flow settings. It is done from the Pipeline that executes the Data Flow. This means that you may get an error message if you attempt to 'Test connection' for a parameterised dataset.</p>
"
"62198162","How to validate incoming files in Azure data factory","<p>is there a way to create a template to validate incoming files including such checks as empty file checks, format, data types, record counts along, and will stop the workflow if any of the checks fail. The solution for this requirement should consider multiple file-formats and reduce the burden on ETL processing and checks to enable scale. </p>

<p>File transfer to occur either by trigger or data currency rule</p>
","<azure><validation><azure-data-factory>","2020-06-04 15:16:57","3096","0","2","62208909","<p>Data Factory more focus on data transfer, not the file filter.</p>

<p>We could using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">get metadata</a> and  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">if-condition</a> to achieve some of the these feature, such as validate the file format, size, file name. You can use Get Metadata to get the file properties and If-condition can help you filter the file.</p>

<p>But that's too complexed for Data Factory to help you achieve all the features you want. </p>

<p><strong>Update:</strong></p>

<p>For example, we can parameter a file in source, :</p>

<p>Create dataset parameter <code>filename</code> and pipeline parameter <code>name</code>:
<a href=""https://i.stack.imgur.com/G6G2n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G6G2n.png"" alt=""enter image description here""></a></p>

<p>Using <code>Get metadata</code> to get its properties: <code>Item type</code>, <code>Exists</code>, <code>Size</code>, <code>Item name</code>. 
<a href=""https://i.stack.imgur.com/6b8FX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6b8FX.png"" alt=""enter image description here""></a></p>

<p>Output:
<a href=""https://i.stack.imgur.com/LDpHs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LDpHs.png"" alt=""enter image description here""></a></p>

<p>For example, We can build <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">expression</a> in <code>if-condition</code> to judge  if it's empyt(size=0):</p>

<pre><code>@equals(activity('Get Metadata1').output.size,0)
</code></pre>

<p>If Ture means it's empty, False no empty. Then we can build the workflow in True or False active.</p>

<p>Hope this helps.</p>
"
"62198162","How to validate incoming files in Azure data factory","<p>is there a way to create a template to validate incoming files including such checks as empty file checks, format, data types, record counts along, and will stop the workflow if any of the checks fail. The solution for this requirement should consider multiple file-formats and reduce the burden on ETL processing and checks to enable scale. </p>

<p>File transfer to occur either by trigger or data currency rule</p>
","<azure><validation><azure-data-factory>","2020-06-04 15:16:57","3096","0","2","62270687","<p>I demonstrate similar techniques to validate source files and take appropriate downstream actions in your pipeline based on those values <a href=""https://www.youtube.com/watch?v=jWSkJdtiJNM"" rel=""nofollow noreferrer"">in this video</a>.</p>
"
"62195948","How to do a Blob storage to API data transfer in Azure Data Factory?","<p>I'm doing a POC that taking data from a csv file which is inside the Blob storage and do some mappings or transformations and send it to a Rest API (POST). I used a Lookup and web activities for this. But we can't do any transformation with these two. I thought of doing transformation with a DataFlow Activity and save it to Blob again and used Lookup and Web activities to do the rest. I have two questions.</p>

<ol>
<li>Is there any other way to do this Blob to API copy?</li>
<li>Does Web activity has 1 min of execution time?</li>
</ol>

<p>Thanks</p>
","<azure><asp.net-core><asp.net-core-webapi><azure-data-factory><azure-blob-storage>","2020-06-04 13:31:58","712","0","1","62254002","<p><strong>Is there any other way to do this Blob to API copy?</strong></p>

<p>No, Data Factory doesn't support Rest API as Sink dataset. We only could get data from REST API and can not transfer data to it. Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Azure Data Factory connector overview</a>
<a href=""https://i.stack.imgur.com/Pjsky.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pjsky.png"" alt=""enter image description here""></a></p>

<p><strong>Does Web activity has 1 min of execution time?</strong></p>

<p>Yes, REST endpoints that the web activity invokes must return a response of type JSON. The activity will timeout at 1 minute with an error if it does not receive a response from the endpoint.</p>

<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web activity in Azure Data Factory</a>.</p>

<p>Hope this helps.</p>
"
"62194908","ADF change columns of data type in Sink using copy data","<p>Does anyone have experienced on transforming data type of columns when copying from source and sink with the specific data type in adf.</p>

<p>From Microsoft documentation, i noticed that the adf copy data can perform
1. Convert from native source types to Azure Data Factory interim data types
2. Convert from Azure Data Factory interim data types to native sink type</p>

<p>Currently, I want to copy data from a hive table which store all the column as String, so when using adf to copy data and landed it as parquet with correct data type, in which some column could be int, datetime, string and so on. Therefore, I have using dynamic mapping when copying data by creating dynamic json code.</p>

<pre><code>""translator"": {
        ""type"": ""TabularTranslator"",
        ""mappings"": [
            {
                ""source"": {
                    ""name"": ""commentid"",
                    ""type"": ""Int32""
                },
                ""sink"": {
                    ""name"": ""commentid"",
                    ""type"": ""Int32""
                }
            },
            {
                ""source"": {
                    ""name"": ""comment"",
                    ""type"": ""String""
                },
                ""sink"": {
                    ""name"": ""comment"",
                    ""type"": ""String""
                }
            },
            {
                ""source"": {
                    ""name"": ""commenteduser"",
                    ""type"": ""String""
                },
                ""sink"": {
                    ""name"": ""commenteduser"",
                    ""type"": ""String""
                }
            },
            {
                ""source"": {
                    ""name"": ""commenteddatetime"",
                    ""type"": ""String""
                },
                ""sink"": {
                    ""name"": ""commenteddatetime"",
                    ""type"": ""String""
                }
            }
        ]
    }
</code></pre>

<p>However, as a result, the copying process is successful without any error message. But when I have create table on top of these parquet file, the data type of these columns are still showing all as String.</p>

<p>Does anyone here have faced this issues before. Thanks.</p>
","<azure><azure-data-factory><azure-data-lake><azure-databricks>","2020-06-04 12:43:14","1469","0","1","62687894","<p>The JSON which you have posted I do not see any transformation in the data types . I am not sure if you just shared the columns which had the default mapping . You can try the expicit mapping and i think it should work</p>
"
"62193998","Azure Data Factory Table Storage Conversion Error","<p>I'm trying to get data from Azure Table Storage using Azure Data Factory. I have a table called orders which has 30 columns. I want to take only 3 columns from this table (PartitionKey, RowKey and DeliveryDate). The DeliveryDate column has different data types like DateTime.Null (String value) and actual datetime values. When I want to preview the data i get the following error:</p>

<p><a href=""https://i.stack.imgur.com/XVkSJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XVkSJ.png"" alt=""enter image description here""></a></p>

<p>The DataSource looks like this:</p>

<pre><code>{
""name"": ""Orders"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""AzureTableStorage"",
        ""type"": ""LinkedServiceReference""
    },
    ""annotations"": [],
    ""type"": ""AzureTable"",
    ""structure"": [
        {
            ""name"": ""PartitionKey"",
            ""type"": ""String""
        },
        {
            ""name"": ""RowKey"",
            ""type"": ""String""
        },
        {
            ""name"": ""DeliveryDate"",
            ""type"": ""String""
        }
    ],
    ""typeProperties"": {
        ""tableName"": ""Orders""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""}
</code></pre>
","<azure><azure-data-factory><azure-table-storage>","2020-06-04 11:55:40","144","0","1","62207318","<p>I test your problem,it works.Can you show me more detail about this or there is something wrong about my test?</p>

<p>Below is my test data:</p>

<p><a href=""https://i.stack.imgur.com/7kJW2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7kJW2.png"" alt=""table""></a></p>

<p><a href=""https://i.stack.imgur.com/NqOgF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NqOgF.png"" alt=""entity""></a></p>

<p><a href=""https://i.stack.imgur.com/muqTV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/muqTV.png"" alt=""preview""></a></p>

<p>The dataset code:</p>

<pre><code>{
    ""name"": ""Order"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureTableStorage"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureTable"",
        ""structure"": [
            {
                ""name"": ""PartitionKey"",
                ""type"": ""String""
            },
            {
                ""name"": ""RowKey"",
                ""type"": ""String""
            },
            {
                ""name"": ""DeliveryDate"",
                ""type"": ""String""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""Table7""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>
"
"62186296","How to pass parameters to pipeline during trigger run in Azure Data Factory?","<p>As far as I know I can pass a parameters in manual run(trigger now). But how if I want to set the pipeline to autorun everyday, and be able to pass a parameter without entering the trigger now pipeline page?</p>

<p>Another question is that during the deign of my pipeline, I have set up few parameters and logic linked to it, like ""if the parameter is null then run all table, it there is value, then only run that table"", that is for user enter re-run for specific table. 
However, I noticed that the message ""Parameters that are not provided a value will not be included in the trigger."", does that mean my logic in the pipeline cannot be setup this way if I want to trigger it automatically everyday?</p>

<p>Thanks a lot! </p>
","<azure><azure-data-factory>","2020-06-04 03:19:47","3688","0","1","62339685","<p>Implementing heavy ADF logic can be difficult. You can set the default value for parameters but I assume those need to be set dynamically? </p>

<p>You could also use pipeline variables and an Activity at the beginning of your pipeline named ""Set variable"" and work with that using expressions to run your activity based on variables that are set with parameters?</p>

<p>In our project we did even something more complicated and we deploy and trigger a Pipeline once a week from Azure Devops. So not the ADF itself triggers the pipeline but AzureDevops scheduled run does.</p>

<p>Powershell:</p>

<pre class=""lang-bsh prettyprint-override""><code>$parameters = @{
    ""parameterName1"" = $parameterValue
    ""parameterName2"" = $ParameterValue
}

Invoke-AzDataFactoryV2Pipeline -DataFactoryName $DataFactoryName -ResourceGroupName 
$ResourceGroupName -PipelineName $pipelineName -Parameter $parameters

</code></pre>

<p>With powershell you can implement any logic you really want at this point passing values to ADF.</p>
"
"62186024","ADF json expression formatting","<p>I've been doing this for about 6 hours now, so I'm turning to the crowd. 
I am using ADF to move data from and API to a DB. I'm using the REST copy data activity and I need to properly format a json for the body param with two pipeline parameters and an item from a for loop. My json needs to be formatted as such:</p>

<pre><code>""key"" : [""value""]
</code></pre>

<p>I'm have difficulty understanding how to format the json body. I believe I need to start the whole body using the json expression:</p>

<pre><code>@json('{""foo"":""bar""}')
</code></pre>

<p>But I am unable to get the pipeline parameters to be properly expressed in the json. This is makes the most sense as far as I understand it and it simply returns what you see when I peek in the input window.</p>

<pre><code>@json('{""foo"":[""activity('bar').output.value]""}

</code></pre>
","<azure-data-factory>","2020-06-04 02:46:17","458","0","1","62186533","<pre><code>""key"":[""@{activity('bar').output.value}""]
</code></pre>

<p>Works, but I still believe I should just be able to pass an array!</p>
"
"62180413","What is the definition of a data factory in ADF?","<p>What is the definition of a <em>data factory</em> in Azure Data Factory? The precise definition has proved elusive in my research. The term gets thrown around a lot but I can't find an instance where it's explicitly nailed down.</p>
","<azure-data-factory>","2020-06-03 18:44:55","55","0","1","62181414","<p>Sometimes in the context of Azure, the ADF resource itself is called a ""data factory"". Like in OOP, imagine it as an object (data factory) created from a class (Azure Data Factory). You can have multiple objects from the same class.</p>

<p>Hope this helped!</p>
"
"62178764","Get count of records in source and sink in Azure data factory","<p>Can someone tell me how to get source and sink count in Azure Data Factory after data copy activity? My source and sink are SQL Server.</p>
","<azure><count><copy><azure-data-factory>","2020-06-03 17:16:54","4288","1","3","62181873","<p>You can get that information from the output JSON of the Copy Activity. Just add an activity following your Copy in the pipeline and you can store the values in a variable or use a data flow to transform and persist the data.</p>

<p><a href=""https://i.stack.imgur.com/ZLjBg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZLjBg.png"" alt=""enter image description here""></a></p>
"
"62178764","Get count of records in source and sink in Azure data factory","<p>Can someone tell me how to get source and sink count in Azure Data Factory after data copy activity? My source and sink are SQL Server.</p>
","<azure><count><copy><azure-data-factory>","2020-06-03 17:16:54","4288","1","3","69252342","<p>As mentioned by Mark, you can use the copy activity output to store in a variable and pass that variable to a stored procedure to insert that value in a table.</p>
<p><img src=""https://i.stack.imgur.com/cvgt8.png"" alt=""Copy_data passing source records read to stored procedure"" /></p>
"
"62178764","Get count of records in source and sink in Azure data factory","<p>Can someone tell me how to get source and sink count in Azure Data Factory after data copy activity? My source and sink are SQL Server.</p>
","<azure><count><copy><azure-data-factory>","2020-06-03 17:16:54","4288","1","3","74302885","<p>May be too late to answer but could help someone.</p>
<p>In may case (reading file from HDFS) I have enabled <code>Import Schema</code> from the <code>Mapping</code> section of the <code>Copy data</code> activity and that has enabled the <code>rowsRead</code> and <code>rowsCopied</code> attributes in the json output</p>
<p><a href=""https://i.stack.imgur.com/XRFeX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XRFeX.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/7feXk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7feXk.png"" alt=""enter image description here"" /></a></p>
<p>Note: rowsRead and rowsCopied won't present by default</p>
"
"62175973","Azure devops assigment of roles rwx in ADLS gen2 container using powershell script path task in devops","<p>I am facing one issue while assiging the roles to storage container rwx so that my data factory can 
    read data from adls gen 2. Below script is running fine using inline script in azure devops 
    powershell 
    script.But when I changed it to script from file path (location is github). I have put below script 
    in .ps1 extension already.</p>

<pre><code>[CmdletBinding()]
param(
 [parameter(Mandatory = $false)] [String] $resourcegroup_name,
 [parameter(Mandatory = $false)] [String] $factoryName,
 [parameter(Mandatory = $false)] [String] $storageaccount_name
 )

$principalID = (Get-AzDataFactoryV2 -ResourceGroupName $resourcegroup_name -Name 
$factoryName).identity.PrincipalId
$storageAccount = Get-AzStorageAccount -ResourceGroupName $resourcegroup_name -AccountName 
$storageaccount_name
$ctx = $storageAccount.Context
$filesystemName = ""adftransformation""
$dirname = ""mfg/""
$dir = New-AzDataLakeGen2Item -Context $ctx -FileSystem $filesystemName -Path $dirname -Directory - 
Permission r-x -Umask ---rwx---  -Property @{""ContentEncoding"" = ""UDF8""; ""CacheControl"" = ""READ""}
$acl = (Get-AzDataLakeGen2Item -Context $ctx -FileSystem $filesystemName -Path $dirname).ACL
$acl = set-AzDataLakeGen2ItemAclObject -AccessControlType user -EntityID $principalID -Permission r-x 
-InputObject $acl 
Update-AzDataLakeGen2Item -Context $ctx -FileSystem $filesystemName -Path $dirname -Acl $acl
</code></pre>

<p>Error I am getting in devops pipline</p>

<blockquote>
  <p>2020-06-03T14:29:18.8399468Z ##[error]Cannot validate argument on
  parameter 'Permission'. The 
      argument ""r-x"" does not match the ""([r-][w-][x-]){3}"" pattern. Supply an argument that matches ""([r-] 
      [w-][x-]){3}"" and try the command again.</p>
</blockquote>

<p>I am not sure why this is happening while choosing script as file path same script is running fine in 
    inline script path</p>

<p><a href=""https://i.stack.imgur.com/CP0Q3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CP0Q3.png"" alt=""enter image description here""></a></p>
","<github><azure-devops><azure-data-factory><azure-powershell><azure-data-lake-gen2>","2020-06-03 14:56:25","220","0","1","62321227","<pre><code>This is the correct script  from Githubif you are taking as a file path.

[CmdletBinding()]
param(
 [parameter(Mandatory = $false)] [String] $resourcegroup_name,
 [parameter(Mandatory = $false)] [String] $factoryName,
 [parameter(Mandatory = $false)] [String] $storageaccount_name
 )
$principalID = (Get-AzDataFactoryV2 -ResourceGroupName $resourcegroup_name -Name 
$factoryName).identity.PrincipalId
$storageAccount = Get-AzStorageAccount -ResourceGroupName $resourcegroup_name - 
AccountName $storageaccount_name
$ctx = $storageAccount.Context
$filesystemName = ""fsname""
$dirname = ""mfg/""
$dir = New-AzDataLakeGen2Item -Context $ctx -FileSystem $filesystemName -Path $dirname 
-Directory -Permission rwxrwxrwx -Umask ---rwx---  -Property @{""ContentEncoding"" = 
""UDF8""; ""CacheControl"" = ""READ""}
$acl = (Get-AzDataLakeGen2Item -Context $ctx -FileSystem $filesystemName -Path 
$dirname).ACL
$acl = set-AzDataLakeGen2ItemAclObject -AccessControlType user -EntityID $principalID 
-Permission rwx -InputObject $acl 
Update-AzDataLakeGen2Item -Context $ctx -FileSystem $filesystemName -Path $dirname - 
Acl $acl
</code></pre>
"
"62162189","Table storage showing data in only string format","<p>I'm using ADF pipeline to copy data from data lake to blob storage and then from blob storage to table storage. </p>

<p><a href=""https://i.stack.imgur.com/tHoCk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tHoCk.png"" alt=""enter image description here""></a></p>

<p>As you can see below, here are the column types in ADF Data Flow Sink - Blob Storage (integer, string, timestamp):</p>

<p><a href=""https://i.stack.imgur.com/LSvTE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LSvTE.png"" alt=""enter image description here""></a></p>

<p>Here is the Mapping settings in Copy data activity:</p>

<p><a href=""https://i.stack.imgur.com/HbTOH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HbTOH.png"" alt=""enter image description here""></a></p>

<p>On checking the output in table storage, I see all columns are of string type:</p>

<p><a href=""https://i.stack.imgur.com/pdzH9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pdzH9.png"" alt=""enter image description here""></a></p>

<p>Why is table storage saving data in string values? How do I resolve this issue in table storage so that it will accept columns in the right type (integer, string, timestamp)? Please let me know. Thank you!</p>
","<azure-table-storage><azure-data-factory>","2020-06-02 22:13:42","244","0","2","62165214","<p>In usually, when load data from blob storage in Data Factory, <strong>all the default data type in blob file are String</strong>, Data Factory will help you convert the data type automatically to Sink. </p>

<p>But it also can not meet all our requests.</p>

<p>I tested copy data from Blob to Table Storage and found that: <strong>if we don't specify the data type manually in Source, after pipeline executed, all the data type will be String in Sink(Table Storage)</strong>.</p>

<p>For example, this my Source blob file:
<a href=""https://i.stack.imgur.com/nhwdM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nhwdM.png"" alt=""enter image description here""></a></p>

<p>If I don't change the source data type, it seems that everything is ok in Sink table:
<a href=""https://i.stack.imgur.com/LLtxh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LLtxh.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/2YL8h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2YL8h.png"" alt=""enter image description here""></a></p>

<p>But after the pipeline executed, the data type in table storage are all String:
<a href=""https://i.stack.imgur.com/UuYp8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UuYp8.png"" alt=""enter image description here""></a></p>

<p>If we change the data type in Source blob manually, and it works ok! 
<a href=""https://i.stack.imgur.com/DVoWF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DVoWF.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/vu3SA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vu3SA.png"" alt=""enter image description here""></a></p>

<p>For another question, a little confuse that just from you screenshot, that seems the UI of Mapping Data Flow Sink, but Mapping Data Flow doesn't support Table Storage as Sink.</p>

<p>Hope this helps.</p>
"
"62162189","Table storage showing data in only string format","<p>I'm using ADF pipeline to copy data from data lake to blob storage and then from blob storage to table storage. </p>

<p><a href=""https://i.stack.imgur.com/tHoCk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tHoCk.png"" alt=""enter image description here""></a></p>

<p>As you can see below, here are the column types in ADF Data Flow Sink - Blob Storage (integer, string, timestamp):</p>

<p><a href=""https://i.stack.imgur.com/LSvTE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LSvTE.png"" alt=""enter image description here""></a></p>

<p>Here is the Mapping settings in Copy data activity:</p>

<p><a href=""https://i.stack.imgur.com/HbTOH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HbTOH.png"" alt=""enter image description here""></a></p>

<p>On checking the output in table storage, I see all columns are of string type:</p>

<p><a href=""https://i.stack.imgur.com/pdzH9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pdzH9.png"" alt=""enter image description here""></a></p>

<p>Why is table storage saving data in string values? How do I resolve this issue in table storage so that it will accept columns in the right type (integer, string, timestamp)? Please let me know. Thank you!</p>
","<azure-table-storage><azure-data-factory>","2020-06-02 22:13:42","244","0","2","62317782","<p>Finally figured out the issue - I was using DelimitedText format for Blob Storage. After converting to Parquet format, I can see data being written to table storage in correct type.</p>
"
"62161758","""source and sink in the copy activity must be connected via the same self-hosted integration runtime.""","<p>I have a requirement where I need to run azure data factory pipeline from copying data from On-Premise database to Azure SQL Server Virtual Machine database(using VM data server due to limitations with azure SQL database and Azure SQL managed instance).</p>

<p>Hence I have created two self-hosted integration Runtime, 1 for on-prem VM database server, and another for azure VM data server.</p>

<p>But when I validate or run the pipeline, getting below error message
""source and sink in the copy activity must be connected via the same self-hosted integration runtime.""</p>

<p>Can some please suggest the possible solution if any...</p>
","<azure-virtual-machine><azure-data-factory>","2020-06-02 21:34:01","2247","0","1","62181786","<p>Basically it means that you cannot have 2 self-hosted integration runtimes in a single copy activity. What you are trying to do is copy from an onpremise source to another onpremise sink, but the IRs are different.</p>

<p>You will need to use 2 copy activities and an intermediate storage. For example:</p>

<p>1st copy: take the data from the source sql to a blob storage as a csv.</p>

<p>2nd copy: grab that csv and insert its data into sink database.</p>

<p>This way, you will be using only 1 IR for each copy activity.</p>

<p>Hope this helped!</p>
"
"62156680","error creating an Azure ML linked service in Azure Data Factory : The client xxx with object id xxx does not have authorization to perform action '","<p>On Azure Data Factory I'm trying to create a linked Azure ML Service to be able to invoke a published model in ADF.</p>

<p>It gives me the following error : </p>

<blockquote>
  <p>Request sent to Azure ML Service for operation 'validateWorkspace' failed with http status code 'Forbidden'. Error message from Azure ML Service: '{""error"":{""code"":""AuthorizationFailed"",""message"":""The client XXX with object id XXX does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '../Microsoft.MachineLearningServices/workspaces/myworkspace' or the scope is invalid. If access was recently granted, please refresh your credentials.""}}'.</p>
</blockquote>

<p>When I check on Azure ML Service workspace name <strong>Acces Control IAM</strong> ,
the related ADF has data factory contributor role on <strong>This Resource</strong> scope.</p>

<p>When I check on ADF <strong>Acces Control IAM</strong> ,
the related Azure ML Workspace has contributor role on <strong>This Resource</strong> scope.</p>

<p>Thank you.</p>
","<azure><azure-data-factory><azure-machine-learning-service>","2020-06-02 16:26:07","872","0","2","62245824","<p>In the DataFactory, you can test the connection to the Workspace.
Have you done this?</p>

<p>If this doesn't work, then re-establish the linked service in your DataFactory and make sure the keys are properly set.</p>
"
"62156680","error creating an Azure ML linked service in Azure Data Factory : The client xxx with object id xxx does not have authorization to perform action '","<p>On Azure Data Factory I'm trying to create a linked Azure ML Service to be able to invoke a published model in ADF.</p>

<p>It gives me the following error : </p>

<blockquote>
  <p>Request sent to Azure ML Service for operation 'validateWorkspace' failed with http status code 'Forbidden'. Error message from Azure ML Service: '{""error"":{""code"":""AuthorizationFailed"",""message"":""The client XXX with object id XXX does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '../Microsoft.MachineLearningServices/workspaces/myworkspace' or the scope is invalid. If access was recently granted, please refresh your credentials.""}}'.</p>
</blockquote>

<p>When I check on Azure ML Service workspace name <strong>Acces Control IAM</strong> ,
the related ADF has data factory contributor role on <strong>This Resource</strong> scope.</p>

<p>When I check on ADF <strong>Acces Control IAM</strong> ,
the related Azure ML Workspace has contributor role on <strong>This Resource</strong> scope.</p>

<p>Thank you.</p>
","<azure><azure-data-factory><azure-machine-learning-service>","2020-06-02 16:26:07","872","0","2","68951136","<p>A little late to this one, but perhaps it will help someone else.</p>
<p>I had this exact same issue and solved it by setting the ADF as Owner of the AML workspace.</p>
<pre><code>az role assignment create --role &quot;Owner&quot; --assignee-object-id &lt;adf-object-id&gt; --scope /subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;rg-name&gt;/providers/Microsoft.MachineLearningServices/workspaces/&lt;aml-workspace-name&gt;
</code></pre>
<p>Or this can be done in the portal:</p>
<p>Navigate to your AML workspace -&gt; Access Control (IAM) -&gt; + Add -&gt; Add role assignment.
Then add your data factory as the owner.</p>
"
"62151632","Issue with retrieving CRM D365 Option set field Data using Azure Data Factory","<p>I have connected to my D635 instance by creating a pipeline and Dataset in azure Data Factory and have given fetchXML as source to retrieve the data. But when I preview the data I can only see number values for Option set fields in my output data.</p>

<p>Could anyone clarify me if we can normalize Optionset field values in ADF and use it?</p>

<p>Thanks!</p>
","<azure><microsoft-dynamics><azure-data-factory>","2020-06-02 12:05:52","774","1","1","64481465","<p>You can use a <strong>link-entity</strong> element in your FetchXML query. You just need to lookup the objecttypecode of your entity.</p>
<p>Once you have the value you can use the following to bring in the textual value of an OptionSet selection.</p>
<pre><code>    &lt;link-entity name=&quot;stringmap&quot; from=&quot;attributevalue&quot; to=&quot;____&quot; visible=&quot;false&quot; link-type=&quot;outer&quot; alias=&quot;____&quot;&gt;
        &lt;filter type=&quot;and&quot;&gt;
            &lt;condition attribute=&quot;objecttypecode&quot; operator=&quot;eq&quot; value=&quot;___&quot;/&gt;
        &lt;/filter&gt;
        &lt;attribute name=&quot;value&quot;/&gt;
    &lt;/link-entity&gt;
</code></pre>
<p><strong>to</strong> should be the name of the OptionSet column on the host/root entity</p>
<p><strong>alias</strong> whatever you want to call the column in the output. I used the same value as <strong>to</strong></p>
<p><strong>value</strong> This is the object type code for your host/root entity. It is an integer value.</p>
"
"62145572","How do i load my sample data to Azure data lake and visualize in Power Bi?","<p>I have a sample data (.CSV)  and I'm able to visualize the same in my <strong>Power Bi desktop</strong> by using get data->text file. I want to load the same data into <strong>Azure data lakes</strong> so that i can access it from  get data-> Azure data lake storage in my Power BI. </p>

<p>My azure data factory and my Power Bi accounts are trial ones. How do i load data into my azure data lake? When i try to do it it is asking for my git repository which is not necessary i suppose.</p>
","<azure><powerbi><azure-data-factory><azure-data-lake>","2020-06-02 05:41:28","95","-1","1","62145863","<p>To get your sample data into the Data Lake, you can go into the Azure Portal, find the data lake object and find the 'Storage Explorer' option. That will open up a file manager like interface to allow folder creation and upload files.</p>

<p><a href=""https://i.stack.imgur.com/inFKU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/inFKU.png"" alt=""enter image description here""></a></p>

<p>You can also use <a href=""https://azure.microsoft.com/en-gb/features/storage-explorer/"" rel=""nofollow noreferrer"">Azure Storage Explorer</a> to load the file.</p>

<p>To load the data in a continuing process, you can use Azure Data Factory. You haven't mentioned if your Data Lake is Gen 1 or Gen 2, but the MS Docs website give a good start on how to use a copy activity to push files to the data lake</p>

<p>Gen 1 - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/load-azure-data-lake-store"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/load-azure-data-lake-store</a></p>

<p>Gen 2 - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/load-azure-data-lake-storage-gen2"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/load-azure-data-lake-storage-gen2</a></p>
"
"62142530","Select last row from csv in Azure Data Factory","<p>I'm pulling in a small ( less than 100kb ) dataset as csv. All I want to do is select the last row of that data and sink it into a different location. </p>

<p>I cannot seem to find a simple way to do this. </p>

<p>I have tried a wrangling data flow, but the ""keep rows"" M function is not supported - though you can select it, it just results in an error. That's annoying because it does exactly what I need in one fell swoop.</p>

<p>I sort of get it working using a last() function on each field, but that is a lot of messing around and it's slow.</p>

<p>Surely there is a better way to do this simple task?</p>

<p>Would greatly appreciate any assistance.</p>

<p>Thanks</p>
","<azure><csv><azure-data-factory>","2020-06-01 23:30:56","1390","0","1","62144098","<p>Mapping Data Flows: Surrogate Key, Aggregate (max), Filter (max row)</p>
"
"62139827","Parsing json key:value in Data Factory","<p>I have a exchange rate API output like such:</p>

<p><a href=""https://i.stack.imgur.com/fDOO9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fDOO9.png"" alt=""enter image description here""></a></p>

<p>I would like get an array to pass through another pipeline as a parameter to iterate over. However I have been fruitless in my endeavors. I would like to have an array of the currencies. In ADF terms I want to set my array variable to the properties of @activity('API Call').output.rates .</p>

<p>I have tried setting @activity('API Call').output.rates to both an array and a string variable (adding string() and array() to the content with out any luck. However, I have been able to set the string variable to @activity('API Call').output.rates.AED/AFN etc, however that is not what I needed. Those curly brackets around rates have got me a little stumped, if they were square brackets no problemo. I've also tried a few goes with string functions to no avail. </p>

<p>Can anyone with familiarity of Azure Dynamic Content push me in the right direction? </p>
","<azure-data-factory>","2020-06-01 19:44:28","1983","2","2","62337368","<p>Rates is an object...  to get an array of the properties inside... you should use javascript's Object.keys() function</p>

<pre><code>var keyArray = Object.keys(@activity('API Call').output.rates);

for(var index = 0; index &lt; keyArray.length; index++){
    var message = 'Value of ' + keyArray[index];
    message += ' is: ' + @activity('API Call').output.rates[keyArray[index]];
    console.log(message);
}
</code></pre>
"
"62139827","Parsing json key:value in Data Factory","<p>I have a exchange rate API output like such:</p>

<p><a href=""https://i.stack.imgur.com/fDOO9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fDOO9.png"" alt=""enter image description here""></a></p>

<p>I would like get an array to pass through another pipeline as a parameter to iterate over. However I have been fruitless in my endeavors. I would like to have an array of the currencies. In ADF terms I want to set my array variable to the properties of @activity('API Call').output.rates .</p>

<p>I have tried setting @activity('API Call').output.rates to both an array and a string variable (adding string() and array() to the content with out any luck. However, I have been able to set the string variable to @activity('API Call').output.rates.AED/AFN etc, however that is not what I needed. Those curly brackets around rates have got me a little stumped, if they were square brackets no problemo. I've also tried a few goes with string functions to no avail. </p>

<p>Can anyone with familiarity of Azure Dynamic Content push me in the right direction? </p>
","<azure-data-factory>","2020-06-01 19:44:28","1983","2","2","69199661","<p>I ended up working around this solution by querying a reference table with the keys and iterating over the object. Obviously not the lightest and cleanest solution but it did fit better into my process than utilizing a function or anything like that.</p>
<p>The concession I had to make was that the keys were going to have to be known before hand, but given that new currencies do not pop up often (with the exception of cryptos, which I'm not working with).</p>
"
"62139646","How to get counts from AlterRow transformation in Azure Data Factory","<p>I have an AlterRow transformation that marks each row with the appropriate CRUD operation in an ADFv2 data flow.  I don't see any output variables on this activity that will give me the total inserts, updates, etc.  I do, however, see methods in the expression syntax to tell me if a particular row is an IsInsert(), IsUpdate(), etc.  </p>

<p>Would the correct way to get counts be to </p>

<ol>
<li>Add another output from the AlterRow transformation</li>
<li>Add derived column that uses the expression syntax IsInsert(), IsUpdate() to set operation type (I, U, D)</li>
<li>Add an aggregate to group by this column to get total counts for each operation</li>
</ol>

<p>When creating the aggregate, I don't see any metadata that would allow me to group by the CRUD operation type so I assume I would have to create this myself, but it seems like it should already be there since that's the purpose of the AlterRow transformation.  Am I working too hard to get these counts?</p>
","<azure-data-factory>","2020-06-01 19:32:36","124","1","1","62144448","<p>Add an aggregate after your AlterRow with no group-by and use these formulas:</p>

<p><a href=""https://i.stack.imgur.com/Iduyp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iduyp.png"" alt=""enter image description here""></a></p>
"
"62136990","java.lang.OutOfMemoryError Copying parquet blob to backup blob using snappy compression","<p>I'm using a Copy Data activity to back up a parquet file in Azure blob storage (source) to another Azure blob storage container (sink). I selected 'snappy' compression for the sink. I am using DefaultIntegrationRuntime (South Central US) - so not self-hosted, meaning I have no control over the environment. I run this on several files at least once a day. Occasionally, I'll get the following error for a source file that is almost 1G in size. There is another source file that is 1.6G in size that never throws the same error.</p>

<pre><code>    Code:21000,
    Message:Failure happened on 'Sink' side.
    ErrorCode=ParquetJavaInvocationException,
    '
        Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
        Message=An error occurred when invoking java,
        message: java.lang.OutOfMemory
        Error:Direct buffer memory
        total entry:19
        java.nio.Bits.reserveMemory(Bits.java:658)
        java.nio.DirectByteBuffer.(DirectByteBuffer.java:123)
        java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
        org.apache.parquet.hadoop.codec.SnappyCompressor.setInput(SnappyCompressor.java:97)
        org.apache.parquet.hadoop.codec.NonBlockedCompressorStream.write(NonBlockedCompressorStream.java:48)
        org.apache.parquet.bytes.CapacityByteArrayOutputStream.writeToOutput(CapacityByteArrayOutputStream.java:219)
        org.apache.parquet.bytes.CapacityByteArrayOutputStream.writeTo(CapacityByteArrayOutputStream.java:239)
        org.apache.parquet.bytes.BytesInput$CapacityBAOSBytesInput.writeAllTo(BytesInput.java:392)
        org.apache.parquet.bytes.BytesInput$SequenceBytesIn.writeAllTo(BytesInput.java:283)
        org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:165)
        org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:98)
        org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:148)
        org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:236)
        org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:122)
        org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:169)
        org.apache.parquet.hadoop.InternalParquetRecordWriter.checkBlockSizeReached(InternalParquetRecordWriter.java:143)
        org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:125)
        org.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:292)
        com.microsoft.datatransfer.bridge.parquet.ParquetBatchWriter.addRows(ParquetBatchWriter.java:61)
        .,
        Source=Microsoft.DataTransfer.Richfile.ParquetTransferPlugin,
    '
    '
        Type=Microsoft.DataTransfer.Richfile.JniExt.JavaBridgeException,
        Message=,
        Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,
    ',
    EventType:0,
    Category:5,
    Data:{FailureInitiator:Sink},
    MsgId:null,
    ExceptionType:null,
    Source:null,
    StackTrace:null,
    InnerEventInfos:[]
</code></pre>

<p>I tried raising the DIUs to 8, but the runtime only used 4 and I still get the error. How can I debug this? </p>
","<azure-data-factory>","2020-06-01 16:50:51","263","0","1","62557591","<p>From MS Support:</p>
<blockquote>
<p>I am working with ADF Product Group (PG) to find a solution for this
randomly failing copy activity issue. I have currently opened an
incident ticket to my ADF PG and they told me that this is a known
issue which is causing failures intermittently when
compress/decompress with snappy and sometimes with your data.</p>
<p>So, recently we have released a fix but it is not fully enabled yet in the
backend...</p>
<p>Our ADF PG has turned on the feature for you. Could you
please run your job and let me know if you still see any issues.</p>
</blockquote>
<p>The error is so intermittent, there have been days in the past before the error occurs. I will report back in a couple weeks with the results.</p>
"
"62134686","Can I trigger my Azure data factory pipeline in 5 working day between 9 am to 6 pm hourly","<p>I have a Azure data factory pipeline that load data of every working day(Run every working day). I want to trigger my pipeline every working day (Mon-Fry) between working hour(9am to 6pm) and hourly.
It should run as daily at 9am then 10am then 11am--------at 6pm.
I have tried tumbling window trigger but I think it does not support time period for trigger interval</p>
","<azure><azure-data-factory>","2020-06-01 14:39:49","1580","2","2","62137667","<p>The recurrence patterns in ADF and Logic Apps don't directly support this combination of requirements. You are certainly welcome to attempt this in an ADF pipeline, but I find Logic Apps to be much easier for this sort of action. Below is an example of how you might configure it:</p>

<p><strong>Create a Logic App with a Recurrence Trigger</strong></p>

<p>In the Trigger, change the Frequency to Day and specify the hours and timezone:
<a href=""https://i.stack.imgur.com/v2JeP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v2JeP.png"" alt=""enter image description here""></a></p>

<p><strong>Calculate the day of the week</strong> </p>

<p>Us the following expression (change to the appropriate timezone) to extract the day of the week into a variable:</p>

<pre><code>dayOfWeek(convertFromUtc(utcNow(), 'Eastern Standard Time'))
</code></pre>

<p>This will return an integer where 0 = Sunday, 1 = Monday, etc.
<a href=""https://i.stack.imgur.com/PZbXL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PZbXL.png"" alt=""enter image description here""></a></p>

<p><strong>Add condition based on the day of the week</strong></p>

<p>Use the dayOfWeek integer variable to determine which days to act (or ignore). In this example, I'm exceluding days 0 (Sunday) and 6 (Saturday):
<a href=""https://i.stack.imgur.com/5jxjB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5jxjB.png"" alt=""enter image description here""></a></p>

<p><strong>Execute the data factory</strong></p>

<p>In the True condition, execute your data factory (do nothing in the False condition):
<a href=""https://i.stack.imgur.com/6Xpwz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Xpwz.png"" alt=""enter image description here""></a></p>
"
"62134686","Can I trigger my Azure data factory pipeline in 5 working day between 9 am to 6 pm hourly","<p>I have a Azure data factory pipeline that load data of every working day(Run every working day). I want to trigger my pipeline every working day (Mon-Fry) between working hour(9am to 6pm) and hourly.
It should run as daily at 9am then 10am then 11am--------at 6pm.
I have tried tumbling window trigger but I think it does not support time period for trigger interval</p>
","<azure><azure-data-factory>","2020-06-01 14:39:49","1580","2","2","68116404","<p>In your trigger, you can select a recurrence of one week, then select the working days and then the hours you are interested in.</p>
<p><a href=""https://i.stack.imgur.com/KE8f4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KE8f4.png"" alt=""Data factory schedule"" /></a></p>
"
"62134430","How to do undo in Azure Data Factory","<p>I am new to Azure Data factory. While developing the pipeline I could not find undo operation in Azure Data Factory. <code>ctrl+z</code> did not work. What is the keyboard shortcut for the undo?</p>
","<azure-data-lake><azure-data-factory>","2020-06-01 14:27:02","6597","6","2","62137546","<p>There is no undo shortcut for ADF. Max you can do is the Discard All option.</p>

<p><a href=""https://i.stack.imgur.com/A40nc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/A40nc.png"" alt=""Discard All Option""></a></p>
"
"62134430","How to do undo in Azure Data Factory","<p>I am new to Azure Data factory. While developing the pipeline I could not find undo operation in Azure Data Factory. <code>ctrl+z</code> did not work. What is the keyboard shortcut for the undo?</p>
","<azure-data-lake><azure-data-factory>","2020-06-01 14:27:02","6597","6","2","68572545","<p>If you are in git mode, you're able to revert any change using a git method of choice. ADF basically stores a few .json files in your git repo, any saves you make in the UI is pushed to the git repository. Changes made in your development branch will also be reflected in the UI after a page refresh.</p>
<p>For example, installing Github for Windows &amp; cloning your collaboration branch enables you to see any change that has been done and which JSON was affected.</p>
<ul>
<li>Right clicking on a commit, and clicking 'revert commit' reverses the selected change. Push it back to your remote git repository, and refresh your page on ADF to see the changes reflected.</li>
<li>If you want to selectively undo, you're going to have to change the JSON yourself in order to partially undo a commit
<ul>
<li>save all your current changes, even though there are ones you do not want.</li>
<li>use the line-by-line differences provided by git (in your most recent changes) in order to identify / isolate your changes, adjust them &amp; push these changes back to the collaboration branch.\</li>
</ul>
</li>
</ul>
"
"62132592","Deleting documents from Cosmos Db collection during incremental load using Azure Data factory","<p>I am trying to copy data from Azure SQL to Azure Cosmos Db Collection (Mongo API). I am using upsert to insert/update the documents on Cosmos Db. However, if any data from source is deleted, how do I delete that document from Cosmos Db.</p>
","<azure-cosmosdb><azure-data-factory>","2020-06-01 12:49:57","789","1","1","62168267","<p>As far as I am aware,it is impossible to deleting documents from Cosmos Db collection during incremental load.</p>

<p>Because,incremental load is comparing <code>LastModifytime</code>.If you delete rows in azure sql,them  don't exist in source and copy data only supports <code>insert</code> and <code>update</code>.</p>

<p>If you want to synchronized your data,please delete them in cosmos db manually.
You can run the delete sql in cosmos db or add a column <code>DeleteStatus</code>.When you want to delete data,update <code>DeleteStatus</code> and <code>LastModifytime</code>then incremental load.Finally,run the sql both in cosmos db and azure sql:</p>

<p><code>delete from xxxx where DeleteStatus = 1</code></p>

<p>Hope these can help you.</p>
"
"62128023","ADFv1 to ADFv2 migration for CosmosToBlobCopyActivity","<p>I'm migrating some of the pipelines under ADFv1 to ADFv2.</p>

<p>I found one pipeline having activity as type <strong>CosmosToBlobCopyActivity</strong>. This is not CosmosDB, but Cosmos structure stream data.</p>

<p>What is the equivalent in ADFv2</p>

<pre><code>""activities"": [
        {
            ""type"": ""CosmosToBlobCopyActivity"",
            ""typeProperties"": {
                ""EmailRecipients"": ""ABC@XYZ.com""
            },
            ""inputs"": [
                {
                    ""name"": ""DS_INPUT""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""DS_OUTPUT""
                }
            ],
            ""policy"": {
                ""timeout"": ""04:00:00"",
                ""concurrency"": 1,
                ""executionPriorityOrder"": ""NewestFirst"",
                ""retry"": 3,
                ""longRetry"": 10,
                ""longRetryInterval"": ""01:10:00""
            },
            ""scheduler"": {
                ""frequency"": ""Day"",
                ""interval"": 1
            },
            ""name"": ""CosmosToBlobCopyActivity"",
            ""description"": ""Pipeline Activity: CosmosToBlobCopyActivity"",
            ""linkedServiceName"": ""DEMOLS""
        }
    ]
</code></pre>
","<azure-data-factory><cosmos>","2020-06-01 08:10:51","48","0","1","62423672","<p>Azure DataFactory dont have an activity type named 'CosmosToBlobCopyActivity'. This is a custom activity in ADF v1. So you need to create a custom activity in ADF v2.</p>

<p>You can have a look of these below doc:</p>

<p>This is how to create custom activity in v1:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-use-custom-activities#create-a-pipeline-with-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-use-custom-activities#create-a-pipeline-with-custom-activity</a></p>

<p>This is how to create custom activity in v2:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a></p>
"
"62127820","Azure Data Factory Rest API pagination issue","<p>I am new to Azure Data Factory and i am trying to copy data from JIRA to Azure Storage using Data Factory.
I am trying the JIRA REST API to fetch the data, but unfortunately i am struggling with pagination. Can Someone help me how to use the pagination in this case.</p>

<p>Request URL:
<a href=""https://jira.abc.com/rest/api/3/search?jql=&amp;startAt=0&amp;maxResult=10"" rel=""nofollow noreferrer"">https://jira.abc.com/rest/api/3/search?jql=&amp;startAt=0&amp;maxResult=10</a></p>

<p>Sample response from JIRA:</p>

<pre><code>{
    ""startAt"" : 0,
    ""maxResults"" : 10,
    ""total"": 200,
    ""isLast"": false,
    ""values"": [
        { /* result 0 */ },
        { /* result 1 */ },
        { /* result 2 */ }
    ]
}
</code></pre>
","<azure><jira><azure-data-factory><jira-rest-api>","2020-06-01 07:56:20","1122","0","1","62139941","<ol>
<li>Click on the Copy Data activity (the block) in your pipeline</li>
<li>Click on
the Source tab</li>
<li>Scroll down to Pagination rules and click the plus
sign to add a rule</li>
<li>As highlighted below, put <code>AbsoluteURL</code> in the first box
and then, for JIRA it appears (according to their docs) that you need to put <code>$._links.next</code> instead of <code>$.next_page</code> in my screenshot.  (This is the reference to the part of the JSON doc returned by JIRA containing the URL of the next page.)</li>
</ol>

<p><a href=""https://i.stack.imgur.com/LhJgQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LhJgQ.png"" alt=""enter image description here""></a></p>

<p>Please comment if it works for you.</p>

<p>Refs:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">MS data factory docs</a></p>

<p><a href=""https://developer.atlassian.com/server/confluence/pagination-in-the-rest-api/"" rel=""nofollow noreferrer"">JIRA API docs</a></p>
"
"62106501","Azure Data Factory Copy Pipeline Did Not Copy Data Into The Sink","<p>I created a test copy pipeline in azure data factory which basically reads 2 sample rows from a text file in azure blob storage and loads it into a table in Azure sql database, the run was successful.</p>

<p>However no records were inserted into the table. The source file is only 34 bytes, i read the minimum block size for azure blob storage is 64KB, could it because my test file is too small that azure failed to read it even though the pipeline ran successfully?</p>
","<azure-blob-storage><azure-data-factory>","2020-05-30 17:51:13","668","1","1","62110910","<blockquote>
  <p>could it because my test file is too small that azure failed to read
  it even though the pipeline ran successfully?</p>
</blockquote>

<p>I believe this is not related to the file size because i did test with single row and it works fine for me.</p>

<p>Please try following suggestions:</p>

<p>1.Check the configuration of sink dataset if it is the exactly what you want.</p>

<p>2.Check the preview the data of source dataset if it is correct.</p>

<p>3.Check the monitor log of your pipeline, especially the input size and output size.</p>

<p><a href=""https://i.stack.imgur.com/ZadxI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZadxI.png"" alt=""enter image description here""></a></p>

<p>4.Try to configure another sink dataset, for example blob storage, to check if the data could be transferred into blob storage sucessfully.</p>

<p>If you have other concern, please feel free to let me know.</p>
"
"62095749","CosmosDB source returning DF-SYS-01","<p>I've built some pipelines and had success importing data from most of my CosmosDB tables, but this one constantly gives me an error which I don't understand. I think it might be caused by table structure, but would highly appreciate second opinion and possible solutions.</p>

<p>Table item:</p>

<pre><code>{
""id"": ""someGuid"",
""name"": ""someString"",    
""pins"": [
    {
        ""type"": ""someString"",
        ""latitude"": 47.03923,
        ""longitude"": -122.89136,
        ""name"": ""someString""
    },
    {
        ""type"": ""someString"",
        ""latitude"": 28.53823,
        ""longitude"": -81.37739,
        ""name"": ""someString""
    }
],
""_rid"": ""vj04AOrfr2s8CT0AAAAAAA=="",
""_self"": ""dbs/vj04AA==/colls/vj04AOrfr2s=/docs/vj04AOrfr2s8CT0AAAAAAA==/"",
""_etag"": ""\""ac00ddc8-0000-0700-0000-5e7428230000\"""",
""_attachments"": ""attachments/"",
""_ts"": 1584670755
}
</code></pre>

<p>Columns are identified correct in Source.Projection Tab where pins are []string, but source can't be loaded (( </p>

<pre><code>""{""message"":""at : (StructType(StructField(area,StringType,true), StructField(date,StringType,true), StructField(resultType,StringType,true), StructField(results,ArrayType(StringType,true),true), StructField(test,StringType,true)),StringType) (of class scala.Tuple2). Details:at : (StructType(StructField(area,StringType,true), StructField(date,StringType,true), StructField(resultType,StringType,true), StructField(results,ArrayType(StringType,true),true), StructField(test,StringType,true)),StringType) (of class scala.Tuple2)"",""failureType"":""UserError"",""target"":""Pins"",""errorCode"":""DFExecutorUserError""}""
</code></pre>
","<azure-data-factory>","2020-05-29 22:59:03","43","0","1","62159301","<p>Was able to solve it by : 1. restarting ADF session  2. Resetting schema for data source - it automatically picked correct schema. Hopefully this answer will help someone in the feature.</p>
"
"62093434","Azure Data Factory Limits","<p>I have created a simple pipeline that operates as such:</p>

<ol>
<li><p>Generates an access token via an Azure Function. No problem. </p></li>
<li><p>Uses a Lookup activity to create a table to iterate through the rows (4 columns by 0.5M rows). No problem.</p></li>
<li>For Each activity (sequential off, batch-size = 10):</li>
<li>(within For Each): Set some variables for checking important values.</li>
<li>(within For Each): Pass values through web activity to return a json.</li>
<li>(within For Each): Copy Data activity mapping parts of the json to the sink-dataset (postgres). </li>
</ol>

<p>Problem: The pipeline slows to a crawl after approximately 1000 entries/inserts. </p>

<p>I was looking at this <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/azure-data-factory-limits.md"" rel=""nofollow noreferrer"">documentation</a> regarding the limits of ADF.</p>

<ul>
<li>ForEach items: 100,000</li>
<li>ForEach parallelism: 20</li>
</ul>

<p>I would expect that this falls within in those limits unless I'm misunderstanding it. 
I also cloned the pipeline and tried it by offsetting the query in one, and it tops out at 2018 entries.</p>

<p>Anyone with more experience be able to give me some idea of what is going on here? </p>
","<azure-data-factory>","2020-05-29 19:33:46","1357","0","1","62123174","<p>As a suggestion, whenever I have to fiddle with variables inside a foreach, I made a new pipeline for the foreach process, and call it from within the foreach. That way I make sure that the variables get their own context for each iteration of the foreach.</p>

<p>Have you already checked that the bottleneck is not at the source or sink? If the database or web service is under some stress, then going sequential may help if your scenario allows that.</p>

<p>Hope this helped!</p>
"
"62089013","Azure Data Factory Pricing - Activity Count","<p>I'm thinking of using Data Factory in order to copy data from a Blob Storage container to an SQL table but I'm not quite sure I understand how the pricing works, specifically how the activities are counted.</p>

<p>So if I have a pipeline with 3 activities that copies the data from a CSV with 1000 lines will the total activity count be 3*1 or 3*1000? In other words, will I be charged based on the number o files it processes or the total number of lines it copies?</p>
","<azure><azure-data-factory>","2020-05-29 15:05:58","734","0","2","62089539","<p>That's 3 activity runs. Activity runs are measured by the thousand, at $1 per. Since these are Copy activities, they consume Data Integration Units (DIU) at $.25 per hour. Pipeline execution time is billed at $.005 per hour. If you add all this up for 1 pipeline with 3 Copy activities that runs for 1 hour, your total bill is like 27 cents.</p>

<p>We run THOUSANDS of pipelines a month, all with many activities including quite a few Copy activities. Our Data Factory billing is still so low that it looks like a rounding error in our total Azure spend.</p>

<p>The exception to this is Data Flow. Data Flow is a Spark wrapper, so you have to pay for Cluster time, which can get expensive quickly if you aren't careful.</p>
"
"62089013","Azure Data Factory Pricing - Activity Count","<p>I'm thinking of using Data Factory in order to copy data from a Blob Storage container to an SQL table but I'm not quite sure I understand how the pricing works, specifically how the activities are counted.</p>

<p>So if I have a pipeline with 3 activities that copies the data from a CSV with 1000 lines will the total activity count be 3*1 or 3*1000? In other words, will I be charged based on the number o files it processes or the total number of lines it copies?</p>
","<azure><azure-data-factory>","2020-05-29 15:05:58","734","0","2","62089572","<p>Actually, you have to pay for 2 important metrics: Orchestration and Execution. Please refer to more details from this <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">document</a>.</p>

<p>1.Orchestration, $1 per 1,000 runs. You have 3 activities, then it should be $ 3/1000.</p>

<p><a href=""https://i.stack.imgur.com/ZNz7Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZNz7Y.png"" alt=""enter image description here""></a></p>

<p>2.Execution, it depends on the DIU you configured,which means the performance of your transmission.</p>

<p><a href=""https://i.stack.imgur.com/xlVsd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xlVsd.png"" alt=""enter image description here""></a></p>
"
"62086586","Can we automate ETL in Azure?","<p>I am currently working on a very interesting ETL project using Azure to transform my data manually. However, transforming data manually can be exhausting and lengthy when I start having several source files to process. My pipeline is working fine for now because I have only a few files to transform but what if I have thousands of excel files? </p>

<p>So what I want to achieve is that I want to extend the project and extract the excel files that are coming from Email using the logic app then apply ETL directly on top of them. Is there any way I can automate ETL in Azure. Can I do ETL without modifying the pipeline for a different type of data manually? How can I make my pipeline flexible to be able to handle data transformation for various types of source data? </p>

<p>Thank you in advance for your help. </p>
","<automation><azure-pipelines><etl><azure-data-factory>","2020-05-29 12:58:17","106","1","1","62088053","<blockquote>
  <p>Can I do ETL without modifying the pipeline for a different type of
  data manually?</p>
</blockquote>

<p>According to your description, i suppose that you already knew the <a href=""https://learn.microsoft.com/en-us/connectors/connector-reference/connector-reference-logicapps-connectors"" rel=""nofollow noreferrer"">ADF connector</a> is supported in the Logic App. You could execute ADF pipeline in the Logic App flow and even pass parameters into ADF pipeline.</p>

<p>Normally, the source and sink service should be fixed in one copy activity, but you could define dynamic file path in the datasets. So you don't need to create multiple copy activities.</p>

<p>If the data types are different, you could try to pass the parameter from Logic App into ADF. Then before the data transmission, you could use Switch activity to route the transmission into different branches.</p>

<p><a href=""https://i.stack.imgur.com/O1C5p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O1C5p.png"" alt=""enter image description here""></a></p>
"
"62081781","How to transfer files from Azure Data Lake to Network File Folder(Remote server folder) using Azure Data Factory","<p>Is it possible to transfer files from Azure Data Lake to remote file server using Azure Data Factory. I don't have any problem with source but i am not sure how i can achieve for destination.</p>

<p>Am I able to transfer files from ADL to local file folder but I am unsure about how we can do this with remote server.</p>

<p>Help me if you have any idea on these.</p>
","<azure><azure-data-factory><azure-data-lake><file-format>","2020-05-29 08:30:18","397","0","1","62088452","<p>Per my knowledge, all the transmissions in ADF are hosted on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">Integration Runtime</a>. So i suppose that there are 2 conditions in this scenario:</p>

<p>1.Your remote server is hosted in the public network, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-integration-runtime"" rel=""nofollow noreferrer"">Default IR</a> should be used. You could consider to expose REST API of your remote file server and try to configure it as sink dataset in the copy activity.</p>

<p>2.Your remote server is hosted in private network, then <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#self-hosted-integration-runtime"" rel=""nofollow noreferrer"">Self-Hosted IR</a> should be used and you have to install Self-Hosted IR. However, currently, MS only supports running the self-hosted IR on a Windows operating system according to the official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#self-hosted-ir-compute-resource-and-scaling"" rel=""nofollow noreferrer"">statement</a>.</p>
"
"62079068","Copy file from One Zone to Other - Azure Data Lake Gen2","<p>I want to copy a file with given path from One Zone of Azure Data Lake to Other Zone of Data Lake.</p>

<p><strong>Example:</strong></p>

<p><strong>Source</strong>: /RawZone/Incremental/2020/05/01/file.parquet</p>

<p><strong>Destination</strong>: /StdZone/Incremental/2020/05/01/file.parquet</p>

<p>Should i be using Copy Activity to read source as dataset and write to Destination.  Or is there a way to just copy file from source to destination in <strong>Azure Data Factory</strong>.</p>
","<azure-data-factory><azure-data-lake>","2020-05-29 05:06:36","84","0","1","62081829","<p>As far as I am aware the Copy Activity is the only way.</p>

<p>You will need a dataset to define where the file is coming from and going (though the path can be parameterised) and its format.</p>

<p>If you want to copy the file as is without alteration, set the dataset format to <code>binary</code> to avoid having to define the file structure and 'waste time' extracting and parsing the data within.</p>
"
"62078862","Skip lines while reading csv - Azure Data Factory","<p>I am trying to copy data from <strong>Blob</strong> to <strong>Azure SQL</strong> using <strong>data flows</strong> within a pipeline.
Data Files is in <strong>csv</strong> format and the <strong>Header is at 4th row</strong> in the csv file.
i want to use the header as is what is available in the csv data file.</p>

<p>I want to <strong>loop</strong> through all the files and upload data.</p>

<p>Thanks</p>
","<azure-blob-storage><azure-data-factory>","2020-05-29 04:41:09","5756","1","2","62079632","<p>Add a Surrogate Key transformation and then a Filter transformation to filter out row number 4.</p>
"
"62078862","Skip lines while reading csv - Azure Data Factory","<p>I am trying to copy data from <strong>Blob</strong> to <strong>Azure SQL</strong> using <strong>data flows</strong> within a pipeline.
Data Files is in <strong>csv</strong> format and the <strong>Header is at 4th row</strong> in the csv file.
i want to use the header as is what is available in the csv data file.</p>

<p>I want to <strong>loop</strong> through all the files and upload data.</p>

<p>Thanks</p>
","<azure-blob-storage><azure-data-factory>","2020-05-29 04:41:09","5756","1","2","66746749","<p>You need to first uncheck the &quot;First row as header&quot; in your CSV dataset. Then you can use the &quot;Skip line count&quot; field in the copy data activity source tab and skip any number of lines you want.</p>
"
"62068154","Azure Data Factory Lookup and For Each","<p>I have a Data Factory Pipeline that I want to have iterate through the rows of a SQL Lookup activity. I have narrowed the query down to three columns and 500 rows. </p>

<p>I understand that to reference a value in the table I use:</p>

<pre><code>@{activity('lookupActivity').output.value[row#].colname}
</code></pre>

<p>However, the for each needs to have something to iterate over. My first guess is to set some array variable to the rows of the returned sql query. So what do I set that variable to?</p>

<pre><code>@{activity('lookupActivity').output.value?
</code></pre>

<p>Lastly, it looks like almost all data is represented as a json in ADF, is this true? And how could I view the output of this look up as a json so I can understand what my dynamic content needs to look like?</p>
","<azure-data-factory>","2020-05-28 15:12:52","16913","4","2","62074848","<p>You can use the output value to for each activity and go through one at a time. You can do sequential or parallel depending on your needs.</p>
"
"62068154","Azure Data Factory Lookup and For Each","<p>I have a Data Factory Pipeline that I want to have iterate through the rows of a SQL Lookup activity. I have narrowed the query down to three columns and 500 rows. </p>

<p>I understand that to reference a value in the table I use:</p>

<pre><code>@{activity('lookupActivity').output.value[row#].colname}
</code></pre>

<p>However, the for each needs to have something to iterate over. My first guess is to set some array variable to the rows of the returned sql query. So what do I set that variable to?</p>

<pre><code>@{activity('lookupActivity').output.value?
</code></pre>

<p>Lastly, it looks like almost all data is represented as a json in ADF, is this true? And how could I view the output of this look up as a json so I can understand what my dynamic content needs to look like?</p>
","<azure-data-factory>","2020-05-28 15:12:52","16913","4","2","62076522","<p>You're right that everything (nearly) is JSON.  (Exception: <a href=""https://stackoverflow.com/questions/53412439/azure-data-factory-v2-activity-execute-pipeline-output)/62038291#62038291"">Azure Data Factory v2: Activity execute pipeline output</a></p>

<p>So you can put your <code>@activity('lookupActivity').output.value</code> which is an array into the foreach activity on the settings tab, like this</p>

<p><a href=""https://i.stack.imgur.com/mGU3O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mGU3O.png"" alt=""enter image description here""></a></p>

<p>Then inside your foreach loop, you reference the current value of one of the columns as <code>@item().colname</code>.</p>
"
"62064967","How to extract data with dd-mm-yyyy format from string column in azure data flow?","<p>I have a dataset that contains dates and string together. I want to extract the date then save it in a <code>date</code> column and string in the <code>task</code> column. I am using azure data flow to achieve this data transformation.</p>

<pre><code>regexExtract({Finish Date Activity}, '^([0-2][0-9]|(3)[0-1])(\-)(((0)[0-9])|((1)[0-2]))(\-)\d{4}$', 1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/1iGGc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1iGGc.png"" alt=""enter image description here""></a></p>

<p>But this does not seem to work for me and getting <code>Unable to parse the expression. Please make sure it is valid.</code> error. Can anyone help me to solve this, please?</p>
","<regex><azure><regex-group><azure-data-factory>","2020-05-28 12:41:46","523","1","1","62065146","<p>You may use</p>

<pre><code>((?:0?[1-9]|[12][0-9]|3[01])-(?:0?[1-9]|1[0-2])-\d{2}(?:\d{2})?)
</code></pre>

<p>Or, if your dates are always at the start of the text:</p>

<pre><code>^((?:0?[1-9]|[12][0-9]|3[01])-(?:0?[1-9]|1[0-2])-\d{2}(?:\d{2})?)
</code></pre>

<p>See the <a href=""https://regex101.com/r/mG2y5N/1"" rel=""nofollow noreferrer"">regex demo</a></p>

<p><strong>Details</strong></p>

<ul>
<li><code>^</code> - start of string</li>
<li><code>(</code> - start of a capturing group #1 (you extract this group value with <code>1</code> argument)</li>
<li><code>(?:0?[1-9]|[12][0-9]|3[01])</code> - a non-capturing group: day value</li>
<li><code>-</code> - a hyphen</li>
<li><code>(?:0?[1-9]|1[0-2])</code> - month part</li>
<li><code>-</code> - a hyphen</li>
<li><code>\d{2}(?:\d{2})?</code> - two- or four-digit year.</li>
</ul>
"
"62062053","Azure Data Factory Pipeline errorCode","<p>I have a relatively simple process set up in Azure Data Factory to copy, cleanse and process some log files from a chatbot which has been running fine until I recently started getting the following errorCode:</p>

<pre><code>    ""errorCode"": ""InvalidTemplate"",
    ""message"": ""Unable to process expressions for action 'EvaluatefinaliseTSCRPTS': 'The function 'bool' was invoked with a parameter that is not valid. The value cannot be converted to the target type"",
    ""failureType"": ""UserError"",
    ""target"": ""finaliseTSCRPTS"",
    ""details"": """"
</code></pre>

<p>I can't seem to identify the error in the ADF process despite going through the code for my process below:</p>

<pre><code>
""name"": ""SearchBot dailyTranscripts"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""MST Validation"",
            ""type"": ""Validation"",
            ""dependsOn"": [],
            ""userProperties"": [],
            ""typeProperties"": {
                ""dataset"": {
                    ""referenceName"": ""teamsLogs"",
                    ""type"": ""DatasetReference""
                },
                ""timeout"": ""0.00:00:30"",
                ""sleep"": 10,
                ""childItems"": true
            }
        },
        {
            ""name"": ""Get MST-TSCRPTS"",
            ""type"": ""Copy"",
            ""dependsOn"": [
                {
                    ""activity"": ""MST Validation"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""userProperties"": [],
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""JsonSource"",
                    ""storeSettings"": {
                        ""type"": ""AzureBlobStorageReadSettings"",
                        ""recursive"": true,
                        ""wildcardFileName"": ""*.json"",
                        ""enablePartitionDiscovery"": false
                    }
                },
                ""sink"": {
                    ""type"": ""JsonSink"",
                    ""storeSettings"": {
                        ""type"": ""AzureBlobStorageWriteSettings"",
                        ""copyBehavior"": ""MergeFiles""
                    },
                    ""formatSettings"": {
                        ""type"": ""JsonWriteSettings"",
                        ""quoteAllText"": true
                    }
                },
                ""enableStaging"": false,
                ""translator"": {
                    ""type"": ""TabularTranslator"",
                    ""mappings"": [
                        {
                            ""source"": {
                                ""path"": ""$['type']""
                            },
                            ""sink"": {
                                ""path"": ""$['type']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['timestamp']""
                            },
                            ""sink"": {
                                ""path"": ""$['timestamp']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['id']""
                            },
                            ""sink"": {
                                ""path"": ""$['id']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['channelId']""
                            },
                            ""sink"": {
                                ""path"": ""$['channelId']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['serviceUrl']""
                            },
                            ""sink"": {
                                ""path"": ""$['serviceUrl']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['from']['id']""
                            },
                            ""sink"": {
                                ""path"": ""$['from']['id']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['from']['aadObjectId']""
                            },
                            ""sink"": {
                                ""path"": ""$['from']['aadObjectId']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['from']['role']""
                            },
                            ""sink"": {
                                ""path"": ""$['from']['role']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['from']['name']""
                            },
                            ""sink"": {
                                ""path"": ""$['from']['name']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['conversation']['conversationType']""
                            },
                            ""sink"": {
                                ""path"": ""$['conversation']['conversationType']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['conversation']['tenantId']""
                            },
                            ""sink"": {
                                ""path"": ""$['conversation']['tenantId']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['conversation']['id']""
                            },
                            ""sink"": {
                                ""path"": ""$['conversation']['id']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['recipient']['id']""
                            },
                            ""sink"": {
                                ""path"": ""$['recipient']['id']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['recipient']['name']""
                            },
                            ""sink"": {
                                ""path"": ""$['recipient']['name']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['recipient']['aadObjectId']""
                            },
                            ""sink"": {
                                ""path"": ""$['recipient']['aadObjectId']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['recipient']['role']""
                            },
                            ""sink"": {
                                ""path"": ""$['recipient']['role']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['channelData']['tenant']['id']""
                            },
                            ""sink"": {
                                ""path"": ""$['channelData']['tenant']['id']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['text']""
                            },
                            ""sink"": {
                                ""path"": ""$['text']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['inputHint']""
                            },
                            ""sink"": {
                                ""path"": ""$['inputHint']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['replyToId']""
                            },
                            ""sink"": {
                                ""path"": ""$['replyToId']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['textFormat']""
                            },
                            ""sink"": {
                                ""path"": ""$['textFormat']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['localTimestamp']""
                            },
                            ""sink"": {
                                ""path"": ""$['localTimestamp']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['locale']""
                            },
                            ""sink"": {
                                ""path"": ""$['locale']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['value']""
                            },
                            ""sink"": {
                                ""path"": ""$['value']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['valueType']""
                            },
                            ""sink"": {
                                ""path"": ""$['valueType']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['name']""
                            },
                            ""sink"": {
                                ""path"": ""$['name']""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$['label']""
                            },
                            ""sink"": {
                                ""path"": ""$['label']""
                            }
                        }
                    ]
                }
            },
            ""inputs"": [
                {
                    ""referenceName"": ""teamsLogs"",
                    ""type"": ""DatasetReference""
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""transcriptsStaging"",
                    ""type"": ""DatasetReference""
                }
            ]
        },
        {
            ""name"": ""finaliseTSCRPTS"",
            ""type"": ""IfCondition"",
            ""dependsOn"": [
                {
                    ""activity"": ""MST Validation"",
                    ""dependencyConditions"": [
                        ""Completed""
                    ]
                },
                {
                    ""activity"": ""Get MST-TSCRPTS"",
                    ""dependencyConditions"": [
                        ""Succeeded"",
                        ""Skipped""
                    ]
                }
            ],
            ""userProperties"": [],
            ""typeProperties"": {
                ""expression"": {
                    ""value"": ""activity('MST Validation').output.exists"",
                    ""type"": ""Expression""
                },
                ""ifTrueActivities"": [
                    {
                        ""name"": ""Combine TSCRPTS"",
                        ""type"": ""Copy"",
                        ""dependsOn"": [],
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""JsonSource"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageReadSettings"",
                                    ""recursive"": true,
                                    ""wildcardFileName"": ""*.json"",
                                    ""enablePartitionDiscovery"": false
                                }
                            },
                            ""sink"": {
                                ""type"": ""JsonSink"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageWriteSettings"",
                                    ""copyBehavior"": ""MergeFiles""
                                },
                                ""formatSettings"": {
                                    ""type"": ""JsonWriteSettings"",
                                    ""quoteAllText"": true
                                }
                            },
                            ""enableStaging"": false
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""transcriptsStaging"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""SearchBotDailyTranscripts"",
                                ""type"": ""DatasetReference"",
                                ""parameters"": {
                                    ""sourceFileName"": ""@concat(formatDateTime(utcnow(), 'yyyy-MM-dd'),'.json')""
                                }
                            }
                        ]
                    },
                    {
                        ""name"": ""Delete Staging"",
                        ""type"": ""Delete"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""Combine TSCRPTS"",
                                ""dependencyConditions"": [
                                    ""Succeeded""
                                ]
                            }
                        ],
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""dataset"": {
                                ""referenceName"": ""transcriptsStaging"",
                                ""type"": ""DatasetReference""
                            },
                            ""enableLogging"": false,
                            ""storeSettings"": {
                                ""type"": ""AzureBlobStorageReadSettings"",
                                ""recursive"": true
                            }
                        }
                    },
                    {
                        ""name"": ""Get monthlyTSCRPTS"",
                        ""type"": ""Copy"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""Combine TSCRPTS"",
                                ""dependencyConditions"": [
                                    ""Succeeded""
                                ]
                            }
                        ],
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""JsonSource"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageReadSettings"",
                                    ""recursive"": true
                                }
                            },
                            ""sink"": {
                                ""type"": ""JsonSink"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageWriteSettings""
                                },
                                ""formatSettings"": {
                                    ""type"": ""JsonWriteSettings"",
                                    ""quoteAllText"": true
                                }
                            },
                            ""enableStaging"": false
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""SearchBotDailyTranscripts"",
                                ""type"": ""DatasetReference"",
                                ""parameters"": {
                                    ""sourceFileName"": ""@concat('2019-',formatDateTime(utcnow(), 'MM'),'-??.json')""
                                }
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""SearchBotMonthlyTranscripts"",
                                ""type"": ""DatasetReference"",
                                ""parameters"": {
                                    ""sourceFileName"": ""@dataset().sourceFileName""
                                }
                            }
                        ]
                    },
                    {
                        ""name"": ""Get yearlyTSCRPTS"",
                        ""type"": ""Copy"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""Get monthlyTSCRPTS"",
                                ""dependencyConditions"": [
                                    ""Succeeded""
                                ]
                            }
                        ],
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""JsonSource"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageReadSettings"",
                                    ""recursive"": true
                                }
                            },
                            ""sink"": {
                                ""type"": ""JsonSink"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageWriteSettings"",
                                    ""copyBehavior"": ""MergeFiles""
                                },
                                ""formatSettings"": {
                                    ""type"": ""JsonWriteSettings"",
                                    ""quoteAllText"": true
                                }
                            },
                            ""enableStaging"": false
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""SearchBotMonthlyTranscripts"",
                                ""type"": ""DatasetReference"",
                                ""parameters"": {
                                    ""sourceFileName"": {
                                        ""value"": ""@concat(formatDateTime(utcnow(), 'yyyy'),'-??.json')"",
                                        ""type"": ""Expression""
                                    }
                                }
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""SearchBotYearlyTranscripts"",
                                ""type"": ""DatasetReference"",
                                ""parameters"": {
                                    ""sourceFileName"": ""@dataset().sourceFileName""
                                }
                            }
                        ]
                    },
                    {
                        ""name"": ""Copy MST-TSCRPTS"",
                        ""type"": ""Copy"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""Delete Staging"",
                                ""dependencyConditions"": [
                                    ""Succeeded""
                                ]
                            }
                        ],
                        ""policy"": {
                            ""timeout"": ""0.00:01:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""JsonSource"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageReadSettings"",
                                    ""recursive"": true,
                                    ""wildcardFileName"": ""*.json"",
                                    ""enablePartitionDiscovery"": false
                                }
                            },
                            ""sink"": {
                                ""type"": ""JsonSink"",
                                ""storeSettings"": {
                                    ""type"": ""AzureBlobStorageWriteSettings"",
                                    ""copyBehavior"": ""MergeFiles""
                                },
                                ""formatSettings"": {
                                    ""type"": ""JsonWriteSettings"",
                                    ""quoteAllText"": true
                                }
                            },
                            ""enableStaging"": false
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""teamsLogs"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""transcriptsHistory"",
                                ""type"": ""DatasetReference""
                            }
                        ]
                    },
                    {
                        ""name"": ""Delete MST-TSCRPTS"",
                        ""type"": ""Delete"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""Copy MST-TSCRPTS"",
                                ""dependencyConditions"": [
                                    ""Succeeded"",
                                    ""Failed""
                                ]
                            }
                        ],
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""dataset"": {
                                ""referenceName"": ""teamsLogs"",
                                ""type"": ""DatasetReference""
                            },
                            ""enableLogging"": false,
                            ""storeSettings"": {
                                ""type"": ""AzureBlobStorageReadSettings"",
                                ""recursive"": true
                            }
                        }
                    },
                    {
                        ""name"": ""run learningList"",
                        ""type"": ""ExecutePipeline"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""Delete MST-TSCRPTS"",
                                ""dependencyConditions"": [
                                    ""Succeeded"",
                                    ""Failed"",
                                    ""Skipped""
                                ]
                            }
                        ],
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""pipeline"": {
                                ""referenceName"": ""runLearningList"",
                                ""type"": ""PipelineReference""
                            },
                            ""waitOnCompletion"": true
                        }
                    }
                ]
            }
        }
    ],
    ""parameters"": {
        ""sourceFileName"": {
            ""type"": ""string"",
            ""defaultValue"": ""@concat(formatDateTime(utcnow(),'yyyy-MM-dd'),'.json')""
        }
    },
    ""annotations"": []
},
""type"": ""Microsoft.DataFactory/factories/pipelines""
</code></pre>

<p>There is no expression 'EvaluatefinaliseTSCRPTS' and I can't find a 'bool' function either. The only hint I've found was a previous question <a href=""https://stackoverflow.com/questions/53754827/adf-v2-failure-when-using-bool-variable"">here</a></p>
","<azure-data-factory>","2020-05-28 10:04:31","1003","0","1","62067531","<p>You have an IF activity named ""finaliseTSCRPTS"", so ""EvaluatefinaliseTSCRPTS"" is most likely the internal name of the function that performs the IF condition check. The message seems to indicate that it cannot evaluate your expression ""activity('MST Validation').output.exists"".</p>
"
"62062029","Azure Data Factory Dataflows - dynamic sink paramterers","<p>I'm working on dataflows that will handle my dimensions load. 
I wanted it to be as parametrized as it can be so i created generic source and sink (both Azure Synapse).
In debug settings of dataflows i can put requested values (tableName and schema name).
It is working for source without issue however i have no idea why but sink is not reading values</p>

<p>I got 
Connection failed
{ ""Message"": ""No value provided for Parameter 'tableName'"" } - RunId: 27be90a3-294a-48fa-93f0-d3fc2d6df3f5
but in debug parametes it's provided.
Anyone knows how to fix it?</p>

<p><a href=""https://i.stack.imgur.com/r74EZ.png"" rel=""nofollow noreferrer"">Debug settings</a></p>
","<azure><azure-data-factory>","2020-05-28 10:03:34","361","0","1","62089435","<p>I tried to reproduce your issue:</p>

<p><a href=""https://i.stack.imgur.com/nHcWc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nHcWc.png"" alt=""enter image description here""></a></p>

<p>Then i configured default value in the sink dataset parameter to solve the issue:</p>

<p><a href=""https://i.stack.imgur.com/kav07.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kav07.png"" alt=""enter image description here""></a></p>
"
"62060159","Use ADF Pipeline RUN ID as table name","<p>I'm trying to use ADF pipeline RUN ID as table name. On running the pipeline, I see the following error:</p>

<p><strong>EDIT</strong></p>

<p>table name contains invalid characters</p>

<p><a href=""https://i.stack.imgur.com/Jmhnp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jmhnp.png"" alt=""enter image description here""></a></p>

<p>Could you please suggest a workaround on this issue? Thank you!</p>
","<azure-data-factory>","2020-05-28 08:24:13","480","0","1","62076629","<p>From <a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-name-rules#microsoftstorage"" rel=""nofollow noreferrer"">here</a> the name has to be alpha and start with a letter so how about this dynamic content:</p>

<p><code>tbl@{replace(pipeline().RunId,'-','')}</code></p>
"
"62058624","How to load data into sql sink using azure?","<p>I am trying to load data into the Azure SQL database using azure data flow. But for some reason, I cannot connect to my sink database. I have tried everything, but no luck. I am able to connect to the SQL database when I create a dataset, but why can't I connect to the database while using the dataset as my sink.</p>

<p>Please find the snippet of my flow below:
<a href=""https://i.stack.imgur.com/df6pg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/df6pg.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/A8pee.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A8pee.png"" alt=""enter image description here""></a></p>

<p>But when I want to use this dataset to load the data in the data flow, it is giving me the following error. The reason I want to load the data into SQL is that I want to delete rows and CSV and other datasets won't let me do this.</p>

<p><a href=""https://i.stack.imgur.com/A8owJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A8owJ.png"" alt=""enter image description here""></a></p>

<p>I would really appreciate it if anyone can help me with this. </p>
","<azure><azure-sql-database><sink><azure-data-factory>","2020-05-28 06:47:10","98","0","1","62060246","<p>Open your <code>Allow Azure services and resources to access this server</code> option and add your Client IP at the firewall settings of Azure SQL database.</p>

<p>Please follow this:</p>

<p><a href=""https://i.stack.imgur.com/1zf6a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1zf6a.png"" alt=""enter image description here""></a></p>
"
"62056682","Use dynamic value as table name of a table storage in Azure Data Factory","<p>I have an ADF pipeline that uses copy data activity for copying data from blob storage to table storage. This pipeline runs on a trigger once every day. I have provided a table name in table storage data set as 'Table1'.</p>

<p><a href=""https://i.stack.imgur.com/xNZ80.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xNZ80.png"" alt=""enter image description here""></a></p>

<p>Instead of providing a hard coded table name value (Table1), is it possible to provide a dynamic value as table name in the table storage such that <strong>RUN ID</strong> of pipeline run is used as the table name in the table storage and copy data from blob to that table in table storage?</p>

<p><a href=""https://i.stack.imgur.com/of23Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/of23Z.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/HX11o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HX11o.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-05-28 03:44:12","1899","1","1","62057295","<p>You could set  a dynamic value as table name.</p>

<p>For example, you can add parameter to the table storage dataset:</p>

<p><a href=""https://i.stack.imgur.com/Xt40s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xt40s.png"" alt=""enter image description here""></a></p>

<p>Then you can set the pipeline parameter to specify the table name:</p>

<p><a href=""https://i.stack.imgur.com/zT112.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zT112.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/M4iSv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M4iSv.png"" alt=""enter image description here""></a></p>

<p>But we can not provide the RUN ID of pipeline run  as the table name in the table storage   and copy data from blob to that table in table storage.</p>

<p>Hope this helps.</p>
"
"62055324","Copy data activity in Azure Data Factory","<p>I have an adf pipeline that uses copy data activity for copying data from blob storage to table storage. This pipeline runs on a trigger once every day. I'm curious to know how this copy data activity actually works. During pipeline run, does it clear the table and copy data from blob to table or does it just add any updated rows to table after comparison?</p>

<p>For example:</p>

<p>After 1st run, table contains below rows:</p>

<pre><code>Row1 xyz 1

Row2 abc **2**
</code></pre>

<p>Now let's say rows in blob are updated as follows:</p>

<pre><code>Row1 xyz 1

Row2 abc **5**
</code></pre>

<p>During 2nd run, does it clear table and  store the above data or only update Row2 with values abc and ""5"" instead of ""2""</p>
","<azure-data-factory>","2020-05-28 01:10:12","651","0","1","62078588","<p>ADF copy activity doesn't update any existing rows in the table. It is an append only process. So, If your table has primary key constraint, the copy activity will fail.</p>

<p>To do an upsert process, you can either use mapping data flow or stored procedure.</p>
"
"62054515","Loading records into Dynamics 365 through ADF","<p>I'm using the Dynamics connector in Azure Data Factory.</p>

<h1>TLDR</h1>

<p>Does this connector support loading child records which need a parent record key passed in? For example if I want to create a <code>contact</code> and attach it to a parent <code>account</code>, I upsert a record with a null <code>contactid</code>, a valid <code>parentcustomerid</code> GUID and set <code>parentcustomeridtype</code> to 1 (or 2) but I get an error.</p>

<h1>Long Story</h1>

<p>I'm successfully connecting to Dynamics 365 and extracting data (for example, the <code>lead</code> table) into a SQL Server table</p>

<p>To test that I can transfer data the other way, I am simply loading the data back from the <code>lead</code> table into the <code>lead</code> entity in Dynamics.</p>

<p>I'm getting this error:</p>

<blockquote>
  <p>Failure happened on 'Sink' side. ErrorCode=DynamicsMissingTargetForMultiTargetLookupField,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=,Source=,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot find the target column for multi-target lookup field: 'ownerid'.</p>
</blockquote>

<p>As a test I removed <code>ownerid</code> from the list of source columns it loads OK.</p>

<p>This is obviously a foreign key value.</p>

<p>It raises two questions for me:</p>

<ol>
<li><p>Specifically with regards to the error message: If I knew which lookup it needed to use, how can I specify which lookup table it should validate against? There's no settings in the ADF connector to allow me to do this.</p></li>
<li><p>This is obviously a foreign key value. If I only had the name (or business key) for this row, how can I easily lookup the foreign key value?</p></li>
</ol>

<p>How is this normally done through other API's, i.e. the web API?</p>

<p>Is there an XRMToolbox addin that would help clarify?</p>

<p>I've also read some posts that imply that you can send pre-connected data in an XML document so perhaps that would help also.</p>

<h1>EDIT 1</h1>

<p>I realised that the <code>lead.ownertypeid</code> field in my source dataset is <code>NULL</code> (that's what was exported).  It's also NULL if I browse it in various Xrmtoolbox tools. I tried hard coding it to <code>systemuser</code> (which is what it actually is in the <code>owner</code> table against the actual owner record) but I still get the same error.</p>

<p>I also notice there's a record with the same PK value in <code>systemuser</code> table</p>

<p>So the same record is in two tables, but how do I tell the dynamics connector which one to use? and why does it even care?</p>

<h1>EDIT 2</h1>

<p>I was getting a similar message for <code>msauto_testdrive</code> for <code>customerid</code>. </p>

<p>I excluded all records with <code>customerid=null</code>, and got the same error.</p>

<h1>EDIT 2</h1>

<p><a href=""https://help.scribesoft.com/scribeinsight/en/Subsystems/AdapterForDynamicsCRM/reference/customerid_and_customeridtype.htm"" rel=""nofollow noreferrer"">This link</a> appears to indicate that I need to set <code>customeridtype</code> to 1 (Account) or 2 (Contact). I did so, but still got the same error.</p>

<p>Also I believe I have the <a href=""https://stackoverflow.com/questions/27373906/ms-crm-2013-attribute-parentcustomeridtype-must-not-be-null-if-attribute-parentc"">same issue as this guy</a>. </p>

<p>Maybe the ADF connector suffers from the same problem.</p>
","<dynamics-crm><azure-data-factory><common-data-service><xrmtoolbox>","2020-05-27 23:30:21","3124","3","2","62289261","<p>This is the ADF limitation with respect to CDS polymorphic lookups like Customer and Owner. <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/37250212-azure-data-factory-dynamics-365-connector-dataset"" rel=""nofollow noreferrer"">Upvote this ADF idea</a></p>

<p>Workaround is to use two temporary source lookup fields (owner team and user in case of owner, account and contact in case of customer) and with parallel branch in a MS Flow to solve this issue. <a href=""https://crmchap.co.uk/sink-limitations-with-the-dynamics-365-customer-engagement-common-data-service-connector-for-azure-data-factory/"" rel=""nofollow noreferrer"">Read more</a>, also you can <a href=""https://crmchap.co.uk/wp-content/uploads/2019/06/ADFImportCustomerFieldData.zip"" rel=""nofollow noreferrer"">download the Flow sample</a> to use.</p>

<blockquote>
  <ul>
  <li>First, create two temporary lookup fields on the entity that you wish to import Customer lookup data into it, to both the Account and Contact entities respectively  </li>
  <li>Within your ADF pipeline flow, you will then need to map the GUID values for your Account and Contact fields to the respective lookup fields created above. The simplest way of doing this is to have two separate columns within your source dataset – one containing Account GUID’s to map and the other, Contact.  </li>
  <li>Then, finally, you can put together a Microsoft Flow that then performs the appropriate mapping from the temporary fields to the Customer lookup field. First, define the trigger point for when your affected Entity record is created (in this case, Contact) and add on some parallel branches to check for values in either of these two temporary lookup fields</li>
  </ul>
</blockquote>

<p><a href=""https://i.stack.imgur.com/bRzVb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bRzVb.png"" alt=""enter image description here""></a></p>

<blockquote>
  <ul>
  <li>Then, if either of these conditions is hit, set up an Update record task to perform a single field update, as indicated below if the ADF Account Lookup field has data within it </li>
  </ul>
</blockquote>
"
"62054515","Loading records into Dynamics 365 through ADF","<p>I'm using the Dynamics connector in Azure Data Factory.</p>

<h1>TLDR</h1>

<p>Does this connector support loading child records which need a parent record key passed in? For example if I want to create a <code>contact</code> and attach it to a parent <code>account</code>, I upsert a record with a null <code>contactid</code>, a valid <code>parentcustomerid</code> GUID and set <code>parentcustomeridtype</code> to 1 (or 2) but I get an error.</p>

<h1>Long Story</h1>

<p>I'm successfully connecting to Dynamics 365 and extracting data (for example, the <code>lead</code> table) into a SQL Server table</p>

<p>To test that I can transfer data the other way, I am simply loading the data back from the <code>lead</code> table into the <code>lead</code> entity in Dynamics.</p>

<p>I'm getting this error:</p>

<blockquote>
  <p>Failure happened on 'Sink' side. ErrorCode=DynamicsMissingTargetForMultiTargetLookupField,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=,Source=,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot find the target column for multi-target lookup field: 'ownerid'.</p>
</blockquote>

<p>As a test I removed <code>ownerid</code> from the list of source columns it loads OK.</p>

<p>This is obviously a foreign key value.</p>

<p>It raises two questions for me:</p>

<ol>
<li><p>Specifically with regards to the error message: If I knew which lookup it needed to use, how can I specify which lookup table it should validate against? There's no settings in the ADF connector to allow me to do this.</p></li>
<li><p>This is obviously a foreign key value. If I only had the name (or business key) for this row, how can I easily lookup the foreign key value?</p></li>
</ol>

<p>How is this normally done through other API's, i.e. the web API?</p>

<p>Is there an XRMToolbox addin that would help clarify?</p>

<p>I've also read some posts that imply that you can send pre-connected data in an XML document so perhaps that would help also.</p>

<h1>EDIT 1</h1>

<p>I realised that the <code>lead.ownertypeid</code> field in my source dataset is <code>NULL</code> (that's what was exported).  It's also NULL if I browse it in various Xrmtoolbox tools. I tried hard coding it to <code>systemuser</code> (which is what it actually is in the <code>owner</code> table against the actual owner record) but I still get the same error.</p>

<p>I also notice there's a record with the same PK value in <code>systemuser</code> table</p>

<p>So the same record is in two tables, but how do I tell the dynamics connector which one to use? and why does it even care?</p>

<h1>EDIT 2</h1>

<p>I was getting a similar message for <code>msauto_testdrive</code> for <code>customerid</code>. </p>

<p>I excluded all records with <code>customerid=null</code>, and got the same error.</p>

<h1>EDIT 2</h1>

<p><a href=""https://help.scribesoft.com/scribeinsight/en/Subsystems/AdapterForDynamicsCRM/reference/customerid_and_customeridtype.htm"" rel=""nofollow noreferrer"">This link</a> appears to indicate that I need to set <code>customeridtype</code> to 1 (Account) or 2 (Contact). I did so, but still got the same error.</p>

<p>Also I believe I have the <a href=""https://stackoverflow.com/questions/27373906/ms-crm-2013-attribute-parentcustomeridtype-must-not-be-null-if-attribute-parentc"">same issue as this guy</a>. </p>

<p>Maybe the ADF connector suffers from the same problem.</p>
","<dynamics-crm><azure-data-factory><common-data-service><xrmtoolbox>","2020-05-27 23:30:21","3124","3","2","62386057","<p>At the time of writing, @Arun Vinoth was 100% correct. However shortly afterwards there was a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365#writing-data-to-a-lookup-field"" rel=""nofollow noreferrer"">documentation</a> update (in response to a GitHub I raised) that explained how to do it.</p>

<p>I'll document how I did it here.</p>

<p>To populate a contact with against a parent account, you need the parent accounts GUID. Then you prepare a dataset like this:</p>

<pre><code>SELECT 
-- a NULL contactid means this is a new record
CAST(NULL as uniqueidentifier) as contactid,
-- the GUID of the parent account
CAST('A7070AE2-D7A6-EA11-A812-000D3A79983B' as uniqueidentifier) parentcustomerid,
-- customer id is an account
'account' [parentcustomerid@EntityReference],
'Joe' as firstname,
'Bloggs' lastname,
</code></pre>

<p>Now you can apply the normal automapping approach in ADF.</p>

<p>Now you can select from this dataset and load into <code>contact</code>. You can apply the usual automapping approach, this is: create datasets without schemas. Perform a copy activity without mapping columns</p>
"
"62053842","deleting rows in azure data flow","<p>I am trying to clean a data frame In azure data flow using alter row operation. I  have created a blob link service with CSV file (5 columns). Then created a data flow as follows: Please refer to the image attached.</p>

<p>enter image description here 
<a href=""https://i.stack.imgur.com/vOILh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vOILh.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/bOXXd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bOXXd.png"" alt=""enter image description here""></a></p>

<p>As you can see in the third image, alterrow still contains zero columns, not picking up columns from the source file. Can anyone tell me why this is happening?</p>

<p><a href=""https://i.stack.imgur.com/yKHEg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yKHEg.png"" alt=""enter image description here""></a></p>
","<azure><azure-blob-storage><azure-data-factory>","2020-05-27 22:23:18","3350","0","1","62055489","<p>As Mark Kromer refers,you can delete your AlterRow1 and add a new AlterRow.If it doesn't work,try doing this in a different browser or clear your browser cache.It looks the same as <a href=""https://stackoverflow.com/questions/61876786/mapping-columns-from-json-in-an-azure-sql-data-flow-task"">this question</a>.</p>
"
"62053174","Loop through a list and get each value in the list","<p>I have a folder in ADLS that has few files. For the purpose of understanding, I will keep it simple. I have the following three files. When I loop through this folder, I want to get the ""file name"" and ""source"" as separate parameters so that I can pass it subsequent activities/pipelines.</p>

<blockquote>
  <p>employee_crm.txt</p>
  
  <p>contractor_ps.txt</p>
  
  <p>manager_director_sap.txt</p>
</blockquote>

<p>I want to put this in an array so that it can be passed accordingly to the subsequent activities.</p>

<p>(employee, contractor, manager_director)</p>

<p>(crm, ps, sap)</p>

<p>I want to pass two parameters to my subsequent activity (may be a stored procedure) as usp_foo (employee, crm) and it will execute the process based on the parameters. Similary, usp_foo (contractor, ps) and usp_foo (manager_director, sap). </p>

<p>How do I get the child items as two separate parameters so that it can be passed to SP?</p>
","<azure><azure-data-factory>","2020-05-27 21:28:20","59","0","1","62067229","<p>To rephrase the question, you would like to 1) get a list of blob names and 2) parse those names into 2 variables. This pattern occurs frequently, so the following steps will guide you through how to accomplish these tasks.</p>

<ol>
<li>Define an ADLS DataSet that specifies the folder. You do not need a schema, and you can optionally parameterize the FileSystem and Directory names:
<a href=""https://i.stack.imgur.com/p9jh0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p9jh0.png"" alt=""enter image description here""></a></li>
<li>To get a list of the objects within, use the GetMetadata activity. Expand the ""Field list"" section and select ""Child Items"" in the drop down:
<a href=""https://i.stack.imgur.com/QoV3E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QoV3E.png"" alt=""enter image description here""></a></li>
<li>Add a Filter activity to make sure you are only dealing with .txt files. Note it targets the ""childItems"" property:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/UXfji.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UXfji.png"" alt=""enter image description here""></a></p>

<p>You may obviously alter these expressions to meet the specific needs of your project.</p>

<ol start=""4"">
<li>Use ForEach activity to loop through each element in the Filter sequentially:
<a href=""https://i.stack.imgur.com/luXZQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/luXZQ.png"" alt=""enter image description here""></a>

<ol start=""5"">
<li>Inside the ForEach, add activities to parse the filename. To access the fileName, use ""item().name"":
<a href=""https://i.stack.imgur.com/D6b50.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D6b50.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/4a5V3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4a5V3.png"" alt=""enter image description here""></a></li>
</ol></li>
</ol>

<p>In my example, I am storing these values as pipeline variables, which are global [hence the need to perform this operation sequentially]. Storing them in an Array for further use gets complicated and tricky in a hurry because of the limited Array and Object support in the Pipeline Expression Language. The inability to have nested foreach activities may also be a factor. </p>

<p>To overcome these, at this point I would pass these values to another pipeline directly inside the ForEach loop.
<a href=""https://i.stack.imgur.com/0Zrt5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Zrt5.png"" alt=""enter image description here""></a></p>

<p>This pattern has the added benefit of allowing individual file execution apart from the folder processing.</p>
"
"62053098","How to check current status of a latest Data Factory Pipeline run with Pipeline name?","<p>Is it possible to check the current status of a latest Data Factory Pipeline run with Pipeline name using .NET SDK? This is what I tried now:</p>

<pre><code>             using (var client = new DataFactoryManagementClient(cred) {SubscriptionId = subscriptionId})
            {
                RunQueryFilter pipeline = new RunQueryFilter(""PipelineName"", ""Equals"", new List&lt;string&gt; { ""Pipeline"" });                

                var before = DateTime.UtcNow;
                var after = before.AddHours(-24);
                var param = new RunFilterParameters(after, before, null, new List&lt;RunQueryFilter&gt; { pipeline }, null);
                PipelineRunsQueryResponse pipelineResponse = client.PipelineRuns.QueryByFactory(
                                                                        resourceGroup,
                                                                        dataFactoryName, param
                                                                    );



            }

How do I get latest run id?
</code></pre>
","<azure-data-factory>","2020-05-27 21:22:07","594","0","1","62055242","<p>Figured it out -</p>

<pre><code>using (var client = new DataFactoryManagementClient(cred) {SubscriptionId = subscriptionId})
            {
                RunQueryFilter pipeline = new RunQueryFilter(""PipelineName"", ""Equals"", new List&lt;string&gt; { ""Pipeline"" });                
                var pipelineRuns = new RunQueryOrderBy(""RunEnd"", ""DESC"");
                var before = DateTime.UtcNow;
                var after = before.AddHours(-24);
                var param = new RunFilterParameters(after, before, null, new List&lt;RunQueryFilter&gt; { pipeline }, new List&lt;RunQueryOrderBy&gt; { pipelineRuns });
                PipelineRunsQueryResponse pipelineResponse = client.PipelineRuns.QueryByFactory(
                                                                        resourceGroup,
                                                                        dataFactoryName, param
                                                                    );

                var runId = pipelineResponse?.Value?[0]?.RunId;

            }
</code></pre>
"
"62052791","How to get the name of the file that triggered the Azure Data Factory pipeline?","<p>I am new to ADF. I have a requirement to load the data from 15 CSV files to 15 Azure Sql database tables.
In the pipeline there is a trigger to run the pipeline every time a blob is created.</p>

<p>I would like to make this pipeline dynamic. My CSV file name contains the tablename.
Example, Input_202005 is the csv and the table name is Input. </p>

<p>Similarly, I have 14 other files/tables whose metadata is different.</p>

<p>Because I want to run the pipeline every time there a blob is created, I do not need a metadata and a foreachfile activity. I want the pipelines to run in parallel for each blob. Is there a way to know which blob/file triggered the pipeline and to get the name of the file without using any parameters in the trigger. I do not want to use 15 trigger parameters.</p>

<p>Or is there a better solution for my requirement? Any suggestions are appreciated.</p>
","<azure><azure-data-factory><azure-blob-storage><azure-triggers><azure-blob-trigger>","2020-05-27 21:01:16","4848","2","1","62076711","<p>Add a parameter to your pipeline, say, <code>triggeringFile</code>.</p>

<p>When you create the trigger, a form pops-out on the right side - after submitting the first page, a second page pops-out - this will ask for a value for the pipeline parameter <code>triggeringFile</code>.  In that box, put <code>@trigger().outputs.body.fileName</code></p>

<p>If the format you gave is the standard then your table name is just <code>@{split(pipeline().parameters.triggeringFile,'_')[0]}</code></p>
"
"62038431","Azure Data Factory - Self Hosted IR","<p>We are using Azure Data Factory to copy SQL data from on premise to Azure Data Lake. 
For this we have installed Self hosted IR in one of the VM in On Premise environment.</p>

<p>Question- The On-Prem VM (with self hosted IR) has to be kept running and incurs additional infra cost. Is there a way to use Azure IR instead for using Self Hosted IR to copy data using ADF from on prem SQL to Azure Data Lake?</p>
","<azure><azure-data-factory>","2020-05-27 08:14:19","278","0","2","62042973","<p>I dont think there is a way to do this, unless you want to expose your onprem database to the internet (which is not recommended). If you do that then maybe you are able to connect to it via AzureIR.</p>

<p>Remember that it doesnt need to be a dedicated vm just to host the IR, you can use it to do other tasks while the IR is not being used.</p>

<p>Hope this helped!</p>
"
"62038431","Azure Data Factory - Self Hosted IR","<p>We are using Azure Data Factory to copy SQL data from on premise to Azure Data Lake. 
For this we have installed Self hosted IR in one of the VM in On Premise environment.</p>

<p>Question- The On-Prem VM (with self hosted IR) has to be kept running and incurs additional infra cost. Is there a way to use Azure IR instead for using Self Hosted IR to copy data using ADF from on prem SQL to Azure Data Lake?</p>
","<azure><azure-data-factory>","2020-05-27 08:14:19","278","0","2","68938893","<ul>
<li><p>If the Integration Runtime is running on your on-premise server, it will not incur any cost within Azure.</p>
</li>
<li><p>If your SQL Server is open to internet and will allow the Azure IR to access it (using SQL Auth) then you can definitely use Azure IR for your data copy activity, but that's usually not the case in most enterprises and so that answer is a NO.</p>
</li>
</ul>
"
"62032963","Run & monitor ADF pipeline run using .NET SDK","<p>I created an ADF pipeline using the Azure Data Factory UI that triggers a run once every day. <strong>Is there a way to get latest pipeline run id &amp; monitor that run using .NET SDK?</strong> Please provide a console application that does the same with an existing ADF pipeline run?</p>

<p>This is what I tried where a pipeline is created using .NET SDK and monitor run (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net</a>):</p>

<pre><code>static async Task Main(string[] args)
        {
            // Authenticate and create a data factory management client
             var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
             ClientCredential cc = new ClientCredential(applicationId, authenticationKey);
             AuthenticationResult result = context.AcquireTokenAsync(
    ""https://management.azure.com/"", cc).Result;
             ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);

            using (var client = new DataFactoryManagementClient(cred) { 
                      SubscriptionId = subscriptionId 
              })
            {
                RunQueryFilter filter1 = new RunQueryFilter(""PipelineName"", ""Equals"", new List&lt;string&gt; { ""Pipeline"" });                
                DateTime before = DateTime.UtcNow;
                DateTime after = before.AddHours(-24);
                RunFilterParameters param = new RunFilterParameters(after, before, null, new List&lt;RunQueryFilter&gt; { filter1 }, null);
                PipelineRunsQueryResponse pipelineResponse = client.PipelineRuns.QueryByFactory(
                                                                        resourceGroup,
                                                                        dataFactoryName, param
                                                                    );

            }



            // Monitor the pipeline run
            Console.WriteLine(""Checking pipeline run status..."");
            PipelineRun pipelineRun;
            while (true)
            {
                pipelineRun = client.PipelineRuns.Get(
                   resourceGroup, dataFactoryName, runResponse.RunId);
                Console.WriteLine(""Status: "" + pipelineRun.Status);
                if (pipelineRun.Status == ""InProgress"" || pipelineRun.Status == ""Queued"")
                    System.Threading.Thread.Sleep(15000);
                else
                    break;
            }
}
</code></pre>

<p>but is it possible to monitor the run by getting latest run id from ADF?</p>
","<azure-data-factory>","2020-05-26 23:18:41","853","1","1","62044305","<p>Step1,please see <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.ipipelinerunsoperations.querybyfactorywithhttpmessagesasync?view=azure-dotnet"" rel=""nofollow noreferrer"">IPipelineRunsOperations.QueryByFactoryWithHttpMessagesAsync</a> method.</p>

<p><a href=""https://i.stack.imgur.com/c27II.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c27II.png"" alt=""enter image description here""></a></p>

<p>Step2: Navigate to RunFilterParameters Class, you could find the property named OrderBy.</p>

<p><a href=""https://i.stack.imgur.com/ENDY7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ENDY7.png"" alt=""enter image description here""></a></p>

<p>Step3: Find <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.runqueryorderby.-ctor?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_Models_RunQueryOrderBy__ctor"" rel=""nofollow noreferrer"">RunQueryOrderBy</a> Class and you could see it accepts 2 parameters. Here, you could set them as <code>RunEnd</code> and <code>DESC</code>.</p>

<p><a href=""https://i.stack.imgur.com/LHfAI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LHfAI.png"" alt=""enter image description here""></a></p>

<p>Step4: Just get the first element of the Pipeline Runs List.</p>
"
"62032921","Running data flows in Azure Data Factory 4 times slower than running in Azure SSIS data flow","<p>Here are the details of this performance test (very simple).  I'm trying to understand why running data flows in the cloud native Azure Data Factory environment (Spark) is so much slower than running data flows hosted in Azure SSIS IR.  My results show that running in latest ADFv2 is over 4 times slower than running the exact same data flow in Azure SSIS (even with a warm IR cluster already warmed up from previous run).  I like all the new features of the v2 data flows but it hardly seems worth the performance hit unless I'm completely missing something.  Eventually I'll be adding more complex data flows but wanted to understand base performance behavior.</p>

<p>Source:
1GB CSV stored in blob storage</p>

<p>Destination:
Azure SQL Server Database (one table and truncated before each run)</p>

<p>When using control flow in ADFv2 using a simple CopyActivity (no data flow)</p>

<p><strong>91 seconds</strong></p>

<p>When using native SSIS package with data flow (Azure Feature Pack to pull from same blob storage) running Azure SSIS with 8 cores.</p>

<p><strong>76 seconds</strong></p>

<p>Pure ADF Cloud Pipeline using DataFlow with warm Azure IR (cached from previous run) 8 (+ 8 Driver cores) with default partitioning (Spark)
(includes 96 seconds cluster startup which is another thing I don't understand since the TTL is 30 minutes on the IR and it was just ran 10 minutes prior)</p>

<p><strong>360 seconds</strong> </p>

<p>Pipeline (LandWithCopy)</p>

<p>{
    ""name"": ""LandWithCopy"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""CopyData"",
                ""type"": ""Copy"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""DelimitedTextSource"",
                        ""storeSettings"": {
                            ""type"": ""AzureBlobStorageReadSettings"",
                            ""recursive"": true,
                            ""wildcardFileName"": ""data.csv"",
                            ""enablePartitionDiscovery"": false
                        },
                        ""formatSettings"": {
                            ""type"": ""DelimitedTextReadSettings""
                        }
                    },
                    ""sink"": {
                        ""type"": ""AzureSqlSink"",
                        ""preCopyScript"": ""TRUNCATE TABLE PatientAR"",
                        ""disableMetricsCollection"": false
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""mappings"": [
                            {
                                ""source"": {
                                    ""name"": ""RecordAction"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""RecordAction"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""UniqueId"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""UniqueId"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Type"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Type"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""TypeDescription"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""TypeDescription"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""PatientId"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""PatientId"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""PatientVisitId"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""PatientVisitId"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""VisitDateOfService"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""VisitDateOfService"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""VisitDateOfEntry"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""VisitDateOfEntry"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""DoctorId"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""DoctorId"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""DoctorName"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""DoctorName"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""FacilityId"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""FacilityId"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""FacilityName"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""FacilityName"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""CompanyName"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""CompanyName"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""TicketNumber"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""TicketNumber"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""TransactionDateOfEntry"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""TransactionDateOfEntry"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""InternalCode"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""InternalCode"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""ExternalCode"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""ExternalCode"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Description"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Description"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Fee"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Fee"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Units"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Units"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""AREffect"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""AREffect"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Action"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Action"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""InsuranceGroup"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""InsuranceGroup"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Payer"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Payer"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""PayerType"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""PayerType"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""PatBalance"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""PatBalance"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""InsBalance"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""InsBalance"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Charges"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Charges"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Payments"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Payments"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Adjustments"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Adjustments"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""TransferAmount"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""TransferAmount"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""FiledAmount"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""FiledAmount"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""CheckNumber"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""CheckNumber"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""CheckDate"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""CheckDate"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""Created"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Created"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""name"": ""ClientTag"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""ClientTag"",
                                    ""type"": ""String""
                                }
                            }
                        ]
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""PAR_Source_DS"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""PAR_Sink_DS"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ],
        ""annotations"": []
    }
}</p>

<p>Pipeline Data Flow (LandWithFlow)
{
    ""name"": ""WriteData"",
    ""properties"": {
        ""type"": ""MappingDataFlow"",
        ""typeProperties"": {
            ""sources"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""PAR_Source_DS"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""GetData""
                }
            ],
            ""sinks"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""PAR_Sink_DS"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""WriteData""
                }
            ],
            ""transformations"": [],
            ""script"": ""source(output(\n\t\tRecordAction as string,\n\t\tUniqueId as string,\n\t\tType as string,\n\t\tTypeDescription as string,\n\t\tPatientId as string,\n\t\tPatientVisitId as string,\n\t\tVisitDateOfService as string,\n\t\tVisitDateOfEntry as string,\n\t\tDoctorId as string,\n\t\tDoctorName as string,\n\t\tFacilityId as string,\n\t\tFacilityName as string,\n\t\tCompanyName as string,\n\t\tTicketNumber as string,\n\t\tTransactionDateOfEntry as string,\n\t\tInternalCode as string,\n\t\tExternalCode as string,\n\t\tDescription as string,\n\t\tFee as string,\n\t\tUnits as string,\n\t\tAREffect as string,\n\t\tAction as string,\n\t\tInsuranceGroup as string,\n\t\tPayer as string,\n\t\tPayerType as string,\n\t\tPatBalance as string,\n\t\tInsBalance as string,\n\t\tCharges as string,\n\t\tPayments as string,\n\t\tAdjustments as string,\n\t\tTransferAmount as string,\n\t\tFiledAmount as string,\n\t\tCheckNumber as string,\n\t\tCheckDate as string,\n\t\tCreated as string,\n\t\tClientTag as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\twildcardPaths:['data.csv']) ~> GetData\nGetData sink(input(\n\t\tRecordAction as string,\n\t\tUniqueId as string,\n\t\tType as string,\n\t\tTypeDescription as string,\n\t\tPatientId as string,\n\t\tPatientVisitId as string,\n\t\tVisitDateOfService as string,\n\t\tVisitDateOfEntry as string,\n\t\tDoctorId as string,\n\t\tDoctorName as string,\n\t\tFacilityId as string,\n\t\tFacilityName as string,\n\t\tCompanyName as string,\n\t\tTicketNumber as string,\n\t\tTransactionDateOfEntry as string,\n\t\tInternalCode as string,\n\t\tExternalCode as string,\n\t\tDescription as string,\n\t\tFee as string,\n\t\tUnits as string,\n\t\tAREffect as string,\n\t\tAction as string,\n\t\tInsuranceGroup as string,\n\t\tPayer as string,\n\t\tPayerType as string,\n\t\tPatBalance as string,\n\t\tInsBalance as string,\n\t\tCharges as string,\n\t\tPayments as string,\n\t\tAdjustments as string,\n\t\tTransferAmount as string,\n\t\tFiledAmount as string,\n\t\tCheckNumber as string,\n\t\tCheckDate as string,\n\t\tCreated as string,\n\t\tClientTag as string,\n\t\tFileName as string,\n\t\tPractice as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\tpreSQLs:['TRUNCATE TABLE PatientAR'],\n\tmapColumn(\n\t\tRecordAction,\n\t\tUniqueId,\n\t\tType,\n\t\tTypeDescription,\n\t\tPatientId,\n\t\tPatientVisitId,\n\t\tVisitDateOfService,\n\t\tVisitDateOfEntry,\n\t\tDoctorId,\n\t\tDoctorName,\n\t\tFacilityId,\n\t\tFacilityName,\n\t\tCompanyName,\n\t\tTicketNumber,\n\t\tTransactionDateOfEntry,\n\t\tInternalCode,\n\t\tExternalCode,\n\t\tDescription,\n\t\tFee,\n\t\tUnits,\n\t\tAREffect,\n\t\tAction,\n\t\tInsuranceGroup,\n\t\tPayer,\n\t\tPayerType,\n\t\tPatBalance,\n\t\tInsBalance,\n\t\tCharges,\n\t\tPayments,\n\t\tAdjustments,\n\t\tTransferAmount,\n\t\tFiledAmount,\n\t\tCheckNumber,\n\t\tCheckDate,\n\t\tCreated,\n\t\tClientTag\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> WriteData""
        }
    }
}</p>
","<azure-data-factory>","2020-05-26 23:14:53","1128","1","1","62033472","<p>We are having the Same issues. Copy Activity without Data Flow is much faster than Data Flow. Our case is Copy Activity vs Data Flow. Not Sure if I'm doing anything wrong.</p>

<p>Our Scenario is just copy from Source to Destination 13 tables based on where Clause. We now have two copy activity which takes 1.5 minutes. So I was thinking may be create Data Flow and do one Source two Sinks. But it's running like 5 minutes to 8 Minutes depending on Cluster startup time. Hope we get an answer. </p>
"
"62030247","Azure | ADF | How to use a String variable to lookup a Key in an Object type Parameter and retrieve its Value","<p>I am using Azure Data Factory. I'm trying to use a String variable to lookup a Key in a JSON array and retrieve its Value. I can't seem to figure out how to do this in ADF.</p>

<p>Details:</p>

<p>I have defined a Pipeline Parameter named ""obj"", type ""Object"" and content: 
<code>{""values"":{""key1"":""value1"",""key2"":""value2""}}</code></p>

<p><a href=""https://i.stack.imgur.com/aGa5Y.png"" rel=""nofollow noreferrer"">Parameter definition</a></p>

<p>I need to use this pipeline to find a value named ""key1"" and return it as ""value1""; ""key2"" and return it as ""value2""... and so on. I'm planning to use my ""obj"" as a dictionary, to accomplish this.</p>

<p>Technically speaking, If i want to find the value for key2, I can use the code below, and it will be returned ""value2"":</p>

<pre><code>@pipeline().parameters.obj.values.key2
</code></pre>

<p>What i can't figure out is how to do it using a variable (instead of hardcoded ""key2""). </p>

<p>To clear things out: I have a for-loop and, inside it, i have just a copy activity: <a href=""https://i.stack.imgur.com/0oVxy.png"" rel=""nofollow noreferrer"">for-each contents</a>
The purpose of the copy activity is to copy the file named <code>item().name</code>, but save it in ADLS as whatever <code>item().name</code> translates to, according to ""obj""  </p>

<p>This is how the for-loop could be built, using Python: <a href=""https://i.stack.imgur.com/5Bxa5.png"" rel=""nofollow noreferrer"">python-for-loop</a></p>

<p>In ADF, I tried a lot of things (using concat, replace...), but none worked. The simpliest woult be this:</p>

<pre><code>@pipeline().parameters.obj.values.item().name
</code></pre>

<p>but it throws the following error: </p>

<p><code>{""code"":""BadRequest"",""message"":""ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'pipeline().parameters.obj.values.item().name'"",""target"":""pipeline/name_of_the_pipeline/runid/run_id"",""details"":null,""error"":null}</code></p>

<p>So, can you please give any ideas how to define my expression? 
I feel this must be really obvious, but I'm not getting there..... 
Thanks.</p>
","<json><azure><azure-data-factory>","2020-05-26 19:49:08","10503","4","2","62045036","<p>Based on your comments, this is the output of a Filter activity. The Filter activity's output is an object that contains an array named value, so you need to iterate over the ""output.value"":
<a href=""https://i.stack.imgur.com/VLQxB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VLQxB.png"" alt=""enter image description here""></a></p>

<p>Inside the ForEach you reference the name of the item using ""item().name"":
<a href=""https://i.stack.imgur.com/4YvnD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4YvnD.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT BASED ON MORE INFORMATION:</strong></p>

<p>The task is to now take the @item().name value and use it as a dynamic property name against a JSON array. This is a bit of a challenge given the limited nature of the Pipeline Expression Language (PEL). Array elements in PEL can only be referenced by their index value, so to do this kind of complex lookup you will need to loop over the array and do some string parsing. Since you are already inside a FOR loop, and nested FOR loops are not supported, you will need to execute another pipeline to handle this process AND the Copy activity. Warning: this gets ugly, but works.</p>

<p><strong>Child Pipeline</strong> </p>

<p>Define a pipeline with two parameters, one for the values array and one for the item().name:
<a href=""https://i.stack.imgur.com/a0Ntu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a0Ntu.png"" alt=""enter image description here""></a></p>

<p>When you execute the child pipeline, pass <em>@pipeline.parameters.obj.values</em> as ""valuesArray"" and <em>@item().name</em> as ""keyValue"".</p>

<ol>
<li><p>You will need several string parsing operations, so create some string variables in the Pipeline:
<a href=""https://i.stack.imgur.com/lUCNf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lUCNf.png"" alt=""enter image description here""></a></p></li>
<li><p>In the Child Pipeline, add a ForEach activity. Check the Sequential box and set the Items to the valuesArray parameter:
<a href=""https://i.stack.imgur.com/p17Dx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p17Dx.png"" alt=""enter image description here""></a></p></li>
<li><p>Inside the ForEach, start by cleaning up the current item and storing it as a variable to make it a little easier to consume.
<a href=""https://i.stack.imgur.com/C3sSW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C3sSW.png"" alt=""enter image description here""></a></p></li>
<li><p>Parse the object key out of the variable [this is where it starts to get a little ugly]:
<a href=""https://i.stack.imgur.com/ybMjy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ybMjy.png"" alt=""enter image description here""></a></p></li>
<li><p>Add an IF condition to test the value of the current key to the keyValue parameter:
<a href=""https://i.stack.imgur.com/ATbWm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ATbWm.png"" alt=""enter image description here""></a></p></li>
<li><p>Add an activity to the TRUE condition that parses the value into a variable [gets really ugly here]:</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/NJp9l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NJp9l.png"" alt=""enter image description here""></a></p>

<p><strong>Meanwhile, back at the Pipeline</strong></p>

<p>At this point, after the ForEach, you will have a variable (IterationValue) that contains the correct value from your original array:
<a href=""https://i.stack.imgur.com/aWJk1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aWJk1.png"" alt=""enter image description here""></a></p>

<p>Now that you have this value, you can use that variable as a DataSet parameter in the Copy activity.</p>
"
"62030247","Azure | ADF | How to use a String variable to lookup a Key in an Object type Parameter and retrieve its Value","<p>I am using Azure Data Factory. I'm trying to use a String variable to lookup a Key in a JSON array and retrieve its Value. I can't seem to figure out how to do this in ADF.</p>

<p>Details:</p>

<p>I have defined a Pipeline Parameter named ""obj"", type ""Object"" and content: 
<code>{""values"":{""key1"":""value1"",""key2"":""value2""}}</code></p>

<p><a href=""https://i.stack.imgur.com/aGa5Y.png"" rel=""nofollow noreferrer"">Parameter definition</a></p>

<p>I need to use this pipeline to find a value named ""key1"" and return it as ""value1""; ""key2"" and return it as ""value2""... and so on. I'm planning to use my ""obj"" as a dictionary, to accomplish this.</p>

<p>Technically speaking, If i want to find the value for key2, I can use the code below, and it will be returned ""value2"":</p>

<pre><code>@pipeline().parameters.obj.values.key2
</code></pre>

<p>What i can't figure out is how to do it using a variable (instead of hardcoded ""key2""). </p>

<p>To clear things out: I have a for-loop and, inside it, i have just a copy activity: <a href=""https://i.stack.imgur.com/0oVxy.png"" rel=""nofollow noreferrer"">for-each contents</a>
The purpose of the copy activity is to copy the file named <code>item().name</code>, but save it in ADLS as whatever <code>item().name</code> translates to, according to ""obj""  </p>

<p>This is how the for-loop could be built, using Python: <a href=""https://i.stack.imgur.com/5Bxa5.png"" rel=""nofollow noreferrer"">python-for-loop</a></p>

<p>In ADF, I tried a lot of things (using concat, replace...), but none worked. The simpliest woult be this:</p>

<pre><code>@pipeline().parameters.obj.values.item().name
</code></pre>

<p>but it throws the following error: </p>

<p><code>{""code"":""BadRequest"",""message"":""ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'pipeline().parameters.obj.values.item().name'"",""target"":""pipeline/name_of_the_pipeline/runid/run_id"",""details"":null,""error"":null}</code></p>

<p>So, can you please give any ideas how to define my expression? 
I feel this must be really obvious, but I'm not getting there..... 
Thanks.</p>
","<json><azure><azure-data-factory>","2020-05-26 19:49:08","10503","4","2","62109477","<p>Hello fellow Pythonista!</p>

<p>The solution in ADF is actually to reference just as you would in Python by enclosing the 'variable' in square brackets.</p>

<p>I created a pipeline with a parameter obj like yours </p>

<p><a href=""https://i.stack.imgur.com/Uvyqk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uvyqk.png"" alt=""enter image description here""></a></p>

<p>and, as a demo, the pipeline has a single <em>Set Variable</em> activity that got the value for key2 into a variable.</p>

<p><a href=""https://i.stack.imgur.com/GEG3M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GEG3M.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/tc9He.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tc9He.png"" alt=""enter image description here""></a></p>

<p>This is documented but you need X-ray vision to spot it <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#complex-expression-example"" rel=""nofollow noreferrer"">here</a>.</p>
"
"62026335","Understanding ADF Data Sets","<p>I understand that that within Azure Data Factory, a dataset is a view of some source data. Can you create a single dataset from a query joining two source tables? Or do you have to create two datasets, a data flow using a join transformation, to create a third dataset, as the sink of that transformation, that is the desired result? </p>

<p>It appears that source tables are not a thing in ADF, just datasets (which are just tables)? I find this highly confusing and perhaps I do not understand the higher context. </p>
","<azure-data-factory>","2020-05-26 15:58:41","116","0","1","62028499","<p>A DataSet is not a view of the data, it's a pointer to a location, and optionally a description of the schema found at that location. In your case, a pointer to a table, so yes you would need a DataSet for each SOURCE table, with a Schema. You can JOIN them together in a Data Flow, then you would need another DataSet for the SINK. Depending on the Sink type, you may or may not need to create a Schema.</p>

<p>I'm assuming some SQL variant for the tables - if so, and if the two tables are in the same Database, then I would recommend doing the JOIN in the SQL as it will be faster and more efficient. If the Sink is also in the same database, then you'll get the most mileage out of a Stored Procedure rather than a Data Flow.</p>
"
"62020706","ADF Tumpling Window Trigger Udpating the startTime property behaviour","<p>I will get straight to the point - I cannot update the <code>startTime</code> property of an existing ADF Tumpling Window Trigger, I get this error always:</p>

<pre><code>{""code"":""DeploymentFailed"",""message"":""At least one resource deployment operation failed.
 Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage 
details."",""details"":[{""code"":""BadRequest"",""message"":""{\r\n \""error\"": {\r\n \""code\"": 
\""TumblingWindowTriggerStartTimeUpdateNotAllowed\"",\r\n \""message\"": \""Start time cannot be updated for 
Tumbling Window Trigger.null\"",\r\n \""target\"": null,\r\n \""details\"": null\r\n }\r\n}""}]}
</code></pre>

<p>To get around this issue, I have to re-create the Tumpling Window Trigger, point it to the same pipeline and use my new <code>startTime</code> value there.</p>

<p>The above behaviour is quite inconvienent IMHO, is there any workaround for the above error to apply the update automatically? This is extremely important because even in my CI scenario the current behaviour keeps spitting the above error. For CI the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">Microsoft guide</a> does not even mention anything on this error. </p>

<p>Switching to Schedule Trigger is not an option because it doesn't allow having a retry policy.</p>
","<azure><azure-data-factory>","2020-05-26 11:04:32","1594","2","1","65596474","<p>We suffering because of the same issue and it seems the only way to override the tumbling window trigger start time is to remove the trigger, just before your ARM template with ADF deployment. And yes, you have to also stop trigger before removing. We didn't parameterize it yet, having currently different priorities but I'm pretty sure it's possible.</p>
<p>Stop and delete triggers happens in our deployment pipeline and we do something like this:</p>
<pre><code>az extension add --name datafactory

$jsonTriggers = az datafactory trigger list --factory-name &quot;###&quot; --resource-group &quot;###&quot; --query &quot;[].name&quot;
$triggers = $jsonTriggers | ConvertFrom-Json

foreach($trigger in $triggers)
{
    az datafactory trigger stop --factory-name &quot;###&quot; --resource-group &quot;###&quot; --name &quot;$trigger&quot;
    az datafactory trigger delete --factory-name &quot;###&quot; --resource-group &quot;###&quot; --name &quot;$trigger&quot; --yes
}
</code></pre>
"
"62018122","Azure Data Factory how to deploy Alerts & Metrics to other environments with DevOps","<p>We have a Azure datafactory fully integrated with DevOps. Every change I make to the datafactory is deployed to all environments (OTAP), except alerts &amp; metrics. I cannot find anything on how to deploy these to the other environments. Is this possible at all?</p>
","<continuous-integration><devops><azure-data-factory>","2020-05-26 08:50:58","695","3","2","62056364","<blockquote>
  <p>Is this possible at all?'</p>
</blockquote>

<p>Quick answer is NO so far. I contacted Microsoft ADF team and got below response:</p>

<blockquote>
  <p>Azure Data Factory utilizes Azure Resource Manager templates to store
  the configuration of your various ADF entities. Entities on Alerts &amp;
  Metrics does not get exported in the ARM template, so Alerts &amp; Metrics
  won’t be integrated using DevOps.</p>
</blockquote>

<p>I did 2 verifications:</p>

<p>1.Check <a href=""https://learn.microsoft.com/en-us/azure/templates/microsoft.datafactory/2018-06-01/factories"" rel=""nofollow noreferrer"">ARM template supported entities</a> in ADF, <code>Alerts &amp; Metrics</code> doesn't exist. </p>

<p>2.Try to export ARM template in the ADF UI but still no <code>Alerts &amp; Metrics</code></p>

<p><a href=""https://i.stack.imgur.com/MDULh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MDULh.png"" alt=""enter image description here""></a></p>

<p>Really understand you would like to integrate all elements in Data Factory including <code>Alerts &amp; Metrics</code> with DevOps. I suggest you submitting feedback <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">here</a> to push improvements of ADF, any voice is welcome.</p>
"
"62018122","Azure Data Factory how to deploy Alerts & Metrics to other environments with DevOps","<p>We have a Azure datafactory fully integrated with DevOps. Every change I make to the datafactory is deployed to all environments (OTAP), except alerts &amp; metrics. I cannot find anything on how to deploy these to the other environments. Is this possible at all?</p>
","<continuous-integration><devops><azure-data-factory>","2020-05-26 08:50:58","695","3","2","73769352","<p>There is a way to work around this one.
ADF alert is a &quot;Microsoft.Insights/metricalerts&quot; resource that you can deploy using ARM deployment operation from Azure Devops.
You can try to create an alert in ADF and then go to Portal, search for: Monitor &gt; Alert &gt; Alert Rule, and find the Alert you created in ADF. In my case there is an Alert called Test</p>
<p>Here is the ARM template exported from the alert</p>
<pre><code>{
&quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;,
&quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
&quot;parameters&quot;: {
    &quot;metricalerts_Alert_name&quot;: {
        &quot;defaultValue&quot;: &quot;Alert&quot;,
        &quot;type&quot;: &quot;String&quot;
    },
    &quot;factories_test_externalid&quot;: {
        &quot;defaultValue&quot;: &quot;/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/yyyyy/providers/Microsoft.DataFactory/factories/test&quot;,
        &quot;type&quot;: &quot;String&quot;
    },
    &quot;actionGroups_actiongroup1_externalid&quot;: {
        &quot;defaultValue&quot;: &quot;/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/yyyyy/providers/microsoft.insights/actionGroups/actiongroup1&quot;,
        &quot;type&quot;: &quot;String&quot;
    }
},
&quot;variables&quot;: {},
&quot;resources&quot;: [
    {
        &quot;type&quot;: &quot;Microsoft.Insights/metricalerts&quot;,
        &quot;apiVersion&quot;: &quot;2018-03-01&quot;,
        &quot;name&quot;: &quot;[parameters('metricalerts_Alert_name')]&quot;,
        &quot;location&quot;: &quot;global&quot;,
        &quot;tags&quot;: {
            &quot;CreatedTimeUtc&quot;: &quot;2022-09-13T05:28:46.0663823Z&quot;
        },
        &quot;properties&quot;: {
            &quot;severity&quot;: 0,
            &quot;enabled&quot;: true,
            &quot;scopes&quot;: [
                &quot;[parameters('factories_test_externalid')]&quot;
            ],
            &quot;evaluationFrequency&quot;: &quot;PT1M&quot;,
            &quot;windowSize&quot;: &quot;PT15M&quot;,
            &quot;criteria&quot;: {
                &quot;allOf&quot;: [
                    {
                        &quot;threshold&quot;: 1,
                        &quot;name&quot;: &quot;PipelineFailedRuns&quot;,
                        &quot;metricNamespace&quot;: &quot;Microsoft.DataFactory/factories&quot;,
                        &quot;metricName&quot;: &quot;PipelineFailedRuns&quot;,
                        &quot;dimensions&quot;: [
                            {
                                &quot;name&quot;: &quot;Name&quot;,
                                &quot;operator&quot;: &quot;Include&quot;,
                                &quot;values&quot;: [
                                    &quot;pipeline2&quot;
                                ]
                            },
                            {
                                &quot;name&quot;: &quot;FailureType&quot;,
                                &quot;operator&quot;: &quot;Include&quot;,
                                &quot;values&quot;: [
                                    &quot;UserError&quot;,
                                    &quot;SystemError&quot;,
                                    &quot;BadGateway&quot;
                                ]
                            }
                        ],
                        &quot;operator&quot;: &quot;GreaterThanOrEqual&quot;,
                        &quot;timeAggregation&quot;: &quot;Total&quot;,
                        &quot;criterionType&quot;: &quot;StaticThresholdCriterion&quot;
                    }
                ],
                &quot;odata.type&quot;: &quot;Microsoft.Azure.Monitor.SingleResourceMultipleMetricCriteria&quot;
            },
            &quot;actions&quot;: [
                {
                    &quot;actionGroupId&quot;: &quot;[parameters('actionGroups_actiongroup1_externalid')]&quot;,
                    &quot;webHookProperties&quot;: {}
                }
            ]
        }
    }
]
}
</code></pre>
<p>For the actionGroups you can refer to</p>
<pre><code>{
  &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&quot;,
  &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
  &quot;parameters&quot;: {
    &quot;groupName&quot;: {
      &quot;defaultValue&quot;: &quot;actiongroup1&quot;,
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;email_receiver_address&quot;: {
      &quot;defaultValue&quot;: &quot;someEmail@gmail.com&quot;,
      &quot;type&quot;: &quot;string&quot;
    }
  },
  &quot;variables&quot;: {},
  &quot;resources&quot;: [
    {
      &quot;type&quot;: &quot;microsoft.insights/actionGroups&quot;,
      &quot;apiVersion&quot;: &quot;2019-03-01&quot;,
      &quot;name&quot;: &quot;[parameters('groupName')]&quot;,
      &quot;location&quot;: &quot;global&quot;,
      &quot;tags&quot;: {
        &quot;CreatedTimeUtc&quot;: &quot;2020-10-21T07:24:08.2808723Z&quot;,
      },
      &quot;properties&quot;: {
        &quot;groupShortName&quot;: &quot;test&quot;,
        &quot;enabled&quot;: true,
        &quot;emailReceivers&quot;: [
          {
            &quot;name&quot;: &quot;test email received&quot;,
            &quot;emailAddress&quot;: &quot;[parameters('email_receiver_address')]&quot;,
            &quot;useCommonAlertSchema&quot;: false
          }
        ],
        &quot;smsReceivers&quot;: [],
        &quot;webhookReceivers&quot;: [],
        &quot;itsmReceivers&quot;: [],
        &quot;azureAppPushReceivers&quot;: [],
        &quot;automationRunbookReceivers&quot;: [],
        &quot;voiceReceivers&quot;: [],
        &quot;logicAppReceivers&quot;: [],
        &quot;azureFunctionReceivers&quot;: []
      }
    }
  ]
}
</code></pre>
"
"62015027","Difference between DataFlow and Pipelines","<p>I do not understand the difference between dataflow and pipeline in Azure Data Factory.</p>

<p>I have read and see DataFlow can Transform Data without writing any line of code.</p>

<p>But I have made a pipeline and this is exactly the same thing.</p>

<p>Thanks</p>
","<azure-data-factory>","2020-05-26 05:08:10","7998","8","2","62023750","<p>Firstly, dataflow activity need to be executed in the pipeline. So I suspect that you are talking about the copy activity and dataflow activity as both of them are used for transferring data from source to sink.</p>

<blockquote>
  <p>I have read and see DataFlow can Transform Data without writing any
  line of code.</p>
</blockquote>

<p>Your could see the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">overview</a> of Data Flow. Data flow allows data engineers to develop graphical data transformation logic without writing code. All data transfer steps are based on visual interfaces.</p>

<blockquote>
  <p>I have made a pipeline and this is exactly the same thing.</p>
</blockquote>

<p>Copy activity could be used for data transmission. However, it has many <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">limitations</a> with column mapping. So,if you just need simple and pure data transmission, Copy Activity could be used. In order to further meet the personalized needs, you could find many <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-transformation-overview"" rel=""nofollow noreferrer"">built-in features</a> in the Data Flow Activity. For example, Derived column, Aggregate,Sort etc. </p>
"
"62015027","Difference between DataFlow and Pipelines","<p>I do not understand the difference between dataflow and pipeline in Azure Data Factory.</p>

<p>I have read and see DataFlow can Transform Data without writing any line of code.</p>

<p>But I have made a pipeline and this is exactly the same thing.</p>

<p>Thanks</p>
","<azure-data-factory>","2020-05-26 05:08:10","7998","8","2","62023779","<p>A Pipeline is an orchestrator and does not transform data. It manages a series of one or more activities, such as Copy Data or Execute Stored Procedure. Data Flow is one of these activity types and is very different from a Pipeline.</p>

<p>Data Flow performs row and column level transformations, such as parsing values, calculations, adding/renaming/deleting columns, even adding or removing rows. At runtime a Data Flow is executed in a Spark environment, not the Data Factory execution runtime.</p>

<p>A Pipeline can run without a Data Flow, but a Data Flow cannot run without a Pipeline.</p>
"
"62001161","Azure data factory: Handling inner failure in until/for activity","<p>I have an Azure data factory v2 pipeline containing an until activity.</p>

<p>Inside the <em>until</em> is a copy activity - if this fails, the error is logged, exactly as in this post, and I want the loop to continue.</p>

<p><a href=""https://stackoverflow.com/questions/56527167/azure-data-factory-pipeline-on-failure/62000996#62000996"">Azure Data Factory Pipeline &#39;On Failure&#39;</a></p>

<p>Although the inner copy activity’s error is handled, the <em>until</em> activity is deemed to have failed because an inner activity has failed.</p>

<p><a href=""https://i.stack.imgur.com/FphIkm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FphIkm.png"" alt=""ADF Screenshots""></a></p>

<p>Is there any way to configure the until activity to continue when an inner activity fails?</p>
","<azure><error-handling><etl><azure-data-factory><until-loop>","2020-05-25 11:06:54","5715","3","1","62038437","<p>Solution</p>

<p>Put the error-handling steps in their own pipeline and run them from an ExecutePipeline activity.  You'll need to pass-in all the parameters required from the outer pipeline.</p>

<p>You can then use the completion (blue) dependency from the ExecutePipeline (rather than success (green)) so the outer pipeline continues to run despite the inner error.</p>

<p>Note that if you want the outer to know what happened in the inner then there is currently no way to pass data out of the ExecutePipeline to its parent (<a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/38690032-add-ability-to-customize-output-fields-from-execut"" rel=""noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/38690032-add-ability-to-customize-output-fields-from-execut</a>).</p>

<p>To solve this, use an sp activity inside the ExecutePipeline to write data to a SQL table, identified with the pipeline run id.  This can be referenced inside the pipeline with <code>@pipeline().RunId</code>.</p>

<p>Then outside the pipeline you can do a lookup in the SQL table, using the run ID to get the right row.</p>

<p>HEALTH WARNING:  </p>

<p>For some weird reason, the output of ExecutePipeline is returned not as a JSON object but as a string.  So if you try to select a property of output like this <code>@activity('ExecutePipelineActivityName').output.something</code> then you get this error:</p>

<p><em>Property selection is not supported on values of type 'String'</em></p>

<p>So, to get the ExecutePipeine's run ID from outside you need:
<code>@json(activity('ExecutePipelineActivityName').output).pipelineRunId</code></p>

<p>I couldn't find this documented in Microsoft's documentation anywhere, hence posting gory details here.</p>
"
"62000725","Use script task to delete records from table using variables for tablename and date","<p>I have a data flow that reads records from an API.
It then inserts those records in a SQL Azure database in a  table.
The problem is that the source data does not have a primary key. The source data does have identical records that are all valid.  I do not want to insert double records. I do not want to do a full load every time I fill that table.  So, the idea is to get a the last day of data from the source, delete the last day of data in the destination table and then insert the data from the source.
Since the source is quite slow, if I delete the data before starting the dataflow, my clients will be with incomplete data for several minutes up until 20 minutes.
I want  the time between deleting and writing the data to be as short as possible.
That is why I am thinking about using a script task  <strong>in the dataflow</strong>, after reading from the source and before writing in the destination table.</p>

<p>The script task would be deleting records from the destination table based on two variables: 1 for the date to select records to be deleted and one to decide on what schema.table that will be.</p>

<p>This is all controlled by variables.
<code>user::delete_date</code> is a datetime
<code>user::schema_table</code> is the schema_name of the table to delete records from. The table name is always the same but the schema varies.
<code>meta_inserted_date</code> is a field in the destination table of type datetime2</p>

<p>So the instruction I would like the script task to execute is:</p>

<pre><code>DELETE FROM user::schema_table WHERE meta_inserted_date &gt;= user::delete_date 
</code></pre>

<p>Since I want this to be done in the dataflow I cannot use a Execute SQL Task.
The only way I see to execute SQL in the dataflow is to use a Script task.
I can understand scripts, can adapt them but writing is another thing… Please help? </p>
","<ssis><azure-sql-database><azure-data-factory>","2020-05-25 10:40:15","577","0","1","62015068","<p>Thanks for billinkc's comment. I will post it as answer to help others who have the same issue.</p>

<p>Every row that is in your source query is going to fire the same delete statement, which is what you want for the first row but it'll get bothersome for the 70th million row. </p>

<p>Hope it helps you.</p>
"
"61971041","Dynamically pass table name in Lookup and For Each activity in ADF","<p>I need to create a solution using ADF to compare Dev &amp; Prod environments(all tables) on Azure SQL Server (different for Dev &amp; Prod environments) and highlight the table differences.
I am using sys.objects to read the list of all tables in Lookup activity(First lookup activity),then pass each table name to ForEach activity wherein second Lookup transformation would read record counts against that table in Prod &amp; Dev environments and compare the record counts between them, finally performing some activity.
I want to know how can I pass each table name read in First Lookup activity to second lookup activity inside ForEach Loop ?</p>

<p>Eg. Outer Lookup activity reads tables A, B, C, D and pass each table name one at a time inside ForEach wherein another lookup would execute something like below query in Prod and Dev environments :</p>

<pre><code>Select count(*) from A ---in first iteration
then Select count(*) from B ---in second iteration
Select count(*) from C ---in third iteration and so on.
</code></pre>
","<azure><azure-sql-database><azure-data-factory>","2020-05-23 11:03:31","1134","0","1","61980996","<p>As you mentioned, your first layer should be like this:</p>

<p><a href=""https://i.stack.imgur.com/28AG5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/28AG5.png"" alt=""enter image description here""></a></p>

<p>For example, the Look Up Activity preview data as below:</p>

<p><a href=""https://i.stack.imgur.com/2zfZc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2zfZc.png"" alt=""enter image description here""></a></p>

<p>The configuration of ForEach Activity:</p>

<p><a href=""https://i.stack.imgur.com/8870K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8870K.png"" alt=""enter image description here""></a></p>

<p>Then the second Look Up Activity inside ForEach Activity should be configured as below:</p>

<p><a href=""https://i.stack.imgur.com/ZliUB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZliUB.png"" alt=""enter image description here""></a></p>

<p>Query sql:</p>

<pre><code>@concat('select count(*) from ', item())
</code></pre>

<p>If you have further nested requirement, you could do the same thing.</p>
"
"61968221","Is there any salesforce trigger that automatically delete record from Azure SQL when records deleted in salesforce object","<p>I have loaded Salesforce Object data in azure SQL and now I want that if records deleted in Salesforce object then these records should be deleted in Azure SQL also. Is there any way to do this?</p>
","<salesforce><azure-sql-database><azure-data-factory>","2020-05-23 06:16:46","404","0","2","61994957","<p>Based on your description, you want to get real-time synchronization between Salesforce and SQL db.</p>

<p>Please evaluate your real situation to consider below two scenarios:</p>

<p>1.If deletions are very frequently. I suggest you logging a list of delete ids so that you could execute a scheduled job in ADF every day to bulk delete corresponding rows in the sql db. Please backup sql db data every day in case any accident occurs.</p>

<p>2.If it is not frequently.
You could considering using <a href=""https://www.janbasktraining.com/blog/what-is-trigger-in-salesforce/"" rel=""nofollow noreferrer"">Apex Trigger</a> in Salesforce.Once data is deleted in Salesforce, delete corresponding row in sql db based on the <code>Id</code>. Please see below threads:</p>

<p><a href=""https://salesforce.stackexchange.com/questions/268229/apex-trigger-to-callout-rest-api-to-external-system-when-the-status-is-updated"">https://salesforce.stackexchange.com/questions/268229/apex-trigger-to-callout-rest-api-to-external-system-when-the-status-is-updated</a></p>

<p><a href=""https://developer.salesforce.com/forums/?id=906F000000092YrIAI"" rel=""nofollow noreferrer"">https://developer.salesforce.com/forums/?id=906F000000092YrIAI</a></p>
"
"61968221","Is there any salesforce trigger that automatically delete record from Azure SQL when records deleted in salesforce object","<p>I have loaded Salesforce Object data in azure SQL and now I want that if records deleted in Salesforce object then these records should be deleted in Azure SQL also. Is there any way to do this?</p>
","<salesforce><azure-sql-database><azure-data-factory>","2020-05-23 06:16:46","404","0","2","61995828","<p>I would look at Change Data Capture, it offers near realtime synchronization of data which I think fits your use case.  See more details in thedocumentation <a href=""https://developer.salesforce.com/docs/atlas.en-us.change_data_capture.meta/change_data_capture/cdc_intro.htm"" rel=""nofollow noreferrer"">here</a>.  It is events based similar to the streaming api, so your application would have to subscribe to the channel corresponding to the object's change events.</p>
"
"61966410","Stored Procedure as Source in Data Flow","<p>I'm trying to execute a stored procedure which will have rows as output but when I try in the Data Flow Source I'm getting error message </p>

<p><code>DF-SYS-01</code> at Source <code>'source1'</code>: </p>

<p><code>com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'EXEC'.</code></p>

<p>My Source is Query option and I'm trying to execute </p>

<p><code>""EXEC [UVREP].spFeedsProduct 'HH',-2""</code></p>

<p>Can't I use Stored Procedure as Source in Data Flow ? I'm able to do the same in Copy Data Activity it works fine? What I'm doing wrong?</p>
","<azure-data-factory>","2020-05-23 01:42:21","1286","1","2","61978059","<p>ADF Data Flow source can take queries or UDFs, but not sprocs.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#source-transformation"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#source-transformation</a></p>

<p>As Joel mentioned in comments, you can use an ADF Stored Proc activity in the pipeline to execute the sproc before your data flow and store the results in a table or staging file (Parquet/CSV) for the data flow source to read it.</p>
"
"61966410","Stored Procedure as Source in Data Flow","<p>I'm trying to execute a stored procedure which will have rows as output but when I try in the Data Flow Source I'm getting error message </p>

<p><code>DF-SYS-01</code> at Source <code>'source1'</code>: </p>

<p><code>com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'EXEC'.</code></p>

<p>My Source is Query option and I'm trying to execute </p>

<p><code>""EXEC [UVREP].spFeedsProduct 'HH',-2""</code></p>

<p>Can't I use Stored Procedure as Source in Data Flow ? I'm able to do the same in Copy Data Activity it works fine? What I'm doing wrong?</p>
","<azure-data-factory>","2020-05-23 01:42:21","1286","1","2","61980471","<p>Thanks MarkKromer and JoelCochran. </p>

<p>Instead of Stored Procedure now I modified using Views. Using a pipeline with lookup and Data Flow inside the for each loop. I have to copy like 12 tables to three different sinks.</p>

<p>Is there a better way?</p>
"
"61964286","Call Azure Function (Python) from Data Factory: Response Content is not a valid JObject","<p>I'm trying to call a python httptriggered Azure Function from Azure Data Factory. It is the default generated code by Visual Studio code (only added comments). First testing it in Azure Functions in the Azure portal and that works.
<a href=""https://i.stack.imgur.com/b0fjW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b0fjW.png"" alt=""enter image description here""></a></p>

<p>Then in ADF via the Azure Function Activity (<a href=""https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/"" rel=""nofollow noreferrer"">followed instrunctions</a>)
<a href=""https://i.stack.imgur.com/skskI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/skskI.png"" alt=""enter image description here""></a></p>

<p>But it keeps returning errors: 3603 - Response Content is not a valid JObject
<a href=""https://i.stack.imgur.com/hD8XM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hD8XM.png"" alt=""enter image description here""></a></p>
","<python><azure><azure-functions><azure-data-factory>","2020-05-22 21:37:22","361","0","1","61965013","<p>As per <a href=""https://learn.microsoft.com/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/azure/data-factory/control-flow-azure-function-activity</a>, ""The return type of the Azure function has to be a valid JObject. (Keep in mind that JArray is not a JObject.) Any return type other than JObject fails and raises the user error Response Content is not a valid JObject"". In other words, the Azure Function needs to return JSON as its response rather than the plain string that the generated code returns.</p>
"
"61964192","Azure Data Factory V2 Dynamic Content","<p>Long story short, I have a data dump that is too large for an azure function. So we are using Data Factory. 
I have tasked another function to generate an access token for an API and output it as part of a json. I would like to set that token to a variable within the pipeline. So far I have this:
<a href=""https://i.stack.imgur.com/I81PG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I81PG.png"" alt=""enter image description here""></a></p>

<p>I'm attempting to use the Dynamic Content ""language"" to set the variable:</p>

<pre><code>@activity('Get_Token').output
</code></pre>

<p>I'd like something like pythons:</p>

<pre><code>token = data.get('data', {}).get('access_token', '')
</code></pre>

<p>As a secondary question, my next step is to use this token to call an API while iterating over another output, so perhaps this exact step can be added into the ForEach? </p>
","<azure><azure-data-factory>","2020-05-22 21:28:52","1010","0","1","62109825","<p>Looks like the variable should be <code>@activity('Get token').output.data.access_token</code> as others have indicated but, as you've guessed, there's no need to assign a variable if you only need it within the foreach.  You can access any predecessor output from that successor activity.  Here's how to use the token while iterating over another output:</p>

<ol>
<li>Let's say your function also outputs <code>listOfThings</code> as an array
within the <code>data</code> key. Then you can set the foreach activity to
iterate over <code>@activity('Get token').output.data.listOfThings</code>.</li>
<li>Inside the foreach you will have (let's say) a Copy activity with a
REST dataset as the source.  Configure the REST <strong>linked service</strong>
with anonymous auth ... </li>
<li>... then you'll find a field called <em>Additional
Headers</em> in the REST <strong>dataset</strong> where you can create a key <code>Authorization</code>
with value as above, <code>Basic @activity('Get token').output.data.access_token</code></li>
<li>The thing that you said you want to iterate over (in the <code>listOfThings</code> JSON array) can be referenced inside the foreach activity with
<code>@item()</code> (or, if it's a member of an item in the listOfThings
iterable then it would be <code>@item().myMember</code>)</li>
</ol>

<p>To make #4 explicit for anyone else arriving here:</p>

<ul>
<li><p>If listOfThings looks like this, <code>listOfThings: [ ""thing1"", ""thing2"", ...]</code></p></li>
<li><p>for example, <code>filenames: [""file1.txt"", ""file2.txt"", ...]</code></p></li>
<li><p>then <code>@item()</code> becomes <code>file1.txt</code> etc.</p></li>
</ul>

<p><strong>whereas</strong></p>

<ul>
<li><p>If listOfThings looks like this, <code>listOfThings: [ {""key1"":""value1"", ""key2"":""value2"" ... }, {""key1"":""value1"", ""key2"":""value2"" ... }, ...]</code></p></li>
<li><p>for example. <code>filenames: [ {""folder"":""folder1"", ""filename"":""file1.txt""}, {""folder"":""folder2"", ""filename"":""file2.txt""}, ... ]</code></p></li>
<li><p>then <code>@item().filename</code> becomes <code>file1.txt</code> etc.</p></li>
</ul>
"
"61961207","How to Create Timestamp column (Inside Pipeline) in destination SQL database table while migrating data from ADLS to SQL","<p>I am migrating bulk of parquetfiles from <strong>ADLS</strong> to <strong>SQL</strong> database table so inside ForEach i used copy activity and it's copy data successfully for all tables.Now in that every table i have to add add column timestamp so it's gives the <strong>DATE</strong> in that column when that data loaded in that respective table.so what should i do in that pipeline so that in that table one timestamp column gets added and give the <strong>DATE</strong> when data gets loaded.</p>
","<sql-server-2008><azure-sql-database><azure-data-lake><azure-data-factory>","2020-05-22 18:04:05","753","0","1","61963695","<p>Use a data flow with a derived column called ""timestamp"" set to currentTimestamp()</p>
"
"61958238","How I can read and store the replies form Web activity (Post) - Azure datafactory","<p>How I can read and store in database the replies form Web activity (Post)  . I would to check that post was successful </p>

<p>Please see attached the image for more easy detail</p>

<p><a href=""https://i.stack.imgur.com/vY5es.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vY5es.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/UFyg6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UFyg6.png"" alt=""enter image description here""></a>
Thanks so much for your help
Javier</p>
","<azure-data-factory>","2020-05-22 15:17:12","49","0","2","61966017","<p>Two conditions:</p>

<p>1.If you just judging the result of POST request for the next execution of activity, you could use the success dependency to connect the following activities.</p>

<p><a href=""https://i.stack.imgur.com/zE1j1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zE1j1.png"" alt=""enter image description here""></a></p>

<p>2.If you want to persist the result of POST request, even send notification to you, please consider using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> to accept the output of Web Activity[<code>@activity('Web1').output</code>]. Pass the output to Azure Function and implement the requirements with code in the Azure Function inside.</p>
"
"61958238","How I can read and store the replies form Web activity (Post) - Azure datafactory","<p>How I can read and store in database the replies form Web activity (Post)  . I would to check that post was successful </p>

<p>Please see attached the image for more easy detail</p>

<p><a href=""https://i.stack.imgur.com/vY5es.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vY5es.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/UFyg6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UFyg6.png"" alt=""enter image description here""></a>
Thanks so much for your help
Javier</p>
","<azure-data-factory>","2020-05-22 15:17:12","49","0","2","62090754","<p>I think in this case is better to user the Copy Data activity (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a>), there you can define your Source Dataset and your Sink Dataset.</p>

<p>In your case the Source will be the HTTP request and the Sink will be the database, any type of the ones offer by Azure.</p>

<p>Please take into consideration the pagination support from the Copy Data activity.</p>

<p>Take care.</p>
"
"61955923","How to split values in Azure Data Factory: CSV to SQL","<p>Is it possible to split the column values in Azure Data Factory? I am wanting to split a value in a column from a CSV into a SQL table. I am wanting to keep the second value ""Training Programmes Manager"" in the same column deleting the 1st and 3rd and the 4th value ""Education"" moved to an already made column in SQL</p>

<p>Value separated by |</p>

<p>Image of value below:</p>

<p><a href=""https://i.stack.imgur.com/auJ0X.png"" rel=""nofollow noreferrer"">Value in CSV</a></p>

<p>Thanks James</p>
","<sql><azure><csv><azure-data-factory>","2020-05-22 13:13:56","1173","1","1","61958727","<p>Since you need to work with a particular column value, you'll need to use a Data Flow.</p>

<ol>
<li>Source: Create a DataSet for your CSV file.</li>
<li>In the Data Flow, use Derived Column to parse the | delimited column into new columns.</li>
<li>Sink to SQL, referencing the new column names.</li>
</ol>
"
"61954802","Using ADF How to Copy Cosmos DB every record individually as complete JSON into a Single column in Azure SQL Table","<p>Using <strong>ADF</strong> how to <strong>Copy Cosmos DB every record</strong> individually as complete JSON into a Single column nvarchar(MAX) in Azure SQL Table</p>
","<azure-sql-database><azure-data-factory>","2020-05-22 12:13:40","282","0","2","62144153","<p>Others have posted almost the same questions, per my experience, I tested and didn't find any way could load JSON data into one row nvarchar(MAX) in Azure SQL database table.</p>

<p>No document and blog talks about this,  I'm sorry that I can't give more evidences to prove it.</p>

<p>Hope this helps.</p>
"
"61954802","Using ADF How to Copy Cosmos DB every record individually as complete JSON into a Single column in Azure SQL Table","<p>Using <strong>ADF</strong> how to <strong>Copy Cosmos DB every record</strong> individually as complete JSON into a Single column nvarchar(MAX) in Azure SQL Table</p>
","<azure-sql-database><azure-data-factory>","2020-05-22 12:13:40","282","0","2","63057363","<p>Assuming you have in Azure SQL DB table, a column named <code>jsondata</code> of type <code>NVARCHAR(MAX)</code>. You can specify the JSON path to map using the &quot;Advanced editor&quot; in the Mapping tab of the Copy activity. Be sure to check the &quot;Map complex values to string&quot; box. See below screenshot:</p>
<p><a href=""https://i.stack.imgur.com/lg522.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lg522.png"" alt=""Mapping for copy Cosmos DB record JSON to Azure SQL DB table NVARCHAR(MAX) column"" /></a></p>
"
"61948065","Does anyone know how to grant permissions on Azure DevOps to azure data factory?","<p>Does anyone know how to grant permissions on Azure DevOps to an MSP01(secondary) account or by using PAT token?</p>

<p>I need to integrate Azure data factory with GIT repo. But in Azure DevOps we use our company domain account and if I add the MSP01 account directly they are managed as ""external"" accounts and they will have restricted access to the project. </p>

<p>So is there a way to connect to azure DevOps from azure data factory using PAT token??</p>
","<azure><authentication><azure-devops><devops><azure-data-factory>","2020-05-22 04:49:20","445","-1","1","61998726","<blockquote>
  <p>So is there a way to connect to azure DevOps from azure data factory using PAT token?</p>
</blockquote>

<p>I am afraid there is no such a way to connect to azure DevOps from azure data factory using PAT token at this moment.</p>

<p><a href=""https://i.stack.imgur.com/sWW05.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sWW05.png"" alt=""enter image description here""></a></p>

<p>To resolve this issue, you could try to add those ""external"" users as guest in AAD to get access to the Azure devops.</p>

<p>You could check <a href=""https://developercommunity.visualstudio.com/idea/365891/ability-to-link-vsts-to-multiple-azure-active-dire.html"" rel=""nofollow noreferrer"">this similar thread</a> and <a href=""https://medium.com/@cprosenjit/azure-devops-pipeline-setup-for-azure-data-factory-v2-8e957cd5141"" rel=""nofollow noreferrer"">the document</a> for some details.</p>

<p>Hope this helps.</p>
"
"61944734","Integer columns not accepted in Azure Table Storage","<p>I'm using ADF pipeline to copy data from data lake to blob storage and then to table storage. Table storage has a column which includes <strong><em>integer</em></strong> values (column: Age). On trying to query the data in table storage </p>

<pre><code>https://myaccount.table.core.windows.net/Customers()?$filter=Age%20eq%2030 -&gt; (age 30 is considered as integer)
</code></pre>

<p>the output is null. </p>

<p>On updating the query to </p>

<pre><code>https://myaccount.table.core.windows.net/Customers()?$filter=Age%20eq%20'30' -&gt; (age 30 is considered as string)
</code></pre>

<p>it returned right output.</p>

<p>I used the same adf pipeline to copy data from data lake to cosmos db sql api. When I used query </p>

<pre><code>SELECT * FROM c WHERE c.Age = 30
</code></pre>

<p>it returned right output. I tried</p>

<pre><code>SELECT * FROM c WHERE c.Age &gt; 30
</code></pre>

<p>it also returned right output.</p>

<p>So, cosmos db is accepting integer columns whereas table storage is only accepting string values. Why is that? How do I resolve this issue in table storage so that it will accept integer columns? Please let me know. Thank you!</p>
","<azure-table-storage><azure-data-factory>","2020-05-21 22:20:12","282","0","1","61999411","<p>Not very sure about the cause, the below is my steps to copy a son data from data lake to azure table.</p>

<p>Firstly this is my test JSON:</p>

<pre><code>{
    ""callingimsi"": ""466920403025604"",
    ""switch1"": ""China"",
    ""switch2"": ""Germany"",
    ""testvalue"":12
}
</code></pre>

<p>Create the JSON Dataset, then below is the default Schema, the testvalue is integer type.</p>

<p><a href=""https://i.stack.imgur.com/S2FAX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2FAX.png"" alt=""enter image description here""></a></p>

<p>Then I just create a table dataset and create the simple pipeline, set the source and sink then debug. In the table the testvalue is Int64 type it supports the query.</p>

<p><a href=""https://i.stack.imgur.com/AlZ5q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AlZ5q.png"" alt=""enter image description here""></a></p>
"
"61939838","filtering data and load it to respective tables using Azure Data factory","<p>I am fetching data from a store procedure that has a column transaction type and it has some values like (Direct, IMPS, net banking) now I wanted to filter the data according to these values and load it into different tables. anyone have any idea how we can achieve this </p>
","<azure><azure-data-factory>","2020-05-21 17:03:25","29","0","1","61942876","<p>Data Flow permits you to Branch and Filter your Source and write it to multiple Sink locations. Here is an image of a real world Data Flow that does exactly this:
<a href=""https://i.stack.imgur.com/8ZEDz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8ZEDz.png"" alt=""enter image description here""></a></p>

<p>Here are the steps:</p>

<ol>
<li>Create the Source using your Stored Procedure</li>
<li>Branch the Source, once for each transaction type (this image represents 3 branches/types)</li>
<li>Apply a Filter to each Branch based on the transaction type criteria</li>
<li>Apply transforms based on the specific transaction types (optional)</li>
<li>Send the Transaction Type data to the appropriate Sink table(s).</li>
</ol>
"
"61939204","Azure Data Factory webhook execution times out instead of relaying errors","<p>I've attempted to set up a simple webhook execution in Azure Data Factory (v2), calling a simple (parameter-less) webhook for an Azure Automation Runbook I set up.</p>

<p>From the Azure Portal, I can see that the webhook is being executed and my runbook is being run, so far so good. The runbook is (currently) returning an error within 1 minute of execution - but that's fine, I also want to test failure scenarios.</p>

<p><strong>Problem</strong>:
Data Factory doesn't seem to be 'seeing' the error result and spins until the timeout (10 minutes) elapses. When I kick off a debug run of the pipeline, I get the same - a timeout and no error result.</p>

<p><em>Update</em>: I've fixed the runbook and it's now completing successfully, but Data Factory is still timing out and is not seeing the success response either. </p>

<p>Here is a screenshot of the setup:</p>

<p><a href=""https://i.stack.imgur.com/Sy6Yd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sy6Yd.png"" alt=""enter image description here""></a></p>

<p>And here is the portal confirming that the webhook is being run by azure data factory, and is completing in under a minute:</p>

<p><a href=""https://i.stack.imgur.com/GYvTz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GYvTz.png"" alt=""enter image description here""></a></p>

<p>WEBHOOKDATA JSON is:</p>

<pre><code>{""WebhookName"":""Start CAMS VM"",""RequestBody"":""{\r\n \""callBackUri\"": \""https://dpeastus.svc.datafactory.azure.com/dataplane/workflow/callback/f7c...df2?callbackUrl=AAEAFF...0927&amp;shouldReportToMonitoring=True&amp;activityType=WebHook\""\r\n}"",""RequestHeader"":{""Connection"":""Keep-Alive"",""Expect"":""100-continue"",""Host"":""eab...ddc.webhook.eus2.azure-automation.net"",""x-ms-request-id"":""7b4...2eb""}}
</code></pre>

<p>So as far as I can tell, things should be in place to pick up on the result (success of failure). Hopefully someone who's done this before knows what I'm missing.</p>

<p>Thanks!</p>
","<azure><webhooks><azure-data-factory><azure-automation><azure-runbook>","2020-05-21 16:28:01","2589","0","2","62029907","<p>I had assumed that Azure would automatically notify the ADF ""callBackUri"" with the result once the runbook completed or errored out (since they take care of 99% of the scaffolding without requiring a line of code).</p>

<p>It turns out that is not the case, and anyone wishing to execute a runbook from ADF will have to manually extract the callBackUri from the Webhookdata input parameter, and POST the result to it when done.</p>

<p>I haven't nailed this down yet, since the <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-webhooks"" rel=""nofollow noreferrer"">Microsoft tutorial sites</a> I've found have a bad habit of taking screenshots of the code that does this rather than providing the code itself:</p>

<p><a href=""https://i.stack.imgur.com/obhka.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/obhka.png"" alt=""enter image description here""></a></p>

<p>I guess I'll come back and edit this once I have it figured out.</p>

<hr>

<p><strong>EDIT</strong> I ended up implementing this by leaving my original Webhook untouched, and creating a ""wrapper""/helper/utility Runbook that will execute an arbitrary webhook, and relay its status to ADF once it's complete.</p>

<p>Here's the full code I ended up with, in case it helps someone else. It's meant to be generic:</p>

<p><strong>Setup / Helper Functions</strong></p>

<pre class=""lang-bsh prettyprint-override""><code>param
(
    [Parameter (Mandatory = $false)]
    [object] $WebhookData
)

Import-Module -Name AzureRM.resources
Import-Module -Name AzureRM.automation

# Helper function for getting the current running Automation Account Job
# Inspired heavily by: https://github.com/azureautomation/runbooks/blob/master/Utility/ARM/Find-WhoAmI
&lt;#
    Queries the automation accounts in the subscription to find the automation account, runbook and resource group that the job is running in.
    AUTHOR: Azure/OMS Automation Team
#&gt;
Function Find-WhoAmI {
    [CmdletBinding()]
    Param()
    Begin { Write-Verbose (""Entering {0}."" -f $MyInvocation.MyCommand) }
    Process {
        # Authenticate
        $ServicePrincipalConnection = Get-AutomationConnection -Name ""AzureRunAsConnection""
        Add-AzureRmAccount `
            -ServicePrincipal `
            -TenantId $ServicePrincipalConnection.TenantId `
            -ApplicationId $ServicePrincipalConnection.ApplicationId `
            -CertificateThumbprint $ServicePrincipalConnection.CertificateThumbprint | Write-Verbose
        Select-AzureRmSubscription -SubscriptionId $ServicePrincipalConnection.SubscriptionID | Write-Verbose 
        # Search all accessible automation accounts for the current job
        $AutomationResource = Get-AzureRmResource -ResourceType Microsoft.Automation/AutomationAccounts
        $SelfId = $PSPrivateMetadata.JobId.Guid
        foreach ($Automation in $AutomationResource) {
            $Job = Get-AzureRmAutomationJob -ResourceGroupName $Automation.ResourceGroupName -AutomationAccountName $Automation.Name -Id $SelfId -ErrorAction SilentlyContinue
            if (!([string]::IsNullOrEmpty($Job))) {
                return $Job
            }
            Write-Error ""Could not find the current running job with id $SelfId""
        }
    }
    End { Write-Verbose (""Exiting {0}."" -f $MyInvocation.MyCommand) }
}

Function Get-TimeStamp {    
    return ""[{0:yyyy-MM-dd} {0:HH:mm:ss}]"" -f (Get-Date)    
}
</code></pre>

<p><strong>My Code</strong></p>

<pre class=""lang-bsh prettyprint-override""><code>
### EXPECTED USAGE ###
# 1. Set up a webhook invocation in Azure data factory with a link to this Runbook's webhook
# 2. In ADF - ensure the body contains { ""WrappedWebhook"": ""&lt;your url here&gt;"" }
#    This should be the URL for another webhook.
# LIMITATIONS:
# - Currently, relaying parameters and authentication credentials is not supported,
#    so the wrapped webhook should require no additional authentication or parameters.
# - Currently, the callback to Azure data factory does not support authentication,
#    so ensure ADF is configured to require no authentication for its callback URL (the default behaviour)

# If ADF executed this runbook via Webhook, it should have provided a WebhookData with a request body.
if (-Not $WebhookData) {
    Write-Error ""Runbook was not invoked with WebhookData. Args were: $args""
    exit 0
}
if (-Not $WebhookData.RequestBody) {
    Write-Error ""WebhookData did not contain a """"RequestBody"""" property. Data was: $WebhookData""
    exit 0
}
$parameters = (ConvertFrom-Json -InputObject $WebhookData.RequestBody)
# And this data should contain a JSON body containing a 'callBackUri' property.
if (-Not $parameters.callBackUri) {
    Write-Error 'WebhookData was missing the expected ""callBackUri"" property (which Azure Data Factory should provide automatically)'
    exit 0
}
$callbackuri = $parameters.callBackUri

# Check for the ""WRAPPEDWEBHOOK"" parameter (which should be set up by the user in ADF)
$WrappedWebhook = $parameters.WRAPPEDWEBHOOK
if (-Not $WrappedWebhook) {
    $ErrorMessage = 'WebhookData was missing the expected ""WRAPPEDWEBHOOK"" peoperty (which the user should have added to the body via ADF)'
    Write-Error $ErrorMessage
}
else
{
    # Now invoke the actual runbook desired
    Write-Output ""$(Get-TimeStamp) Invoking Webhook Request at: $WrappedWebhook""
    try {    
        $OutputMessage = Invoke-WebRequest -Uri $WrappedWebhook -UseBasicParsing -Method POST
    } catch {
        $ErrorMessage = (""An error occurred while executing the wrapped webhook $WrappedWebhook - "" + $_.Exception.Message)
        Write-Error -Exception $_.Exception
    }
    # Output should be something like: {""JobIds"":[""&lt;JobId&gt;""]}
    Write-Output ""$(Get-TimeStamp) Response: $OutputMessage""    
    $JobList = (ConvertFrom-Json -InputObject $OutputMessage).JobIds
    $JobId = $JobList[0]
    $OutputMessage = ""JobId: $JobId""         

    # Get details about the currently running job, and assume the webhook job is being run in the same resourcegroup/account
    $Self = Find-WhoAmI
    Write-Output ""Current Job '$($Self.JobId)' is running in Group '$($Self.ResourceGroupName)' and Automation Account '$($Self.AutomationAccountName)'""
    Write-Output ""Checking for Job '$($JobId)' in same Group and Automation Account...""

    # Monitor the job status, wait for completion.
    # Check against a list of statuses that likely indicate an in-progress job
    $InProgressStatuses = ('New', 'Queued', 'Activating', 'Starting', 'Running', 'Stopping')
    # (from https://learn.microsoft.com/en-us/powershell/module/az.automation/get-azautomationjob?view=azps-4.1.0&amp;viewFallbackFrom=azps-3.7.0)  
    do {
        # 1 second between polling attempts so we don't get throttled
        Start-Sleep -Seconds 1
        try { 
            $Job = Get-AzureRmAutomationJob -Id $JobId -ResourceGroupName $Self.ResourceGroupName -AutomationAccountName $Self.AutomationAccountName
        } catch {
            $ErrorMessage = (""An error occurred polling the job $JobId for completion - "" + $_.Exception.Message)
            Write-Error -Exception $_.Exception
        }
        Write-Output ""$(Get-TimeStamp) Polled job $JobId - current status: $($Job.Status)""
    } while ($InProgressStatuses.Contains($Job.Status))

    # Get the job outputs to relay to Azure Data Factory
    $Outputs = Get-AzureRmAutomationJobOutput -Id $JobId -Stream ""Any"" -ResourceGroupName $Self.ResourceGroupName -AutomationAccountName $Self.AutomationAccountName
    Write-Output ""$(Get-TimeStamp) Outputs from job: $($Outputs | ConvertTo-Json -Compress)""
    $OutputMessage = $Outputs.Summary
    Write-Output ""Summary ouput message: $($OutputMessage)""
}

# Now for the entire purpose of this runbook - relay the response to the callback uri.
# Prepare the success or error response as per specifications at https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity#additional-notes
if ($ErrorMessage) {
    $OutputJson = @""
{
    ""output"": { ""message"": ""$ErrorMessage"" },
    ""statusCode"": 500,
    ""error"": {
        ""ErrorCode"": ""Error"",
        ""Message"": ""$ErrorMessage""
    }
}
""@
} else {
    $OutputJson = @""
{
    ""output"": { ""message"": ""$OutputMessage"" },
    ""statusCode"": 200
}
""@
}
Write-Output ""Prepared ADF callback body: $OutputJson""
# Post the response to the callback URL provided
$callbackResponse = Invoke-WebRequest -Uri $callbackuri -UseBasicParsing -Method POST -ContentType ""application/json"" -Body $OutputJson

Write-Output ""Response was relayed to $callbackuri""
Write-Output (""ADF replied with the response: "" + ($callbackResponse | ConvertTo-Json -Compress))
</code></pre>

<p>At a high-level, steps I've taken are to:</p>

<ol>
<li>Execute the ""main"" Webhook - get back a ""Job Id""</li>
<li>Get the current running job's ""context"" (resource group and automation account info) so that I can poll the remote job.</li>
<li>Poll the job until it is complete</li>
<li>Put together either a ""success"" or ""error"" response message in the format that Azure Data Factory expects.</li>
<li>Invoke the ADF callback.</li>
</ol>
"
"61939204","Azure Data Factory webhook execution times out instead of relaying errors","<p>I've attempted to set up a simple webhook execution in Azure Data Factory (v2), calling a simple (parameter-less) webhook for an Azure Automation Runbook I set up.</p>

<p>From the Azure Portal, I can see that the webhook is being executed and my runbook is being run, so far so good. The runbook is (currently) returning an error within 1 minute of execution - but that's fine, I also want to test failure scenarios.</p>

<p><strong>Problem</strong>:
Data Factory doesn't seem to be 'seeing' the error result and spins until the timeout (10 minutes) elapses. When I kick off a debug run of the pipeline, I get the same - a timeout and no error result.</p>

<p><em>Update</em>: I've fixed the runbook and it's now completing successfully, but Data Factory is still timing out and is not seeing the success response either. </p>

<p>Here is a screenshot of the setup:</p>

<p><a href=""https://i.stack.imgur.com/Sy6Yd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sy6Yd.png"" alt=""enter image description here""></a></p>

<p>And here is the portal confirming that the webhook is being run by azure data factory, and is completing in under a minute:</p>

<p><a href=""https://i.stack.imgur.com/GYvTz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GYvTz.png"" alt=""enter image description here""></a></p>

<p>WEBHOOKDATA JSON is:</p>

<pre><code>{""WebhookName"":""Start CAMS VM"",""RequestBody"":""{\r\n \""callBackUri\"": \""https://dpeastus.svc.datafactory.azure.com/dataplane/workflow/callback/f7c...df2?callbackUrl=AAEAFF...0927&amp;shouldReportToMonitoring=True&amp;activityType=WebHook\""\r\n}"",""RequestHeader"":{""Connection"":""Keep-Alive"",""Expect"":""100-continue"",""Host"":""eab...ddc.webhook.eus2.azure-automation.net"",""x-ms-request-id"":""7b4...2eb""}}
</code></pre>

<p>So as far as I can tell, things should be in place to pick up on the result (success of failure). Hopefully someone who's done this before knows what I'm missing.</p>

<p>Thanks!</p>
","<azure><webhooks><azure-data-factory><azure-automation><azure-runbook>","2020-05-21 16:28:01","2589","0","2","62143763","<p>For those looking, I created a secondary approach to the above solution - one that executes a Runbook (with parameters) from a Webhook, rather than a invoking a nested Webhook. This has a couple of benefits:</p>

<ul>
<li>Parameters can be passed through to the Runbook (rather than requiring parameters to be baked-into a new Webhook.</li>
<li>A Runbook from another Azure Automation Account / Resource Group can be invoked.</li>
<li>There's no need to poll the status of the job, since the <code>Start-AzureRmAutomationRunbook</code> commandlet has a <code>-Wait</code> parameter.</li>
</ul>

<p>Here's the code:</p>

<pre class=""lang-bsh prettyprint-override""><code>param
(
    # Note: While ""WebhookData"" is the only root-level parameter (set by Azure Data Factory when it invokes this webhook)
    #       The user should ensure they provide (via the ADF request body) these additional properties required to invoke the runbook:
    #       - RunbookName
    #       - ResourceGroupName (TODO: Can fill this in by default if not provided)
    #       - AutomationAccountName (TODO: Can fill this in by default if not provided)
    #       - Parameters (A nested dict containing parameters to forward along)
    [Parameter (Mandatory = $false)]
    [object] $WebhookData
)

Import-Module -Name AzureRM.resources
Import-Module -Name AzureRM.automation

Function Get-TimeStamp {    
    return ""[{0:yyyy-MM-dd} {0:HH:mm:ss}]"" -f (Get-Date)    
}

# If ADF executed this runbook via Webhook, it should have provided a WebhookData with a request body.
if (-Not $WebhookData) {
    Write-Error ""Runbook was not invoked with WebhookData. Args were: $args""
    exit 0
}
if (-Not $WebhookData.RequestBody) {
    Write-Error ""WebhookData did not contain a """"RequestBody"""" property. Data was: $WebhookData""
    exit 0
}
$parameters = (ConvertFrom-Json -InputObject $WebhookData.RequestBody)
# And this data should contain a JSON body containing a 'callBackUri' property.
if (-Not $parameters.callBackUri) {
    Write-Error 'WebhookData was missing the expected ""callBackUri"" property (which Azure Data Factory should provide automatically)'
    exit 0
}
$callbackuri = $parameters.callBackUri

# Check for required parameters, and output any errors.
$ErrorMessage = ''
$RunbookName = $parameters.RunbookName
$ResourceGroupName = $parameters.ResourceGroupName
$AutomationAccountName = $parameters.AutomationAccountName
if (-Not $RunbookName) {
    $ErrorMessage += 'WebhookData was missing the expected ""RunbookName"" property (which the user should have added to the body via ADF)`n'
} if (-Not $ResourceGroupName) {
    $ErrorMessage += 'WebhookData was missing the expected ""ResourceGroupName"" property (which the user should have added to the body via ADF)`n'
} if (-Not $AutomationAccountName) {
    $ErrorMessage += 'WebhookData was missing the expected ""AutomationAccountName"" property (which the user should have added to the body via ADF)`n'
} if ($ErrorMessage) {
    Write-Error $ErrorMessage
} else {
    # Set the current automation connection's authenticated account to use for future Azure Resource Manager cmdlet requests.
    # TODO: Provide the user with a way to override this if the target runbook doesn't support the AzureRunAsConnection
    $ServicePrincipalConnection = Get-AutomationConnection -Name ""AzureRunAsConnection""
    Add-AzureRmAccount -ServicePrincipal `
        -TenantId $ServicePrincipalConnection.TenantId `
        -ApplicationId $ServicePrincipalConnection.ApplicationId `
        -CertificateThumbprint $ServicePrincipalConnection.CertificateThumbprint | Write-Verbose
    Select-AzureRmSubscription -SubscriptionId $ServicePrincipalConnection.SubscriptionID | Write-Verbose 

    # Prepare the properties to pass on to the next runbook - all provided properties exept the ones specific to the ADF passthrough invocation
    $RunbookParams = @{ }
    if($parameters.parameters) {
        $parameters.parameters.PSObject.Properties | Foreach { $RunbookParams[$_.Name] = $_.Value }
        Write-Output ""The following parameters will be forwarded to the runbook: $($RunbookParams | ConvertTo-Json -Compress)""
    }

    # Now invoke the actual runbook desired, and wait for it to complete
    Write-Output ""$(Get-TimeStamp) Invoking Runbook '$($RunbookName)' from Group '$($ResourceGroupName)' and Automation Account '$($AutomationAccountName)'""
    try {    
        # Runbooks have this nice flag that let you wait on their completion (unlike webhook-invoked)
        $Result = Start-AzureRmAutomationRunbook -Wait -Name $RunbookName -AutomationAccountName $AutomationAccountName -ResourceGroupName $ResourceGroupName –Parameters $RunbookParams
    } catch {
        $ErrorMessage = (""An error occurred while invoking Start-AzAutomationRunbook - "" + $_.Exception.Message)
        Write-Error -Exception $_.Exception
    }
    # Digest the result to be relayed to ADF
    if($Result) {
        Write-Output ""$(Get-TimeStamp) Response: $($Result | ConvertTo-Json -Compress)""
        $OutputMessage = $Result.ToString()
    } elseif(-Not $ErrorMessage) {
        $OutputMessage = ""The runbook completed without errors, but the result was null.""
    }
}

# Now for the entire purpose of this runbook - relay the response to the callback uri.
# Prepare the success or error response as per specifications at https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity#additional-notes
if ($ErrorMessage) {
    $OutputJson = @{
        output = @{ message = $ErrorMessage }
        statusCode = 500
        error = @{
            ErrorCode = ""Error""
            Message = $ErrorMessage
        }
    } | ConvertTo-Json -depth 2
} else {
    $OutputJson = @{
        output = @{ message = $OutputMessage }
        statusCode = 200
    } | ConvertTo-Json -depth 2
}
Write-Output ""Prepared ADF callback body: $OutputJson""
# Post the response to the callback URL provided
$callbackResponse = Invoke-WebRequest -Uri $callbackuri -UseBasicParsing -Method POST -ContentType ""application/json"" -Body $OutputJson

Write-Output ""Response was relayed to $callbackuri""
Write-Output (""ADF replied with the response: "" + ($callbackResponse | ConvertTo-Json -Compress))
</code></pre>
"
"61938453","Using Azure Data Factory to read only one file from blob storage and load into a DB","<p>I'd like to read just one file from a blob storage container and load it into a copy operation into a DB, after the arrival of the file has set off a trigger.</p>

<p>Using Microsoft Documentation, the closest I seem to do is read all the file in order of Modified Date.</p>

<p>Would anyone out there know how to read one file after it has arrived in my blob storage?</p>

<p>EDIT:
Just to clarify, I would look to read only the latest file automatically. Without hardcoding the filename.</p>
","<csv><azure-sql-database><azure-pipelines><azure-blob-storage><azure-data-factory>","2020-05-21 15:50:56","1006","1","1","61943005","<p>You can specify a single Blob in the DataSet. This value can be hard coded or variables (using Data Set Parameters):</p>

<p><a href=""https://i.stack.imgur.com/ZmAw2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZmAw2.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/x5JHh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x5JHh.png"" alt=""enter image description here""></a></p>

<p>If you need to run this process whenever a new blob is created/updated, you can use the Event Trigger:</p>

<p><a href=""https://i.stack.imgur.com/vQlTr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vQlTr.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT</strong>:</p>

<p>Based on your addition of ""only the latest"", I don't have a direct solution. Normally, you could use Lookup or GetMetadata activities, but neither they nor the expression language support sorting or ordering. One option would be to use an Azure Function to determine the file to process.</p>

<p>However - if you think about the Event Trigger I mention above, every time it fires the file (blob) is the most recent one in the folder. If you want to coalesce this across a certain period of time, something like this might work:</p>

<ol>
<li>Logic App 1 on event trigger: store the blob name in a log [blob, SQL, whatever works for you]. </li>
<li>Logic App 2  OR ADF pipeline on recurrence trigger: read the log to grab the ""most recent"" blob name. </li>
</ol>
"
"61937105","Data Factory - Retrieve value from field with dash ""-"" from JSON file","<p>In my pipeline I reach through REST API using GET request to a 3rd party database. As an output I receive  a bunch of JSON files. The number of JSON files I have to download (same as number of iterations I will have to use) is in one of the fields in JSON file. The problem is that the field's name is 'page-count' which contains ""-"".</p>

<pre><code>@activity('Lookup1').output.firstRow.meta.page.page-count
</code></pre>

<p>Data Factory considers dash in field's name as a minus sign, so I get an error instead of value from that field.</p>

<pre><code>{""code"":""BadRequest"",""message"":""ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'activity('Lookup1').output.firstRow.meta.page.['page-count']'"",""target"":""pipeline/Product_pull/runid/f615-4aa0-8fcb-5c0a144"",""details"":null,""error"":null}
</code></pre>

<p>This is how the structure of JSON file looks like:</p>

<pre><code> ""firstRow"": {
        ""meta"": {
            ""page"": {
                ""number"": 1,
                ""size"": 1,
                ""page-count"": 7300,
                ""record-count"": 7300
            },
            ""non-compliant-record-count"": 7267
        }
    },
    ""effectiveIntegrationRuntime"": ""intergrationRuntimeTest1"",
    ""billingReference"": {
        ""activityType"": ""PipelineActivity"",
        ""billableDuration"": [
            {
                ""meterType"": ""SelfhostedIR"",
                ""duration"": 0.016666666666666666,
                ""unit"": ""Hours""
            }
        ]
    },
    ""durationInQueue"": {
        ""integrationRuntimeQueue"": 1
    }
}
</code></pre>

<p>How to solve this problem?</p>
","<json><azure-data-factory>","2020-05-21 14:44:31","1281","0","2","62103161","<p>The below syntax works when retrieving the value for a json element with a hyphen.  It is otherwise treated as a minus sign by the parser. It does not seem to be documented by Microsoft however I managed to get this to work through trial and error on a project of mine.</p>
<pre><code>@activity('Lookup1').output.firstRow.meta.page['page-count']
</code></pre>
"
"61937105","Data Factory - Retrieve value from field with dash ""-"" from JSON file","<p>In my pipeline I reach through REST API using GET request to a 3rd party database. As an output I receive  a bunch of JSON files. The number of JSON files I have to download (same as number of iterations I will have to use) is in one of the fields in JSON file. The problem is that the field's name is 'page-count' which contains ""-"".</p>

<pre><code>@activity('Lookup1').output.firstRow.meta.page.page-count
</code></pre>

<p>Data Factory considers dash in field's name as a minus sign, so I get an error instead of value from that field.</p>

<pre><code>{""code"":""BadRequest"",""message"":""ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'activity('Lookup1').output.firstRow.meta.page.['page-count']'"",""target"":""pipeline/Product_pull/runid/f615-4aa0-8fcb-5c0a144"",""details"":null,""error"":null}
</code></pre>

<p>This is how the structure of JSON file looks like:</p>

<pre><code> ""firstRow"": {
        ""meta"": {
            ""page"": {
                ""number"": 1,
                ""size"": 1,
                ""page-count"": 7300,
                ""record-count"": 7300
            },
            ""non-compliant-record-count"": 7267
        }
    },
    ""effectiveIntegrationRuntime"": ""intergrationRuntimeTest1"",
    ""billingReference"": {
        ""activityType"": ""PipelineActivity"",
        ""billableDuration"": [
            {
                ""meterType"": ""SelfhostedIR"",
                ""duration"": 0.016666666666666666,
                ""unit"": ""Hours""
            }
        ]
    },
    ""durationInQueue"": {
        ""integrationRuntimeQueue"": 1
    }
}
</code></pre>

<p>How to solve this problem?</p>
","<json><azure-data-factory>","2020-05-21 14:44:31","1281","0","2","66775879","<p>This worked for us too. We had the same issue where we couldn't reference an output field that contained a dash(<code>-</code>). We referenced this post and used the square brackets and single quote and it worked!</p>
<p>Example below.</p>
<pre><code>@activity('get_token').output.ADFWebActivityResponseHeaders['Set-Cookie']
</code></pre>
"
"61924435","Is it possible to update pipeline parameter value from mapping data flow?","<p><strong>use-case :</strong> </p>

<p>Trying to create a pipeline which receives bulk data from source SQL and to sink to different tables in destination SQL based on one column. The column dedicates the table name in the destination.</p>

<p>I have declared a parameter at data flow which needs to be updated based on the column value and use the same value as dynamic table name in the sink.</p>

<p>any thoughts ?</p>
","<azure-data-factory>","2020-05-20 23:05:56","280","0","1","62148964","<p>As far as I am aware,it is impossible.</p>

<p>According to this <a href=""https://azure.microsoft.com/en-us/updates/azure-data-factory-mapping-data-flows-public-preview-adds-parameter-support/"" rel=""nofollow noreferrer"">document</a>,you can get values and change them from pipeline in data flow,but you can't update them from data flow.</p>

<p>Hope this can help you.</p>
"
"61914947","How to add the diagnostic settings to the azure service using .NET SDK?","<p>How the diagnostic settings can be added and configured to the Azure Data Factory from an asp.net core web app using c#?</p>
","<c#><.net><azure><asp.net-core-webapi><azure-data-factory>","2020-05-20 13:54:57","250","0","1","61928664","<p>You can use azure management sdk.</p>

<p><strong>1.Install the following nuget package:</strong></p>

<p><a href=""https://www.nuget.org/packages/Microsoft.Azure.Management.Fluent/"" rel=""nofollow noreferrer"">Microsoft.Azure.Management.Fluent, version 1.33.0</a>.</p>

<p><a href=""https://www.nuget.org/packages/Microsoft.Azure.Management.Monitor.Fluent/"" rel=""nofollow noreferrer"">Microsoft.Azure.Management.Monitor.Fluent, version 1.33.0</a>.</p>

<p><strong>2.Create credentials:</strong></p>

<p>If you have <a href=""https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"" rel=""nofollow noreferrer"">azure cli</a> installed locally, or you can use <a href=""https://learn.microsoft.com/en-us/azure/cloud-shell/overview"" rel=""nofollow noreferrer"">azure cli from azure portal</a> directly. Then follow <a href=""https://learn.microsoft.com/en-us/dotnet/azure/sdk/authentication#mgmt-auth"" rel=""nofollow noreferrer"">this article</a> to create credentials. In short, type the following azure cli command:</p>

<pre><code>az ad sp create-for-rbac --sdk-auth
</code></pre>

<p>Then you get the <code>clientId</code>, <code>clientSecret</code>, <code>tenantId</code> from the output, please save these values. The output like below:</p>

<p><a href=""https://i.stack.imgur.com/2cGIW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2cGIW.jpg"" alt=""enter image description here""></a></p>

<p><strong>3.Then use the code below:</strong></p>

<pre><code>        string clientId = ""xxx"";
        string clientSecret = ""xxx"";
        string tenantId = ""xxx"";

        var credentials = SdkContext.AzureCredentialsFactory
                    .FromServicePrincipal(clientId,
                                          clientSecret,
                                          tenantId,
                                          AzureEnvironment.AzureGlobalCloud);

        var azure = Azure
            .Configure()
            .WithLogLevel(HttpLoggingDelegatingHandler.Level.Basic)
            .Authenticate(credentials)
            .WithDefaultSubscription();

        azure.DiagnosticSettings
            .Define(""test2"")
            //the resource id of your ADF
            .WithResource(""subscriptions/xxx/resourcegroups/xxx/providers/Microsoft.DataFactory/factories/your_ADF_name"")
            //the resource id of your azure log analytics
            .WithLogAnalytics(""subscriptions/xxx/resourcegroups/xxxx/providers/microsoft.operationalinsights/workspaces/your_azure_log_analytics_name"")
            .WithLog(""ActivityRuns"", 7)
            .WithLog(""PipelineRuns"", 7)
            .WithMetric(""AllMetrics"", TimeSpan.FromMinutes(5), 0)
            .Create();
</code></pre>

<p>Here is a <a href=""https://github.com/Azure-Samples/eventhub-dotnet-manage-event-hub-events/blob/master/Program.cs#L85"" rel=""nofollow noreferrer"">sample code</a> from github.</p>

<p>And you can also you azure monitor rest api <a href=""https://learn.microsoft.com/en-us/rest/api/monitor/diagnosticsettings/createorupdate"" rel=""nofollow noreferrer"">Diagnostic Settings - Create Or Update</a> to setup it.</p>
"
"61912376","Handling multiple destinations in Azure Data Factory - Data flow pipeline","<p>I am having 3 source tables S1, S2 and S3. I am transforming some data and inserting to 3 sink tables D1, D2 and D3. D2 has foreign key relation with D1 and D3 has foreign key relation with D2. I am generating new guid(uuid) using derived column and using those as primary key for 3 tables. While previewing data of derived column, everything looks fine and I am able to see the data correctly. But while debugging, I am getting foreign key relation exception. </p>

<p>I tried disabling constraints and then execution went good. And after I enable it, there was no issues and the relations was proper. So what I doubt is, if the sequence in which the data is inserted is causing the issue. Is their any way by which I can insert D1, followed by D3 and D3 in same data flow pipeline?</p>

<p>See below the screenshot.</p>

<p><a href=""https://i.stack.imgur.com/6MnTT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6MnTT.png"" alt=""Azure Data Factory - Data Flow""></a></p>
","<azure><etl><pipeline><azure-data-factory>","2020-05-20 11:49:44","629","0","1","61925481","<p>In your data flow design UI, click on Settings and set the sink ordering there</p>

<p><a href=""https://i.stack.imgur.com/ptHYU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ptHYU.png"" alt=""enter image description here""></a></p>
"
"61907436","How to push new or updated files from Azure Data lake to File folder using Azure Data Factory","<p>I am able to push data using copy tool from Azure data lake to local file folders using Azure Data Factory.</p>

<p>But I just want to push only updated or new files from Azure data lake to File Folder (I mean only delta pull) .</p>

<p>I tried with parameter option but not able to give parameters. Can anyone of you suggest how to achieve this .</p>
","<azure><file><azure-data-factory><azure-data-lake>","2020-05-20 07:30:22","369","0","1","61907721","<blockquote>
  <p>I just want to push only updated or new files from Azure data lake to
  File Folder (I mean only delta pull) .</p>
</blockquote>

<p>Based on your description, you want to implement incremental load in ADLS source.Actually, this is implemented with LastModifiedDate property of files.</p>

<p><a href=""https://i.stack.imgur.com/NwjNc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NwjNc.png"" alt=""enter image description here""></a></p>

<p>You could execute the pipeline in the schedule so that you could load the new files regularly.More details, please refer to below links:</p>

<p>1.<a href=""https://azure.microsoft.com/en-us/blog/incrementally-copy-new-files-by-lastmodifieddate-with-azure-data-factory/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/incrementally-copy-new-files-by-lastmodifieddate-with-azure-data-factory/</a></p>

<p>2.<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview#loading-new-and-changed-files-only-by-using-lastmodifieddate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview#loading-new-and-changed-files-only-by-using-lastmodifieddate</a></p>
"
"61899883","Concurrent file processing in data flow activity Azure Data Factory","<p>When using control flow, it's possible to use a GetMetadata activity to retrieve a list of files in a blob storage account and then pass that list to a for each activity where the Sequential flag is false to process all files concurrently (in parallel) up to the max batch size according to the activities defined in the for each loop.</p>

<p>However, when reading about data flows in the following article from Microsoft (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern</a>), they indicate the following: </p>

<blockquote>
  <p>A mapping data flow will execute better
  when the Source transformation iterates over multiple files instead of
  looping via the For Each activity. We recommend using wildcards or
  file lists in your source transformation. The Data Flow process will
  execute faster by allowing the looping to occur inside the Spark
  cluster. For more information, see Wildcarding in Source
  Transformation.</p>
  
  <p>For example, if you have a list of data files from July 2019 that you
  wish to process in a folder in Blob Storage, below is a wildcard you
  can use in your Source transformation.</p>
  
  <p>DateFiles/<em>_201907</em>.txt</p>
  
  <p>By using wildcarding, your pipeline will only contain one Data Flow
  activity. This will perform better than a Lookup against the Blob
  Store that then iterates across all matched files using a ForEach with
  an Execute Data Flow activity inside.</p>
</blockquote>

<p>Based on this finding, I have configured a data flow task where the source is a blob directory of files and it processes all files in that directory with no control loops.  I do not, however, see any options to process files concurrently within the data flow.  I do, however, see an Optimize tab where you can set the partitioning option.</p>

<p>Is this option only for processing a single large file into multiple threads or does this control how many files it processes concurrently within the directory where the source is pointing?</p>

<p>Is the documentation assuming the for each control loop is set to ""Sequential"" (I can see why that would be true if it was, but having a hard time believing it if it's running one file at a time in the data flow)?</p>
","<azure-data-factory>","2020-05-19 20:08:38","1780","1","1","61902663","<p>Inside data flow, each source transformation will read all of the files indicated in the folder or wildcard path and store those contents into data frames in memory for processing.</p>

<p>Setting the partitioning manually from the Optimize tab will instruct ADF the partitioning scheme you wish to use inside Spark.</p>

<p>To process each file individually 1x1, use the control flow capabilities in the pipeline.</p>

<p>Iterate over each file you wish to process and send the name of the file into the data flow via iterator parameter inside a For Each setting the execution to Sequential.</p>
"
"61899677","Calculate Hashes in Azure Data Factory","<p>We have a requirement where we want to copy the files and folders from on premise to the Azure Blob Storage. Before copying the files I want to calculate the hashes and put that in a file at the source location.
We want this to be done using Azure Data Factory. I am not finding any option in Azure Data Factory to calculate the hashes for a file system type of objects. I am able to find the hashes for a blob once its landed at destination. 
Can some one guide me how this can be achieved.</p>
","<azure-data-factory>","2020-05-19 19:56:25","5116","0","1","61905459","<p>You need to use data flows in data factory to transform the data.
In a mapping data flow you can just add a column using derived column with an expression using for example the md5() or sha2() function to produce a hash.</p>
"
"61892883","how exactly does self hosted integration runtime authenticate itself to the azure data factory in the backend?","<p>There are multiple articles online, which tell you about the process to setup an self hosted integration runtime, and I understand we need to copy the authentication key provided by ADF and use it when registering the integration runtime on self hosted machine.</p>

<p>But I would like to understand:</p>

<p>(1)  more details on how this handshake actually happens ? Does the auth key have certificate thumbprint ?</p>

<p>(2) does this key gets refreshed to ensure better security ?</p>
","<azure-data-factory><onpremises-gateway>","2020-05-19 13:57:46","1018","0","1","61903956","<blockquote>
  <p>(1) more details on how this handshake actually happens ? Does the
  auth key have certificate thumbprint ?</p>
</blockquote>

<p>Two main statements:</p>

<p>1.Self-hosted integration runtime node encrypts the credentials by using <a href=""https://learn.microsoft.com/en-us/dotnet/standard/security/how-to-use-data-protection"" rel=""nofollow noreferrer"">Windows Data Protection Application Programming Interface</a> (DPAPI) and saves the credentials locally.</p>

<p>2.Azure Data Factory communicates with the self-hosted integration runtime via a control channel that uses a <a href=""https://learn.microsoft.com/en-us/azure/azure-relay/relay-what-is-it#wcf-relay"" rel=""nofollow noreferrer"">shared Azure Service Bus Relay</a> connection.</p>

<p>Actually you could get more details about the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#command-flow-and-data-flow"" rel=""nofollow noreferrer"">Self-Hosted Command flow</a>,especially the No.3 and No.4 points.</p>

<blockquote>
  <p>(2) does this key gets refreshed to ensure better security ?</p>
</blockquote>

<p>Based on my investigations, there is no evidence that the auth key will be refreshed.If you concerns the security anyway, you could refresh it manually by yourself.</p>
"
"61877308","Column names missing in Blob as well as Table while running Azure Data Factory (ADF) Pipeline","<p>I'm using ADF for storing data from data lake to blob in data flow and copy data from blob to table using copy activity. </p>

<p>This is the data structure in container:</p>

<p>_committed_111 (Block blob)
_started_111 (Block blob)
_SUCCESS (Block blob)
part-00000-1623-1-c000.csv (Block blob)
part-00001-1624-1-c000.csv (Block blob)</p>

<p>I was able to successfully copy data to table storage however column names are missing in both blob as well as table. How do I add it as a part ADF data flow/pipeline?</p>
","<azure-table-storage><azure-data-factory><azure-blob-storage>","2020-05-18 19:14:11","778","1","1","61877489","<p>When headers are missing the problem most of the time is that the dataset you are using to store data un the lake doesnt have the First row as header option checked.</p>

<p><a href=""https://i.stack.imgur.com/DSfh7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DSfh7.png"" alt=""First row as header checked""></a></p>

<p>If this is already checked, make sure the source is giving the headers properly.</p>

<p>Hope this helped!</p>
"
"61876786","Mapping columns from JSON in an Azure SQL Data Flow task","<p>I am attempting a simple SELECT action on a Source JSON dataset in an Azure Data Factory data flow, but I am getting an error message that none of the columns from my source are valid. I use the exact configuration as the video, except instead of a CSV file, I use a JSON file. </p>

<p>In the video, at 1:12, you can see that after configuring the source dataset, the source projection shows all of the columns from the source schema.  Below is a screen shot from the tutorial video:
image.png
<a href=""https://i.stack.imgur.com/E1ULZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1ULZ.jpg"" alt=""enter image description here""></a></p>

<p>And below is a screen shot from my attempt:
(I blurred the column names because they match column names from a vendor app)
<a href=""https://i.stack.imgur.com/3SzrQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3SzrQ.jpg"" alt=""enter image description here""></a></p>

<p>Note in my projection, I am unable to modify the data types or the format.  I'm not sure why not, but I don't need to modify either so I moved on.   I did try with a CSV and I was able to modify the data types.  I'm assuming this is a JSON thing, but I'm noting here just in case there is some configuration that I should take a look at.</p>

<p>At 6:48 in the video, you'll see the user add a select task, exactly as I have done.  Below is a screen shot of the select task in the tutorial immediately following adding the task:
<a href=""https://i.stack.imgur.com/VdLfm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VdLfm.jpg"" alt=""enter image description here""></a></p>

<p>Notice the source columns all appear.  Below is a screen shot of my select task:
<a href=""https://i.stack.imgur.com/GkPXg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GkPXg.png"" alt=""enter image description here""></a></p>

<p>I'm curious why the column names are missing?  If I type them in manually, I get an error: ""Column not found""</p>

<p>For reference, below are screen shots of my Data Source setup.  I'm using a Data Lake Storage Gen2 Linked Service connected via Managed Identity and the AutoResolvingIntegrationRuntime.
<a href=""https://i.stack.imgur.com/UduXY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UduXY.jpg"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/HL3ZM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HL3ZM.jpg"" alt=""enter image description here""></a>
Note that I tried to do this with a CSV as well.  I was able to edit the datatype and format on a CSV, but I get the same column not found error on the next step.</p>
","<azure><azure-data-factory>","2020-05-18 18:46:30","1099","1","1","61878383","<p>Try doing this in a different browser or clear your browser cache.  It may just be a formatting thing in the auto-generated JSON.  This has happened to me before.</p>
"
"61868206","Removing extra comma from a column while exporting the data into csv file using Azure Data Factory","<p>I am having data in my sql table as shown below screenshot, which is having an extra comma in values column after the values. It is actually list of values which is having more values.
<a href=""https://i.stack.imgur.com/n68li.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n68li.png"" alt=""enter image description here""></a></p>

<p>I have to import this data into a pipe delimited csv file. And it is shown as below screenshot.
<a href=""https://i.stack.imgur.com/C8r7b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C8r7b.png"" alt=""enter image description here""></a></p>

<p>How will I remove the additional comma from that column while importing the data into a csv file with pipe delimiter.</p>

<p>I am performing the import using an azure data factory pipeline. Is there any way to avoid the extra comma from imported file or while importing?</p>

<p>Is there any way to make this changes at the time of importing the file to an ADLS location through ADF? Any changes that has to be make in ADF?</p>
","<azure><csv><export-to-csv><azure-data-factory>","2020-05-18 11:07:49","866","0","1","61874256","<p>As Joel commented, you can just modify your query to do that while extracting. It might look like this:</p>

<pre><code>select ID, timestamp, replace([values], ',', '') as values from [YourTable]
</code></pre>

<p>Hope this helped!</p>
"
"61865302","Is it possible to trigger ADF pipeline from Power App?","<p>I'm wondering if it is possible to trigger an Azure Data Factory pipeline from a Microsoft Power App, and if so, how would one go about configuring this?</p>

<ul>
<li>I was unable to find a PowerApp connector trigger in Azure Data Factory</li>
<li>I was unable to find a PowerApp connector trigger in Azure Logic Apps</li>
<li>I have experience with Azure, but no experience in PowerApps</li>
</ul>

<p>If you have any ideas or information for me that would be great.</p>

<p>Thanks!</p>
","<triggers><azure-data-factory><azure-logic-apps><powerapps>","2020-05-18 08:18:16","3146","1","2","61865360","<p>You can trigger an Azure Data Factory pipeline using the REST API like this:</p>

<blockquote>
  <p>The following sample command shows you how to run your pipeline by using the REST API manually:</p>

<pre><code>POST  
https://management.azure.com/subscriptions/mySubId/resourceGroups/myResourceGroup/providers/Microsoft.DataFactory/factories/myDataFactory/pipelines/copyPipeline/createRun?api-version=2017-03-01-preview
</code></pre>
</blockquote>

<p>More information: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#manual-execution-on-demand"" rel=""nofollow noreferrer"">Pipeline execution and triggers in Azure Data Factory</a> &rarr; <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#manual-execution-on-demand"" rel=""nofollow noreferrer"">Manual execution (on-demand)</a> &rarr; <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#rest-api"" rel=""nofollow noreferrer"">REST API</a></p>
"
"61865302","Is it possible to trigger ADF pipeline from Power App?","<p>I'm wondering if it is possible to trigger an Azure Data Factory pipeline from a Microsoft Power App, and if so, how would one go about configuring this?</p>

<ul>
<li>I was unable to find a PowerApp connector trigger in Azure Data Factory</li>
<li>I was unable to find a PowerApp connector trigger in Azure Logic Apps</li>
<li>I have experience with Azure, but no experience in PowerApps</li>
</ul>

<p>If you have any ideas or information for me that would be great.</p>

<p>Thanks!</p>
","<triggers><azure-data-factory><azure-logic-apps><powerapps>","2020-05-18 08:18:16","3146","1","2","70222248","<p>There is now a PowerApps connecter you can use to create a pipeline run: <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#create-a-pipeline-run"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/connectors/azuredatafactory/#create-a-pipeline-run</a></p>
"
"61863265","Do I need a storage (of some sort) when pulling data in Azure Data factory","<p>*Data newbie here *</p>

<p>Currently, to run analytics report on data pulled from Dynamics 365, I use Power BI.
Issue with this is, Power BI is quite slow processing large data. I carry out a number of transform steps (e.g. Merge, Join, deleting or renaming columns, etc). So, when I try to run a query in Power BI with said steps, it takes a long time to complete.</p>

<p>So, as a solution, I decided to make use of Azure Data Factory(ADF). The plan is to use ADF to pull the data from CRM (i.e. Dynamics 365), perform transformations and publish the data. Then I'll use Power BI for visual analytics.</p>

<p>My question is:
What azure service will I need in addition to Data Factory? Will I need to store the data I pulled from CRM somewhere - like Azure Data Lake or Blob storage? Or can I do the transformation on the fly, right after the data is ingested?</p>

<p>Initially, I thought I could use the 'copy' activity to ingest data from CRM and start playing with the data. But using the copy activity, I needed to provide a sink (destination for the data. Which has to be a storage of some sort).
I also thought, I could make use of the 'lookup' activity. I tried to use it, but getting errors (no exception message is produced).</p>

<p>I have scoured the internet for a similar process (i.e. Dynamics 365 -> Data Factory -> Power BI), but I've not been able to find any.</p>

<p>Most of the processes I've seen however, utilises some sort of data storage right after data ingest. </p>

<p>All response welcome. Even if you believe I am going about this the wrong way.
Thanks.</p>
","<azure-data-factory>","2020-05-18 05:58:06","74","0","1","61874438","<p>Few things here:</p>

<ul>
<li>The copy activity just moves data from a source, to a sink. It doesnt modify it on the fly.</li>
<li>The lookup activity is just to look for some atributes to use later on the same pipeline.</li>
<li>ADF cannot publish a dataset to power bi (although it may be able to push to a streaming dataset).</li>
</ul>

<p>You approach is correct, but you need that last step of transforming the data. You have a lot of options here, but since you are already familiar with Power Bi you can use the Wrangling Dataflows, which allows you to take a file from the datalake, apply some power query and save a new file in the lake. You can also use Mapping Dataflows, databricks, or any other data transformation tool.</p>

<p>Lastly, you can pull files from a data lake with Power Bi to make your report with the data on this new file.</p>

<p>Of course, as always in Azure there are a lot of ways to solve problems or architect services, this is the one I consider simpler for you.</p>

<p>Hope this helped!</p>
"
"61859634","Transform .xlsx in BLOB storage to .csv using pandas without downloading to local machine","<p>I'm dealing with a transformation from .xlsx file to .csv. I tested locally a python script that downloads .xlsx files from a container in blob storage, manipulate data, save results as .csv file (using pandas) and upload it on a new container. Now I should bring the python script to ADF to build a pipeline to automate the task. I'm dealing with two kind of problems:</p>
<ol>
<li>First problem: I can't figure out how to complete the task without downloading the file on my local machine.</li>
</ol>
<p>I found these threads/tutorials but the &quot;azure&quot; v5.0.0 meta-package is deprecated
<a href=""https://stackoverflow.com/questions/61752574/read-excel-files-from-input-blob-storage-container-and-export-to-csv-in-outpu/61845771#61845771;"">read excel files from &quot;input&quot; blob storage container and export to csv in &quot;output&quot; container with python</a></p>
<p><a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/batch/tutorial-run-python-batch-azure-data-factory.md"" rel=""nofollow noreferrer"">Tutorial: Run Python scripts through Azure Data Factory using Azure Batch</a></p>
<p>Sofar my code is:</p>
<pre><code>import os
import sys
import pandas as pd
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, PublicAccess

# Create the BlobServiceClient that is used to call the Blob service for the storage account
conn_str = 'XXXX;EndpointSuffix=core.windows.net'
blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)
container_name = &quot;input&quot;
blob_name = &quot;prova/excel/AAA_prova1.xlsx&quot;

container = ContainerClient.from_connection_string(conn_str=conn_str, container_name=container_name)
downloaded_blob = container.download_blob(blob_name)
df = pd.read_excel(downloaded_blob.content_as_bytes(), skiprows = 4)   
data = df.to_csv (r'C:\mypath/AAA_prova2.csv' ,encoding='utf-8-sig', index=False)
full_path_to_file = r'C:\mypath/AAA_prova2.csv'
local_file_name = 'prova\csv\AAA_prova2.csv'

#upload in blob
blob_client = blob_service_client.get_blob_client(
    container=container_name, blob=local_file_name)
with open(full_path_to_file, &quot;rb&quot;) as data:
    blob_client.upload_blob(data)
</code></pre>
<ol start=""2"">
<li>Second problem: with this method I can deal only with the specific name of the blob, but in the future I'll have to parametrize the script (i.e. select only blob names starting with AAA_). I can't understand if I have to deal with this in the python script or if I can manage to filter the file through ADF (i.e. adding a Filter File task before running the python script). I can't find any tutorial/code snippet and any help or hint or documentation would be very appreciated.</li>
</ol>
<p><strong>EDIT</strong></p>
<p>I modified the code to avoid to download to local machine, now it works (problem #1 solved)</p>
<pre><code>from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from io import BytesIO
import pandas as pd

filename = &quot;excel/prova.xlsx&quot;

container_name=&quot;input&quot;

blob_service_client = BlobServiceClient.from_connection_string(&quot;XXXX==;EndpointSuffix=core.windows.net&quot;)
container_client=blob_service_client.get_container_client(container_name)
blob_client = container_client.get_blob_client(filename)
streamdownloader=blob_client.download_blob()

stream = BytesIO()
streamdownloader.download_to_stream(stream)

df = pd.read_excel(stream, skiprows = 5)

   
local_file_name_out = &quot;csv/prova.csv&quot;
container_name_out = &quot;input&quot;

blob_client = blob_service_client.get_blob_client(
    container=container_name_out, blob=local_file_name_out)
blob_client.upload_blob(df.to_csv(path_or_buf = None , encoding='utf-8-sig', index=False))
</code></pre>
","<python><python-3.x><azure><azure-blob-storage><azure-data-factory>","2020-05-17 22:29:24","2132","2","1","65946741","<p>Azure Functions, Python 3.8 Version of an Azure function. Waits for a blob trigger from Excel. Then does some stuff and used a good chunk of your code for final completion.</p>
<p>Note the split to trim off the .xlsx of the file name.</p>
<p>This is what I ended up with:</p>
<pre><code>source_blob = (f&quot;https://{account_name}.blob.core.windows.net/{uploadedxlsx.name}&quot;)
file_name = uploadedxlsx.name.split(&quot;/&quot;)[2]
container_name = &quot;container&quot;
container_client=blob_service_client.get_container_client(container_name)
blob_client = container_client.get_blob_client(f&quot;Received/{file_name}&quot;)
streamdownloader=blob_client.download_blob()

stream = BytesIO()
streamdownloader.download_to_stream(stream)

df = pd.read_excel(stream)

file_name_t = file_name.split(&quot;.&quot;)[0]

local_file_name_out = f&quot;Converted/{file_name_t}.csv&quot;
container_name_out = &quot;out_container&quot;

blob_client = blob_service_client.get_blob_client(
    container=container_name_out, blob=local_file_name_out)
blob_client.upload_blob(df.to_csv(path_or_buf = None , encoding='utf-8-sig', index=False))
</code></pre>
"
"61859546","Azure Data Factory: complex (JSON) state management","<p>I need to handle some complex state with Azure Data Factory (ADF). AFAIK, the only mechanic that Azure provide you with - is <code>SetVariable</code> activity which accepts only single value. </p>

<p><em>What to do if I need something more complex?</em> A JSON structure for instance</p>

<p><em>How can I perform the following steps:</em></p>

<ol>
<li>Read some state from SQL database</li>
<li>Enrich the SQL-query results with ADF pipeline variables</li>
<li>Pass the enriched result as JSON payload to ADF's <code>Webhook</code> activity</li>
<li>Accept <code>JSON</code> as response to <code>Webhook</code> activity and store it into SQL database</li>
</ol>
","<azure><azure-data-factory>","2020-05-17 22:18:18","393","1","1","62110999","<p>Set variable activity only supports following formats, which is exactly what you mentioned in your question.</p>

<p><a href=""https://i.stack.imgur.com/LB3MH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LB3MH.png"" alt=""enter image description here""></a></p>

<p>I highly recommand you to refer my previous answer:<a href=""https://stackoverflow.com/questions/60933787/azure-data-factory-sql-to-nested-json/60945020#60945020"">Azure Data Factory - SQL to nested JSON</a>. With that solution, you could get json string by executing sql in the copy activity SQL DB source dataset.</p>

<p>Then please pass the json string into your Web Activity as parameter and parse the json string in the Web Activity inside. I think this could solve your problem.</p>
"
"61843367","How do you use conditional Activity in ADF in case of failure?","<p>I have pretty simple requirement but not sure how this can be implemented in ADF. I have a lookup activity which check certain value in SQL dataset. If it fails I want pipeline variable to be set to one value and if it succeeded then to another and continue on with pipeline. I tried to use ""Success/Failure"" output of lookup activity but issue is that I can not continue with pipeline since it waits for both activities to finish. I tried to use ""IF"" activity but I can not figure out how do I detect if previous activity failed to put it as ""IF"" Condition. What are my choices?</p>
","<azure-data-factory>","2020-05-16 20:49:08","720","0","1","62110060","<p>Put your lookup and variable setting on success/fail in their own pipeline.
Then call that pipeline in your main pipeline using ExecutePipeline activity - use the Completed result from ExecutePipeline as the dependency for subsequent tasks so that they run whether the inner failed or not.</p>

<p>See <a href=""https://stackoverflow.com/questions/62001161/azure-data-factory-handling-inner-failure-in-until-for-activity"">this post</a> for more help.</p>
"
"61824910","Cheapest sink in Azure Data Factory Data Flow","<p>I'm using Azure Data Factory for reading data from a Data Lake and storing the filtered data into Cosmos DB (Sql Api) in one container. I'm using Integration Runtime - Memory Optimized, 4 (+4 Driver Cores) in ADF and Autoscale 20,000 RU/s in Cosmos DB. The ADF pipeline takes ~20 min to complete the triggered run. I'm planning to run ADF pipeline once every day. On checking the monthly cost associated with ADF and Cosmos DB, I see that combined cost of ADF and Cosmos DB is ~$150. (ADF cost: ~70$ Cosmos DB: ~$70). Is there an alternate sink that I could use instead of Cosmos DB to reduce the cost. 
Note: I need to use a sink where I can query.</p>

<p>Here is the time spent in sink transformation: <a href=""https://i.stack.imgur.com/0zlKT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0zlKT.png"" alt=""enter image description here""></a> <a href=""https://i.stack.imgur.com/QvAel.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QvAel.png"" alt=""enter image description here""></a></p>
","<azure-data-factory><azure-cosmosdb-sqlapi>","2020-05-15 17:19:07","414","0","1","61825515","<p>Why use autoscale? ADF understands 429's so can apply back pressure and copy the data at a minimum 400 RU/s</p>
"
"61808060","What is the difference between ADF Pipeline and ADF Data flow?","<p>What is the difference between ADF Pipeline and ADF Data flow? Why are sinks/sources supported in Pipeline and Data flow different? Is it possible to create a pipeline that reads data from source, filter, use joins and store data to a sink without a data flow? Please let me know.</p>
","<azure-data-factory>","2020-05-14 21:52:07","14841","13","1","61808682","<p>Pipelines are for process orchestration. Data Flow is for data transformation.</p>

<p>In ADF, Data Flows are built on Spark using data that is in Azure (blob, adls, SQL, synapse, cosmosdb).</p>

<p>Connectors in pipelines are for copying data and job orchestration. There are 90+ connectors available there that stretch across on-prem and other clouds.</p>

<p>We are always incrementally adding more connectors to data flows.</p>
"
"61806406","How to use Wildcard Filenames in Azure Data Factory SFTP?","<p>I am using Data Factory V2 and have a dataset created that is located in a third-party SFTP. The SFTP uses a SSH key and password. I was successful with creating the connection to the SFTP with the key and password. I can now browse the SFTP within Data Factory, see the only folder on the service and see all the TSV files in that folder.</p>

<p>Naturally, Azure Data Factory asked for the location of the file(s) to import. I use the ""Browse"" option to select the folder I need, but not the files. I want to use a wildcard for the files.</p>

<p>When I opt to do a *.tsv option after the folder, I get errors on previewing the data. When I go back and specify the file name, I can preview the data. So, I know Azure can connect, read, and preview the data if I don't use a wildcard.</p>

<p>Looking over the documentation from Azure, I see they recommend not specifying the folder or the wildcard in the dataset properties. I skip over that and move right to a new pipeline. Using Copy, I set the copy activity to use the SFTP dataset, specify the wildcard folder name ""MyFolder*"" and wildcard file name like in the documentation as ""*.tsv"".</p>

<p>I get errors saying I need to specify the folder and wild card in the dataset when I publish. Thus, I go back to the dataset, specify the folder and *.tsv as the wildcard.</p>

<p>In all cases: this is the error I receive when previewing the data in the pipeline or in the dataset.</p>

<blockquote>
  <p>Can't find SFTP path '/MyFolder/*.tsv'. Please check if the path exists. If the path you configured does not start with '/', note it is a relative path under the given user's default folder ''. No such file . </p>
</blockquote>

<p>Why is this that complicated? What am I missing here? The dataset can connect and see individual files as:</p>

<blockquote>
  <p>/MyFolder/MyFile_20200104.tsv</p>
</blockquote>

<p>But fails when you set it up as</p>

<blockquote>
  <p>/MyFolder/*.tsv</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/uipE8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uipE8.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-05-14 20:01:58","20908","3","2","61806944","<p>I use Copy frequently to pull data from SFTP sources. You mentioned in your question that the documentation says to NOT specify the wildcards in the DataSet, but your example does just that. Instead, you should specify them in the Copy Activity Source settings.</p>

<p>In my implementations, the DataSet has no parameters and no values specified in the Directory and File boxes:
<a href=""https://i.stack.imgur.com/NFYxS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NFYxS.png"" alt=""enter image description here""></a></p>

<p>In the Copy activity's Source tab, I specify the wildcard values. Those can be text, parameters, variables, or expressions. I've highlighted the options I use most frequently below.
<a href=""https://i.stack.imgur.com/p0KbN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p0KbN.png"" alt=""enter image description here""></a></p>
"
"61806406","How to use Wildcard Filenames in Azure Data Factory SFTP?","<p>I am using Data Factory V2 and have a dataset created that is located in a third-party SFTP. The SFTP uses a SSH key and password. I was successful with creating the connection to the SFTP with the key and password. I can now browse the SFTP within Data Factory, see the only folder on the service and see all the TSV files in that folder.</p>

<p>Naturally, Azure Data Factory asked for the location of the file(s) to import. I use the ""Browse"" option to select the folder I need, but not the files. I want to use a wildcard for the files.</p>

<p>When I opt to do a *.tsv option after the folder, I get errors on previewing the data. When I go back and specify the file name, I can preview the data. So, I know Azure can connect, read, and preview the data if I don't use a wildcard.</p>

<p>Looking over the documentation from Azure, I see they recommend not specifying the folder or the wildcard in the dataset properties. I skip over that and move right to a new pipeline. Using Copy, I set the copy activity to use the SFTP dataset, specify the wildcard folder name ""MyFolder*"" and wildcard file name like in the documentation as ""*.tsv"".</p>

<p>I get errors saying I need to specify the folder and wild card in the dataset when I publish. Thus, I go back to the dataset, specify the folder and *.tsv as the wildcard.</p>

<p>In all cases: this is the error I receive when previewing the data in the pipeline or in the dataset.</p>

<blockquote>
  <p>Can't find SFTP path '/MyFolder/*.tsv'. Please check if the path exists. If the path you configured does not start with '/', note it is a relative path under the given user's default folder ''. No such file . </p>
</blockquote>

<p>Why is this that complicated? What am I missing here? The dataset can connect and see individual files as:</p>

<blockquote>
  <p>/MyFolder/MyFile_20200104.tsv</p>
</blockquote>

<p>But fails when you set it up as</p>

<blockquote>
  <p>/MyFolder/*.tsv</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/uipE8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uipE8.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-05-14 20:01:58","20908","3","2","72201583","<p>You can specify till the base folder here and then on the Source Tab select Wildcard Path specify the subfolder in first block (if there as in some activity like delete its not present) and *.tsv in the second block.</p>
<p><a href=""https://i.stack.imgur.com/CBKhu.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
"
"61799251","Copy CSV data to Azure SQL database with nullable columns","<p>I have CSV files, where some of the columns are empty. I would like to insert NULL for all nullable SQL table target columns. Where do I do that mapping? The ADF editor does not give me any options that I can see.</p>
","<csv><azure-sql-database><azure-data-factory>","2020-05-14 13:59:19","70","0","1","61810105","<p>You could follow my steps.</p>

<p>This my csv file:</p>

<p><a href=""https://i.stack.imgur.com/IwcL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IwcL3.png"" alt=""enter image description here""></a></p>

<p>This my sink table schema:</p>

<pre><code>create table test3(
    id int,
    name varchar(50),
    age int
)
</code></pre>

<p>Copy active:</p>

<p><strong>Source:</strong></p>

<p><a href=""https://i.stack.imgur.com/E62uf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E62uf.png"" alt=""enter image description here""></a></p>

<p><strong>Sink:</strong></p>

<p>Just choose the destination table in Sink dataset
<a href=""https://i.stack.imgur.com/YptGq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YptGq.png"" alt=""enter image description here""></a></p>

<p><strong>Mapping:</strong></p>

<p>I mapping the column manually:
<a href=""https://i.stack.imgur.com/Cu5nP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cu5nP.png"" alt=""enter image description here""></a></p>

<p>That's all the operations, run the pipeline:
<a href=""https://i.stack.imgur.com/Nk9Jv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nk9Jv.png"" alt=""enter image description here""></a></p>

<p>The empty column in csv will be inserted to table as <code>NULL</code>:</p>

<p><a href=""https://i.stack.imgur.com/Ws5H7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ws5H7.png"" alt=""enter image description here""></a></p>

<p>Hope this help.</p>
"
"61797366","To validate the header name of each field of a csv file in azure blob storage using Azure data factory v2 pipelines","<p>I have a scenario where the user will upload a file with some data and a header in that file. i need to process the file and make sure that the field names in the header are correct and have no whitespaces and no special characters. 
eg. User dropped file in storage account contains the following header
<a href=""https://i.stack.imgur.com/L4xvb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L4xvb.png"" alt=""enter image description here""></a></p>

<p>i need to change it to this</p>

<p><a href=""https://i.stack.imgur.com/3HPCy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3HPCy.png"" alt=""enter image description here""></a></p>

<p>How can i do this ADF v2 ?</p>
","<csv><azure-data-factory>","2020-05-14 12:28:59","899","0","1","61820800","<p>Data Factory won't really do this as is, but if this is part of a larger ETL process, you can rename the columns in a Data Flow using Select.</p>

<p>Source:
<a href=""https://i.stack.imgur.com/zd90H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zd90H.png"" alt=""enter image description here""></a></p>

<p>Add a Select node and go to the ""Select settings"" tab. If you know the schema, you can just fix the columns manually here:
<a href=""https://i.stack.imgur.com/CbUEm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CbUEm.png"" alt=""enter image description here""></a></p>

<p>You can also use a Rule-based mapping to remove spaces from all the column names. To do this, remove all the existing mappings and add the following:
<a href=""https://i.stack.imgur.com/IKWrK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IKWrK.png"" alt=""enter image description here""></a>
""true()"" in this context means apply to all columns, and '$$' refers to the column name. The ""Inspect"" tab will show the updated column names:</p>

<p><a href=""https://i.stack.imgur.com/vHEMX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vHEMX.png"" alt=""enter image description here""></a></p>
"
"61793642","DIU not going beyond 4","<p>For a simple Copy activity that copies from ADLS Gen1 to Gen2, I set DIU to 32 but this doesn't reflect when I run the copy activity. 
If I set DIU to Auto, even then the DIU is always 4 but never more than that.  (ParallelCopies setting do reflect properly)
Region for Gen1, Gen2, Default Azure IR--> East US 2</p>

<p>Attaching screenshots with some details.</p>

<p>Any suggestions please?</p>

<p><a href=""https://i.stack.imgur.com/V42kP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V42kP.jpg"" alt=""Copy - Input and Output results""></a></p>
","<azure-data-factory>","2020-05-14 09:17:01","820","2","2","61836710","<p>The range of DIU setting is <code>2-256</code>.However, specific behaviors of DIU in different copy scenarios are different even though you set the number as you want.</p>

<p>Please see the table <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#data-integration-units"" rel=""nofollow noreferrer"">list</a> here,especially for the below part:</p>

<p><a href=""https://i.stack.imgur.com/fowZh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fowZh.png"" alt=""enter image description here""></a></p>

<p>DIU has some limitations as you seen,so you could choose the optimal setting with your custom scenario.</p>
"
"61793642","DIU not going beyond 4","<p>For a simple Copy activity that copies from ADLS Gen1 to Gen2, I set DIU to 32 but this doesn't reflect when I run the copy activity. 
If I set DIU to Auto, even then the DIU is always 4 but never more than that.  (ParallelCopies setting do reflect properly)
Region for Gen1, Gen2, Default Azure IR--> East US 2</p>

<p>Attaching screenshots with some details.</p>

<p>Any suggestions please?</p>

<p><a href=""https://i.stack.imgur.com/V42kP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V42kP.jpg"" alt=""Copy - Input and Output results""></a></p>
","<azure-data-factory>","2020-05-14 09:17:01","820","2","2","61907937","<p>I was trying to copy 1GB data, thus somehow DIU never crossed 4. </p>

<p>But when I tried to copy 10GB data, I noticed DIU started scaling up beyond 4. </p>
"
"61790833","How to run two activities on different days in same ADF pipeline on different days?","<p>I have an ADF pipline which has to be run daily. Activity 1 and 2  should trigger every day. But there is one condition. On every Sunday activity 3 has to be run. On the other 6 days, activity 4 has to be run.</p>

<p>Is it possible to do it in ADF? 
Or do I have to create two separate pipelines one to run on size days and another one to run on Sundays?</p>

<p>Can somebody help me with this?</p>

<p><a href=""https://i.stack.imgur.com/53ufP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/53ufP.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-05-14 06:36:45","921","0","2","61810381","<p>I would suggest you add a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">If Condition activity</a> before active 3 and 4 :</p>

<p><a href=""https://i.stack.imgur.com/19j8t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/19j8t.png"" alt=""enter image description here""></a></p>

<p>Using bellow expression to filter the Sunday and other day of week in the If Condition expression:</p>

<pre><code>@equals(dayOfWeek(utcnow()),'0')
</code></pre>

<p>Get the day of week from current UTC time, then compare to the return integer(Sunday is 0).</p>

<p>Add the Copy active 3 to Ture active, add Copy active to False active.</p>

<p>All the copy actives will run in the rule:</p>

<ol>
<li>Monday--Saturday: Copy active 1,2,3 will be triggered.</li>
<li>Sunday: Copy active 1,2,4 will be triggered.</li>
</ol>

<p>Hope this helps.</p>
"
"61790833","How to run two activities on different days in same ADF pipeline on different days?","<p>I have an ADF pipline which has to be run daily. Activity 1 and 2  should trigger every day. But there is one condition. On every Sunday activity 3 has to be run. On the other 6 days, activity 4 has to be run.</p>

<p>Is it possible to do it in ADF? 
Or do I have to create two separate pipelines one to run on size days and another one to run on Sundays?</p>

<p>Can somebody help me with this?</p>

<p><a href=""https://i.stack.imgur.com/53ufP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/53ufP.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-05-14 06:36:45","921","0","2","64310811","<p>I try to use <code>@equals(dayOfWeek(utcnow()), '6')</code> to trigger my pipeline on every saturday,but failed.Because <code>dayOfWeek(utcnow())</code> return value is int,I changed it <code>@equals(dayOfWeek(utcnow()), 6)</code> and it work.</p>
<p>List of <code>dayOfWeek()</code> return value</p>
<ul>
<li>Sunday:   0</li>
<li>Monday:   1</li>
<li>Tuesday:  2</li>
<li>Wednesday:    3</li>
<li>Thursday: 4</li>
<li>Friday:   5</li>
<li>Saturday: 6</li>
</ul>
"
"61787760","Azure Table Storage Sink in ADF Data Flow","<p><a href=""https://i.stack.imgur.com/F97d0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F97d0.png"" alt=""enter image description here""></a></p>

<p>Here is how my ADF Pipeline looks like. In Data Flow, I read some data from a source, perform filter &amp; join and store data to a sink. My plan was to use Azure Table Storage as the sink. However, according to <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/34981"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/34981</a>, ADF Data Flow does not support Azure Table Storage as a sink. Is there an alternative to use Azure Table Storage as the sink in Data Flow?</p>
","<azure-table-storage><azure-data-factory>","2020-05-14 01:35:38","1406","1","2","61790896","<p>No, it is impossible. Azure Table Storage can not be the sink of data flow.</p>
<p>Only these six dataset is allowed:</p>
<p><a href=""https://i.stack.imgur.com/LgLNB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LgLNB.png"" alt=""enter image description here"" /></a></p>
<p>Not only these limits. When as the sink of the dataflow, Azure Blob Storage and Azure Data Lake Storage Gen1&amp;Gen2 only support four format: JSON, Avro, Text, Parquet.'</p>
<p>At least for now, your idea is not a viable solution.</p>
<p>For more information, have a look of this offcial doc:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink#supported-sink-connectors-in-mapping-data-flow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink#supported-sink-connectors-in-mapping-data-flow</a></p>
"
"61787760","Azure Table Storage Sink in ADF Data Flow","<p><a href=""https://i.stack.imgur.com/F97d0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F97d0.png"" alt=""enter image description here""></a></p>

<p>Here is how my ADF Pipeline looks like. In Data Flow, I read some data from a source, perform filter &amp; join and store data to a sink. My plan was to use Azure Table Storage as the sink. However, according to <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/34981"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/34981</a>, ADF Data Flow does not support Azure Table Storage as a sink. Is there an alternative to use Azure Table Storage as the sink in Data Flow?</p>
","<azure-table-storage><azure-data-factory>","2020-05-14 01:35:38","1406","1","2","64279793","<p>Even today it isn't possible. One option could be (we are solving a similar case like this currently) to use a Blob Storage as a temporary destination.</p>
<ol>
<li><p>The data flow will store the result in the Blob Storage. The source data is processed by all these different transformations in the data flow and prepared well for table storage, e.g. PartitionKey, RowKey, and all other columns are there.</p>
</li>
<li><p>A subsequent Copy Activity will move the data from Blob Storage into Table Storage easily.</p>
</li>
</ol>
<p>The marked part of the pipeline is doing exactly this:</p>
<ul>
<li><strong>Full Orders</strong> runs the data flow</li>
<li><strong>to Table Storage</strong> copy activity moves data into the Table Storage</li>
</ul>
<p><a href=""https://i.stack.imgur.com/09aFz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/09aFz.png"" alt=""Pipeline"" /></a></p>
"
"61775578","How to pass variable to ADF Execute Pipeline Activity?","<p><strong>Environment</strong>:
I have around 100 pipelines that run on a number of triggers.</p>

<p><strong>Outcome:</strong> I want to create a master pipeline that calls those 100 pipelines. </p>

<p><strong>Currently</strong>, I've created a list of pipeline names and put them to an array. Then I was hoping to use forEach and execute pipeline activities to pass those names.</p>

<p><strong>Issue</strong>, it seems that execute pipeline activity does not take variables or it is not obvious how to do it.</p>

<p>I do not want to create master pipeline manually as it can change often and I hope there must be better way to do it than manually.</p>
","<etl><azure-data-factory>","2020-05-13 13:12:46","2110","0","1","61777591","<p>You are correct that the ""Invoked pipeline"" setting of the Execute Pipeline activity does not support a variable value: the Pipeline name must be known at design time. This makes sense when you consider parameter handling.</p>

<p>One way around this is to create an Azure Function to execute the pipeline. <a href=""https://stackoverflow.com/questions/59085000/method-to-put-alerts-on-long-running-azure-data-factory-pipeline/59290603#59290603"">This answer</a> has the .Net code I leverage in my pipeline management work. It's a  couple years old, so probably needs an update. If you need them to run sequentially, you'll need to build a larger framework to monitor and manage the executions, which is also discussed in that answer. There is a concurrency limit (~40 per pipeline, I believe), so you couldn't run all 100 simultaneously.</p>
"
"61770247","Get all Azure Data Factory pipeline runs using .NET sdk","<p>How can be loaded all pipeline runs from Azure Data Factory using .NET SDK in one HTTP query to the factory? 
I have tried the <code>QueryByFactoryWithHttpMessagesAsync</code> method but it returns only 100 last runs.</p>

<pre><code>var filter = new RunFilterParameters(DateTime.MinValue, DateTime.Now);

return client.PipelineRuns.QueryByFactoryWithHttpMessagesAsync(
                dataFactoryParameters.ResourceGroup,
                dataFactoryParameters.DataFactoryName,
                filter).Result.Body.Value;
</code></pre>
","<azure><asp.net-core><.net-core><azure-data-factory>","2020-05-13 08:54:18","361","0","1","61778450","<p>I don't have a code sample, but the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.pipelinerunsoperationsextensions.querybyfactoryasync?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_PipelineRunsOperationsExtensions_QueryByFactoryAsync_Microsoft_Azure_Management_DataFactory_IPipelineRunsOperations_System_String_System_String_Microsoft_Azure_Management_DataFactory_Models_RunFilterParameters_System_Threading_CancellationToken_"" rel=""nofollow noreferrer"">QueryByFactoryAsync method</a> returns a <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.pipelinerunsqueryresponse?view=azure-dotnet"" rel=""nofollow noreferrer"">PipelineRunsQueryResponse</a> instance, which includes a <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.pipelinerunsqueryresponse.continuationtoken?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_Models_PipelineRunsQueryResponse_ContinuationToken"" rel=""nofollow noreferrer"">ContinuationToken</a> property. You should be able to use that to get the next group of values.</p>
"
"61765127","Azure SSIS Package - Execute SQL Task","<p>I have created a simple SSIS package with ""Execute SQL Task"". When I am trying to execute the package in Azure, it fails with the below error:</p>

<blockquote>
  <p>Error: Failed to acquire connection ""my connection"". Connection may
  not be configured correctly or you may not have the right permissions
  on this connection.</p>
  
  <p>Error: SSIS Error Code DTS_E_OLEDBERROR.  An OLE DB error has
  occurred. Error code: 0x80004005. An OLE DB record is available. 
  Source: ""Microsoft SQL Server Native Client 11.0""  Hresult: 0x80004005
  Description: ""Login timeout expired"". An OLE DB record is available. 
  Source: ""Microsoft SQL Server Native Client 11.0""  Hresult: 0x80004005
  Description: ""A network-related or instance-specific error has
  occurred while establishing a connection to SQL Server. Server is not
  found or not accessible. Check if instance name is correct and if SQL
  Server is configured to allow remote connections. For more information
  see SQL Server Books Online."". An OLE DB record is available.  Source:
  ""Microsoft SQL Server Native Client 11.0""  Hresult: 0x80004005 
  Description: ""Named Pipes Provider: Could not open a connection to SQL
  Server [53]. "".</p>
</blockquote>

<p>However, it works fine when I run it with some other task. Can someone tell me what could be the issue? I am guessing SSIS package execution from Azure does not support ""Execute SQL Task"".</p>

<p>Any suggestions would be greatly appreciated.</p>
","<azure><ssis><azure-data-factory>","2020-05-13 02:03:41","505","1","1","61786834","<p>After going through lot of documentation online, I have found that: SSIS Package execution through an SSIS Integration Runtime using a proxy supports only Data flows.</p>

<p>Refer Limitation section in the below link:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/self-hosted-integration-runtime-proxy-ssis"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/self-hosted-integration-runtime-proxy-ssis</a> </p>

<p>To fix this I have create another SSIS Integration Runtime that joins the same VNet as Azure SQL Server Managed Instance.</p>
"
"61759760","How can I identify issues on an Azure Data Factory pipeline that works in debug mode but not when triggered?","<p>On Azure Data Factory I have a parametrized pipeline that uses the Copy Data activity, with the Source being <strong>OData</strong> and the Sink is an on-prem SQL server. They're being executed with an self-hosted integration runtime.</p>

<p>The pipeline was working successfully until last week or so, being able to dynamically copy data from 32 tables. Now, I'm having issues running the same pipeline for 2 of these tables. They work successfully when executed in debug mode, but not when executed via triggers, even though the parameters are the same. <strong>There are no changes to be published on these pipelines</strong>, I'm aware that the trigger executes the pipeline version published while debug executes it with unsaved changes.</p>

<p><strong>Issue with table 1</strong></p>

<p>The pipeline completely fails, the error is related to the table not being found in the sink. Error message:</p>

<blockquote>
  <p>Operation on target Load Data failed: Failure happened on 'Source'
  side.
  ErrorCode=UserErrorODataRequestNotSucceeded,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Requesting
  response from path [redacted] and query  failed with status code
  NotFound and message <strong>The resource you are looking for has been
  removed, had its name changed, or is temporarily
  unavailable.</strong>.,Source=Microsoft.DataTransfer.Runtime.ODataConnector,'</p>
</blockquote>

<p>Again, when executed via debug, with the same parameters, it works as usual. I've tried removing the parametrization from the copy data activity and creating a separate pipeline for this table, but the result was the same, it's unable to find the table in the sink unless executed in debug.</p>

<p><strong>Issue with table 2</strong></p>

<p>The pipelines successfully executes, however it loads data only for 16 out of 20 columns. All of the column names and types are the same in the source and sink. When executed in debug mode, all 20 columns are populated. The copy data activity does not provide any details about those unpopulated columns, as seen below.</p>

<p><a href=""https://i.stack.imgur.com/3vw31.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3vw31.png"" alt=""Copy activity details""></a></p>

<p>I would expect a warning or something telling me about incompatible columns, but nothing can be found. This specific issue seems to be happening with only 1 table out of the 32 tables being loaded (at least that I'm aware of).</p>

<p>I didn’t have any of these issues until last week. Why does these issues happens only for some tables and also doesn't happen in debug mode? How can I further troubleshoot these problems?</p>
","<azure><azure-data-factory>","2020-05-12 18:53:49","1006","2","1","61869326","<p>I've opened a support ticket with Microsoft and figured out the issue - changes to the master branch weren't being published correctly, they were ""corrupted"". The Data Factory was working with old metadata/code and never updating as it should, hence why it worked in debug mode (current/new metadata) but not with triggers (published metadata/code).</p>

<p>The issue was fixed by recreating the linked services connection with OData and replacing it in the data sets that were using it.</p>
"
"61751018","Enable azure diagonstic setting using ARM template for azure data factory,azure sql","<p>When I am enabling the diagonstic setting fromt the azure portal for ADF &amp; Azuresql, in the ARM template I am not able to find anything in ARM with respect to diagonstic setting.Similar way for keyvault and sql I need the ARM template for enabling the diagonstic setting.</p>

<p>I tried from my side for ADF since I new to ARM template I am not able to find the method for enabling the diagonstic  setting.</p>

<pre><code>{
    ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
    ""contentVersion"": ""1.0.0.0"",
    ""parameters"": {
        ""factoryName"": {
            ""type"": ""string"",
            ""metadata"": {
                ""description"": ""The name of the Data Factory""
            }
        }
    },
    ""resources"": [
        {
            ""type"": ""Microsoft.DataFactory/factories"",
            ""apiVersion"": ""2018-06-01"",
            ""name"": ""[parameters('factoryName')]"",
            ""location"": ""[resourceGroup().location]"",
            ""identity"": {
                ""type"": ""SystemAssigned""
            },
            ""properties"": {
            },
            ""resources"": [
                {
                    ""type"": ""Microsoft.DataFactory/factories/providers/diagnosticSettings"",
                    ""apiVersion"": ""2017-05-01-preview"",
                    ""name"": ""[concat(parameters('factoryName'),'/microsoft.insights/', parameters('settingName'))]"",
                    ""location"": ""[resourceGroup().location]"",
                    ""dependsOn"": [
                        ""[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]""
                    ],
                    ""properties"": {
                        ""name"": ""[parameters('DS03')]"",
                        ""workspaceId"": ""[/subscriptions/3xxxxx-xxxxx-x-xxxx--xx/resourceGroups/BDAZxfdfG01]""
                    }
                }
            ]
        }
    ]
}
</code></pre>
","<azure><azure-sql-database><azure-data-factory><azure-resource-manager><azure-keyvault>","2020-05-12 11:52:27","632","-1","1","61761798","<p>The ARM template above is creating the diagnostic settings; however it is not actually configuring the logging of anything.  Add the following for all Data Factory metrics after your workspaceID property.</p>

<pre><code>    ""logAnalyticsDestinationType"": ""Dedicated"",
    ""logs"": [
      {
        ""category"": ""PipelineRuns"",
        ""enabled"": true,
        ""retentionPolicy"": {
          ""enabled"": false,
          ""days"": 0
        }
      },
      {
        ""category"": ""TriggerRuns"",
        ""enabled"": true,
        ""retentionPolicy"": {
          ""enabled"": false,
          ""days"": 0
        }
      },
      {
        ""category"": ""ActivityRuns"",
        ""enabled"": true,
        ""retentionPolicy"": {
          ""enabled"": false,
          ""days"": 0
        }
      }
    ],
    ""metrics"": [
      {
        ""category"": ""AllMetrics"",
        ""timeGrain"": ""PT1M"",
        ""enabled"": true,
        ""retentionPolicy"": {
          ""enabled"": false,
          ""days"": 0
        }

      }
    ]
</code></pre>

<p>Besides configuring the diagnostic settings what metrics and diagnostics must be select to send to log analytics.  These fields align to those on the diagnostic blade:
<a href=""https://i.stack.imgur.com/RSUnl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSUnl.png"" alt=""enter image description here""></a></p>

<p>The ""logAnalyticsDestinationType"": ""Dedicated"" is to ensure the logs go to their own table as opposed to the default AzureDiagnostic table.  There is <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/resource-logs-collect-workspace"" rel=""nofollow noreferrer"">documented limitation</a> in the original table.</p>
"
"61745999","Parameterised load in Azure Mapping Data Flows","<p>I am trying to create a Data Warehouse load using Azure Data Factory and Mapping data flows. I have run into an issue when I am trying to sink my data. I am using the following Mapping Data Flow</p>

<p><a href=""https://i.stack.imgur.com/XOFwK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XOFwK.png"" alt=""Full Mapping Data Flow""></a></p>

<p><a href=""https://i.stack.imgur.com/xBgKD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xBgKD.png"" alt=""Select RowsToInsert""></a>
My problem initially is with the Select tas. Since the parameter columns are being loaded from a config table I need this task to only select two columns from the list and insert into my target table. The first is the ID of the table and the second is a column called <code>hashKey</code> that is created in the Derived Calculation tab.</p>

<p>I get the following error when I try: <code>Cannot read property 'length' of undefined</code></p>

<p>How do I get around this problem?</p>
","<azure-data-factory>","2020-05-12 07:24:20","130","0","1","62336416","<p>I managed to resolve my problem I was having around creating a meta-data driven column names in my pipeline. The error message I was seeing before was user error. </p>

<p>I resolved my error by creating a rule-based mapping. An example of my solution is below.</p>

<p>A few things to note:</p>

<p>""name "" in the image below is the name of column in the pipeline
the items in on the right-hand side are used to rename target columns</p>

<p><a href=""https://i.stack.imgur.com/tXO2R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tXO2R.png"" alt=""Metadata Driven rename""></a></p>
"
"61743164","SSL certificate usage in Azure Data Factory","<p>I want to use a SSL certificate while creating a linked service in azure data factory to DB2 database. While we do it using JDBC we don't provide password for DB and only set SSL properties.
How can we do that in azure data factory? Password seems to a mandatory field there.</p>
","<azure><ssl><db2><azure-data-factory>","2020-05-12 03:13:55","1980","0","1","61745417","<p>Yes, as you said, when Data Factory connect to DB2, we must provide the password:</p>

<p><a href=""https://i.stack.imgur.com/yCeM3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yCeM3.png"" alt=""enter image description here""></a></p>

<p>Even we can connect the DB2 with connection string with the  SSL certificate, we still need the password:</p>

<p><a href=""https://i.stack.imgur.com/7GsGc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7GsGc.png"" alt=""enter image description here""></a></p>

<p>That means that Data Factory doesn't support using SSL certificate to connect to DB2 without the username and password. </p>

<p>Hope this helps.</p>
"
"61739714","Databricks Ingest use cases","<p>I've just found a new Databricks feature called <a href=""https://databricks.com/blog/2020/02/24/introducing-databricks-ingest-easy-data-ingestion-into-delta-lake.html"" rel=""nofollow noreferrer"">Databricks Data Ingestion</a>. There is very little material about it at this point. </p>

<p><em>When I should use Databricks Data Ingestion instead of existing mature tools like Azure Data Factory (ADF) or Apache Nifi?</em></p>

<p>Both ADF and Nifi can ingest into ADLS/S3, and AFAIK ADLS/S3 can be mapped to Databricks DBFS without copying any data, and parquet files <a href=""https://docs.databricks.com/spark/latest/spark-sql/language-manual/convert-to-delta.html"" rel=""nofollow noreferrer"">can be easily converted into Delta format</a>. So what is the benefit or use cases for using new tool? What I am missing?</p>
","<apache-nifi><azure-data-factory><databricks><data-ingestion>","2020-05-11 21:29:14","1131","2","1","61809758","<p>There are three items in the blog post.</p>

<ol>
<li>Auto Loader </li>
<li>COPY INTO </li>
<li>Data Ingestion from 3rd party sources</li>
</ol>

<p>Auto Loader and COPY INTO simplify state management of your data ingest pipeline. What I mean by state management is the management of what files or events have been ingested and processed. With Nifi, Airflow, ADF, you need a separate state store to track which files have been ingested or not. ETL systems often 'move' ingested files to another folder. This is still state management. Others might track the file in a database or a no-sql data store.</p>

<p>Before Auto Loader or COPY INTO, you would have to:
1. Detect files in landing zone
2. Compare file with files already ingested.
2. Present the file for processing
3. Track which file you've ingested.</p>

<p>If these steps get messed up, fall behind, then a file might be ingested and processed twice or lost. Moving files has a cost in complexity.</p>

<p>With Auto Loader or COPY INTO, in one statement, you can setup a streaming or incremental data ingest. Set an archive policy on the landing zone for 7 days or 48 hours, your landing zone clears itself automatically. Your code &amp; architecture is greatly simplified.</p>

<p>Auto Loader (for streams) and COPY INTO (for re-curing batch jobs) utilize Databricks under the covers to track and manage the state for you. For Auto Loader, Databricks will setup the infrastructure, SNS, SQS that greatly reduces the latency of your streaming data ingest.</p>

<p>The Third item in the blog post, is an announcement of partnerships with well established data ingest companies that have a wide array of out of the box connectors to your enterprise data. These companies work with Delta Lake.</p>

<p>You would still use Nifi, Airflow, Stream Sets etc. to acquire the data from source systems. Those tools would now only trigger the 'COPY INTO' command as needed for batch / micro batch ingest. Auto Loader will just run continuously or run when triggered. </p>
"
"61738721","Associate an Azure DataFactory pipeline to a billing report","<p>Is there any way to label the DataFactory pipelines so you can see the invoice and associate the cost with a particular use case?</p>

<p>Or does some other alternative that helps to fulfill this purpose exist?</p>
","<azure><azure-data-factory>","2020-05-11 20:30:28","90","0","1","61742925","<p>I'm afraid we can't achieve that.</p>

<p>We only can see the cost details for each  Service Resource in Data Factory per month:
<a href=""https://i.stack.imgur.com/5xwZ7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5xwZ7.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61738276","Filter source data in Azure Data Factory (Data Flow) to get most recent values","<p>I have the following data in my source:</p>

<p><a href=""https://i.stack.imgur.com/mmd22.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mmd22.png"" alt=""enter image description here""></a></p>

<p>I want to filter the data such that output will only include data with most recent date.
For example: DEF has 2 values 2.5, 3-Jan and 4, 4-Jan. I want rows with most recent date (4, 4-Jan) so that the output will contain the following result. How do I do that in data flow?</p>

<p><a href=""https://i.stack.imgur.com/o8o8f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o8o8f.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-05-11 20:02:33","1824","0","1","61748979","<p>I tried it successfully.Please follow the steps.</p>

<p>First step:
create a <code>source</code>(I used csv file with the data you provided),<code>RecentDate</code> column should be like '01-04-2020' rather than '04-Jan' because '04-Jan' can't be sorted.
The setting of Projection see the picture.</p>

<p>Second step:
create an <code>aggregate</code>, please choose 'Movie' column in the <code>Group By</code> configuration, and the setting of <code>Aggregates</code> please see the image.</p>

<p>Third step:
create a <code>new branch</code>(click '+' at the bottom right corner of source)</p>

<p>Fourth step:
create a <code>join</code> and setting please see the image.</p>

<p>The last step:
create a <code>select</code> and remove two duplicate column('Movie' and 'RecentDate') then output the <code>sink</code>.</p>

<p>Hope these can help you.</p>

<p>Below is all images:</p>

<p><a href=""https://i.stack.imgur.com/lwWy1.png"" rel=""nofollow noreferrer"">image</a></p>

<hr>

<p><strong><em>Update Answer:</em></strong></p>

<p>Below is how to output only max rate:</p>

<p>This is my test data:
<code>
Movie,MaxRate,RecentDate
ABC,3,02-01-2020
DEF,2.5,03-02-2020
DEF,4,04-01-2020
DEF,6,03-02-2020
</code>
This is total flow image:</p>

<p><a href=""https://i.stack.imgur.com/Rmyln.png"" rel=""nofollow noreferrer"">total</a></p>

<p>First step:create a source.The setting of Projection like this</p>

<p><a href=""https://i.stack.imgur.com/woRjD.png"" rel=""nofollow noreferrer"">source</a></p>

<p>Second step: create an aggregate, please choose 'Movie' and 'RecentDate' column in the Group By configuration, and the setting of Aggregates is below：</p>

<p><a href=""https://i.stack.imgur.com/f8aa2.png"" rel=""nofollow noreferrer"">aggregate</a></p>

<p>Third step: create an aggregate, please choose 'Movie' column in the Group By configuration, and the setting of Aggregates is below：</p>

<p><a href=""https://i.stack.imgur.com/cbbFh.png"" rel=""nofollow noreferrer"">aggregate2</a></p>

<p>Fourth step: create a join and setting please see the image</p>

<p><a href=""https://i.stack.imgur.com/mBHZa.png"" rel=""nofollow noreferrer"">join</a></p>

<p>The last step: create a select and remove two duplicate column('Movie' and 'RecentDate') 
then name 'Rate' as 'MaxRate',finally output the sink. </p>

<p>Below is output:
<code>
Movie,MaxRate,RecentDate
ABC,3,02-01-2020
DEF,6,03-02-2020
</code></p>

<p>If you want to output the min rate,just change the <code>max($$)</code> to <code>min($$)</code> in Second step.</p>

<p>If you want to output the both max and min rate, please do like above(max rate flow) until creating<code>select</code>finish and then <code>New branch</code> do min rate flow until creating <code>select</code> finish,finally join two select and delete duplicate column.</p>

<p>If you have another question,please let me know.</p>
"
"61724044","Azure Data Factory - Error creating a parameterised mapping dataflow","<p>I am having an error when trying to create a parameterised Mapping Data Flow. More specifically, I have the following error. Anyone have suggestions on how to fix it or what the error may be? </p>

<pre><code>{ ""Message"": ""ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'body('DataFlowDebugExpressionResolver')?.Data Vault Loadb734571b6d5a414ea8387a08077f1ff1?.DataVaultSource.sourcetable': expected token 'EndOfData' and actual 'Identifier'."" } - RunId: 24ee9884-610d-4061-a9be-670aeb8f1660
</code></pre>

<p>Thanks @Leon and @Joel for your responses. I am attaching my pipelines here for your consideration</p>

<p>[<img src=""https://i.stack.imgur.com/nRx31.png"" alt=""Data Source connection with Parameter. I can preview data with no errors""><a href=""https://i.stack.imgur.com/nRx31.png"" rel=""noreferrer"">1</a></p>

<p><a href=""https://i.stack.imgur.com/OMdD7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OMdD7.png"" alt=""Error caused in the data flow when trying to debug or run the mapping data for (Runtime Error)""></a></p>
","<azure-data-factory>","2020-05-11 07:06:27","2993","5","2","61745291","<p>I have found the resolution to the problem I raised yesterday. The error was caused by the name of the Mapping Data Flow (Data Vault Load).</p>

<p>I raised a request from Microsoft and they had the following suggestion </p>

<blockquote>
  <p>Spaces in name of object and parameters does not go well</p>
</blockquote>

<p>Once I removed the spaces in my Mapping Data Flow name it, this particular error was resolved.</p>

<p><a href=""https://i.stack.imgur.com/OrwXN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OrwXN.png"" alt=""Fixed Error by removing spaces in the name""></a></p>

<p>Thanks to everyone that responded.</p>
"
"61724044","Azure Data Factory - Error creating a parameterised mapping dataflow","<p>I am having an error when trying to create a parameterised Mapping Data Flow. More specifically, I have the following error. Anyone have suggestions on how to fix it or what the error may be? </p>

<pre><code>{ ""Message"": ""ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'body('DataFlowDebugExpressionResolver')?.Data Vault Loadb734571b6d5a414ea8387a08077f1ff1?.DataVaultSource.sourcetable': expected token 'EndOfData' and actual 'Identifier'."" } - RunId: 24ee9884-610d-4061-a9be-670aeb8f1660
</code></pre>

<p>Thanks @Leon and @Joel for your responses. I am attaching my pipelines here for your consideration</p>

<p>[<img src=""https://i.stack.imgur.com/nRx31.png"" alt=""Data Source connection with Parameter. I can preview data with no errors""><a href=""https://i.stack.imgur.com/nRx31.png"" rel=""noreferrer"">1</a></p>

<p><a href=""https://i.stack.imgur.com/OMdD7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OMdD7.png"" alt=""Error caused in the data flow when trying to debug or run the mapping data for (Runtime Error)""></a></p>
","<azure-data-factory>","2020-05-11 07:06:27","2993","5","2","70395686","<p>I have been looking for going around looking for this answer it is 2021 and Microsoft still didn't add any validation or restriction for the naming of the Data Flow</p>
"
"61721387","Azure Data Factory Pipeline Orchestration Framework","<p>I got around 9 source system (on premises / cloud based). </p>

<ol>
<li>Every source system has to end up in Raw zone of Data Lake for both Initial and Incremental Load.</li>
<li>But few will go through standardisation zone and then to Staging (SQL Tables) which will be ultimately 
consumed by Raw Vault (Data Vault 2.0) processing.</li>
<li>Rest will be straight from Raw Zone to Staging (SQL Tables)</li>
</ol>

<p>We got all the pipeline done to move data from source => raw zone => standardisation zone => Staging.  </p>

<p>Now am looking into ADF Orchestration Framework, where i should be able to dynamically execute pipeline based on the Metadata / next valid Pipeline to run.</p>

<p>With Execute Pipleline there is no option to specify dynamically which pipeline to execute.</p>

<p>What is the best option Logic App / Azure function.</p>

<p>I want to pass the Pipeline name dynamically.</p>
","<azure-functions><azure-data-factory><azure-logic-apps><procfwk>","2020-05-11 02:31:48","416","1","1","61723951","<p>If you don't have too many pipelines to direct to, you can use the <code>Switch activity</code> in a pipeline to run different pipelines for different cases based on metadata.</p>
"
"61706462","Azure Data Factory connecting to Blob Storage via Access Key","<p>I'm trying to build a very basic data flow in Azure Data Factory pulling a JSON file from blob storage, performing a transformation on some columns, and storing in a SQL database.  I originally authenticated to the storage account using Managed Identity, but I get the error below when attempting to test the connection to the source: </p>

<blockquote>
  <p>com.microsoft.dataflow.broker.MissingRequiredPropertyException:
  account is a required property for [myStorageAccountName].
  com.microsoft.dataflow.broker.PropertyNotFoundException: Could not
  extract value from [myStorageAccountName] - RunId: xxx</p>
</blockquote>

<p>I also see the following message in the Factory Validation Output:</p>

<blockquote>
  <p>[MyDataSetName] AzureBlobStorage does not support SAS,
  MSI, or Service principal authentication in data flow.</p>
</blockquote>

<p>With this I assumed that all I would need to do is switch my Blob Storage Linked Service to an Account Key authentication method.  After I switched to Account Key authentication though and select my subscription and storage account, when testing the connection I get the following error:</p>

<blockquote>
  <p>Connection failed Fail to connect to
  <a href=""https://[myBlob].blob.core.windows.net/"" rel=""noreferrer"">https://[myBlob].blob.core.windows.net/</a>: Error Message: The
  remote server returned an error: (403) Forbidden. (ErrorCode: 403,
  Detail: This request is not authorized to perform this operation.,
  RequestId: xxxx), make sure the
  credential provided is valid. The remote server returned an error:
  (403) Forbidden.StorageExtendedMessage=, The remote server returned an
  error: (403) Forbidden. Activity ID:
  xxx.</p>
</blockquote>

<p>I've tried selecting from Azure directly and also entering the key manually and get the same error either way.  One thing to note is the storage account only allows access to specified networks.  I tried connecting to a different, public storage account and am able to access fine.  The ADF account has the Storage Account Contributor role and I've added the IP address of where I am working currently as well as the IP range of Azure Data Factory that I found here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses</a></p>

<p>Also note, I have about 5 copy data tasks working perfectly fine with Managed Identity currently, but I need to start doing more complex operations.</p>

<p>This seems like a similar issue as <a href=""https://stackoverflow.com/questions/59959655/unable-to-create-a-linked-service-in-azure-data-factory"">Unable to create a linked service in Azure Data Factory</a> but the Storage Account Contributor and Owner roles I have assigned should supersede the Reader role as suggested in the reply.  I'm also not sure if the poster is using a public storage account or private.</p>

<p>Thank you in advance.</p>
","<azure><azure-storage><azure-blob-storage><azure-data-factory>","2020-05-10 02:35:26","11676","8","2","61711558","<p>This is what you faced now:</p>
<p><a href=""https://i.stack.imgur.com/pRsnS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pRsnS.png"" alt=""enter image description here"" /></a></p>
<p>From the description we know that is a connection error of storage. I also set the contributer role to the data factory, but still get the problem.</p>
<p>The problem comes from the network and firewall of your storage account. Please have a check of it.</p>
<p><a href=""https://i.stack.imgur.com/sM7Tn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sM7Tn.png"" alt=""enter image description here"" /></a></p>
<p>Make sure you have add the client id and the 'Trusted Microsoft services' exception.</p>
<p>Have a look of this doc:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security#trusted-microsoft-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security#trusted-microsoft-services</a></p>
<p>Then, go to your adf, choose these:</p>
<p><a href=""https://i.stack.imgur.com/oUIZI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oUIZI.png"" alt=""enter image description here"" /></a></p>
<p>After that, it should be ok.</p>
"
"61706462","Azure Data Factory connecting to Blob Storage via Access Key","<p>I'm trying to build a very basic data flow in Azure Data Factory pulling a JSON file from blob storage, performing a transformation on some columns, and storing in a SQL database.  I originally authenticated to the storage account using Managed Identity, but I get the error below when attempting to test the connection to the source: </p>

<blockquote>
  <p>com.microsoft.dataflow.broker.MissingRequiredPropertyException:
  account is a required property for [myStorageAccountName].
  com.microsoft.dataflow.broker.PropertyNotFoundException: Could not
  extract value from [myStorageAccountName] - RunId: xxx</p>
</blockquote>

<p>I also see the following message in the Factory Validation Output:</p>

<blockquote>
  <p>[MyDataSetName] AzureBlobStorage does not support SAS,
  MSI, or Service principal authentication in data flow.</p>
</blockquote>

<p>With this I assumed that all I would need to do is switch my Blob Storage Linked Service to an Account Key authentication method.  After I switched to Account Key authentication though and select my subscription and storage account, when testing the connection I get the following error:</p>

<blockquote>
  <p>Connection failed Fail to connect to
  <a href=""https://[myBlob].blob.core.windows.net/"" rel=""noreferrer"">https://[myBlob].blob.core.windows.net/</a>: Error Message: The
  remote server returned an error: (403) Forbidden. (ErrorCode: 403,
  Detail: This request is not authorized to perform this operation.,
  RequestId: xxxx), make sure the
  credential provided is valid. The remote server returned an error:
  (403) Forbidden.StorageExtendedMessage=, The remote server returned an
  error: (403) Forbidden. Activity ID:
  xxx.</p>
</blockquote>

<p>I've tried selecting from Azure directly and also entering the key manually and get the same error either way.  One thing to note is the storage account only allows access to specified networks.  I tried connecting to a different, public storage account and am able to access fine.  The ADF account has the Storage Account Contributor role and I've added the IP address of where I am working currently as well as the IP range of Azure Data Factory that I found here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses</a></p>

<p>Also note, I have about 5 copy data tasks working perfectly fine with Managed Identity currently, but I need to start doing more complex operations.</p>

<p>This seems like a similar issue as <a href=""https://stackoverflow.com/questions/59959655/unable-to-create-a-linked-service-in-azure-data-factory"">Unable to create a linked service in Azure Data Factory</a> but the Storage Account Contributor and Owner roles I have assigned should supersede the Reader role as suggested in the reply.  I'm also not sure if the poster is using a public storage account or private.</p>

<p>Thank you in advance.</p>
","<azure><azure-storage><azure-blob-storage><azure-data-factory>","2020-05-10 02:35:26","11676","8","2","61784906","<p>At the very bottom of the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""noreferrer"">article listed above</a> about white listing IP ranges of the integration runtime, Microsoft says the following:</p>

<blockquote>
  <p>When connecting to Azure Storage account, IP network rules have no
  effect on requests originating from the Azure integration runtime in
  the same region as the storage account. For more details, please refer
  <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security#grant-access-from-an-internet-ip-range"" rel=""noreferrer"">this article</a>.</p>
</blockquote>

<p>I spoke to Microsoft support about this and the issue is that white listing public IP addresses does not work for resources within the same region because since the resources are on the same network, they connect to each other using private IP's rather than public.</p>

<p>There are four options to resolve the original issue:</p>

<ul>
<li>Allow access from all networks under Firewalls and Virtual Networks in the storage account (obviously this is a concern if you are storing sensitive data).  I tested this and it works.</li>
<li>Create a new Azure hosted integration runtime that runs in a different region.  I tested this as well.  My ADF data flow is running in East region and I created a runtime that runs in East 2 and it worked immediately.  The issue for me here is I would have to have this reviewed by security before pushing to prod because we'd be sending data across the public network, even though it's encrypted, etc, it's still not as secure as having two resources talking to each other in the same network.</li>
<li>Use a separate activity such as an HDInsight activity like Spark or an SSIS package.  I'm sure this would work, but the issue with SSIS is cost as we would have to spin up an SSIS DB and then pay for the compute.  You also need to execute multiple activities in the pipeline to start and stop the SSIS pipeline before and after execution.  Also I don't feel like learning Spark just for this.</li>
<li>Finally, the solution that works that I used is I created a new connection that replaced the Blob Storage with a Data Lakes Gen 2 connection for the data set.  It worked like a charm.  Unlike Blob Storage connection, Managed Identity is supported for Azure Data Lakes Storage Gen 2 as <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#managed-identity"" rel=""noreferrer"">per this article.</a> In general, the more specific the connection type, the more likely the features will work for the specific need.</li>
</ul>
"
"61702704","Unzip files from a ZIP file and use them as source in an ADF Copy activity","<p>I see there is a way to deflate a ZIP file but when there are multiple .csv files within a ZIP, how do I specify which to use as my source for copy activity? It is now parsing both csv files and giving as a single file and I'm not able to select the file I want as source</p>
","<azure><azure-data-factory><zip>","2020-05-09 19:43:24","485","2","1","61707841","<p>According to my test, we can't unzip <code>.zip</code> file in the ADF to get the file name lists in the ADF dataset. So, i provide below workaround for your reference.</p>

<p>Firstly, you could use Azure Function Activity to trigger a function which is for the decompression of your zip file.You only need to get the file name list then return it as an array.</p>

<p>Secondly, use ForEach Activity to loop the result, to get your desired file name.</p>

<p>Finally, inside ForEach Activity, please use <code>@item()</code> in the Dataset to configure the specific file path so that you could you could refer it in the copy activity.</p>
"
"61698566","How to get max of a given column from ADF Copy Data activity","<p>I have a copy data activity for on-premise SQL Server as source and ADLS Gen2 as sink. There is a control table to pickup tableName, watermarkDateColumn and the watermarkDatetime to pull incremental data from the source database. </p>

<p>After data is pulled/loaded in sink, I want to get the max of the watermarkDateColumn in my dataset. Can it be obtained from <code>@activity('copyActivity1').output</code>?</p>

<p>I'm not allowed to use one extra lookup activity to query the source table for getting the max(watermarkDateColumn) in pipeline.</p>
","<copy><pipeline><lookup><azure-data-factory>","2020-05-09 14:46:14","2064","2","1","61768148","<p>Copy activity only could be used for data transmission,not for any other aggregation feature. So @activity('copyActivity1').output won't help. Since you said you can't use lookup activity, i'm afraid your requirement is not available so far.</p>

<p>If you prefer not using additional activities, I suggest you using Data Flow Activity instead which is more flexible.There is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate"" rel=""nofollow noreferrer"">built-in aggregation</a> feature in the Data Flow Activity.</p>

<p><a href=""https://i.stack.imgur.com/doqhB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/doqhB.png"" alt=""enter image description here""></a></p>
"
"61697565","Build a pipeline in azure data factory to load Excel files, format content, transform in csv and send to azure sql DB","<p>I'm approaching to Azure environment and watching tutorials/reading documents, but I'm trying to figure out how to setup a flow that enables the process that I will describe hereunder. The starting point are reports in .xlsx format produced monthly by Mktg Dept: the requirements are to bring them in Azure SQL DB so that data can be stored and analysed. Sofar I managed to put those files (previously manually converted in .csv format) in a BLOB storage and build an ADF pipeline that copy each file in a table on the SQL DB.
The problem is that as far as I understood with ADF it's not possible to directly manage xlsx files, and I'm wondering how to set up an automated procedure that enables the conversion from .xlsx to .csv and save them on BLOB storage. I was thinking about adding to the pipeline a python script/Databricks notebook to convert format, but I'm not sure this could be the best solution. Any hint/reference to existing tutorial or resources would be very appreciated </p>
","<azure><azure-data-factory><databricks>","2020-05-09 13:33:40","562","0","1","62163196","<p>I found a <a href=""https://visualbi.com/blogs/microsoft/azure/consuming-excel-files-azure-data-factory/"" rel=""nofollow noreferrer"">tutorial</a> which uses Logic Apps to do the conversion.</p>

<p>Datanovice indirectly suggested using a Custom activity to run either a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">C#</a> or <a href=""https://learn.microsoft.com/en-us/azure/batch/tutorial-run-python-batch-azure-data-factory"" rel=""nofollow noreferrer"">Python</a> application to do the conversion for you.</p>

<p>The least expensive solution would be to do the conversion before uploading to blob, like Datanovice said.</p>
"
"61691763","Azure DevOps Data Factory Dataset and pipeline CI/CD Parameters","<p>I am trying to build the CI/CD for Azure Data Factory using Azure DevOps.  I am able to setup the Pipeline and Release.  However I have a problem:</p>

<p>Dataset </p>

<p>I have 2 environments, DEV and PROD, How can I pass parameter in the CD pipeline to change the File path (e.g. dev and prod) in different deployment stage (dev and prod environment) in Sink and Source</p>

<p><a href=""https://i.stack.imgur.com/bMY6M.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Thank you for your help!</p>
","<azure><azure-devops><devops><azure-data-factory>","2020-05-09 04:21:10","457","2","1","61729515","<p>There is another approach to publish ADF, from master (collaboration) branch.
You can define (replace) value for every single node (property) in json file (ADF object).
It will resolve your problem as you can provide separate CSV config file per each environment (stage).</p>

<p>Example of CSV config file (<code>config-stage-UAT.csv</code>):</p>

<pre><code>type,name,path,value
pipeline,PL_CopyMovies,activities[0].outputs[0].parameters.BlobContainer,UAT
</code></pre>

<p>Then just run such cmdlet in PowerShell:</p>

<pre><code>Publish-AdfV2FromJson -RootFolder ""$RootFolder"" -ResourceGroupName ""$ResourceGroupName"" -DataFactoryName ""$DataFactoryName"" -Location ""$Location"" -Stage ""stage-UAT""
</code></pre>

<p>Check this out:
<a href=""https://www.powershellgallery.com/packages/azure.datafactory.tools/"" rel=""nofollow noreferrer"">azure.datafactory.tools</a> (PowerShell module)</p>
"
"61686164","Calling a Variable in a Concat statement on Azure Data Factory","<p>I am calling the following expression purely in a variable activity as I am stepping through the logic to understand where this problem is coming from:</p>

<p>@concat('{d'',variables('QODBC Past Date'),''}')</p>

<p>I keep on getting the following error:</p>

<blockquote>
  <p>Invalid
  Position 25 Syntax error: Missing comma between arguments</p>
</blockquote>

<p>I am clearly missing something, but when I remove either the variable expression inside the concat or the two strings, it works. Anyone know what I've gotten wrong here?</p>

<p>The desired output is 
{d'2020-04-08'}</p>

<p>This is a dynamic content that I am going to place in a SQL query for date filter context.</p>

<p>That variable is the date input that is created further up in the pipeline. </p>

<p>Thank you!</p>
","<azure><azure-data-factory>","2020-05-08 18:57:28","5245","1","1","61699460","<p>Since ' is the string delimiter you need to delimit it if you want it in the output. You can delimit it by doubling it up so try this: </p>

<pre><code>@concat('{d''',variables('QODBC Past Date'),'''}') 
</code></pre>

<p>Notice 3 ' in a row in two places - one for the end of string and two to become one in the output.</p>
"
"61674890","Azure data factory promotion from test to dev, authentication method is Managed identity for Azuressql,using azure devops CI/CD","<p>I am using ADF and authentication method is Managedidentity -- then code is pushed to adf_publish 
    branch in Git hub and from there we are building the CI/CD in azure devops.</p>

<p>Problem - I used Managed identity for all the resources used in ADF like (keyvault,storage,azuresql) 
for Keyvault I am geeting this http url (<a href=""https://BDAdhfsKV01.vault.azure.net/"" rel=""nofollow noreferrer"">https://BDAdhfsKV01.vault.azure.net/</a>) this url we are 
paramterised in Azure devops release pipline (https://""$(vaultname)"".vault.azure.net/) like this, 
but for Azure sql while using Managed identity I am getting blank in connection string after 
publishing to adf_publish branch to Git. How can be prameterised or promote to dev. </p>

<p>Just for refrence attaching the devops screen shot how I have paramterisded the keyvault URL at CD.
Same case blank connection string using Managed identity is with Azure synapse as well , Is there any 
help on this or any issue with Managed identity.</p>

<p><img src=""https://i.stack.imgur.com/EpfUA.png"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/FnWZL.png"" alt=""enter image description here""></p>
","<github><azure-devops><azure-sql-database><azure-data-factory><azure-managed-identity>","2020-05-08 08:16:17","308","1","1","62040017","<p>You just need to set the connection string as below:</p>

<pre><code>""connectionString"": ""Server=tcp:&lt;servername&gt;.database.windows.net,1433;Database=&lt;databasename&gt;;Connection Timeout=30""
</code></pre>

<p>You can also refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#managed-identity"" rel=""nofollow noreferrer"">tutorial</a>.</p>
"
"61674001","Copy one file at a time from one container to another using azure data factory v2","<p>I am trying to copy one file from one container to another another in a storage account. the scenario i implemented works fine for a single file. but for multiple files, it is copying both of them in one copy activity. i want the file to be moved one at a time and after a single copy to provide a delay of 1 min, then proceed with the next file copy.</p>

<p>i created a pipeline with the move File template but it did not work for multiple files.
i have taken the source and sink dataset as csv datasets and not binary. i will not be aware of the pattern or the names of the files.</p>

<p>when a user input say about 10 files, i want to copy it one at a time and also provide a delay between each copy. this has to happen between 2 storage account containers.</p>

<p>i have tried to use move files template too. but it did not work for multiple. Please help me.</p>
","<azure><azure-blob-storage><azure-data-factory>","2020-05-08 07:18:47","940","3","1","61684409","<p>Sanaa, to force a sequential processing, check the ""Sequential"" checkbox:
<a href=""https://i.stack.imgur.com/fCJiA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fCJiA.png"" alt=""enter image description here""></a></p>

<p>Time delay could be achieved by adding ""Wait"" action:
<a href=""https://i.stack.imgur.com/xrMXF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xrMXF.png"" alt=""enter image description here""></a></p>
"
"61664944","Azure Data Factory: security for Web and Webhook activities","<p>I want to integrate ADF pipeline and Nifi flow through ADF's <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity"" rel=""nofollow noreferrer"">Webhook</a> activities. Means ADF trigger Nifi through HTTP call, than wait till Nifi respond to HTTP hook.
Nifi is deployed on Azure VMs as Docker container. </p>

<p>As far as I know, Azure Data Factory (ADF) has only public endpoint and doesn't have firewall.</p>

<p><em>Is there any secure way to arrange HTTP communication between ADF and Nifi?</em></p>

<p>Important: I'm not passing sensitive data between ADF and Nifi, only pipeline arguments. But I want to make sure that nobody except Nifi can trigger ADF Webhook. And nobody except ADF may trigger Nifi HTTP listener.</p>
","<azure><networking><azure-data-factory><azure-virtual-network><azure-security>","2020-05-07 18:19:03","477","0","1","62185313","<p>The way you can call a API from ADF is by using web activity , unfortunately web activity can only access url on public domain . I don't think you can use ADF for this . </p>

<p><a href=""https://github.com/MicrosoftDocs/azure-docs/issues/17779"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/17779</a></p>
"
"61658519","How to export azure data factory from one account to another","<p>i have two azure account with different subscriptions and i want to move azure factory from one account to another.Any help would be appreciated.
Thanks</p>
","<azure-data-factory>","2020-05-07 13:03:14","217","0","1","61658829","<p>You could export the arm template of your ADF account and import it into other destination.Please refer to this document:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">Continuous integration and delivery in Azure Data Factory</a>.</p>

<p><a href=""https://i.stack.imgur.com/daKM8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/daKM8.png"" alt=""enter image description here""></a></p>

<p>You could <a href=""http://%20https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">customize the parameters of ARM template</a> to change the different subscription information.</p>
"
"61658334","Tumbling Window Trigger in Azure Data Factory","<p>How can I parameterize the tumbling window trigger in ADF to run between a specific time(9Am-10AM) daily.
Is it possible to achieve it through the MyWindowStart and MyWindowEnd parameters?</p>

<p>Regards,
Sandeep</p>
","<triggers><azure-data-factory>","2020-05-07 12:52:54","563","0","1","61662597","<p><a href=""https://i.stack.imgur.com/UlEpY.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/UlEpY.png</a></p>

<p>ADFv2-A schedule trigger like mentioned in attachment would do a similar thing</p>

<p>ADFv1-Add offset in pipeline and datasets related to the same pipeline</p>

<p>""scheduler"": {
          ""frequency"": ""Day"",
          ""interval"": 1,
          ""offset"": ""09:30:00""
        }</p>
"
"61658102","Azure Data Facory - Connect to google's Firebase DB","<p>My client would like to extract some data from a Firebase/Firestore database with Azure Data Factory. I'm not seeing an obvious way to make this connection.</p>

<p>Any slick ways to connect to Firebase from Azure Data Factory?</p>
","<azure-data-factory>","2020-05-07 12:41:18","746","1","1","61670843","<p>Actually, Data factory doesn't support the google's Firebase DB connector.</p>

<p>We could not find the Google Firebase DB connector in all the supported database connector list:</p>

<p><a href=""https://i.stack.imgur.com/YsKW0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YsKW0.png"" alt=""enter image description here""></a></p>

<p>For more details, please reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Azure Data Factory connector overview</a></p>

<p>Others also post a <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/39632509-google-firebase-connector"" rel=""nofollow noreferrer"">Google Firebase Connector support</a> request in Data Factory feedback, but still no responds:</p>

<p><a href=""https://i.stack.imgur.com/jsHC5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jsHC5.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61657165","Azure DataFactory - diagnostics settings - activate logging to resource specific table","<p>I want to change diagnostics settings on my azure datafactory resource.</p>

<p>Actually, i'm logging into Azure Diagnostics table but i encounter a problem with max size (500 columns limit).</p>

<p>In my ARM template, i have added this :</p>

<pre><code>""logAnalyticsDestinationType"": ""Dedicated""
</code></pre>

<p>But when i deploy this new template, my logging table stay the same -> Azure Diagnostics</p>

<p>What could be wrong ?</p>

<p>Thanks for your help</p>
","<azure><azure-data-factory><azure-rm-template><azure-log-analytics><azure-diagnostics>","2020-05-07 11:55:58","164","0","1","61663760","<p>Your ARM template looks correct.  Just to make sure if you click Diagnostic settings is it correct?</p>

<p>Are you expecting the old records to be migrated to the new tables?  If so that would not happen.</p>

<p>You should see tables like this appear after 10-20 minutes as it's a schema on read so it needs to create the tables on the fly:</p>

<p><a href=""https://i.stack.imgur.com/xr0Ce.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xr0Ce.png"" alt=""enter image description here""></a></p>

<p>If you have waited and don't see the tables the 500 column ""bug"" that you are referring to might be impacting this as well. I am familiar with it and after working with MS on it, there is no quick and easy fix to this.  </p>

<p>I would suggest creating a new workspace and send the logs there and the tables should be created. If you have any subsequent queries/dashboards they'd have to be updated to <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/log-query/cross-workspace-query"" rel=""nofollow noreferrer"">query across workspaces</a>.  </p>

<p>In fact because of this limit MS recommends now have a <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/resource-logs-collect-workspace"" rel=""nofollow noreferrer"">separate workspace per application</a></p>
"
"61655882","Data Factory: JSON data is interpreted as expression - ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression","<p>I want to copy items from </p>

<p><em>CosmosDB database<strong>A</strong>/productCollection</em></p>

<p>to </p>

<p><em>CosmosDB database<strong>B</strong>/productCollection</em></p>

<p>Therefore I decided to use Azure Data Factory.
I actived also ""Export as-is to JSON files or Cosmos DB collection"".</p>

<p>The read operation works as expected.<br>
Unfortunately, the write operation stops because of an error related to the data:</p>

<blockquote>
  <p>ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'Currency'</p>
</blockquote>

<pre><code>{
""ProductName"": ""Sample"",
""Price"": {
    ""@Currency"": ""GBP"",
    ""$"": ""2624.83""
}
</code></pre>

<p>}</p>

<p>I'm not able to change to input data itself.
The output data has to equal the input data.</p>

<p>Is there possiblity, that <em>@Currency</em> will not be interpreted as an expression</p>

<p>In ARM, this part is failling:  </p>

<blockquote>
  <p>Price.{@Currency}</p>
</blockquote>
","<azure-cosmosdb><azure-data-factory>","2020-05-07 10:46:47","2038","1","2","61681517","<p>I tried to reproduce your issue but it works for me. I used copy activity to transfer data from account A to account B.</p>

<p><a href=""https://i.stack.imgur.com/1rasa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1rasa.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/nCGaQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nCGaQ.png"" alt=""enter image description here""></a></p>

<p>Additional, if this operation is just need to be executed once, please consider using <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#JSON"" rel=""nofollow noreferrer"">Azure Cosmos DB Migration Tool</a>. It's free for usage. You could export the data from cosmos db A as json file then import it into cosmos db B very simply.Also, it could be executed in the cmd so that it could be made as a scheduled job on the windows system.</p>
"
"61655882","Data Factory: JSON data is interpreted as expression - ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression","<p>I want to copy items from </p>

<p><em>CosmosDB database<strong>A</strong>/productCollection</em></p>

<p>to </p>

<p><em>CosmosDB database<strong>B</strong>/productCollection</em></p>

<p>Therefore I decided to use Azure Data Factory.
I actived also ""Export as-is to JSON files or Cosmos DB collection"".</p>

<p>The read operation works as expected.<br>
Unfortunately, the write operation stops because of an error related to the data:</p>

<blockquote>
  <p>ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'Currency'</p>
</blockquote>

<pre><code>{
""ProductName"": ""Sample"",
""Price"": {
    ""@Currency"": ""GBP"",
    ""$"": ""2624.83""
}
</code></pre>

<p>}</p>

<p>I'm not able to change to input data itself.
The output data has to equal the input data.</p>

<p>Is there possiblity, that <em>@Currency</em> will not be interpreted as an expression</p>

<p>In ARM, this part is failling:  </p>

<blockquote>
  <p>Price.{@Currency}</p>
</blockquote>
","<azure-cosmosdb><azure-data-factory>","2020-05-07 10:46:47","2038","1","2","74959498","<p>I had the same problem and I was able to resolve accordingly.</p>
<p>I am using a Pipeline with a Source that is a Dataset referencing JSON data.</p>
<p>Clicking the button highlighted below.</p>
<p><a href=""https://i.stack.imgur.com/EC1lA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EC1lA.png"" alt=""Dataset Editor"" /></a></p>
<p>I had to change the JSON from</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;SourceDataset&quot;,
  &quot;properties&quot;: {
    &quot;linkedServiceName&quot;: {
      &quot;referenceName&quot;: &quot;StorageAccountLink&quot;,
      &quot;type&quot;: &quot;LinkedServiceReference&quot;
    },
    &quot;annotations&quot;: [],
    &quot;type&quot;: &quot;Json&quot;,
    &quot;typeProperties&quot;: {
      &quot;location&quot;: {
        &quot;type&quot;: &quot;AzureBlobStorageLocation&quot;,
        &quot;container&quot;: &quot;test-data&quot;
      }
    },
    &quot;schema&quot;: {
      &quot;type&quot;: &quot;object&quot;,
      &quot;properties&quot;: {
        &quot;@context&quot;: {
          &quot;type&quot;: &quot;string&quot;
        },
        &quot;value&quot;: {
          &quot;type&quot;: &quot;array&quot;,
          &quot;items&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
              &quot;id&quot;: {
                &quot;type&quot;: &quot;string&quot;
              }
            }
          }
        }
      }
    }
  }
}
</code></pre>
<p>To ( Escaping the @ with @@ )</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;SourceDataset&quot;,
  &quot;properties&quot;: {
    &quot;linkedServiceName&quot;: {
      &quot;referenceName&quot;: &quot;StorageAccountLink&quot;,
      &quot;type&quot;: &quot;LinkedServiceReference&quot;
    },
    &quot;annotations&quot;: [],
    &quot;type&quot;: &quot;Json&quot;,
    &quot;typeProperties&quot;: {
      &quot;location&quot;: {
        &quot;type&quot;: &quot;AzureBlobStorageLocation&quot;,
        &quot;container&quot;: &quot;test-data&quot;
      }
    },
    &quot;schema&quot;: {
      &quot;type&quot;: &quot;object&quot;,
      &quot;properties&quot;: {
        &quot;@@context&quot;: {
          &quot;type&quot;: &quot;string&quot;
        },
        &quot;value&quot;: {
          &quot;type&quot;: &quot;array&quot;,
          &quot;items&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
              &quot;id&quot;: {
                &quot;type&quot;: &quot;string&quot;
              }
            }
          }
        }
      }
    }
  }
}
</code></pre>
"
"61653592","Filter Copy-Data source using variable","<p>Scenario: I have multiple Views on a Azure SQL Database as source for a Copy Data pipeline. The Views contain data for multiple customers so I need the pipeline filtered by a customer ID. </p>

<p>I can do this using the Source query and just hard code the Customer ID but I'd like to make it more generic and use a variable to be set once and it be used to filter all the views. It is something that at first glance should be pretty straight forward.</p>

<p>Setting the variable is not a problem but I can't figure out the syntax to use in the Query. Or is there another mechanism I can use?</p>

<p>The basic pipeline (links as I can't embed yet):
<a href=""https://i.stack.imgur.com/sZppo.png"" rel=""nofollow noreferrer"">Basic Pipeline</a></p>

<p>Filtering using this: <a href=""https://i.stack.imgur.com/yJCaZ.png"" rel=""nofollow noreferrer"">Query</a></p>

<p>Update:
Went with a solution very similar to Jay Gong below. Didn't use @Concat but assigned parameter to variable in SQL code and used in where clause. Will look into @Concat as I suspect it's slightly more efficient.</p>
","<azure-data-factory>","2020-05-07 08:49:05","182","1","1","61910970","<p>You could consider passing parameters into ADF to complete query sql in source query blank.The sql could be dynamic content with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#concat"" rel=""nofollow noreferrer"">@concat built-in function</a>.</p>

<p>For example:</p>

<p><a href=""https://i.stack.imgur.com/s2JP9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s2JP9.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/cxqkG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cxqkG.png"" alt=""enter image description here""></a></p>
"
"61649418","Filter in Azure Data Factory","<p>I have 2 sources as follows: </p>

<p><a href=""https://i.stack.imgur.com/KTbel.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KTbel.png"" alt=""enter image description here""></a></p>

<p>I need to do a filter on Source 1 such that output contains only values with Colors except Green, Blue, Black:</p>

<p><a href=""https://i.stack.imgur.com/0XDIS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0XDIS.png"" alt=""enter image description here""></a></p>

<p>This is how my data flow looks like:</p>

<p><a href=""https://i.stack.imgur.com/7lJUV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7lJUV.png"" alt=""enter image description here""></a></p>

<p>Is there a different way to do this using a join with Source 2 Color?</p>
","<sql><join><azure-data-factory>","2020-05-07 03:30:44","60","0","1","61649503","<p>If you want to filter the person names based on color names, then you'll need to first join and then you can filter on color name.</p>
"
"61648093","How to compute a variable or column of comma separated values from multiple rows of the same column","<p><strong>Scenario</strong>: azure data flow processing bulk records from a csv dataset. for doing dependent jobs at destination sql required a comma separated ids from multiple rows of that csv. Can some one help how to do this.</p>

<p>Tried using derived column step with coalesce, concat functions,  didn't get the result looking for.</p>

<p><a href=""https://i.stack.imgur.com/dC8sZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dC8sZ.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-05-07 01:04:43","809","0","1","61648804","<p>Use the collect() aggregate function. This will act like a string agg. It was just released last week.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#collect"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#collect</a></p>

<p><a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/adf-adds-new-hierarchical-data-handling-and-new-flexibility-for/ba-p/1353956"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/adf-adds-new-hierarchical-data-handling-and-new-flexibility-for/ba-p/1353956</a></p>
"
"61644108","calling stored procedure with dataflow parameters in azure mapping dataflow sink pre and post SQL scripts","<p>Can i user SQL stored procedure on pre and post SQL scripts in sink nodes. If yes how to do if no what are the alternatives.</p>

<p><strong>Scenario</strong>: instead of using DB triggers do some work upon insert and update i would like to use this feature of mappings dataflows wot avoid triggers which are going to create problem while writing bulk data in target table.</p>

<p><a href=""https://i.stack.imgur.com/sOnlT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sOnlT.png"" alt=""enter image description here""></a></p>

<p>the working snapshot with no parameters passed to stored procedure</p>

<p><a href=""https://i.stack.imgur.com/msWPQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/msWPQ.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-05-06 19:50:34","792","0","2","61650862","<p>For example, if the parameter name is ""parameter"", please try the expremission in the Post SQL script: concat('EXEC [demo].',$parameter), this will make it as a string script.</p>

<p>Hope this helps.</p>
"
"61644108","calling stored procedure with dataflow parameters in azure mapping dataflow sink pre and post SQL scripts","<p>Can i user SQL stored procedure on pre and post SQL scripts in sink nodes. If yes how to do if no what are the alternatives.</p>

<p><strong>Scenario</strong>: instead of using DB triggers do some work upon insert and update i would like to use this feature of mappings dataflows wot avoid triggers which are going to create problem while writing bulk data in target table.</p>

<p><a href=""https://i.stack.imgur.com/sOnlT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sOnlT.png"" alt=""enter image description here""></a></p>

<p>the working snapshot with no parameters passed to stored procedure</p>

<p><a href=""https://i.stack.imgur.com/msWPQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/msWPQ.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-05-06 19:50:34","792","0","2","61650868","<p>Thanks Leon.</p>

<p>Example expression : </p>

<p>concat('EXEC [', $df_tenantDBName,'].[testPrePostSQLFromSink] ',$df_tenantDBName,',',$df_tenantDBName)</p>
"
"61643401","Throttling requests in Cosmos DB SQL API","<p>I'm running a simple adf pipeline for storing data from data lake to cosmos db (sql api).</p>

<p>After setting database throughput to Autopilot 4000 RU/s, the run took ~11 min and I see 207 throttling requests. On setting database throughput to Autopilot 20,000 RU/s, the run took ~7 min and I see 744 throttling requests. Why is that? Thank you!</p>

<p><a href=""https://i.stack.imgur.com/6sR8r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6sR8r.png"" alt=""enter image description here""></a></p>
","<azure-cosmosdb><azure-data-factory><azure-cosmosdb-sqlapi>","2020-05-06 19:10:32","748","0","1","61648981","<p>Change the Indexing Policy to <em>None</em> from <em>Consistent</em> for the ADF copy activity and then change back to Consistent when done.  </p>

<p><strong>Azure Cosmos DB supports two indexing modes:</strong></p>

<ul>
<li>Consistent: The index is updated synchronously as you create, update or delete items. This means that the consistency of your read queries will be the consistency configured for the account.</li>
<li>None: Indexing is disabled on the container. This is commonly used when a container is used as a pure key-value store without the need for secondary indexes. It can also be used to improve the performance of bulk operations. After the bulk operations are complete, the index mode can be set to Consistent and then monitored using the <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/how-to-manage-indexing-policy#dotnet-sdk"" rel=""nofollow noreferrer"">IndexTransformationProgress</a> until complete.</li>
</ul>

<p><strong>How to modify the indexing policy:</strong></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/index-policy#modifying-the-indexing-policy"" rel=""nofollow noreferrer"">Modifying the indexing policy</a></p>
"
"61641678","Azure Data Factory - Azure Data Lake Gen1 access","<p>A file is being added by the Logic Apps to the Data Factory V2
I have a Data Factory that access 'data lake gen 1' to process the file. I receive the following error, when I try to debug the data factory after file is added.</p>

<pre><code> ""ErrorCode=FileForbidden,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read a 'AzureDataLakeStore' file. File path: 'Stem/Benchmark/DB_0_Measures_1_05052020 - Copy - Copy - rounded, date changed - Copy (3).csv'.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (403) Forbidden.,Source=System,'"",
</code></pre>

<p>When I ""Apply to children""  after next load permission, error is gone.
Tried so far:
- Assigned permission in Data Lake for the Data Factory and it`s children.</p>

<ul>
<li>Assigned permission in Data Lake Folder for the Data Factory and it's children.</li>
<li>Added data factory as a contributor to data lake.</li>
<li>Added data factory as an owner to data lake.</li>
<li>Allowed ""all Azure services to access this Data Lake Storage Gen1 account"".</li>
</ul>

<p>After all tries, still need manually to ""apply permission to children"" for each file added.</p>

<p>Is there anyway to fix this?</p>
","<azure><azure-data-factory><azure-data-lake>","2020-05-06 17:41:40","458","0","2","61648902","<p>Can reproduce your error:</p>

<p><a href=""https://i.stack.imgur.com/Vyuo4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vyuo4.png"" alt=""enter image description here""></a></p>

<p>This is how I resolve:</p>

<p><a href=""https://i.stack.imgur.com/iEkp8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iEkp8.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/f37BX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f37BX.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/nwLNU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nwLNU.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/bOhnS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bOhnS.png"" alt=""enter image description here""></a></p>

<p>And my account is the owner of the data lake gen1. The datafactory is the contributor of the data lake gen1.</p>

<p><a href=""https://i.stack.imgur.com/Tly3n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tly3n.png"" alt=""enter image description here""></a></p>
"
"61641678","Azure Data Factory - Azure Data Lake Gen1 access","<p>A file is being added by the Logic Apps to the Data Factory V2
I have a Data Factory that access 'data lake gen 1' to process the file. I receive the following error, when I try to debug the data factory after file is added.</p>

<pre><code> ""ErrorCode=FileForbidden,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read a 'AzureDataLakeStore' file. File path: 'Stem/Benchmark/DB_0_Measures_1_05052020 - Copy - Copy - rounded, date changed - Copy (3).csv'.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (403) Forbidden.,Source=System,'"",
</code></pre>

<p>When I ""Apply to children""  after next load permission, error is gone.
Tried so far:
- Assigned permission in Data Lake for the Data Factory and it`s children.</p>

<ul>
<li>Assigned permission in Data Lake Folder for the Data Factory and it's children.</li>
<li>Added data factory as a contributor to data lake.</li>
<li>Added data factory as an owner to data lake.</li>
<li>Allowed ""all Azure services to access this Data Lake Storage Gen1 account"".</li>
</ul>

<p>After all tries, still need manually to ""apply permission to children"" for each file added.</p>

<p>Is there anyway to fix this?</p>
","<azure><azure-data-factory><azure-data-lake>","2020-05-06 17:41:40","458","0","2","67280593","<p>you need to give Read + Execute Access on parent folders and then do what @Bowman Zhu mentioned above.</p>
"
"61641233","Formatting Time without colon in Azure Data Factory (Java SimpleDateFormat)","<p>I'd like to format a timestamp like yyyy-MM-dd'T'HH-mm-ss as using colons causes issues when reading through ADF.</p>

<p>I'm writing it in ADF which uses Java SimpleDateFormat - can't seem to find anywhere in docs about writing a timestamp like this.</p>

<p>Is it possible and if so, how?</p>
","<java><azure><simpledateformat><azure-data-factory>","2020-05-06 17:18:22","507","1","2","61712330","<p>Based on the ADF built-in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">Date Functions</a> , your requirement can't be implemented so far.</p>

<p>However, you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#formatdatetime"" rel=""nofollow noreferrer"">formatDateTime</a> to convert your <code>datetime</code> into <code>String</code>.Then you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#string-functions"" rel=""nofollow noreferrer"">substring</a> to assemble your desired format.</p>
"
"61641233","Formatting Time without colon in Azure Data Factory (Java SimpleDateFormat)","<p>I'd like to format a timestamp like yyyy-MM-dd'T'HH-mm-ss as using colons causes issues when reading through ADF.</p>

<p>I'm writing it in ADF which uses Java SimpleDateFormat - can't seem to find anywhere in docs about writing a timestamp like this.</p>

<p>Is it possible and if so, how?</p>
","<java><azure><simpledateformat><azure-data-factory>","2020-05-06 17:18:22","507","1","2","72345908","<p>In Azure DataFlow, for individual items of time/date, use as follows :</p>
<pre><code>year(currentDate())
month(currentDate())
dayOfMonth(currentDate())
hour(currentUTC())
minute(currentUTC())
second(currentUTC())
</code></pre>
<p>Concat and use as required.</p>
"
"61637536","Azure Data Factory - Data Flow - Set Variable","<p>Within my pipeline I have a small data flow activity that reads a value out of a JSON file that I would like to use back in the main flow.</p>

<p>From withing the data flow I would like to set a variable with this value, however I've not found a way to do that.</p>

<p>So how do I set a variable with a new value from within a 'data flow' ??</p>
","<azure-data-factory>","2020-05-06 14:18:30","2084","0","2","61643191","<p>Every data flow activity needs to have a Sink. Sink that data value from your data flow results into a destination store (i.e. Blob store). You can then read that value from your data flow in a subsequent Lookup activity to load it into a pipeline variable.</p>
"
"61637536","Azure Data Factory - Data Flow - Set Variable","<p>Within my pipeline I have a small data flow activity that reads a value out of a JSON file that I would like to use back in the main flow.</p>

<p>From withing the data flow I would like to set a variable with this value, however I've not found a way to do that.</p>

<p>So how do I set a variable with a new value from within a 'data flow' ??</p>
","<azure-data-factory>","2020-05-06 14:18:30","2084","0","2","64721076","<p>Now we can set values in a dataflow without the need of using sink to load a file in Blob store and using extra activities to read it. Check <code>Cached lookup</code> documentation:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink#cache-sink"" rel=""nofollow noreferrer"">Cache Sink</a><br />
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-expression-builder#cached-lookup"" rel=""nofollow noreferrer"">Cached lookup</a></p>
<p>In the <code>Sink</code> we can select <code>Cache</code> instead of Dataset...</p>
<p><a href=""https://i.stack.imgur.com/LObo4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LObo4.png"" alt="""" /></a></p>
<p>Then we can use that value in any of the other activities:</p>
<p>like <code>mySinkName#outputs()[myRowNumber].myColumnName</code></p>
<p><a href=""https://i.stack.imgur.com/zcgnm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zcgnm.png"" alt="""" /></a></p>
"
"61637233","file checks before copying data to Azure data lake through Azure data factory","<p>Currently I am building a data pipeline where I wanted to copy data from one blob storage to Azure data lake through Azure data factory but before creating a data pipeline i wanted to have a file check kind of thing ie it should check the directory if file found or not ,for eg: i have a csv file if file is present then start copying to adls otherwise through an error filenot found. I know we can do this in python but in adf how to add that in pipeline. Any help will be appreciated.</p>
","<python><azure><pipeline><azure-data-factory><azure-data-lake>","2020-05-06 14:05:07","910","0","1","61648633","<p>I would use metadata activity to get a list of all items in your blob storage (select your blob as a dataset):
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>

<p>Then you might need to check if an item is a file, not a folder. For that you can add a combination of ""ForEach"" and ""If condition"" activities. In that case you can refer to each item from the Metadata step using <code>@activity('GetMetadata').output.childitems</code> expression and <code>@equals(item().type, 'File')</code> expression to check if it is a file. </p>
"
"61629516","Any cli support for Azure data factory's publish process to export ARM templates?","<p>We have setup ADF with Azure DevOps git integration. Our CD pipeline is triggered when there are new ARM templates pushed to adf_publish branch.</p>

<p>Current process on our pipeline is, once a PR is approved it is merged to collaboration branch (Master). Once this is done, we go to the portal and manually click on ADF's publish button to generate the ARM templates in adf_publish branch. 
I am unable to automate this end to end due to us having to manually Publish the changes.</p>

<p>Ideally I am looking at automating the publishing part. Once the PR is approved our pipeline should be able to execute the publishing process automatically. 
I have been unable to find a way to automate the publishing part. </p>

<p>So the question is, is there a way to execute ADF publishing process via AZ or PoweShell CLI or some other way?
Suggestions welcome.
Thanks</p>
","<azure><automation><azure-devops><azure-data-factory>","2020-05-06 07:21:45","1023","3","1","61649381","<blockquote>
  <p>Any cli support for Azure data factory's publish process to export ARM templates?</p>
</blockquote>

<p>There is a <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/53444"" rel=""nofollow noreferrer"">same issue</a> on the github about this issue, and Nowinski provided a brand new PowerShell open-source module to publish the whole Azure Data Factory code from <code>master</code> branch:</p>

<p><a href=""https://github.com/SQLPlayer/azure.datafactory.tools#publish-from-azure-devops"" rel=""nofollow noreferrer"">Publish from Azure DevOps</a></p>

<pre><code>variables:
  ResourceGroupName: 'rg-devops-factory'
  DataFactoryName: 'SQLPlayerDemo'
steps:
- powershell: |
   Install-Module Az.DataFactory -MinimumVersion ""1.7.0"" -Force
   Install-Module -Name ""azure.datafactory.tools"" -Force
   Import-Module -Name ""azure.datafactory.tools"" -Force
  displayName: 'PowerShell Script'
steps:
- task: AzurePowerShell@4
  displayName: 'Azure PowerShell script: InlineScript'
  inputs:
    azureSubscription: 'Subscription'
    ScriptType: InlineScript
    Inline: |
     Publish-AdfV2FromJson -RootFolder ""$(System.DefaultWorkingDirectory)/_ArtifactName_/"" -ResourceGroupName ""$(ResourceGroupName)"" -DataFactoryName ""$(DataFactoryName)"" -Location ""$(Location)"" -Stage ""$(Release.EnvironmentName)""

    FailOnStandardError: true
    azurePowerShellVersion: LatestVersion```
</code></pre>

<p>Besides, we could try to use code repository with ADF not by Exporting of ARM Template.</p>

<p>Please check this document <a href=""https://sqlplayer.net/2019/06/deployment-of-azure-data-factory-with-azure-devops/"" rel=""nofollow noreferrer"">Deployment of Azure Data Factory with Azure DevOps</a> for some more details.</p>

<p>Hope this helps.</p>
"
"61623462","Using mapping data flows within for-each activity of azure data-factory pipeline","<p><strong>Scenario</strong>: I have SQL DB CDC net changes coming into a csv file. In the pipeline for each row in the csv file need to check for updates, inserts and deletes and do some changes in my warehouse DB.</p>

<p><strong>Pipeline</strong>:</p>

<p><a href=""https://i.stack.imgur.com/tpaJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tpaJS.png"" alt=""pipeline""></a></p>

<p><strong>mapping dataflow:</strong></p>

<p><a href=""https://i.stack.imgur.com/mIKHZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mIKHZ.png"" alt=""dataflow""></a></p>

<p><strong>Problem</strong>: </p>

<p>How to use the item() from foreach in dataflow and fetch that record from csv file and process. I'm not seeing any field/option in source step of dataflow.</p>
","<azure-data-factory>","2020-05-05 21:33:11","1883","2","2","61624871","<p>No need to use ForEach here. The data flow can read each line from your CDC source file and you can apply the appropriate insert, update, merge, delete operation as policies in the Alter Row transformation.</p>
"
"61623462","Using mapping data flows within for-each activity of azure data-factory pipeline","<p><strong>Scenario</strong>: I have SQL DB CDC net changes coming into a csv file. In the pipeline for each row in the csv file need to check for updates, inserts and deletes and do some changes in my warehouse DB.</p>

<p><strong>Pipeline</strong>:</p>

<p><a href=""https://i.stack.imgur.com/tpaJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tpaJS.png"" alt=""pipeline""></a></p>

<p><strong>mapping dataflow:</strong></p>

<p><a href=""https://i.stack.imgur.com/mIKHZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mIKHZ.png"" alt=""dataflow""></a></p>

<p><strong>Problem</strong>: </p>

<p>How to use the item() from foreach in dataflow and fetch that record from csv file and process. I'm not seeing any field/option in source step of dataflow.</p>
","<azure-data-factory>","2020-05-05 21:33:11","1883","2","2","61625985","<p>Firstly, you could add a parameter in Data Flow:</p>

<p><a href=""https://i.stack.imgur.com/VOos9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VOos9.png"" alt=""enter image description here""></a></p>

<p>Then out of the Data flow, click the Data Flow, set the data flow parameter with Pipeline expression:</p>

<p><a href=""https://i.stack.imgur.com/UDH32.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UDH32.png"" alt=""enter image description here""></a></p>

<p>Then you could set the Foreach <code>item()</code> as the dataflow parameter:</p>

<p><a href=""https://i.stack.imgur.com/V8XIm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V8XIm.png"" alt=""enter image description here""></a></p>

<p>Now,you can use the item() from foreach in dataflow and fetch that record from csv file and process.</p>

<p>Hope this helps.</p>
"
"61621314","How can we access the column in dynamic expression of pre copy script in adf copy activity?","<p>How can we access the column in dynamic expression of pre copy script in adf copy activity
As I need to trim) substring the excess characters coming from source before loading into sql db.</p>

<p>Can anyone help me with expression.</p>

<p>Eg: suppose I have column called 'address' and it has data more than 50 characters .
Can this data be trimmed before loading in sink.</p>

<p>Since I don't want to make changes in target dB and would like to handle it before loading.</p>

<p>Thanks in advance.</p>
","<azure><substring><azure-data-factory><dynamic-expression>","2020-05-05 19:23:13","399","0","1","61623471","<p>Parul, I would utilize a dataflow: </p>

<p>1) Create a new mapping dataflow</p>

<p>2) Use your original table as a source.</p>

<p>3.1) If you really need the stripped address column in your output table, add a new ""Derived Column"" step and use ""left (address, 50)"" expression, add ""select"" step to get rid of the original address column.</p>

<p>3.2) If you don't need it at all, use ""select"" step and remove it.</p>

<p>4) Sink your table</p>

<p>5) In your pipeline, add ""Move &amp; Transform"" -> ""Data Flow"" activity and choose the dataflow you created</p>

<p>Hope that helps</p>
"
"61616812","Azure Data Factory - Copy Data - add values to output","<p>I have a 'Copy Data' activity withing Azure Data Factory that calls out to a REST endpoint and stores the data in a JSON file.</p>

<p>Within the JSON data that is retrieved there is some data about the total number of records and remaining records that I would like to add to the output of the 'Copy Data' activity.</p>

<p>How would I extract this information and add it to the output, so the next activity in the flow could make use of it?</p>
","<azure-data-factory>","2020-05-05 15:28:10","352","0","1","61616881","<p>The output of any activity at a given time is; </p>

<pre><code>@activity(“Activity Name”).output
</code></pre>

<p>so for example you could get the total number of rows copied like this;</p>

<pre><code>@activity(""Activity Name"").output.rowsCopied
</code></pre>

<p>you should be able to access everything from the output object;</p>

<ul>
<li>dataRead </li>
<li>dataWritten  </li>
<li>rowsRead</li>
<li>rowsCopied</li>
<li>copyDuration</li>
</ul>

<p>etc..</p>
"
"61606859","Azure Data Factory Pipeline Cost","<p>I'm using azure data factory for some migration project. while doing it I came a cross with some clarification. I just want to know if I keep a pipeline in ADF without using it will there be a cost for that? I need to run some pipelines according to a schedule like weekly or monthly. Please help</p>
","<azure><azure-pipelines><azure-data-factory><azure-sdk-.net><azure-scheduler>","2020-05-05 06:14:11","1127","1","2","61606999","<blockquote>
  <p>I just want to know if I keep a pipeline in ADF without using it will
  there be a cost for that?</p>
</blockquote>

<p>Quick answer is NO. Based on <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/"" rel=""nofollow noreferrer"">ADF pricing document</a>,the billing consists of <code>Data pipelines</code> and <code>SQL Server Integration Services</code>.</p>

<p>Your account only need to pay when you execute your activities(in the pipelines). Or it is related to migration of SQL Server DB.</p>

<p><a href=""https://i.stack.imgur.com/9GseY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9GseY.png"" alt=""enter image description here""></a></p>
"
"61606859","Azure Data Factory Pipeline Cost","<p>I'm using azure data factory for some migration project. while doing it I came a cross with some clarification. I just want to know if I keep a pipeline in ADF without using it will there be a cost for that? I need to run some pipelines according to a schedule like weekly or monthly. Please help</p>
","<azure><azure-pipelines><azure-data-factory><azure-sdk-.net><azure-scheduler>","2020-05-05 06:14:11","1127","1","2","61643472","<p>Yes: $0.80 / month / inactive pipeline. According to <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">Data Pipeline Pricing</a>, under the heading <em>Inactive pipelines</em>,</p>

<blockquote>
  <p>A pipeline is considered inactive if it has no associated trigger or any runs within the month. An inactive pipeline is charged at $0.80 per month.</p>
</blockquote>
"
"61606042","Azure Powershell Runbook converting excel file in blob storage to csv","<p>I am trying to call an Azure Powershell Runbook as a webhook from Data Factory. This PowerShell converts excel(.xlsx) to CSV which resides in blob storage. The script currently only changes file format from .xlsx to .csv and hence is not properly converted to CSV. When previewed in the data factory, this CSV file is encoded. What is the proper way of converting xlsx to csv Using Azure PowerShell runbook or python. I want to automate this in Azure. This is what it is currently doing:</p>

<pre><code>`#Get the files to convert from excel container
$rawFile = Get-AzureStorageBlob -Container $rawFilesContainer -Context $storageAccount -Blob $fileName
$rawFileName = $rawFile.Name
$rawFileWithoutExtension = $rawFile.Name.Substring(0, $rawFile.Name.IndexOf('.')) 
$csvFile = $rawFileWithoutExtension + '.csv'

#Convert and save in csv container
Start-AzureStorageBlobCopy -SrcBlob $rawFileName -SrcContainer $rawFilesContainer -DestBlob $csvFile -DestContainer $outputContainer -Context $storageAccount -DestContext $storageAccount -Force`
</code></pre>

<p>Thanks</p>
","<azure-powershell><azure-data-factory><azure-blob-storage><azure-automation><azure-runbook>","2020-05-05 04:55:40","896","0","1","61621883","<p>Try the following route</p>

<p>1.Store your Power-Shell script .ps on a blob container.
2.Create a C# console app to execute the power-shell script stored in the blob.
3.In ADFv2 create a pipeline,configure a custom activity to run the console app you created.
4.Schedule it using a trigger.</p>

<p>You will need a storage account,batch account,batch windows vm pool inside the batch account.</p>
"
"61603864","How to unzip and move files in Azure?","<p>Problem: I get an email with a zip file. In that zip are two files. I want to extract one of those files and place it in a folder in ADL. </p>

<p>I've automated this before using logic apps but the zip and extra file is throwing a wrench in the gears here. So far I've managed to get a logic app going to download the zip into a blob container and another logic app to extract the files to another container. Don't know how to proceed from there. Should I be using data factory? I want this automated and to run every week every time I receive an email from a specific sender. </p>
","<azure><azure-pipelines><azure-data-factory><azure-logic-apps>","2020-05-05 00:30:32","596","1","1","61604622","<p><strong>Update:</strong></p>

<p>I am sorry, dont notice your source is ADL, the below steps only need to change the source as ADL is ok. the key is select the Compression type of your source, it will unzip the file for you.</p>

<p><strong>Original Answer:</strong></p>

<ol>
<li>Create a pipeline, </li>
</ol>

<p><a href=""https://i.stack.imgur.com/2vap8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2vap8.png"" alt=""enter image description here""></a></p>

<p>2.Create a activity.</p>

<p><a href=""https://i.stack.imgur.com/KGtXe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KGtXe.png"" alt=""enter image description here""></a></p>

<p>3.After you create a copy data activity, you need to choose the source and the sink. From your description, you need to unzip a file in a storage container to another container. So, please follow these steps:</p>

<p><a href=""https://i.stack.imgur.com/AiiSi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AiiSi.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/y3Gek.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y3Gek.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/7UiKZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7UiKZ.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/07dEw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/07dEw.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/rZTvO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rZTvO.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Rp22a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rp22a.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/JQ4GT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JQ4GT.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/s4K6p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s4K6p.png"" alt=""enter image description here""></a></p>

<p>And the sink is similar, also choose the azure storage blob and choose the same linked service. Select the container that you want to copy to.</p>

<p>4.Then let's Validate all. If there is no problem, we can publish them.</p>

<p><a href=""https://i.stack.imgur.com/S5AV6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S5AV6.png"" alt=""enter image description here""></a></p>

<ol start=""5"">
<li>Now please trigger your pipeline:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/jTMEl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jTMEl.png"" alt=""enter image description here""></a></p>

<p>6.After that, your zip file will successful unzip and copy to another container.:)</p>
"
"61593352","Azure data Factory escape character and quote issue - copy activity","<p>I have ADF pipelines exporting (via copy activity) data from Azure SQL DB to Data Lake (ADLS2) and then from there to another Azure SQL DB. It was working fine until some characters appeared.</p>
<p>This is how the culprit record looks in the first Azure SQL DB:
&quot;Gasunie\</p>
<p><a href=""https://i.stack.imgur.com/7fiNl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7fiNl.png"" alt=""enter image description here"" /></a></p>
<p>This is how the dataset is set up in ADF to export it into ADLS:
Column delimiter - pipe
Row delimiter - autodetect
Encoding - Default(UTF-8)
Escape character - Backslash()
Quote character - Double quote (&quot;)</p>
<p><a href=""https://i.stack.imgur.com/RUicp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RUicp.png"" alt=""enter image description here"" /></a></p>
<p>This is how the exported file looks like in notepad++ (it's a pipe-delimited file):
&quot;&quot;Gasunie&quot;</p>
<p><a href=""https://i.stack.imgur.com/hRuC8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hRuC8.png"" alt=""enter image description here"" /></a></p>
<p>These are the settings for the adls dataset in ADF when loading it from adls to azure SQL DB:
Column delimiter - comma
Row delimiter - autodetect
Encoding - Default(UTF-8)
Escape character - Backslash()
Quote character - Double quote (&quot;)</p>
<p>Note it's comma-delimited now but that is not causing any problems.</p>
<p><a href=""https://i.stack.imgur.com/gWOY1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gWOY1.png"" alt=""enter image description here"" /></a></p>
<p>But this is how it looks once loaded:
&quot;Gasunie&quot;|1|||||||||||...
The backslash that was originally there has somehow caused it to stop the delimiting for the next few columns.</p>
<p><a href=""https://i.stack.imgur.com/MRsEf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MRsEf.png"" alt=""enter image description here"" /></a></p>
<p>I have tried many, many various different settings for the quotes and escape characters but they create more problems for other data in the dataset.</p>
<p>Does anyone know how I can correct it without having to ask for the source to be corrected?</p>
<p>Note: there is a reason why it's stored in adls so it can't be a copy from Azure sql DB to another Azure Sql DB.</p>
<p>It's built from a template by consultants, and highly parameterized so inserting dataflows to process the files in adls would be a very lengthy process.</p>
<p>Any help is appreciated. Thank you.</p>
","<csv><escaping><quotes><azure-data-factory>","2020-05-04 13:26:58","17302","3","2","61594872","<p>Just a suggestion, I came up with a situation when copying data from azuresql to datalake,
carriage return and newline characters were splitting saved csv file on adls. I replaced them in the query using following code and it worked. 
<code>replace (REPLACE(Description,CHAR(10),''),char(13),'')</code> Description</p>

<p>You may try the below</p>

<p><code>REPLACE([yourcolumn],char(34),' ')</code></p>

<p><img src=""https://i.stack.imgur.com/H0DS8.png"" alt=""image1"">
<img src=""https://i.stack.imgur.com/jX9WZ.jpg"" alt=""image2""></p>
"
"61593352","Azure data Factory escape character and quote issue - copy activity","<p>I have ADF pipelines exporting (via copy activity) data from Azure SQL DB to Data Lake (ADLS2) and then from there to another Azure SQL DB. It was working fine until some characters appeared.</p>
<p>This is how the culprit record looks in the first Azure SQL DB:
&quot;Gasunie\</p>
<p><a href=""https://i.stack.imgur.com/7fiNl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7fiNl.png"" alt=""enter image description here"" /></a></p>
<p>This is how the dataset is set up in ADF to export it into ADLS:
Column delimiter - pipe
Row delimiter - autodetect
Encoding - Default(UTF-8)
Escape character - Backslash()
Quote character - Double quote (&quot;)</p>
<p><a href=""https://i.stack.imgur.com/RUicp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RUicp.png"" alt=""enter image description here"" /></a></p>
<p>This is how the exported file looks like in notepad++ (it's a pipe-delimited file):
&quot;&quot;Gasunie&quot;</p>
<p><a href=""https://i.stack.imgur.com/hRuC8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hRuC8.png"" alt=""enter image description here"" /></a></p>
<p>These are the settings for the adls dataset in ADF when loading it from adls to azure SQL DB:
Column delimiter - comma
Row delimiter - autodetect
Encoding - Default(UTF-8)
Escape character - Backslash()
Quote character - Double quote (&quot;)</p>
<p>Note it's comma-delimited now but that is not causing any problems.</p>
<p><a href=""https://i.stack.imgur.com/gWOY1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gWOY1.png"" alt=""enter image description here"" /></a></p>
<p>But this is how it looks once loaded:
&quot;Gasunie&quot;|1|||||||||||...
The backslash that was originally there has somehow caused it to stop the delimiting for the next few columns.</p>
<p><a href=""https://i.stack.imgur.com/MRsEf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MRsEf.png"" alt=""enter image description here"" /></a></p>
<p>I have tried many, many various different settings for the quotes and escape characters but they create more problems for other data in the dataset.</p>
<p>Does anyone know how I can correct it without having to ask for the source to be corrected?</p>
<p>Note: there is a reason why it's stored in adls so it can't be a copy from Azure sql DB to another Azure Sql DB.</p>
<p>It's built from a template by consultants, and highly parameterized so inserting dataflows to process the files in adls would be a very lengthy process.</p>
<p>Any help is appreciated. Thank you.</p>
","<csv><escaping><quotes><azure-data-factory>","2020-05-04 13:26:58","17302","3","2","62076417","<p>I faced a similar problem.</p>

<p>What I think happens for you is this.</p>

<ul>
<li>The data is 9 characters, like so ""Gasunie\</li>
<li>The output is written ""quoted"" and uses \ as the escape character.</li>
<li>So the output will be ""your_text"", but any quotes in your_text are replaced with \"" </li>
<li>So the output is ""\""Gasunie\"" - the outside quotes enclose your text and the
inside one has been escaped with \</li>
</ul>

<p>Now we come to read this back in: it seems to be parsed like this.</p>

<ul>
<li>The first quote is the start of your quoted field value, so from here on I'm reading your text field value.</li>
<li>Then I see \"" which is a quote character (that has been escaped).</li>
<li>Then I see Gasunie</li>
<li>Then I see \"" which is a quote character (that has been escaped).</li>
<li>Then I see field delimiters but as I'm still thinking I'm inside a quoted field then they are just text, so are included in my output ""Gasunie""|1|||||||||||...</li>
<li>I keep reading characters into this field until I reach the next double quote at which point I'm expecting a new delimiter to start the next field.</li>
</ul>

<p>So the problem is that ADF is putting quotes round whatever string it has in hand and writing this to the output; while on input it's parsing left to right so any string ending in the escape character is a problem.  I'm not sure if you'd call that a bug.</p>

<p><strong>What can you do?</strong></p>

<p>In your case, just change the escape character to something that's never seen in your input (maybe @ or { or something).  Then the \"" at the end of your output text is no longer an escaped quote.</p>

<p><strong>My similar case</strong> - when is the escape character not an escape character?</p>

<p>I have a field that contains a comma, but comma is also the field separator.  My data comes from a third party and they have handily escaped this comma for me, using a backslash, thus:</p>

<p><code>Field One, Field\,Two, Field Three</code></p>

<p>I have \ as my escape character so you'd think this will give me three output fields: </p>

<p><code>| Field One | Field,Two | Field Three |</code></p>

<p>Wrong.  The escape character only works when it's inside the quoted field.  My input is not quoted, so the backslash is just treated as text and the comma is a field separator meaning my output has four fields</p>

<p><code>| Field One | Field\ | Two | Field Three |</code></p>

<p>Solution:  tell my ADF dataset there are no quote characters around my input - then it treats anything after a comma as the text field and applies the escape character as expected.</p>

<p><a href=""https://i.stack.imgur.com/Yy9WC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yy9WC.png"" alt=""enter image description here""></a></p>

<p>You might also be interested in this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/35482144-text-format-escape-char-only-if-needed-or-per-fiel"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/35482144-text-format-escape-char-only-if-needed-or-per-fiel</a>.</p>

<p>So if you're looking for ""escaped comma in csv is creating extra field"", I hope this saves you a bit of time!</p>
"
"61582237","Copy Data in Azure Data Factory is slow","<p>I am using Azure Data Factory. I have a <code>Copy Data</code> task that takes 7 seconds for a file with 17 kb. I wold like to copy from one folder to on subfolder on the same folder. I have my files in my Azure DL v2.</p>

<p>This is my <code>Copy File</code></p>

<p><a href=""https://i.stack.imgur.com/xDNhM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xDNhM.png"" alt=""enter image description here""></a> </p>

<p><a href=""https://i.stack.imgur.com/rusuh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rusuh.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/S2wDg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2wDg.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/MFe2c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MFe2c.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/l2h3C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l2h3C.png"" alt=""enter image description here""></a></p>

<p>this is execution log</p>

<p><a href=""https://i.stack.imgur.com/0idQK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0idQK.png"" alt=""enter image description here""></a></p>

<p>As you can see, 7 seconds to copy a csv file with 217 rows and less than 20kb. I can do the same things faster in SSIS. Why ADF is too slow?</p>
","<azure><azure-data-factory>","2020-05-03 22:03:21","825","0","1","61769187","<p>I understand concerning the performance of ADF execution is a very common issue. If you have any channels to contact Azure Support ADF team(the help feature on the portal),that will be the fastest,detailed and the most official way.</p>

<p>Here on the forum, i only could provide some official suggestions with you which is mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-troubleshooting"" rel=""nofollow noreferrer"">document</a>.Since you said you are using Auto IR, which is Azure IR actually.</p>

<p>1.Follow the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-troubleshooting#performance-tuning-tips"" rel=""nofollow noreferrer"">performance tuning tips</a> to try another region for testing.</p>

<p><a href=""https://i.stack.imgur.com/gUGIw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gUGIw.png"" alt=""enter image description here""></a></p>

<p>2.Follow the suggestions of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-troubleshooting#troubleshoot-copy-activity-on-azure-ir"" rel=""nofollow noreferrer"">troubleshooting copy activity on Azure IR</a>.</p>
"
"61564943","Stop running Azure Data Factory Pipeline when it is still running","<p>I have a <code>Azure Data Factory Pipeline</code>. My trigger has been set for every each 5 minutes. 
Sometimes my Pipeline takes more than 5 mins to finished its jobs. In this case, Trigger runs again and creates another instance of my Pipeline and two instances of the same pipeline make problem in my ETL.
How can I be sure than just one instance of my pipeline runs at time? </p>

<p><a href=""https://i.stack.imgur.com/mE4cQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mE4cQ.png"" alt=""enter image description here""></a> </p>

<p>As you can see there are several instances running of my pipelines</p>

<p><a href=""https://i.stack.imgur.com/rC3Zu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rC3Zu.png"" alt=""enter image description here""></a></p>
","<triggers><azure-data-factory>","2020-05-02 19:11:56","9638","3","5","61569906","<p>Few options I could think of:</p>

<p><strong>OPT 1</strong></p>

<p>Specify 5 min timeout on your pipeline activities:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities</a>
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-policy"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-policy</a></p>

<p><strong>OPT 2</strong></p>

<p>1) Create a 1 row 1 column sql RunStatus table: 1 will be our ""completed"", 0 - ""running"" status</p>

<p>2) At the end of your pipeline add a stored procedure activity that would set the bit to 1.</p>

<p>3) At the start of your pipeline add a lookup activity to read that bit. </p>

<p>4) The output of this lookup will then be used in if condition activity:</p>

<ul>
<li>if 1 - start the pipeline's job, but before that add another stored procedure activity to set our status bit to 0.</li>
<li>if 0 - depending on the details of your project: do nothing, add a wait activity, send an email, etc. </li>
</ul>

<p>To make a full use of this option, you can turn the table into a log, where the new line with start and end time will be added after each successful run (before initiating a new run, you can check if the previous run had the end time). Having this log might help you gather data on how much does it take to run your pipeline and perhaps either add more resources or increase the interval between the runs. </p>

<p><strong>OPT 3</strong></p>

<p>Monitor the pipeline run with SDKs (have not tried that, so this is just to possibly direct you):
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically</a></p>

<p>Hopefully you can use at least one of them</p>
"
"61564943","Stop running Azure Data Factory Pipeline when it is still running","<p>I have a <code>Azure Data Factory Pipeline</code>. My trigger has been set for every each 5 minutes. 
Sometimes my Pipeline takes more than 5 mins to finished its jobs. In this case, Trigger runs again and creates another instance of my Pipeline and two instances of the same pipeline make problem in my ETL.
How can I be sure than just one instance of my pipeline runs at time? </p>

<p><a href=""https://i.stack.imgur.com/mE4cQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mE4cQ.png"" alt=""enter image description here""></a> </p>

<p>As you can see there are several instances running of my pipelines</p>

<p><a href=""https://i.stack.imgur.com/rC3Zu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rC3Zu.png"" alt=""enter image description here""></a></p>
","<triggers><azure-data-factory>","2020-05-02 19:11:56","9638","3","5","61597689","<p>My first thought is that the recurrence is too frequent under these circumstances. If the graph you shared is all for the same pipeline, then most of them take close to 5 minutes, but you have some that take 30, 40, even 60 minutes. Situations like this are when a simple recurrence trigger probably isn't sufficient. What is supposed to happen while the 60 minute one is running? There will be 10-12 runs that wouldn't start: so they still need to run or can they be ignored?</p>

<p>To make sure all the pipelines run, and manage concurrency, you're going to need to build a queue manager of some kind. ADF cannot handle this itself, so I have built such a system internally and rely on it extensively. I use a combination of Logic Apps, Stored Procedures (Azure SQL), and Azure Functions to queue, execute, and monitor pipeline executions. Here is a high level break down of what you probably need:</p>

<ol>
<li><p>Logic App 1: runs every 5 minutes and queues an ADF job in the SQL database.</p></li>
<li><p>Logic App 2: runs every 2-3 minutes and checks the queue to see if a) there is not a job currently running (status = 'InProgress') and 2) there is a job in the queue waiting to run (I do this with a Stored Procedure). IF this state is met: execute the next ADF and update its status to 'InProgress'.</p>

<ul>
<li>I use an Azure Function to submit jobs instead of the built in Logic App activity because I have better control over variable parameters. Also, they can return the newly created ADF RunId, which I rely in #3.</li>
</ul></li>
<li><p>Logic App 3: runs every minute and updates the status of any 'InProgress' jobs.</p>

<ul>
<li>I use an Azure Function to check the status of the ADF pipeline based on RunId.</li>
</ul></li>
</ol>
"
"61564943","Stop running Azure Data Factory Pipeline when it is still running","<p>I have a <code>Azure Data Factory Pipeline</code>. My trigger has been set for every each 5 minutes. 
Sometimes my Pipeline takes more than 5 mins to finished its jobs. In this case, Trigger runs again and creates another instance of my Pipeline and two instances of the same pipeline make problem in my ETL.
How can I be sure than just one instance of my pipeline runs at time? </p>

<p><a href=""https://i.stack.imgur.com/mE4cQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mE4cQ.png"" alt=""enter image description here""></a> </p>

<p>As you can see there are several instances running of my pipelines</p>

<p><a href=""https://i.stack.imgur.com/rC3Zu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rC3Zu.png"" alt=""enter image description here""></a></p>
","<triggers><azure-data-factory>","2020-05-02 19:11:56","9638","3","5","61621642","<p>Try changing the concurrency of the pipeline to 1.</p>

<p>Link: <a href=""https://www.datastackpros.com/2020/05/prevent-azure-data-factory-from-running.html"" rel=""nofollow noreferrer"">https://www.datastackpros.com/2020/05/prevent-azure-data-factory-from-running.html</a></p>
"
"61564943","Stop running Azure Data Factory Pipeline when it is still running","<p>I have a <code>Azure Data Factory Pipeline</code>. My trigger has been set for every each 5 minutes. 
Sometimes my Pipeline takes more than 5 mins to finished its jobs. In this case, Trigger runs again and creates another instance of my Pipeline and two instances of the same pipeline make problem in my ETL.
How can I be sure than just one instance of my pipeline runs at time? </p>

<p><a href=""https://i.stack.imgur.com/mE4cQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mE4cQ.png"" alt=""enter image description here""></a> </p>

<p>As you can see there are several instances running of my pipelines</p>

<p><a href=""https://i.stack.imgur.com/rC3Zu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rC3Zu.png"" alt=""enter image description here""></a></p>
","<triggers><azure-data-factory>","2020-05-02 19:11:56","9638","3","5","61643735","<p>It sounds like you're trying to run a process more or less constantly, which is a good fit for <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#tumbling-window-trigger"" rel=""noreferrer"">tumbling window triggers</a>. You can create a dependency such that the trigger is dependent on itself - so it won't run until the previous run has completed.</p>

<p>Start by <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""noreferrer"">creating a trigger that runs a pipeline on a tumbling window</a>, then <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tumbling-window-trigger-dependency"" rel=""noreferrer"">create a tumbling window trigger dependency</a>. The section at the bottom of that article discusses <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tumbling-window-trigger-dependency#tumbling-window-self-dependency-properties"" rel=""noreferrer"">""tumbling window self-dependency properties""</a>, which shows you what the code should look like once you've successfully set this up.</p>
"
"61564943","Stop running Azure Data Factory Pipeline when it is still running","<p>I have a <code>Azure Data Factory Pipeline</code>. My trigger has been set for every each 5 minutes. 
Sometimes my Pipeline takes more than 5 mins to finished its jobs. In this case, Trigger runs again and creates another instance of my Pipeline and two instances of the same pipeline make problem in my ETL.
How can I be sure than just one instance of my pipeline runs at time? </p>

<p><a href=""https://i.stack.imgur.com/mE4cQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mE4cQ.png"" alt=""enter image description here""></a> </p>

<p>As you can see there are several instances running of my pipelines</p>

<p><a href=""https://i.stack.imgur.com/rC3Zu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rC3Zu.png"" alt=""enter image description here""></a></p>
","<triggers><azure-data-factory>","2020-05-02 19:11:56","9638","3","5","75936114","<p>Go to your pipeline,</p>
<p>Go to Settings -&gt; Concurrency -&gt; Set to 1</p>
<p>This will set the number of runs of that pipeline to 1.Only 1 instance of the pipeline will run at a time. All the other pipeline runs will be queued.</p>
"
"61554239","Azure Data Factory - Expression to Source Dataset Column","<p>I have an simple Azure Data Factory project aiming to copy data from an external service (Service Now) to an Azure Table Storage.</p>

<p>To keep things simple, consider source dataset only as an id and a creation Date:</p>

<pre><code>ID, CreationDate
1 , 2020-05-02T10:00:00
2 , 2020-05-02T11:00:00
</code></pre>

<p>I want to copy it to the Azure Table with the same structure/columns, but I want to extract date from datetime column to use as Partition Key, and use the ID as Row Key (if possible, still maintaining the original ID column).</p>

<p>I think I need to use some expression to get the column values mapped to Partition/RowKey, but I didn´t found any expression that help me.</p>

<pre><code>@formatDateTime(????source.CreationDate????, 'yyyy-MM-dd')
</code></pre>

<p>Thanks in advance for any help with the correct expression for this scenario.</p>

<p>Regards,</p>
","<azure-data-factory><azure-table-storage>","2020-05-02 03:50:56","240","1","1","61608284","<p>Based on test, the source column can't be referred in the dynamic content in Copy Activity.</p>

<p>You could try to add a column in the source dataset which extracts date from <code>CreationDate</code> column like this:</p>

<pre><code>ID, CreationDate,ShortDate
1 , 2020-05-02T10:00:00,2020-05-02
2 , 2020-05-02T11:00:00,2020-05-02
</code></pre>

<p>then use <code>ShortDate</code> as Partition Key.</p>
"
"61538963","Filter JSON data using Azure Data Factory","<p>I currently have some log files from a chatbot built on Microsoft's Bot Framework that I'm trying to process using Azure Data Factory. I'm running a daily process to create a daily JSON file of the logs and now I want to build on this to create a process to create another JSON file that only includes some of the logs. For example, I just want to include the logs that are of type ""trace"" and ""message"". I've tried looking at how I can use features such as lookup and filter but just can't work out how to accomplish what I'm aiming for.</p>

<p>Ideally I want to create a process where I isolate the ""message"" logs that include the phrase ""learning list"" and capture the ""trace"" log that appears before it so I can capture the message that was sent to the chatbot that resulted in the ""learning list"" response.</p>

<p>I've included a sample of the logs below:</p>

<pre><code>{""type"":""message"",""timestamp"":""2020-04-29T14:02:28.808Z"",""text"":""teams""}
{""type"":""trace"",""timestamp"":""2020-04-29T14:02:32.345Z"",""value"":{""message"":{""text"":""teams"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:02:28.808Z"",""localTimestamp"":""2020-04-29T14:02:28.808Z"",""id"":""1588168948793"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[{""questions"":[""How do I use Teams""],""answer"":""[Here's](https://xxx.pdf) our guide to Teams."",""score"":0.805,""id"":13,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}},{""questions"":[""I'm looking for the People Team Handbook""],""answer"":""[Here](https://xxx.pdf) is the People Team Handbook."",""score"":0.39189999999999997,""id"":3,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T14:00:10.157Z"",""text"":""[Here](https://xxx.pdf) is the OMG UK New Starter's Handbook.""}
{""type"":""delay"",""timestamp"":""2020-04-29T14:02:31.799Z"",""value"":2000}
{""type"":""trace"",""timestamp"":""2020-04-29T14:00:09.895Z"",""value"":{""message"":{""text"":""new starters"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:00:06.656Z"",""localTimestamp"":""2020-04-29T14:00:06.656Z"",""id"":""1588168806641"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""omgsearchbot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[{""questions"":[""Where is the OMG UK New Starter's Handbook""],""answer"":""[Here](https://xxx.pdf) is the OMG UK New Starter's Handbook."",""score"":0.7702,""id"":38,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""typing"",""timestamp"":""2020-04-29T14:02:31.799Z""}
{""type"":""message"",""timestamp"":""2020-04-29T14:02:32.539Z"",""text"":""[Here's](https://xxx.pdf) our guide to Teams.""}
{""type"":""typing"",""timestamp"":""2020-04-29T14:04:05.874Z""}
{""type"":""message"",""timestamp"":""2020-04-29T14:04:03.177Z"",""text"":""help""}
{""type"":""message"",""timestamp"":""2020-04-29T14:04:06.410Z"",""text"":""You can ask me to find lots of different types of documents for you, such as:\n\n - Info on Bankside \n\n - Specialist Service summaries\n\n - IT guides \n\n - Lots of different People Guides \n\nJust type in what you're looking for and I'll try and find it for you. \n\n🤓""}
{""type"":""trace"",""timestamp"":""2020-04-29T14:04:06.158Z"",""value"":{""message"":{""text"":""help"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:04:03.177Z"",""localTimestamp"":""2020-04-29T14:04:03.177Z"",""id"":""1588169043168"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[{""questions"":[""Help"",""What can you do""],""answer"":""You can ask me to find lots of different types of documents for you, such as:\n\n - Info on Bankside \n\n - Specialist Service summaries\n\n - IT guides \n\n - Lots of different People Guides \n\nJust type in what you're looking for and I'll try and find it for you. \n\n🤓"",""score"":1,""id"":30,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T14:04:56.946Z"",""text"":""widget""}
{""type"":""delay"",""timestamp"":""2020-04-29T14:04:05.874Z"",""value"":2000}
{""type"":""typing"",""timestamp"":""2020-04-29T14:04:59.425Z""}
{""type"":""delay"",""timestamp"":""2020-04-29T14:04:59.425Z"",""value"":2000}
{""type"":""trace"",""timestamp"":""2020-04-29T14:04:59.762Z"",""value"":{""message"":{""text"":""widget"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:04:56.946Z"",""localTimestamp"":""2020-04-29T14:04:56.946Z"",""id"":""1588169096931"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T14:04:59.966Z"",""text"":""I'm sorry, I don't have an answer to that question yet, but I'll add it to my learning list!""}
{""type"":""message"",""timestamp"":""2020-04-29T14:08:36.024Z"",""text"":""widget""}
{""type"":""typing"",""timestamp"":""2020-04-29T14:08:38.518Z""}
{""type"":""trace"",""timestamp"":""2020-04-29T14:08:38.775Z"",""value"":{""message"":{""text"":""widget"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:08:36.024Z"",""localTimestamp"":""2020-04-29T14:08:36.024Z"",""id"":""1588169316004"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""delay"",""timestamp"":""2020-04-29T14:08:38.518Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T14:08:39.000Z"",""text"":""I'm sorry, I don't have an answer to that question yet, but I'll add it to my learning list!""}
{""type"":""delay"",""timestamp"":""2020-04-29T14:08:49.250Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T14:08:46.809Z"",""text"":""what is teams?""}
{""type"":""typing"",""timestamp"":""2020-04-29T14:08:49.250Z""}
{""type"":""message"",""timestamp"":""2020-04-29T14:08:49.743Z"",""text"":""[Here's](https://xxx.pdf) our guide to Teams.""}
{""type"":""trace"",""timestamp"":""2020-04-29T14:08:49.515Z"",""value"":{""message"":{""text"":""what is teams?"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:08:46.809Z"",""localTimestamp"":""2020-04-29T14:08:46.809Z"",""id"":""1588169326794"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[{""questions"":[""How do I use Teams""],""answer"":""[Here's](xxx.pdf) our guide to Teams."",""score"":0.7812,""id"":13,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}},{""questions"":[""Where is the season ticket loan form""],""answer"":""The season ticket loan form is [here](https://xxx.docx)."",""score"":0.311,""id"":33,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""delay"",""timestamp"":""2020-04-29T14:08:59.621Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T14:08:57.323Z"",""text"":""what is sharepoint?""}
{""type"":""typing"",""timestamp"":""2020-04-29T14:08:59.621Z""}
{""type"":""message"",""timestamp"":""2020-04-29T14:09:00.082Z"",""text"":""Try this [guide](https://xxx.pdf).""}
{""type"":""message"",""timestamp"":""2020-04-29T14:11:55.926Z"",""text"":""travel loan""}
{""type"":""trace"",""timestamp"":""2020-04-29T14:08:59.892Z"",""value"":{""message"":{""text"":""what is sharepoint?"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:08:57.323Z"",""localTimestamp"":""2020-04-29T14:08:57.323Z"",""id"":""1588169337303"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[{""questions"":[""How do I use SharePoint""],""answer"":""Try this [guide](https://xxx.pdf)."",""score"":0.95,""id"":14,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}},{""questions"":[""What are the OMG benefits""],""answer"":""You can find details of all our benefits [here](https://xxx.pdf)."",""score"":0.4697,""id"":4,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""typing"",""timestamp"":""2020-04-29T14:11:58.563Z""}
{""type"":""delay"",""timestamp"":""2020-04-29T14:11:58.563Z"",""value"":2000}
{""type"":""trace"",""timestamp"":""2020-04-29T14:11:58.844Z"",""value"":{""message"":{""text"":""travel loan"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:11:55.926Z"",""localTimestamp"":""2020-04-29T14:11:55.926Z"",""id"":""1588169515916"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[{""questions"":[""Where is the personal loan form""],""answer"":""The personal loan form is [here](https://xxx.docx)."",""score"":0.6198,""id"":32,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}},{""questions"":[""Where is the season ticket loan form""],""answer"":""The season ticket loan form is [here](https://xxx.docx)"",""score"":0.5587,""id"":33,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T14:12:44.255Z"",""text"":""travel loan""}
{""type"":""message"",""timestamp"":""2020-04-29T14:11:59.118Z"",""text"":""The personal loan form is [here](https://xxx.docx)""}
{""type"":""delay"",""timestamp"":""2020-04-29T14:12:46.693Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T14:12:47.196Z"",""text"":""The season ticket loan form is [here](xxx.docx)""}
{""type"":""trace"",""timestamp"":""2020-04-29T14:12:46.947Z"",""value"":{""message"":{""text"":""travel loan"",""textFormat"":""plain"",""type"":""message"",""timestamp"":""2020-04-29T14:12:44.255Z"",""localTimestamp"":""2020-04-29T14:12:44.255Z"",""id"":""1588169564239"",""channelId"":""msteams"",""serviceUrl"":""https://smba.trafficmanager.net/emea/"",""from"":{""id"":""29:1Tgnjrvpvsg9voZ7PGpR4lzSB3VOLeUa7Hnl9oxOEEflevU6O346pAD_GGwagLA6BnH1Ir8kjIse5FSOjznhFnw"",""name"":""Sean Betts"",""aadObjectId"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""role"":""user""},""conversation"":{""conversationType"":""personal"",""tenantId"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37"",""id"":""a:1sfqZBXIxkjxMv9nMwQsWT_Vl9lTo8OTipwPdmOKFiGEgKDTqvu0rA1o4TKcc71tkiMHVmgg3YZgKhnhzU9kXqvWkDEP2KZXACcxd8YViOz6uthdEPCJrbsWKcDN9xJRE""},""recipient"":{""id"":""28:dfa7e609-37b7-442f-b0b7-f11d5bbc73d8"",""name"":""SearchBot""},""entities"":[{""locale"":""en-GB"",""country"":""GB"",""platform"":""Mac"",""type"":""clientInfo""}],""channelData"":{""tenant"":{""id"":""f3e0ea3d-dd96-4574-83b5-7f35c71cde37""}},""locale"":""en-GB""},""queryResults"":[{""questions"":[""Where is the season ticket loan form"",""Where is the travel loan form""],""answer"":""The season ticket loan form is [here](https://xxx.docx?d=w422ff4c734754bfe9f0a03e602b67989&amp;csf=1&amp;web=1&amp;e=DgdTBd)."",""score"":0.95,""id"":33,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}},{""questions"":[""Where is the personal loan form""],""answer"":""The personal loan form is [here](https://xxx.docx?d=wda346b28670040518308b74d36f672f0&amp;amp;csf=1&amp;amp;web=1&amp;amp;e=aYHsmf)."",""score"":0.5345,""id"":32,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""typing"",""timestamp"":""2020-04-29T14:12:46.693Z""}
{""type"":""trace"",""timestamp"":""2020-04-29T08:34:51.019Z"",""value"":{""message"":{""type"":""message"",""id"":""4RbdyZGhJblDsnWwZjZUfB-a|0000000"",""timestamp"":""2020-04-29T08:34:48.092Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""4RbdyZGhJblDsnWwZjZUfB-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""hi"",""entities"":[{""type"":""ClientCapabilities"",""requiresBotState"":true,""supportsListening"":true,""supportsTts"":true}],""channelData"":{""clientActivityID"":""1588149288082pbqyejo0nqb"",""clientTimestamp"":""2020-04-29T08:34:48.082Z""}},""queryResults"":[],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T08:34:48.537Z"",""text"":""Welcome to BhavBot! Ask me a question and I will try to answer it.""}
{""type"":""typing"",""timestamp"":""2020-04-29T08:34:57.525Z""}
{""type"":""conversationUpdate"",""timestamp"":""2020-04-29T08:34:37.687Z""}
{""type"":""delay"",""timestamp"":""2020-04-29T08:34:50.615Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T08:34:51.084Z"",""text"":""I'm sorry, I don't have an answer to that question yet, but I'll add it to my learning list!""}
{""type"":""message"",""timestamp"":""2020-04-29T08:34:48.092Z"",""text"":""hi""}
{""type"":""message"",""timestamp"":""2020-04-29T08:34:55.394Z"",""text"":""Teams""}
{""type"":""message"",""timestamp"":""2020-04-29T08:35:04.330Z"",""text"":""Try this guide - https://xxx.pdf""}
{""type"":""typing"",""timestamp"":""2020-04-29T08:35:04.073Z""}
{""type"":""message"",""timestamp"":""2020-04-29T08:34:58.049Z"",""text"":""Here's our guide to Teams - https://xxx.pdf""}
{""type"":""message"",""timestamp"":""2020-04-29T08:35:01.938Z"",""text"":""Sharepoint""}
{""type"":""delay"",""timestamp"":""2020-04-29T08:35:04.073Z"",""value"":2000}
{""type"":""conversationUpdate"",""timestamp"":""2020-04-29T09:19:43.903Z""}
{""type"":""delay"",""timestamp"":""2020-04-29T08:34:57.525Z"",""value"":2000}
{""type"":""trace"",""timestamp"":""2020-04-29T08:34:57.955Z"",""value"":{""message"":{""type"":""message"",""id"":""4RbdyZGhJblDsnWwZjZUfB-a|0000003"",""timestamp"":""2020-04-29T08:34:55.394Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""4RbdyZGhJblDsnWwZjZUfB-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""Teams"",""channelData"":{""clientActivityID"":""1588149295371d9xy4sw90yp"",""clientTimestamp"":""2020-04-29T08:34:55.371Z""}},""queryResults"":[{""questions"":[""How do I use Teams""],""answer"":""Here's our guide to Teams - https://xxx.pdf"",""score"":0.81349999999999989,""id"":13,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""trace"",""timestamp"":""2020-04-29T08:35:04.252Z"",""value"":{""message"":{""type"":""message"",""id"":""4RbdyZGhJblDsnWwZjZUfB-a|0000005"",""timestamp"":""2020-04-29T08:35:01.938Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""4RbdyZGhJblDsnWwZjZUfB-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""Sharepoint"",""channelData"":{""clientActivityID"":""15881493019184oofaqrynuo"",""clientTimestamp"":""2020-04-29T08:35:01.918Z""}},""queryResults"":[{""questions"":[""How do I use SharePoint""],""answer"":""Try this guide - https://xxx.pdf"",""score"":0.95,""id"":14,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""typing"",""timestamp"":""2020-04-29T09:20:27.881Z""}
{""type"":""delay"",""timestamp"":""2020-04-29T09:20:27.881Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T09:19:53.074Z"",""text"":""Here's our guide to Teams - https://xxx.pdf""}
{""type"":""delay"",""timestamp"":""2020-04-29T09:19:52.713Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T09:19:44.098Z"",""text"":""Welcome to SearchBot! Ask me a question and I will try to answer it.""}
{""type"":""message"",""timestamp"":""2020-04-29T09:20:25.737Z"",""text"":""annalect""}
{""type"":""typing"",""timestamp"":""2020-04-29T09:19:52.713Z""}
{""type"":""trace"",""timestamp"":""2020-04-29T09:19:53.011Z"",""value"":{""message"":{""type"":""message"",""id"":""ACjTjAWTJtvJeh6szBjzie-a|0000001"",""timestamp"":""2020-04-29T09:19:50.525Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""ACjTjAWTJtvJeh6szBjzie-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""Teams"",""entities"":[{""type"":""ClientCapabilities"",""requiresBotState"":true,""supportsListening"":true,""supportsTts"":true}],""channelData"":{""clientActivityID"":""1588151990502bgbcx3nbrkb"",""clientTimestamp"":""2020-04-29T09:19:50.502Z""}},""queryResults"":[{""questions"":[""How do I use Teams""],""answer"":""Here's our guide to Teams - https://xxx.pdf"",""score"":0.81349999999999989,""id"":13,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T09:19:50.525Z"",""text"":""Teams""}
{""type"":""typing"",""timestamp"":""2020-04-29T08:37:42.242Z""}
{""type"":""trace"",""timestamp"":""2020-04-29T08:37:42.604Z"",""value"":{""message"":{""type"":""message"",""id"":""K1XxtCn6qeEDuW2ASjWv6R-a|0000000"",""timestamp"":""2020-04-29T08:37:28.594Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""K1XxtCn6qeEDuW2ASjWv6R-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""hi"",""entities"":[{""type"":""ClientCapabilities"",""requiresBotState"":true,""supportsListening"":true,""supportsTts"":true}],""channelData"":{""clientActivityID"":""1588149448577tc6dant0tj9"",""clientTimestamp"":""2020-04-29T08:37:28.577Z""}},""queryResults"":[],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T08:37:40.215Z"",""text"":""Welcome to SearchBot! Ask me a question and I will try to answer it.""}
{""type"":""message"",""timestamp"":""2020-04-29T09:20:28.280Z"",""text"":""Here's the Annalect one-pager - https:xxx.pdf""}
{""type"":""delay"",""timestamp"":""2020-04-29T08:37:42.242Z"",""value"":2000}
{""type"":""conversationUpdate"",""timestamp"":""2020-04-29T08:37:25.793Z""}
{""type"":""trace"",""timestamp"":""2020-04-29T09:20:28.218Z"",""value"":{""message"":{""type"":""message"",""id"":""ACjTjAWTJtvJeh6szBjzie-a|0000003"",""timestamp"":""2020-04-29T09:20:25.737Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""ACjTjAWTJtvJeh6szBjzie-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""annalect"",""channelData"":{""clientActivityID"":""1588152025726lxcqw61814d"",""clientTimestamp"":""2020-04-29T09:20:25.726Z""}},""queryResults"":[{""questions"":[""I want to find out more about Annalect""],""answer"":""Here's the Annalect one-pager - https://xxx.pdf?csf=1&amp;web=1&amp;e=dGEBZ5"",""score"":0.7965000000000001,""id"":16,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T08:37:28.594Z"",""text"":""hi""}
{""type"":""message"",""timestamp"":""2020-04-29T08:37:42.698Z"",""text"":""I'm sorry, I don't have an answer to that question yet, but I'll add it to my learning list!""}
{""type"":""message"",""timestamp"":""2020-04-29T08:37:53.695Z"",""text"":""Benefits""}
{""type"":""typing"",""timestamp"":""2020-04-29T08:37:48.440Z""}
{""type"":""typing"",""timestamp"":""2020-04-29T08:37:55.843Z""}
{""type"":""trace"",""timestamp"":""2020-04-29T08:37:48.611Z"",""value"":{""message"":{""type"":""message"",""id"":""K1XxtCn6qeEDuW2ASjWv6R-a|0000003"",""timestamp"":""2020-04-29T08:37:46.327Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""K1XxtCn6qeEDuW2ASjWv6R-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""Teams"",""channelData"":{""clientActivityID"":""1588149466306udb6svuls1"",""clientTimestamp"":""2020-04-29T08:37:46.306Z""}},""queryResults"":[{""questions"":[""How do I use Teams""],""answer"":""Here's our guide to Teams - https://xxx.pdf"",""score"":0.81349999999999989,""id"":13,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""delay"",""timestamp"":""2020-04-29T08:37:55.843Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T08:37:48.674Z"",""text"":""Here's our guide to Teams - https://xxx.pdf""}
{""type"":""delay"",""timestamp"":""2020-04-29T08:37:48.440Z"",""value"":2000}
{""type"":""message"",""timestamp"":""2020-04-29T08:37:46.327Z"",""text"":""Teams""}
{""type"":""trace"",""timestamp"":""2020-04-29T08:37:56.014Z"",""value"":{""message"":{""type"":""message"",""id"":""K1XxtCn6qeEDuW2ASjWv6R-a|0000005"",""timestamp"":""2020-04-29T08:37:53.695Z"",""serviceUrl"":""https://webchat.botframework.com/"",""channelId"":""webchat"",""from"":{""id"":""de4b2545-f94a-4a86-8ab8-e639889a2cf8"",""name"":""You"",""role"":""user""},""conversation"":{""id"":""K1XxtCn6qeEDuW2ASjWv6R-a""},""recipient"":{""id"":""omgsearchbot@8LBpMqDWWGw"",""name"":""omgsearchbot""},""textFormat"":""plain"",""locale"":""en-US"",""text"":""Benefits"",""channelData"":{""clientActivityID"":""1588149473675fitfj50t5w8"",""clientTimestamp"":""2020-04-29T08:37:53.675Z""}},""queryResults"":[{""questions"":[""What are the OMG benefits""],""answer"":""You can find details of all our benefits here - https://xxx.pdf"",""score"":0.88,""id"":4,""source"":""Editorial"",""metadata"":[],""context"":{""isContextOnly"":false,""prompts"":[]}}],""knowledgeBaseId"":""078a1d0a-8d56-4f47-8836-34d1e818f2cc"",""scoreThreshold"":0.3,""top"":3,""strictFilters"":[],""metadataBoost"":[],""context"":{}}}
{""type"":""message"",""timestamp"":""2020-04-29T08:37:56.120Z"",""text"":""You can find details of all our benefits here - https://xxx.pdf""}
</code></pre>
","<json><botframework><azure-data-factory>","2020-05-01 08:14:14","1490","0","1","61543780","<p>Since the filter data you are looking for is inside the JSON file, you'll probably have to use Data Flow to solve this problem. Inside the Data Flow, use a Filter row modifier to select only the rows that meet your criteria. Since it looks like the row schemas are different based on the type, so you could branch multiple Filters for each type and Select only the desired columns. For the final output, you could either aggregate the branches back together using a Union, or have multiple Sinks (one for each type).</p>
"
"61532258","Slow Azure Data Factory Pipeline","<p>I am using <code>Azure Data Factory V2</code> to transfer some csv files from <code>Azure Data Lake</code> to <code>Azure Synapse</code></p>

<p>I have a loop to find all files in special folder on my <code>DataLake</code>.</p>

<p><a href=""https://i.stack.imgur.com/8WaQL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8WaQL.png"" alt=""enter image description here""></a></p>

<p>After i have a DataFlow to transfer data from staging to main table.</p>

<p><a href=""https://i.stack.imgur.com/uQfqv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uQfqv.png"" alt=""enter image description here""></a></p>

<p>In my for-each loop, at first i am cleaning my staging table by a SP then I am reading data from csv file (one by one). to transfer data from CVS to my staging table i am using <code>Copy Data</code> task. I am reading all columns as <code>varchar</code> and all columns in staging table are <code>varchar</code>(there is no casting here)</p>

<p><a href=""https://i.stack.imgur.com/VilwI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VilwI.png"" alt=""enter image description here""></a></p>

<p>Each file has about 20 columns and about 216 rows.</p>

<p>I am wondering why for just three files my pipeline takes so much times?</p>

<p><a href=""https://i.stack.imgur.com/mUH7k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mUH7k.png"" alt=""enter image description here""></a></p>

<p>this is my task for cleaning staging table.</p>

<p><a href=""https://i.stack.imgur.com/Wm00s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wm00s.png"" alt=""enter image description here""></a></p>

<p>This is my SQL server scale and usage.</p>

<p>I ran my pipeline exactly after resuming my Synapse service. that is only pipeline and service that work with my synapse.</p>

<p><a href=""https://i.stack.imgur.com/xSaLS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xSaLS.png"" alt=""enter image description here""></a></p>

<p>This is my stored procedure : </p>

<p><a href=""https://i.stack.imgur.com/EGWdv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EGWdv.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/B34ui.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B34ui.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/IHZfk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IHZfk.png"" alt=""enter image description here""></a></p>

<pre><code>CREATE PROCEDURE [stg].[...._Truncate]
AS
    TRUNCATE TABLE [stg].[....e]
GO
</code></pre>

<p>This is my DF</p>

<p><a href=""https://i.stack.imgur.com/JODLf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JODLf.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/P64Vb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P64Vb.png"" alt=""enter image description here""></a></p>

<pre><code>    SELECT 
        Convert(int,S.[MMSI]) AS [MMSI] ,
        Convert(int,S.[IMO] )  AS [IMO] ,
        Convert(int,S.[SHIP_ID] )AS [SHIP_ID] ,
        Convert(numeric(8, 5),S.[LAT] )  AS [LAT] ,
        Convert(numeric(8, 5),S.[LON] )  AS [LON] ,
        Convert(int,S.[SPEED] )  AS [SPEED] ,
        Convert(int,S.[HEADING] ) AS [HEADING] ,
        Convert(int,S.[COURSE] )  AS [COURSE] ,
        Convert(int,S.[STATUS] ) AS [STATUS] ,
        Convert(datetime,S.[TIMESTAMP] )  AS [TIMESTAMP] ,
        Convert(varchar(5),S.[DSRC] )  AS [DSRC] ,
        Convert(int,S.[UTC_SECONDS] ) AS [UTC_SECONDS] ,

           'M....._Simple' AS [ETL_CREATED_BY], 
           GETUTCDATE() AS [ETL_CREATED_DATE], 
           CONVERT(BIGINT, replace(CONVERT(VARCHAR, GETDATE(), 112), '/', '') + replace(CONVERT(VARCHAR, GETDATE(), 108), ':', '')) AS [ETL_PROCESS_ID]
    FROM [stg].[....e] AS s
</code></pre>

<p>This is my Derived columns</p>

<p><a href=""https://i.stack.imgur.com/NpiIV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NpiIV.png"" alt=""enter image description here""></a></p>

<p>This is ending mapping in my data flow</p>

<p><a href=""https://i.stack.imgur.com/HYvGV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HYvGV.png"" alt=""enter image description here""></a></p>

<p>Should I do somethings here ? </p>

<p><a href=""https://i.stack.imgur.com/5c95s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5c95s.png"" alt=""enter image description here""></a></p>
","<azure-pipelines><azure-data-lake><azure-data-factory><azure-synapse>","2020-04-30 20:34:22","5332","3","1","61596799","<p>I think the question is getting a bit muddled here, so I'm going to attempt an answer. Understand that there are a lot of potential factors and my response is not intended to be an exhaustive review of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance"" rel=""nofollow noreferrer"">Data Flow performance techniques</a>.</p>

<p>First, let me sum up the project as I understand it. For each file in an ADLS folder, it appears you have the following:</p>

<ol>
<li>Stored Procedure to truncate a Synapse staging table.</li>
<li>Copy activity to copy data from ADLS to Synapse staging table</li>
<li>DataFlow to read the data from the Synapse staging table, process it, and Write it back to a different Synapse table</li>
<li>Execute another pipeline to Archive the file.</li>
</ol>

<p>From what I gather, it seems that this process is working. The question is in regards to the execution time of the Data Flow. </p>

<p><strong>GENERAL</strong> performance guidelines to consider:</p>

<ul>
<li>Since you are running multiple DFs in succession, use a Custom Integration Runtime with ComputeOptimized type, 4 Cores, and Time To Live (TTL) greater than 0. [Not too long though, as you will pay for the environment to be active during the TTL time.] NOTE: last I was aware, DF requires the Region to be ""Auto Resolve"".</li>
</ul>

<p><a href=""https://i.stack.imgur.com/RBQkv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RBQkv.png"" alt=""enter image description here""></a></p>

<ul>
<li>Any time you WRITE to Synapse, make sure to define a Polybase Staging Storage Account:</li>
</ul>

<p><a href=""https://i.stack.imgur.com/64J0L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/64J0L.png"" alt=""enter image description here""></a></p>

<ul>
<li><p>Be aware of cross-region operations: network Latency can be a real killer AND costs you money. For fastest performance, the Storage, Data Factory, and Synapse resources should all be in the same data center.</p></li>
<li><p>Source and Sink partitioning CAN help with very large data sets and complex scenarios, but is a fairly tricky topic and (most likely) would not help in your scenario.</p></li>
</ul>

<p><strong>SPECIFIC</strong> to your posted scenario, I would consider redesigning the workflow. From a high level, you are doing the following for every (small) file:</p>

<ol>
<li>Clearing a Synapse table</li>
<li>Writing from blob to Synapse</li>
<li>Reading the data you just wrote from Synapse</li>
<li>Writing the data back to Synapse [after some minimal processing].</li>
</ol>

<p>My personal rule of thumb is to not use Data Flow to cross the Synapse boundary: if the operation is from Synapse to the same Synpase, then run the logic in Synapse. In other words, since the Source and Sink are both in Synapse, I would replace the Data Flow with a Stored Procedure.</p>

<p>Even better, I would consolidate the COPY activity and the Data Flow to a single Data Flow. The Data Flow can read the ADLS as a source, transform the data, and INSERT it to Synapse. This would remove the Staging table from the process. DF can even Archive the files after the operation:
<a href=""https://i.stack.imgur.com/sL4aC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sL4aC.png"" alt=""Data Flow Source options tab""></a></p>

<p>Finally, I would seek to limit the number of Data Flow executions. Is there a reason you have to process this file by file? The Data Flow source could easily process the entire folder, even capturing the file name as a column if you have logic based on that value [see image above]. This would remove the need to run the Data Flow many times and could dramatically improve overall performance.</p>
"
"61528244","Azure Data Factory - Data Flow - Join- Broadcast","<p>On 30th April 2020, Azure Data Factory Data Flows show a new option on the Optimize Tab of the Join Activity in the Data Flow. I get a validation error on the pipeline that atleast one side should be a part of the Broadcast. When I fixed the validation issues and published the data factory all of the data flows have broken. Please find attached the snapshot. 
<a href=""https://i.stack.imgur.com/vrXn2.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory>","2020-04-30 16:29:25","2724","0","2","61533870","<p>Can you please clear your browser cache and reload your pipeline?  There are some updated libraries that seem to have put your browser into an incomplete state.</p>
"
"61528244","Azure Data Factory - Data Flow - Join- Broadcast","<p>On 30th April 2020, Azure Data Factory Data Flows show a new option on the Optimize Tab of the Join Activity in the Data Flow. I get a validation error on the pipeline that atleast one side should be a part of the Broadcast. When I fixed the validation issues and published the data factory all of the data flows have broken. Please find attached the snapshot. 
<a href=""https://i.stack.imgur.com/vrXn2.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory>","2020-04-30 16:29:25","2724","0","2","61620265","<p>Problem was resolved next day 5/1/2020. We tried using Edge as well as Chrome with same results, so thinking not cache. Next day everything was working again.</p>
"
"61520585","Azure Data Factory : Set a limit to copy number of files using Copy activity","<p>I have a copy activity used in my pipeline to copy files from Azure data Lake gen 2. The source location may have 1000's of files and the files are required to be copied but we need to set a limit for the number files required to be copied. Is there any option available in ADF to achieve the same barring a custom activity?</p>

<p>Eg: I have 2000 files available in Data lake, but while running the pipeline i should able to pass a parameter to copy only 500 files.</p>

<p>Regards,
Sandeep</p>
","<azure-data-lake><azure-data-factory>","2020-04-30 09:50:55","1738","1","2","61537647","<p>I think you can use a lookup activity with the for each loop and a copy activity to achieve this . You will have to use a counter variable also ( this is make the process slow , as you will have to copy i file at a time ) . The loopkup acitivty has a limit of 5000 at this time , so you will have to keep that in mind . </p>
"
"61520585","Azure Data Factory : Set a limit to copy number of files using Copy activity","<p>I have a copy activity used in my pipeline to copy files from Azure data Lake gen 2. The source location may have 1000's of files and the files are required to be copied but we need to set a limit for the number files required to be copied. Is there any option available in ADF to achieve the same barring a custom activity?</p>

<p>Eg: I have 2000 files available in Data lake, but while running the pipeline i should able to pass a parameter to copy only 500 files.</p>

<p>Regards,
Sandeep</p>
","<azure-data-lake><azure-data-factory>","2020-04-30 09:50:55","1738","1","2","61648757","<p>I would use metadata activity to get a list of all items in your data lake: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>

<p>After that, you can use a ""ForEach"" step to loop through the list of files and copy them. In order to set a limit, you can use create two variables/parameters: <em>limit</em> and <em>files_copied</em>. In the beginning of each step, check if your <em>files_copied</em> is less than <em>limit</em>, perform the copy operation and add 1 to <em>files_copied</em>. </p>

<p>Alternatively, you can create a database with the names of all the files after the first step and then use lookup and for each steps, just like @HimanshuSinha-msft mentioned. In the Lookup step you can use SQL OFFSET+FETCH query in combination with your <em>limit</em> parameter to process only certain number of files. That can also solve the 5k limit of the lookup activity.</p>
"
"61519943","Request unit limit in Cosmos DB Sql API","<p>I'm using ADF pipeline for storing data from data lake into cosmos db sql api. On triggering pipeline run, I see the following error:</p>

<p><a href=""https://i.stack.imgur.com/rRlVE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rRlVE.png"" alt=""enter image description here""></a></p>

<p>Currently I'm using Throughput 5000 RU/s for the cosmos db container. Please help me understand why it is showing this error?</p>

<p><a href=""https://i.stack.imgur.com/xynfR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xynfR.png"" alt=""enter image description here""></a></p>

<p>Here is my pipeline:</p>

<p><a href=""https://i.stack.imgur.com/fePkx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fePkx.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/lGHjz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lGHjz.png"" alt=""enter image description here""></a></p>
","<azure-cosmosdb><azure-data-factory><azure-cosmosdb-sqlapi>","2020-04-30 09:16:09","308","1","1","61521275","<p>For saving your cost, don't unlimited increase RUs setting of cosmos db. Based on the error message you provided in your answer,it indicates that cosmos db can't process so many rows in unit time. So i suggest you throttling the transfer data procedure. Please consider using below configuration for the sink dataset of your copy activity.</p>

<p><a href=""https://i.stack.imgur.com/OqnYa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OqnYa.png"" alt=""enter image description here""></a></p>

<p>The default value is 10,000, please decrease the number here if you do not concern the speed of transmission.More details ,please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#azure-cosmos-db-sql-api-as-sink"" rel=""nofollow noreferrer"">document</a>.</p>

<p><a href=""https://i.stack.imgur.com/dAWee.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dAWee.png"" alt=""enter image description here""></a></p>

<p>Additional, you could throttling the max concurrent connections:</p>

<p><a href=""https://i.stack.imgur.com/SkSAv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SkSAv.png"" alt=""enter image description here""></a></p>
"
"61517854","Cost associated with ADF Data Flow","<p>What is the cost associated with Azure Data Factory Data Flow? Does the bill depend upon the data flows/Integration Runtimes we create? Also, does the bill depend on when we run a pipeline or starts billing from the time we create ADF Data Flow? If that's the case, is there an option to disable/pause ADF Data Flow Account in Azure?</p>
","<azure-data-factory>","2020-04-30 07:16:58","3612","3","1","61518591","<p>Data Flows are visually-designed components inside of Data Factory that enable data transformations at scale. You pay for the Data Flow cluster execution and debugging time per vCore-hour. The minimum cluster size to run a Data Flow is 8 vCores. Execution and debugging charges are prorated by the minute and rounded up. Preview pricing discounts will continue until November 30, 2019.</p>

<p><a href=""https://i.stack.imgur.com/sfGFg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sfGFg.png"" alt=""enter image description here""></a></p>

<p>Reference here: <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">Data Flow Execution and Debugging</a>.</p>

<p>Azure also provide the examples to help you know more about the mapping data flow pricing, please reference:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/pricing-concepts#using-mapping-data-flow-debug-for-a-normal-workday"" rel=""nofollow noreferrer"">Using mapping data flow debug for a normal workday</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/pricing-concepts#transform-data-in-blob-store-with-mapping-data-flows"" rel=""nofollow noreferrer"">Transform data in blob store with mapping data flows</a></li>
</ol>

<p>Hope this helps.</p>
"
"61508711","Azure DataFactory Integration Runtime","<p>I have manually created an integration runtime in  Azure Data Factory. I have read few articles that said - Once we create an integration runtime in Data Factory, Microsoft bills for it though there is no activity using it unless it is terminated.</p>

<p>Is this true?</p>
","<azure-data-factory>","2020-04-29 18:09:44","165","1","1","61514525","<p>Azure integration runtime provides a fully managed, serverless compute in Azure. You don't have to worry about infrastructure provision, software installation, patching, or capacity scaling. In addition, you only pay for the duration of the actual utilization.</p>

<p>Ref Azure document here: </p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ir-compute-resource-and-scaling"" rel=""nofollow noreferrer"">Azure IR compute resource and scaling</a>.</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/pricing-concepts"" rel=""nofollow noreferrer"">Understanding Data Factory pricing through examples</a></li>
</ol>

<p>To know more about the Data Factory pricing, you could reference here <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">Data Factory Pipeline Orchestration and Execution</a>:</p>

<p><a href=""https://i.stack.imgur.com/9Jvry.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Jvry.png"" alt=""enter image description here""></a></p>

<p>If no active executed on the IR, you don't need pay for it.</p>

<p>Hope this helps.</p>
"
"61500129","Triggering LogicApp from datafactory","<p>i have a data factory with selfhosted runtime environment of windows 2016 VMScaleSet. </p>

<p>I have created logic app with access control configured to access from vmss. i created http trigger in logic app. </p>

<p>when i try to invoke http trigger from datafactory web activity, i am getting following error </p>

<p>""errorCode"": ""2011"",
    ""message"": ""Could not load file or assembly 'CodeGenerator, Version=1.1.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35' or one of its dependencies. The system cannot find the file specified."",
    ""failureType"": ""UserError"",
    ""target"": ""SendMail"",
    ""details"": []
}</p>

<p>i am not sure, what is this assembly and where is this assembly needs to be installed and registered.</p>

<p>please help me to resolve this issue. </p>

<p>thanks!!</p>
","<azure-data-factory><azure-logic-apps><runtime-environment>","2020-04-29 10:54:53","60","0","1","61541440","<p>I have uninstalled the Selfhosted Integrated runtime and installed new version of SHIR and .net framework latest version ie (.netframework 8).</p>

<p><a href=""https://www.microsoft.com/en-us/download/details.aspx?id=39717"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/download/details.aspx?id=39717</a>
<a href=""https://dotnet.microsoft.com/download/dotnet-framework/thank-you/net48-web-installer"" rel=""nofollow noreferrer"">https://dotnet.microsoft.com/download/dotnet-framework/thank-you/net48-web-installer</a></p>

<p>it resolved this issue. i am able to trigger logic app from my datafactory pipeline</p>
"
"61498485","where is the Azure runtime Integration has to be installed?","<p>I am trying to pull the data from the on premise oracle database to the azure blob storage using azure data factory. When I went through the documentation it suggested to install the self hosted integration runtime in a windows machine. </p>

<p>So my question is where we need to install this runtime exactly. Do we have to create a separate VM for this as I can't install anything in the oracle hosted machine? If yes then how will the newly created VM will have access to connect with the oracle database. </p>

<p>Please let me know if some more clarity is required. I am new to this technology so please suggest me any other measure that I can go through to pull the data from on premise oracle database to azure blob storage. </p>

<p>Thanks</p>
","<oracle><azure><azure-blob-storage><azure-data-factory>","2020-04-29 09:26:19","48","0","1","61512955","<p>Self-hosted Integration Runtime should be installed on one of the machines in <strong>your local network</strong>, near to the database. Via the network, the app should be able to connect to your Oracle database like any other client application.<br>
The following picture explains it pretty well:</p>

<p><img src=""https://i.stack.imgur.com/KWn4X.png"" alt="""">
Probably a lot of your questions raised because you were thinking about VM in Azure, weren't you?</p>
"
"61498017","Passing timestamp to azure mapping data flow","<p>I'm trying to pass last modified date and time to my data flow as parameter. Can anyone tell me what is a correct way to pass it as parameter. I've tried multiple things like. passing utcnow() from activity throws error saying file not found whereas passing directly from dataflow works fine.
I figured out using dataflow expression works fine for ucnow() whereas pipeline expressions fails. </p>
","<azure><azure-data-factory><azure-data-lake><dataflow>","2020-04-29 09:03:44","2170","1","1","61525774","<p>The Pipeline expression language is a different and a little more limited than the Data Flow expression language. While Data Flow supports a richer variable type system, Pipelines only support String, Boolean, and Array types. Since there is no Date or Timestamp types, the date functions in the Pipeline expression language return strings:</p>

<p><a href=""https://i.stack.imgur.com/SG0t6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SG0t6.png"" alt=""enter image description here""></a></p>

<p>If you want to use the UTC value from the pipeline instead of the data flow, you will need define a <em>string</em> parameter on the Data Flow:
<a href=""https://i.stack.imgur.com/1Zs1e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Zs1e.png"" alt=""enter image description here""></a></p>

<p>Then pass the string of utcnow() to the Data Flow as a Pipeline Expression:
<a href=""https://i.stack.imgur.com/nt18F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nt18F.png"" alt=""enter image description here""></a></p>

<p>In the expression, use the utcnow() function to get the string value:
<a href=""https://i.stack.imgur.com/offY2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/offY2.png"" alt=""enter image description here""></a></p>

<p>In the Data Flow, use Derived Column to convert it to the desired type:
<a href=""https://i.stack.imgur.com/X1d7G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X1d7G.png"" alt=""enter image description here""></a></p>
"
"61478714","Azure data factory dataflow not present null values in JSON format","<p>I am looking for how to present my null values with the Azure Data Factory Dataflow activity. 
Currently, I am using standard dataflow for modelling my data from Azure SQL database and drag it into Blob storage in JSON format. 
<a href=""https://i.stack.imgur.com/ZdKMo.png"" rel=""nofollow noreferrer"">But if the dataset source has a null value, the column in the row is skipped in JSON</a>. </p>

<p>UPDATE:
<a href=""https://i.stack.imgur.com/I8iPQ.png"" rel=""nofollow noreferrer"">Here is the results of having data, having entry string as data and null value</a>.</p>

<p>Please, give me some advice on how to proceed, in order to resolve the problem. With Copy activity I don't have the same issue - null values are presented with """"</p>

<p>Thanks in advice!
Cheers,
Vesela</p>
","<azure><azure-data-factory><dataflow>","2020-04-28 11:06:51","4956","1","1","61493321","<p>I now have a solution, you could reference it.</p>

<p>I recreate a table with the schema like bellow, column <code>setNum</code> has null value:</p>

<p><a href=""https://i.stack.imgur.com/1XDBt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1XDBt.png"" alt=""enter image description here""></a></p>

<p>In the Data flow, I added a DerivedColumn to help us convert the null value to ''.</p>

<p><a href=""https://i.stack.imgur.com/EOTC2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EOTC2.png"" alt=""enter image description here""></a></p>

<p><strong>Expression:</strong>
    <code>iifNull(setNum,'')</code>: if the setNum is null, then replace it with ''.</p>

<p><a href=""https://i.stack.imgur.com/QTSA7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QTSA7.png"" alt=""enter image description here""></a></p>

<p>Now, run the pipeline, 
<a href=""https://i.stack.imgur.com/mIwMX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mIwMX.png"" alt=""enter image description here""></a></p>

<p>Check the data in blob:</p>

<p><a href=""https://i.stack.imgur.com/8Y7p0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Y7p0.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61474610","How to parameterise the delimiter for the Source in Azure Data Factory (V2) Data Flows?","<p>I have a situation where I am using Data Flows to ingest data from Blob Storage. The file format is CSV, however some files are comma delimited and some are pipe delimited. </p>

<p>It is possible to parameterise the delimiter of the dataset itself in ADF. </p>

<p><a href=""https://i.stack.imgur.com/HzvWM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HzvWM.png"" alt=""enter image description here""></a></p>

<p>However in the Dataflow activity the Source Settings do not provide the option to either dataset parameters or change the delimiter.</p>

<p><a href=""https://i.stack.imgur.com/4Qlv0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Qlv0.png"" alt=""enter image description here""></a></p>

<p>It's not even possible to parameterise the choice of dataset. This means that I have to create a new Dataflow for each delimiter which is not practical.</p>
","<csv><delimiter><azure-data-factory><parameterization>","2020-04-28 07:12:16","1153","1","1","61475016","<p>I tried this and it works well.</p>

<p>Source dataset parameter setting, add parameter <code>ColDelimiter</code>:</p>

<p><a href=""https://i.stack.imgur.com/y5A6h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y5A6h.png"" alt=""enter image description here""></a></p>

<p>Create a pipeline parameter <code>ColDelimiter</code>:</p>

<p><a href=""https://i.stack.imgur.com/Tlemt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tlemt.png"" alt=""enter image description here""></a></p>

<p>Then click the Data Flow, we can set the source parameter:</p>

<p><a href=""https://i.stack.imgur.com/TfXYY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TfXYY.png"" alt=""enter image description here""></a></p>

<p>When run the pipeline, we could set the column delimiter from paramter:
<a href=""https://i.stack.imgur.com/uHkfm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uHkfm.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61473405","Incremental load without date or primary key column using azure data factory","<p>I am having a source lets say SQL DB or an oracle database and I wanted to pull the table data to Azure SQL database. But the problem is I don't have any date column on which data is getting inserting or a primary key column. So is there any other way to perform this operation.</p>
","<azure><tsql><azure-data-factory>","2020-04-28 05:46:48","878","0","1","61473535","<p>One way of doing it semi-incremental is to partition the table by a fairly stable column in the source table, then you can use mapping data flow to compare the partitions ( can be done with row counts, aggregations, hashbytes etc ). Each load you store the compare output in the partitions metadata somewhere to be able to compare it again the next time you load. That way you can reload only the partitions that were changed since your last load.</p>
"
"61462653","Can I use same integration run time server for multiple on-premise environments","<p>We have IR server installed for ADF to connect on-premise environment.
Now we have another requirement, that we need to connect to one of the on-premise SFTP server and pull the files from their network.</p>

<p>Can I use the existed IR server to connect to SFTP of other on-premise environment.</p>

<p>Here two on-premise networks are different.</p>

<p>Regards,
Srinivas</p>
","<sftp><azure-data-factory>","2020-04-27 15:40:53","2421","0","2","61466258","<p>You can use the same IR for as many data sources as you want, as long it can reach them. If the vm where the IR is installed can reach the other environment, then of course you can! 
If not, maybe you can talk with a network admin to figure out some way. </p>

<p>If none of this is possible, then just install a new IR on a vm in this new environment.</p>

<p>Hope this helped!</p>
"
"61462653","Can I use same integration run time server for multiple on-premise environments","<p>We have IR server installed for ADF to connect on-premise environment.
Now we have another requirement, that we need to connect to one of the on-premise SFTP server and pull the files from their network.</p>

<p>Can I use the existed IR server to connect to SFTP of other on-premise environment.</p>

<p>Here two on-premise networks are different.</p>

<p>Regards,
Srinivas</p>
","<sftp><azure-data-factory>","2020-04-27 15:40:53","2421","0","2","61471879","<p>Yes, you can.</p>

<p>You can associate a self-hosted integration runtime with multiple on-premises machines or virtual machines in Azure. These machines are called nodes. You can have up to four nodes associated with a self-hosted integration runtime.</p>

<p>You can associate multiple nodes by installing the self-hosted integration runtime software from Download Center. Then, register it by using either of the authentication keys that were obtained from the <strong>New-AzDataFactoryV2IntegrationRuntimeKey</strong> cmdlet, as described in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<p>Note</p>

<p>You don't need to create a new self-hosted integration runtime to associate each node. You can install the self-hosted integration runtime on another machine and register it by using the same authentication key.</p>

<p>For more details, please reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability"" rel=""nofollow noreferrer"">High availability and scalability</a></p>
"
"61462153","Error while passing arguments to Pyspark Script in Azure data Factory","<p>I am running a PySpark script from Azure Data Factory. 
I have mentioned the arguments in the given section under Script/Jar as below.</p>

<p><a href=""https://i.stack.imgur.com/VYXfG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VYXfG.png"" alt=""enter image description here""></a></p>

<p>The arguments are a Key Value pair.
Arguments are being submitted fine as seen below.</p>

<pre><code>--arg '--APP_NAME ABC' --arg '--CONFIG_FILE_PATH wasbs://ABC --arg '--OUTPUT_INFO wasbs://XYZ
</code></pre>

<p>When the pipeline is executed I am getting the below Error.</p>

<pre><code>usage: Data.py [-h] --CONFIG_FILE_PATH CONFIG_FILE_PATH --OUTPUT_INFO
                      OUTPUT_INFO --ACTION_CODE ACTION_CODE --RUN_ID RUN_ID
                      --APP_NAME APP_NAME --JOB_ID JOB_ID --TASK_ID TASK_ID
                      --PCS_ID PCS_ID --DAG_ID DAG_ID
Data.py: error: argument --CONFIG_FILE_PATH is required.

</code></pre>
","<apache-spark><pyspark><azure-data-factory><azure-hdinsight>","2020-04-27 15:16:11","515","0","1","61628690","<p>You can passing arguments to Pyspark Script in Azure data Factory.</p>

<p><a href=""https://i.stack.imgur.com/QkVO7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QkVO7.png"" alt=""enter image description here""></a></p>

<p><strong>Code:</strong></p>

<pre><code>{
    ""name"": ""SparkActivity"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Spark1"",
                ""type"": ""HDInsightSpark"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""rootPath"": ""adftutorial/spark/script"",
                    ""entryFilePath"": ""WordCount_Spark.py"",
                    ""arguments"": [
                        ""--input-file"",
                        ""wasb://sampledata@chepra.blob.core.windows.net/data"",
                        ""--output-file"",
                        ""wasb://sampledata@chepra.blob.core.windows.net/results""
                    ],
                    ""sparkJobLinkedService"": {
                        ""referenceName"": ""AzureBlobStorage1"",
                        ""type"": ""LinkedServiceReference""
                    }
                },
                ""linkedServiceName"": {
                    ""referenceName"": ""HDInsight"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        ],
        ""annotations"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p><strong>WALKTHROUGH for passing arguments in ADF:</strong></p>

<p><a href=""https://i.stack.imgur.com/aC0hg.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aC0hg.gif"" alt=""enter image description here""></a></p>

<p><strong>Some of the example for passing the parameters in Azure Data Factory:</strong></p>

<pre><code>{
    ""name"": ""SparkSubmit"",
    ""properties"": {
        ""description"": ""Submit a spark job"",
        ""activities"": [
            {
                ""type"": ""HDInsightMapReduce"",
                ""typeProperties"": {
                    ""className"": ""com.adf.spark.SparkJob"",
                    ""jarFilePath"": ""libs/spark-adf-job-bin.jar"",
                    ""jarLinkedService"": ""StorageLinkedService"",
                    ""arguments"": [
                        ""--jarFile"",
                        ""libs/sparkdemoapp_2.10-1.0.jar"",
                        ""--jars"",
                        ""/usr/hdp/current/hadoop-client/hadoop-azure-2.7.1.2.3.3.0-3039.jar,/usr/hdp/current/hadoop-client/lib/azure-storage-2.2.0.jar"",
                        ""--mainClass"",
                        ""com.adf.spark.demo.Demo"",
                        ""--master"",
                        ""yarn-cluster"",
                        ""--driverMemory"",
                        ""2g"",
                        ""--driverExtraClasspath"",
                        ""/usr/lib/hdinsight-logging/*"",
                        ""--executorCores"",
                        ""1"",
                        ""--executorMemory"",
                        ""4g"",
                        ""--sparkHome"",
                        ""/usr/hdp/current/spark-client"",
                        ""--connectionString"",
                        ""DefaultEndpointsProtocol=https;AccountName=&lt;YOUR_ACCOUNT&gt;;AccountKey=&lt;YOUR_KEY&gt;"",
                        ""input=wasb://input@&lt;YOUR_ACCOUNT&gt;.blob.core.windows.net/data"",
                        ""output=wasb://output@&lt;YOUR_ACCOUNT&gt;.blob.core.windows.net/results""
                    ]
                },
                ""inputs"": [
                    {
                        ""name"": ""input""
                    }
                ],
                ""outputs"": [
                    {
                        ""name"": ""output""
                    }
                ],
                ""policy"": {
                    ""executionPriorityOrder"": ""OldestFirst"",
                    ""timeout"": ""01:00:00"",
                    ""concurrency"": 1,
                    ""retry"": 1
                },
                ""scheduler"": {
                    ""frequency"": ""Day"",
                    ""interval"": 1
                },
                ""name"": ""Spark Launcher"",
                ""description"": ""Submits a Spark Job"",
                ""linkedServiceName"": ""HDInsightLinkedService""
            }
        ],
        ""start"": ""2015-11-16T00:00:01Z"",
        ""end"": ""2015-11-16T23:59:00Z"",
        ""isPaused"": false,
        ""pipelineMode"": ""Scheduled""
    }
}
</code></pre>
"
"61460963","Need to read a CSV file using Azure Data Factory Activity","<p>I have a CSV file. 
I need to read each row and access the column values and for each row I need to call a foreach activity.</p>

<p>With what activity can I achieve this?</p>
","<azure-data-factory>","2020-04-27 14:18:36","2867","1","1","61470962","<p>Assuming that the CSV file is in a cloud storage , you can use the lookup activity . Please beware that lookup activity has a limitation of 5000 at this time . Once you have done that you can use a FE loop and iterate through it .
Hope this helps </p>
"
"61459391","Is there any method to replace the access token of linked service in azure data factory using python?","<p>I want to replace the access token and the refresh token of the linked service once the refresh token expires using a python script.</p>
","<azure><oauth><access-token><azure-data-factory><refresh-token>","2020-04-27 12:54:04","338","0","1","61471888","<p>Per my knowledge, there is no built-in refresh token mechanism in ADF so far. So,i suggest you creating a trigger to implement your need.</p>

<p>For example,using Time Trigger Azure Function because you know the interval of your token.Just set the schedule of Azure Function as that interval. Then use <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.operations.linkedservicesoperations?view=azure-python"" rel=""nofollow noreferrer"">Update method</a> with ADF python sdk inside Azure Function. Of course,this scheduled task can be replaced by other mechanisms you know.</p>
"
"61456136","ADF pipeline going into queue state","<p>I have a Copy activity where the source and destination are both Blobs.
When i tried the copy pipeline previously,it ran successfully.
But currently it is going into queue state for a long time i.e. 30 minutes.</p>

<p><a href=""https://i.stack.imgur.com/JbK35.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JbK35.png"" alt=""enter image description here""></a></p>

<p>Can i know the reason behind it?</p>
","<azure><azure-data-factory>","2020-04-27 09:55:35","811","0","1","61457065","<p>This is not an answer/solution to the problem. Since i cannot comment yet, had to put it in the Answer section. It could be delay in assigning the compute resources. Please check the details.
You can check the details by hovering mouse pointer between Name and Type beside Copy.</p>

<p><a href=""https://i.stack.imgur.com/Bk6Nt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bk6Nt.png"" alt=""enter image description here""></a></p>
"
"61455887","How to clean workspace of Azure Data Factory using task of Azure Devops?","<p>I want to clean Azure Data Factory workspace before deploying my dev environment to it, How can i clean the workspace using task of Azure DevOps?</p>
","<azure><azure-devops><azure-functions><azure-data-factory>","2020-04-27 09:41:15","160","0","1","61456419","<p>Imo, the easiest way is to arrange a dedicated resource group for your instance of ADF and perform deployment using a <strong>Complete</strong> mode. 
In this case:</p>

<blockquote>
  <p>Resource Manager deletes resources that exist in the resource group
  but aren't specified in the template.</p>
</blockquote>

<p>Therefore, no separate task needed for this, but the deployment of ARM template is to be set to a right mode.</p>

<p>In such case, avoid of placing the other objects, like keyvault or storage account in that group, since they will be wiped out during the deployment of data factory.</p>

<p><strong>References:</strong> <a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/deployment-modes"" rel=""nofollow noreferrer"">Azure Resource Manager deployment modes</a></p>
"
"61444169","""Failed to connect to: we.frontend.clouddatahub.net"" error while registering Integration Runtime of Azure Data Factory","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#set-up-an-existing-self-hosted-ir-via-local-powershell"" rel=""nofollow noreferrer"">This is what</a> I followed to setup IR.</p>

<p>In the final step of Registering Azure Data factory self hosted intergration runtime, we need to provide the Authentication Key. then the installation is making a call to internet. Isn't this strange as the VM could be in a private network? </p>

<p>If the VM is not connected to internet and it gets this error then what to do? <code>""Failed to connect to: we.frontend.clouddatahub.net""</code> </p>

<p>This is the error I get </p>

<pre><code>Failed to execute the command ' -Key xxx'. Error message: Microsoft.DataTransfer.DIAgentClient.HostServiceException: Failed to get service token from ADF service with key xxxx and time cost is: 3.0786307 seconds, the error code is: UnexpectedFault, activityId is: xxx and detailed error message is An error occurred while sending the request.
The underlying connection was closed: An unexpected error occurred on a send.
Authentication failed because the remote party has closed the transport stream.
</code></pre>

<p>The issue seems to be disabled remote access. How can I enable it? <code>Dmgcmd -era 8060</code> is not working. </p>

<p>I have also a related issue logged <a href=""https://stackoverflow.com/questions/61480652/remote-access-issue-in-self-hosted-integration-runtime"">as another VM works and this fails</a></p>
","<azure><azure-data-factory>","2020-04-26 16:24:52","2321","0","1","61453049","<p>Even if you have some private network where the communication can go without any restrictions between your data sources and your integration runtime, the integration runtime application needs to be able to communicate with the Azure data factory services as well. Try whitelisting the IPs for your region in the networking settings of your Azure VM or in your firewall - according to this:</p>

<p><a href=""https://learn.microsoft.com/sv-se/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""nofollow noreferrer"">https://learn.microsoft.com/sv-se/azure/data-factory/azure-integration-runtime-ip-addresses</a></p>
"
"61439034","U-SQL script for JSON's transformation","<p>I have JSON-file with many levels and need to transform it in Azure Data Lake Analytics to get all data started from ""vin"" level (incl.) </p>

<p>The cutted json is:</p>

<pre><code>""vehicleStatusResponse"": {
    ""vehicleStatuses"": [
        {
            ""vin"": ""ABC1234567890"",
            ""triggerType"": {
                ""triggerType"": ""TIMER"",
                ""context"": ""RFMS"",
                ""driverId"": {
                    ""tachoDriverIdentification"": {
                        ""driverIdentification"": ""123456789"",
                        ""cardIssuingMemberState"": ""BRA"",
                        ""driverAuthenticationEquipment"": ""CARD"",
                        ""cardReplacementIndex"": ""0"",
                        ""cardRenewalIndex"": ""1""
                    }
                }
            },
            ""receivedDateTime"": ""2020-02-12T04:11:19.221Z"",
            ""hrTotalVehicleDistance"": 103306960,
            ""totalEngineHours"": 3966.6216666666664,
            ""driver1Id"": {
                ""tachoDriverIdentification"": {
                    ""driverIdentification"": ""BRA1234567""
                }
            },
            ""engineTotalFuelUsed"": 48477520,
            ""accumulatedData"": {
                ""durationWheelbaseSpeedOverZero"": 8309713,
                ""distanceCruiseControlActive"": 8612200,
                ""durationCruiseControlActive"": 366083,
                ""fuelConsumptionDuringCruiseActive"": 3064170,
                ""durationWheelbaseSpeedZero"": 5425783,
                ""fuelWheelbaseSpeedZero"": 3332540,
                ""fuelWheelbaseSpeedOverZero"": 44709670,
                ""ptoActiveClass"": [
                    {
                        ""label"": ""wheelbased speed &gt;0"",
                        ""seconds"": 16610,
                        ""meters"": 29050,
                        ""milliLitres"": 26310
                    },
                    {
                        ""label"": ""wheelbased speed =0"",
                        ""seconds"": 457344,
                        ""milliLitres"": 363350
</code></pre>

<p>There is a script for this case:</p>

<pre><code>CREATE ASSEMBLY IF NOT EXISTS [Newtonsoft.Json] FROM ""adl://#####.azuredatalakestore.net/Newtonsoft.Json.dll"";
CREATE ASSEMBLY IF NOT EXISTS [Microsoft.Analytics.Samples.Formats] FROM ""adl://#####.azuredatalakestore.net/Microsoft.Analytics.Samples.Formats.dll"";


REFERENCE ASSEMBLY [Newtonsoft.Json];
REFERENCE ASSEMBLY [Microsoft.Analytics.Samples.Formats];

USING Microsoft.Analytics.Samples.Formats.Json;

DECLARE @InputFile string = ""response.json"";
DECLARE @OutputFile string = ""response.csv"";


@json =
EXTRACT
    vin string,
    triggerType string,
    driverIdentification string,
    receivedDateTime DateTime,
    hrTotalVehicleDistance int,
    totalEngineHours float,
    engineTotalFuelUsed int,
    durationWheelbaseSpeedOverZero int,
    distanceCruiseControlActive int,
    durationCruiseControlActive int,
    fuelConsumptionDuringCruiseActive int,
    durationWheelbaseSpeedZero int,
    fuelWheelbaseSpeedZero int
    ptoActiveClass string
    label string
    seconds int
    meters int
    millilitres int


FROM
    @InputFile
USING new MultiLevelJsonExtractor(""vehicleStatusResponse.vehicleStatuses.vin[*]"",
    true,
    ""vin"",
    ""triggerType"",
    ""driverIdentification"",
    ""receivedDateTime"",
    ""hrTotalVehicleDistance"",
    ""totalEngineHours"",
    ""engineTotalFuelUsed"",
    ""durationWheelbaseSpeedOverZero"",
    ""distanceCruiseControlActive"",
    ""durationCruiseControlActive"",
    ""fuelConsumptionDuringCruiseActive"",
    ""durationWheelbaseSpeedZero"",
    ""fuelWheelbaseSpeedZero"", 
    ""ptoActiveClass"",
    ""label"",
    ""seconds"",
    ""meters"",
    ""millilitres""
    );
@new =
SELECT
    vin,
    triggerType,
    driverIdentification,
    receivedDateTime,
    hrTotalVehicleDistance,
    totalEngineHours,
    engineTotalFuelUsed,
    durationWheelbaseSpeedOverZero,
    distanceCruiseControlActive,
    durationCruiseControlActive,
    fuelConsumptionDuringCruiseActive,
    durationWheelbaseSpeedZero,
    fuelWheelbaseSpeedZero,
    ptoActiveClass,
    label,
    seconds,
    meters,
    millilitres
FROM @json;
OUTPUT @new
TO @OutputFile
USING Outputters.Csv();
</code></pre>

<p>Anyway, I get only blank response.csv with no data. What's wrong in my script? Will be interesting if you may have other ways to transform hierarchical json data.</p>
","<json><azure><azure-data-lake><azure-data-factory><u-sql>","2020-04-26 10:12:58","90","1","1","61823254","<p>You are not correctly extracting the JSON. You need to use it like:</p>

<pre class=""lang-cs prettyprint-override""><code>FROM
    @InputFile
USING new MultiLevelJsonExtractor(""vehicleStatusResponse.vehicleStatuses.vin[*]"",
    true,
    ""tachoDriverIdentification.driverIdentification""
    );
</code></pre>

<p>You can read more about JSON advanced JSON manipulation using <code>U-SQL</code> <a href=""https://www.taygan.co/blog/2018/03/02/azure-data-lake-series-working-with-json-part-3"" rel=""nofollow noreferrer"">here</a>.</p>
"
"61435153","Data Factory - unzip a password protected zip","<p>I am successfully downloading a ZIP from a secure FTP site into my Blob. However, this is a special case and the ZIP is password protected. It seems my only option is special code, but I am unsure what that exactly means. I have never coded data factories, only used the UI with advanced expressions.</p>

<p>Can anyone point me in the right direction?</p>

<p>Thanks,
Zach</p>
","<azure-data-factory>","2020-04-26 02:35:03","1596","1","1","61441846","<p>Per my observation and test,it seems ADF only supports decompression of zip file though,it is not allowed for password protected zip file.</p>

<p><a href=""https://i.stack.imgur.com/i9jkE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i9jkE.png"" alt=""enter image description here""></a></p>

<p>As workaround,i suggest you following the solutions mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs-legacy#unsupported-file-types-and-compression-formats"" rel=""nofollow noreferrer"">official document</a>:</p>

<p>1.Use Azure Function Activity to decompress protected file with function code.You could choose your desired development language.</p>

<p>2.Use Custom Activity to decompress the file with task which is executed on the Azure Batch node actually.</p>
"
"61427384","Azure Data Factory Dynamic Content with base64 Conversion","<p>I am making a HTTP Triggered Azure Function Call from Azure Data Factory. I am not able to make the HTTP body of the function activity correct for HTTP POST. This is the HTTP body</p>

<p>{ ""filename"": ""@{item().BatchId}.json"", ""filecontent"": @{base64(item().BatchId)} }</p>

<p>I am passing two things: filename and filecontent, which needs to be base64 encoded before sending it to the function. base64 function is NOT working with the dynamic value of BatchId but the filename is working fine in the above example. </p>
","<azure><azure-data-factory><dynamic-expression>","2020-04-25 14:40:24","1709","1","1","61432207","<p>It was not working because the BatchId was not a string. Following line started working - </p>

<p>{ ""filename"": ""@{item().BatchId}.json"", containername:""insightsfiles"", ""filecontent"": ""@{base64(string(item().BatchId)) }""}</p>
"
"61417698","Accessing metadata results in a nested pipeline for Azure Data Factory","<p>I have a pipeline built that reads metadata from a blob container subfolder raw/subfolder.  I then execute a foreach loop with another get metadata task to get data for each subfolder, it returns the following type of data.  /raw/subfolder1/folder1, /raw/subfolder2/folder1, /raw/subfolder2/folder1 and so on.  I need another foreach loop to access the files inside of each folder.  The problem is that you cannot run a foreach loop inside of another foreach loop so I cannot iterate further on the files.</p>

<p>I have an execute datapipline that calls the above pipeline and then uses a foreach.  My issue with this is that I'm not finding a way to pass the item().name from the above iteration to my new pipeline.  It doesn't appear you can pass in objects form the previous pipeline?  How would I be able to accomplish this nested foreach metat data gathering so I can iterate further on my files?</p>
","<azure><foreach><azure-data-factory>","2020-04-24 21:25:47","983","1","2","61448238","<p>Have you tried using parameters? Here is how it would look like:</p>

<ol>
<li>In your parent pipeline, click on the ""Execute Pipeline"" activity which triggers the inner (your new pipeline) go to <em>Settings</em> and specify item name as a parameter ""name"".
<a href=""https://i.stack.imgur.com/BIMZt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BIMZt.png"" alt=""enter image description here""></a></li>
<li>In your inner pipeline, click anywhere on empty space and add new parameter ""name"". 
<a href=""https://i.stack.imgur.com/tzbgX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tzbgX.png"" alt=""enter image description here""></a></li>
<li>Now you can refer to that parameter like this: pipeline().parameters.name</li>
</ol>
"
"61417698","Accessing metadata results in a nested pipeline for Azure Data Factory","<p>I have a pipeline built that reads metadata from a blob container subfolder raw/subfolder.  I then execute a foreach loop with another get metadata task to get data for each subfolder, it returns the following type of data.  /raw/subfolder1/folder1, /raw/subfolder2/folder1, /raw/subfolder2/folder1 and so on.  I need another foreach loop to access the files inside of each folder.  The problem is that you cannot run a foreach loop inside of another foreach loop so I cannot iterate further on the files.</p>

<p>I have an execute datapipline that calls the above pipeline and then uses a foreach.  My issue with this is that I'm not finding a way to pass the item().name from the above iteration to my new pipeline.  It doesn't appear you can pass in objects form the previous pipeline?  How would I be able to accomplish this nested foreach metat data gathering so I can iterate further on my files?</p>
","<azure><foreach><azure-data-factory>","2020-04-24 21:25:47","983","1","2","61456144","<p>Using Parameters works in this scenario as @Andrii mentioned. 
For more on passing parameters between activities refer to this link.
<a href=""https://azure.microsoft.com/en-in/resources/azure-data-factory-passing-parameters/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-in/resources/azure-data-factory-passing-parameters/</a></p>
"
"61414190","SSIS component to remove linefeeds from all (n)varchar fields in dataflow","<p>I use a SSIS component to retrieve data from an API.
The API delivers XML,
This XML contains
data like this:</p>

<pre><code>&lt;tag&gt;
code
&lt;/tag&gt;
</code></pre>

<p>Linefeeds before and after 'code'</p>

<p>That means that what is entered into my (n)varchar columns in SQL looks like this:
<code>CHAR(10)codeCHAR(10)</code></p>

<p>That messes up a lot, among other things the way things look in the reports.
So the CHAR(10) needs to disappear. It needs to be filtered out between the source component and the destination component in my dataflow.
I could use derived columns for this but it concerns 9 dataflows with (in total) 385 (n)varchar fields.
A lot of work!
Is it possible to use a script component that simply does a <code>replace(field,linefeed,'')</code> on each (n)varchar? So that all data passes through this component and exits stripped of linefeeds?</p>

<p>If so, how do I do this?
I am pretty ok with SQL, can read most languages but need some help on writing this in a C# of VB.NET.
I am designing this in VS2019 and deploying to ADFv2 IS. Targets are SQL Azure databases.</p>

<p>I cannot simply run some SQL  after the SSIS job has run since the next run will do a upsert on the data. It will then conclude that <code>CHAR(10)codeCHAR(10)</code> is different from <code>code</code> and insert a new line, violating the unique constraint on <code>code</code>. So that's why I need to do it after getting the data and before writing the data. </p>
","<ssis><azure-data-factory><script-component>","2020-04-24 17:35:54","102","1","2","61448921","<p>You don't necessarily need SSIS to do this.  You can use the <code>TRIM</code> function in SQL Server 2017 and later and Azure SQL DB.  Insert your XML into a staging table unchanged from the original.  Then call a stored procedure that cleans up the XML and optionally inserts it into another table.  An example:</p>

<pre><code>--INSERT INTO ...
SELECT 
    TRIM( CHAR(10) + CHAR(13) FROM someXML.value( '(tag/text())[1]', 'VARCHAR(20)' ) ) trimmedTag
FROM xmlTest
</code></pre>

<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/trim-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">TRIM</a> gives you the option to add characters you want to remove.  In the above example I've used the <code>CHAR</code> function to remove line feed (10) and carriage return (13) respectively.</p>

<p>From within SSIS you could use an Execute SQL Task to call the proc.</p>
"
"61414190","SSIS component to remove linefeeds from all (n)varchar fields in dataflow","<p>I use a SSIS component to retrieve data from an API.
The API delivers XML,
This XML contains
data like this:</p>

<pre><code>&lt;tag&gt;
code
&lt;/tag&gt;
</code></pre>

<p>Linefeeds before and after 'code'</p>

<p>That means that what is entered into my (n)varchar columns in SQL looks like this:
<code>CHAR(10)codeCHAR(10)</code></p>

<p>That messes up a lot, among other things the way things look in the reports.
So the CHAR(10) needs to disappear. It needs to be filtered out between the source component and the destination component in my dataflow.
I could use derived columns for this but it concerns 9 dataflows with (in total) 385 (n)varchar fields.
A lot of work!
Is it possible to use a script component that simply does a <code>replace(field,linefeed,'')</code> on each (n)varchar? So that all data passes through this component and exits stripped of linefeeds?</p>

<p>If so, how do I do this?
I am pretty ok with SQL, can read most languages but need some help on writing this in a C# of VB.NET.
I am designing this in VS2019 and deploying to ADFv2 IS. Targets are SQL Azure databases.</p>

<p>I cannot simply run some SQL  after the SSIS job has run since the next run will do a upsert on the data. It will then conclude that <code>CHAR(10)codeCHAR(10)</code> is different from <code>code</code> and insert a new line, violating the unique constraint on <code>code</code>. So that's why I need to do it after getting the data and before writing the data. </p>
","<ssis><azure-data-factory><script-component>","2020-04-24 17:35:54","102","1","2","61517905","<p>I could not find a solution that would not require extensive coding, at the cost of performance.
One solution I tried was writing the XML as a CSV, do a search and replace on the CSV en then import that into a table.
Though it worked, it made the solution unnessecary complex and it was detrimental in terms of performance. It was fighting symptoms, not curing the issue.
So I went and talked to the guys delivering the API and they removed these linefeeds. That turned out to be the only good solution.</p>
"
"61406782","How to set event trigger with containername/*/pathname in ADF?","<p>My blob storage have path like this</p>

<pre><code>ctn/team1/Raw
   /team2/Raw
   /team3/Raw
</code></pre>

<p>I want to trigger ADF pipeline when blob is created in any Raw path.
How to config event trigger in this case ?</p>
","<azure><azure-data-factory><eventtrigger>","2020-04-24 10:58:37","1003","1","1","61416727","<p>You can have the trigger setting as below :</p>
<p><code>Blob path begins with = team</code> - (assuming 'ctn' as container)</p>
<p>In case if 'ctn' is not a container and a root folder then you can have <code>Blob path begins with = ctn/team</code></p>
<p><a href=""https://i.stack.imgur.com/r7jRD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r7jRD.png"" alt=""enter image description here"" /></a></p>
<p><strong>Note</strong>: In case if you have multiple sub-folders under <code>team*/</code> (see below example) , then the above setting will create events for all the blobs that were created in the subfolders (RAW, RAW2, ..)</p>
<p>For example:</p>
<pre><code>ctn/team1/Raw
         /Raw2
         /Raw3    
   /team2/Raw
         /Raw2
   /team3/Raw
         /Raw2
</code></pre>
<p>Currently events triggers do not support wildcards in the path. And if you have multiple sub folders and want to create events for a specific sub folder as mentioned in above example, then you will have to create event triggers with specific paths.</p>
<p>Hope this helps.</p>
"
"61406619","Is there way to reference overrideParameters from a file in ADF CI/CD?","<p>I need to make use of <code>overrideParameters</code> in an ADF CI/CD scenario like described in the official Azure <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">documentation</a> - can't seem to find a way even in the <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-resource-group-deployment?view=azure-devops"" rel=""nofollow noreferrer"">Azure Resource Group Deployment task</a> , is there a way to reference <code>overrideParameters</code> from a file instead of being forced to create a long ugly string with space seperation?</p>
","<azure><azure-devops><azure-pipelines><azure-data-factory>","2020-04-24 10:49:29","188","0","1","65806395","<p>You can reference the values of each parameter from a variable template. That still requires you to list the override parameters but the values are sourced dynamically and thus can change across environments. Alternatively, you could put the whole deployment task into a template but then you would still have to think about dynamic environment variables in the template. A simple example below.</p>
<p><em>variable-template.yml</em></p>
<pre><code>variables: 

  ...

  # subscription and resource group 
  azureSubscription: 'my_subscription'
  subscriptionId: 'my_subscription_id'
  resourceGroupName: 'my_rg'

  # key vault
  keyVaultName: 'my_kv'
  keyVaultSecretsList: 'storageAccountConnectionString,storageAccountKey'

  # data factory
  dataFactoryRepoPath: 'my_adf_repo_path'
  dataFactoryName: 'my_adf'

  # storage accounts
  storageAccountName: 'my_sa'

  ...
</code></pre>
<p><em>azure-pipelines.yml</em></p>
<pre><code>...

- task: AzureKeyVault@1
  inputs:
    azureSubscription: ${{ variables.azureSubscription }}
    KeyVaultName: ${{ variables.keyVaultName }}
    SecretsFilter: ${{ variables.keyVaultSecretsList }}
    RunAsPreJob: true

- task: AzureResourceManagerTemplateDeployment@3
  displayName: 'ARM Template deployment'
  inputs:
    deploymentScope: 'Resource Group'
    azureResourceManagerConnection: ${{ variables.azureSubscription }}
    subscriptionId: ${{ variables.subscriptionId }}
    action: 'Create Or Update Resource Group'
    resourceGroupName: ${{ variables.resourceGroupName }}
    location: 'West Europe'
    templateLocation: 'Linked artifact'
    csmFile: '$(System.DefaultWorkingDirectory)/adf_branch/${{ variables.dataFactoryRepoPath }}/ARMTemplateForFactory.json'
    csmParametersFile: '$(System.DefaultWorkingDirectory)/adf_branch/${{ variables.dataFactoryRepoPath }}/ARMTemplateParametersForFactory.json'
    overrideParameters: |
     -factoryName ${{ variables.dataFactoryName }} 
     -storageAccountSink_properties_typeProperties_url ${{ variables.storageAccountName }} 
     -storageAccountSink_accountKey $(storageAccountKey) 
     -storageAccountSink_connectionString $(storageAccountConnectionString)
    deploymentMode: 'Incremental'

...
</code></pre>
"
"61403678","Azure DF - when extracting a datetime from a database into a CSV it sometimes gets interpreted as a datetime2","<p>When running an Azure Data Factory copy from CSV to a Synapse table we get intermittent Truncate errors. The destination table schema (in Synapse) is a mirror of the schema we originally extracted the data from.</p>

<p>What we find happening is that the original extract misinterpreted a datetime as a datetime2 and render the relevant field as such: 2019-10-07 11:22:31.4400000
When we run the copy from Azure Data Lake Storage Gen2 to the mirrored Synapse Table this schema has the field as a datetime. 
The copy function attempts a conversion from string (being CSV and all) into datetime (as that is the same as the originating table) but fails. (Error: Conversion failed when converting date and/or time from character string.)
Interestingly this issue is intermittent - the original datetime field is sometimes correctly rendered into the CSV as: 2019-10-07 11:22:31.440 (go figure). </p>

<p>We have limited desire to refactor all our SQL Db Schemas into datetime2 data types (for obvious reasons).
Anyone know if we are missing something here?</p>
","<azure><azure-data-factory><azure-synapse>","2020-04-24 07:54:55","437","1","1","61603269","<p>Try changing the Mapping of the source to Datetime:</p>

<ul>
<li>specify the date format ""yyyy-MM-dd""</li>
<li>Run the pipeline</li>
</ul>

<p>Alternatively:</p>

<ul>
<li>Change the mapping of date format to string</li>
<li>Use the stored procedure approach to insert/copy data</li>
</ul>
"
"61400888","Azure data factory says temp table is an ""Invalid object name""?","<p>I have a very large query that I am attempting to use in a Copy Data Azure Data Factory (ADF) workflow.  This is a simplified version of it:</p>

<pre><code>CREATE TABLE #tmp
    (
        Name VARCHAR(255)
        ,… --some other fields
    );
    INSERT INTO #tmp
    (
      Name
  ,… --some other fields
    )
    EXECUTE sp_getData


    SELECT t.Name
FROM #tmp AS t
    
BEGIN TRY 
    DROP TABLE #tmp 
END TRY 
BEGIN CATCH 
END CATCH
</code></pre>

<p>When I specify this query as my ""Source"" for my ADF copy workflow it immediately displays the error: </p>

<blockquote>
  <p>A database operation failed with the following error: 'Invalid object
  name '#tmp'.' Invalid object name '#tmp'.,
  SqlErrorNumber=208,Class=16,State=0, . Activity ID:
  707769bc-a864-4e15-8217-bec1ef9462bb</p>
</blockquote>

<p>If I run this query directly in SSMS it executes fine with no errors displayed at all.  But if I'm in the ADF creation wizard and I click the ""Validate"" button, it displays the error message, displays ""loading preview"" for a while and then returns data as expected.  If I try to click ""Next"" to continue on with the Copy Data setup it will not let me continue.  Unfortunately it doesn't not seem to specify which line in the query is causing the error so I'm not exactly sure where the problem lies.</p>

<p>Why would I be getting this error in ADF but not in SSMS itself?  I'm using the same account to execute the query and I can't see any differences other than one is being run from ADF and one is being run directly from SSMS.  Is this a limitation of ADF? How can I fix this?</p>
","<sql><sql-server><azure><azure-data-factory>","2020-04-24 03:57:20","1774","2","1","61402335","<p>Internally the #temp table are created in tempdb , but then Linked service is pointing to a specified DB and so this error . </p>

<p>Please use the table variable @temp table and it will work just fine . </p>
"
"61396598","Does Azure Data Factory (ADF) support linked service Azure Cosmos DB (Table API)?","<p>Does Azure Data Factory (ADF) support linked service Azure Cosmos DB (Table API)? If not, is it possible to create linked service Azure Table Storage and provide connection string of Cosmos DB (Table API)?</p>

<p>Thank you!</p>
","<azure-data-factory>","2020-04-23 20:41:29","296","2","1","61400277","<p>Data Factory doesn't support Azure Cosmos DB (Table API), only support Azure Cosmos DB (SQL API):</p>

<p><a href=""https://i.stack.imgur.com/EnAKM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EnAKM.png"" alt=""enter image description here""></a></p>

<p>Please reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview#supported-data-stores"" rel=""nofollow noreferrer"">Supported data stores</a></p>

<p>When we create the linked Azure Table Storage, there is no way to provide the connection string of Cosmos DB (Table API).</p>

<p><a href=""https://i.stack.imgur.com/kYgN3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kYgN3.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61392714","Log Azure Data Factory Pipeline run events to Azure App Insights","<p>Is there a way to publish ADF pipeline run events with status to App Insights?</p>
","<azure-data-factory>","2020-04-23 16:54:22","1174","1","1","61393250","<p>Per my knowledge, you could use Web Activity in the ADF to invoke the Application Insights REST API after execution of your main activities(Or using Execute Pipeline Activity to execute your root pipeline and get the status or output of it).Then send it to App Insights REST API.</p>

<p>More details,please refer to this document:<a href=""https://www.ben-morris.com/using-azure-data-factory-with-the-application-insights-rest-api/"" rel=""nofollow noreferrer"">https://www.ben-morris.com/using-azure-data-factory-with-the-application-insights-rest-api/</a></p>
"
"61392394","Azure Cluster Startup time in Pipeline","<p>I have a pipeline that has 10 Dataflow activities and each uses AutoResolveIntegrationRuntime default integration cluster. </p>

<p>When I trigger the pipeline, cluster startup takes around 4 mins for each Dataflow totalling 40 mins to complete pipeline execution. Can I avoid this? If so, how?</p>

<p>Thanks,
Karthik</p>
","<azure><azure-pipelines><azure-data-factory>","2020-04-23 16:36:49","1603","2","1","61397721","<p>You will want to either put those data flows on your pipeline canvas without dependency lines so that they all run in parallel, or set a TTL in your Azure IR and use that same Azure IR for each activity. This way, each subsequent activity can use a warm pool and start-up in 1-2 mins instead of 4 mins.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview#azure-integration-runtime-and-data-flow-strategies"" rel=""nofollow noreferrer"">Here is an explanation of these different methods</a>.</p>

<p><a href=""https://www.youtube.com/watch?v=VT_2ZV3a7Fc"" rel=""nofollow noreferrer"">And here is how to configure TTL to set a warm pool for your factory</a>.</p>
"
"61381768","How to remove duplicates in a file using Azure Data Factory without using Dataflow or Databricks or Azure datalake analytics","<p>I am creating a data pipeline to copy data from one file to another. My input file has 4 columns and my output file has 2 columns. I want to copy only column 1 and column 3 of input file and store it in output file. Once that is copied I want to remove the duplicates from the output file. But I cannot use Dataflow or Databricks or Azure datalake analytics because I don't have compute in my setup.
Is there any way to do it without using compute ?</p>
","<azure><azure-data-factory>","2020-04-23 07:38:42","4380","1","2","61397753","<p>You need a compute environment for de-duping. There are pre-built patterns in the ADF pipeline gallery for distinct rows and de-dupe.</p>
"
"61381768","How to remove duplicates in a file using Azure Data Factory without using Dataflow or Databricks or Azure datalake analytics","<p>I am creating a data pipeline to copy data from one file to another. My input file has 4 columns and my output file has 2 columns. I want to copy only column 1 and column 3 of input file and store it in output file. Once that is copied I want to remove the duplicates from the output file. But I cannot use Dataflow or Databricks or Azure datalake analytics because I don't have compute in my setup.
Is there any way to do it without using compute ?</p>
","<azure><azure-data-factory>","2020-04-23 07:38:42","4380","1","2","61397947","<p>We have two ask , </p>

<ol>
<li>Copy selected columns : We can do that and select the columns two be copied  under ""mapping"" . Its possible .</li>
</ol>

<p>2.Removing duplicates : I think its a transformation and at this time , its not possible . </p>

<p>If you want to use Azure data lake analytics ( ADLA ) , I think you can achieve that by USQL .</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics</a></p>
"
"61377120","Execute a pipeline after a completion pipeline","<p>I have a first pipeline that ingest data for multiple country from BigQuery to Azure, it's an operation that copy bigquery transformed data into azure.</p>

<p>On Data Factory, i create multiple folders for each country that will have multiple pipeline, for example, a specific machine learning model only for 1 or 2 countries, a data prepration pipeline for an application for only 5 countries etc. </p>

<p>I think i need this folder construction for each market to keep it clear for anybody that needs to implement a pipeline and avoid errors.</p>

<p>My main problem by doing that is how i can call, for example, a machine learning pipeline in my folder UK that can only start after the first pipeline, the bigquery copy data to azure, completed ?</p>

<p>I can't call the Execution Pipeline activity because my first pipeline bigquerytoazure  is executed by himself, it's the very important step that needs to be executed before any other pipeline can be executed.</p>

<p>Is there any way to call completed pipeline without the Execution Pipeline activated ?</p>

<p>I thought about creating a dummy blob storage in the first pipeline that can work as a trigger for all pipeline after this first one ?</p>

<p>Thanks by advance, hope i was clear.</p>
","<azure-data-factory>","2020-04-23 00:02:24","259","0","1","61380262","<p>Data Factory event trigger based on the blob storage. I think that's the best way. </p>

<p>Another way you can think about using <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview"" rel=""nofollow noreferrer"">Logic App</a>, add a trigger to listen the BigQuery table in SQL database, if the BigQuery table modified, then execute a data factory pipeline. Create a work flow for the pipelines run.</p>

<p><a href=""https://i.stack.imgur.com/Knqlk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Knqlk.png"" alt=""enter image description here""></a></p>

<p>Work flow:</p>

<ol>
<li>SQL Server Trigger: when an item is modified.</li>
<li>Add a parallel branch</li>
<li>Data Factory Action: Get a pipeline run</li>
</ol>

<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sqlazure"" rel=""nofollow noreferrer"">Automate workflows for SQL Server or Azure SQL Database by using
    Azure Logic Apps</a></p>

<p>Hope this helps.</p>
"
"61368247","Concurrency in Azure DataFactory","<p>I have an Azure Data Factory Pipeline where it has 24 parallel jobs : databricks notebooks.
In the Pipeline configuration I set concurrency to 4 but when I run the Pipeline the 24 jobs start to run in parallel although I want just 4 of them to start running in the first place. 
According to the documentation <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-scheduling-and-execution.md"" rel=""nofollow noreferrer"">here</a> , I can run 4 parallel jobs and the others will be in queued status. 
am I missing another paramater to configure ?
<a href=""https://i.stack.imgur.com/5PXJv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5PXJv.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance :) </p>
","<concurrency><azure-data-factory><databricks><orchestration>","2020-04-22 14:57:17","661","0","1","61376101","<p>The link which you are following is for ADF v1 , it clearly mention's that on top of the doc . The feature which you are trying to use is for adf v2 .</p>

<p>""[!NOTE] This article applies to version 1 of Data Factory. If you are using the current version of the Data Factory service, see pipeline execution and triggers article.""</p>
"
"61363052","Can we hit a URL and check for the response as a job in Azure Data Factory?","<p>Say I have a URL like (<a href=""https://hello.world.com"" rel=""nofollow noreferrer"">https://hello.world.com</a>). Is it possible to hit this URL in Azure Data Factory and observe the response?</p>
","<azure><azure-data-factory><databricks><azure-databricks>","2020-04-22 10:37:58","146","0","1","61363875","<p>Yes, you can. You can use the <code>Web</code> activity.
Specify the URL you want and choose <code>Get</code> as method.
This will output the html content in the <em>response</em> json element.</p>

<p><a href=""https://i.stack.imgur.com/jVl0r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jVl0r.png"" alt=""enter image description here""></a></p>
"
"61360354","Azure Datafactory export ARM template via CLI/API/PowerShell","<p>Is there a way to export ARM template from particular (not collaboration) branch via CLI/API/PowerShell? </p>

<p>I haven't found it in docs, only UI way, but it doesn't feet my needs</p>
","<azure-data-factory>","2020-04-22 08:17:31","715","1","1","61375050","<p>Unfortunately this is not supported currently. Please feel free to comment and/or up-vote the below feature request shared by other users. Doing so will help to increase the priority of feature request. </p>

<p>ADF Feature Request link: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/35327899-add-powershell-cmdlet-support-for-import-export-ar"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/35327899-add-powershell-cmdlet-support-for-import-export-ar</a></p>
"
"61356373","For Each with internal activities Azure Datafactory","<p>Is there a way to send the number of times the cycle was executed?</p>

<p>that is, I have a <em>For Each</em> that executes an ExePipeline and it has 6 activities and only to the last activity I need to send it the number of times that the <em>for each</em> was executed.</p>

<p>at the end of <em>For Each</em> it shows how much data ""ItemsCount"" entered but I couldn't call that value in the last activity of the pipeline.</p>

<p>someone to help me thanks.</p>
","<azure><pipeline><azure-data-factory>","2020-04-22 02:38:32","79","0","1","61357946","<p>It may depend on what you're using for your items in the foreach activity.
But for example, if your foreach activity is looping over the content of a file that you have retrieved by a previous lookup activity - you can get the count of these items by for example:</p>

<pre><code>@activity('Lookup activity name').output.count
</code></pre>
"
"61351776","create a database from a copy C# application using Azure Sql","<p>I am trying to figure out the best way to create a database from a copy of a database. </p>

<p>The C# application has the user register to become a user of the website and once the registration is approved then it needs to look at the Main database and create a direct copy of it. So for this example, MainDB(name of the main database) has all of the schema needed to create a copy for the user. So when the user registers it would create UserDB (name of the copy database for user).</p>

<p>Right now I am doing this by the below code, </p>

<pre><code>        SqlConnection cn;
        SqlCommand command;
        SqlDataReader dataReader;

            cn.Open();
            string dbName = UserDB;
            command = new SqlCommand(""CREATE DATABASE "" + dbName + "" AS COPY OF MainDB"", cn);
            command.CommandTimeout = 120;
            dataReader = command.ExecuteReader();
            cn.Close();
</code></pre>

<p>This is working, but it is so unpredictable on how much time it takes to populate in azure, that I can not figure out a reasonable amount of time to wait before a timeout and throwing an error when creating a user for the new database.</p>

<p>I was wondering if there is a faster way to do this for a C# application. Possibly importing a bacpac file of the database into an Azure Data Factory, and creating the database from there? But I have not been able to find an example of this to run code using a C# application. I have just read that it reads faster, not sure if true.</p>

<p>Any help is much appreciated!</p>
","<c#><sql><azure><azure-sql-database><azure-data-factory>","2020-04-21 19:36:47","454","0","3","61418676","<p>If I understand the issue after looking at the code , you are creating a USERDB and want to have the schema and the data in the USERDB  from MASTERDB .</p>

<p>Well you can use a bacpac file ,  bacpac file is simply a ZIP file with the data for each schema . If I were you , I could have copied the files to a blob and let ADF copy the data to the USERDB . The advantage of doing this way is that we can implement parallelism in the copy activity and the USERDB is will be ready sooner . </p>
"
"61351776","create a database from a copy C# application using Azure Sql","<p>I am trying to figure out the best way to create a database from a copy of a database. </p>

<p>The C# application has the user register to become a user of the website and once the registration is approved then it needs to look at the Main database and create a direct copy of it. So for this example, MainDB(name of the main database) has all of the schema needed to create a copy for the user. So when the user registers it would create UserDB (name of the copy database for user).</p>

<p>Right now I am doing this by the below code, </p>

<pre><code>        SqlConnection cn;
        SqlCommand command;
        SqlDataReader dataReader;

            cn.Open();
            string dbName = UserDB;
            command = new SqlCommand(""CREATE DATABASE "" + dbName + "" AS COPY OF MainDB"", cn);
            command.CommandTimeout = 120;
            dataReader = command.ExecuteReader();
            cn.Close();
</code></pre>

<p>This is working, but it is so unpredictable on how much time it takes to populate in azure, that I can not figure out a reasonable amount of time to wait before a timeout and throwing an error when creating a user for the new database.</p>

<p>I was wondering if there is a faster way to do this for a C# application. Possibly importing a bacpac file of the database into an Azure Data Factory, and creating the database from there? But I have not been able to find an example of this to run code using a C# application. I have just read that it reads faster, not sure if true.</p>

<p>Any help is much appreciated!</p>
","<c#><sql><azure><azure-sql-database><azure-data-factory>","2020-04-21 19:36:47","454","0","3","61429215","<p>How up to date does the UserDB have to be?  I ask because if the performance is that unpredictable and responding immediately is important you’d consider running it in the background ahead of time so you have a recent DB you can use on standby.</p>

<p>Also is it the schema AND the data?  Because if it’s just the schema then copying whole database seems like overkill.</p>

<p>How big is the database right now?  Bacpac files work well for small databases, but once you’re in the range of a few hundred GB good luck </p>

<p>Happy to look further into this</p>
"
"61351776","create a database from a copy C# application using Azure Sql","<p>I am trying to figure out the best way to create a database from a copy of a database. </p>

<p>The C# application has the user register to become a user of the website and once the registration is approved then it needs to look at the Main database and create a direct copy of it. So for this example, MainDB(name of the main database) has all of the schema needed to create a copy for the user. So when the user registers it would create UserDB (name of the copy database for user).</p>

<p>Right now I am doing this by the below code, </p>

<pre><code>        SqlConnection cn;
        SqlCommand command;
        SqlDataReader dataReader;

            cn.Open();
            string dbName = UserDB;
            command = new SqlCommand(""CREATE DATABASE "" + dbName + "" AS COPY OF MainDB"", cn);
            command.CommandTimeout = 120;
            dataReader = command.ExecuteReader();
            cn.Close();
</code></pre>

<p>This is working, but it is so unpredictable on how much time it takes to populate in azure, that I can not figure out a reasonable amount of time to wait before a timeout and throwing an error when creating a user for the new database.</p>

<p>I was wondering if there is a faster way to do this for a C# application. Possibly importing a bacpac file of the database into an Azure Data Factory, and creating the database from there? But I have not been able to find an example of this to run code using a C# application. I have just read that it reads faster, not sure if true.</p>

<p>Any help is much appreciated!</p>
","<c#><sql><azure><azure-sql-database><azure-data-factory>","2020-04-21 19:36:47","454","0","3","64611610","<p>I'm not sure if recovering a database from bacpac will be significantly faster. But the time spread required for recovery should be less.</p>
<p>The following C # code will import the database:</p>
<pre><code>using Microsoft.SqlServer.Dac;

var services = new DacServices (connectionString);    
services.ImportBacpac (BacPackage.Load (fileName), restoredDatabaseName, cancellationToken);
</code></pre>
<p>Please note that the restored database will be restored at the default price tier. This can be very important if you are restoring several small databases, each of them will consume ~ 450 $.</p>
<p>With the help of the following sql command you can find out in what price tier the original database was.</p>
<pre><code>SELECT
    s.edition as [Edition],
    s.service_objective as [ServiceObjective],
    s.elastic_pool_name as [ElasticPoolName]
FROM
    sys.databases d
        JOIN sys.database_service_objectives s
            ON d.database_id = s.database_id
WHERE d.Name = @dbName
</code></pre>
<p>After importing the .bacpac, install the edition for the database:</p>
<pre><code>ALTER DATABASE [@dbName] MODIFY (EDITION = @ edition, SERVICE_OBJECTIVE = @ ServiceObjective)
</code></pre>
<p>or elastic pool:</p>
<pre><code>ALTER DATABASE [@dbName] MODIFY (SERVICE_OBJECTIVE = ELASTIC_POOL (name = [@elastic_pool_name]));
</code></pre>
"
"61350125","Unit Testing ADF Pipeline with SpecFlow","<p>I'm starting off with SpecFlow to test my Azure Data Factory pipelines. I'm using VS 2019 &amp; .NET Framework 4.7.2. When I try to run my test, it starts executing and then completes instantaneously. I've also observed that it doesn't hit the breakpoints which I've placed in the <code>feature file</code> as well as in the <code>binding</code>. This is my <code>packages.config</code> file:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;packages&gt;
  &lt;package id=""Microsoft.Azure.Management.DataFactory"" version=""4.8.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""Microsoft.IdentityModel.Clients.ActiveDirectory"" version=""5.2.7"" targetFramework=""net472"" /&gt;
  &lt;package id=""Microsoft.NET.Test.Sdk"" version=""15.0.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""Microsoft.Rest.ClientRuntime"" version=""2.3.20"" targetFramework=""net472"" /&gt;
  &lt;package id=""Microsoft.Rest.ClientRuntime.Azure"" version=""3.3.19"" targetFramework=""net472"" /&gt;
  &lt;package id=""Microsoft.TestPlatform.TestHost"" version=""15.0.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""MSTest.TestAdapter"" version=""1.3.2"" targetFramework=""net472"" /&gt;
  &lt;package id=""MSTest.TestFramework"" version=""1.3.2"" targetFramework=""net472"" /&gt;
  &lt;package id=""Newtonsoft.Json"" version=""10.0.3"" targetFramework=""net472"" /&gt;
  &lt;package id=""SpecFlow"" version=""2.4.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""SpecRun.Runner"" version=""3.2.22"" targetFramework=""net472"" /&gt;
  &lt;package id=""SpecRun.SpecFlow.2-4-0"" version=""3.2.22"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.IO"" version=""4.3.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.Net.Http"" version=""4.3.4"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.Runtime"" version=""4.3.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.Security.Cryptography.Algorithms"" version=""4.3.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.Security.Cryptography.Encoding"" version=""4.3.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.Security.Cryptography.Primitives"" version=""4.3.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.Security.Cryptography.X509Certificates"" version=""4.3.0"" targetFramework=""net472"" /&gt;
  &lt;package id=""System.ValueTuple"" version=""4.3.0"" targetFramework=""net472"" /&gt;
&lt;/packages&gt;
</code></pre>

<p>When I checked my log file, I could find this line in the log file that got generated: <code>No tests executed (activation needed)</code>. Can someone please tell me where I'm going wrong here? Note that my solution is building properly.</p>

<p>PFB the entire log for your reference:</p>

<pre><code>2020-04-21T23:25:19.8268118+05:30;VSTestExecutionThread;Info;SpecFlow+Runner execution started
2020-04-21T23:25:19.8268118+05:30;VSTestExecutionThread;Info;SpecFlow+Runner 3.2.22 in Framework clr40 in x86 mode execution started
2020-04-21T23:25:19.8268118+05:30;VSTestExecutionThread;Info;TestAdapter Location: C:\Users\ddc\source\repos\ADF.UnitTest\packages\SpecRun.Runner.3.2.22\tools\net45\TechTalk.SpecRun.VisualStudio.TestAdapter.dll
2020-04-21T23:25:20.0532065+05:30;VSTestExecutionThread;Info;SpecRun: running tests in C:\Users\ddc\source\repos\ADF.UnitTest\ADF.UnitTest\bin\Debug\ADF.UnitTest.dll
2020-04-21T23:25:20.0532065+05:30;VSTestExecutionThread;Info;Output folder configured to ""C:\Users\ddc\source\repos\ADF.UnitTest\TestResults"" (default)
2020-04-21T23:25:20.0542043+05:30;VSTestExecutionThread;Info;Profile: ADF.UnitTest
2020-04-21T23:25:20.1309989+05:30;VSTestExecutionThread;Info;Log file path: C:\Users\ddc\source\repos\ADF.UnitTest\TestResults\ADF.UnitTest_ADF.UnitTest_2020-04-21T232520.log
2020-04-21T23:25:20.1429665+05:30;Thread#14;Info;Checking activation
2020-04-21T23:25:21.6090483+05:30;Thread#14;Error;No tests executed (activation needed)
2020-04-21T23:25:21.6339842+05:30:Total: 0
2020-04-21T23:25:21.6369734+05:30:Succeeded: 0
2020-04-21T23:25:21.6369734+05:30:Ignored: 0
2020-04-21T23:25:21.6369734+05:30:Pending: 0
2020-04-21T23:25:21.6379706+05:30:Skipped: 0
2020-04-21T23:25:21.6379706+05:30:Failed: 0
2020-04-21T23:25:21.6449520+05:30;VSTestExecutionThread;Info;Adding attachments to VSTest
========== End of log file ==========
</code></pre>
","<c#><asp.net><specflow><azure-data-factory>","2020-04-21 18:04:58","780","0","1","61413239","<p>I had the same issue pop up when I updated my SpecFlow NuGet packages.  SpecFlow now requires you to setup an account via a Microsoft account.  </p>

<p>Check the Output from the tests in Visual Studio and there should be a link to activate an account.  See the documentation here: <a href=""https://specflow.org/2020/introducing-the-specflow-account/"" rel=""nofollow noreferrer"">https://specflow.org/2020/introducing-the-specflow-account/</a></p>
"
"61348713","find all nvarchar fields in database and do a replace(<field>,""CHAR(10)"",'') on them","<p>I get data through a xml file. I use a third party component for this.
(Zapsysys , I am in no way affiliated with them but maybe somebody knows their product)
The data in the XML looks like this:</p>

<pre><code>&lt;customer&gt;
""Johnny""
&lt;/customer&gt;
</code></pre>

<p>What I end up with in the table (customers) is a nvarchar (surname) with the following content:</p>

<pre><code>CHAR(10)JohnnyCHAR(10)
</code></pre>

<p>This is in every nvarchar field that gets read from the XML. The component actually does extract what it reads. But those chars mess up quite a lot statements.</p>

<p><code>select * from customers where surname = 'Johnny'</code> yields no results.</p>

<p><code>select * from customers where surname like '%Johnny%'</code> </p>

<p>or  </p>

<p><code>select * from customers where replace(surname,char(10),'') = 'Johnny</code> do. </p>

<p>Not very pretty.</p>

<p>One way to solve this is to use views with a sh*tload of replace statements.
But wouldn't it be great if I could run a procedure that wipes these CHAR(10) from every nvarchar field?</p>

<p>It must be possible to write an update statement that finds all the nvarchar fields and does a replace(,""CHAR(10)"",'') on them?</p>

<p>To be more clear: I do know how updatestatements work. I'm looking for a way to avoid writing update statements for each field in my db of type (n)varchar</p>

<p>update:</p>

<p>came up with this code after a suggestion from @matt (see answer marked as solution)</p>

<pre><code>declare @temptable table (id 

    int identity(1,1), sql nvarchar(4000))

        insert into @temptable(sql)
        SELECT 'UPDATE '+quotename(i.TABLE_SCHEMA)+'.'+quotename(i.TABLE_NAME) +' SET 
        '+quotename(i.COLUMN_NAME)+' = REPLACE('+quotename(i.COLUMN_NAME)+', CHAR(10),'''')'
        FROM INFORMATION_SCHEMA.COLUMNS  i
        inner join sys.tables t on i.TABLE_NAME = t.name
        WHERE DATA_TYPE = 'NVARCHAR' 
        and t.type = 'U'
        and TABLE_SCHEMA = 'myschema'


        declare @i as int = 1
        declare @sql as nvarchar(max)
        declare @max as int = (select max(id) from @temptable)

        while @i &lt;= @max
        BEGIN
        set @sql = (select [sql] from @temptable where id = @i)
        exec sp_executesql @sql
        --print cast(@i as varchar(5)) + '/'+cast(@max as varchar(5)) + ' done, ' +cast(@max-@i as varchar(5)) + ' to go...'
        set @sql = ''
        set @i = @i+1
        END
</code></pre>
","<tsql><ssis><azure-sql-database><azure-data-factory>","2020-04-21 16:47:24","156","0","3","61364609","<p>Sure, you could run an update on that surname field as part of your import. Something like this would work for you:</p>

<pre><code>UPDATE customers
SET surname = replace(surname,char(10),'')
</code></pre>

<p>Or you could use some dynamic SQL like this to generate up the update statements, you could quickly change this so that it executes:</p>

<pre><code>SELECT 'UPDATE '+TABLE_CATALOG+'.'+TABLE_SCHEMA+'.'+COLUMN_NAME+' SET 
'+COLUMN_NAME+' = REPLACE('+COLUMN_NAME+', CHAR(10),'''')'
FROM INFORMATION_SCHEMA.COLUMNS
WHERE DATA_TYPE = 'NVARCHAR'
</code></pre>
"
"61348713","find all nvarchar fields in database and do a replace(<field>,""CHAR(10)"",'') on them","<p>I get data through a xml file. I use a third party component for this.
(Zapsysys , I am in no way affiliated with them but maybe somebody knows their product)
The data in the XML looks like this:</p>

<pre><code>&lt;customer&gt;
""Johnny""
&lt;/customer&gt;
</code></pre>

<p>What I end up with in the table (customers) is a nvarchar (surname) with the following content:</p>

<pre><code>CHAR(10)JohnnyCHAR(10)
</code></pre>

<p>This is in every nvarchar field that gets read from the XML. The component actually does extract what it reads. But those chars mess up quite a lot statements.</p>

<p><code>select * from customers where surname = 'Johnny'</code> yields no results.</p>

<p><code>select * from customers where surname like '%Johnny%'</code> </p>

<p>or  </p>

<p><code>select * from customers where replace(surname,char(10),'') = 'Johnny</code> do. </p>

<p>Not very pretty.</p>

<p>One way to solve this is to use views with a sh*tload of replace statements.
But wouldn't it be great if I could run a procedure that wipes these CHAR(10) from every nvarchar field?</p>

<p>It must be possible to write an update statement that finds all the nvarchar fields and does a replace(,""CHAR(10)"",'') on them?</p>

<p>To be more clear: I do know how updatestatements work. I'm looking for a way to avoid writing update statements for each field in my db of type (n)varchar</p>

<p>update:</p>

<p>came up with this code after a suggestion from @matt (see answer marked as solution)</p>

<pre><code>declare @temptable table (id 

    int identity(1,1), sql nvarchar(4000))

        insert into @temptable(sql)
        SELECT 'UPDATE '+quotename(i.TABLE_SCHEMA)+'.'+quotename(i.TABLE_NAME) +' SET 
        '+quotename(i.COLUMN_NAME)+' = REPLACE('+quotename(i.COLUMN_NAME)+', CHAR(10),'''')'
        FROM INFORMATION_SCHEMA.COLUMNS  i
        inner join sys.tables t on i.TABLE_NAME = t.name
        WHERE DATA_TYPE = 'NVARCHAR' 
        and t.type = 'U'
        and TABLE_SCHEMA = 'myschema'


        declare @i as int = 1
        declare @sql as nvarchar(max)
        declare @max as int = (select max(id) from @temptable)

        while @i &lt;= @max
        BEGIN
        set @sql = (select [sql] from @temptable where id = @i)
        exec sp_executesql @sql
        --print cast(@i as varchar(5)) + '/'+cast(@max as varchar(5)) + ' done, ' +cast(@max-@i as varchar(5)) + ' to go...'
        set @sql = ''
        set @i = @i+1
        END
</code></pre>
","<tsql><ssis><azure-sql-database><azure-data-factory>","2020-04-21 16:47:24","156","0","3","61369744","<p>This should give you a list of columns to build a cursor around:</p>

<pre><code>select COLUMN_NAME
from INFORMATION_SCHEMA.COLUMNS
where DATA_TYPE in ('varchar','nvarchar')
    and TABLE_NAME = [your table name]
</code></pre>

<p>This one works much smoother.</p>
"
"61348713","find all nvarchar fields in database and do a replace(<field>,""CHAR(10)"",'') on them","<p>I get data through a xml file. I use a third party component for this.
(Zapsysys , I am in no way affiliated with them but maybe somebody knows their product)
The data in the XML looks like this:</p>

<pre><code>&lt;customer&gt;
""Johnny""
&lt;/customer&gt;
</code></pre>

<p>What I end up with in the table (customers) is a nvarchar (surname) with the following content:</p>

<pre><code>CHAR(10)JohnnyCHAR(10)
</code></pre>

<p>This is in every nvarchar field that gets read from the XML. The component actually does extract what it reads. But those chars mess up quite a lot statements.</p>

<p><code>select * from customers where surname = 'Johnny'</code> yields no results.</p>

<p><code>select * from customers where surname like '%Johnny%'</code> </p>

<p>or  </p>

<p><code>select * from customers where replace(surname,char(10),'') = 'Johnny</code> do. </p>

<p>Not very pretty.</p>

<p>One way to solve this is to use views with a sh*tload of replace statements.
But wouldn't it be great if I could run a procedure that wipes these CHAR(10) from every nvarchar field?</p>

<p>It must be possible to write an update statement that finds all the nvarchar fields and does a replace(,""CHAR(10)"",'') on them?</p>

<p>To be more clear: I do know how updatestatements work. I'm looking for a way to avoid writing update statements for each field in my db of type (n)varchar</p>

<p>update:</p>

<p>came up with this code after a suggestion from @matt (see answer marked as solution)</p>

<pre><code>declare @temptable table (id 

    int identity(1,1), sql nvarchar(4000))

        insert into @temptable(sql)
        SELECT 'UPDATE '+quotename(i.TABLE_SCHEMA)+'.'+quotename(i.TABLE_NAME) +' SET 
        '+quotename(i.COLUMN_NAME)+' = REPLACE('+quotename(i.COLUMN_NAME)+', CHAR(10),'''')'
        FROM INFORMATION_SCHEMA.COLUMNS  i
        inner join sys.tables t on i.TABLE_NAME = t.name
        WHERE DATA_TYPE = 'NVARCHAR' 
        and t.type = 'U'
        and TABLE_SCHEMA = 'myschema'


        declare @i as int = 1
        declare @sql as nvarchar(max)
        declare @max as int = (select max(id) from @temptable)

        while @i &lt;= @max
        BEGIN
        set @sql = (select [sql] from @temptable where id = @i)
        exec sp_executesql @sql
        --print cast(@i as varchar(5)) + '/'+cast(@max as varchar(5)) + ' done, ' +cast(@max-@i as varchar(5)) + ' to go...'
        set @sql = ''
        set @i = @i+1
        END
</code></pre>
","<tsql><ssis><azure-sql-database><azure-data-factory>","2020-04-21 16:47:24","156","0","3","61370857","<p>First you'll need a good <a href=""https://en.wikipedia.org/wiki/N-gram"" rel=""nofollow noreferrer"">N-Grams</a> function such as the one covered <a href=""https://www.sqlservercentral.com/articles/nasty-fast-n-grams-part-1-character-level-unigrams"" rel=""nofollow noreferrer"">here</a>. The version I am including below is the NVARCHAR(4000) version (Kudos to Larnu for his contribution.) I used NGramsN4K to build a NVARCHAR(4000) <a href=""https://www.sqlservercentral.com/scripts/patreplace8k"" rel=""nofollow noreferrer"">PatReplace</a> function. I use different schemas for my functions but dbo will work just fine. </p>

<p>Note that this: </p>

<pre><code>SELECT pr.NewString 
FROM   samd.patReplaceN4K('ൈൈƐABCƐƐ123ˬˬˬˬXYZˤˤ','[^0-9a-zA-Z]','') AS pr;
</code></pre>

<p><strong>Returns:</strong> ABC123XYZ</p>

<p>All characters that don't match this pattern: <code>[^0-9a-zA-Z]</code> have been excluded. Now let's use the function against a table with records containing bad characters, remove them, then join them to a table with good values. Note my comments. </p>

<pre><code>-- Sample data
DECLARE @Customers  TABLE (CustomerId INT IDENTITY, Surname NVARCHAR(100));
DECLARE @GoodValues TABLE (Surname NVARCHAR(100));

INSERT @Customers  (Surname) VALUES (CHAR(10)+'Johnny'+CHAR(10)),('Smith'),('Jones'+CHAR(160));
INSERT @goodvalues (Surname) VALUES('Johnny'),('Smith'),('Jones'),('James');

-- Fail:
SELECT c.CustomerId, g.Surname
FROM   @Customers  AS c
JOIN   @GoodValues AS g 
  ON   c.Surname = g.Surname;

-- Success:
SELECT c.CustomerId, g.Surname
FROM        @Customers  AS c
CROSS APPLY samd.patreplaceN4K(c.Surname,'[^0-9a-zA-Z ]','') AS pr
JOIN        @GoodValues AS g 
  ON        pr.newString = g.Surname;
</code></pre>

<p><strong>samd.NGramsN4K</strong></p>

<pre><code>CREATE FUNCTION samd.NGramsN4K
(
  @string NVARCHAR(4000), -- Input string
  @N      INT             -- requested token size
)
/*****************************************************************************************
[Purpose]:
 A character-level N-Grams function that outputs a contiguous stream of @N-sized tokens 
 based on an input string (@string). Accepts strings up to 4000 NVARCHAR characters long.
 For more information about N-Grams see: http://en.wikipedia.org/wiki/N-gram. 

[Author]:
  Alan Burstein

[Compatibility]:
 SQL Server 2008+, Azure SQL Database

[Syntax]:
--===== Autonomous
 SELECT ng.position, ng.token
 FROM   samd.NGramsN4K(@string,@N) AS ng;

--===== Against a table using APPLY
 SELECT      s.SomeID, ng.position, ng.token
 FROM        dbo.SomeTable                  AS s
 CROSS APPLY samd.NGramsN4K(s.SomeValue,@N) AS ng;

[Parameters]:
 @string  = The input string to split into tokens.
 @N       = The size of each token returned.

[Returns]:
 Position = bigint; the position of the token in the input string
 token    = NVARCHAR(4000); a @N-sized character-level N-Gram token

[Dependencies]:
 1. core.rangeAB (iTVF)

[Developer Notes]:
 1. NGramsN4K is not case sensitive

 2. Many functions that use NGramsN4K will see a huge performance gain when the optimizer
    creates a parallel execution plan. One way to get a parallel query plan (if the 
    optimizer does not chose one) is to use make_parallel by Adam Machanic which can be 
    found here:
 sqlblog.com/blogs/adam_machanic/archive/2013/07/11/next-level-parallel-plan-porcing.aspx

 3. When @N is less than 1 or greater than the datalength of the input string then no 
    tokens (rows) are returned. If either @string or @N are NULL no rows are returned.
    This is a debatable topic but the thinking behind this decision is that: because you
    can't split 'xxx' into 4-grams, you can't split a NULL value into unigrams and you 
    can't turn anything into NULL-grams, no rows should be returned.

    For people who would prefer that a NULL input forces the function to return a single
    NULL output you could add this code to the end of the function:

    UNION ALL 
    SELECT 1, NULL
    WHERE NOT(@N &gt; 0 AND @N &lt;= DATALENGTH(@string)) OR (@N IS NULL OR @string IS NULL);

 4. NGramsN4K is deterministic. For more about deterministic functions see:
    https://msdn.microsoft.com/en-us/library/ms178091.aspx

[Examples]:
--===== 1. Turn the string, 'ɰɰXɰɰ' into unigrams, bigrams and trigrams
 DECLARE @string NVARCHAR(4000) = N'ɰɰXɰɰ';
 BEGIN
   SELECT ng.Position, ng.Token FROM samd.NGramsN4K(@string,1) AS ng; -- unigrams (@N=1)
   SELECT ng.Position, ng.Token FROM samd.NGramsN4K(@string,2) AS ng; -- bigrams  (@N=2)
   SELECT ng.Position, ng.Token FROM samd.NGramsN4K(@string,3) AS ng; -- trigrams (@N=3)
   SELECT ng.Position, ng.Token FROM samd.NGramsN4K(@string,4) AS ng; -- 4-grams  (@N=4)
 END

--===== 2. Scenarios where the function would not return rows
 SELECT ng.Position, ng.Token FROM samd.NGramsN4K('abcd',5)   AS ng; -- 5-grams  (@N=5)
 SELECT ng.Position, ng.Token FROM samd.NGramsN4K(N'x', 0)    AS ng;
 SELECT ng.Position, ng.Token FROM samd.NGramsN4K(N'x', NULL) AS ng;

 This will fail:
 --SELECT ng.Position, ng.Token FROM samd.NGramsN4K(N'x',-1)    AS ng;

--===== 3. How many times the substring ""ƒƓ"" appears in each record
 BEGIN
   DECLARE @table TABLE(stringID int identity primary key, string NVARCHAR(100));
   INSERT @table(string)
   VALUES (N'ƒƓ123ƒƓ'),(N'123ƒƓƒƓƒƓ'),(N'!ƒƓ!ƒƓ!'),(N'ƒƓ-ƒƓ-ƒƓ-ƒƓ-ƒƓ');

   SELECT t.String, Occurances = COUNT(*) 
   FROM @table                            AS t
   CROSS APPLY samd.NGramsN4K(t.string,2) AS ng
   WHERE       ng.token = N'ƒƓ'
   GROUP BY    t.string;
 END;
-----------------------------------------------------------------------------------------
[Revision History]:
 Rev 00 - 20170324 - Initial Development - Alan Burstein
 Rev 01 - 20180829 - Changed TOP logic and startup-predicate logic in the WHERE clause
                   - Alan Burstein
 Rev 02 - 20191129 - Redesigned to leverage rangeAB - Alan Burstein
 Rev 03 - 20200416 - changed the cast from NCHAR(4000) to NVARCHAR(4000)
                   - Removed: WHERE @N BETWEEN 1 AND s.Ln; this must now be handled
                     manually moving forward. - Alan Burstein
*****************************************************************************************/
RETURNS TABLE WITH SCHEMABINDING AS RETURN
SELECT
  Position = r.RN,                                              -- Token Position
  Token    = CAST(SUBSTRING(@string,r.RN,@N) AS NVARCHAR(4000)) -- @N-Sized Token
FROM        (VALUES(DATALENGTH(ISNULL(NULLIF(@string,N''),N'X'))/2)) AS s(Ln)
CROSS APPLY core.rangeAB(1,s.Ln-(ISNULL(@N,1)-1),1,1)                AS r
GO
</code></pre>

<p><strong>samd.patReplaceN4K</strong></p>

<pre><code>CREATE FUNCTION samd.patReplaceN4K
(
  @string  NVARCHAR(4000), -- Input String
  @pattern NVARCHAR(50),   -- Pattern to match/replace
  @replace NVARCHAR(20)    -- What to replace the matched pattern with
)
/*****************************************************************************************
[Purpose]:
 Given a string (@string), a pattern (@pattern), and a replacement character (@replace)
 patReplaceN4K will replace any character in @string that matches the @Pattern parameter 
 with the character, @replace.

[Author]:
 Alan Burstein

[Compatibility]:
  SQL Server 2008+

[Syntax]:
--===== Basic Syntax Example
 SELECT pr.NewString
 FROM   samd.patReplaceN4K(@String,@Pattern,@Replace) AS pr;

[Parameters]:
 @string  = NVARCHAR(4000); The input string to manipulate
 @pattern = NVARCHAR(50);   The pattern to match/replace
 @replace = NVARCHAR(20);   What to replace the matched pattern with

[Returns]:
 Inline Table Valued Function returns:
 NewString = NVARCHAR(4000); The new string with all instances of @Pattern replaced with
             The value of @Replace.

[Dependencies]:
 core.ngramsN4k (ITVF)

[Developer Notes]:
 1. @Pattern IS case sensitive but can be easily modified to make it case insensitive
 2. There is no need to include the ""%"" before and/or after your pattern since since we 
    are evaluating each character individually
 3. Certain special characters, such as ""$"" and ""%"" need to be escaped with a ""/""
    like so: [/$/%]
 4. As is the case with functions which leverage samd.ngrams or samd.ngramsN4k, 
    samd.patReplaceN4K is almost always dramatically faster with a parallel execution
    plan. One way to get a parallel query plan (if the optimizer does not choose one) is
    to use make_parallel by Adam Machanic found here:
  sqlblog.com/blogs/adam_machanic/archive/2013/07/11/next-level-parallel-plan-porcing.aspx

    On my PC (8 logical CPU, 64GB RAM, SQL 2019) samd.patReplaceN4K is about 4X
    faster when executed using all 8 of my logical CPUs. 
 5. samd.patReplaceN4K is deterministic. For more about deterministic functions see:
    https://msdn.microsoft.com/en-us/library/ms178091.aspx

[Examples]:
--===== 1. Remove non alphanumeric characters
 SELECT pr.NewString 
 FROM   samd.patReplaceN4K('ൈൈƐABCƐƐ123ˬˬˬˬXYZˤˤ','[^0-9a-zA-Z]','') AS pr;

--===== 2. Replace numeric characters with a ""*""
 SELECT pr.NewString
 FROM  samd.patReplaceN4K('My phone number is 555-2211','[0-9]','*') AS pr;

--==== 3. Using againsts a table
 DECLARE @table TABLE(OldString varchar(60));
 INSERT  @table VALUES ('Call me at 555-222-6666'), ('phone number: (312)555-2323'),
                       ('He can be reached at 444.665.4466 on Monday.');

 SELECT      t.OldString, pr.NewString
 FROM        @table                                     AS t
 CROSS APPLY samd.patReplaceN4K(t.oldstring,'[0-9]','*') AS pr;

[Revision History]:
-----------------------------------------------------------------------------------------
Rev 01  - 20200422 - Created - Alan Burstein
*****************************************************************************************/
RETURNS TABLE WITH SCHEMABINDING AS RETURN
SELECT newString = 
(
  SELECT CASE WHEN @string = a.Blank THEN a.Blank ELSE
           CASE WHEN PATINDEX(@pattern,a.Token)&amp;0x01=0 THEN ng.token ELSE @replace END END
  FROM        samd.NGramsN4K(@string,1) AS ng
  CROSS APPLY (VALUES(CAST('' AS NVARCHAR(4000)),
                      ng.token COLLATE Latin1_General_BIN)) AS a(Blank,Token)
  ORDER BY ng.position
  FOR XML PATH(''),TYPE
).value('text()[1]', 'NVARCHAR(4000)');
GO
</code></pre>
"
"61345060","Stop Integration Runtimes in Data Factory","<p>I am figuring out to stop integration runtime in Data Factory; it is not terminated even after inactivity and is burning money.</p>

<p>Can someone suggest me anything?</p>

<p>Thanks in advance.</p>
","<azure><azure-data-factory>","2020-04-21 13:44:08","1797","1","1","61351734","<p>That is the default Azure IR that comes with every factory. It contains configuration information used by activities so that it can use the proper compute environment for copy, data flow, external activities, etc. You are only billed when you are executing those activities or using debug.</p>
"
"61341893","trigger in azure data factory that runs daily basis only between 8:30 and 6:00 with 30 minutes interval","<p>I need to create a trigger on daily basis between 8:30 and 6:00 with 30 minutes interval in azure data factory. can any of u please help me with steps to create?</p>
","<triggers><azure-data-factory>","2020-04-21 10:58:03","51","0","1","61356379","<p>Please follow my configuration as below:</p>

<p><a href=""https://i.stack.imgur.com/32bjd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/32bjd.png"" alt=""enter image description here""></a></p>

<p>It would be triggered every day at the specific time.</p>

<p>One little issue: It will be triggered at <code>18:30</code> because the limitation of configuration.You could just set a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">if-condition activity</a> before all of your stuffs to judge if it is <code>18:30</code>,then just leave the pipeline.</p>
"
"61336465","Filter csv file according to null values using azure data factory","<p>I am having a CSV file in blob now I wanted to push that CSV file into SQL table using azure data factory but want I want is to put a check condition on CSV data if any cell has null value so that row data will copy into an error table like for an example I have ID, name and contact column in CSV so for any record lets say contact is null(1, 'Gaurav', NULL) so in that case, this row will insert into an error table and if there is no null in the row then that row will go into the master table</p>

<p><strong>Note:</strong> As the sink is SQL on a VM so we can't create any this over there we have to handle this on data factory level only</p>
","<azure><csv><azure-data-factory>","2020-04-21 05:21:31","2961","1","1","61337563","<p>This can be done using a mapping data flow in ADF. One way of doing it is to use a derived column with an expression that that does the null check with for example the isNull() function. That way you can populated a new column with some value for the different cases, which you can then use in a conditional split to redirect the different streams to different sinks.</p>
"
"61336401","Merging two or more files from a storage account based on a column using Azure Data Factory","<p>I need to merge/concat two files which are present in my Azure Storage Account using an ADF pipeline.</p>

<p>There is a merge option in copy activity of azure, but this will merge similar files. The file has column with pipe separated data</p>

<p>My requirement is: Append the second file columns to the first file columns based on a particular ID</p>

<p>Example>></p>

<pre><code>File1
ID|Name|Age|XX|YY    
001|Abc|20|x1|y1
002|Dfg|30|x2|y2


File2
ID|AA|BB
001|a1|b1
002|a2|b2
</code></pre>

<p>Now the output of my ADF activity can be a different file or in file 1 or 2 with the below fomat</p>

<pre><code>OutputFile
ID|Name|Age|XX|YY|AA|BB
001|Abc|20|x1|y1|a1|b1
002|Dfg|30|x2|y2|a2|b2
</code></pre>

<p>Note: the above files were combined based on ID.</p>

<p>How can the same be achieved using adf pipeline activity?</p>

<p>As mentioned earlier, have tried the merge copy activity behavior. This will not satisfy my requirements.</p>

<p><a href=""https://stackoverflow.com/questions/56550727/azure-data-factory-how-to-merge-all-files-of-a-folder-into-one-file"">azure data factory: how to merge all files of a folder into one file</a></p>
","<azure><azure-storage><azure-pipelines><azure-data-factory>","2020-04-21 05:14:59","1037","0","2","61345507","<p>It sounds like the Merge Copy functions like a UNION in SQL, so it makes since this wouldn't work for your scenario. Given the constraints you've mentioned, specifically the lack of Data Flow, you're going to have to do some custom coding somewhere. I have two ideas on how I would tackle this problem.</p>

<ol>
<li><p>You mentioned in your comments that the original data files were pulled from SQL Server using Copy. That means (assuming they are from the same server) that you could solve the problem on the source side by writing a Stored Procedure that a) Unpivots the two tables into temp tables then b) Pivots the results into a staging table. That Staging table would then be the Source for your Copy activity. I would prefer this route because the SQL engine would be ideal for this kind of work: when all the data I'm operating on is in SQL Server, I try to put the work there as well.  Also, Stored Procedures are really easy to work with in Data Factory.</p>

<p><strong>1a</strong>. If you are against the Staging table concept, then your Stored Procedure could use Polybase to write the results out to Blob storage instead, in which case there would be no Copy activity in your pipeline. </p></li>
<li><p>If you must use the text files you previously produced, then an Azure Batch job with custom code to read the blobs, perform the work, and output the target blob is probably your best alternative. This would be my choice of last resort because I find Azure Batch tedious and difficult to work with, but sometimes only custom code will do.</p></li>
</ol>

<p>If you end up doing something else, I'd be very curious to learn how you solve this problem.</p>
"
"61336401","Merging two or more files from a storage account based on a column using Azure Data Factory","<p>I need to merge/concat two files which are present in my Azure Storage Account using an ADF pipeline.</p>

<p>There is a merge option in copy activity of azure, but this will merge similar files. The file has column with pipe separated data</p>

<p>My requirement is: Append the second file columns to the first file columns based on a particular ID</p>

<p>Example>></p>

<pre><code>File1
ID|Name|Age|XX|YY    
001|Abc|20|x1|y1
002|Dfg|30|x2|y2


File2
ID|AA|BB
001|a1|b1
002|a2|b2
</code></pre>

<p>Now the output of my ADF activity can be a different file or in file 1 or 2 with the below fomat</p>

<pre><code>OutputFile
ID|Name|Age|XX|YY|AA|BB
001|Abc|20|x1|y1|a1|b1
002|Dfg|30|x2|y2|a2|b2
</code></pre>

<p>Note: the above files were combined based on ID.</p>

<p>How can the same be achieved using adf pipeline activity?</p>

<p>As mentioned earlier, have tried the merge copy activity behavior. This will not satisfy my requirements.</p>

<p><a href=""https://stackoverflow.com/questions/56550727/azure-data-factory-how-to-merge-all-files-of-a-folder-into-one-file"">azure data factory: how to merge all files of a folder into one file</a></p>
","<azure><azure-storage><azure-pipelines><azure-data-factory>","2020-04-21 05:14:59","1037","0","2","61365292","<p>Though I was not able to find a solution using ADF, came up with a Common Table Expression(CTE) which will help me combine those in the db and then extract them to a file</p>

<p>Refer this for CTE - <a href=""https://www.geeksforgeeks.org/cte-in-sql/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/cte-in-sql/</a></p>
"
"61330766","Azure Data Factory vs Synapse Workspace pipelines","<p>Can anyone explain to me what the difference is between Azure Synapse Workspace Pipelines and Azure Data Factory? There seem to be a lot of overlap or perhaps as if Data Factory is becoming part of Synapse Workspace pipelines?</p>

<p>I am still trying to get some clarity on this.
Thanks
Mike</p>
","<azure><azure-data-factory><azure-synapse>","2020-04-20 19:44:53","805","0","1","61335954","<p>ADF is built into Azure Synapse Workspace. It surfaces as Pipelines and Data Flows.</p>
"
"61329833","How to pass a Data Flow Parameter in Key Column in Sink Tanformation while updating a data?","<p>I am implementing SCD Type2 through Data Flow. I having created a Parameter in it where I will pass a column name and this Parameter I am using in Sink Transformation in Key Column.</p>

<p><a href=""https://i.stack.imgur.com/7Zms8.png"" rel=""nofollow noreferrer"">Passing a parameter in Key Column in Data Flow</a> </p>

<p>I have selected the Add Dynamic Content and then Parameter, after that I selected the parameter I have created in Data Flow. Then it shows like <strong>""$Key_col""</strong>.
But when I run the pipeline it gives me an error- </p>

<p><strong>{""message"":""at Sink 'sink1'(Line 56/Col 6): Column operands are not allowed in literal expressions. Details:at Sink 'sink1'(Line 56/Col 6): Column operands are not allowed in literal expressions"",""failureType"":""UserError"",""target"":""Update_Existing_Records"",""errorCode"":""DFExecutorUserError""}</strong></p>

<p>Can anyone please tell me how resolve this error or any workaround for this Problem.</p>
","<azure><azure-sql-database><azure-blob-storage><azure-data-factory>","2020-04-20 18:50:04","5449","2","2","61334904","<p>Key column doesn't support set with parameter. You only can choose the exist column in sink.</p>

<p>The column name that you pick as the key here will be used by ADF as part of the subsequent update, upsert, delete. Therefore, you must pick a column that exists in the Sink mapping. If you wish to not write the value to this key column, then click ""Skip writing key columns"".</p>

<p>Please reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#mapping-data-flow-properties"" rel=""nofollow noreferrer"">Mapping data flow properties</a>.</p>

<p>The parameter <code>Key_col</code> is not exist in the sink, even if it has the same name.</p>

<p><strong>Update:</strong></p>

<p>Data Flow parameter:</p>

<p><a href=""https://i.stack.imgur.com/xX45D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xX45D.png"" alt=""enter image description here""></a></p>

<p>If we want to using update, we must add an Alter row active:</p>

<p><a href=""https://i.stack.imgur.com/ZNF5L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZNF5L.png"" alt=""enter image description here""></a></p>

<p>Sink, key column choose exist column 'name':</p>

<p><a href=""https://i.stack.imgur.com/eA6Oy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eA6Oy.png"" alt=""enter image description here""></a></p>

<p>Pipeline runs successful:</p>

<p><a href=""https://i.stack.imgur.com/G5W5p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G5W5p.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/igpPi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/igpPi.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61329833","How to pass a Data Flow Parameter in Key Column in Sink Tanformation while updating a data?","<p>I am implementing SCD Type2 through Data Flow. I having created a Parameter in it where I will pass a column name and this Parameter I am using in Sink Transformation in Key Column.</p>

<p><a href=""https://i.stack.imgur.com/7Zms8.png"" rel=""nofollow noreferrer"">Passing a parameter in Key Column in Data Flow</a> </p>

<p>I have selected the Add Dynamic Content and then Parameter, after that I selected the parameter I have created in Data Flow. Then it shows like <strong>""$Key_col""</strong>.
But when I run the pipeline it gives me an error- </p>

<p><strong>{""message"":""at Sink 'sink1'(Line 56/Col 6): Column operands are not allowed in literal expressions. Details:at Sink 'sink1'(Line 56/Col 6): Column operands are not allowed in literal expressions"",""failureType"":""UserError"",""target"":""Update_Existing_Records"",""errorCode"":""DFExecutorUserError""}</strong></p>

<p>Can anyone please tell me how resolve this error or any workaround for this Problem.</p>
","<azure><azure-sql-database><azure-blob-storage><azure-data-factory>","2020-04-20 18:50:04","5449","2","2","61335879","<p>Yes, this work. You just need to put single quotes around the parameter value like this:</p>

<p>""'$Key_col'""</p>

<p>I'm using double-quotes for string interpolation in this solution, so paste it in your expression exactly as that.</p>
"
"61327393","ADF run a pipeline taking parameter values from a file for each row","<p>I have requirement to bulk load few data files for different entities. Instead of creating separate pipelines for each entities I want to </p>

<ol>
<li><p>create a control file with columns - EntitiyName, SourceDirectory, SourceFileStartswith, TargetDirectory</p></li>
<li><p>for each row(each Entity) pipeline will be called where as other three values - SourceDirectory, SourceFileStartswith, TargetDirectory will be passed</p></li>
<li><p>data file will be picked as supplied SourceDirectory and SourceFileStartswith and to be loaded into path TargetDirectory </p></li>
</ol>

<p>Please guide how this will be created in ADF.</p>
","<parameters><azure-data-factory>","2020-04-20 16:29:36","199","1","1","61451366","<p>To achieve your requirement,please try ForEach Activity and Copy Activity.</p>

<p>Step1: Create an array parameter to pass the <code>EntitiyName, SourceDirectory, SourceFileStartswith, TargetDirectory</code> as json object in array.</p>

<p><a href=""https://i.stack.imgur.com/frQlD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/frQlD.png"" alt=""enter image description here""></a></p>

<p>Step2: Refer the parameter in ForEach Activity.</p>

<p><a href=""https://i.stack.imgur.com/FXrxb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FXrxb.png"" alt=""enter image description here""></a></p>

<p>Step3: Inside ForEach Activity,please use <code>@item().EntityName</code>,<code>@item().SourceDirectory</code> etc. in Copy Activity to configure the source and sink dataset.</p>
"
"61324338","ADF-Azure Data factory multiple wild card filtering","<p>I have a condition where i have more than 2 types of files which i have to filter out. I can filter out 1 type using wildcard, something like: *.csv but cant do something like *.xls, *.zip.</p>

<p>I have a pipeline which should convert csv, avro, dat files into .parquet format. But, folder also have .zip, excel, powerpoint files and i want them o be filtered out. Instead of using 3-4 activities i am finding if any way i can use (or) condition to filter out multiple extensions using wildcard option of data factory? </p>
","<file><pipeline><azure-data-factory>","2020-04-20 14:01:04","2311","1","1","61340164","<p>Dynamic content can't accept multiple <code>wildcards</code> or <code>Regular expression</code> based on my test.</p>

<p><a href=""https://i.stack.imgur.com/5rgLm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5rgLm.png"" alt=""enter image description here""></a></p>

<p>You have to using multiple activities to match the different types of your files.Or you could consider a workaround that using LookUp activity+For-each Activity.</p>

<p>1.LookUp Activity loads all the file names from specific folder.(Child Item)</p>

<p>2.Check the file format in the for-each activity condition.(using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#endswith"" rel=""nofollow noreferrer"">endswith</a> built-in feature)</p>

<p>3.If the file format matches the filter condition, then go into the <code>True</code> branch and configure it as dynamic path of dataset in the copy activity.</p>
"
"61323049","Azure Data Factory / Aggregate as comma separated string","<p>I want to aggregate by data using <code>Aggregate</code>. </p>

<p>Here is the scenario:</p>

<p>I've a table having values as below:</p>

<pre><code>Key | Value
1   | v1
1   | v2
2   | v1
2   | v3
</code></pre>

<p>After performing aggregation, I want output as below:</p>

<pre><code>Key | Value
1   | v1, v2
2   | v1, v3
</code></pre>

<p>I tried to find <code>String_Agg</code> function which is not available in ADF.</p>
","<azure><ssis><etl><azure-data-factory><data-transform>","2020-04-20 12:56:37","2644","2","1","61347355","<p>Faced the same issue recently. I ended up sinking the table, then creating a new dataflow that would query that table using SQL GROUP BY and STRING_AGG(). </p>

<p><strong>MAY 2020 update:</strong></p>

<p>Microsoft released a new feature - collect transformation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#collect"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#collect</a> 
It can be used in aggregation step and produce an array with the values you need. It can be then followed by the derived column step to cast the string array toString().</p>
"
"61322252","Avoiding long compute acquisition time for sequential dataflows","<p>I am experiencing very long compute acquisition times when running flows in sequence (7-8 minutes). I have a pipeline with several flows all running the same integration runtime with TTL = 15 minutes. It was my understanding that several flows executed one after the other and running on the same integration runtime would only incur long acquisition times for the first and not for subsequent flows, but I experience very sporadic behavior with subsequent flows sometimes spinning up very fast and other times much slower (3-8 minutes). How can this be avoided?</p>
","<azure-data-factory>","2020-04-20 12:12:53","74","0","1","61335777","<p>If you are using sequential data flow activity executions against the same Azure IR in the same data factory using a TTL, then you should see 1-3 minute startup times for each compute environment. If you are experiencing longer latencies, then please create a ticket on the Azure portal.  Make sure you are following the techniques described <a href=""https://www.youtube.com/watch?v=VT_2ZV3a7Fc"" rel=""nofollow noreferrer"">here</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview#azure-integration-runtime-and-data-flow-strategies"" rel=""nofollow noreferrer"">here</a>.</p>
"
"61320037","Understanding compute acquisition times across pipelines","<p>I am struggling to optimize my data factory pipeline to achieve as little time spent in spinning up compute for dataflows. </p>

<p>My understanding is that if we set up a runtime with a TTL of say 15 minutes, then all subsequent flows executed in a sequence following this should experience very short compute acquisition times, but does this also hold true, when switching from one pipeline to the other - in the image below, would flow 3 utilize that the runtime was already spun up in flow 1? I ask because I see very sporadic behavior.</p>

<p><a href=""https://i.stack.imgur.com/kuPPl.png"" rel=""nofollow noreferrer"">Pipeline example</a></p>
","<azure-data-factory>","2020-04-20 10:16:53","34","0","1","61335728","<p>If you are using the same Azure IR inside of the same factory, yes. However, the activities must be executed sequentially, otherwise, ADF will spin-up another pool for you. That's because Databricks parallel job executions are not supported in job clusters. I describe the techniques in <a href=""https://www.youtube.com/watch?v=VT_2ZV3a7Fc"" rel=""nofollow noreferrer"">this video</a> and in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview#azure-integration-runtime-and-data-flow-strategies"" rel=""nofollow noreferrer"">this document</a>.</p>
"
"61319477","Cannot see Author & Monitor card in the Azure Data Factory Overiew page","<p>I have created Azure Data Factory account and i can't see author &amp; monitor option where we can do all the ETL stuff.</p>

<p>Please help !</p>

<p>Regards,
Aditya</p>
","<azure><azure-data-factory>","2020-04-20 09:45:53","304","-2","2","61323568","<p>If the portal doesnt work for you, you can always go to adf.azure.com and login there. You should see your newly created data factory and start working on it!</p>

<p>Hope this helped!</p>
"
"61319477","Cannot see Author & Monitor card in the Azure Data Factory Overiew page","<p>I have created Azure Data Factory account and i can't see author &amp; monitor option where we can do all the ETL stuff.</p>

<p>Please help !</p>

<p>Regards,
Aditya</p>
","<azure><azure-data-factory>","2020-04-20 09:45:53","304","-2","2","61345474","<p>Actually it worked when i change the resource group region to Central US</p>
"
"61313174","Any workaround for Azure Data Factory not supporting complex ORC type (list, map)","<p>I am using ADF (Apr 2020) and trying to</p>

<ul>
<li>Copy Data from Blob store to SQL Database</li>
<li>The Blob store (source) have snappy compressed ORC files.</li>
<li>Few fields in ORC schema are List type.</li>
</ul>

<p>The ""Copy Data"" pipelines complains that complex types (List, Map) cannot be handled.</p>

<p>Is this is documented behavior (not supporting complex type in ORC) from ADF? Or, I am doing something wrong.</p>

<p>Is there any preferred workaround?</p>
","<azure><azure-data-factory><orc>","2020-04-19 23:49:42","761","0","1","61315322","<blockquote>
  <p>Complex data types are not supported (STRUCT, MAP, LIST, UNION).</p>
</blockquote>

<p>It is not supported according to the documentation that can be found here:
<a href=""https://learn.microsoft.com/bs-cyrl-ba/azure/data-factory/format-orc"" rel=""nofollow noreferrer"">https://learn.microsoft.com/bs-cyrl-ba/azure/data-factory/format-orc</a> </p>
"
"61311288","Rule-based mapping on Copy Activity in Azure Data Factory","<p>I'm trying to create a dynamic mapping when I use copy data activity on Azure Data Factory.
I want to create a parquet file that contains the same data that I'm reading from the source but I want to modfy some columns names to remove white spaces on it (<a href=""https://stackoverflow.com/questions/59044405/azure-data-factory-ms-access-as-source-database-error"">It's a bug of Parquet format</a>) and I want to do that automatically.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern"" rel=""nofollow noreferrer"">I have seen that this is possible in mapping data flow</a>, but I don't see any such functionality on Copy Activity (Mapping data flow is limited to a few connectors as a source, so I can't use it).</p>

<p>As you can see on the image, it seems that I can only modify individual columns, not a few of them that fullfil certain conditions</p>

<p><a href=""https://i.stack.imgur.com/ORuBd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ORuBd.png"" alt=""enter image description here""></a></p>

<p>How can I do that?</p>

<p>Thanks in advance</p>
","<parquet><azure-data-factory>","2020-04-19 20:35:10","1066","1","2","61319286","<p>The Copy activity can change from one file type to another, eg csv to json, parquet to database but it does not inherently allow any transform, such as changing the content of any columns, even adding additional columns.</p>

<p>Alternately consider using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"" rel=""nofollow noreferrer"">ADF to call a Databricks notebook</a> for these complex rule-based transforms.</p>
"
"61311288","Rule-based mapping on Copy Activity in Azure Data Factory","<p>I'm trying to create a dynamic mapping when I use copy data activity on Azure Data Factory.
I want to create a parquet file that contains the same data that I'm reading from the source but I want to modfy some columns names to remove white spaces on it (<a href=""https://stackoverflow.com/questions/59044405/azure-data-factory-ms-access-as-source-database-error"">It's a bug of Parquet format</a>) and I want to do that automatically.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern"" rel=""nofollow noreferrer"">I have seen that this is possible in mapping data flow</a>, but I don't see any such functionality on Copy Activity (Mapping data flow is limited to a few connectors as a source, so I can't use it).</p>

<p>As you can see on the image, it seems that I can only modify individual columns, not a few of them that fullfil certain conditions</p>

<p><a href=""https://i.stack.imgur.com/ORuBd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ORuBd.png"" alt=""enter image description here""></a></p>

<p>How can I do that?</p>

<p>Thanks in advance</p>
","<parquet><azure-data-factory>","2020-04-19 20:35:10","1066","1","2","67299000","<p>Here is a solution to apply a dynamic column name mapping with ADF so that you can still use the copy data activities with parquet format, even when the source column names have pesky white-space characters which are not supported.</p>
<p>The solution involves three parts:</p>
<ol>
<li>Dynamically generate your list of mapped column names. The example below demonstrates how you could encode the white-space from an SQL database table source dataset dynamically with a lookup activity (referred to as 'lookup column mapping' below).</li>
</ol>
<pre><code>;with cols as (
select 
    REPLACE(column_name, (' ', '__wspc__') as new_name,
    column_name as old_name
from INFORMATION_SCHEMA.columns
where table_name = '@{pipeline().parameters.SOURCE_TABLE}'
and table_schema = '@{pipeline().parameters.SOURCE_SCHEMA}'
)
select ' |||'+old_name+'||'+new_name+'|' as mapping
from cols;
</code></pre>
<ol start=""2"">
<li>Use an expression to repack the column mapping derived in the lookup activity in step 1. into the json syntax expected by the copy data activity template. You can insert this into a set variable activity with Array type variable (referred to as 'column_mapping_list' below).</li>
</ol>
<pre><code>@json(
    concat(
        '[ ',
        join(
            split(
                join(
                    split(
                        join(
                            split(
                                join(
                                    xpath(
                                        xml(
                                            json(
                                                concat( 
                                                '{\&quot;root_xml_node\&quot;: ', 
                                                    string(activity('lookup column mapping').output),
                                                '}' 
                                                )
                                            )
                                        ),
                                    '/root_xml_node/value/mapping/text()',
                                    )
                                ','
                                ),
                            '|||'
                            ),
                        '{\&quot;source\&quot;: { \&quot;name\&quot;: \&quot;'
                        ), 
                    '||'
                    ), 
                '\&quot; },\&quot;sink\&quot;: { \&quot;name\&quot;: \&quot;'
                ),
            '|'
            ), 
        '\&quot; }}'
        ),  
    ' ]'
    )
)
</code></pre>
<p>Unfortunately the expression is more convoluted than we would like as the xpath function requires a single root node which is not provided by the lookup activity output, and the string escaping of the ADF json templates present some challenges to simplifying this.</p>
<ol start=""3"">
<li>Lastly, use the column mapping list variable as &quot;dynamic content&quot; in the mapping section of the copy data activity with the following expression</li>
</ol>
<pre><code>@json(
    concat(
        '{ \&quot;type\&quot;: \&quot;TabularTranslator\&quot;, \&quot;mappings\&quot;:',
        string(variables('column_mapping_list')),
    '}'
    )
)
</code></pre>
<p>Expected results:<br />
<strong>Step 1.</strong><br />
'my wspccol' -&gt; '|||my wspccol||my__wspc__wspcol|'</p>
<p><strong>Step 2.</strong><br />
'|||my wspccol||my__wspc__wspccol|' -&gt; ['{ &quot;source&quot;: { &quot;name&quot;: &quot;my wspccol&quot; }, &quot;sink&quot;: { &quot;name&quot;: &quot;my__wspc__wspccol&quot; } }']</p>
<h2><strong>Step 3.</strong></h2>
<pre><code>{ 
    &quot;type&quot;: &quot;TabularTranslator&quot;, 
    &quot;mappings&quot;: [
        { 
            &quot;source&quot;: { &quot;name&quot;: &quot;my wspccol&quot; }, 
            &quot;sink&quot;: { &quot;name&quot;: &quot;my__wspc__wspccol&quot; } 
        }
    ] 
}
</code></pre>
<p>Additionally:</p>
<ul>
<li>Keep in mind that the solution can be as easily reversed, so that if you want to load that parquet file back into an SQL table with the original column names then you can use the same expressions to build your dynamic copy data mapping; Just switch over the old_names, new_names in step 1. to map back to the original names.</li>
<li>A data type can also by specified in the mapping where needed. Adjust the syntax accordingly, following the documentation here: <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.tabulartranslator.mappings?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.tabulartranslator.mappings?view=azure-dotnet</a></li>
</ul>
"
"61307430","Parquet file name in Azure Data Factory","<p>I'm copying data from an Oracle DB to ADLS using a copy activity of  Azure Data Factory. 
The result of this copy is a parquet file that contains the same data of the 
table that I have copied but the name of this resultant parquet file is like this:</p>

<pre><code>data_32ecaf24-00fd-42d4-9bcb-8bb6780ae152_7742c97c-4a89-4133-93ea-af2eb7b7083f.parquet
</code></pre>

<p>And I need that this name is stored like this: </p>

<p><code>TableName-Timestamp.parquet</code> </p>

<p>How can I do that with Azure Data Factory? </p>

<p>Another question: Is there a way to add hierarchy when this file is being written? For example, I use the same 
pipeline for writting several tables and I want to create a new folder for each table. I can do that if I create a new Dataset for each table 
to write, but I want to know if is there a way to do that automatically (Using dynamic content).</p>

<p>Thanks in advance.</p>
","<parquet><azure-data-factory>","2020-04-19 16:02:10","1672","0","1","61314282","<p>You could set a pipeline parameter to achieve it.</p>

<p>Here's the  example I tried copy data from Azure SQL database to ADLS,  it also should works for oracle to ADLS.</p>

<p><strong>Set pipeline parameter:</strong> set the Azure SQL/Oracle table name which need to copy to ADLS: </p>

<p><a href=""https://i.stack.imgur.com/UxXMr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UxXMr.png"" alt=""enter image description here""></a></p>

<p><strong>Source dataset:</strong> </p>

<p>Add dynamic content to set table name:</p>

<p><a href=""https://i.stack.imgur.com/qBnHT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qBnHT.png"" alt=""enter image description here""></a></p>

<p><strong>Source:</strong> </p>

<p>Add dynamic content: set table name with pipeline parameter:
<a href=""https://i.stack.imgur.com/V8Hmv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V8Hmv.png"" alt=""enter image description here""></a></p>

<p><strong>Sink dataset:</strong></p>

<p>Add dynamic content to set Parquet file name:</p>

<p><a href=""https://i.stack.imgur.com/SiTMb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SiTMb.png"" alt=""enter image description here""></a></p>

<p><strong>Sink:</strong></p>

<p>Add dynamic content to set Parquet file name with pipeline parameter:</p>

<p>Format: TableName-Timestamp.parquet:</p>

<pre><code>@concat(pipeline().parameters.tablename,'-',utcnow())
</code></pre>

<p>Then execute the pipeline, you will get the Parquet file like <code>TableName-Timestamp.parquet</code>:</p>

<p>About your another question:</p>

<p>You could add dynamic content set folder name for each table, just follow this:</p>

<p><a href=""https://i.stack.imgur.com/6jh0X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6jh0X.png"" alt=""enter image description here""></a></p>

<p>For example, if we copy the table ""test"", the result we will get:</p>

<pre><code>container/test/test-2020-04-20T02:01:36.3679489Z.parquet
</code></pre>

<p>Hope this helps.</p>
"
"61282551","data factory linked services common data services","<p>i am trying to connect to the common data services by using my Office 365 account and it is not successful. here is the error:</p>

<blockquote>
  <p>Unable to Login to Dynamics CRM: Invalid Login Information : An
  unsecured or incorrectly secured fault was received from the other
  party. See the inner FaultException for the fault code and detail. An
  unsecured or incorrectly secured fault was received from the other
  party. See the inner FaultException for the fault code and detail. =>
  Authentication FailureUnable to Login to Dynamics CRM Unable to Login
  to Dynamics CRM The creator of this fault did not specify a Reason.</p>
</blockquote>

<p>I can use Power BI common data services connection and successfully connect to the model and pick the entities however it is not successful in data factory!</p>

<p>I have googled thew error and as you can guess there might be whole range of issues but mostly talking about the time difference. I tried to use the on-Prem Runtime to see if it makes any difference but still no hope.</p>

<p>PS: My server is ""crm6.dynamics.com""</p>
","<dynamics-crm><azure-data-factory><common-data-service>","2020-04-17 23:59:14","272","0","1","61320083","<p>did you try visiting home.dynamics.com and from there on choose custom app. This shall land you on Dynamics. Also can you visit admin.powerplatform.com and see there atleast 2 env, One as default and other as your CRM. Remember default is not your crm.</p>
"
"61280612","Disaster recovery set up for Azure Data Factory service","<p>I just added azure data factory service to my subscription. During the setup I was able to select only one region, what happens if disaster happens in this region? How does ADF guarantees high availability?
Do we need to wait till recovery or is there any similar setup like in ADLS2(GRS &amp; RA-GRS).</p>
","<azure><azure-data-factory>","2020-04-17 21:02:08","3006","2","1","61316856","<p>No statements of Disaster Recovery could be found in the ADF official document.Based on my researching,ADF only provides cloud-based data integration work flow, the DR is affected by the supported data stores in ADF actually. I provide some clues for your reference:</p>

<p>1.The statement of Location option when you create ADF:</p>

<p><a href=""https://i.stack.imgur.com/rNvnc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rNvnc.png"" alt=""enter image description here""></a></p>

<p>2.High availability for Azure Integration Runtime,it is affected by DU setting(allocation of compute resources):<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#data-integration-units"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#data-integration-units</a></p>

<p>3.High availability for Self-Hosted Integration Runtime,it could be better if you create multiple nodes in  the on-premise environment:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability</a></p>
"
"61276347","Renaming existing Azue Data Factory","<p>I would like to move my existing Azure Data Factory from one resource group to another but with new name. </p>

<p>Is there a way to change ADF name before moving or post moving. This is required to adhere to the azure resource naming policy enforced at account level.</p>
","<azure><azure-data-factory>","2020-04-17 16:30:34","2716","4","4","61276480","<p>I don't think it is possible to rename an existing Azure Data Factory.</p>

<p>However an alternate solution would be to copy or clone an Azure Data Factory. You can find more information about copy/clone Azure Data Factory here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-clone-data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-clone-data-factory</a>.</p>

<p>One of the use cases mentioned in this link is exactly that:</p>

<blockquote>
  <p><strong>Renaming resources</strong>. Azure doesn't support renaming resources. If you want to rename a data factory, you can clone the data factory with
  a different name, and then delete the existing one.</p>
</blockquote>
"
"61276347","Renaming existing Azue Data Factory","<p>I would like to move my existing Azure Data Factory from one resource group to another but with new name. </p>

<p>Is there a way to change ADF name before moving or post moving. This is required to adhere to the azure resource naming policy enforced at account level.</p>
","<azure><azure-data-factory>","2020-04-17 16:30:34","2716","4","4","61276483","<p>You can't change the unique name given to an Azure resource, though you <em>can</em> move something to a different resource group.</p>

<p>To get a new name, you'd need to redeploy your Data Factory assets (pipelines, etc) to a new Azure Data Factory.</p>
"
"61276347","Renaming existing Azue Data Factory","<p>I would like to move my existing Azure Data Factory from one resource group to another but with new name. </p>

<p>Is there a way to change ADF name before moving or post moving. This is required to adhere to the azure resource naming policy enforced at account level.</p>
","<azure><azure-data-factory>","2020-04-17 16:30:34","2716","4","4","61276918","<p>Just to add to existing answers: that is not true or not entirely true. ARM supports renaming resources (just create an <strong>application insights</strong> resource and test that). But the resource providers (most of them anyway) dont support that.</p>

<p>Azure Data Factory doesnt support renames.</p>
"
"61276347","Renaming existing Azue Data Factory","<p>I would like to move my existing Azure Data Factory from one resource group to another but with new name. </p>

<p>Is there a way to change ADF name before moving or post moving. This is required to adhere to the azure resource naming policy enforced at account level.</p>
","<azure><azure-data-factory>","2020-04-17 16:30:34","2716","4","4","68811418","<p>If you have not done so already add your current Data Factory to Git Repo eg MyOldDataFactory.
Clone the repo to a new repo myNewDataFactory
Create a new DataFactory with the correct name where ever you need it and hook it up to the cloned Git Repo myNewDataFactory repo.</p>
<p>You will need to fix all the links to Key Vaults and Shared Integration services etc but at least you will get all of your code and changes you make to fix your New Data Factory are save in your new repo and do not corrupt your old Git repo or old Data Factory.</p>
"
"61272370","SFTP Connection customization / parameterization","<p>I want to have a pipeline that read some parameters from database and pass those to the dataSet and then to the linked service. The scenario is a sFTP connection to pickup files and I want to be able to pass the values for the connection. </p>

<p>I posted a question in azure-docs and somehow I was able to go only half of the way. - <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/52385"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/52385</a></p>

<p>I was able to put parameters in the SFTP connection (and executing the test, passing the values for the parameteres, works fine). The problem is when I try to use that connection in a dataset. Using a csv dataset, I don't get the option to pass to the linked service the connection details/parameters.</p>

<ul>
<li>Host </li>
<li>Port </li>
<li>User name </li>
<li>Password (already allows to retrieve from AKV)</li>
</ul>

<p>Any help would be appreciated,
Thanks in advance,
Manuel</p>
","<azure><azure-data-factory>","2020-04-17 13:03:59","809","0","1","61283449","<p>You can create a parameterized SFTP linked service by using below JSON code. This sample has parameterized Host, Port, UserName properties. </p>

<pre><code>{
    ""name"": ""Sftp_LinkedServiceParameterized"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""parameters"": {
            ""HostParameter"": {
                ""type"": ""string"",
                ""defaultValue"": ""defaultValue""
            },
            ""PortParameter"": {
                ""type"": ""string"",
                ""defaultValue"": ""defaultValue""
            },
            ""UserNameParameter"": {
                ""type"": ""string"",
                ""defaultValue"": ""defaultValue""
            }
        },
        ""annotations"": [],
        ""type"": ""Sftp"",
        ""typeProperties"": {
            ""host"": ""@{linkedService().HostParameter}"",
            ""port"": ""@{linkedService().PortParameter}"",
            ""skipHostKeyValidation"": true,
            ""authenticationType"": ""Basic"",
            ""userName"": ""@{linkedService().UserNameParameter}"",
            ""encryptedCredential"": ""XXXXXXXXXXXXXXXXXXXencryptedCredentialXXXXXXXXXXXXXXXXXXXXXXXXX""
        }
    }
}
</code></pre>

<p>After creating the linked service, publish it first and then refresh. Now create a data set in which you want to use this Linked service. You will have to create dataset parameters for Host, Port and UserName properties and map them to Linked service properties in the Dataset connection tab as below.</p>

<p>Below sample shows input pipeline parameters (Host, Port, UserName):  </p>

<p><a href=""https://i.stack.imgur.com/iflTF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iflTF.png"" alt=""enter image description here""></a></p>

<p>Below image show dataset parameters (Host, Port, UserName)</p>

<p><a href=""https://i.stack.imgur.com/6Orkg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Orkg.png"" alt=""enter image description here""></a></p>

<p>Below image show how to map Dataset parameters to Linked Service parameters (Host, Port, UserName)</p>

<p><a href=""https://i.stack.imgur.com/Bgv9l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bgv9l.png"" alt=""enter image description here""></a></p>

<p>Below image shows how to map input pipeline parameters to dataset parameters.</p>

<p><a href=""https://i.stack.imgur.com/tJDD0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tJDD0.png"" alt=""enter image description here""></a></p>

<p>This way you will be able to pass Linked service parameter values from <code>source/pipeline parameters -&gt; dataset parameters -&gt; linked service parameters</code></p>

<p><a href=""https://i.stack.imgur.com/vYJY0.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vYJY0.gif"" alt=""enter image description here""></a></p>

<p>Hope this helps. </p>
"
"61270787","Azure Datafactory Pipeline Failed inside a scheduled trigger","<p>I have created 2 pipeline in Azure Datafactory. We have a custom activity created to run a python script  inside the pipeline.When the pipeline is executed manually it successfully run for n number of time.But i have created a scheduled trigger of an interval of 15 minutes in order to run the 2 pipelines.The first execution successfully runs but in the next interval i am getting the error ""Operation on target PyScript failed: Hit unexpected exception and execution failed."" we are blocked wiht this.any input on this would be really helpful.</p>
","<azure><azure-pipelines><azure-data-factory><azure-triggers>","2020-04-17 11:41:56","553","0","1","61279768","<p>from <a href=""https://learn.microsoft.com/azure/data-factory/data-factory-troubleshoot-guide#custom"" rel=""nofollow noreferrer"">ADF troubleshooting guide</a>, it states...</p>

<p><strong>Custom Activity :</strong></p>

<p>The following table applies to Azure Batch.</p>

<p><strong>Error code</strong>: 2500</p>

<p><strong>Message</strong>: Hit unexpected exception and execution failed.</p>

<p><strong>Cause</strong>: Can't launch command, or the program returned an error code.</p>

<p><strong>Recommendation</strong>: Ensure that the executable file exists. If the program started, make sure stdout.txt and stderr.txt were uploaded to the storage account. It's a good practice to emit copious logs in your code for debugging.</p>

<p><strong>Related helpful doc</strong>: <a href=""https://learn.microsoft.com/azure/batch/tutorial-run-python-batch-azure-data-factory"" rel=""nofollow noreferrer"">Tutorial: Run Python scripts through Azure Data Factory using Azure Batch</a> </p>

<p>Hope this helps.</p>

<p>If you are still blocked, please share failed pipeline run ID &amp; failed activity run ID, for further analysis.</p>
"
"61269854","How to explicitly fail azure data factory pipeline?","<p>Is there any method to explicitly fail an azure data factory pipeline?</p>
","<azure-data-factory>","2020-04-17 10:52:05","2816","2","3","61276970","<p>If you would like to fail your pipeline explicitly, one possible way is to have an invalid URL in your web activity which will fail the Web activity, which inturn will result in your pipeline to fail.</p>

<p>There is an existing feature request related to the same requirement in ADF user voice forum suggested by other ADF users. I would recommend you please up-vote and/or comment on this feedback which will help to increase the priority of the feature request implementation.</p>

<p>ADF User voice feedback related to this requirement: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/38143873-a-new-activity-for-cancelling-the-pipeline-executi"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/38143873-a-new-activity-for-cancelling-the-pipeline-executi</a></p>

<p><strong>Additional info</strong>:
In case if you just want to cancel your pipeline run then you can have a Web activity which calls the below REST API to cancel the pipeline run by using the pipelinerunID (you can get this value by using dynamic expression - @pipeline().RunId)</p>

<p>REST API to Cancel the Pipeline Run: POST <a href=""https://management.azure.com/subscriptions/%7BsubscriptionId%7D/resourceGroups/%7BresourceGroupName%7D/providers/Microsoft.DataFactory/factories/%7BfactoryName%7D/pipelineruns/%7BrunId%7D/cancel?api-version=2018-06-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelineruns/{runId}/cancel?api-version=2018-06-01</a></p>

<p>MS Doc related to Rest API: <a href=""https://learn.microsoft.com/rest/api/datafactory/pipelineruns/cancel"" rel=""nofollow noreferrer"">ADF Pipeline Runs - Cancel</a></p>

<p>Hope this helps.</p>
"
"61269854","How to explicitly fail azure data factory pipeline?","<p>Is there any method to explicitly fail an azure data factory pipeline?</p>
","<azure-data-factory>","2020-04-17 10:52:05","2816","2","3","61303109","<p>It depends at which level you would like to stop the operation. For example at MSSQL DB operation, from SP you may RAISERROR above level 18 to raise an exception. As already stated incase of web Activity, request a non existent URL. That would also stop the process</p>
"
"61269854","How to explicitly fail azure data factory pipeline?","<p>Is there any method to explicitly fail an azure data factory pipeline?</p>
","<azure-data-factory>","2020-04-17 10:52:05","2816","2","3","69212435","<p>Just set a variable with <code>@string(div(1, 0))</code></p>
<p><a href=""https://i.stack.imgur.com/jVLCX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jVLCX.png"" alt="""" /></a></p>
<p><a href=""https://i.stack.imgur.com/biiMo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/biiMo.png"" alt="""" /></a></p>
"
"61259571","How can I change Azure data factory pipeline parameter dynamically?I want to assign new value to pipeline parameter from 'Metadata activity'","<p>I want to obtain a parameter from pipeline output. Currently as per my knowledge Azure data factory pipeline output can not be customized. Hence I want to pass my output string in pipeline parameter, to be able to extract it from pipeline output json. </p>
","<azure><parameters><azure-pipelines><azure-data-factory>","2020-04-16 20:31:50","419","0","1","61264942","<p>The output of a pipeline is an output of an activity with in that pipeline . You can use the ""Execute Pipeline"" activity and trigger a new  pipeline .</p>
"
"61257106","Azure Data Factory Copy Activity is dropping columns on the floor","<p>first time, long time.</p>

<p>I'm running an import of a csv file that has 734 columns in Azure Data Factory Copy Activity. Data factory is not reading the last 9 columns and is populating with NULL. Even in the preview I can see that the columns have no values but the schema for those columns is detected. Is there a limit of columns in Copy to 725?</p>
","<azure><azure-data-factory>","2020-04-16 18:12:17","557","0","2","61265157","<p>As Joel said there is no restriction for 725 or so columns . I suggest </p>

<ol>
<li>Go to the mapping tab and only pick 726th column ( if you have a header it will be easy or ADF will generate header like Prop_726( most probably) , copy the data to blob as sink , If the blob has the field , that means that you have a data type issue on the table . </li>
</ol>

<p>Let me know how its goes , if you are still facing the issue , please share some dummy data for 726th column . </p>
"
"61257106","Azure Data Factory Copy Activity is dropping columns on the floor","<p>first time, long time.</p>

<p>I'm running an import of a csv file that has 734 columns in Azure Data Factory Copy Activity. Data factory is not reading the last 9 columns and is populating with NULL. Even in the preview I can see that the columns have no values but the schema for those columns is detected. Is there a limit of columns in Copy to 725?</p>
","<azure><azure-data-factory>","2020-04-16 18:12:17","557","0","2","61374077","<p>Here is what happened. I had the file in zip folders, and I thought I had to unzip the files first to process them. It turns out that when unzipping through ADF, it stripped the quotation marks from my columns, and then one of the columns had an escape character in it. That escape character shifted everything over, and resulted in me losing nine columns. </p>

<p>But I did learn a bunch of things NOT to do, so it wasn't a total waste of time. Thanks for the answers! </p>
"
"61254112","Azure data factory store variable to file in blob","<p>I was trying to store output of variable in Azure data factory to file in blob</p>
","<azure><azure-data-factory>","2020-04-16 15:29:47","1760","3","1","61263433","<p>You could send the variable to Azure Function as parameter in the ADF. Then stored the value into file with sdk inside Azure Function.</p>

<p>The flow like below,using Set Variable Activity+Azure Function Activity: </p>

<p><a href=""https://i.stack.imgur.com/qYzaW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qYzaW.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/pCGYt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pCGYt.png"" alt=""enter image description here""></a></p>
"
"61247731","Azure Data Factory: can I pause a Pipeline run while another Pipeline is running and then resume the first Pipeline?","<p>So I have this Pipeline that runs for a long time (weeks), which loads some tables with Data Factory and processes them with Databricks.</p>

<p>Also, I have another Pipeline that is run each day for a couple of hours. However, the Databricks cluster seems not to be powerful enough to run both pipelines simultaneously, as it throws an error when both Pipelines are active (seems to be a memory error, ""Spark driver has stopped unexpectedly"").</p>

<p>The daily Pipeline is highest priority, though, so ideally I would like to <strong>pause</strong> for around 3 hours the long term Pipeline, then <strong>execute the daily</strong> trigger, and then <strong>resume</strong> the long term Pipeline execution.</p>

<p>Is it possible to do that?</p>

<p>Thanks in advance!</p>
","<azure><azure-pipelines><azure-data-factory><azure-databricks>","2020-04-16 10:12:49","986","0","2","61253748","<p>The simple answer: No. You should look for the solution in the databricks cluster throwing the error. There is no way to pause a pipeline because another pipeline in running.</p>
"
"61247731","Azure Data Factory: can I pause a Pipeline run while another Pipeline is running and then resume the first Pipeline?","<p>So I have this Pipeline that runs for a long time (weeks), which loads some tables with Data Factory and processes them with Databricks.</p>

<p>Also, I have another Pipeline that is run each day for a couple of hours. However, the Databricks cluster seems not to be powerful enough to run both pipelines simultaneously, as it throws an error when both Pipelines are active (seems to be a memory error, ""Spark driver has stopped unexpectedly"").</p>

<p>The daily Pipeline is highest priority, though, so ideally I would like to <strong>pause</strong> for around 3 hours the long term Pipeline, then <strong>execute the daily</strong> trigger, and then <strong>resume</strong> the long term Pipeline execution.</p>

<p>Is it possible to do that?</p>

<p>Thanks in advance!</p>
","<azure><azure-pipelines><azure-data-factory><azure-databricks>","2020-04-16 10:12:49","986","0","2","61263724","<p>You cannot pause a running pipeline. Azure devops pipeline doesnot have this feature currently. Below screenshots lists all the statuses that a build could be. As you can see ""pause"" is not one of them.</p>

<p><a href=""https://i.stack.imgur.com/93uza.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/93uza.png"" alt=""enter image description here""></a></p>

<p>You might have to ask for a bigger capacity to run another cluster.</p>

<p>However, you can always <a href=""https://developercommunity.visualstudio.com/spaces/8/index.html"" rel=""nofollow noreferrer"">submit a feature</a> request(Click <strong>Suggest a feature</strong> and choose <strong>Azure Devops</strong>) to Microsoft Development team. Hope they will consider implementing this feature in the future sprints.</p>

<p><a href=""https://i.stack.imgur.com/NmAb4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NmAb4.png"" alt=""enter image description here""></a></p>
"
"61247134","How to declare a Dictionary in Azure Data Factory?","<p>Is there a simple way to declare a dictionary or to convert a SQL result (Lookup activity) to a dictionary?</p>

<p>Example:</p>

<p>SQL Lookup response:</p>

<pre><code>{
 ""Id"": ""12313ased"",
 ""Name"": ""john""
},
{
 ""Id"": ""123dsada"",
 ""Name"": ""doe""
}
</code></pre>

<p>What I want:
<code>Dict(""12313ased"": ""john"", ""123dsada"", ""doe"")</code> (or whatever format Azure Data Factory works best for this).</p>
","<azure><azure-data-factory>","2020-04-16 09:38:56","2076","3","1","69286004","<p>It is possible to declare a dictionary parameter in Azure Data Factory (ADF) / Synapse Pipelines.  Declare an <strong>Object</strong> parameter and populate it like this:</p>
<p><a href=""https://i.stack.imgur.com/XmX9z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XmX9z.png"" alt=""Parameter definition"" /></a></p>
<p>The dictionary itself takes the form:</p>
<pre><code>{
  &quot;values&quot;: {
    &quot;key1&quot;: &quot;value1&quot;,
    &quot;key2&quot;: &quot;value2&quot;
  }
}
</code></pre>
<p>You can then fetch values out by key using expressions like:</p>
<pre><code>@pipeline().parameters.pDictionary.values['key1']
</code></pre>
<p>or more dynamically, eg using another parameter:</p>
<pre><code>@pipeline().parameters.pDictionary.values[pipeline().parameters.pKeyToCheck]
</code></pre>
<p>or using a variable:</p>
<pre><code>@pipeline().parameters.pDictionary.values[variables('varKeyToCheck')]
</code></pre>
<p>Variables in ADF do not support the Object type directly.  You can however loop through the output from things like Lookups with the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">For Each</a> activity.</p>
"
"61246095","Azure Data Factory - If Condition only run on specific hours","<p>I'm trying to build a control flow in ADF pipeline where an If Condition should only run on specific hours of the day. The pipeline has a trigger that runs every hour, but because of run limitations in the external system, this system should only be called at 6, 8, 10, 12 and so on during the day. </p>

<p>In SQL I would do something like <code>CASE WHEN DATEPART(HOUR, GETDATE()) IN (6,8,10,12) THEN 1 ELSE 0</code></p>

<p>I fully believe there is a way to construct such an expression in ""dynamic content"", but I really don't know where to being. Should the hours be there written in an variable array or directly in the expression?</p>

<p>Thanks in advance!</p>
","<expression><azure-data-factory><hour>","2020-04-16 08:43:15","2065","1","1","61266935","<p>I think your requirement could be implemented with built-in expression supported by ADF.</p>

<p>Please try configure Dynamic content in the If-condition Activity as below:</p>

<pre><code>@contains('06081012',substring(formatDateTime(utcnow()),11,2))
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZukPi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZukPi.png"" alt=""enter image description here""></a></p>
"
"61243603","How to pass an array for column pattern matching in mapping dataflow derived column from CSV file through pipeline?","<p>I have a mapping data flow with a derived column, where I want to use a column pattern for matching against an array of columns using in()</p>

<p><a href=""https://i.stack.imgur.com/avHfb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/avHfb.png"" alt=""enter image description here""></a></p>

<p>The data flow is executed in a pipeline, where I set the parameter $owColList_md5 based on a variable that I've populated from a single-line CSV file containing a comma-separated string</p>

<p><a href=""https://i.stack.imgur.com/vDKRo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vDKRo.png"" alt=""enter image description here""></a></p>

<p>If I have a single column name in the CSV file/variable encapsuled in single quotes and have the ""Expression"" checkbox ticked, it works.
The problem is to get it to work with multiple columns. There seems to be parsing problems having multiple items in the variable each encapsuled in single-quotes, or potentially with the comma separating them. This often causes errors executing the data flow with messages like ""store is not defined"" etc</p>

<p>I've tried having ''col1'',''col2'' and ""col1"",""col2"" (2x single quotes and double quotes) in the CSV file. I've also tried having the file without quotes, trying to replace the comma with escaped quotes (using ) in the derived column pattern expression with no luck.</p>

<p>How do you populate this array in the derived column based on the data flow parameter which is based on the comma-separated string in the CSV file / variable from the pipeline with column names in a working way?</p>
","<azure-data-factory>","2020-04-16 05:56:43","2211","0","1","61256623","<p>While array types are not supported as data flow parameters, passing in a comma-separated string can work if you use the <code>instr()</code> function to match.</p>

<p>Say you have two columns, col1 and col2. Pass in a parameter with value '""col1"",""col2""'.</p>

<p><a href=""https://i.stack.imgur.com/0rQXP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0rQXP.png"" alt=""enter image description here""></a></p>

<p>Then use <code>instr($&lt;yourparamname&gt;, '""' + name + '""') &gt; 0</code> to see if the column name exists within the string you pass in. Note: You do not need double quotes, but the can be useful if you have column names that are subsets of other columns names such as id1 and id11.</p>

<p><a href=""https://i.stack.imgur.com/HcQeT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HcQeT.png"" alt=""enter image description here""></a></p>

<p>Hope this helps!</p>
"
"61240726","why the default auto created sql tables column length is -1 in azure data factory?? and how to make it fixed?","<p>I am trying to read data from csv to azure sql db using copy activity. I have selected auto create option for destination table in sink dataset properties. 
copy activity is working fine but all columns are getting created with nvarchar(max) or length -1. I dont want -1 length as default length for my auto created sink table columns.
Does any one know how to change column length or create fixed length columns while auto table creation in azure data factory?</p>
","<azure><azure-sql-database><azure-data-factory><sqldatatypes><maxlength>","2020-04-16 00:21:36","933","1","1","61535192","<p>As you have clearly detailed, the <code>auto create table</code> option in the copy data activity is going to create a table with generic column definitions. You can run the copy activity initially in this way and then return to the target table and run T-SQL statements to further define the desired column definitions. There is no option to define table schema with the <code>auto create table</code> option.</p>

<p><code>ALTER TABLE table_name ALTER COLUMN column_name new_data_type(size);</code></p>

<p>Clearly, the new column definition(s) must match the data that has been initially copied via the first copy activity but this is how you would combine the <code>auto create table</code> option and resulting in clearly defined column definitions. </p>

<p>It would be useful if you created a <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">Azure Data Factory UserVoice</a> entry to request that the auto create table functionality include creation of column definitions as a means of saving the time needed to go back an manually alter the columns to meet specific requirements, etc.</p>
"
"61240601","Azure Data Factory 2 : How to split a file into multiple output files","<p>I'm using Azure Data Factory and am looking for the complement to the ""Lookup"" activity.  Basically I want to be able to write a single line to a file.</p>

<p>Here's the setup:</p>

<ul>
<li>Read from a CSV file in blob store using a Lookup activity</li>
<li>Connect the output of that to a For Each

<ul>
<li>within the For Each, take each record (a line from the file read by the Lookup activity) and write it to a distinct file, named dynamically.</li>
</ul></li>
</ul>

<p>Any clues on how to accomplish that?</p>
","<azure-data-factory><azure-blob-storage>","2020-04-16 00:07:25","8786","2","2","61261218","<p>Data Flow would probably be better for this, but as a quick hack, you can do the following to read the text file line by line in a pipeline:</p>

<ol>
<li><p>Define your source dataset to output a line as a single column. Normally I would use ""NoDelimiter"" for this, but that isn't supported by Lookup. As a workaround, define it with an incorrect Column Delimiter (like | or \t for a CSV file). You should also go to the Schema tab, and CLEAR the schema. This will generate a column in the output named ""Prop_0"".</p></li>
<li><p>In the foreach activity, set the Items to the Lookup's ""output.value"" and check ""Sequential"".
<a href=""https://i.stack.imgur.com/4LgNY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4LgNY.png"" alt=""enter image description here""></a></p></li>
<li><p>Inside the foreach, you can use item().Prop_0 to grab the text of the line:
<a href=""https://i.stack.imgur.com/kKyt3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kKyt3.png"" alt=""enter image description here""></a></p></li>
<li><p>To the best of my understanding, creating a blob isn't directly supported by pipelines [hence my suggestion above to look into Data Flow]. It is, however, very simple to do in Logic Apps. If I was tackling this problem, I would create a logic app with an HTTP Request Received trigger, then call it from ADF with a Web activity and send the text line and dynamic file name in the payload.</p></li>
</ol>
"
"61240601","Azure Data Factory 2 : How to split a file into multiple output files","<p>I'm using Azure Data Factory and am looking for the complement to the ""Lookup"" activity.  Basically I want to be able to write a single line to a file.</p>

<p>Here's the setup:</p>

<ul>
<li>Read from a CSV file in blob store using a Lookup activity</li>
<li>Connect the output of that to a For Each

<ul>
<li>within the For Each, take each record (a line from the file read by the Lookup activity) and write it to a distinct file, named dynamically.</li>
</ul></li>
</ul>

<p>Any clues on how to accomplish that?</p>
","<azure-data-factory><azure-blob-storage>","2020-04-16 00:07:25","8786","2","2","66690469","<p>Use Data flow, use the derived column activity to create a filename column. Use the filename column in sink. Details on how to implement dynamic filenames in ADF is describe here: <a href=""https://kromerbigdata.com/2019/04/05/dynamic-file-names-in-adf-with-mapping-data-flows/"" rel=""nofollow noreferrer"">https://kromerbigdata.com/2019/04/05/dynamic-file-names-in-adf-with-mapping-data-flows/</a></p>
"
"61235130","Using ADF Web Activity for each row in a Query","<p>Good Day!</p>

<p>I am pretty new to <strong>ADF</strong> and need some guidance on how to best accomplish using the <strong>ADF Web Activity</strong> for each record in a <strong>query/view</strong>. I have a system where we need to add new users daily.  I have built a query that returns what users are new and I would like to call a rest API to add their accounts in other systems.  Today, I accomplish this in a java program where we get a result set and then iterate through each row and call the <em>API</em>.  </p>

<p>I am attempting to replicate this in <strong>ADF</strong> and running into not understand how to accomplish this. The <code>ForEach</code> activity does not appear to be able to connect to a dataset or query.  I have seen other examples of using the <code>ForEach</code> when you are building a parameter list and understand how that would work.  (for example: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal</a>.)</p>

<p>Can anyone give me some direction on how you run an activity for each row in a dataset?</p>
","<foreach><azure-data-factory>","2020-04-15 17:41:47","849","0","1","61256936","<p>I was able to find a way to do this using the Lookup Activity and then passing the output of the lookup to the foreach activity.  Here is a my pipeline:</p>

<p><a href=""https://i.stack.imgur.com/QcxGg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QcxGg.png"" alt=""AdF Pipeline with Lookup ad ForEach activity""></a></p>

<p>When passing the data from the Lookoup to the ForEach look you wan to set the ForEach items to ""@activity('Lookup1').output.value""  If you use the 'add dynamic content' selection and pick the output data sent ADF sets the value to @activity('Lookup1').output which will through a weird error about the length function when you run it.</p>

<p>In my research I found this demo to be very helpful: <a href=""https://www.youtube.com/watch?v=ROq5mVrZPY0"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ROq5mVrZPY0</a></p>
"
"61233046","Azure Data Flow (Pass output of one data flow to another in the pipeline)","<p>I have a requirement where I have to pass the select transformation output from one data flow (data flow) to another directly.</p>

<p>Example:</p>

<ol>
<li><p>I have a data flow with a SELECT transformation as Final step.</p></li>
<li><p>I have another data flow that needs to take the above SELECT transformation output as input.</p></li>
<li><p>Currently, I am storing the output of first data flow into a table and getting the data from the table in second data flow which takes long to execute. I want to avoid storing into the table.</p></li>
</ol>

<p>Thanks,
Karthik </p>
","<azure-data-factory>","2020-04-15 15:51:59","635","1","1","61235846","<p>Data flows require your logic to terminate with a Sink when you execute them from a pipeline, so you must persist your output somewhere in Azure. The next pipeline activity can read from that output dataset.</p>
"
"61229675","How to convert excel file to csv in azure data factory","<p>Hi I'm looking for some help here. I am new to Azure Data Factory and I need to convert some Excel files to csv, how can I do this? </p>
","<excel><azure><csv><azure-data-factory>","2020-04-15 13:14:48","5106","1","2","61420641","<p>As i know,Excel file is not supported by ADF so far,only <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs"" rel=""nofollow noreferrer"">these formats</a> are supported.</p>

<p>So,you could try some workarounds to process excel files in ADF:</p>

<p>1.Use Databricks Activity to load excel files and convert into csv files,please refer to this case:<a href=""https://stackoverflow.com/questions/44196741/how-to-construct-dataframe-from-a-excel-xls-xlsx-file-in-scala-spark"">How to construct Dataframe from a Excel (xls,xlsx) file in Scala Spark?</a></p>

<p>2.Use Azure Function Activity to convert excel files into csv files,you could choose development language to implement that. Then output the csv files into specific path and use them in the next steps.</p>

<p>3.Try Custom Activity which is actually tasks(for example,execute a Powershell Script) running on the Azure Batch Node.This approach is more flexible and easy to operate.</p>
"
"61229675","How to convert excel file to csv in azure data factory","<p>Hi I'm looking for some help here. I am new to Azure Data Factory and I need to convert some Excel files to csv, how can I do this? </p>
","<excel><azure><csv><azure-data-factory>","2020-04-15 13:14:48","5106","1","2","73189358","<p>As of <strong>2022</strong> <code>.xlsx</code> and <code>.xls</code> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-excel"" rel=""nofollow noreferrer"">files are supported</a> as source but not as sink.</p>
<p>This means that we can use a <code>copy</code> activity in Azure Synapse / Azure Data Factory to extract the data from a specific sheet of the <code>xlsx</code> file (=source) and copy it to a <code>csv</code> file (=sink).</p>
"
"61216143","how to create data factory's integrated runtime in arm template","<p>I am trying to deploy data factory using ARM template. It is easy to use the exported template to create a deployment pipeline. </p>

<p>However, as the data factory needs to access an on-premise database server, I need to have an integrated runtime. The problem is how can I include the run time in the arm template?</p>

<p>The template looks like this and we can see that it is trying to include the runtime:</p>

<pre><code>{
    ""name"": ""[concat(parameters('factoryName'), '/OnPremisesSqlServer')]"",
    ""type"": ""Microsoft.DataFactory/factories/linkedServices"",
    ""apiVersion"": ""2018-06-01"",
    ""properties"": 
    {
        ""annotations"": [],
        ""type"": ""SqlServer"",
        ""typeProperties"": {
            ""connectionString"": ""[parameters('OnPremisesSqlServer_connectionString')]""
        },
        ""connectVia"": {
            ""referenceName"": ""OnPremisesSqlServer"",
            ""type"": ""IntegrationRuntimeReference""
        }
    },
    ""dependsOn"": [
        ""[concat(variables('factoryId'), '/integrationRuntimes/OnPremisesSqlServer')]""
    ]
},
{
    ""name"": ""[concat(parameters('factoryName'), '/OnPremisesSqlServer')]"",
    ""type"": ""Microsoft.DataFactory/factories/integrationRuntimes"",
    ""apiVersion"": ""2018-06-01"",
    ""properties"": {
        ""type"": ""SelfHosted"",
        ""typeProperties"": {}
    },
    ""dependsOn"": []
}
</code></pre>

<p>Running this template gives me this error:</p>

<pre><code>\""connectVia\"": {\r\n      \""referenceName\"": \""OnPremisesSqlServer\"",\r\n      \""type\"": \""IntegrationRuntimeReference\""\r\n    }\r\n  }\r\n} and error is: Failed to encrypted linked service credentials on self-hosted IR 'OnPremisesSqlServer', reason is: NotFound, error message is: No online instance..
</code></pre>

<p>The problem is that I will need to type in some key in the integrated runtime's UI, so it can be registered in azure. But I  can only get that key from my data factory instance's UI. So above arm template deployment will always fail at least once. I am wondering if there is a way to create the run time independently?</p>
","<azure-data-factory>","2020-04-14 19:58:18","1652","1","1","61224677","<blockquote>
  <p>The problem is that I will need to type in some key in the integrated
  runtime's UI, so it can be registered in azure. But I can only get
  that key from my data factory instance's UI. So above arm template
  deployment will always fail at least once. I am wondering if there is
  a way to create the run time independently?</p>
</blockquote>

<p>It seems that you already know how to create <a href=""https://learn.microsoft.com/en-us/azure/templates/microsoft.datafactory/2018-06-01/factories/integrationruntimes#IntegrationRuntime"" rel=""nofollow noreferrer"">Self-Hosted IR in the ADF ARM</a>.</p>

<pre><code>{
          ""name"": ""[concat(parameters('dataFactoryName'), '/integrationRuntime1')]"",
          ""type"": ""Microsoft.DataFactory/factories/integrationRuntimes"",
          ""apiVersion"": ""2018-06-01"",
          ""properties"": {
            ""additionalProperties"": {},
            ""description"": ""jaygongIR1"",
            ""type"": ""SelfHosted""
            }
        }
</code></pre>

<p>Result:</p>

<p><a href=""https://i.stack.imgur.com/FPENZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FPENZ.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/RyOFd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RyOFd.png"" alt=""enter image description here""></a></p>

<p>Only you concern is that <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=39717"" rel=""nofollow noreferrer"">Windows IR Tool</a> need to be configured with <code>AUTHENTICATION KEY</code> to access ADF Self-Hosted IR node.So,it should be <code>Unavailable</code> status once it is created.This flow is make sense i think,authenticate key should be created first,then you can use it to configure On-Premise Tool.You can't implement all of things in one step because these behaviors are operated on both of azure and on-premise sides.</p>

<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#install-and-register-a-self-hosted-ir-from-microsoft-download-center"" rel=""nofollow noreferrer"">Self-Hosted IR Tool document</a> ,the Register steps can't be implemented with Powershell code. So,all steps can't be processed in the flow are creating IR and getting Auth key,not for Registering in the tool.</p>
"
"61210270","Supported data providers for Oracle in Azure Data Factory SSIS IR","<p>We are trying to use Azure Data Factory SSIS Integration Runtime service to run a set of existing SSIS packages (basically to transfer data from oracle to SQL Server).</p>

<p>However, when deploying, there is a validation error saying ""Package xxx: Connection xxx contains unsupported provider.""</p>

<p>We tried using the following two providers in the SSIS packages, both of them were reported as unsupported:</p>

<ol>
<li>""Oracle Provider for OLE DB"" </li>
<li>""OracleClient Data Provider""</li>
</ol>

<p>What are the supported providers for accessing Oracle in ADF with SSIS IR?</p>
","<oracle><ssis><azure-data-factory>","2020-04-14 14:35:47","653","1","2","61219578","<p>Your SSIS package is access on-premise local Oracle DB.</p>

<p>As you know, Azure can not connect to on-premise resource directly. For example, when we need access the local SQL server, we must use Self-host integration runtime.</p>

<p>When deploy it to Azure and run the package in Data Factory, you will get the error. </p>

<p>We can get the reason from the document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ssis-integration-runtime"" rel=""nofollow noreferrer"">Azure-SSIS Integration Runtime</a>:</p>

<p><strong>Azure-SSIS IR network environment</strong></p>

<p>Azure-SSIS IR can be provisioned in either public network or private network. <strong>On-premises data access is supported by joining Azure-SSIS IR to a Virtual Network that is connected to your on-premises network</strong>.</p>

<p>The solution: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/join-azure-ssis-integration-runtime-virtual-network"" rel=""nofollow noreferrer"">Join an Azure-SSIS integration runtime to a virtual network</a>:</p>

<p>If your SSIS packages access data stores/resources that allow only specific static public IP addresses and you want to secure access to those resources from Azure-SSIS IR, you can bring your own public IP addresses for Azure-SSIS IR while joining it to a virtual network and then add an IP firewall rule to the relevant resources to allow access from those IP addresses.</p>

<p>I'm sorry I can't test it for you because I don't Oracle environment. </p>

<p>Hope this helps.</p>
"
"61210270","Supported data providers for Oracle in Azure Data Factory SSIS IR","<p>We are trying to use Azure Data Factory SSIS Integration Runtime service to run a set of existing SSIS packages (basically to transfer data from oracle to SQL Server).</p>

<p>However, when deploying, there is a validation error saying ""Package xxx: Connection xxx contains unsupported provider.""</p>

<p>We tried using the following two providers in the SSIS packages, both of them were reported as unsupported:</p>

<ol>
<li>""Oracle Provider for OLE DB"" </li>
<li>""OracleClient Data Provider""</li>
</ol>

<p>What are the supported providers for accessing Oracle in ADF with SSIS IR?</p>
","<oracle><ssis><azure-data-factory>","2020-04-14 14:35:47","653","1","2","61452016","<p>When connecting to Oracle using a provider such as ""Oracle Provider for OLE DB"" we need to install that to the node(s) running SSIS IR. This can be done by customizing the SSIS IR using the portal.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup</a></p>

<p>Steps summary:</p>

<ol>
<li>Download and edit the sample script (main.cmd) provided in the above
link (from the <strong>publicpreview</strong> container)</li>
<li>Upload it with the ODAC122010Xcopy_x64.zip file to an Azure
Blob Container. </li>
<li>Get a ""Shared Access Signature"" uri for this folder.</li>
<li>Shutdown SSIS IR.</li>
<li>Go to customize the SSIS IR and provide the link copied.</li>
<li>Restart SSIS IR.</li>
</ol>
"
"61207340","Accessing 'Details' component of ADF activity run instead of 'Output'","<p>ADFv2 seems to be allowing accessing Output of an activity using @('activityname').output but in the monitor we also see a 'Details' component for certain activity types (e.g. HDInsightHive, SQL etc), so is there any way we can access it? It contains very valuable information and it would be great if there is a way to access it somehow. The ADF drop down (adding dynamic values) obviously doesn't show it. This internally available information if exposed via UI would be a great plus.</p>

<p>The following screenshot shows how the detail window appears, basically one has to click on this yellow highlighted icon:-</p>

<p><a href=""https://i.stack.imgur.com/lcOrh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lcOrh.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-04-14 12:09:30","101","1","1","61229614","<p>Now maybe i know your concern.You just wonder whether information could be accessed in the <code>Details</code> window. For example,my test copy activity:</p>

<p><a href=""https://i.stack.imgur.com/66qWy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/66qWy.png"" alt=""enter image description here""></a></p>

<p>Actually,based on my observation, the information listed in above window are exactly information in the error output.No more additional monitor information,just more readable and clear. </p>

<p><a href=""https://i.stack.imgur.com/1nNgj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1nNgj.png"" alt=""enter image description here""></a></p>

<p>Besides,we can't find any more object of Activity Run Response from <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/activityruns/querybypipelinerun#activityrun"" rel=""nofollow noreferrer"">REST API</a>.</p>

<p><a href=""https://i.stack.imgur.com/jBzEu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jBzEu.png"" alt=""enter image description here""></a></p>

<p>So,if you are interested in the details info,please refer the output object.</p>
"
"61206273","Delete last record from flat file in blob to Azure data warehouse","<p>I have some pipe delimited flat files in Blob storage and in each file I have a header and footer record with filename, date of extract and the number of records. I am using ADF pipeline with Polybase toload into Azure DWH. I could skip header record but unable to skip the footer. The only way I could think of is creating staging table with all varchar and load into staging and then convert the data types back into main tables. But that is not working as the number of columns is different to the footer and the data. Is there any easier way to do this? Please advise.</p>
","<azure-blob-storage><data-warehouse><azure-data-factory><delete-row><skip>","2020-04-14 11:08:32","740","1","1","61214170","<p>Polybase does not have an explicit option for removing footer rows but it does have a set of rejection options which you could potentially take advantage of.  If you set your <code>REJECT_TYPE</code> as <code>VALUE</code> (rather than <code>PERCENTAGE</code>) and your <code>REJECT_VALUE</code> AS <code>1</code> you are telling Polybase to reject one row only.  If your footer is in a different format to the main data rows, it will be rejected but your query should not fail.</p>

<pre><code>CREATE EXTERNAL TABLE yourTable
...    
&lt;reject_options&gt; ::=  
{  
    | REJECT_TYPE = value,  
    | REJECT_VALUE = 2 
</code></pre>

<p>Please post a simple, anonmyised example of your file with headers, columns and footers if you need further assistance.</p>

<p><strong>Update:</strong> Check this blog post for information on tracking rejected rows:</p>

<p><a href=""https://azure.microsoft.com/en-us/blog/load-confidently-with-sql-data-warehouse-polybase-rejected-row-location/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/load-confidently-with-sql-data-warehouse-polybase-rejected-row-location/</a></p>
"
"61190574","Extracting and Transforming Data from local MySQL to Azure Synapse Data Warehouse","<p>I'm trying to setup a Demo Data Warehouse in Azure Synapse. I would like to extract data from a local MySQL database, transform and aggregate some data and store it in fact-/dimension tables in Azure Synapse Analytics.</p>

<p>Currently I have an instance of Azure SQL Data Warehouse and Data Factory. I created a connection to my MySQL database in Data Factory and my thought was, i can use this connector as input for a new Data Flow, which transforms the dataset and stores it to my destination dataset, which is linked to my Azure Synapse Data Warehouse.</p>

<p>The Problem is, Data Factory just support some Azure Services like Azure Data Lake or Azure SQL Database as Source for a new Data Flow.</p>

<p>What would be the best practice for solving this Problem? Create an Instance of Azure SQL Database, copy the Data from the local MySQL Database to the Azure SQL Database and use it then as Source for a new Data Flow?</p>
","<azure><etl><azure-data-factory><azure-synapse>","2020-04-13 14:54:59","624","1","1","61193197","<p>Best practice here is to use the Copy Activity in an ADF pipeline to land the data from MySQL into Parquet in Blob or ADLS G2, then transform the data using Data Flows.</p>
"
"61189405","Reading Postgress (encoded in SQL_ASCII) from Data Factory","<p>I am trying to read a on-premise postgres Database that is encoded in SQL_ASCII from the Azure Data Factory Copy Data Activity in order to copy the database's data into a Azure Data Lake.</p>

<p>I am running into encoding issues with special characters such as ""è"" an ""é"" and I am quite clueless as to how I should go about fixing this.</p>

<p>When seting up my source Dataset provinding a given table, it's preview shows the characters as ""?"". Does anyone have an idea of how I could fix this ?</p>

<p>Note that I cannot change the database's encoding because it is not under my control.</p>

<p>Any help will be greatly appreciated! </p>
","<postgresql><azure><character-encoding><azure-data-factory>","2020-04-13 13:52:45","42","1","1","61193665","<p>I was able to fix/workaround my issue by using the Postgres ANSI ODBC driver (64 bits) and specifying the DSN as an additional property on the Postgres Linked Service inside Azure Data Factory</p>
"
"61175117","how to copy incremental data from on-premises MySql to Azure MySql when on-premises MySql has no date field for reference using ADF","<p>I was trying to copy from on-premises mysql to azure mysql using ADF. Some tables have date filled so, that I can use tumbling window feature and populate incremental data. But some of the tables don't have any date filled for reference.</p>

<p>Is their any feature to get incremental data for such tables?</p>
","<mysql><azure><azure-data-factory>","2020-04-12 17:04:46","101","0","1","61180638","<p>Per my experience,data fields(such as createTime,modifyTime) have to be produced when you design table structure.It's significant for log monitor,troubleshooting and increment.As @Martin said in the comment,you always need to have a field to determine the criteria for increments.</p>

<p>If it is acceptable, you could add data field(just auto completed,no need to change your other strategy) to guaranteed consistency from now on.If the table can't be modified,maybe you need to consider adding other residence for new data at the same time.For example, Azure table storage,which you could control columns. </p>
"
"61174460","Append data to existing file in Azure Data Lake storage from REST API","<p>I have set up pipeline that fetch data from REST API and drops it down into ADLS storage gen1 , I am also seeing the files generated </p>

<p><strong>REST API > ADF pipeline(get bearer token + Copy activity) > ADLS</strong> </p>

<p>But when new data comes in from that API , data is replacing the current content in that file  instead of appending at the last line every time </p>

<p>is there any dynamic action that i need to provide or something ? can someone please put me in right direction .</p>

<p>Note: i can able to see the content inside the file , no errors at all</p>
","<rest><azure-data-factory><azure-data-lake>","2020-04-12 16:12:30","1904","0","1","61189794","<p>Given the nature of Blob storage, I don't think this is possible with a standard Copy activity. Azure Blobs have several types, the most common of which is BlockBlob, which is almost surely the type generated by ADF operations. A BlockBlob cannot be changed or updated, only overwritten, which explains the behavior you are experiencing. In order to update the content of a blob, it must be defined as an AppendBlob, which permits adding new content. The AppendBlob type must be declared when the Blob is created.</p>

<p>The only way I know to accomplish this (both creating the AppendBlob and appending content to it) is via the Azure Storage SDK, which includes classes and methods specific to dealing with AppendBlobs. This operation would require some custom code (I'll assume a C# console app) to access the storage account and append to the Blob. In order to incorporate this into your pipeline, you will need to replace your Copy activity with a Custom activity, which will execute your C# code in an Azure Batch account.</p>

<p>An alternative to consider is permitting the Copy activity to generate a new blob every time instead of trying to work with a single Blob. Data lakes and MPPs in general are designed to work with many files at a time, so depending on your use case that may be a more reasonable approach.</p>
"
"61149582","Unable to get scalar value of a query on cosmos db in azure data factory","<p>I am trying to get the count of all records present in cosmos db in a lookup activity of azure data factory. I need this value to do a comparison with other value activity outputs.</p>

<p>The query I used is <code>SELECT VALUE count(1) from c</code></p>

<p>When I try to preview the data after inserting this query I get an error saying </p>

<pre><code>One or more errors occurred. Unable to cast object of type  
 'Newtonsoft.Json.Linq.JValue' to type 'Newtonsoft.Json.Linq.JObject'
</code></pre>

<p>as shown in the below image:
<a href=""https://i.stack.imgur.com/7WQLe.png"" rel=""nofollow noreferrer"">snapshot of my azure lookup activity settings</a></p>

<p>Could someone help me in resolving this error and if this is the limitation of azure data factory how can I get the count of all the rows of the cosmos db document using some other ways inside azure data factory?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-04-10 22:29:18","985","3","2","61168471","<p>I reproduce your issue on my side exactly.</p>

<p><a href=""https://i.stack.imgur.com/nBjbv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nBjbv.png"" alt=""enter image description here""></a></p>

<p>I think the count result can't be mapped as normal JsonObject. As workaround,i think you could just use Azure Function Activity(Inside Azure Function method ,you could use SDK to execute any sql as you want) to output your desired result: <code>{""number"":10}</code>.Then bind the Azure Function Activity with other activities in ADF.</p>

<hr>

<p>Here is contradiction right now:</p>

<p>The query sql outputs a scalar array,not other things like jsonObject,or even jsonstring.</p>

<p><a href=""https://i.stack.imgur.com/dxLcj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dxLcj.png"" alt=""enter image description here""></a></p>

<p>However, ADF Look Up Activity only accepts JObject,not JValue. I can't use any convert built-in function here because the query sql need to be produced with correct syntax anyway. I already submitted a ticket to MS support team,but get no luck with this limitation.</p>

<p><a href=""https://i.stack.imgur.com/5ipmz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5ipmz.png"" alt=""enter image description here""></a></p>

<p>I also tried <code>select count(1) as num from c</code> which works in the cosmos db portal. But it still has limitation because the sql crosses partitions.</p>

<p><a href=""https://i.stack.imgur.com/BQAs3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BQAs3.png"" alt=""enter image description here""></a></p>

<p>So,all i can do here is trying to explain the root cause of issue,but can't change the product behaviours.</p>

<p>2 rough ideas:</p>

<p>1.Try no-partitioned collection to execute above sql to produce json output.</p>

<p>2.If the count is not large,try to query columns from db and loop the result with ForEach Activity.</p>
"
"61149582","Unable to get scalar value of a query on cosmos db in azure data factory","<p>I am trying to get the count of all records present in cosmos db in a lookup activity of azure data factory. I need this value to do a comparison with other value activity outputs.</p>

<p>The query I used is <code>SELECT VALUE count(1) from c</code></p>

<p>When I try to preview the data after inserting this query I get an error saying </p>

<pre><code>One or more errors occurred. Unable to cast object of type  
 'Newtonsoft.Json.Linq.JValue' to type 'Newtonsoft.Json.Linq.JObject'
</code></pre>

<p>as shown in the below image:
<a href=""https://i.stack.imgur.com/7WQLe.png"" rel=""nofollow noreferrer"">snapshot of my azure lookup activity settings</a></p>

<p>Could someone help me in resolving this error and if this is the limitation of azure data factory how can I get the count of all the rows of the cosmos db document using some other ways inside azure data factory?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-04-10 22:29:18","985","3","2","68845675","<p>You can use:</p>
<pre><code> select top 1 column from c order by column desc 
</code></pre>
"
"61144119","Creating MongoDB linked Service in Azure Data Factory","<p>I am having issues setting up a Mongo DB linked service in Azure Data Factory. We are already using Mongo DB connection on our On-Premise platform to extract some data from Client side. </p>

<p>Now, we are migrating all our on-premise process to Azure. So this connection is part of many other things that we are migrating. I am using below connection string. </p>

<p>This same string is working perfectly fine on our on-premise environment but for some reason, not working in Azure.Could someone please help</p>

<pre><code>ConnectionString=""mongodb://username:password@jproduct-hekrl.gvt.mongodb.net:20007/management-core?ssl=true&amp;replicaSet=Product-shard-0&amp;serverSelectionTimeoutMS=10000&amp;connectTimeoutMS=10000&amp;authSource=admin&amp;authMechanism=SCRAM-SHA-1""
</code></pre>

<p><a href=""https://i.stack.imgur.com/0bX2X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0bX2X.png"" alt=""""></a></p>

<p><a href=""https://i.stack.imgur.com/uYFXu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uYFXu.png"" alt=""enter image description here""></a></p>
","<mongodb><azure><azure-pipelines><azure-data-factory>","2020-04-10 16:06:55","976","1","1","61144863","<p>Is difficult to answer without knowing the actual error message but I'll try: </p>

<p>Are you pasting the connection String just as you are showing us? with the initial words included? </p>

<p>The connection String should be in the format below:</p>

<pre><code>mongodb://[username:password]@host[:port][/[database][?options]]
</code></pre>

<p>As you can see, there is no <code>ConnectionString=</code> at the beginning, So in your case it seems it should be as follows:</p>

<pre><code>mongodb://username:password@jproduct-hekrl.gvt.mongodb.net:20007/management-core?ssl=true&amp;replicaSet=Product-shard-0&amp;serverSelectionTimeoutMS=10000&amp;connectTimeoutMS=10000&amp;authSource=admin&amp;authMechanism=SCRAM-SHA-1
</code></pre>

<p>Just need to remove the <code>ConnectionString=""</code> from the beginning</p>

<p>More info <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb"" rel=""nofollow noreferrer"">here</a></p>
"
"61140725","Can we create table in SQL by passing column information from Azure blob using Azure data flow or Azure data factory?","<p>I have CSV files that contain SQL Table column information as Column_Name, Data_Type, Primary_Key. But not contain any data, Can we use Azure Data flow or Azure Data Factory to create table by using these type of CSV file for particular table in SQL. </p>
","<sql-server><azure><azure-data-factory><azure-blob-storage>","2020-04-10 12:56:48","82","0","1","61199431","<p>According my experience about Data Factory,  we can not pass the csv file content which  contains SQL Table column information as Column_Name, Data_Type, Primary_Key to create table in SQL. </p>

<p>Data Factory or Data flow all couldn't achieve that.</p>

<p>You also reference the another answer here: <a href=""https://stackoverflow.com/a/61181031/10549281"">https://stackoverflow.com/a/61181031/10549281</a></p>

<p>Hope this helps.</p>
"
"61136151","How to create a table in SQL Database from a CSV file in Blob which contain all the column name with its data type through Data Flow or ADF pipeline?","<p>I am having a CSV file in my Azure Blob Storage which contain all the column name with its data Data type of respective tables.</p>

<p>I want to create a table in SQL Database from this Blob file with the same column name with its corresponding datatype without doing the mapping.</p>

<p>I have created a table through data flow but I have to set the data type of each column manually. But I don't want to do this. 
When I create a table it should accept the same data types in the source as well as synch which was given in the CSV file.</p>

<p>[<img src=""https://i.stack.imgur.com/pQpKf.png"" alt=""This is CSV file which contain column name with its data type"">]</p>

<p>[<img src=""https://i.stack.imgur.com/dOBgi.png"" alt=""In Data Flow, source it takes all column as string"">]</p>

<p>When I import the schema it takes full column as <code>ID (int)</code> and data type as <code>String</code> but I want when I import the schema and create a table it will take column name as <code>ID</code> and data type as <code>INT</code> and it will do same all column names for multiple tables.</p>

<p>Please let me know if you have a solution to this problem.</p>
","<mapping><azure-sql-database><azure-blob-storage><azure-data-factory>","2020-04-10 07:55:23","998","0","1","61181031","<p>In Data Factory, when we copy data from the <a href=""https://www.howtogeek.com/348960/what-is-a-csv-file-and-how-do-i-open-it/"" rel=""nofollow noreferrer"">CSV file</a>, we set <code>first row as column</code>, that means that the first row data will be set as column name, <code>id(int)</code> and <code>Name(varchar)</code>. As you know, the default column name data type is <code>String</code>(in Data Factory)/<code>varchar(128)</code>(in SQL database), we can not change it.</p>

<p><a href=""https://i.stack.imgur.com/n3DWv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n3DWv.png"" alt=""enter image description here""></a></p>

<p>We can not create the table with schema as column name! There's no solution to this problem.</p>

<p>But Data Factory will auto help us create the suitable column data type mapping for us. </p>

<p>For example, if your csv file is like this:</p>

<p><a href=""https://i.stack.imgur.com/eBCAt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eBCAt.png"" alt=""enter image description here""></a> </p>

<p>Auto schema mapping will convert ""1""(string) to <code>1(int)</code> in Azure SQL database.</p>

<p>Hope this helps.</p>
"
"61135409","Count records inserted per minute","<p>I have a logging table. In that table I log http requests that SSIS is executing in order to download data from web-APIs. Some web api's restrict the number of request that you may sent per timeframe</p>

<p>Now what I need to do is check if SSIS is firing more than an X amount of http request per Y seconds (treshold) per url.</p>

<p>The result should look like this (number_of_request is not based on actual data, rest is).</p>

<ul>
<li>Treshold = 60seconds </li>
<li>[min_start],[max_start] (the first and last startime within that treshold</li>
<li>url but without params</li>
</ul>

<p>(I need min and max so that I can find which request are in that
treshold)</p>

<pre><code>[treshold], [min_start],[max_start],[url], [number_of_requests]
[60seconds1], [06:00:31],[06:00:47], [MyUrl.com], [25]
[60seconds2], [06:00:51],[06:01:22], [MyUrl.com], [62]
[60seconds3], [06:03:49],[06:05:38], [MyUrl.com], [1]   &lt;-- This is interesting, query last longer than treshold. How to cope with that?
</code></pre>

<p>In the following table you will find the (fictious) request sent and their respective starting times.
I guess I have to do something with ranking but how?</p>

<pre><code>CREATE TABLE [test].[logging](
    [id] [int] IDENTITY(1,1) NOT NULL,
    [taskname] [nvarchar](256) NULL,
    [start] [datetime2](7) NULL,
    [url] [nvarchar](100) NULL
) 
GO
SET IDENTITY_INSERT [test].[logging] ON 
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (1, N'tf_finmut', CAST(N'2020-04-10T06:00:31.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=8')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (2, N'tf_finmut', CAST(N'2020-04-10T06:00:36.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=10')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (3, N'tf_finmut', CAST(N'2020-04-10T06:00:42.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=12')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (4, N'tf_finmut', CAST(N'2020-04-10T06:00:47.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=14')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (5, N'tf_finmut', CAST(N'2020-04-10T06:00:51.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=16')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (6, N'tf_finmut', CAST(N'2020-04-10T06:00:56.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=18')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (7, N'tf_finmut', CAST(N'2020-04-10T06:01:01.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=20')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (8, N'tf_finmut', CAST(N'2020-04-10T06:01:07.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=22')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (9, N'tf_finmut', CAST(N'2020-04-10T06:01:12.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=24')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (10, N'tf_finmut', CAST(N'2020-04-10T06:01:17.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=26')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (11, N'tf_finmut', CAST(N'2020-04-10T06:01:22.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=28')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (12, N'tf_finmut', CAST(N'2020-04-10T06:03:49.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=30')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (13, N'tf_finmut', CAST(N'2020-04-10T06:05:38.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=32')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (14, N'tf_finmut', CAST(N'2020-04-10T06:07:15.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=34')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (15, N'tf_finmut', CAST(N'2020-04-10T06:08:57.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=36')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (16, N'tf_finmut', CAST(N'2020-04-10T06:09:06.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=39')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (17, N'tf_finmut', CAST(N'2020-04-10T06:09:11.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=41')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (18, N'tf_finmut', CAST(N'2020-04-10T06:09:16.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=43')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (19, N'tf_finmut', CAST(N'2020-04-10T06:09:22.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=45')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (20, N'tf_finmut', CAST(N'2020-04-10T06:09:29.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=47')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (21, N'tf_finmut', CAST(N'2020-04-10T06:09:34.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=49')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (22, N'tf_finmut', CAST(N'2020-04-10T06:09:40.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=51')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (23, N'tf_finmut', CAST(N'2020-04-10T06:09:45.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=53')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (24, N'tf_finmut', CAST(N'2020-04-10T06:09:50.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=55')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (25, N'tf_finmut', CAST(N'2020-04-10T06:10:01.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=57')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (26, N'tf_finmut', CAST(N'2020-04-10T06:10:07.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=59')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (27, N'tf_finmut', CAST(N'2020-04-10T06:12:47.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=61')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (28, N'tf_finmut', CAST(N'2020-04-10T06:14:32.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=63')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (29, N'tf_finmut', CAST(N'2020-04-10T06:16:16.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=65')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (30, N'tf_finmut', CAST(N'2020-04-10T06:17:59.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=67')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (31, N'tf_finmut', CAST(N'2020-04-10T06:19:07.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=70')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (32, N'tf_finmut', CAST(N'2020-04-10T06:19:47.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=72')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (33, N'tf_finmut', CAST(N'2020-04-10T06:19:52.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=74')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (34, N'tf_finmut', CAST(N'2020-04-10T06:19:58.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=76')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (35, N'tf_finmut', CAST(N'2020-04-10T06:20:03.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=78')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (36, N'tf_finmut', CAST(N'2020-04-10T06:20:09.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=80')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (37, N'tf_finmut', CAST(N'2020-04-10T06:20:14.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=82')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (38, N'tf_finmut', CAST(N'2020-04-10T06:20:19.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=84')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (39, N'tf_finmut', CAST(N'2020-04-10T06:20:24.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=86')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (40, N'tf_finmut', CAST(N'2020-04-10T06:20:38.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=88')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (41, N'tf_finmut', CAST(N'2020-04-10T06:20:44.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=90')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (42, N'tf_finmut', CAST(N'2020-04-10T06:23:02.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=92')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (43, N'tf_finmut', CAST(N'2020-04-10T06:24:31.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=94')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (44, N'tf_finmut', CAST(N'2020-04-10T06:25:56.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=96')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (45, N'tf_finmut', CAST(N'2020-04-10T06:27:20.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=98')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (46, N'tf_finmut', CAST(N'2020-04-10T06:27:30.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=101')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (47, N'tf_finmut', CAST(N'2020-04-10T06:27:36.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=103')
GO
INSERT [test].[logging] ([id], [taskname], [start], [url]) VALUES (48, N'tf_finmut', CAST(N'2020-04-10T06:27:42.0000000' AS DateTime2), N'https://MyUrl.com/find&amp;id=105')
GO
SET IDENTITY_INSERT [test].[logging] OFF
GO
</code></pre>
","<sql><ssis><azure-sql-database><azure-data-factory>","2020-04-10 06:57:57","52","2","1","61139072","<p>This is a gaps-and-islands problem, probably best approached with <code>lag()</code> and a cumulative sum to define the islands:</p>

<pre><code>select trunc_url, min(start), max(start), count(*)
from (select l.*,
             sum(case when prev_start &gt;= dateadd(second, -60, start)
                      then 0 else 1
                 end) over (partition by trunc_url order by start) as grp
      from (select l.*, v.trunc_url,
                   lag(start) over (partition by trunc_url order by start) as prev_start
            from logging l cross apply
                 (values (left(url, len(url) - charindex('/', reverse(url))))) v(trunc_url)
           ) l
     ) l
group by trunc_url, grp
order by trunc_url, min(start)
</code></pre>

<p><a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=8c0a538438af205d2b12a5ff879b635e"" rel=""nofollow noreferrer"">Here</a> is a db&lt;>fiddle.</p>

<p>These results don't match your desired results, but they make sense to me based on the question.</p>
"
"61130581","Meaning of ADX Cache utilization more than 100%","<p>We see Cache utilization dashboard for an ADX cluster on Azure portal, but at times I have noticed that this utilization shows up to be more than 100%. I am trying to understand how to interpret it. Say , for example , if cache utilization shows up as 250% , does it mean that 100% of memory cache is utilized and then beyond that 150% disk cache is being utilized?</p>
","<azure-data-factory><azure-data-explorer>","2020-04-09 21:34:15","632","1","2","61130741","<p>as explained in the <a href=""https://learn.microsoft.com/en-us/azure/data-explorer/using-metrics#supported-azure-data-explorer-metrics"" rel=""nofollow noreferrer"">documentation</a> for the Cache Utilization metric:</p>
<blockquote>
<p>[this is the] Percentage of allocated cache resources currently in use by the cluster.</p>
<p>Cache is the size of SSD allocated for user activity according to the defined cache policy.</p>
<p>An average cache utilization of 80% or less is a sustainable state for a cluster.</p>
<p>If the average cache utilization is above 80%, the cluster should be scaled up to a storage optimized pricing tier or scaled out to more instances. Alternatively, adapt the cache policy (fewer days in cache).</p>
<p>If cache utilization is over 100%, the size of data to be cached, according to the caching policy, is larger that the total size of cache on the cluster.</p>
</blockquote>
"
"61130581","Meaning of ADX Cache utilization more than 100%","<p>We see Cache utilization dashboard for an ADX cluster on Azure portal, but at times I have noticed that this utilization shows up to be more than 100%. I am trying to understand how to interpret it. Say , for example , if cache utilization shows up as 250% , does it mean that 100% of memory cache is utilized and then beyond that 150% disk cache is being utilized?</p>
","<azure-data-factory><azure-data-explorer>","2020-04-09 21:34:15","632","1","2","74370923","<p>Utilization &gt; 100% means that there's not enough room in the (SSD) cache to hold all the data that the policy indicates should be cached. If auto-scale is enabled then the cluster will be scaled-out as a result.</p>
<p>The cache applies an LRU eviction policy, so that even when utilization exceeds 100% query performance will be as good as possible (though, of course, if queries constantly reference data more than what the cache can hold some performance degradation will be observed.)</p>
"
"61129552","The integration runtime has two disk drives, is there a way to parameterize the Host field in the linked service file system?","<p>I'm attempting to deploy an azure data factory with a copy data pipeline that pulls files from one or more deployed / on-prem file system paths and dumps them in blob storage. The source file paths on the file system may span multiple different drives (e.g. - C:\fileshare1 vs D:\fileshare2) and may include network locations referenced via UNC paths (e.g. - \localnetworkresource\fileshare3).</p>

<p>I'd like to configure a single local file system connection and source data set and just parameterize the linked service's host property. Then my pipeline would just iterate over a collection of file share paths and reuse the dataset and linked service connection. However, it doesn't look like there's any way to have the data set or pipeline provide the host information to the linked service. It's certainly possible to provide folder information from the pipeline and dataset, but that will be concatenated to the host specified in the linked service connection and therefore won't allow me access to different drives or network resources.</p>

<p>It was reasonably straightforward to do this by configuring separate linked service connections, data sets and pipelines for each distinct file share that needed to be included, but I'd prefer to manage a single pipeline.</p>

<p><strong>I already tried to create the JSON of the linked services but it didn't work, someone who can help me?</strong></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services</a></p>
","<azure><azure-functions><azure-data-factory>","2020-04-09 20:18:25","439","0","2","61133652","<p>Yes, you can parameterize file system linked service as follows. First you need to create a Filesystem linked service, then you can modify the JSON code to add parameter section as below:</p>

<pre><code>{
    ""name"": ""OnPremFileSystemLinkedService_Parameterized"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""type"": ""FileServer"",
        ""parameters"": {
            ""HostParameter"": {
                ""type"": ""string"",
                ""defaultValue"": ""C:\\[Folder]""
            },
            ""userIDParameter"": {
                ""type"": ""string"",
                ""defaultValue"": ""DOMAIN\\USERNAME""
            }
        },
        ""annotations"": [],
        ""typeProperties"": {
            ""host"": ""@{linkedService().HostParameter}"",
            ""userId"": ""@{linkedService().userIDParameter}"",
            ""encryptedCredential"": ""XXXXXXXXXXXencryptedKeyXXXXXXXXX""
        },
        ""connectVia"": {
            ""referenceName"": ""MySelfHostedIR"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p>In my sample I just used single File share as input and a copy activity. But as per your requirement, you can pass your FileShare collection list to a ForEach activity and iterate over each FileShare and pass those values to your Copy Activity -> Source/Sink Data set parameters -> Linked service parameters properties. </p>

<p>Below is a sample on how to use a parameterized File system linked service </p>

<p><a href=""https://i.stack.imgur.com/SAgLC.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SAgLC.gif"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61129552","The integration runtime has two disk drives, is there a way to parameterize the Host field in the linked service file system?","<p>I'm attempting to deploy an azure data factory with a copy data pipeline that pulls files from one or more deployed / on-prem file system paths and dumps them in blob storage. The source file paths on the file system may span multiple different drives (e.g. - C:\fileshare1 vs D:\fileshare2) and may include network locations referenced via UNC paths (e.g. - \localnetworkresource\fileshare3).</p>

<p>I'd like to configure a single local file system connection and source data set and just parameterize the linked service's host property. Then my pipeline would just iterate over a collection of file share paths and reuse the dataset and linked service connection. However, it doesn't look like there's any way to have the data set or pipeline provide the host information to the linked service. It's certainly possible to provide folder information from the pipeline and dataset, but that will be concatenated to the host specified in the linked service connection and therefore won't allow me access to different drives or network resources.</p>

<p>It was reasonably straightforward to do this by configuring separate linked service connections, data sets and pipelines for each distinct file share that needed to be included, but I'd prefer to manage a single pipeline.</p>

<p><strong>I already tried to create the JSON of the linked services but it didn't work, someone who can help me?</strong></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services</a></p>
","<azure><azure-functions><azure-data-factory>","2020-04-09 20:18:25","439","0","2","61148565","<p><a href=""https://i.stack.imgur.com/7XDgq.jpg"" rel=""nofollow noreferrer"">This is how I solved it :)</a></p>
<p>the configuration was as follows:</p>
"
"61123171","How to set the file name based on a parameter in the sink?","<p>Is it possible to set the file name based on parameter in the sink of a mapping data flow?
The expression builder thinks this is fine, and the parameter is drag-and-drop:able in the interface, but when running the data flow in a pipeline (that passes a pipeline parameter into the data flow parameter $OutFileName) it fails with ""unsupported syntax in expression. Details:at Sink""</p>

<p>I have tried different variations of the expression, like ""{$OutFileName}"" and with the toString() function and so on...</p>

<p><a href=""https://i.stack.imgur.com/jsEFJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jsEFJ.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-04-09 14:11:52","672","0","2","61128634","<p>Is $OutFileName a data flow param of type String? If so, all you should have to do from the pipeline is set it like this in activity's parameter setting: (choose ""data flow expression"") 'myfile.txt'</p>

<p><a href=""https://i.stack.imgur.com/1dIxA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1dIxA.png"" alt=""enter image description here""></a></p>
"
"61123171","How to set the file name based on a parameter in the sink?","<p>Is it possible to set the file name based on parameter in the sink of a mapping data flow?
The expression builder thinks this is fine, and the parameter is drag-and-drop:able in the interface, but when running the data flow in a pipeline (that passes a pipeline parameter into the data flow parameter $OutFileName) it fails with ""unsupported syntax in expression. Details:at Sink""</p>

<p>I have tried different variations of the expression, like ""{$OutFileName}"" and with the toString() function and so on...</p>

<p><a href=""https://i.stack.imgur.com/jsEFJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jsEFJ.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-04-09 14:11:52","672","0","2","61224860","<p>The expression for the sink file name must contain single quotes, meaning that you have to have them in the pipeline expression/parameter you're passing - or you must have the box for ""Expression"" un-checked in the parameter tab of the data flow activity in the pipeline as it will then add the quotes.</p>
"
"61121324","Terraform azurerm_data_factory vsts_configuration failing with Error: Error configuring Repository for Data Factory","<p>I'm trying to set up the code repository in Azure Data Factory using Terraform deploying with Azure Cloud Shell with contributor access following this: <a href=""https://www.terraform.io/docs/providers/azurerm/r/data_factory.html#vsts_configuration"" rel=""nofollow noreferrer"">https://www.terraform.io/docs/providers/azurerm/r/data_factory.html#vsts_configuration</a></p>

<p>I'm getting the error message:</p>

<blockquote>
  <p>Error: Error configuring Repository for Data Factory ""adf-name""
  (Resource Group ""rg-name""):
  datafactory.FactoriesClient#ConfigureFactoryRepo: Failure responding
  to request: StatusCode=403 -- Original Error: autorest/azure: Service
  returned an error. Status=403 Code=""AuthorizationFailed"" Message=""The
  client 'xxx@xxx.com' with object id 'xxxxx' does not have
  authorization to perform action
  'Microsoft.DataFactory/locations/configureFactoryRepo/action' over
  scope '/subscriptions/xxxxxx' or the scope is invalid. If access was recently granted,
  please refresh your credentials.</p>
</blockquote>

<p>I've de-sensitised the client, object id and scope.</p>

<p>I am able to set up the code repository in the portal, but fails when I try and run the terraform in the Azure Cloud Shell. Has anyone seen this error message before or know how to get past it?</p>

<p>Code snip it:</p>

<pre><code>`provider ""azurerm"" {
  version = ""=2.3.0""
  features {}
}
resource ""azurerm_data_factory"" ""example"" {
  name                = var.adf_name
  location            = var.location
  resource_group_name = var.rg_name
  vsts_configuration {
    account_name      = var.account_name
    branch_name       = var.branch_name
    project_name      = var.project_name
    repository_name   = var.repo_name
    root_folder       = var.root_folder
    tenant_id         = var.tenant_id
  }
}`
</code></pre>
","<azure><terraform><azure-data-factory><terraform-provider-azure>","2020-04-09 12:37:25","2388","3","1","61457892","<p>A custom role had to be added for the action ‘ Microsoft.DataFactory/locations/configureFactoryRepo/action’ and assigned to the service principal. Contributor role itself was not enough to set up the code repository for Azure Data Factory using Terraform azurerm. </p>
"
"61119140","Map nested JSON in Azure Data Factory to raw object","<p>Since ADF (Azure Data Factory) isn't able to handle complex/nested JSON objects, I'm using OPENJSON in SQL to parse the objects. But, I can't get the 'raw' JSON from the following object:</p>

<pre><code>{
   ""rows"":[
      {
         ""name"":""Name1"",
         ""attribute1"":""attribute1"",
         ""attribute2"":""attribute2""
      },
      {
         ""name"":""Name2"",
         ""attribute1"":""attribute1"",
         ""attribute2"":""attribute2""
      },
      {
         ""name"":""Name3"",
         ""attribute1"":""attribute1"",
         ""attribute2"":""attribute2""
      }
   ]
}
</code></pre>

<p><strong>Config 1</strong></p>

<p>When I use this config: <a href=""https://i.stack.imgur.com/zvs4R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zvs4R.png"" alt=""enter image description here""></a></p>

<p>I get all the names listed</p>

<ol>
<li>Name1</li>
<li>Name2</li>
<li>Name3</li>
</ol>

<p>Result:</p>

<p><a href=""https://i.imgur.com/xx0eLQK.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/xx0eLQK.png"" alt=""enter image description here""></a></p>

<p><strong>Config 2</strong></p>

<p>When I use this config:</p>

<p><a href=""https://i.stack.imgur.com/XgDJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XgDJX.png"" alt=""enter image description here""></a></p>

<p>I get the whole JSON in one record:</p>

<ol>
<li>[ {{full JSON}} ] </li>
</ol>

<p>Result:</p>

<p><a href=""https://i.imgur.com/S5QW88s.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/S5QW88s.png"" alt=""enter image description here""></a></p>

<p><strong>Needed config</strong></p>

<p>But, what I want, is this result:</p>

<ol>
<li>{ ""name"":""Name1"", ""attribute1"":""attribute1"", ""attribute2"":""attribute2  }</li>
<li>{ ""name"":""Name2"", ""attribute1"":""attribute1"", ""attribute2"":""attribute2  }</li>
<li>{ ""name"":""Name3"", ""attribute1"":""attribute1"", ""attribute2"":""attribute2  }</li>
</ol>

<p>Result:</p>

<p><a href=""https://i.imgur.com/mz88k7D.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/mz88k7D.png"" alt=""enter image description here""></a></p>

<p>So, I need the iteration of Config 1, with the raw JSON per row. Everytime I use the $['rows'], or $['rows'][0], it seems to 'forget' to iterate.</p>

<p>Anyone?</p>
","<json><azure-data-factory>","2020-04-09 10:31:35","6159","1","2","61128492","<p>Have you tried Data Flows to handle JSON structures? We have that feature built-in with data flow transformations like derived column, flatten, and sink mapping.</p>
"
"61119140","Map nested JSON in Azure Data Factory to raw object","<p>Since ADF (Azure Data Factory) isn't able to handle complex/nested JSON objects, I'm using OPENJSON in SQL to parse the objects. But, I can't get the 'raw' JSON from the following object:</p>

<pre><code>{
   ""rows"":[
      {
         ""name"":""Name1"",
         ""attribute1"":""attribute1"",
         ""attribute2"":""attribute2""
      },
      {
         ""name"":""Name2"",
         ""attribute1"":""attribute1"",
         ""attribute2"":""attribute2""
      },
      {
         ""name"":""Name3"",
         ""attribute1"":""attribute1"",
         ""attribute2"":""attribute2""
      }
   ]
}
</code></pre>

<p><strong>Config 1</strong></p>

<p>When I use this config: <a href=""https://i.stack.imgur.com/zvs4R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zvs4R.png"" alt=""enter image description here""></a></p>

<p>I get all the names listed</p>

<ol>
<li>Name1</li>
<li>Name2</li>
<li>Name3</li>
</ol>

<p>Result:</p>

<p><a href=""https://i.imgur.com/xx0eLQK.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/xx0eLQK.png"" alt=""enter image description here""></a></p>

<p><strong>Config 2</strong></p>

<p>When I use this config:</p>

<p><a href=""https://i.stack.imgur.com/XgDJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XgDJX.png"" alt=""enter image description here""></a></p>

<p>I get the whole JSON in one record:</p>

<ol>
<li>[ {{full JSON}} ] </li>
</ol>

<p>Result:</p>

<p><a href=""https://i.imgur.com/S5QW88s.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/S5QW88s.png"" alt=""enter image description here""></a></p>

<p><strong>Needed config</strong></p>

<p>But, what I want, is this result:</p>

<ol>
<li>{ ""name"":""Name1"", ""attribute1"":""attribute1"", ""attribute2"":""attribute2  }</li>
<li>{ ""name"":""Name2"", ""attribute1"":""attribute1"", ""attribute2"":""attribute2  }</li>
<li>{ ""name"":""Name3"", ""attribute1"":""attribute1"", ""attribute2"":""attribute2  }</li>
</ol>

<p>Result:</p>

<p><a href=""https://i.imgur.com/mz88k7D.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/mz88k7D.png"" alt=""enter image description here""></a></p>

<p>So, I need the iteration of Config 1, with the raw JSON per row. Everytime I use the $['rows'], or $['rows'][0], it seems to 'forget' to iterate.</p>

<p>Anyone?</p>
","<json><azure-data-factory>","2020-04-09 10:31:35","6159","1","2","61135154","<p>The copy active can help us achieve it.</p>

<p>For example I copy <code>B.json</code> fron container ""backup"" to another Blob container ""testcontainer"" .</p>

<p>This is my <code>B.json</code>  source dataset:</p>

<p><a href=""https://i.stack.imgur.com/E1EkL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1EkL.png"" alt=""enter image description here""></a></p>

<p><strong>Source:</strong></p>

<p><a href=""https://i.stack.imgur.com/VlStd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VlStd.png"" alt=""enter image description here""></a></p>

<p><strong>Sink:</strong>
<a href=""https://i.stack.imgur.com/6YkX8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6YkX8.png"" alt=""enter image description here""></a></p>

<p><strong>Mapping:</strong></p>

<p><a href=""https://i.stack.imgur.com/jmb6A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jmb6A.png"" alt=""enter image description here""></a></p>

<p><strong>Pipeline executed successful:</strong></p>

<p><a href=""https://i.stack.imgur.com/TQH90.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TQH90.png"" alt=""enter image description here""></a></p>

<p><strong>Check the data in testcontainer:</strong></p>

<p><a href=""https://i.stack.imgur.com/tx0An.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tx0An.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>

<hr>

<p><strong>Update:</strong></p>

<p>Copy the nested json to SQL.</p>

<p>Source is the same <code>B.json</code> in blob.</p>

<p><strong>Sink dataset:</strong></p>

<p><a href=""https://i.stack.imgur.com/tdWq0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tdWq0.png"" alt=""enter image description here""></a></p>

<p><strong>Sink:</strong></p>

<p><a href=""https://i.stack.imgur.com/NZXBn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NZXBn.png"" alt=""enter image description here""></a></p>

<p><strong>Mapping:</strong></p>

<p><a href=""https://i.stack.imgur.com/AFMhI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AFMhI.png"" alt=""enter image description here""></a></p>

<p><strong>Run pipeline:</strong></p>

<p><a href=""https://i.stack.imgur.com/rL2ND.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rL2ND.png"" alt=""enter image description here""></a></p>

<p><strong>Check the data in SQL database:</strong></p>

<p><a href=""https://i.stack.imgur.com/wFko8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wFko8.png"" alt=""enter image description here""></a></p>
"
"61118566","How to publish pipeline in Azure Data Factory enabled with DeVOPS GIT repo using .NET Data Factory SDK (C# )?","<p>I’m facing some issue in deploying the pipeline to Data Factory with GIT enabled (DevOps) and need someone help or advise on that.</p>

<p>When I deploy the pipeline through below code snippet its deploying into Data Factory Repo but instead we need to publish the code to Azure DevOps GIT Repo.</p>

<p>Below is a code snippet used to publish pipeline to ADF using .NET Data Factory SDK (C#)</p>

<p>// Authenticate and create a data factory management client  </p>

<pre><code>var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
ClientCredential cc = new ClientCredential(applicationId, AuthenticationKey);
AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
DataFactoryManagementClient client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId,  };
</code></pre>

<p>// Below snippet deploys pipeline into data factory Repo</p>

<pre><code>client.Pipelines.CreateOrUpdate(resourceGroup, dataFactoryName, pipelineName, pipeline);
</code></pre>

<p>But now we need to publish this pipeline code to master branch for the DevOps GIT enabled in our ADF.</p>

<p>Below is one code snippet I found but not sure how to set this object to data factory in deploying the pipeline to DevOps GIT.</p>

<pre><code>FactoryRepoConfiguration repo = new FactoryVSTSConfiguration(""account Name"", ""Repo name"", ""branch name"", ""/"", ""project name"");
</code></pre>

<p>I have gone through many blogs but didn’t find any help on that.</p>

<p>So could someone please help me on this? </p>
","<c#><azure><azure-data-factory>","2020-04-09 10:00:13","3068","4","3","61200079","<p>If i don't misunderstanding your requirement,you just wanna implement below creation with sdk code as same as portal(This is just sample project,no security for it then):</p>

<p><a href=""https://i.stack.imgur.com/nngxL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nngxL.png"" alt=""enter image description here""></a></p>

<p>In fact, you are very close to your goal because you already found <code>FactoryRepoConfiguration</code> Class. Please refer to the <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/Factories/CreateOrUpdate#request-body"" rel=""nofollow noreferrer"">document</a>,you could define 2 types of Repo: <code>VSTS</code> and <code>Github</code>.</p>

<p><a href=""https://i.stack.imgur.com/E1qbH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1qbH.png"" alt=""enter image description here""></a></p>

<p>I tested <code>Github</code> for your reference:</p>

<p><a href=""https://i.stack.imgur.com/sM89S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sM89S.png"" alt=""enter image description here""></a></p>
"
"61118566","How to publish pipeline in Azure Data Factory enabled with DeVOPS GIT repo using .NET Data Factory SDK (C# )?","<p>I’m facing some issue in deploying the pipeline to Data Factory with GIT enabled (DevOps) and need someone help or advise on that.</p>

<p>When I deploy the pipeline through below code snippet its deploying into Data Factory Repo but instead we need to publish the code to Azure DevOps GIT Repo.</p>

<p>Below is a code snippet used to publish pipeline to ADF using .NET Data Factory SDK (C#)</p>

<p>// Authenticate and create a data factory management client  </p>

<pre><code>var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
ClientCredential cc = new ClientCredential(applicationId, AuthenticationKey);
AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
DataFactoryManagementClient client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId,  };
</code></pre>

<p>// Below snippet deploys pipeline into data factory Repo</p>

<pre><code>client.Pipelines.CreateOrUpdate(resourceGroup, dataFactoryName, pipelineName, pipeline);
</code></pre>

<p>But now we need to publish this pipeline code to master branch for the DevOps GIT enabled in our ADF.</p>

<p>Below is one code snippet I found but not sure how to set this object to data factory in deploying the pipeline to DevOps GIT.</p>

<pre><code>FactoryRepoConfiguration repo = new FactoryVSTSConfiguration(""account Name"", ""Repo name"", ""branch name"", ""/"", ""project name"");
</code></pre>

<p>I have gone through many blogs but didn’t find any help on that.</p>

<p>So could someone please help me on this? </p>
","<c#><azure><azure-data-factory>","2020-04-09 10:00:13","3068","4","3","61269190","<p>As per Microsoft's feedback there is no SDK which can publish code directly into a GIT branch in ADF.</p>

<p>I have posted an idea in ADF forums to implement the same and below is the link. If anyone have a same requirements request to up vote the posted idea in below link.</p>

<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/40195249-implement-a-net-sdk-to-publish-pipeline-code-to-a"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/40195249-implement-a-net-sdk-to-publish-pipeline-code-to-a</a></p>

<p>Thanks.</p>
"
"61118566","How to publish pipeline in Azure Data Factory enabled with DeVOPS GIT repo using .NET Data Factory SDK (C# )?","<p>I’m facing some issue in deploying the pipeline to Data Factory with GIT enabled (DevOps) and need someone help or advise on that.</p>

<p>When I deploy the pipeline through below code snippet its deploying into Data Factory Repo but instead we need to publish the code to Azure DevOps GIT Repo.</p>

<p>Below is a code snippet used to publish pipeline to ADF using .NET Data Factory SDK (C#)</p>

<p>// Authenticate and create a data factory management client  </p>

<pre><code>var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
ClientCredential cc = new ClientCredential(applicationId, AuthenticationKey);
AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
DataFactoryManagementClient client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId,  };
</code></pre>

<p>// Below snippet deploys pipeline into data factory Repo</p>

<pre><code>client.Pipelines.CreateOrUpdate(resourceGroup, dataFactoryName, pipelineName, pipeline);
</code></pre>

<p>But now we need to publish this pipeline code to master branch for the DevOps GIT enabled in our ADF.</p>

<p>Below is one code snippet I found but not sure how to set this object to data factory in deploying the pipeline to DevOps GIT.</p>

<pre><code>FactoryRepoConfiguration repo = new FactoryVSTSConfiguration(""account Name"", ""Repo name"", ""branch name"", ""/"", ""project name"");
</code></pre>

<p>I have gone through many blogs but didn’t find any help on that.</p>

<p>So could someone please help me on this? </p>
","<c#><azure><azure-data-factory>","2020-04-09 10:00:13","3068","4","3","62021884","<p>There is another approach to publish ADF, from master (collaboration) branch or even from your local machine (if needed). With that approach, you can develop your ADF using your .NET framework and when all files ready - publish them into ADF service.<br>
Check this out:
<a href=""https://www.powershellgallery.com/packages/azure.datafactory.tools/"" rel=""nofollow noreferrer"">azure.datafactory.tools</a> (PowerShell module)</p>
"
"61110248","When creating a Copy Activity in an Azure Data Factory pipeline, how do I map a CSV sheet with 5 columns to a CSV sheet with 20 columns?","<p>So I have an input CSV sheet that I want to copy into an output CSV sheet. The output CSV sheet has all the columns in the input sheet, plus a bunch of other columns. (I will be copying data into those from other input sheets later.)</p>

<p>When I run the pipeline containing my Copy Activity, the only columns present in the new output sheet are the 5 columns from the input sheet, I assume because those are the only ones in the mapping. However, I've also tried creating 15 ""Additional Columns"" in the ""source"" section of the Copy Activity --- just trying out things like ""test"", \""test\"", test, @test, @pipeline().DataFactory, $$FILEPATH, etc. --- but when I debug the pipeline and go back to my container and look at the output sheet, still only the 5 columns from the input sheet are present there!</p>

<p>How do I get the output sheet to contain columns that are not present in the input sheet? Do I need to create an ARM template?</p>

<p>I am doing this entirely via the Azure Portal, btw.</p>
","<csv><azure-blob-storage><azure-data-factory>","2020-04-08 21:21:58","397","0","2","61112586","<p>This will be much easier to design in ADF's data flows instead by creating Derived Columns to append to your output sink schema</p>
"
"61110248","When creating a Copy Activity in an Azure Data Factory pipeline, how do I map a CSV sheet with 5 columns to a CSV sheet with 20 columns?","<p>So I have an input CSV sheet that I want to copy into an output CSV sheet. The output CSV sheet has all the columns in the input sheet, plus a bunch of other columns. (I will be copying data into those from other input sheets later.)</p>

<p>When I run the pipeline containing my Copy Activity, the only columns present in the new output sheet are the 5 columns from the input sheet, I assume because those are the only ones in the mapping. However, I've also tried creating 15 ""Additional Columns"" in the ""source"" section of the Copy Activity --- just trying out things like ""test"", \""test\"", test, @test, @pipeline().DataFactory, $$FILEPATH, etc. --- but when I debug the pipeline and go back to my container and look at the output sheet, still only the 5 columns from the input sheet are present there!</p>

<p>How do I get the output sheet to contain columns that are not present in the input sheet? Do I need to create an ARM template?</p>

<p>I am doing this entirely via the Azure Portal, btw.</p>
","<csv><azure-blob-storage><azure-data-factory>","2020-04-08 21:21:58","397","0","2","61112747","<p>This works fine on my side. Is there any differences?</p>

<p><a href=""https://i.stack.imgur.com/Eomfx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Eomfx.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/1CxEG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1CxEG.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/wYkDe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wYkDe.png"" alt=""enter image description here""></a></p>
"
"61109985","Azure datafactory v2 Execute Pipeline with For Each","<p>I am trying to use ""Execute Pipeline"" to invoke a Pipe which has a ForEach activity. I get an error.</p>

<ol>
<li>Json for Execute pipe:</li>
</ol>



<pre><code>[
    {
        ""name"": ""pipeline3"",
        ""properties"": {
            ""activities"": [
                {
                    ""name"": ""Test_invoke1"",
                    ""type"": ""ExecutePipeline"",
                    ""dependsOn"": [],
                    ""userProperties"": [],
                    ""typeProperties"": {
                        ""pipeline"": {
                            ""referenceName"": ""MAIN_SA_copy1"",
                            ""type"": ""PipelineReference""
                        },
                        ""waitOnCompletion"": true
                    }
                }
            ],
            ""annotations"": []
        }
    }
]
</code></pre>

<ol start=""2"">
<li>Jason for Invoke pipe for each activity : </li>
</ol>



<pre><code>[
    {
        ""name"": ""MAIN_SA_copy1"",
        ""properties"": {
            ""activities"": [
                {
                    ""name"": ""Collect_SA_Data"",
                    ""type"": ""ForEach"",
                    ""dependsOn"": [],
                    ""userProperties"": [],
                    ""typeProperties"": {
                        ""items"": {
                            ""value"": ""@pipeline().parameters.TableNames"",
                            ""type"": ""Expression""
                        },
                        ""batchCount"": 15,
                        ""activities"": [
                            {
                                ""name"": ""Sink_SAdata_toDL"",
                                ""type"": ""Copy"",
                                ""dependsOn"": [],
                                ""policy"": {
                                    ""timeout"": ""7.00:00:00"",
                                    ""retry"": 0,
                                    ""retryIntervalInSeconds"": 30,
                                    ""secureOutput"": false,
                                    ""secureInput"": false
                                },
                                ""userProperties"": [
                                    {
                                        ""name"": ""Destination"",
                                        ""value"": ""@{pipeline().parameters.DLFilePath}/@{item()}""
                                    }
                                ],
                                ""typeProperties"": {
                                    ""source"": {
                                        ""type"": ""SqlServerSource"",
                                        ""sqlReaderQuery"": {
                                            ""value"": ""@concat('SELECT * FROM ',item())"",
                                            ""type"": ""Expression""
                                        }
                                    },
                                    ""sink"": {
                                        ""type"": ""AzureBlobFSSink""
                                    },
                                    ""enableStaging"": false,
                                    ""parallelCopies"": 1,
                                    ""dataIntegrationUnits"": 4
                                },
                                ""inputs"": [
                                    {
                                        ""referenceName"": ""SrcDS_StructuringAnalytics"",
                                        ""type"": ""DatasetReference""
                                    }
                                ],
                                ""outputs"": [
                                    {
                                        ""referenceName"": ""ADLS"",
                                        ""type"": ""DatasetReference"",
                                        ""parameters"": {
                                            ""FilePath"": ""@pipeline().parameters.DLFilePath"",
                                            ""FileName"": {
                                                ""value"": ""@concat(item(),'.orc')"",
                                                ""type"": ""Expression""
                                            }
                                        }
                                    }
                                ]
                            }
                        ]
                    }
                }
            ],
            ""parameters"": {
                ""DLFilePath"": {
                    ""type"": ""string"",
                    ""defaultValue"": ""extracts/StructuringAnalytics""
                },
                ""TableNames"": {
                    ""type"": ""array"",
                    ""defaultValue"": [
                        ""fom.FOMLineItem_manual""
                    ]
                }
            },
            ""variables"": {
                ""QryTableColumn"": {
                    ""type"": ""String""
                },
                ""QryTable"": {
                    ""type"": ""String""
                }
            },
            ""folder"": {
                ""name"": ""StructuringAnalytics""
            },
            ""annotations"": []
        },
        ""type"": ""Microsoft.DataFactory/factories/pipelines""
    }
]
</code></pre>

<p>I get an error: </p>



<pre><code>[
    {
        ""errorCode"": ""BadRequest"",
        ""message"": ""Operation on target Collect_SA_Data failed: The execution of template action 'Collect_SA_Data' failed: the result of the evaluation of 'foreach' expression '@pipeline().parameters.TableNames' is of type 'String'. The result must be a valid array."",
        ""failureType"": ""UserError"",
        ""target"": ""Test_invoke1"",
        ""details"": """"
    }
]
</code></pre>

<p>Input: </p>



<pre><code>""pipeline"": {
    ""referenceName"": ""MAIN_SA_copy1"",
    ""type"": ""PipelineReference""
},
""waitOnCompletion"": true,
""parameters"": {
    ""DLFilePath"": ""extracts/StructuringAnalytics"",
    ""TableNames"": ""[\""fom.FOMLineItem_manual\""]""
}
</code></pre>
","<azure><azure-data-factory>","2020-04-08 21:03:07","3381","1","2","61111882","<p>Please try updating your dynamic expression of ForEach Items as below:</p>

<pre><code>{
    ""value"": ""@array(pipeline().parameters.TableNames)"",
    ""type"": ""Expression""
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/XME1d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XME1d.png"" alt=""enter image description here""></a></p>

<p>Hope this helps. </p>
"
"61109985","Azure datafactory v2 Execute Pipeline with For Each","<p>I am trying to use ""Execute Pipeline"" to invoke a Pipe which has a ForEach activity. I get an error.</p>

<ol>
<li>Json for Execute pipe:</li>
</ol>



<pre><code>[
    {
        ""name"": ""pipeline3"",
        ""properties"": {
            ""activities"": [
                {
                    ""name"": ""Test_invoke1"",
                    ""type"": ""ExecutePipeline"",
                    ""dependsOn"": [],
                    ""userProperties"": [],
                    ""typeProperties"": {
                        ""pipeline"": {
                            ""referenceName"": ""MAIN_SA_copy1"",
                            ""type"": ""PipelineReference""
                        },
                        ""waitOnCompletion"": true
                    }
                }
            ],
            ""annotations"": []
        }
    }
]
</code></pre>

<ol start=""2"">
<li>Jason for Invoke pipe for each activity : </li>
</ol>



<pre><code>[
    {
        ""name"": ""MAIN_SA_copy1"",
        ""properties"": {
            ""activities"": [
                {
                    ""name"": ""Collect_SA_Data"",
                    ""type"": ""ForEach"",
                    ""dependsOn"": [],
                    ""userProperties"": [],
                    ""typeProperties"": {
                        ""items"": {
                            ""value"": ""@pipeline().parameters.TableNames"",
                            ""type"": ""Expression""
                        },
                        ""batchCount"": 15,
                        ""activities"": [
                            {
                                ""name"": ""Sink_SAdata_toDL"",
                                ""type"": ""Copy"",
                                ""dependsOn"": [],
                                ""policy"": {
                                    ""timeout"": ""7.00:00:00"",
                                    ""retry"": 0,
                                    ""retryIntervalInSeconds"": 30,
                                    ""secureOutput"": false,
                                    ""secureInput"": false
                                },
                                ""userProperties"": [
                                    {
                                        ""name"": ""Destination"",
                                        ""value"": ""@{pipeline().parameters.DLFilePath}/@{item()}""
                                    }
                                ],
                                ""typeProperties"": {
                                    ""source"": {
                                        ""type"": ""SqlServerSource"",
                                        ""sqlReaderQuery"": {
                                            ""value"": ""@concat('SELECT * FROM ',item())"",
                                            ""type"": ""Expression""
                                        }
                                    },
                                    ""sink"": {
                                        ""type"": ""AzureBlobFSSink""
                                    },
                                    ""enableStaging"": false,
                                    ""parallelCopies"": 1,
                                    ""dataIntegrationUnits"": 4
                                },
                                ""inputs"": [
                                    {
                                        ""referenceName"": ""SrcDS_StructuringAnalytics"",
                                        ""type"": ""DatasetReference""
                                    }
                                ],
                                ""outputs"": [
                                    {
                                        ""referenceName"": ""ADLS"",
                                        ""type"": ""DatasetReference"",
                                        ""parameters"": {
                                            ""FilePath"": ""@pipeline().parameters.DLFilePath"",
                                            ""FileName"": {
                                                ""value"": ""@concat(item(),'.orc')"",
                                                ""type"": ""Expression""
                                            }
                                        }
                                    }
                                ]
                            }
                        ]
                    }
                }
            ],
            ""parameters"": {
                ""DLFilePath"": {
                    ""type"": ""string"",
                    ""defaultValue"": ""extracts/StructuringAnalytics""
                },
                ""TableNames"": {
                    ""type"": ""array"",
                    ""defaultValue"": [
                        ""fom.FOMLineItem_manual""
                    ]
                }
            },
            ""variables"": {
                ""QryTableColumn"": {
                    ""type"": ""String""
                },
                ""QryTable"": {
                    ""type"": ""String""
                }
            },
            ""folder"": {
                ""name"": ""StructuringAnalytics""
            },
            ""annotations"": []
        },
        ""type"": ""Microsoft.DataFactory/factories/pipelines""
    }
]
</code></pre>

<p>I get an error: </p>



<pre><code>[
    {
        ""errorCode"": ""BadRequest"",
        ""message"": ""Operation on target Collect_SA_Data failed: The execution of template action 'Collect_SA_Data' failed: the result of the evaluation of 'foreach' expression '@pipeline().parameters.TableNames' is of type 'String'. The result must be a valid array."",
        ""failureType"": ""UserError"",
        ""target"": ""Test_invoke1"",
        ""details"": """"
    }
]
</code></pre>

<p>Input: </p>



<pre><code>""pipeline"": {
    ""referenceName"": ""MAIN_SA_copy1"",
    ""type"": ""PipelineReference""
},
""waitOnCompletion"": true,
""parameters"": {
    ""DLFilePath"": ""extracts/StructuringAnalytics"",
    ""TableNames"": ""[\""fom.FOMLineItem_manual\""]""
}
</code></pre>
","<azure><azure-data-factory>","2020-04-08 21:03:07","3381","1","2","65700008","<p>I guess you were using the UI to set the pipeline and its parameters and I guess you expected to put the array parameter of the called pipeline as everywhere else like this:
(It is all my guess, because I just did exactly the same, with the same result)</p>
<p><a href=""https://i.stack.imgur.com/DwrEJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DwrEJ.png"" alt=""Wrong"" /></a></p>
<p>The trick is to define the array in the Code ([&quot;table1&quot;, &quot;table2&quot;]):
<a href=""https://i.stack.imgur.com/KhJei.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KhJei.png"" alt=""Code"" /></a></p>
<p>The input in the UI will look like this:</p>
<p><a href=""https://i.stack.imgur.com/Vv84k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vv84k.png"" alt=""Right"" /></a></p>
<p>Now it works!<br />
It seems, that the Datafactory is otherwise treating the whole array as one element of some array. Hence, the solution with the array() function sometimes works.<br />
It looks like a bug, defining array parameter input..</p>
<p>(Had to edit the answer, I first thought omiting the colons in the UI input would be enough)</p>
"
"61098163","Unable to read Multi-select option set in Azure Data Factory","<p>I am getting data from Dynamics CE with Data Factory and want to write it to an Azure Data Lake. But I encounter this Data Type error: Microsoft.Xrm.Sdk.OptionSetValueCollection is not supported.</p>

<p>I've checked the forums that say it is not supported and I checked this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365#data-type-mapping-for-dynamics"" rel=""nofollow noreferrer"">Microsoft Docs</a> that says AttributeType.MultiSelectPicklist is not supported.</p>

<p>I was wondering if anyone has a workaround for retrieving columns with a multi-select option set value from Dynamics CE or is encountering the same issue.</p>

<p>Does it mean we can not use this data type in Dynamics CE if we want to let it flow through the Data Factory?</p>

<p>Kind Regards</p>

<p>Berry</p>
","<azure-data-factory><custom-data-type>","2020-04-08 10:08:50","1265","2","1","61110335","<p>After verifying with ADF team it is confirmed that this column type <code>OptionSetValueCollection</code> is not supported in ADF Dynamics CRM connector.</p>

<p>I’m not sure whether Dynamics side can do any column type conversion. 
Without existing connectors, the only way is to write some custom code which can implement the similar job as Dynamics CRM connector, where we can implement to fetch Option Set Value Collection column value. ADF supports to run customer application by using <a href=""https://learn.microsoft.com/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a>. This should work but requires additional custom development.</p>

<p>I would recommend you to please share your feedback/suggestion in ADF user voice forum: <a href=""https://feedback.azure.com/forums/270578-azure-data-factory"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-azure-data-factory</a>.
All the feedback shared in this forum are monitored and reviewed by ADF engineering team. Please do share the feedback/suggestion link here once it is created, as it would help other users with similar idea to up-vote/comment on your feedback. </p>

<p>Ref Doc : <a href=""https://learn.microsoft.com/azure/data-factory/connector-dynamics-crm-office-365#data-type-mapping-for-dynamics"" rel=""nofollow noreferrer"">Data type mapping for Dynamics</a></p>
"
"61096991","Loop through all containers in a blob storage container with ADF","<p>I want to loop through all containers in a blob storage account with Azure Data Factory. (Because all data supplying parties have their own container but with the same files). The number of containers will increase during time.</p>

<p>Any suggestions?</p>
","<azure-data-factory><azure-blob-storage>","2020-04-08 09:02:02","821","4","1","61112804","<p>Your need could be implemented by Get Metadata Activity simply.</p>

<p>Create DataSet based on your Blob Storage Account Linked Service and don't set any container name.</p>

<p><a href=""https://i.stack.imgur.com/dbC6c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dbC6c.png"" alt=""enter image description here""></a></p>

<p>Then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get Metadata Activity</a> to get all container names in your account.</p>

<p><a href=""https://i.stack.imgur.com/X88As.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X88As.png"" alt=""enter image description here""></a></p>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/vFhyz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vFhyz.png"" alt=""enter image description here""></a></p>
"
"61095801","Unable to create a folder inside a blob container using azure data factory","<p>New to ADF so pardon me if this is pretty basic.</p>

<p>Trying to copy bla.docx from source(blob container)/screenshots(folder)/bla.docx to target(blob container)/screenshots(folder).</p>

<p>But screenshots folder doesn't exist in target container. Need to make it using the pipeline.</p>

<p>How to achieve that? </p>
","<azure><azure-data-factory>","2020-04-08 07:52:51","1743","1","1","61096912","<p>You have to create 2 datasets for source and sink individually. It's ok that you have no target directory in the sink dataset. You just need to define it in the sink dataset configuration and ADF will create it for you.</p>

<p><a href=""https://i.stack.imgur.com/Rptkk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rptkk.png"" alt=""enter image description here""></a></p>

<p>Test output:</p>

<p><a href=""https://i.stack.imgur.com/hC15f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hC15f.png"" alt=""enter image description here""></a></p>
"
"61082330","Spark submit in Azure Data Factory","<p>How do we integrate a spark-scala JAR in Azure data factory? I have an existing Spark Jar built with Scala and I need to invoke this from ADF's - Spark Activity. </p>
","<scala><azure><apache-spark><azure-data-factory><azure-hdinsight>","2020-04-07 14:26:42","602","0","1","61222798","<p><strong>Note:</strong> In Azure Data Factory the entryFile must be either a <strong>Python</strong> or a <strong>jar file</strong>, you would need to compile your Scala source code to jar file before running it as spark applications.</p>

<p><a href=""https://i.stack.imgur.com/5NoXu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5NoXu.png"" alt=""enter image description here""></a></p>

<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark</a></p>
"
"61081511","How do I execute a Azure Data Factory pipeline X times (for X distinct parameter values)?","<p>I have a pipeline that is triggered daily at a certain time, which works fine. This pipeline gets the current date as the input parameter, which is also fine.</p>

<p>Now I'm doing an alternative version of that same pipeline <strong>to simulate past dates</strong>, in which the parameter is the date value (for instance '2018-05-23' for May 23rd 2018). However, if I want to run the pipeline for the whole year 2018 (so I'd need 365 distinct parameters), how would I do that?</p>

<p>Of course I could trigger it manually with January 1st, then January 2nd and so on, but that would take forever. I'd like to trigger January 1st and then <strong>sequentially have the pipeline executed</strong> for the other 364 days of the year, once January 1st is finished.</p>

<p>Thanks in advance!</p>
","<azure><azure-data-factory>","2020-04-07 13:47:03","664","0","1","61083877","<p>So... to loop through a series of dates, I'd suggest populating an input parameter with an array of date values (you can generate this in a straightforward way through code, and pass this as a parameter to your pipeline).</p>

<p>Then, in your pipeline, you can define a ForEach loop, iterating through each of the date entries in your parameter, executing the pipeline for each date.</p>

<p>Based on your requirement to run the pipelines in sequence, without parallelism, just be sure to set <code>isSequential</code> to <code>true</code> in your pipeline's <code>ForEach</code> definition. Something like this, as a simple example:</p>

<pre><code>{  
   ""name"":""..."",
   ""type"":""ForEach"",
   ""typeProperties"":{  
      ""isSequential"":""true"",
        ""items"": {
            ""value"": ""@pipeline().parameters.datesToProcess"",
            ""type"": ""Expression""
        },
      ""activities"":[  
         { 
            ...
         }
      ]
    }
}
</code></pre>

<p>You can read more about <code>ForEach</code> in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">this article</a>.</p>
"
"61079303","Read a text file and send the content to a Azure Function in Azure Data Factory","<p>I must have missed something big time when it comes to Azure Data Factory. This is what I want to do.
A have a textfile stored on my local network, I want to get that content (as is) and send that to a Azure function and then the azure function should work with that content to mapp it into a specific json object and send the result back to Data Factory.</p>

<p>But I can't find a simple way to take the content of the file and send it into the Azure Function. The closed I am getting is to use the 'lookup' activity with a dataset that have some characters as column and row delimiter that don't exist in the file. That will give me the data but wrapped in a Json object that looks like this:</p>

<pre><code>{
  ""count"":1"",
  ""value"": {
    ""Prop_0"":""MY_TEXT_FILES_CONTENT""
  }
}
</code></pre>

<p>But that seems a bit overkill what I want is either</p>

<pre><code>MY_TEXT_FILES_CONTENT
</code></pre>

<p>or </p>

<pre><code>{
 ""output"": ""MY_TEXT_FILES_CONTENT""
}
</code></pre>

<p>I must have missed something, isn't this like one of the most common task in integration to read a simple text file to get the content of it...</p>

<p>Best regards
/Magnus</p>
","<azure-functions><text-files><azure-data-factory>","2020-04-07 11:44:14","1266","-1","1","61079613","<p>IMHO, the client application can just upload the file to a BLOB in azure and have the function triggered as soon as the upload is completed. The function can start processing and internally handle the <code>JSON</code> transformation / mapping and send to the data factory.</p>

<p>This is suggested given the fact that you are operating on files and that they can be of a higher size also and there are API limits and streaming limitations might be there in the functions</p>
"
"61070676","Set a variable from a Source Result in Azure Data Factory","<p>I've been trying to do this. From a data source pointing to my first database, I would like to have a date-time reference that I then use into another source on a different database as a parameter to limit the number of registers.</p>

<p>I will be something like this:</p>

<p>Source_1:  Implement the next code and save the result in a variable @dateRef.</p>

<pre><code>SELECT IIF(UpdateDtm &gt; CreatedDtm, UpdateDtm, COALESCE(CreatedDtm,UpdateDtm, cast('1900-01-01' as smalldatetime))) as dateRef
FROM 
(
    SELECT MAX(UpdatedDtm) as UpdateDtm, MAX(CreatedDtm) as  CreatedDtm
    FROM schema.table
) max_value
</code></pre>

<p>Source_2: Use @dateRef on my select statement.</p>

<pre><code>select  Client, location
from schema.table
where date &gt;= @dateRef
</code></pre>

<p>I'm brand new with Data Factory, for the last years I've been working with Pentaho where I could do this kind of operations, but here is a little confusing for me. I looked for some examples. However, nothing seems to show the scenario that I want to implement. </p>

<p>Regards,</p>
","<variables><set><azure-data-factory>","2020-04-06 23:47:56","1009","0","1","61086428","<p>To achieve your needs i think you could consider using Azure Function Activity to execute above sql:</p>

<pre><code>SELECT IIF(UpdateDtm &gt; CreatedDtm, UpdateDtm, COALESCE(CreatedDtm,UpdateDtm, cast('1900-01-01' as smalldatetime))) as dateRef FROM  (
    SELECT MAX(UpdatedDtm) as UpdateDtm, MAX(CreatedDtm) as  CreatedDtm
    FROM schema.table ) max_value
</code></pre>

<p>Then you output the result from Azure Function so that you could use it in the next step.No need to set it in the variable. You could refer the value with <code>@activity('Azure Function1').output</code></p>

<p>Next step is using above output in the sql,you can't just write <code>@activity('Azure Function1').output</code> into sql.You have to use <code>@concat()</code> method. For example:</p>

<pre><code>@concat('select  Client, location
from schema.table
where date &gt;=', activity('Azure Function1').output)
</code></pre>
"
"61069970","ADF Shared SHIR permissions","<p>I'm trying to configure an ADF self-hosted integration runtime to be shared with another ADF in the same RG. I'm getting the following error:</p>

<p>(I removed the id specifics)</p>

<p>Error occurred when grant permission to [Object-ID]. Error: {""error"":{""code"":""AuthorizationFailed"",""message"":""The client [My-Username] with object id [object-id] does not have authorization to perform action 'Microsoft.Authorization/roleAssignments/write' over scope '/subscriptions/[object-id]/resourcegroups/DataEngineering-RG/providers/Microsoft.DataFactory/factories/[Data Factory Name]/integrationRuntimes/[IR-Name]/providers/Microsoft.Authorization/roleAssignments/[Role Object ID]' or the scope is invalid. If access was recently granted, please refresh your credentials.""}}</p>

<p>Question- what role is required for me to be able to perform this action and at what level does that access need to be granted (eg. Subscription Level, RG level, ADF Level)? </p>

<p>Note: I have Azure Data Factory Contributor level access currently.</p>

<p>Thanks in advance</p>
","<azure><permissions><azure-data-factory>","2020-04-06 22:29:37","1668","1","1","61075602","<blockquote>
  <p>what role is required for me to be able to perform this action </p>
</blockquote>

<p>You need the <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#owner"" rel=""noreferrer""><code>Owner</code></a> or <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#user-access-administrator"" rel=""noreferrer""><code>User Access Administrator</code></a> role (maybe other roles, just check this <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles"" rel=""noreferrer"">doc</a>, see the json file of each role, if the <code>actions</code> include <code>Microsoft.Authorization/roleAssignments/write</code> , it will be able to do the operation.) 
You can also create a <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/custom-roles"" rel=""noreferrer"">custom role</a> which has <code>Microsoft.Authorization/roleAssignments/write</code> in its <code>actions</code>, it depends on your requirements.</p>

<blockquote>
  <p>and at what level does that access need to be granted (eg. Subscription Level, RG level, ADF Level)?</p>
</blockquote>

<p>The three levels are all correct. 
The RBAC role in Azure is inherited, e.g. if you assign the <code>Owner</code> role to your user account in the subscription, the account will also have the <code>Owner</code> role in all the resource groups/resources of the subscription. But if you just assign the user account in the ADF level, it will not be  able to access other resources in the subscriptions.</p>

<p>So to fix the issue, just navigate to the ADF mentioned in the error message/RG/Subscription in the portal -> <code>Access control (IAM)</code> -> <code>Add</code> -> add your user account as an e.g <code>Owner</code> role, then it will work fine.</p>
"
"61067826","Convert Row Count to INT in Azure Data Factory","<p>I am trying to use a Lookup Activity to return a row count. I am able to do this, but once I do, I would like to run an If Statement against it and if the count returns more than 20MIL in rows, I want to execute an additional pipeline for further table manipulation. The issue, however, is that I can not compare the returned value to a static integer. Below is the current Dynamic Expression I have for this If Statement:</p>

<p>@greater(int(activity('COUNT_RL_WK_GRBY_LOOKUP').output),20000000)</p>

<p>and when fired, the following error is returned:
{
    ""errorCode"": ""InvalidTemplate"",
    ""message"": ""The function 'int' was invoked with a parameter that is not valid. The value cannot be converted to the target type"",
    ""failureType"": ""UserError"",
    ""target"": ""If Condition1"",
    ""details"": """"
}</p>

<p>Is it possible to convert this returned value to an integer in order to make the comparison? If not, is there a possible work around in order to achieve my desired result?</p>
","<azure-data-factory><dynamic-expression>","2020-04-06 19:48:59","2190","1","2","61092019","<p>Looks like the issue is with your dynamic expression. Please correct your dynamic expression similar to below and retry.</p>

<ul>
<li><p>If <code>firstRowOnly</code> is set to true :  <code>@greater(int(activity('COUNT_RL_WK_GRBY_LOOKUP').output.firstRow.propertyname),20000000)</code></p></li>
<li><p>If <code>firstRowOnly</code> is set to false : <code>@greater(int(activity('COUNT_RL_WK_GRBY_LOOKUP').output.value[zero based index].propertyname),20000000)</code></p></li>
<li><p>The lookup result is returned in the <code>output</code> section of the activity run result.</p></li>
<li>When <code>firstRowOnly</code> is set to true (default), the output format is as shown in the following code. The lookup result is under a fixed firstRow key. To use the result in subsequent activity, use the pattern of <code>@{activity('MyLookupActivity').output.firstRow.TableName}</code>.
Sample Output JSON code is as follows:</li>
</ul>

<blockquote>
<pre><code>{
    ""firstRow"":
    {
        ""Id"": ""1"",
        ""TableName"" : ""Table1""
    }
}
</code></pre>
</blockquote>

<ul>
<li>When <code>firstRowOnly</code> is set to false, the output format is as shown in the following code. A count field indicates how many records are returned. Detailed values are displayed under a fixed value array. In such a case, the Lookup activity is followed by a Foreach activity. You pass the value array to the ForEach activity items field by using the pattern of <code>@activity('MyLookupActivity').output.value</code>. To access elements in the value array, use the following syntax: <code>@{activity('lookupActivity').output.value[zero based index].propertyname}</code>. An example is <code>@{activity('lookupActivity').output.value[0].tablename}</code>.
Sample Output JSON Code is as follows: </li>
</ul>

<blockquote>
<pre><code>{
    ""count"": ""2"",
    ""value"": [
        {
            ""Id"": ""1"",
            ""TableName"" : ""Table1""
        },
        {
            ""Id"": ""2"",
            ""TableName"" : ""Table2""
        }
    ]
}
</code></pre>
</blockquote>

<p>Hope this helps.</p>
"
"61067826","Convert Row Count to INT in Azure Data Factory","<p>I am trying to use a Lookup Activity to return a row count. I am able to do this, but once I do, I would like to run an If Statement against it and if the count returns more than 20MIL in rows, I want to execute an additional pipeline for further table manipulation. The issue, however, is that I can not compare the returned value to a static integer. Below is the current Dynamic Expression I have for this If Statement:</p>

<p>@greater(int(activity('COUNT_RL_WK_GRBY_LOOKUP').output),20000000)</p>

<p>and when fired, the following error is returned:
{
    ""errorCode"": ""InvalidTemplate"",
    ""message"": ""The function 'int' was invoked with a parameter that is not valid. The value cannot be converted to the target type"",
    ""failureType"": ""UserError"",
    ""target"": ""If Condition1"",
    ""details"": """"
}</p>

<p>Is it possible to convert this returned value to an integer in order to make the comparison? If not, is there a possible work around in order to achieve my desired result?</p>
","<azure-data-factory><dynamic-expression>","2020-04-06 19:48:59","2190","1","2","71622501","<p>Do this - when you run the debugger look at the output from your lookup.  It will give a json string including the alias for the result of your query. If it's not firstrow set then you get a table.  But for first you'll get output then firstRow and then your alias.  So that's what you specify.</p>
<p>For example...if you put alias of your count as Row_Cnt then...
@greater(activity('COUNT_RL_WK_GRBY_LOOKUP').output.firstRow.Row_Cnt,20000000)</p>
<p>You don't need the int function.  You were trying to do that (just like I was!) because it was complaining about datatype.  That's because you were returning a bunch of json text as the output instead of the value you were after.  Totally makes sense after you realize how it works.  But it is NOT intuitively obvious because it's coming back with data but its string stuff from json, not the value you're after.  And functions like equals are just happy with that.  It's not until you try to do something like greater where it looks for numeric value that it chokes.</p>
"
"61066742","Scape character in Azure password","<p>I was triying to configure a Linked Service in Azure Data Factory (ADF) using ADF GUI and when I type the password of the DB  I want to connect to I get following error message: </p>

<pre><code>Connection failed
Expecting connection string of format ""key1=value1; key2=value2""
</code></pre>

<p>If I configure it to take this password from a Keyvault I get following error message: </p>

<pre><code>The value of the property '' is invalid: 'Format of the initialization string does not conform to specification starting at index 105
</code></pre>

<p>I have tried to connect to this DB using DBeaver and I can conect without a problem so the URL, password and User are correct and I think that the problem is in the password string because it contains a semi colon (;), my password is like this:</p>

<pre><code>pass(GdA;+pass
</code></pre>

<p>So I think that the problem is that I have to scape semi colon character in order to configure that Linked Service (If I remove this semi colon I get a login error). I have tried what other posts said: <a href=""https://stackoverflow.com/questions/36454733/apostrophe-single-quote-in-connection-string"">1</a> and <a href=""https://stackoverflow.com/questions/53723706/secret-name-is-not-supporting-special-character"">2</a> but I still can not configure this Linked Service.</p>

<p>Maybe I have to try to create this Linked Service using the Azure CLI but do you have any other proposal?</p>

<p>Thanks in advance.</p>

<p>EDIT: Addition of anonymez linked service JSON:</p>

<pre><code>{
""name"": ""LS_P_Oracle_BAS"",
""type"": ""Microsoft.DataFactory/factories/linkedservices"",
""properties"": {
    ""description"": ""Test"",
    ""annotations"": [],
    ""type"": ""Oracle"",
    ""typeProperties"": {
        ""connectionString"": ""host=zzz.net;port=1521;servicename=yyy.com;user id=USER"",
        ""encryptedCredential"": ""eyJDcmVkZW50aWFsSWQiOiJmZTI3NDU4MS1iNWYyLTQ0YmItYTcwYS0wNzQxZTFkOWY5NTkiLCJWZXJzaW9uIjoiMi4wIiwiQ2xhc3NUeXBlIjoiTWljcm9zb2Z0LkRhdGFQcm94eS5Db3JlLkludGVyU2VydmljZURhdGFDb250cmFjdC5DcmVkZW50aWFsU1UwNkNZMTQifQ==""
    },
    ""connectVia"": {
        ""referenceName"": ""INTEGRATIONRUNTIME"",
        ""type"": ""IntegrationRuntimeReference""
    }
}
}
</code></pre>

<p>EDIT2: Connection configuration using Keyvault:</p>

<pre><code>{
""name"": ""LS_P_Oracle_BAS"",
""type"": ""Microsoft.DataFactory/factories/linkedservices"",
""properties"": {
    ""description"": ""Test"",
    ""annotations"": [],
    ""type"": ""Oracle"",
    ""typeProperties"": {
        ""connectionString"": ""host=yyy.net;port=1521;ServiceName=zzz.com;user id=USER_READER"",
        ""password"": {
            ""type"": ""AzureKeyVaultSecret"",
            ""store"": {
                ""referenceName"": ""LS_P_KeyVault_yyy"",
                ""type"": ""LinkedServiceReference""
            },
            ""secretName"": ""BAS-reader-password""
        }
    },
    ""connectVia"": {
        ""referenceName"": ""INTEGRATIONRUNTIME"",
        ""type"": ""IntegrationRuntimeReference""
    }
}
}
</code></pre>

<p>The error that I get with this second linked service JSON:</p>

<pre><code>The value of the property '' is invalid: 'Format of the initialization string does not conform to specification starting at index 105.'. Format of the initialization string does not conform to specification starting at index 105.
</code></pre>
","<azure><azure-data-factory>","2020-04-06 18:41:51","738","0","1","61259187","<p>After further investigation on this issue, it is identified as a bug in ADF Oracle connector, which cannot handle properties that contain semi colon. Current ETA for the bug fix deployment is end of June (Note: This is tentative date)</p>

<p>Unfortunately there is no workaround at the moment other than changing the password (avoiding semi colon in connection properties) until the fix is deployed. </p>

<p>I will keep below MSDN thread updated as soon as I have further updates regarding this fix.</p>

<p>MSDN thread: <a href=""https://social.msdn.microsoft.com/Forums/en-US/10a3454f-69b8-41bb-9af3-8846f44f6dfe/scape-character-in-azure-password?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/10a3454f-69b8-41bb-9af3-8846f44f6dfe/scape-character-in-azure-password?forum=AzureDataFactory</a></p>

<p>We appreciate you patience and sincere apologizes for all the inconvenience caused. </p>
"
"61064737","Cannot create a dataset with multiple tables joined in Azure Data Factory","<p>I am moving data from On-premise SQL DB to Azure cloud.
I have a problem creating a dataset in Azure data factory(V2) with multiple tables joined. I can select only one table while creating a dataset. </p>
","<sql-server><azure><ssis><azure-sql-database><azure-data-factory>","2020-04-06 16:43:17","244","0","1","61071656","<p>You could run the table joined query to get the data as Source dataset in Data Factory.</p>

<p>For example, here's the query joined two tables emp and tb1 :</p>

<p><a href=""https://i.stack.imgur.com/BR65f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BR65f.png"" alt=""enter image description here""></a></p>

<p>Create On-premise SQL Server Source dataset:</p>

<p><a href=""https://i.stack.imgur.com/45add.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/45add.png"" alt=""enter image description here""></a></p>

<p>Using query to get the dataset which joined multiple table:</p>

<pre><code>select eno,ename,age from emp left join tb1 on emp.eno=tb1.id
</code></pre>

<p><a href=""https://i.stack.imgur.com/1g6KF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1g6KF.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"61064622","Rerun activity in Azure Data Factory V2 using .net sdk","<p>I am trying to rerun activity in Azure Data Factory as in this article - <a href=""https://azure.microsoft.com/en-us/blog/rerun-activities-inside-your-data-factory-pipelines/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/rerun-activities-inside-your-data-factory-pipelines/</a>
There is manually example, but I want provide this process automatically using .net sdk. Could you provide examples, links or any information how to rerun activities using .net sdk</p>

<p>Thank you for any help or suggestions!</p>
","<c#><.net><azure><sdk><azure-data-factory>","2020-04-06 16:37:06","492","3","1","61098338","<p>I know your concern, the activity could be rerun based on the statement in the video clearly. So,you definitely know how to do the operations in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually"" rel=""nofollow noreferrer"">ADF UI</a>.</p>

<p>If you refer to the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory?view=azure-dotnet"" rel=""nofollow noreferrer"">source code reference of ADF</a>,you can't find any methods related to Activity Run.</p>

<p><a href=""https://i.stack.imgur.com/M8g5s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M8g5s.png"" alt=""enter image description here""></a></p>

<p>In fact, it is set up by the <code>startActivityName</code> property in the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.ipipelinesoperations.createrunwithhttpmessagesasync?view=azure-dotnet"" rel=""nofollow noreferrer"">IPipelinesOperations.CreateRunWithHttpMessagesAsync</a> method.</p>

<p><a href=""https://i.stack.imgur.com/PGcBm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGcBm.png"" alt=""enter image description here""></a></p>
"
"61057965","Convert csv files,text files,pdf files into json using Azure Data Factory","<p>I have a scenario in which I'm pulling files from different sources, The files are in text,csv,json and pdf format. I'm pulling these files by using copy data activity of ADF. Now, the files should be loaded into cosmos db into the same table. I'm able to load the JSON files in cosmos db but other files format cannot feed into cosmos db. so I'm trying to convert other files into JSON but no help. I have gone through custom activity solution but due to some restriction I'm not able to complete the solution.</p>

<p>Please let me know is there any way to transform text,csv,pdf files into JSON and can be feed into cosmos db.</p>
","<azure-functions><azure-cosmosdb><azure-data-factory>","2020-04-06 10:43:07","4727","2","1","61095697","<p>Data Factory can convert the .csv file to .json during file copy active.</p>

<p>For example:</p>

<p>Source dataset:</p>

<p><a href=""https://i.stack.imgur.com/6EWYP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6EWYP.png"" alt=""enter image description here""></a> </p>

<p>Sink dataset:</p>

<p><a href=""https://i.stack.imgur.com/apai5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/apai5.png"" alt=""enter image description here""></a></p>

<p>Sink:</p>

<p><a href=""https://i.stack.imgur.com/xNopO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xNopO.png"" alt=""enter image description here""></a></p>

<p>Mapping:
<a href=""https://i.stack.imgur.com/iss3B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iss3B.png"" alt=""enter image description here""></a></p>

<p>Pipeline Running:
<a href=""https://i.stack.imgur.com/osSSD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/osSSD.png"" alt=""enter image description here""></a></p>

<p>Check the new json file in the container:</p>

<p><a href=""https://i.stack.imgur.com/K4tuC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K4tuC.png"" alt=""enter image description here""></a></p>

<p>This example just want to tell you that Data Factory can help convert some format data to .json file. But not all the files can be converted. You must test by yourself and get the answer.</p>

<p>Hope this helps.</p>
"
"61056582","Azure Datafactory Ingestion JSON Format","<p>I'm doing some research into cloudbased ETL/ELT systems that can work with deeply nested JSON / XML Documents.</p>

<p>I found Azure Datafactory which seems to meet my requirements. However the documentation left me unsure about whether Datafactory can work with those deeply nested Documents. In the examples I found here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-json"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-json</a> all objects are ""flat"", with zero nesting. </p>

<p>My question is: Does Datafactory Support nested Documents?</p>

<p>Thanks a lot</p>
","<azure><etl><azure-data-factory>","2020-04-06 09:22:14","68","0","1","61062086","<p>Yes, it's possible to ingest the data from nested JSON  file by using azure databricks. Reference link: 
<a href=""https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html</a></p>
"
"61044479","Azure data factory copy activity error records exception handling","<p>In copy-activity, there is a feature in the settings where we can tell the system what to do on an error. There are 3 options available. Abort on first error-record, skip all error-rows, skip error-rows and log them.
Can we achieve the following functionality somehow?
- If there are more than 10% of records in the file throws error then skip that file, else load</p>

<p>Thanks in advance</p>
","<exception><error-handling><record><azure-data-factory>","2020-04-05 14:51:50","735","0","1","61044544","<p>I have a workaround, please let me know if you have a better one
- Load the data to the target using fault-tolerance setting as ""skip and log incompatible rows""
- We will get the total number of rows reached the copy-activity and number of rows that went into the target. From those 2 numbers we can calculate the %.
- if that % is more than 10 (or the threshold), remove data from the target and notify the relevant team or put that into the audit log</p>

<p>FYI - this feature is available in snowflake-DW, if we are directly using scripts to load data.</p>
"
"61033267","Ignore bad rows when copying data from Blob to Azure SQL via C# .NET Framework managed Data Factory","<p>I am following <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-dot-net"" rel=""nofollow noreferrer"">this tutorial</a> to load data in Azure SQL from a flat file in a Blob.</p>

<p>This method requires a C# .NET Framework console app to create / manage the Blob dataset, Azure SQL sink, and Data Factory.</p>

<p>The only problem is that with big, messy data files I inevitably encounter some rows which contain an extra delimiter or which are somehow otherwise malformed. The data is too big to clean locally before the upload to the blob.</p>

<p>A normal solution is to ignore the bad rows, i.e. to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance"" rel=""nofollow noreferrer"">allow fault tolerance</a>.</p>

<p>There's an example of how to do this with JSON:</p>

<pre><code>""typeProperties"": {
    ""source"": {
        ""type"": ""BlobSource""
    },
    ""sink"": {
        ""type"": ""SqlSink"",
    },
    ""enableSkipIncompatibleRow"": true,
    ""redirectIncompatibleRowSettings"": {
         ""linkedServiceName"": {
              ""referenceName"": ""&lt;Azure Storage or Data Lake Store linked service&gt;"",
              ""type"": ""LinkedServiceReference""
            },
            ""path"": ""redirectcontainer/erroroutput""
     }
}
</code></pre>

<p>However, I haven't been able to determine if there is some equivalent for this in C#. I tried modifying my original connection string for the Azure SQL sink from this:</p>

<pre><code>    // Specify the sink Azure SQL Database information
    string azureSqlConnString =
        ""Server=tcp:mydb.database.windows.net,1433;"" +
        ""Database=mydb;"" +
        ""User ID=myuser;"" + 
        ""Password=mypassword;"" + 
        ""Trusted_Connection=False;Encrypt=True;Connection Timeout=30"";
    string azureSqlTableName = ""dbo.mytable"";

    string storageLinkedServiceName = ""AzureStorageLinkedService"";
    string sqlDbLinkedServiceName = ""AzureSqlDbLinkedService"";
    string blobDatasetName = ""BlobDataset"";
    string sqlDatasetName = ""SqlDataset"";
    string pipelineName = ""Adfv2TutorialBlobToSqlCopy"";
</code></pre>

<p>to this:</p>

<pre><code>    // Specify the sink Azure SQL Database information
    string azureSqlConnString =
        ""Server=tcp:mydb.database.windows.net,1433;"" +
        ""Database=mydb;"" +
        ""User ID=myuser;"" + 
        ""Password=mypassword;"" + 
        ""enableSkipIncompatibleRow= true;"" + 
        ""Trusted_Connection=False;Encrypt=True;Connection Timeout=30"";
    string azureSqlTableName = ""dbo.mytable"";

    string storageLinkedServiceName = ""AzureStorageLinkedService"";
    string sqlDbLinkedServiceName = ""AzureSqlDbLinkedService"";
    string blobDatasetName = ""BlobDataset"";
    string sqlDatasetName = ""SqlDataset"";
    string pipelineName = ""Adfv2TutorialBlobToSqlCopy"";
</code></pre>

<p>as a guess, but it didn't work:</p>

<blockquote>
  <p>""message"":
  ""ErrorCode=UserErrorInvalidDbConnectionString,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Invalid
  database connection string provided. Check the connection string at
  'Sink'
  side.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.ArgumentException,Message=Keyword
  not supported: 'enableskipincompatiblerow'.,Source=System.Data,'"",</p>
</blockquote>

<p>Is there a way to do this?</p>

<p>The <code>SkipErrorFile</code> <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.copyactivity?view=azure-dotnet"" rel=""nofollow noreferrer"">property</a> of <code>CopyActivity</code> class says it gets or sets fault tolerance. I got the <code>CopyActivity</code> implementation to accept the property with <code>SkipErrorFile = new SkipErrorFile { }</code> but it did not seem to change the behavior in the desired way.</p>
","<c#><azure><azure-sql-database><azure-data-factory>","2020-04-04 18:58:54","275","1","1","61036297","<p>I have a workaround but not a true solution, so I'm going to self-answer but not select my answer as the solution.</p>

<p>The workaround is to create a fake delimiter (one that does not exist in the data, like <code>\t\t</code>) and a Azure SQL sink table with only 1 column.</p>

<p>From there I can load the data into a single column and, after it is in Azure SQL, parse it with either SQL or any language in Databricks (Python, R, Scala or SQL) so that I can use the HPC cloud environment to clean and parse the data at scale.</p>
"
"61022816","Add additional column with value of an existing column in Copy Data stage Azure Data Factory (not using SQL)","<p>I want to add an additional column to the dataset of the Copy Data stage in Azure Data Factory.</p>

<p><a href=""https://i.stack.imgur.com/Uf7zM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uf7zM.png"" alt=""enter image description here""></a></p>

<p>I am not using a SQL Query as source but a Dynamics 365 fecthxml, which limits the additional column being added in the query.</p>
","<dynamics-crm><azure-data-factory><fetchxml>","2020-04-04 01:45:20","1032","0","1","61095894","<p>I'm pretty certain you can't do this within the Copy data activity. According to the documentation, you can add dynamic and static data, but you can't get to the data being copied.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>
"
"61007231","azure data factory get lookup activity error output","<p>Im my pipeline, there is only one lookup activity and a stored procedure activity following it when the lookup fails.</p>

<p>The lookup sends a query like </p>

<pre><code>    select 1/count(*) as result from sometable
</code></pre>

<p>The stored procedure activity calls a stored precedure with a parameter named 'error'.</p>

<p>Basically I want this lookup to fail when count(*) is 0, and then I want to capture the divide by 0 error message from the activity output and use that as the input parameter for the stored procedure. </p>

<p>The output from the lookup when count(*) is 0 is:</p>

<pre><code>    {
        ""errorCode"": ""2100"",
        ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedFileOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Upload file failed at path 37b1ec96-be95-4010-8547-10387fc407a3\\result.,Source=Microsoft.DataTransfer.Common,''Type=System.Data.SqlClient.SqlException,Message=Divide by zero error encountered.,Source=.Net SqlClient Data Provider,SqlErrorNumber=8134,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=8134,State=1,Message=Divide by zero error encountered.,},],'"",
        ""failureType"": ""UserError"",
        ""target"": ""Lookup source count"",
        ""details"": []
    }
</code></pre>

<p>So in the stored procedure activity, I want to put the message from above to the parameter of the stored procedure. I have tried @activity('Lookup source count').output.message for the input parameter. But get this error:</p>

<pre><code>    {
        ""errorCode"": ""InvalidTemplate"",
        ""message"": ""The expression 'activity('Lookup source count').output.message' cannot be evaluated because property 'message' doesn't exist, available properties are ''."",
        ""failureType"": ""UserError"",
        ""target"": ""log fail Lookup source count"",
        ""details"": """"
    }
</code></pre>

<p>I have also tried many other options but none of them works. How can I get the error message from the lookup when it fails?</p>
","<azure-data-factory>","2020-04-03 07:46:22","6679","4","1","61008349","<p>Using, ""Add dynamic content"", use this as your SP parameter value</p>

<p><code>@activity('&lt;name of your lookup&gt;').Error.Message</code></p>
"
"61006363","What is the compatible .NET Framework for ADF Custom .NET Activity?","<p>Actually I am developing a .NET tool for ADF custom .NET activity. The tool is running fine if the .NET framework is 4.5 or 4.5.2. But it is not running if I switch the framework version to 4.6. It is not even writing log in adfjobs folder. But it creates an empty stdout.txt file. Which means it's not supporting the v4.6? But actually I need the tool in v4.6 to support my further development. Should I have to do something with the batch account? 
Storage Type: Azure Lake Storage Gen 2.</p>
","<azure-blob-storage><azure-data-factory><.net-framework-version><azure-batch>","2020-04-03 06:41:40","67","0","1","61014084","<p>In ADF v2, custom activity runs in an Azure Batch pool. That pool has a server defined (ex: Windows Server 2016, 2019, etc). That server/vm level will determine what version of .NET is installed. You may need to create a new pool with a more up to date server version.</p>
"
"61005985","How to parameterize part of filename and output that to a column in azure data factory?","<p>I have read the docs about parameterizing, my table needs only the month name from the file name to be extracted and output that to a column? how can i do this? without creating external file format?( i tried using parameters but i am not clear on this)</p>

<p>Can you please explain in detail?</p>

<p>Thank you!</p>
","<azure><parameters><azure-data-factory>","2020-04-03 06:08:42","78","0","1","61051970","<p>According my experience,  we can't parameterize part of filename and output that to a column in azure data factory.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get metadata</a> is the only activity which Data Factory provides for us to get the filename.</p>

<p><a href=""https://i.stack.imgur.com/tXvOG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tXvOG.png"" alt=""enter image description here""></a></p>

<p>We can pass the filename as parameter to specify the new file or create a new filename with expression.</p>

<p>There is no ways can transfer the filename as a column. </p>

<p>Hope this helps.</p>
"
"61004205","How do I load entire file content as a text into a column AzureSQLDW table?","<p>I have a some file in an azure data lake 2 and I want to load them as a column value nvarchar(max) in AzureSQLDW. The table in AzureSQLDW is heap. I couldn't find any way to do it? All I see is column delimited when load them into multiple rows instead of one row in single column. How I achieve this?</p>
","<azure-data-factory><azure-synapse><azure-data-lake-gen2>","2020-04-03 02:30:11","391","0","3","61004410","<p>I don't guarantee this will work, but try using COPY INTO and define non-present values for row and column delimiters. Make your target a single column table.</p>
"
"61004205","How do I load entire file content as a text into a column AzureSQLDW table?","<p>I have a some file in an azure data lake 2 and I want to load them as a column value nvarchar(max) in AzureSQLDW. The table in AzureSQLDW is heap. I couldn't find any way to do it? All I see is column delimited when load them into multiple rows instead of one row in single column. How I achieve this?</p>
","<azure-data-factory><azure-synapse><azure-data-lake-gen2>","2020-04-03 02:30:11","391","0","3","61014283","<p>I would create a Source Dataset with a single column. You do this by specifying ""No delimiter"":</p>

<p><a href=""https://i.stack.imgur.com/PX3RV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PX3RV.png"" alt=""enter image description here""></a></p>

<p>Next, go to the ""Schema"" tab and Import the schema, which should create a single column called ""Prop_0"":
<a href=""https://i.stack.imgur.com/FIVWa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FIVWa.png"" alt=""enter image description here""></a></p>

<p>Now the data should come through as a single string instead of delimited columns.</p>
"
"61004205","How do I load entire file content as a text into a column AzureSQLDW table?","<p>I have a some file in an azure data lake 2 and I want to load them as a column value nvarchar(max) in AzureSQLDW. The table in AzureSQLDW is heap. I couldn't find any way to do it? All I see is column delimited when load them into multiple rows instead of one row in single column. How I achieve this?</p>
","<azure-data-factory><azure-synapse><azure-data-lake-gen2>","2020-04-03 02:30:11","391","0","3","75657984","<p>The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delimited-text"" rel=""nofollow noreferrer"">Microsoft Documentation</a> confirms that the &quot;No delimiter&quot; (aka &quot;empty string&quot;) option for row and for column delimiters is &quot;only supported for mapping data flow but not Copy activity.&quot;
<a href=""https://i.stack.imgur.com/mFbkC.png"" rel=""nofollow noreferrer"">Microsoft wording</a></p>
<p>The only workaround I can find for this when using Copy Activity is same as you, setting the delimiter to a non-printable character not present in file, which will work same as no delimiter unless that character at some point appears in a file.</p>
<p>Note that setting empty string to parameter for dataset also doesn't work in Copy Activity
<a href=""https://i.stack.imgur.com/tt8A3.png"" rel=""nofollow noreferrer"">parameratise also doesn't work</a></p>
<p>Vote for the feature in the Azure ideas forum if interested <a href=""https://feedback.azure.com/d365community/idea/1c4084cb-9dbc-ed11-a81b-6045bd79fc6e"" rel=""nofollow noreferrer"">here</a></p>
"
"61003828","Azure Data Factory Copy Task OnPremises DB to Azure Database","<p>Am trying to copy data tables from On-Premises MSSQL to Azure SQL Tables.</p>

<p>My setting for Table Option on Sink Tab of Copy Activity is to set to 'Auto Create Table' (This will automatically create sink table if doesn't exists and this doesn't support using blob storage as staging.)</p>

<p>When executed with above setting it never finishes and it just shows status in progress.</p>

<p>But when source is other than MSSQL it works fine.  Tested with CSV files and Oracle db.</p>

<p><a href=""https://i.stack.imgur.com/roH3n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/roH3n.png"" alt=""enter image description here""></a></p>

<p>If I set Table Option to None, i can enable staging, but now it expects to have target table defined before it loads.</p>

<p><a href=""https://i.stack.imgur.com/X32tr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X32tr.png"" alt=""enter image description here""></a></p>

<p>Any clues why it's not working for MSSQL ??</p>
","<azure-sql-database><azure-data-factory>","2020-04-03 01:37:22","207","1","1","61054907","<p>I tested the same operation with you and don't get the error:</p>

<p><a href=""https://i.stack.imgur.com/SNsmh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SNsmh.png"" alt=""enter image description here""></a></p>

<p>Sink dataset</p>

<p><a href=""https://i.stack.imgur.com/zYbX6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zYbX6.png"" alt=""enter image description here""></a></p>

<p>Operation: Auto create table:</p>

<p><a href=""https://i.stack.imgur.com/Cm0nW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cm0nW.png"" alt=""enter image description here""></a></p>

<p>I think the most possible reason is caused by the Microsoft Integration Runtime Configuration Manager. I don't know if the error is solved now,  you could restart it and Data factory and try again.</p>

<p>Hope this helps.</p>
"
"60996541","Azure Data Factory : Source data set has a stored procedure with user-defined Table Type","<p>I have a stored procedure, which accepts the user-defined Table type as a parameter. I want to use this SP in the source data set in Azure data factory's copy data activity. I have seen examples where we can use user-defined Table type in the sink data set but not with the source data set.</p>

<p>Is it possible to execute an SP with user-defined Table type as parameter, to get the data out of the SQL and copy it over blob storage?</p>

<p>Here is my SP:</p>

<pre><code>CREATE PROCEDURE [export].[up_get_qnxt_enrollment_date]
      @effectiveDate DATETIME
     , @lobPlanDimIDs export.intList READONLY
AS
BEGIN
 ......
END
</code></pre>

<p>The export.intList is the user defined Table type:</p>

<pre><code>CREATE TYPE [export].[intList] AS TABLE(
    [Id] [int] NULL,
    [Name] [varchar(256)] NULL
)
</code></pre>

<p>I am not able to set this parameter in the Azure DF's source data set. I tried setting this a JSON array, but with no luck:</p>

<pre><code>""typeProperties"": {
                    ""source"": {
                        ""type"": ""AzureSqlSource"",
                        ""sqlReaderStoredProcedureName"": ""[export].[up_get_qnxt_enrollment_date]"",
                        ""storedProcedureParameters"": {
                            ""effectiveDate"": {
                                ""type"": ""DateTime"",
                                ""value"": ""2004-01-01""
                            },
                            ""lobPlanDimIDs"": {
                                ""type"": ""export.intList"",
                                ""value"": [
                                    {
                                        ""Id"": {   ""type"": ""int"",  ""value"": 1   },
                                        ""Name"": {  ""type"":  ""String"", ""value"": ""ABC"" }
                                    },
                                    {
                                        ""Id"": {   ""type"": ""int"",  ""value"": 2   },
                                        ""Name"": {  ""type"":  ""String"", ""value"": ""DEF"" }
                                    }
                                ]
                            }
                        },
                        ""queryTimeout"": ""02:00:00""
</code></pre>

<p>Is there anything I am missing or this functionality is not available in Azure DF yet?</p>

<p><strong>Update:</strong> 
As per Martin Esteban Zurita's suggestion, I have used a query instead of SP. This is a perfectly fine workaround. I have also created a feature request:
<a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/40130446-support-user-defined-table-as-input-parameter-to-s"" rel=""nofollow noreferrer"">Support for User-defined table in Azure DF for Source</a></p>

<p>Please upvote if you need this functionality.</p>
","<azure><stored-procedures><azure-pipelines><azure-data-factory><user-defined-types>","2020-04-02 16:35:00","361","1","1","61013253","<p>For a more configuration based approach, you can check this blog entry from pragmatic works: <a href=""https://blog.pragmaticworks.com/using-stored-procedure-in-azure-data-factory"" rel=""nofollow noreferrer"">https://blog.pragmaticworks.com/using-stored-procedure-in-azure-data-factory</a></p>

<p>When I faced this issue, the way I've solved it was by using query instead of using stored procedure
<a href=""https://i.stack.imgur.com/OHo26.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OHo26.png"" alt=""enter image description here""></a></p>

<p>And then call the query like you would call it from the sql command line, for example:</p>

<pre><code>DECLARE @tmp DATETIME
SET @tmp = GETDATE()
DECLARE @lob AS intList
insert into @lob (Id, Name) select 1, 'ABC'
insert into @lob (Id, Name) select 2, 'DEF'
EXEC [up_get_qnxt_enrollment_date] @tmp, @lob
</code></pre>

<p>Hope this helped!</p>
"
"60992769","Get queries from github and execute them in order","<p>I have in my github repo several folders with queries.</p>

<p>folder Common
folder </p>

<p> is also a variable in my project.</p>

<p>I would like to use ADFv2 IS to 
1. get all the queries  in the folder common and execute these in alphanumeric order
2. get all the queries  in the folder  and execute these in alphanumeric order</p>

<p>Any idea how this can be done? Can ADFv2 IS get data from github and f.e. store the contents in a table?</p>
","<sql-server><github><ssis><azure-data-factory>","2020-04-02 13:31:22","27","1","1","60993385","<p>Use a script task</p>

<pre><code>string URL = [url of the file you want];
System.Net.WebClient wc = new System.Net.WebClient();
wc.DownloadFile(URL,@""c:\SQL.sql"");
</code></pre>
"
"60991642","Dynamic outputfilename in Data Flows in Azure Data Factory results in folders instead of files","<p>I am setting up a Data Flow in ADF that takes an Azure Table Dataset as Source, adds a Derived Column that adds a column with the name ""filename"" and a dynamic value, based on a data field from the source schema.</p>

<p>Then the output is sent to a sink that is linked to a DataSet that is attached to Blob Storage (tried ADLS Gen2 and standard Blob storage).</p>

<p>However, after executing the pipeline, instead of finding multiple files in my container, I see there are folders created with the name <code>filename=ABC123.csv</code> that on its own contains other files (it makes me think of parquet files):</p>

<pre><code>- filename=ABC123.csv
  + _started_UNIQUEID
  + part-00000-tid-UNIQUEID-guids.c000.csv
</code></pre>

<p>So, I'm clearly missing something, as I would need to have single files listed in the dataset container with the name I have specified in the pipeline.</p>

<p>This is how the pipeline looks like:
<a href=""https://i.stack.imgur.com/cY4RQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cY4RQ.png"" alt=""Screenshot""></a></p>

<p>The Optimize tab of the Sink shape looks like this:
<a href=""https://i.stack.imgur.com/YSZ9O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YSZ9O.png"" alt=""enter image description here""></a></p>

<p>Here you can see the settings of the Sink shape:
<a href=""https://i.stack.imgur.com/EKOKM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EKOKM.png"" alt=""enter image description here""></a></p>

<p>And this is the code of the pipeline (however some parts are edited out):</p>

<pre class=""lang-yaml prettyprint-override""><code>source(output(
        PartitionKey as string,
        RowKey as string,
        Timestamp as string,
        DeviceId as string,
        SensorValue as double
    ),
    allowSchemaDrift: true,
    validateSchema: false,
    inferDriftedColumnTypes: true) ~&gt; devicetable
devicetable derive(filename = Isin + '.csv') ~&gt; setoutputfilename
setoutputfilename sink(allowSchemaDrift: true,
    validateSchema: false,
    rowUrlColumn:'filename',
    mapColumn(
        RowKey,
        Timestamp,
        DeviceId,
        SensorValue
    ),
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~&gt; distributetofiles
</code></pre>

<p>Any suggestions or tips?  (I'm rather new to ADF, so bear with me)</p>
","<azure-data-factory>","2020-04-02 12:30:30","4777","1","2","61000875","<p>I recently struggled through something similar to your scenario (but not exactly the same). There are a lot of options and moving parts here, so this post is not meant to be exhaustive. Hopefully something in it will steer you towards the solution you are after.</p>

<p><strong>Step 1: Source Partitioning</strong>
In Data Flow, you can group like rows together via Set Partitioning. One of the many options is by Key (a column in the source):
<a href=""https://i.stack.imgur.com/nvucS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nvucS.png"" alt=""enter image description here""></a></p>

<p>In this example, we have 51 US States (50 states + DC), and so will end up with 51 partitions.</p>

<p><strong>Step 2: Sink Settings</strong>
As you found out, the ""As data in column"" option results in a structured folder name like {columnName}={columnValue}. I've been told this is because it is a standard in Hadoop/Spark type environments. Inside that folder will be a set of files, typically with non-human-friendly GUID based names.</p>

<p>""Default"" will give much the same result you currently have, without the column based folder name. Output to Single File"" is pretty self-explanatory, and the farthest thing from the solution you are after. If you want control over the final file names, the best option I have found is the ""Pattern"" option. This will generate file(s) with the specified name and a variable number [n]. I honestly don't know what Per partition would generate, but it <em>may</em> get close to you the results you are after, 1 file per column value.</p>

<p><a href=""https://i.stack.imgur.com/OrFA7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OrFA7.png"" alt=""enter image description here""></a></p>

<p>Some caveats:</p>

<ul>
<li>The folder name is defined in the Sink Dataset, NOT in the Data Flow. Dataset parameters is really probably ""Step 0"". For Blob type output, you could probably hard code the folder name like ""myfolder/fileName-[n]"". YMMV.</li>
<li>Unfortunately, none of these options will permit you to use a derived column to generate the file name. [If you open the expression
editor, you'll find that ""Incoming schema"" is not populated.]</li>
</ul>

<p><strong>Step 3: Sink Optimize</strong>
The last piece you may experiment with is Sink Partitioning under the Optimize tab:
<a href=""https://i.stack.imgur.com/bPdJl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bPdJl.png"" alt=""enter image description here""></a></p>

<p>""Use current partitioning"" will group the results based on the partition set in the Source configuration. ""Single partition"" will group all the results into a single output group (almost certainly NOT what you want). ""Set partitioning"" will allow you to re-group the Sink data based on a Key column. Unlike the Sink settings, this WILL permit you to access the derived column name, but my guess is that you will end up with the same folder naming problem you have now.</p>

<p>At the moment, this is all I know. I believe that there is a combination of these options that will produce what you want, or something close to it. You may need to approach this in multiple steps, such as have this flow output to incorrectly named folders to a staging location, then have another pipeline/flow that processes each folder and collapses the results the desired name.</p>
"
"60991642","Dynamic outputfilename in Data Flows in Azure Data Factory results in folders instead of files","<p>I am setting up a Data Flow in ADF that takes an Azure Table Dataset as Source, adds a Derived Column that adds a column with the name ""filename"" and a dynamic value, based on a data field from the source schema.</p>

<p>Then the output is sent to a sink that is linked to a DataSet that is attached to Blob Storage (tried ADLS Gen2 and standard Blob storage).</p>

<p>However, after executing the pipeline, instead of finding multiple files in my container, I see there are folders created with the name <code>filename=ABC123.csv</code> that on its own contains other files (it makes me think of parquet files):</p>

<pre><code>- filename=ABC123.csv
  + _started_UNIQUEID
  + part-00000-tid-UNIQUEID-guids.c000.csv
</code></pre>

<p>So, I'm clearly missing something, as I would need to have single files listed in the dataset container with the name I have specified in the pipeline.</p>

<p>This is how the pipeline looks like:
<a href=""https://i.stack.imgur.com/cY4RQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cY4RQ.png"" alt=""Screenshot""></a></p>

<p>The Optimize tab of the Sink shape looks like this:
<a href=""https://i.stack.imgur.com/YSZ9O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YSZ9O.png"" alt=""enter image description here""></a></p>

<p>Here you can see the settings of the Sink shape:
<a href=""https://i.stack.imgur.com/EKOKM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EKOKM.png"" alt=""enter image description here""></a></p>

<p>And this is the code of the pipeline (however some parts are edited out):</p>

<pre class=""lang-yaml prettyprint-override""><code>source(output(
        PartitionKey as string,
        RowKey as string,
        Timestamp as string,
        DeviceId as string,
        SensorValue as double
    ),
    allowSchemaDrift: true,
    validateSchema: false,
    inferDriftedColumnTypes: true) ~&gt; devicetable
devicetable derive(filename = Isin + '.csv') ~&gt; setoutputfilename
setoutputfilename sink(allowSchemaDrift: true,
    validateSchema: false,
    rowUrlColumn:'filename',
    mapColumn(
        RowKey,
        Timestamp,
        DeviceId,
        SensorValue
    ),
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~&gt; distributetofiles
</code></pre>

<p>Any suggestions or tips?  (I'm rather new to ADF, so bear with me)</p>
","<azure-data-factory>","2020-04-02 12:30:30","4777","1","2","61006018","<p>You're seeing the ghost files left behind by the Spark process in your dataset folder path. When you use 'As data in column', ADF will write the file using your field value starting at the container root.</p>

<p>You'll see this noted on the 'Column with file name' property:</p>

<p><a href=""https://i.stack.imgur.com/DFyuR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DFyuR.png"" alt=""enter image description here""></a></p>

<p>So, if you navigate to your storage container root, you should see the ABC123.csv file.</p>

<p>Now, if you want to put that file in a folder, just prepend that folder name in your Derived Column transformation formula something like this:</p>

<p>""output/folder1/{Isin}.csv""</p>

<p>The double-quotes activate ADF's string interpolation. You can combine literal text with formulas that way.</p>
"
"60991584","How do I get custom metadata parameters ( Key- Value) for any file/container in blob storage while using Get Metadata activity in a pipeline?","<p>I am using Get Metadata activity to fetch metadata of files/folders from my azure storage account. I want to fetch the metadata parameters which are not in the field list of the Get Metadata activity for eg. custom metadata ( key- value) pair. 
Is there any way I can do this ?</p>
","<azure><metadata><azure-data-factory>","2020-04-02 12:27:51","827","0","1","61112989","<blockquote>
  <p>Is custom metadata ( key- value) pair supported in ADF Get Metadata
  Activity?</p>
</blockquote>

<p>The answer is NO.You could only access columns mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#metadata-options"" rel=""nofollow noreferrer"">document</a>.If you have to refer more metadata information,i provide 2 workarounds with you:</p>

<p>1.Use Azure Function Activity to implement your custom requirements. For example,use blob storage sdk in the azure function which has <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-container-properties-metadata"" rel=""nofollow noreferrer"">Get Properties and Metadata method</a> from blob container and files.This is for urgent.</p>

<p>2.Submit feedback to ADF team to ask adding more common metadata of folders and files.This is for long time.</p>
"
"60990877","Different JSON Pattern in Data Factory","<p>I have a data source with this structure.</p>

<pre><code>{
  ""records"": [
    { 
      ""dateRep"": ""02/04/2020"",
      ""day"": ""2"",
      ""month"": ""4"",
      ""year"": ""2020"",
      ""cases"": ""26"",
      ""deaths"": ""0"",
      ""countriesAndTerritories"": ""Afghanistan"",
      ""geoId"": ""AF"",
      ""countryterritoryCode"": ""AFG"",
      ""popData2018"": ""37172386""
    }
}
</code></pre>

<p>In the azure data factory, I need to create a row for every entry in the record array. Does anybody know, how to realize this?</p>
","<json><azure><azure-data-factory>","2020-04-02 11:48:44","96","1","1","61052420","<p>Please reference my steps:</p>

<p>This is the demo json,  I copied from the link to my Blob Storage as <strong>A.json</strong>:</p>

<pre><code>{   
  ""records"": 
    [
    { 
      ""dateRep"": ""05/04/2020"",
      ""day"": ""5"",
      ""month"": ""4"",
      ""year"": ""2020"",
      ""cases"": ""35"",
      ""deaths"": ""1"",
      ""countriesAndTerritories"": ""Afghanistan"",
      ""geoId"": ""AF"",
      ""countryterritoryCode"": ""AFG"",
      ""popData2018"": ""37172386""
    }, 
    { 
      ""dateRep"": ""04/04/2020"",
      ""day"": ""4"",
      ""month"": ""4"",
      ""year"": ""2020"",
      ""cases"": ""0"",
      ""deaths"": ""0"",
      ""countriesAndTerritories"": ""Afghanistan"",
      ""geoId"": ""AF"",
      ""countryterritoryCode"": ""AFG"",
      ""popData2018"": ""37172386""
    }, 
    { 
      ""dateRep"": ""03/04/2020"",
      ""day"": ""3"",
      ""month"": ""4"",
      ""year"": ""2020"",
      ""cases"": ""43"",
      ""deaths"": ""0"",
      ""countriesAndTerritories"": ""Afghanistan"",
      ""geoId"": ""AF"",
      ""countryterritoryCode"": ""AFG"",
      ""popData2018"": ""37172386""
    }
    ]
}
</code></pre>

<p>Copy active choose the <strong>A.json</strong> as <strong>Source dataset</strong>:
<a href=""https://i.stack.imgur.com/ffEiY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ffEiY.png"" alt=""enter image description here""></a></p>

<p><strong>Sink dataset</strong>:</p>

<p>I create this dep table as sink table in my Azure SQL database:</p>

<pre><code>create table dep (
    dateRep date,
       day int,
      month int,
      year int,
      cases int,
      deaths int,
      countriesAndTerritories varchar(100),
      geoId varchar(50),
      countryterritoryCode varchar(50),
      popData2018  int  
    ) 
</code></pre>

<p><a href=""https://i.stack.imgur.com/L1K9J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L1K9J.png"" alt=""enter image description here""></a></p>

<p><strong>Mappings</strong>:</p>

<p><a href=""https://i.stack.imgur.com/5ghbl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5ghbl.png"" alt=""enter image description here""></a></p>

<p><strong>Run the pipeline:</strong></p>

<p><a href=""https://i.stack.imgur.com/xjXZL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xjXZL.png"" alt=""enter image description here""></a></p>

<p><strong>Check the data in Azure SQL database:</strong></p>

<p><a href=""https://i.stack.imgur.com/Rk0GJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rk0GJ.png"" alt=""enter image description here""></a></p>

<p>This will help you create a row for every entry in the record array.</p>

<p>Hope this helps.</p>
"
"60990307","How to check record count in a csv file uploaded in azure blob storage?","<p>So i am uploading a 2gb  csv file to my BLOB storage, and i want the record count (no of rows) of this file, so that i can validate after it gets loaded to ADW. Is there any way to get record count(like column count) in azure itself.</p>

<p>Thanks in advance</p>
","<sql-server><azure><azure-data-factory>","2020-04-02 11:17:05","1323","2","1","60990659","<p>Azure Blobs are not like local files: You'd have to download (or stream) your blob to something that works through the file to perform any calculation you're trying to do.</p>

<p>Alternatively, you could mount your blob storage to something like Databricks (Spark cluster) and write your code there (same basic concept).</p>

<p>Or... you could do your record counts prior to (or during) your upload to blob storage.</p>

<p>Ultimately, how you perform this counting is really up to you. Blob storage is just bulk storage and knows nothing about file formats.</p>
"
"60989844","How to create DataFrame from the an array in Scala?","<p>I have a use case where I need to create a DataFrame from an array. </p>

<p>I've created a DataFrame that reads a CSV then I am using a map to process/transform it further.</p>

<pre class=""lang-scala prettyprint-override""><code>    var mapTransform = df1.collect.map( 
      line =&gt; {
      // line.split("","") logic for fields separation
      //transformation logic here for various fields

      (field1+"",""+field2+"",""+field3);  
      }
    )
</code></pre>

<p>From this, I am getting an <code>array(Array[String])</code> which is transformed result.</p>

<p>I want to further convert it DataFrames with separate columns so that later it can be used to write to DB or file, however, I am facing an issue. Is it possible to do it? Any solutions?</p>
","<scala><dataframe><apache-spark><azure-data-factory>","2020-04-02 10:51:28","239","0","1","60990067","<p>This does your job:
<code>spark.sparkContext.parallelize(mapTransform.toSeq)</code>
But note that you must avoid methods that produce non-rdd, as they load all the contents of the array to the one node and that's ineffective in the general case.</p>

<p>Also, there's a convention turn <code>var</code>s to <code>val</code>s as much as possible.</p>
"
"60985290","azure data factory: Linked services parameterization (making linked service dynamic)","<p>Scenario: In a multi-tenant architecture, I have the same batch job (ETL) running for multiple clients (tenant-wise). In this case only variable is source and target. In ADF my pipes/mapping-data-flows are statc, the dynamic things are LinkedServices. Even datasets also can be static, because I am expecting same tables in sources and targets  across clients.</p>

<h2>How can I make LinkedServices dynamic?</h2>

<p>This is how I implemented it through Scripts (shell, python).
- A wrapper script will be initiated with client_code as a parameter
- client's DB details and connection details will be available in a DB, get it from there
- ETL job (script) will be initiated with the respective client's source n target DB details
- Logging/ auditing is done at all steps</p>

<p>How can I achieve this through ADF?</p>
","<dynamic><parameters><connection><azure-data-factory>","2020-04-02 06:17:52","235","1","1","61008515","<p>You can parameterize your linked service and pass dynamic values at run time. To achieve your goal, you have to add a parameter to your linked service and parameterize the server name in the linked service definition. </p>

<p><a href=""https://learn.microsoft.com/de-de/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">Parameterize linked services</a></p>
"
"60971414","Problem Importing JSON data in CosmosDB from Azure Data Factory Copy","<p>My Azure data Factory Pipeline has a COPY operation to <strong>COPY data from CSV to CosmosDB</strong>.<br>
CSV file has 3 columns to import in CosmosDB.   </p>

<p>CosmosDB Collection will receive: </p>

<pre><code>Column1 in Partition Key
Column2 in Id
Column3 whole JSON Value
</code></pre>

<p>when I run COPY it copies the data in String format in CosmosDB for all 3 columns. Because <strong>there is no JSON or Map type available</strong> while we specify ColumnType.</p>

<p><a href=""https://i.stack.imgur.com/L32It.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L32It.png"" alt=""enter image description here""></a></p>

<p>What should I do to import JSON in value field, instead of String or TEXT. Below is the sample I am getting in CosmosDB:
<a href=""https://i.stack.imgur.com/OLI9s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OLI9s.png"" alt=""enter image description here""></a></p>
","<azure-cosmosdb><azure-data-factory>","2020-04-01 12:58:21","680","1","1","60986500","<p>Per my knowledge, no such features could help you convert string data to object format in adf cosmos db configuration. </p>

<p>So, as workaround, I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-cosmos-db-triggered-function"" rel=""nofollow noreferrer"">Azure Function Cosmos DB Trigger</a> to process every document when they imported into database. Please refer to my function code:</p>

<pre><code>using System.Collections.Generic;
using Microsoft.Azure.Documents;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Host;
using Newtonsoft.Json.Linq;
using System;
using Microsoft.Azure.Documents.Client;

namespace TestADF
{
    public static class Function1
    {
        [FunctionName(""Function1"")]
        public static void Run([CosmosDBTrigger(
            databaseName: ""db"",
            collectionName: ""item"",
            ConnectionStringSetting = ""documentdbstring"",
            LeaseCollectionName = ""leases"")]IReadOnlyList&lt;Document&gt; input, TraceWriter log)
        {
            if (input != null &amp;&amp; input.Count &gt; 0)
            {
                log.Verbose(""Start........."");
                String endpointUrl = ""https://***.documents.azure.com:443/"";
                String authorizationKey = ""key"";
                String databaseId = ""db"";
                String collectionId = ""item"";

                DocumentClient client = new DocumentClient(new Uri(endpointUrl), authorizationKey);

                for (int i = 0; i &lt; input.Count; i++)
                {
                    Document doc = input[i];
                    if ((doc.GetPropertyValue&lt;String&gt;(""Value"") == null) || (!doc.GetPropertyValue&lt;String&gt;(""Value"")))
                    {                       
                        String V= doc.GetPropertyValue&lt;String&gt;(""Value"");
                        JObject obj = JObject.Parse(V);

                        doc.SetPropertyValue(""Value"", obj );

                        client.ReplaceDocumentAsync(UriFactory.CreateDocumentUri(databaseId, collectionId, doc.Id), doc);

                        log.Verbose(""Update document Id "" + doc.Id);
                    }

                }
            }
        }
    }
}
</code></pre>
"
"60968022","Azure Data Factory: Wrong and unexpected Datatype conversion during import from csv to sql server via pipeline","<p>I am trying to load data from a csv to a sql server database using an Azure pipeline copy data operator. During the import data is converted to other types.</p>

<p>in the Source preview in the pipeline I see the following</p>

<p>1- the value ""0044"" is converted to 44</p>

<p>2- the value 2020000000000000 is converted to 2E+16</p>

<p>3- the value 5.2 is converted to February 5th</p>

<p>4- the value 9.78 is converted to September 1978</p>

<p>so far i could not find a solution for 0044,</p>

<p>I the other cases here is what I did:</p>

<p>for 2 I enclosed the number 2020000000000000 in """" then it worked, though for some reason I get it enclosed in four "" like so: """"2020000000000000""""
for 3 and 4 I replaced the dot for a comma and then it worked.</p>

<p>But I would like to be able to tell the import utility to treat everything just as string and do the conversions in the database.</p>

<p>how can I achive this?</p>

<p>the code shows following for one of the columns in 3 and 4:</p>

<p>(</p>

<pre><code>       ""source"":(

              ""name"": ""Amount""

              ""type"": ""String""

       )

       ""sink"":(

              ""name"": ""Amount""

              ""type"": ""String""

       )
</code></pre>

<p>)</p>

<p>Best Regards,</p>
","<azure><azure-pipelines><azure-data-factory><import-from-csv>","2020-04-01 09:47:37","1605","1","2","60985312","<p>All the default data type in csv is String.</p>

<p><a href=""https://i.stack.imgur.com/iURgF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iURgF.png"" alt=""enter image description here""></a> </p>

<p>For Azure SQL database/SQL Server, we can not store data '0044' as <strong>int</strong> data type. You need convert '0044' as String:</p>

<p><a href=""https://i.stack.imgur.com/ZOMgw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZOMgw.png"" alt=""enter image description here""></a></p>

<p>We could using select convert to <code>44</code> to '0044':</p>

<pre><code>select right('0000'+ltrim([a]),4) new_a, b from test12
</code></pre>

<p><a href=""https://i.stack.imgur.com/YXc9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YXc9c.png"" alt=""enter image description here""></a></p>

<p>When we copy data from csv file, you need think about if the data in csv file is valid data type in Azure SQL database/SQL Server. For example, the data '2020000000000000' is out of int length.</p>

<p>It's very important to design the sink table. So the suggestions is that you first create the sink table in you Azure SQL database with the suitable data type for every column, then set the column mappings in Copy active manually:</p>

<p><strong>Mapping settings:</strong></p>

<p><a href=""https://i.stack.imgur.com/pjsjl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pjsjl.png"" alt=""enter image description here""></a> </p>

<p><strong>Pipeline running:</strong></p>

<p><a href=""https://i.stack.imgur.com/KqKkO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KqKkO.png"" alt=""enter image description here""></a></p>

<p>Data check in SQL database:</p>

<p><a href=""https://i.stack.imgur.com/1jHZK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1jHZK.png"" alt=""enter image description here""></a></p>

<p><strong>Update:</strong></p>

<p>The issue is solved by Ramiro Kollmannsperger himself now:</p>

<p>""my sink table in the database has only nvarchar columns. I did this so after a lot of headaches with datatypes and length. I decided that it is easier for me to just do the conversions from nvarchar in the database into a staging table. What helped in the end was to do the schema import in the source Dataset where the csv is read. There is a tab ""connection"" and next to it another tab ""schema"" where you can import the schema. After doing this it worked.""</p>

<p>Hope this helps.</p>
"
"60968022","Azure Data Factory: Wrong and unexpected Datatype conversion during import from csv to sql server via pipeline","<p>I am trying to load data from a csv to a sql server database using an Azure pipeline copy data operator. During the import data is converted to other types.</p>

<p>in the Source preview in the pipeline I see the following</p>

<p>1- the value ""0044"" is converted to 44</p>

<p>2- the value 2020000000000000 is converted to 2E+16</p>

<p>3- the value 5.2 is converted to February 5th</p>

<p>4- the value 9.78 is converted to September 1978</p>

<p>so far i could not find a solution for 0044,</p>

<p>I the other cases here is what I did:</p>

<p>for 2 I enclosed the number 2020000000000000 in """" then it worked, though for some reason I get it enclosed in four "" like so: """"2020000000000000""""
for 3 and 4 I replaced the dot for a comma and then it worked.</p>

<p>But I would like to be able to tell the import utility to treat everything just as string and do the conversions in the database.</p>

<p>how can I achive this?</p>

<p>the code shows following for one of the columns in 3 and 4:</p>

<p>(</p>

<pre><code>       ""source"":(

              ""name"": ""Amount""

              ""type"": ""String""

       )

       ""sink"":(

              ""name"": ""Amount""

              ""type"": ""String""

       )
</code></pre>

<p>)</p>

<p>Best Regards,</p>
","<azure><azure-pipelines><azure-data-factory><import-from-csv>","2020-04-01 09:47:37","1605","1","2","61121316","<p>my sink table in the database has only nvarchar columns. 
I did this so after a lot of headaches with datatypes and length.</p>

<p>I decided that it is easier for me to just do the conversions from nvarchar in the database into a staging table. </p>

<p>What helped in the end was to do the schema import in the source Dataset where the csv is read. 
There is a tab ""connection"" and next to it another tab ""schema"" where you can import the schema. After doing this it worked. </p>
"
"60967890","Copy image from REST to Blob using DataFacotry","<p>In my scenario I need to consume an external REST api. One of the fields in the response is a url to an image. What I'm trying to achieve is to grab the pic behind that url and store it in the blob storage. This would be easy with a Function or WebJob but is there a way to do it with DataFactory on its own?</p>
","<azure-data-factory>","2020-04-01 09:40:11","103","0","1","60995843","<p>Based on my research,only <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">Http Connector</a> supports downloading file which could be used in the copy activity as source dataset.</p>

<p><a href=""https://i.stack.imgur.com/9JBxK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JBxK.png"" alt=""enter image description here""></a></p>
"
"60964612","Using XML with REST API through Azure Datafactory V2","<p>I need to connect with a webservice through a REST API, get data and insert that data in a Azure SQL Database.
We are using Apache Airflow for that but our engineer is leaving. Finding a replacement has proven to be very hard. Programming skills (Python) are required to work with Apache Airflow.</p>

<p>That is why I am investigating if we use can Datafactory since it requires less programming skills.
However, it seems that the REST connector only understands JSON, while our source is delivering XML.</p>

<p>What would be a possible solution getting XML from a REST API using datafactory? That requires little to no programming skills?</p>
","<json><xml><rest><azure-data-factory><twinfield>","2020-04-01 06:05:49","1296","0","1","60965807","<p>Based upon the documentation, Data Factory Rest connector will only consume JSON from rest services.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest</a>
Have you tried to alter the Accept header in your request to application/json
Basically this request the response to be in JSON and not XML.</p>
<p>Regards Frode.</p>
"
"60955104","Convert any JSON, multiple-times nested structure into the KEY and VALUE fields","<p>I was requested to build an ETL pipeline in Azure. This pipeline should</p>

<ol>
<li>read ORC file submitted by the vendor to ADLS</li>
<li>parse the PARAMS field, existing in the ORC structure, where JSON structure is stored, and add it as two new fields (KEY, VALUE) to the output</li>
<li>write the output to the  Azure SQL database</li>
</ol>

<p>The problem is, that there are different types of JSONs structures used by the different types of records. I do not want to write a custom expression per each of the class of JSON struct (there would be like hundreds of them). Rather, I'm looking for a generic mechanism, that will be able to parse them apart of the type of the input JSON structure.</p>

<p>At the moment, to fulfill this requirement, I was using the ADF built-in connector for ORC. The process in its current design:</p>

<ol>
<li>Use a copy activity that reads ORC and moves data to Azure SQL database</li>
<li><p>Use the following TSQL statement as part of stored procedure executed after the 1. to parse the PARAMS field content</p>

<pre><code>SELECT uuid, 
       AttrName = a1.[key] + 
                    COALESCE('.' + a2.[key], '') + 
                    COALESCE('.' + a3.[key], '') + 
                    COALESCE('.' + a4.[key], ''), 
       AttrValue = COALESCE(a4.value, a3.value, a2.value, a1.value)
FROM ORC.EventsSnapshot_RawData
     OUTER APPLY OPENJSON(params) a1
                                  OUTER APPLY
(
    SELECT [key], 
           value, 
           type
    FROM OPENJSON(a1.value)
    WHERE ISJSON(a1.value) = 1
) a2
  OUTER APPLY
(
    SELECT [key], 
           value, 
           type
    FROM OPENJSON(a2.value)
    WHERE ISJSON(a2.value) = 1
) a3
  OUTER APPLY
(
    SELECT [key], 
           value, 
           type
    FROM OPENJSON(a3.value)
    WHERE ISJSON(a3.value) = 1
) a4
</code></pre></li>
</ol>

<p>The number of required OUTER APPLY statements is determined at the beginning by counting occurrences  of ""["" in the PARAMS field value and then used to dynamically generate the SQL executed via <code>sp_executesql</code></p>

<p>Unfortunately, this approach is quite inefficient in terms of execution time, as for 11 MM of records it takes c.a. 3.5 hours to finish</p>

<p>Someone suggested me to use Data Bricks. Ok, so I:</p>

<ol>
<li><p>created the notebook with the following python code to read ORC from ADLS and materialize it to Data Bricks table</p>

<pre><code>    orcfile = ""/mnt/adls/.../Input/*.orc""
    eventDf = spark.read.orc(orcfile)
    #spark.sql(""drop table if exists  ORC.Events_RawData"")
    eventDf.write.mode(""overwrite"").saveAsTable(""ORC.Events_Raw"")
</code></pre>

<ol start=""2"">
<li>now I'm trying to find out a code that would give the result I get from TSQL OPENJSONs. I started with Python code that utilizes recursion to parse the PARAMS attribute, however, it is even more inefficient than TSQL in terms of execution speed.</li>
</ol></li>
</ol>

<p>Can you please suggest me the correct way of achieving the goal, i.e. converting the PARAMS attribute to KEY, VALUE attributes in a generic way?</p>

<p>[EDIT]
Please find below a sample JSON structures that needs to be standarized into the expected  structure</p>

<p><strong>Sample1</strong></p>

<pre><code>    {
    ""correlationId"": ""c3xOeEEQQCCA9sEx7-u6FA"",
    ""eventCreateTime"": ""2020-05-12T15:38:23.717Z"",
    ""time"": 1589297903717,
    ""owner"": {
        ""ownergeography"": {
            ""city"": ""abc"",
            ""country"": ""abc""
        },
        ""ownername"": {
            ""firstname"": ""abc"",
            ""lastname"": ""def""
        },
        ""clientApiKey"": ""xxxxx"",
        ""businessProfileApiKey"": null,
        ""userId"": null
    },
    ""campaignType"": ""Mobile push""
}
</code></pre>

<p><strong>Sample2</strong></p>

<pre><code>{
    ""correlationIds"": [
        {
            ""campaignId"": ""iXyS4z811Rax"",
            ""correlationId"": ""b316233807ac68675f37787f5dd83871""
        }
    ],
    ""variantId"": 1278915,
    ""utmCampaign"": """",
    ""ua.os.major"": ""8""
    }
</code></pre>

<p><strong>Sample3</strong></p>

<pre><code>{
    ""correlationId"": ""ls7XmuuiThWzktUeewqgWg"",
    ""eventCreateTime"": ""2020-05-12T12:40:20.786Z"",
    ""time"": 1589287220786,
    ""modifiedBy"": {
        ""clientId"": null,
        ""clientApiKey"": ""xxx"",
        ""businessProfileApiKey"": null,
        ""userId"": null
    },
    ""campaignType"": ""Mobile push""
}
</code></pre>

<p><strong>Sample expected output</strong>
(Spark dataFrame)
<a href=""https://i.stack.imgur.com/MOqtW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MOqtW.png"" alt=""enter image description here""></a></p>
","<json><tsql><azure-data-factory><databricks>","2020-03-31 16:15:35","336","1","1","61799899","<p>Well, this is your <em>get all and everything</em> approach :-)</p>

<p>First we create a declared table variable and fill it with your samples to simuate your issue (please try to provide this yourself the next time).</p>

<pre class=""lang-sql prettyprint-override""><code>DECLARE @table TABLE(ID INT IDENTITY, AnyJSON NVARCHAR(MAX));
INSERT INTO @table VALUES
(N' {
    ""correlationId"": ""c3xOeEEQQCCA9sEx7-u6FA"",
    ""eventCreateTime"": ""2020-05-12T15:38:23.717Z"",
    ""time"": 1589297903717,
    ""owner"": {
        ""ownergeography"": {
            ""city"": ""abc"",
            ""country"": ""abc""
        },
        ""ownername"": {
            ""firstname"": ""abc"",
            ""lastname"": ""def""
        },
        ""clientApiKey"": ""xxxxx"",
        ""businessProfileApiKey"": null,
        ""userId"": null
    },
    ""campaignType"": ""Mobile push""
}')
,(N'{
    ""correlationIds"": [
        {
            ""campaignId"": ""iXyS4z811Rax"",
            ""correlationId"": ""b316233807ac68675f37787f5dd83871""
        }
    ],
    ""variantId"": 1278915,
    ""utmCampaign"": """",
    ""ua.os.major"": ""8""
    }')
,(N'{
    ""correlationId"": ""ls7XmuuiThWzktUeewqgWg"",
    ""eventCreateTime"": ""2020-05-12T12:40:20.786Z"",
    ""time"": 1589287220786,
    ""modifiedBy"": {
        ""clientId"": null,
        ""clientApiKey"": ""xxx"",
        ""businessProfileApiKey"": null,
        ""userId"": null
    },
    ""campaignType"": ""Mobile push""
}');
</code></pre>

<p>--The query</p>

<pre class=""lang-sql prettyprint-override""><code>WITH recCTE AS
(
    SELECT ID
          ,CAST(1 AS BIGINT) AS ObjectIndex
          ,CAST(N'000' COLLATE DATABASE_DEFAULT AS NVARCHAR(MAX)) SortString
          ,1 AS NestLevel
          ,CAST(CONCAT(N'Root-',ID,'.') COLLATE DATABASE_DEFAULT AS NVARCHAR(MAX)) AS JsonPath
          ,CAST(N'$' COLLATE DATABASE_DEFAULT AS NVARCHAR(MAX)) AS JsonKey
          ,CAST(AnyJSON COLLATE DATABASE_DEFAULT AS NVARCHAR(MAX)) AS JsonValue 
          ,CAST(CASE WHEN ISJSON(AnyJSON)=1 THEN AnyJSON COLLATE DATABASE_DEFAULT ELSE NULL END AS NVARCHAR(MAX)) AS NestedJSON 
    FROM @table t

    UNION ALL

    SELECT r.ID
          ,ROW_NUMBER() OVER(ORDER BY (SELECT NULL))
          ,CAST(CONCAT(r.SortString,STR(ROW_NUMBER() OVER(ORDER BY (SELECT NULL)),3)) AS NVARCHAR(MAX))
          ,r.NestLevel+1
          ,CAST(CONCAT(r.JsonPath, A.[key] + N'.') COLLATE DATABASE_DEFAULT AS NVARCHAR(MAX))
          ,CAST(A.[key] COLLATE DATABASE_DEFAULT AS NVARCHAR(MAX))
          ,r.JsonValue  COLLATE DATABASE_DEFAULT
          ,CAST(A.[value] COLLATE DATABASE_DEFAULT AS NVARCHAR(MAX))
    FROM recCTE r
    CROSS APPLY OPENJSON(r.NestedJSON) A
    WHERE ISJSON(r.NestedJSON)=1
)
SELECT ID
      ,JsonPath
      ,JsonKey
      ,NestedJSON AS JsonValue
FROM recCTE 
WHERE ISJSON(NestedJSON)=0
ORDER BY recCTE.ID,SortString;
</code></pre>

<p>The result</p>

<pre><code>+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.correlationId.                  | correlationId   | c3xOeEEQQCCA9sEx7-u6FA           |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.eventCreateTime.                | eventCreateTime | 2020-05-12T15:38:23.717Z         |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.time.                           | time            | 1589297903717                    |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.owner.ownergeography.city.      | city            | abc                              |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.owner.ownergeography.country.   | country         | abc                              |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.owner.ownername.firstname.      | firstname       | abc                              |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.owner.ownername.lastname.       | lastname        | def                              |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.owner.clientApiKey.             | clientApiKey    | xxxxx                            |
+---+----------------------------------------+-----------------+----------------------------------+
| 1 | Root-1.campaignType.                   | campaignType    | Mobile push                      |
+---+----------------------------------------+-----------------+----------------------------------+
| 2 | Root-2.correlationIds.0.campaignId.    | campaignId      | iXyS4z811Rax                     |
+---+----------------------------------------+-----------------+----------------------------------+
| 2 | Root-2.correlationIds.0.correlationId. | correlationId   | b316233807ac68675f37787f5dd83871 |
+---+----------------------------------------+-----------------+----------------------------------+
| 2 | Root-2.variantId.                      | variantId       | 1278915                          |
+---+----------------------------------------+-----------------+----------------------------------+
| 2 | Root-2.utmCampaign.                    | utmCampaign     |                                  |
+---+----------------------------------------+-----------------+----------------------------------+
| 2 | Root-2.ua.os.major.                    | ua.os.major     | 8                                |
+---+----------------------------------------+-----------------+----------------------------------+
| 3 | Root-3.correlationId.                  | correlationId   | ls7XmuuiThWzktUeewqgWg           |
+---+----------------------------------------+-----------------+----------------------------------+
| 3 | Root-3.eventCreateTime.                | eventCreateTime | 2020-05-12T12:40:20.786Z         |
+---+----------------------------------------+-----------------+----------------------------------+
| 3 | Root-3.time.                           | time            | 1589287220786                    |
+---+----------------------------------------+-----------------+----------------------------------+
| 3 | Root-3.modifiedBy.clientApiKey.        | clientApiKey    | xxx                              |
+---+----------------------------------------+-----------------+----------------------------------+
| 3 | Root-3.campaignType.                   | campaignType    | Mobile push                      |
+---+----------------------------------------+-----------------+----------------------------------+
</code></pre>

<p>The idea in short:</p>

<ul>
<li>we use a recursive CTE to walk this down.</li>
<li>The query will test any fragment (<code>[value]</code> coming from <code>OPENJSON</code>) for being valid JSON.</li>
<li>If the fragment is valid, this walks deeper and deeper.</li>
<li>The column <code>SortString</code> is needed to get a final sort order.</li>
</ul>

<p>Come back, if you have any open questions.</p>
"
"60951946","Adding a ADF permissions to key vault in ARM","<p>Basically,  I'm trying to add the ability for a data factory to be able to get see secrets from a key vault via the ARM key-vault template so it's applied on a release.</p>

<p>However, the issue comes when I trying to release the project I get an error saying the data factory is not in the same resource group (which was kind of expected), however, I can't see a way of passing in the resource group in order for the function to see the correct resource group where the data factory is located. </p>

<pre><code>""accessPolicies"": [

          {
            ""tenantId"": ""[subscription().tenantId]"",
            ""objectId"": ""[reference(concat('Microsoft.DataFactory/factories/', parameters('DataFactoryName')),'2018-06-01','Full').identity.principalId]"",
            ""permissions"": {
              ""secrets"": [
                ""Get""
              ]
            }
          }
</code></pre>

<p>Can anyone help</p>
","<azure><azure-data-factory><azure-resource-manager><azure-keyvault><azure-rm-template>","2020-03-31 13:38:07","521","0","1","60963732","<p>Add the parameter <code>OtherGroupName</code> in the <code>parameters</code>, the value of  <code>OtherGroupName</code> needs to be the resource group name of your datafactory.</p>

<pre><code>""OtherGroupName"":{
    ""type"": ""String""
}
</code></pre>

<p>Then use the <code>accessPolicies</code> like below:</p>

<pre><code> ""accessPolicies"": [
                    {
                        ""tenantId"": ""[subscription().tenantId]"",
                        ""objectId"": ""[reference(ResourceId(parameters('OtherGroupName'), 'Microsoft.DataFactory/factories', parameters('DataFactoryName')),'2018-06-01','Full').identity.principalId]"",
                        ""permissions"": {
                            ""keys"": [],
                            ""secrets"": [
                                ""Get""
                            ],
                            ""certificates"": []
                        }
                    }
                ]
</code></pre>

<hr>

<p><strong>My complete sample</strong>:</p>

<pre><code>{
    ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
    ""contentVersion"": ""1.0.0.0"",
    ""parameters"": {
        ""vaults_joykeyvault12_name"": {
            ""type"": ""String""
        },
        ""DataFactoryName"": {
            ""type"": ""String""
        },
        ""OtherGroupName"":{
            ""type"": ""String""
        }
    },
    ""variables"": {},
    ""resources"": [
        {
            ""type"": ""Microsoft.KeyVault/vaults"",
            ""apiVersion"": ""2016-10-01"",
            ""name"": ""[parameters('vaults_joykeyvault12_name')]"",
            ""location"": ""eastus"",
            ""tags"": {},
            ""properties"": {
                ""sku"": {
                    ""family"": ""A"",
                    ""name"": ""Standard""
                },
                ""tenantId"": ""[subscription().tenantId]"",
                ""accessPolicies"": [
                    {
                        ""tenantId"": ""[subscription().tenantId]"",
                        ""objectId"": ""[reference(ResourceId(parameters('OtherGroupName'), 'Microsoft.DataFactory/factories', parameters('DataFactoryName')),'2018-06-01','Full').identity.principalId]"",
                        ""permissions"": {
                            ""keys"": [],
                            ""secrets"": [
                                ""Get""
                            ],
                            ""certificates"": []
                        }
                    }
                ],
                ""enabledForDeployment"": false,
                ""enabledForDiskEncryption"": false,
                ""enabledForTemplateDeployment"": false,
                ""enableSoftDelete"": true
            }
        }
    ]
}
</code></pre>

<p>I test it with powershell <code>New-AzResourceGroupDeployment</code>, it works fine.</p>

<p><a href=""https://i.stack.imgur.com/NtynU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NtynU.png"" alt=""enter image description here""></a></p>

<p>Check in the portal:</p>

<p><a href=""https://i.stack.imgur.com/CPG2o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CPG2o.png"" alt=""enter image description here""></a></p>
"
"60941775","ADF Data Flows do not complete in debug mode","<p>I am building pipelines on Azure Data Factory, using the Mapping Data Flow activity (Azure SQL DB to Synapse). The pipelines complete in debug mode, when I enable sampling data for the sources. When I disable sampling data and run the debug, I make no progress in the pipeline. i.e. none of the transformations complete (yellow dot)</p>

<p>To improve this, should I increase the batch size from the source/sink (how do I determine a batch size), increase the number of partitions (how do I determine a good number of partitions)</p>
","<azure><azure-synapse><azure-data-factory>","2020-03-31 00:50:27","1408","0","1","60943676","<p>What is the size of the Spark compute cluster you have set in the Azure Integration Runtime under data flow properties. Start there by creating an Azure IR with enough cores to provide RAM for your process. Then you can adjust the partitions and batch sizes. Much of the learnings in this area are shared here at this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance"" rel=""nofollow noreferrer"">ADF data flow performance guide</a>.</p>
"
"60936464","CI/CD for Azure Data Factory - Cannot publish in the collaboration branch","<p>We are working in the customer Azure tenant as members and integrated DevOps GIT to the ADF resource.</p>

<p>We have successfully created and merged the changes to the collaboration branch but have not been able to publish from there to the adf_publish repo as we get the following error:</p>

<blockquote>
  <p>""The client ""<em>value</em>"" with the id Object ""<em>value</em>"" is not authorized
  to perform the action ""...Microsoft.Resources / deployments / write"" on
  the scope ""...
  /providers/Microsoft.Resources/deployments/publishing_1585587726090""
  or the scope is invalid. If access has been granted recently, update
  the credentials.""</p>
</blockquote>

<p>What could be the problem? I am already in the collaboration branch (master).</p>

<p><strong>The configuration of our accounts are:</strong></p>

<p><strong>Data factory role:</strong> Data Factory contributor</p>

<p><strong>DevOps role:</strong> Contributor </p>

<p><strong>Data factory GIT repo security configuration:</strong>
<a href=""https://i.stack.imgur.com/nNUmt.png"" rel=""nofollow noreferrer"">repo security configuration</a></p>
","<azure><continuous-integration><azure-data-factory>","2020-03-30 17:50:02","518","1","1","60938128","<p>You need to be Data Factory Contributor at the resource group level. </p>
"
"60933885","Azure data flow cross join","<p>I have one requirement that I need to achieve with Azure data flow.</p>

<p><a href=""https://i.stack.imgur.com/QkOmY.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>As you can see in output that we can easily using cross and split function in sql query but same thing how can I achieve using data flow.</p>
","<azure><azure-sql-database><azure-data-factory><dataflow><paas>","2020-03-30 15:24:46","1314","0","1","60962001","<p>You can do this in ADF Data Flows.</p>

<p><a href=""https://i.stack.imgur.com/7bf0b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7bf0b.png"" alt=""enter image description here""></a></p>

<ol>
<li>From your flat file source, add a derived column</li>
<li>Create a new column called ""countries"".We'll take your multi-valued column and spit it into that array using this expression: split(country,',')</li>
<li>Use Flatten to denormalize into the output you asked for</li>
<li>Flatten: Set unroll to countries (I called it myarray in the screenshot) and take out the original comma-delimited field from your field mappings.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/SWeJl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SWeJl.png"" alt=""enter image description here""></a></p>
"
"60933787","Azure Data Factory - SQL to nested JSON","<p>I have a SQL database with tables for Staff and Appointments (1 staff : many appointments). I'd like to use Azure Data Factory to output this to nested JSON in a Blob store in a format similar to the below:</p>

<pre><code>[
   {
      ""staffid"":""101"",
      ""firstname"":""Donald"",
      ""lastname"":""Duck"",
      ""appointments"":[
         {
            ""appointmentid"":""201"",
            ""startdate"":""2020-02-01T00:00:00"",
            ""enddate"":""2020-04-29T23:00:00""
         },
         {
            ""appointmentid"":""202"",
            ""startdate"":""2020-01-01T00:00:00"",
            ""enddate"":""2020-01-31T00:00:00""
         }
      ]
   },
   {
      ""staffid"":""102"",
      ""firstname"":""Mickey"",
      ""lastname"":""Mouse"",
      ""appointments"":[
         {
            ""appointmentid"":""203"",
            ""startdate"":""2020-02-01T00:00:00"",
            ""enddate"":""2020-04-29T23:00:00""
         },
         {
            ""appointmentid"":""204"",
            ""startdate"":""2020-01-01T00:00:00"",
            ""enddate"":""2020-01-31T00:00:00""
         }
      ]
   }
]
</code></pre>

<p>I've tried using the Copy activity but this produces flat JSON structures rather than the nested structure described above. Has anyone got a way to do this please?</p>
","<sql><json><azure><azure-data-factory>","2020-03-30 15:20:07","2063","2","1","60945020","<p>More scenarios for JSON data in ADF is flattening. However,according to your description,your need producing JSON contains Json array group by some columns.Something like merge <code>appointment things</code> by same <code>staff</code>.</p>

<p>If my understanding is right,then you could get some clues from my previous case:<a href=""https://stackoverflow.com/questions/60169317/how-to-split-into-sub-documents-with-nesting-separator/60184711#60184711"">How to split into Sub Documents with Nesting separator?</a>. Please refer to my test:</p>

<p>Simulate your sample data:</p>

<p><a href=""https://i.stack.imgur.com/RMUO3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RMUO3.png"" alt=""enter image description here""></a></p>

<p>Use sql in sql db source dataset:</p>

<pre><code>select app.staffid,app.firstname,app.lastname,
'appointments' = (
            SELECT
                appointmentid AS 'appointmentid',startdate as 'startdate',enddate as 'enddate'
            FROM
                dbo.appoint as app2
            where app2.staffid = app.staffid and
            app2.firstname = app.firstname and
            app2.lastname = app.lastname
            FOR JSON PATH)
from dbo.appoint as app
group by app.staffid,app.firstname,app.lastname
FOR JSON Path;
</code></pre>

<p><a href=""https://i.stack.imgur.com/pgNuL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pgNuL.png"" alt=""enter image description here""></a></p>

<p>Output in blob storage:</p>

<p><a href=""https://i.stack.imgur.com/02IWI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/02IWI.png"" alt=""enter image description here""></a></p>

<p>I try to verify the json format and it is correct.</p>

<p><a href=""https://i.stack.imgur.com/hicD1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hicD1.png"" alt=""enter image description here""></a></p>
"
"60929357","Custom activity failed due to permission","<p>I have created Azure Batch pool and tasks are running fine using Python with Pool autouser, admin. But when I run same commands from Custom Activity in Data Factory like import pandas or import pyspark it gives me permission error. Azure Batch task is showing up User Identity as nonadmin (pool). </p>

<p>How would I change the user identity to pool or task autouser admin?</p>
","<azure-data-factory><azure-batch>","2020-03-30 11:15:44","277","0","1","60966046","<blockquote>
  <p>How would I change the user identity to pool or task autouser admin?</p>
</blockquote>

<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/batch/batch-user-accounts#elevated-access-for-tasks"" rel=""nofollow noreferrer"">document</a>,there are two options for elevation level:</p>

<ul>
<li>NonAdmin: The task runs as a standard user without elevated access.
The default elevation level for a Batch user account is always
NonAdmin.</li>
<li>Admin: The task runs as a user with elevated access and operates with
full Administrator permissions.</li>
</ul>

<p>However, you can't change the permission level of the auto-user account when you using custom activity in ADF.Please see the statement <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity-permissions"" rel=""nofollow noreferrer"">here</a>.</p>
"
"60926029","ADF Access to ADLS Gen2 within VNet","<p>I'm looking for some assistance in configuring an ADLS Gen2 which has the firewall enabled and is joined to a VNet so that I can connect to it from ADF.</p>

<p>If I add the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""nofollow noreferrer"">IP addresses</a> for the Azure IR to the storage firewall, without the adding the VNet, the linked service connection is successful, however, when I add the VNet to the storage, the connection test fails with the generic error message as if the firewall is blocking the connection.</p>

<p>I have also tried adding the Azure IR IP addresses / DataFactory service tag to the NSG as an inbound rule but this makes no difference, the connection still fails.</p>

<p>Any suggestions?</p>

<p>Thanks</p>
","<azure-storage><azure-data-factory><azure-virtual-network><azure-nsg>","2020-03-30 07:42:19","1705","1","1","60927995","<p>If you add the VNets to the firewall of the storage account, it means that you enable a Service endpoint for Azure Storage within the VNet, In this case, you are <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security#grant-access-from-a-virtual-network"" rel=""nofollow noreferrer"">allowed access storage only from specific subnets</a>.</p>

<p>Currently, Data Factory is now a <strong>Trusted Service</strong> <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security#exceptions"" rel=""nofollow noreferrer"">exceptions</a> in the Azure Storage firewall.
Integration runtime (Azure, Self-hosted, and SSIS) can now connect to Storage without having to be inside the same virtual network or requiring you to allow all inbound connections to the service. </p>

<p><strong>Steps</strong> to connect to Azure Storage (using Azure blob or Azure Data lake Gen2 linked service) as ‘Trusted Service’ from <a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/data-factory-is-now-a-trusted-service-in-azure-storage-and-azure/ba-p/964993"" rel=""nofollow noreferrer"">this blog</a>.</p>

<ol>
<li>Grant Data Factory’s Managed identity access to read data in storage’s access control. For more detailed instructions, please refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#managed-identity"" rel=""nofollow noreferrer"">this</a>.</li>
<li>Create the linked service using Managed identities for Azure resources authentication</li>
<li>Modify the firewall settings in Azure Storage account to select ‘Allow trusted Microsoft Services…’. </li>
</ol>

<p>For more references: <a href=""https://towardsdatascience.com/how-to-secure-your-azure-data-factory-pipeline-e2450502cd43"" rel=""nofollow noreferrer"">https://towardsdatascience.com/how-to-secure-your-azure-data-factory-pipeline-e2450502cd43</a></p>
"
"60925477","How to load multiple files with overlapping but dynamic schema columns using azure copy activity?","<p>I have around 7-8 files with different but overlapping schemas. However i want the schema of the target (azure synapse table) to be the schema of the largest no of columns.</p>

<p>my files have - 61 columns, 93 columns and 96 columns
so my target table will have 96 columns</p>

<p>but while loading the 61 column tables, i want the rest 30 something rows to be loaded as NULL. I am using data flow to load recursively using *.csv option. </p>

<p>Can anyone please let me know how to achieve this?</p>

<p>my sources are csv files in azure BLOB and my target is one single table with the schema of the 96 columns.</p>

<p>(one more catch is, just one of the files has two columns named different but values are same, i dont know how to map this else it will look like two more new column and we will end up having 98 columns)</p>

<p>also i cant do manual mapping because doing that for these many files would take a lot of time.</p>

<p>Please let me know.
Thanks in advance</p>
","<azure><azure-data-factory>","2020-03-30 06:57:51","668","1","1","60965139","<p>I try to implement your requirement with copy activity in ADF.</p>

<p>Prepare for the test data:</p>

<p>three files in blob storage container.</p>

<p><a href=""https://i.stack.imgur.com/IJDBm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IJDBm.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/VWsJF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VWsJF.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Z7Zjy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z7Zjy.png"" alt=""enter image description here""></a></p>

<p>Column setting in SQL DB:</p>

<p><a href=""https://i.stack.imgur.com/53COr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/53COr.png"" alt=""enter image description here""></a></p>

<p>Then use Get Metadata Activity and For Each Activity in ADF:</p>

<p><a href=""https://i.stack.imgur.com/cGbiV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cGbiV.png"" alt=""enter image description here""></a></p>

<p>Configuration for Get Metadata Activity:</p>

<p><a href=""https://i.stack.imgur.com/JVBgm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JVBgm.png"" alt=""enter image description here""></a></p>

<p>Configuration for For Each Activity:</p>

<p><a href=""https://i.stack.imgur.com/Ljvz1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ljvz1.png"" alt=""enter image description here""></a></p>

<p>Inside For Each Activity ,please use copy activity. Set blob storage as source dataset and sql db(i use sql db for test,you could use synapse db here) as sink dataset.</p>

<p><a href=""https://i.stack.imgur.com/63692.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/63692.png"" alt=""enter image description here""></a></p>

<p>Test Result in SQL DB:</p>

<p><a href=""https://i.stack.imgur.com/bfuDX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bfuDX.png"" alt=""enter image description here""></a></p>

<p>Furthermore,to be honest,i didn't find any way to map <code>just one of the files has two columns named different but values are same</code>. I believe it's not supported by ADF so far. Since you definitely know which column matches this situation, you may deal with them using code later.(which i think it's not too tough)</p>
"
"60910111","Custom Script in Azure Data Factory & Azure Databricks","<p>I have a requirement to parse a lot of small files and load them into a database in a flattened structure. I prefer to use ADF V2 and SQL Database to accomplish it. The file parsing logic is already available using Python script and I wanted to orchestrate it in ADF. I could see an option of using Python Notebook connector to Azure Databricks in ADF v2. May I ask if I will be able to just run a plain Python script in Azure Databricks through ADF? If I do so, will I just run the script in Databricks cluster's driver only and might not utilize the cluster's full capacity. I am also thinking of calling Azure functions as well. Please advise which one is more appropriate in this case.</p>
","<azure><azure-data-factory><azure-databricks>","2020-03-29 04:14:56","368","0","1","60928050","<p>Just provide some ideas for your reference.</p>

<p>Firstly, you are talking about Notebook and Databricks which means ADF's own <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">copy activity</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create"" rel=""nofollow noreferrer"">Data Flow</a> can't meet your needs, since as i know, ADF could meet just simple flatten feature! If you miss that,please try that first.</p>

<p>Secondly,if you do have more requirements beyond ADF features, why not just leave it?Because Notebook and Databricks don't have to be used with ADF,why you want to pay more cost then? For Notebook, you have to install packages by yourself,such as pysql or pyodbc. For Azure Databricks,you could mount azure blob storage and access those files as File System.In addition,i suppose you don't need many workers for cluster,so just configure it as 2 for max.</p>

<p><a href=""https://i.stack.imgur.com/fW4pC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fW4pC.png"" alt=""enter image description here""></a></p>

<p>Databricks is more suitable for managing as a job i think.</p>

<p>Azure Function also could be an option.You could create a blob trigger and load the files into one container. Surely,you have to learn the basic of azure function if you are not familiar with it.However,Azure Function could be more economical.</p>
"
"60883585","Azure Data Factory error on IT: ""Only an Azure integration runtime with 'Auto Resolve' location can be used in Data Flow Activity.""","<p>Since Azure have some problems due to CoronaVirus, I try to run my dataflow on a separate IR, but I get the following message:
""Only an Azure integration runtime with 'Auto Resolve' location can be used in Data Flow Activity.""</p>

<p>I tried to change the default Integration runtime, but it's not editable. The point is that I am trying to change the region more close to mine and give more compute power...</p>

<p>Anyone anyidea</p>
","<azure-data-factory>","2020-03-27 09:53:07","654","1","1","60891455","<p>We are looking at allowing Azure IR for data flows to pin to specific regions instead of only ""Auto Resolve"".</p>

<p>In the interim, you can export your Data Factory and import the factory, or use your Git repo, to move the factory to another region.</p>

<p>That will allow the data flow Azure IR to spin-up compute in another region.</p>
"
"60883244","What is *.servicebus.windows.net used for in Azure Data Factory?","<p>In a hybrid scenario with self-hosted integration runtime installed to connect to on-premises database, and firewall allowing outbound port 443 *.frontend.clouddatahub.net which according to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations"" rel=""nofollow noreferrer"">docs</a> is <em>Required by the self-hosted integration runtime to connect to the Data Factory service</em>.</p>

<p>The integration runtime is able to connect to Azure Data Factory with status <em>Running (Limited)</em> and a warning saying that the cloud service cannot connect to the integration runtime through Service Bus.</p>

<p>The same <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations"" rel=""nofollow noreferrer"">docs</a> say that outbound port 443 *.servicebus.windows.net is <em>Required by the self-hosted integration runtime to connect to data movement services in Azure Data Factory</em>. If I understand correctly this includes copying data to the on-premises database and therefore is required.</p>

<p>What exactly is Service Bus used for in Azure Data Factory, and preferably from the integration runtime perspective? The figure for Hybrid scenarios shows <em>Command Channel</em> and <em>Data Channel</em>. Does the latter represent data sent over Service Bus?</p>

<p>Any insight will be appreciated. Thanks for your consideration and have a nice day!</p>
","<azure><azure-data-factory>","2020-03-27 09:28:39","3517","1","1","60921029","<p>It's not data, but instructions.  If you look at the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#command-flow-and-data-flow"" rel=""nofollow noreferrer"">webpage</a> on creating self-hosted integration run times it states this:</p>

<blockquote>
  <p>Azure Data Factory communicates with the self-hosted integration
  runtime to schedule and manage jobs. Communication is via a control
  channel that uses a shared Azure Service Bus Relay connection</p>
</blockquote>

<p>Basically Factory is saying to integration runtime ""Do you want me to do anything?"".  You do not have to provision or configure Service Bus so I wouldn't worry about it too much.</p>
"
"60867607","Ways to keep .csv format in azure data lake","<p>I am new to Azure and currently trying to design a pipeline where the first step is syncing a local directory (has multiple csvs) with an Azure Container using Azure Data Factory. Problem is Azure keeps changing my .csv files to .txt. Is there a workaround to this? I just need a one to one transfer of the files as is. </p>

<p>I should mention that when I manually upload my csv files to the lake, it's fine. The format change happens when I try to sync the local folder with a self hosted integration runtime.</p>

<p>Thank you! </p>
","<azure><azure-data-factory><azure-data-lake>","2020-03-26 12:45:07","113","0","1","60879327","<p>When we copy the csv file from local folder to Azure Data Lake, we can set the the <strong>File extension: .csv</strong>:</p>

<p><a href=""https://i.stack.imgur.com/V7dLA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V7dLA.png"" alt=""enter image description here""></a></p>

<p>This option will help you  keep the .csv  file format.</p>

<p>Hope this helps. </p>
"
"60865771","Azure DataFactory - Filter activity condition expression","<p>I am looking at a folder and filtering files based on condition to pass onto next activity. There are many files in the folder and have a filter activity:
Files I need to filter are in below format:
Extract_2020Mar20_ENV1_200.csv
Extract_2020Mar20_ENV2_200.csv</p>

<p>Condition I need to filter above files are - Files starting with name 'Extract' AND contain 'ENV'
How do I reflect that in my condition part of my filter activity:
I can do one condition but not two … 
@and(equals(item().type,'File'),startswith(item().name,'Extract'))</p>

<p>NB - I am only picking files from the folder.
Pls assist...</p>
","<azure><azure-data-factory>","2020-03-26 11:03:29","925","0","1","60868050","<p>You can use a wildcard characters (? for just one, and * for many) in your dataset (no need for expressions in the pipeline! helps to have a cleaner config) to reflect this, so in place of the file name you can type:</p>

<pre><code>Extract_*_ENV?_*.csv
</code></pre>

<p>And that dataset will only pick those files that match the condition in that folder.</p>

<p>This is just an example to get you the idea, if the number after ENV can get to 2 digits:</p>

<pre><code>Extract_*_ENV*_*.csv
</code></pre>

<p>Doc here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#copy-activity-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#copy-activity-properties</a></p>

<p>Hope this helped!</p>
"
"60865609","How can Azure Data Factory access a Custom Data Connector","<p>I've just started to look at Azure Data Factory as a possible way to get data we are currently consuming for Power BI via custom connectors, primarily to access Graph APIs. I can't see if the same data is available to Azure Data Factory. Is there any way to achieve this?</p>
","<powerbi><azure-data-factory><connector>","2020-03-26 10:53:15","1140","1","2","60867038","<p>Azure Data Factory has a number of different features which may help:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web activity</a> - call REST APIs from ADF pipeline; can only access public URLs</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity"" rel=""nofollow noreferrer"">Webhook activity</a> - call endpoints and pass callback URL</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure function</a> - run Azure functions in the pipeline; functions are very flexible so could probably do this</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">Custom activity via Azure Batch</a> - run .net via Azure Batch; very customisable</li>
<li>Databricks notebook - call a notebook written in Scala, Python, R, Java or SparkSQL - completely customisable</li>
</ul>

<p>Alternately look at <a href=""https://learn.microsoft.com/en-us/power-bi/service-dataflows-overview"" rel=""nofollow noreferrer"">Power BI Data Flows</a> which offers self-service ETL but remember the destination for your ""L"" is only really Azure Data Lake Gen 2 and Power BI Datasets.</p>
"
"60865609","How can Azure Data Factory access a Custom Data Connector","<p>I've just started to look at Azure Data Factory as a possible way to get data we are currently consuming for Power BI via custom connectors, primarily to access Graph APIs. I can't see if the same data is available to Azure Data Factory. Is there any way to achieve this?</p>
","<powerbi><azure-data-factory><connector>","2020-03-26 10:53:15","1140","1","2","62068282","<p>We decided to use Logic Apps, rather than Data Factory, which offer a convenient means to access Graph APIs, as Logic Apps support OAuth well i.e. we're not using Data Connectors any more</p>

<p>In addition, we put some of the more complicated logic into Stored Procedures, as Logic Apps, despite their name, can only handle basic logic</p>
"
"60859703","ADF copy data activity - check for duplicate records before inserting into SQL db","<p>I have a very simple ADF pipeline to copy data from local mongoDB (self-hosted integration environment) to Azure SQL database. </p>

<p>My pipleline is able to copy the data from mongoDB and insert into SQL db. 
Currently if I run the pipeline it inserts duplicate data if run multiple times. </p>

<p>I have made _id column as unique in SQL database and now running pipeline throws and error because of SQL constraint wont letting it insert the record. </p>

<p><strong>How do I check for duplicate _id before inserting into SQL db?</strong> </p>

<p>should I use Pre-copy script / stored procedure? 
Some guidance / directions would be helpful on where to add extra steps. Thanks  </p>
","<azure><azure-sql-database><azure-data-factory>","2020-03-26 01:17:05","6599","1","4","60861820","<p>You Should implement your SQL Logic to eliminate duplicate at the Pre-Copy Script<a href=""https://i.stack.imgur.com/qjPUX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qjPUX.jpg"" alt=""enter image description here""></a></p>
"
"60859703","ADF copy data activity - check for duplicate records before inserting into SQL db","<p>I have a very simple ADF pipeline to copy data from local mongoDB (self-hosted integration environment) to Azure SQL database. </p>

<p>My pipleline is able to copy the data from mongoDB and insert into SQL db. 
Currently if I run the pipeline it inserts duplicate data if run multiple times. </p>

<p>I have made _id column as unique in SQL database and now running pipeline throws and error because of SQL constraint wont letting it insert the record. </p>

<p><strong>How do I check for duplicate _id before inserting into SQL db?</strong> </p>

<p>should I use Pre-copy script / stored procedure? 
Some guidance / directions would be helpful on where to add extra steps. Thanks  </p>
","<azure><azure-sql-database><azure-data-factory>","2020-03-26 01:17:05","6599","1","4","60862159","<p>Azure Data Factory <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""noreferrer"">Data Flow</a> can help you achieve that:</p>

<p><a href=""https://i.stack.imgur.com/nfAeR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nfAeR.png"" alt=""enter image description here""></a></p>

<p>You can follow these steps:</p>

<ol>
<li>Add two sources: Cosmos db table(source1) and SQL database table(source2).</li>
<li><p>Using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join"" rel=""noreferrer"">Join</a> active to get all the data from two tables(left join/full join/right join) on Cosmos table.id= SQL table.id.
<a href=""https://i.stack.imgur.com/m8IP5.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/m8IP5.png"" alt=""enter image description here""></a> </p></li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row"" rel=""noreferrer"">AlterRow</a>  expression to filter the duplicate _id, it not duplicate then insert it.
<a href=""https://i.stack.imgur.com/hz0h1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hz0h1.png"" alt=""enter image description here""></a></p></li>
<li><p>Then mapping the no-duplicate column to the Sink SQL database table.</p></li>
</ol>

<p>Hope this helps.</p>
"
"60859703","ADF copy data activity - check for duplicate records before inserting into SQL db","<p>I have a very simple ADF pipeline to copy data from local mongoDB (self-hosted integration environment) to Azure SQL database. </p>

<p>My pipleline is able to copy the data from mongoDB and insert into SQL db. 
Currently if I run the pipeline it inserts duplicate data if run multiple times. </p>

<p>I have made _id column as unique in SQL database and now running pipeline throws and error because of SQL constraint wont letting it insert the record. </p>

<p><strong>How do I check for duplicate _id before inserting into SQL db?</strong> </p>

<p>should I use Pre-copy script / stored procedure? 
Some guidance / directions would be helpful on where to add extra steps. Thanks  </p>
","<azure><azure-sql-database><azure-data-factory>","2020-03-26 01:17:05","6599","1","4","60872200","<p>Currently I got the solution using a Stored Procedure which look like a lot less work as far this requirement is concerned. </p>

<p>I have followed this article:
<a href=""https://www.cathrinewilhelmsen.net/2019/12/16/copy-sql-server-data-azure-data-factory/"" rel=""nofollow noreferrer"">https://www.cathrinewilhelmsen.net/2019/12/16/copy-sql-server-data-azure-data-factory/</a></p>

<p>I created table type and used in stored procedure to check for duplicate. </p>

<p>my sproc is very simple as shown below: </p>

<pre><code>SET QUOTED_IDENTIFIER ON
GO
ALTER PROCEDURE [dbo].[spInsertIntoDb]
    (@sresults dbo.targetSensingResults READONLY)
AS

BEGIN

MERGE dbo.sensingresults AS target
USING @sresults AS source
ON (target._id = source._id)

WHEN NOT MATCHED THEN
    INSERT (_id, sensorNumber, applicationType, place, spaceType, floorCode, zoneCountNumber, presenceStatus, sensingTime, createdAt, updatedAt, _v)
    VALUES (source._id, source.sensorNumber, source.applicationType, source.place, source.spaceType, source.floorCode,
     source.zoneCountNumber, source.presenceStatus, source.sensingTime, source.createdAt, source.updatedAt, source.updatedAt);
END
</code></pre>

<p>I think using stored proc should do for and also will help in future if I need to do more transformation. </p>

<p>Please let me know if using sproc in this case has potential risk in future ? </p>
"
"60859703","ADF copy data activity - check for duplicate records before inserting into SQL db","<p>I have a very simple ADF pipeline to copy data from local mongoDB (self-hosted integration environment) to Azure SQL database. </p>

<p>My pipleline is able to copy the data from mongoDB and insert into SQL db. 
Currently if I run the pipeline it inserts duplicate data if run multiple times. </p>

<p>I have made _id column as unique in SQL database and now running pipeline throws and error because of SQL constraint wont letting it insert the record. </p>

<p><strong>How do I check for duplicate _id before inserting into SQL db?</strong> </p>

<p>should I use Pre-copy script / stored procedure? 
Some guidance / directions would be helpful on where to add extra steps. Thanks  </p>
","<azure><azure-sql-database><azure-data-factory>","2020-03-26 01:17:05","6599","1","4","60911100","<p>To remove the duplicates you can use the pre-copy script. OR what you can do is you can store the incremental or new data into a temp table using copy activity and use a store procedure to delete only those  Ids from the main table which are in temp table after deletion insert the temp table data into the main table. and then drop the temp table.</p>
"
"60858087","Azure Data Factory - Incremental Load to Azure Data Lake","<p>I want to have Incremental Load Pattern for a Source System where there is no Audit Fields which state when was the record last modified. Example: Lasted Modified on (date time)</p>

<p>But these tables are defined with Primary Keys and Unique Keys which are used by the application to updated the record when ever there is any any change in the attribute.</p>

<p>Now question is how can i determine Delta's every day and load them into Azure Data Lake using Azure Data Factory / Databricks.</p>

<p>Should I stage full set of data from current day and current day -1 and determine delta's by using hash values ?  </p>

<p>Or there is a better way?</p>
","<azure-data-factory><azure-databricks><delta-lake><azure-data-lake-gen2>","2020-03-25 22:09:00","1072","2","1","61110862","<p>As this database is not huge in size, ended up creating pipeline where it loads full dataset into sql staging and then writes back to Data Lake into relevant location for Initial Load Dataset and then promotes the sql staging to PreviousDay schema.  </p>

<p>There on for next incremental it reads full dataset into sql staging and then compares with PreviousDays dataset, get the changed records and writes to Data Lake into relevant incremental location.  Then drops the existing PreviousDay dataset and promote Staging dataset to previousDay so that is ready for next incremental.</p>
"
"60857350","Azure Data Factory Event Trigger - Storage Account Key in Json?","<p>we have a storage account that is locked down. My pipeline has connections that reference a key vault to get the access token for the storage account.</p>
<p>When I create an event trigger in ADF, ADF lets me find and connect to the storage account (without asking for a key or prompting me to select the linked service connection). It tells me what files it will include based on my begins with and ends with values (it found 2 files). It saves successfully.</p>
<p>When I publish it, I get this error in between publish to adf-publish and generating the arm templates.</p>
<hr />
<p>The attempt to configure storage notifications for the provided storage account ****failed. Please ensure that your storage account meets the requirements described at <a href=""https://aka.ms/storageevents"" rel=""nofollow noreferrer"">https://aka.ms/storageevents</a>. The error is Failed to retrieve credentials for request=RequestUri=https://management.azure.com/subscriptions/********/resourceGroups/&amp;lt;resource group name&amp;gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage account name here to gen 2 data lake&gt;/listAccountSas, Method=POST, response=StatusCode=400, StatusDescription=Bad Request, IsSuccessStatusCode=False, Content=System.Net.HttpWebResponse, responseContent={&amp;#34;error&amp;#34;:{&amp;#34;code&amp;#34;:&amp;#34;InvalidValuesForRequestParameters&amp;#34;,&amp;#34;message&amp;#34;:&amp;#34;Values for request parameters are invalid: keyToSign.&amp;#34;}}</p>
<hr />
<p>I believe this is due to the fact that ADF trigger creation process (and therefore its JSON) does not allow you to point to a Key Vault to get the access token for the storage account you are connecting to. Is this the issue? Is there a fix for this?</p>
<p>Appreciate any help, thanks - April</p>
","<azure><azure-data-factory><azure-eventgrid>","2020-03-25 21:02:54","598","0","2","63298638","<p>I think the storage account is attached to a VNET and running behind the firewall. I faced similar issue because of this. You may remove the firewall once and configure the trigger and then bring the firewall back.</p>
"
"60857350","Azure Data Factory Event Trigger - Storage Account Key in Json?","<p>we have a storage account that is locked down. My pipeline has connections that reference a key vault to get the access token for the storage account.</p>
<p>When I create an event trigger in ADF, ADF lets me find and connect to the storage account (without asking for a key or prompting me to select the linked service connection). It tells me what files it will include based on my begins with and ends with values (it found 2 files). It saves successfully.</p>
<p>When I publish it, I get this error in between publish to adf-publish and generating the arm templates.</p>
<hr />
<p>The attempt to configure storage notifications for the provided storage account ****failed. Please ensure that your storage account meets the requirements described at <a href=""https://aka.ms/storageevents"" rel=""nofollow noreferrer"">https://aka.ms/storageevents</a>. The error is Failed to retrieve credentials for request=RequestUri=https://management.azure.com/subscriptions/********/resourceGroups/&amp;lt;resource group name&amp;gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage account name here to gen 2 data lake&gt;/listAccountSas, Method=POST, response=StatusCode=400, StatusDescription=Bad Request, IsSuccessStatusCode=False, Content=System.Net.HttpWebResponse, responseContent={&amp;#34;error&amp;#34;:{&amp;#34;code&amp;#34;:&amp;#34;InvalidValuesForRequestParameters&amp;#34;,&amp;#34;message&amp;#34;:&amp;#34;Values for request parameters are invalid: keyToSign.&amp;#34;}}</p>
<hr />
<p>I believe this is due to the fact that ADF trigger creation process (and therefore its JSON) does not allow you to point to a Key Vault to get the access token for the storage account you are connecting to. Is this the issue? Is there a fix for this?</p>
<p>Appreciate any help, thanks - April</p>
","<azure><azure-data-factory><azure-eventgrid>","2020-03-25 21:02:54","598","0","2","75128995","<p>It's not strictly necessary to disable the firewall. You can also use this feature on your storage account.</p>
<p><a href=""https://i.stack.imgur.com/EC3J4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EC3J4.png"" alt=""Allow Azure services on the trusted services list to access this storage account"" /></a></p>
"
"60852549","Azure Data Factory - When enabled git n azure Data factory the Trigger Not Working","<p>I am not able to schedule a event trigger automatically when Git is enabled in azure data factory.
Manual trigger is working and the automatic trigger wouldn't work and not appear in monitor tab.</p>
<p><img src=""https://i.stack.imgur.com/5pHHC.png"" alt=""enter image description here"" /></p>
","<azure><azure-data-factory>","2020-03-25 15:59:53","171","0","1","63357816","<p>You need to publish the changes for the event trigger to be activated. I just tested without publishing and uploading file into blob, trigger did not work. After publishing, it worked. Option is as shown below.</p>
<p><a href=""https://i.stack.imgur.com/xxQVs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xxQVs.png"" alt=""enter image description here"" /></a></p>
"
"60852027","Can we use different run-time in Azure Data Factory v2 (ADFv2) Copy Activity?","<p>I have copy activity, where source is Oracle on premises connected through Self-hosted IR and destination is Microsoft Synapse connected via Azure Run-time. These run-time is defined in connections (Linked Services). </p>

<p>But while execution pipeline is using Self Hosted Run-time through-out and overriding the run-time of Azure Synapse. And because of that connection is failing.</p>

<p>Is this default behavior? Can't I run pipeline with 2 different run-time. </p>
","<azure-data-factory><azure-synapse>","2020-03-25 15:30:54","923","1","2","60855515","<p>Integration runtimes are defined at the linked service level.  So you should have a linked service definition for your Oracle database and a separate linked service definition for your Azure Synapse Analytics (formerly known as Azure SQL Data Warehouse).  So you can specify different integration runtimes, eg here's an example:</p>

<p><a href=""https://i.stack.imgur.com/rkRmk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rkRmk.png"" alt=""Synapse linked service""></a></p>

<p>NB Azure Synapse Analytics is using the Autoresolve runtime and does not need a self-hosted integration runtime (SHIR) as it is a native PaaS service.  Are you getting a specific error?  If so, please post details.</p>
"
"60852027","Can we use different run-time in Azure Data Factory v2 (ADFv2) Copy Activity?","<p>I have copy activity, where source is Oracle on premises connected through Self-hosted IR and destination is Microsoft Synapse connected via Azure Run-time. These run-time is defined in connections (Linked Services). </p>

<p>But while execution pipeline is using Self Hosted Run-time through-out and overriding the run-time of Azure Synapse. And because of that connection is failing.</p>

<p>Is this default behavior? Can't I run pipeline with 2 different run-time. </p>
","<azure-data-factory><azure-synapse>","2020-03-25 15:30:54","923","1","2","60860903","<p>Thanks @wBob but I am sorry that is not true, I found the answer at Microsoft documentation. </p>

<p><strong>Copying between a cloud data source and a data source in private network</strong>: if either source or sink linked service points to a self-hosted IR, the copy activity is executed on that self-hosted Integration Runtime.</p>

<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use</a></p>
"
"60848779","How to use Azure Powershell Datafactory cmdlets against specific branches in Azure DevOps repositories?","<p>I have a Datafactory in AzureDevOps Repos. 
<a href=""https://i.stack.imgur.com/eSWtT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eSWtT.png"" alt=""enter image description here""></a></p>

<p>I am trying to use Azure Powershell cmdlets to create a trigger, such as the following :<a href=""https://i.stack.imgur.com/2Sbq6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Sbq6.png"" alt=""enter image description here""></a></p>

<p>The problem is, when I run the above command, the changes are <strong>not showing</strong> on the Az DevOps Repo. So the trigger I just created does not appear in the Repo.  </p>

<p>As a workaround, I have been able to manually create JSON files in my local git repo > merge with local master > push to online master and then the trigger works just fine and the JSON file is also visible in the Az DevOps Repo.
<a href=""https://i.stack.imgur.com/0W2cw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0W2cw.png"" alt=""enter image description here""></a></p>

<p>But how to instruct the Powershell cmdlets to work on a specific branch? Because when I just use PS cmdlets, whatever is added (trigger/dataset/pipeline) does not show up online in the Azure DevOps Repo.  </p>

<p>So, question is, where do cmdlets like <strong>Set-AzDatafactorV2Trigger</strong> operate/make the changes and can that be changed to work against specific branches in a git repo?</p>
","<powershell><azure-powershell><azure-data-factory><powershell-cmdlet><azure-git-deployment>","2020-03-25 12:23:18","161","0","1","69180085","<p>I believe that when you run powershell commands against a Data Factory which is connected to git, the updates are made to the &quot;live mode&quot; of ADF and not the the   git version of the factory.</p>
<p>Have you tried to switch to &quot;live mode&quot; to see if your triggers are there:
<a href=""https://i.stack.imgur.com/dZEEK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dZEEK.png"" alt=""enter image description here"" /></a></p>
"
"60833019","Run cosmos scope script in ADF V2","<p>I want to run cosmos scope script in ADF pipeline.
Scope script takes input parameter </p>

<blockquote>
  <ol>
  <li>current date </li>
  <li>folder path.</li>
  </ol>
</blockquote>

<p>My challenge is how can I read and call the parameters in my scope script(I am using scope activity).</p>
","<azure><azure-data-factory><azure-cosmosdb-mongoapi>","2020-03-24 14:24:35","1116","0","2","63639992","<p>Add these 2 parameters as scope script parameters.
Once you load the script in ADF under the activity in advanced you should see those parameters.
Then you can pass in the static/dynamic values to these parameters from ADF scope activity.</p>
"
"60833019","Run cosmos scope script in ADF V2","<p>I want to run cosmos scope script in ADF pipeline.
Scope script takes input parameter </p>

<blockquote>
  <ol>
  <li>current date </li>
  <li>folder path.</li>
  </ol>
</blockquote>

<p>My challenge is how can I read and call the parameters in my scope script(I am using scope activity).</p>
","<azure><azure-data-factory><azure-cosmosdb-mongoapi>","2020-03-24 14:24:35","1116","0","2","76105543","<p>To read parameters into your scope script, insert these two lines in the .script file</p>
<pre><code>#DECLARE Path string = &quot;@@PathParameterName@@&quot;;
#DECLARE Date string = &quot;@@DateParameterName@@&quot;;
</code></pre>
"
"60823527","How to archive old CosmosDB data to Azure Table using Azure Data Factory when CosmosDB collection documents have different properties?","<p>I'm trying to archive old data from CosmosDB into Azure Tables but I'm very new to Azure Data Factory and I'm not sure what would be a good approach to do this. At first, I thought that this could be done with a Copy Activity but because the properties from my documents stored in the CosmosDB source vary, I'm getting mapping issues. Any idea on what would be a good approach to tackle this archiving process?</p>

<p>Basically, the way I want to store the data is to copy the document root properties as they are, and store the nested JSON as a serialized string.</p>

<p>For example, if I wanted to archive these 2 documents :</p>

<pre><code>[
  {
    ""identifier"": ""1st Guid here"",
    ""Contact"": {
      ""Name"":  ""John Doe"",
      ""Age"": 99
    }
  },
  { 
    ""identifier"": ""2nd Guid here"",
    ""Distributor"": {
       ""Name"": ""Jane Doe"",
       ""Phone"": {
         ""Number"": ""12345"",
         ""IsVerified"": true
       }
    }
  }
]
</code></pre>

<p>I'd like these documents to be stored in Azure Table like this:</p>

<pre><code>identifier      | Contact                                   | Distributor 
""Ist Guid here"" | ""{ \""Name\"": \""John Doe\"", \""Age\"": 99 }"" | null
""2nd Guid here"" |  null                                     | ""{\""Name\"":\""Jane Doe\"",\""Phone\"":{\""Number\"":\""12345\"",\""IsVerified\"":true}}""
</code></pre>

<p>Is this possible with the Copy Activity?</p>

<p>I tried using the mapping tab inside the CopyActivity, but when I try to run it I get an error saying that the dataType for one of the Nested JSON columns that are not present in the first row cannot be inferred.</p>
","<azure><azure-cosmosdb><azure-data-factory><azure-table-storage><backup-strategies>","2020-03-24 00:47:57","147","1","1","60823967","<p>Please follow my configuration in Mapping Tag.</p>

<p><a href=""https://i.stack.imgur.com/ayGnS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ayGnS.png"" alt=""enter image description here""></a></p>

<p>Test output with your sample data:</p>

<p><a href=""https://i.stack.imgur.com/LGqCA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LGqCA.png"" alt=""enter image description here""></a></p>
"
"60811806","Azure: Orchestration file transfers - Which Azure components are best suited?","<p>I have a project in which we need to transfer files (mostly SFTP-based, but also HTTP), between 20+ systems. We have currently identified +200 different files that needs to be transferred. We would like a setup in Azure where the different transfers can be setup, monitored and logged, however, we are unsure which way to go.</p>

<p><strong>The question is:</strong> Which Azure components would be best suited for the above task? Which components would you use?</p>

<p>One possible solution would be to implement at large set of Azure functions, each responsible for one file transfer. This would require us to setup the monitoring ourselves, and it will result in a very large number of functions.</p>

<p>We have also been looking towards Azure data factory and Azure Logic apps, but we are unsure if they would provide any benefits with regards to monitoring, re-running failed jobs etc.</p>
","<azure><azure-functions><azure-data-factory><azure-logic-apps><file-transfer>","2020-03-23 10:31:30","44","1","1","60826528","<p>As you already mentioned in your description,obviously, Azure Function is not suitable for your scenario because you have to build a large number of functions to do the transfer work.Moreover,it's painful to monitoring such scale of function executions.You need to distinguish the log data and persist them into table storage or something like that which causes more cost. So,it's passed!</p>

<p>In my opinion, ADF is the best solution for you.It could be monitored by <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">many ways</a> and it supports re-run feature,please follow  this <a href=""https://channel9.msdn.com/Shows/Azure-Friday/Rerun-activities-inside-your-Azure-Data-Factory-pipelines"" rel=""nofollow noreferrer"">video</a>.Also,another distinct feature for ADF is Self-Hosted Integration Runtime which supports transmission between on-premise system and Azure cloud environment.</p>

<p>As for Logic App,i'm can't find any re-run feature related to it so i don't think it could attract your attention.</p>
"
"60799702","How to create/start cluster from data bricks web activity by invoking databricks rest api","<p>I have 2 requirements:</p>

<p>1:I have a clusterID. I need to start the cluster from a ""Wb Activity"" in ADF. The activity parameters look like this:</p>

<pre><code>url:https://XXXX..azuredatabricks.net/api/2.0/clusters/start
body: {""cluster_id"":""0311-004310-cars577""}
Authentication: Azure Key Vault Client Certificate
</code></pre>

<p>Upon running this activity I am encountering with below error:</p>

<pre><code>""errorCode"": ""2108"",

""message"": ""Error calling the endpoint 
'https://xxxxx.azuredatabricks.net/api/2.0/clusters/start'. Response status code: ''. More 
 details:Exception message: 'Cannot find the requested object.\r\n'.\r\nNo response from the 
 endpoint. Possible causes: network connectivity, DNS failure, server certificate validation or 
timeout."",

""failureType"": ""UserError"",
""target"": ""GetADBToken"",
""GetADBToken"" is my activity name.
</code></pre>

<p>The above security mechanism is working for other Databricks related activity such a running jar which is already installed on my databricks cluster.</p>

<p>2: I want to create a new cluster with the below settings:</p>

<pre><code>url:https://XXXX..azuredatabricks.net/api/2.0/clusters/create

    body:{

      ""cluster_name"": ""my-cluster"",
      ""spark_version"": ""5.3.x-scala2.11"",
      ""node_type_id"": ""i3.xlarge"",
      ""spark_conf"": {
      ""spark.speculation"": true
  },
    ""num_workers"": 2
}
</code></pre>

<p>Upon calling this api, if a cluster creation is successful I would like to capture the cluster id in the next activity.</p>

<p>So what would be the output of the above activity and how can I access them in an immediate ADF activity?</p>
","<azure-data-factory><azure-databricks>","2020-03-22 13:08:54","556","0","1","60808950","<p>For #2 ) Can you please check if you change the version 
  ""spark_version"": ""5.3.x-scala2.11""
to 
""spark_version"": ""6.4.x-scala2.11"" </p>

<p>if that helps </p>
"
"60795937","How to increment a parameter in an Azure Data Factory Until Activity?","<p>I am accessing a RESTful API that pages results in groups of 50 using the HTTP connector.  The REST connector doesn't seem to support Client Certificates so I can't use the pagination in that.</p>

<p>I have a Pipeline Variable called <code>SkipIndex</code> that defaults to 0.  Inside the Until loop I have a Copy Data Activity that works (HTTP source to BLOB sink), then a Set Variable Activity that I am trying to get to increment this Variable.</p>

<pre><code>{
    ""name"": ""Add 50 to SkipIndex"",
    ""type"": ""SetVariable"",
    ""dependsOn"": [
        {
            ""activity"": ""Copy next to temp"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        }
    ],
    ""userProperties"": [],
    ""typeProperties"": {
        ""variableName"": ""SkipIndex"",
        ""value"": {
            ""value"": ""50++"",
            ""type"": ""Expression""
        }
    }
}
</code></pre>

<p>Everything I have tried results in errors such as ""The expression contains self referencing variable. A variable cannot reference itself in the expression."" and the one above with <code>50++</code> causes a sink error during debug.</p>

<p>How can I get the Until loop to increment this variable after it retrieves data?</p>
","<azure-data-factory>","2020-03-22 04:28:16","8121","7","1","60807124","<p>Agree that REST Connector does supports pagination but does not for <code>Client Certificates</code> Authentication type.</p>

<p>For the idea of your <code>Until activity</code> scenario,i am tripped by the <code>can't self-reference a variable in an expression</code> limitation also. Maybe you could make a little trick on that: Add one more variable to persist the index number.</p>

<p>For example,i got 2 variables: <code>count</code> and <code>indexValue</code></p>

<p>Until Activity:</p>

<p><a href=""https://i.stack.imgur.com/eHS6r.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eHS6r.png"" alt=""enter image description here""></a></p>

<p>Inside Until Activity:</p>

<p><a href=""https://i.stack.imgur.com/B4ceP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/B4ceP.png"" alt=""enter image description here""></a></p>

<p>V1:</p>

<p><a href=""https://i.stack.imgur.com/LEOnF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LEOnF.png"" alt=""enter image description here""></a></p>

<p>V2:</p>

<p><a href=""https://i.stack.imgur.com/LusRm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LusRm.png"" alt=""enter image description here""></a></p>

<p>BTW, no usage of <code>50++</code> in ADF.</p>
"
"60795887","How to use Certificate Authentication with the REST connector in Azure Data Factory?","<p>I'd like to use some of the more advanced features of the REST Dataset in Azure Data Factory (mainly pagination), but I can't see where to use Certificate Authentication.  I have an HTTP Dataset working well with Certificate Authentication, but the only options seem to be:</p>

<ul>
<li>Anonymous</li>
<li>Basic</li>
<li>AAD Service Principal</li>
<li>Managed Identity</li>
</ul>

<p>Is there some way to use the AAD Service Principal or Managed Identity with a certificate?  Maybe I can do this using the Advanced settings?</p>

<p>The docs say that the HTTP connector is ""less functional comparing to REST connector"" but this seems to be a pretty big omission from the REST version.</p>
","<azure-data-factory>","2020-03-22 04:17:47","600","0","1","60809631","<blockquote>
  <p>Is there some way to use the AAD Service Principal or Managed Identity
  with a certificate? Maybe I can do this using the Advanced settings?</p>
</blockquote>

<p>SP and MI can't be applied for the certificate authentication, for AAD instead. please refer to this simple <a href=""https://thecloudhub.com/2019/03/whats-an-azure-service-principal-and-managed-identity/"" rel=""nofollow noreferrer"">blog</a>.</p>

<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">REST Connector document</a>,it only supports above 4 types of authenticate options. If you want to carry certificate with request,please use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">HTTP</a> connector.</p>
"
"60785718","Azure data factory query","<p>Case:- there is one URL in which I need to select one input from the dropdown, another input from the dropdown to convert some data and one more input to select data in CSV format and then click submit button, the output is generated in form of CSV so I need to automate this process by ADF. </p>

<p>What I have done:-I am using web activity with the copy activity to load URL data using the POST method into the blob. But the output I am getting is full HTTP response of page. However, I have created also parameters for mapping but it didn't work.what would be the solution of getting only these parameters to select data in URL and get only selected values output?</p>
","<azure><azure-data-factory>","2020-03-21 08:11:29","536","0","1","60804337","<p>Data Factory is not the best for web scraping, that's not its function in the technology stack.</p>

<p>What I usually do when I have to get data from a website, is an Azure Function activity. You can create an Azure Function in a language of your liking (python, .net, java, powershell, node, etc), do the web scraping and store whatever data you want in a blob storage. Then, call the function from data factory with the Azure Function Activity.</p>

<p>Hope this helped!</p>
"
"60780682","Trigger option not working with parameter pipeline where pipeline running successfully in debug mode","<p>I have created parameterized pipeline in Azure data factory.
This pipeline runs fine if i run by clicking on debug.
But it is failing by executing through trigger option. it is not recognizing source database. Throwing below error.</p>

<blockquote>
  <p>Error</p>
  
  <p>Operation on target Get Table And Procedure List failed: Failure
  happened on 'Source' side.
  ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
  database operation failed with the following error: 'Invalid object
  name
  'stg.controlTableLoop'.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Invalid
  object name 'stg.controlTableLoop'.,Source=.Net SqlClient Data
  Provider,SqlErrorNumber=208,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=208,State=1,Message=Invalid
  object name 'stg.controlTableLoop'.,},],'</p>
</blockquote>

<p>Trigger option not working with parameter pipeline where pipeline running successfully in debug mode</p>
","<azure><azure-data-factory>","2020-03-20 19:43:20","616","0","1","60841721","<p>Just for summary:</p>

<p>It is some kind of refresh issue as Data Factory link service was point to wrong secret name where as local DevOps branch was point to correct secret name. As trigger always point to data factory ARM template JSON, Trigger option was failing with data source issue. </p>

<p>Re-publish from Git branch resolve this issue.</p>
"
"60772569","Azure Data Factory Pipeline to copy multiple tables fails","<p>My first Post, Sorry in advance if i'm doing anything wrong</p>

<p>I've followed the instructions per the Microsoft Document below and created a data factory to copy multiple tables' data from an On-Prem database to Azure database.</p>

<p><a href=""http://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy"" rel=""nofollow noreferrer"">here</a></p>

<p>The Pipeline fails with the following error : </p>

<pre><code>PipelineName      : GetTableListAndTriggerCopyData
LastUpdated       : 3/20/2020 10:16:42 AM
Parameters        : {}
RunStart          : 3/20/2020 10:16:10 AM
RunEnd            : 3/20/2020 10:16:42 AM
DurationInMs      : 32430
Status            : Failed
Message           : Operation on target TriggerCopy failed:Operation on target IterateSQLTables failed: Activity failed because an inner activity failed
</code></pre>

<p>Json data for both pipelines are as below:</p>

<p><strong><em>1st Pipeline IterateAndCopySQLTables</em></strong></p>

<pre><code>{
""name"": ""IterateAndCopySQLTables"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""IterateSQLTables"",
            ""type"": ""ForEach"",
            ""typeProperties"": {
                ""isSequential"": ""false"",
                ""items"": {
                    ""value"": ""@pipeline().parameters.tableList"",
                    ""type"": ""Expression""
                },
                ""activities"": [
                    {
                        ""name"": ""CopyData"",
                        ""description"": ""Copy data from On Prem SQL database to Azure SQL database"",
                        ""type"": ""Copy"",
                        ""inputs"": [
                            {
                                ""referenceName"": ""OnPremSourceDataset"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""AzureSinkDataset"",
                                ""type"": ""DatasetReference"",
                                ""parameters"": {
                                    ""SinkTableName"": ""[@{item().TABLE_NAME}]""
                                }
                            }
                        ],
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""SqlSource"",
                                ""sqlReaderQuery"": ""SELECT * FROM dbo.[@{item().TABLE_NAME}]""
                            },
                            ""sink"": {
                                ""type"": ""AzureSqlSink"",
                                ""preCopyScript"": ""TRUNCATE TABLE dbo.[@{item().TABLE_NAME}]""
                            }

                        }
                    }
                ]
            }
        }
    ],
    ""parameters"": {
        ""tableList"": {
            ""type"": ""Array""
        }
    }
}
}
</code></pre>

<p><strong><em>2nd PipeLine : GetTableListAndTriggerCopyData</em></strong></p>

<pre><code>{
""name"":""GetTableListAndTriggerCopyData"",
""properties"":{
    ""activities"":[
        { 
            ""name"": ""LookupTableList"",
            ""description"": ""Retrieve the table list from On Prem SQL database"",
            ""type"": ""Lookup"",
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlSource"",
                    ""sqlReaderQuery"": ""SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_SCHEMA = 'dbo' and TABLE_NAME in ('Brd','Loc')""
                },
                ""dataset"": {
                    ""referenceName"": ""OnPremSourceDataset"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""TriggerCopy"",
            ""type"": ""ExecutePipeline"",
            ""typeProperties"": {
                ""parameters"": {
                    ""tableList"": {
                        ""value"": ""@array(activity('LookupTableList').output.value)"",
                        ""type"": ""Expression""
                    }
                },
                ""pipeline"": {
                    ""referenceName"": ""IterateAndCopySQLTables"",
                    ""type"": ""PipelineReference""
                },
                ""waitOnCompletion"": true
            },
            ""dependsOn"": [
                {
                    ""activity"": ""LookupTableList"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ]
        }
    ]
}
}
</code></pre>
","<database><azure><azure-pipelines><azure-data-factory>","2020-03-20 10:40:57","479","1","1","62123907","<p>Thanks all for ur comments.</p>

<p>The issue was in (GetTableListAndTriggerCopyData)</p>

<p>""value"": ""@array(activity('LookupTableList').output.value)"",
 Is wrong, the correct syntax is :
""value"": ""@activity('LookupTableList').output.value)"",</p>
"
"60770028","Is there any way to copy incremental data from SQL Server to Blob Storage Through Azure Data Factory?","<p>I am trying to copy incremental data from Azure SQL to Blob Storage but it overwrite all the existing data in every file. Can you provide me any work around for this problem</p>
","<azure><azure-data-factory><azure-blob-storage><azure-sql-server>","2020-03-20 07:20:07","287","0","2","60770747","<p>Your pipeline will need to only capture <strong>'delta' data</strong> rather than all data.
It will require a number of lookup activities, copy activity which only copies delta data and a stored proc activity for updating the last modified date (to be used for future copying)</p>

<p>Check out:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-powershell</a></p>
"
"60770028","Is there any way to copy incremental data from SQL Server to Blob Storage Through Azure Data Factory?","<p>I am trying to copy incremental data from Azure SQL to Blob Storage but it overwrite all the existing data in every file. Can you provide me any work around for this problem</p>
","<azure><azure-data-factory><azure-blob-storage><azure-sql-server>","2020-03-20 07:20:07","287","0","2","61074765","<p>Create new file in each delta load and after that append delta file data to original file. For append file data you use this link <a href=""https://stackoverflow.com/questions/46416808/how-to-append-a-text-file-in-an-azure-blob-with-a-azure-function"">How to Append a Text File in an Azure Blob with a Azure Function</a>.</p>
"
"60762851","In azure data factory copy activity ,fault tolerance is not applicable to check constraint","<p>I want to copy data from one table to another in azure data factory using copy activity.In source there are no constraints but on sink I have check constraint on age column  --->  check(age>=18). What I have observed is,if while copy activity runs to copy data from source to sink if even one record fails due to check constraint the entire copy activity fails and it will not copy any row to sink even though i have given option to skip and log error records.</p>

<p>Please give me solution to load all good records to sink table and all records that fail due to check constraint should be skipped and logged somewhere. </p>
","<azure><stored-procedures><fault-tolerance><azure-data-factory>","2020-03-19 18:09:47","402","0","1","60768603","<p>Try the <code>Skip and log incompatible rows</code> option for <code>Fault Tolerance</code></p>

<p><a href=""https://i.stack.imgur.com/dqf9v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dqf9v.png"" alt=""enter image description here""></a></p>
"
"60759049","Using dynamic data source name in Lookup Transformation inside Azure Data Flows","<p>Inside my data pipeline, I am trying to merge two datasets, a data from SQL table and a corresponding json file in Azure Blob Storage. 
Each row in SQL data contains a name of blob file, what I am trying to achieve is, for each row get the file mentioned in of the columns, join that json blob data with row and write to another blob.</p>

<p>First i tried using <code>Lookup</code> and <code>ForEach</code> activities inside <code>Data Pipelines</code>, parameterised blob data set to read the name based on value in column but then I didn't find any way to write each data along with blob data, it did copied the blob file though.</p>

<p>Then I tried using <code>Lookup Transformation</code> inside <code>Data Flows</code>, but i failed to use column value as the name of a second source to merge data from.</p>

<p>Is there anyway inside data pipeline or data flows to merge two datasets where the name of one of the dataset is dynamic?</p>
","<azure><azure-data-factory>","2020-03-19 14:19:21","313","0","1","60781387","<p>Yes, you can do this in Data Flows. The dataset, filenames, tables, and columns can all be parameterized.</p>
"
"60758944","Importing ARM template for creating ADF resources not creating any","<p>I am new to ADF &amp; ARM. I have a blank Data Factory-v2(TestDataFactory-123Test) which I want to get it populated using an existing ADF(TestDataFactory-123). I followed step by step what is mentioned in the official documentation <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#create-a-resource-manager-template-for-each-environment"" rel=""nofollow noreferrer"">Create a Resource Manager template for each environment</a>. The deployment shows succeeded but I can't see anything in it. I used 'Build your own template in the editor' option in the portal for importing the existing ARM template. Am I missing anything?</p>

<p>Below is the ARM which I got by 'exporting' the ARM for TestDataFactory-123:</p>

<pre><code>{
  ""$schema"": ""http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""parameters"": {
    ""factoryName"": {
      ""type"": ""string"",
      ""metadata"": ""Data Factory name"",
      ""defaultValue"": ""TestDataFactory-123""
    },
    ""AzureBlobStorageLinkedService_connectionString"": {
      ""type"": ""secureString"",
      ""metadata"": ""Secure string for 'connectionString' of 'AzureBlobStorageLinkedService'"",
      ""defaultValue"": ""TestDataFactory-123""
    }
  },
  ""variables"": {
    ""factoryId"": ""[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]""
  },
  ""resources"": [
    {
      ""name"": ""[concat(parameters('factoryName'), '/AzureBlobStorageLinkedService')]"",
      ""type"": ""Microsoft.DataFactory/factories/linkedServices"",
      ""apiVersion"": ""2018-06-01"",
      ""properties"": {
        ""annotations"": [],
        ""type"": ""AzureBlobStorage"",
        ""typeProperties"": {
          ""connectionString"": ""[parameters('AzureBlobStorageLinkedService_connectionString')]""
        }
      },
      ""dependsOn"": []
    },
    {
      ""name"": ""[concat(parameters('factoryName'), '/InputDataset')]"",
      ""type"": ""Microsoft.DataFactory/factories/datasets"",
      ""apiVersion"": ""2018-06-01"",
      ""properties"": {
        ""linkedServiceName"": {
          ""referenceName"": ""AzureBlobStorageLinkedService"",
          ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""Binary"",
        ""typeProperties"": {
          ""location"": {
            ""type"": ""AzureBlobStorageLocation"",
            ""fileName"": ""emp.txt"",
            ""folderPath"": ""input"",
            ""container"": ""adftutorial""
          }
        }
      },
      ""dependsOn"": [
        ""[concat(variables('factoryId'), '/linkedServices/AzureBlobStorageLinkedService')]""
      ]
    },
    {
      ""name"": ""[concat(parameters('factoryName'), '/OutputDataset')]"",
      ""type"": ""Microsoft.DataFactory/factories/datasets"",
      ""apiVersion"": ""2018-06-01"",
      ""properties"": {
        ""linkedServiceName"": {
          ""referenceName"": ""AzureBlobStorageLinkedService"",
          ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""Binary"",
        ""typeProperties"": {
          ""location"": {
            ""type"": ""AzureBlobStorageLocation"",
            ""folderPath"": ""output"",
            ""container"": ""adftutorial""
          }
        }
      },
      ""dependsOn"": [
        ""[concat(variables('factoryId'), '/linkedServices/AzureBlobStorageLinkedService')]""
      ]
    },
    {
      ""name"": ""[concat(parameters('factoryName'), '/CopyPipeline')]"",
      ""type"": ""Microsoft.DataFactory/factories/pipelines"",
      ""apiVersion"": ""2018-06-01"",
      ""properties"": {
        ""activities"": [
          {
            ""name"": ""CopyFromBlobToBlob"",
            ""type"": ""Copy"",
            ""dependsOn"": [],
            ""policy"": {
              ""timeout"": ""7.00:00:00"",
              ""retry"": 0,
              ""retryIntervalInSeconds"": 30,
              ""secureOutput"": false,
              ""secureInput"": false
            },
            ""userProperties"": [],
            ""typeProperties"": {
              ""source"": {
                ""type"": ""BinarySource"",
                ""storeSettings"": {
                  ""type"": ""AzureBlobStorageReadSettings"",
                  ""recursive"": true
                }
              },
              ""sink"": {
                ""type"": ""BinarySink"",
                ""storeSettings"": {
                  ""type"": ""AzureBlobStorageWriteSettings""
                }
              },
              ""enableStaging"": false
            },
            ""inputs"": [
              {
                ""referenceName"": ""InputDataset"",
                ""type"": ""DatasetReference"",
                ""parameters"": {}
              }
            ],
            ""outputs"": [
              {
                ""referenceName"": ""OutputDataset"",
                ""type"": ""DatasetReference"",
                ""parameters"": {}
              }
            ]
          }
        ],
        ""annotations"": []
      },
      ""dependsOn"": [
        ""[concat(variables('factoryId'), '/datasets/InputDataset')]"",
        ""[concat(variables('factoryId'), '/datasets/OutputDataset')]""
      ]
    }
  ]
}
</code></pre>
","<azure-data-factory>","2020-03-19 14:12:20","596","0","1","60771463","<p>The fix was as simple as replacing the 'defaultValue' for the 'factoryName' parameter with the name of the empty data factory viz. 'TestDataFactory-123Test' and not the existing one 'TestDataFactory-123'! Also, I replaced the 'defaultValue' of the 'AzureBlobStorageLinkedService_connectionString' parameter with the actual connection string. </p>
"
"60757063","Execute egg directly from Azure Data Factory","<h2>Question</h2>

<p><em>How to execute egg file from Azure Data Factory (AD) pipeline?</em> Currently I'm able only to call Databricks notebook from where executing egg file. Any way to do that directly?</p>

<h2>What have been done</h2>

<p>Following <a href=""https://stackoverflow.com/questions/57765991/how-to-run-python-egg-present-in-azure-databricks-from-azure-data-factory/58057198#58057198"">this answer</a>,  I got the following exception: </p>

<pre><code>{
    ""errorCode"": ""3201"",
    ""message"": ""Must specify one jar or maven library for jar task, either via jar_uri or libraries."",
    ""failureType"": ""UserError"",
    ""target"": ""Execute Egg"",
    ""details"": []
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/QdEGAl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QdEGAl.png"" alt=""enter image description here""></a></p>

<h2>Code and structure</h2>

<p>On my local machine I can execute <code>python dist/hello_world-1.0-py2.7.egg</code>, that will print <code>'Hello world!'</code></p>

<pre><code>src
 |-__init__.py
 |-main.py
__main__.py
setup.py
</code></pre>

<p><em>setup.py</em></p>

<pre><code>from setuptools import setup, find_packages

setup(
    name='hello-world',
    version='1.0',
    packages=find_packages(),
    py_modules=['__main__']
)
</code></pre>

<p><em>__main_ _.py</em></p>

<pre><code>from src.main import run

if __name__ == '__main__':
    run()
</code></pre>

<p><em>src/main.py</em></p>

<pre><code>def run():
    print('Hello world!')


if __name__ == '__main__':
    run()
</code></pre>
","<python><azure><azure-data-factory><azure-databricks><egg>","2020-03-19 12:15:41","515","5","1","62869866","<p>It seems you selected Jar activity in Azure Data Factory, instead of Python activity.</p>
<p><a href=""https://i.stack.imgur.com/xIrhl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xIrhl.png"" alt=""Databricks activities in Azure Data Factory"" /></a></p>
<p>In the Jar activity, the &quot;Main class name&quot; expects full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library.</p>
<p>If you select Python activity, you can specify Python file name and upload your egg library.</p>
<p><a href=""https://i.stack.imgur.com/WwG0o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WwG0o.png"" alt=""enter image description here"" /></a></p>
<p>You can find more details about it here:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python</a></p>
"
"60754994","Dynamic SQL to Load azure copy data activity","<p>I am trying to create a dynamic query in copy data activity to extract data based on a timestamp from a table. The lookup also gets the right value</p>

<p>I have tried</p>

<ol>
<li><p>select * from [dbo].t_tablename where last_changed_date >
@{formatDateTime(activity('Lookup_Get Last changed
date').output.firstRow.lastRecordTimeStamp,'yyyy-MM-ddTHH:mm:ssZ')}</p></li>
<li><p>select * from [dbo].t_tablename where last_changed_date >
@{activity('Lookup_Get Last changed
date').output.firstRow.lastRecordTimeStamp}</p></li>
<li><p>select * from [dbo].t_tablename where last_changed_date >
@activity('Lookup_Get Last changed
date').output.firstRow.lastRecordTimeStamp</p></li>
</ol>

<p>and other options but it always gives an ERROR 2200 which is a syntax error.</p>

<p>Can someone suggest what is the approach?</p>

<p>if i try this(It works)
select * from [dbo].t_tablename where last_changed_date >'2019-11-27 16:17:00.000'</p>
","<azure><azure-pipelines><azure-data-factory><azure-data-explorer>","2020-03-19 10:13:53","2178","1","2","60758815","<p>Is the syntax error coming from SQL or from ADF?</p>

<p>Assuming it is SQL, when building the statement dynamically, you still need 's (quotes) around the where clause value:</p>

<pre><code>select * from [dbo].t_tablename where last_changed_date &gt; **'**@{formatDateTime(activity('Lookup_Get Last changed date').output.firstRow.lastRecordTimeStamp,'yyyy-MM-ddTHH:mm:ssZ')}**'**
</code></pre>

<p>If it is ADF, try building the value in a concat expression:</p>

<pre><code>@concat('select * from [dbo].t_tablename where last_changed_date &gt; ''', activity('Lookup_Get Last changed date').output.firstRow.lastRecordTimeStamp, '''')
</code></pre>

<p>In either case, you need quotes around the value.</p>
"
"60754994","Dynamic SQL to Load azure copy data activity","<p>I am trying to create a dynamic query in copy data activity to extract data based on a timestamp from a table. The lookup also gets the right value</p>

<p>I have tried</p>

<ol>
<li><p>select * from [dbo].t_tablename where last_changed_date >
@{formatDateTime(activity('Lookup_Get Last changed
date').output.firstRow.lastRecordTimeStamp,'yyyy-MM-ddTHH:mm:ssZ')}</p></li>
<li><p>select * from [dbo].t_tablename where last_changed_date >
@{activity('Lookup_Get Last changed
date').output.firstRow.lastRecordTimeStamp}</p></li>
<li><p>select * from [dbo].t_tablename where last_changed_date >
@activity('Lookup_Get Last changed
date').output.firstRow.lastRecordTimeStamp</p></li>
</ol>

<p>and other options but it always gives an ERROR 2200 which is a syntax error.</p>

<p>Can someone suggest what is the approach?</p>

<p>if i try this(It works)
select * from [dbo].t_tablename where last_changed_date >'2019-11-27 16:17:00.000'</p>
","<azure><azure-pipelines><azure-data-factory><azure-data-explorer>","2020-03-19 10:13:53","2178","1","2","60761815","<p>select * from [dbo].t_tablename where last_changed_date > '@{activity('Lookup_Get Last changed date').output.firstRow.lastRecordTimeStamp}'</p>

<p>The above  code is  working now...as it is a datetime we have to include the activity in between ' '</p>
"
"60752181","Azure ADF - Array elements can only be selected using an integer index","<p>Hi I am trying to select  Status from Json Array in azure data factory</p>

<pre><code>{
    ""dataRead"": 2997,
    ""dataWritten"": 2714,
    ""filesWritten"": 1,
    ""sourcePeakConnections"": 1,
    ""sinkPeakConnections"": 1,
    ""rowsRead"": 11,
    ""rowsCopied"": 11,
    ""copyDuration"": 3,
    ""throughput"": 0.976,
    ""errors"": [],
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (East US)"",
    ""usedDataIntegrationUnits"": 4,
    ""billingReference"": {
        ""activityType"": ""DataMovement"",
        ""billableDuration"": [
            {
                ""meterType"": ""AzureIR"",
                ""duration"": 0.06666666666666667,
                ""unit"": ""DIUHours""
            }
        ]
    },
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""AzureSqlDatabase"",
                ""region"": ""East US""
            },
            ""sink"": {
                ""type"": ""AzureBlobStorage"",
                ""region"": ""East US""
            },
            ""status"": ""Succeeded"",
            ""start"": ""2020-03-19T06:24:39.0666585Z"",
            ""duration"": 3,
            ""usedDataIntegrationUnits"": 4,
            ""usedParallelCopies"": 1,
</code></pre>

<p>I have tried selecting <code>@activity('Copy data From CCP TO Blob').output.executionDetails.status</code>.It throws an error:</p>

<blockquote>
  <p>'Array elements can only be selected using an integer index'.</p>
</blockquote>

<p>Any way to resolve it?</p>
","<json><azure><parameters><azure-pipelines><azure-data-factory>","2020-03-19 06:51:00","15051","3","2","60752419","<p><code>executionDetails</code> is an array, you have to set index to refer elements in it.</p>

<p>Please try:</p>

<pre><code>@activity('Copy data From CCP TO Blob').output.executionDetails[0].status
</code></pre>
"
"60752181","Azure ADF - Array elements can only be selected using an integer index","<p>Hi I am trying to select  Status from Json Array in azure data factory</p>

<pre><code>{
    ""dataRead"": 2997,
    ""dataWritten"": 2714,
    ""filesWritten"": 1,
    ""sourcePeakConnections"": 1,
    ""sinkPeakConnections"": 1,
    ""rowsRead"": 11,
    ""rowsCopied"": 11,
    ""copyDuration"": 3,
    ""throughput"": 0.976,
    ""errors"": [],
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (East US)"",
    ""usedDataIntegrationUnits"": 4,
    ""billingReference"": {
        ""activityType"": ""DataMovement"",
        ""billableDuration"": [
            {
                ""meterType"": ""AzureIR"",
                ""duration"": 0.06666666666666667,
                ""unit"": ""DIUHours""
            }
        ]
    },
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""AzureSqlDatabase"",
                ""region"": ""East US""
            },
            ""sink"": {
                ""type"": ""AzureBlobStorage"",
                ""region"": ""East US""
            },
            ""status"": ""Succeeded"",
            ""start"": ""2020-03-19T06:24:39.0666585Z"",
            ""duration"": 3,
            ""usedDataIntegrationUnits"": 4,
            ""usedParallelCopies"": 1,
</code></pre>

<p>I have tried selecting <code>@activity('Copy data From CCP TO Blob').output.executionDetails.status</code>.It throws an error:</p>

<blockquote>
  <p>'Array elements can only be selected using an integer index'.</p>
</blockquote>

<p>Any way to resolve it?</p>
","<json><azure><parameters><azure-pipelines><azure-data-factory>","2020-03-19 06:51:00","15051","3","2","60752770","<p>Thank you for the reply</p>

<p>Yes, we have to use slicing and indexing the lists and Dictionaries</p>

<p>I have tried Dispensing_Unit_Master_Dim</p>

<p>@activity('Copy data From CCP TO Blob').output.executionDetails[0]['status'] and it works</p>

<p>0 and status there is no Dot</p>
"
"60743264","How to Dynamically adding HTTP endpoint to load data into azure data lake by using Azure Data Factory and the REST api is cookie autheticated","<p>I am trying to dynamically add/update linked service REST based on certain trigger/events to consume a RESP API to be authenticated using cookie which provides telemetry data. This telemetry data will be stored in Data Lake Gen2 and then will use Data Bricks to move to secondary data storage/SQL Server.</p>

<p>Have someone tried this? I am not able to find the cookie based Auth option while adding the linked service REST.</p>

<p>Also how to create data pipes dynamically or to have the parameters of the rest api to be dynamic ?</p>
","<azure><rest><session-cookies><azure-data-factory><data-pipeline>","2020-03-18 16:03:37","547","0","1","63790819","<p>Currently, unfortunately this is not possible using Azure data factory native components/activities. For now at least, you cannot get access to the response cookies from a web request in data factory. Someone has put a feature request for this or something that might help, see <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/35857762-support-response-headers-and-cookies-as-output-fro"" rel=""nofollow noreferrer"">here</a></p>
<p>It might be possible to do this via an Azure function to get/save the cookie and then send it as part of a following request. I had a similar problem but resorted to using Azure functions for all of it, but I guess you could just do the authentication part with a function! ;-)</p>
<p><strong>EDIT: update</strong></p>
<p>Actually, after I wrote this I went back to check if this was still the case and looks like things have changed. There now appears (never seen this before) in the web response output, a property called &quot;ADFWebActivityResponseHeaders&quot; and as you can see there is property for the &quot;Set-Cookie&quot;</p>
<p>See example below:-</p>
<p><a href=""https://i.stack.imgur.com/zUjsm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zUjsm.png"" alt=""enter image description here"" /></a></p>
"
"60743012","Azure Devps: How to specify Data Factory Parameters for Pipeline and Link Services","<p>I am trying to build a release pipeline for azure data factory. I have parameterized link service like database with username and server-name. Also, I have specified some pipeline parameters.</p>

<p>In azure devops release pipeline, these parameters do not appear in template parameters list. However link services connection string uses them which cause the deployment to fail.</p>

<p>The SQl connection string format is: Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=@{linkedService().SQLDBServer};Initial Catalog=TESTDB;User ID=@{linkedService().SQLDBUserName}</p>

<p>Now, the deployment throws exception for SQLDBServer and SQLDBuserName</p>

<p>My question is that how to I specify/replace pipeline or link-services connection parameters in release pipeline? </p>
","<azure-devops><azure-data-factory>","2020-03-18 15:47:45","143","0","1","60747875","<p>I found that using arm-template-parameters-definition.json file I can use parameters defined in pipeline.</p>
"
"60739834","You are not allowed to make changes or publish from 'Data Factory' mode as your factory has GIT enabled","<p>I am facing a issue. </p>

<p>I have a data factory with 20 piplines and dataset and linked services I enabled git with name xyz project and created adf-publish branch in that ,I worked almost 1 week in adf-publish branch. After one week my client is saying we have created new azure devops project with xyz1 prroject name.Now my changes is in adf-publish branch which comes under xyz project.</p>

<p>My question is that how can I save changes again ADF since I tried but I got a error 
You are not allowed to make changes or publish from 'Data Factory' mode as your factory has GIT enabled.</p>

<p>How can I populated the adf_publish branch changes to my adf -- then I will delete the adfpublish branch -- and again setup the repos to new project xyz1. </p>

<p>As u can see in this images I tried to change in my ADF but not able to publish it.</p>

<p><a href=""https://i.stack.imgur.com/1z8tn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1z8tn.png"" alt=""enter image description here""></a></p>

<p>Thank
Anuj</p>
","<deployment><continuous-integration><azure-data-factory><paas>","2020-03-18 12:42:12","7801","1","2","60741547","<p>I'm not sure I understand your scenario properly, but you will be able to commit directly to adf if you remove git from it.</p>

<p>To do that, go to the Data Factory overview and click on Git repo settings:</p>

<p><a href=""https://i.stack.imgur.com/hEusC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hEusC.png"" alt=""enter image description here""></a></p>

<p>Then just click on Remove Git, at the bottom left of the new window that pops up.</p>

<p>After that you will be able to save changes directly, and also attach another git repo if thats what you need.</p>

<p>Hope this helped!</p>
"
"60739834","You are not allowed to make changes or publish from 'Data Factory' mode as your factory has GIT enabled","<p>I am facing a issue. </p>

<p>I have a data factory with 20 piplines and dataset and linked services I enabled git with name xyz project and created adf-publish branch in that ,I worked almost 1 week in adf-publish branch. After one week my client is saying we have created new azure devops project with xyz1 prroject name.Now my changes is in adf-publish branch which comes under xyz project.</p>

<p>My question is that how can I save changes again ADF since I tried but I got a error 
You are not allowed to make changes or publish from 'Data Factory' mode as your factory has GIT enabled.</p>

<p>How can I populated the adf_publish branch changes to my adf -- then I will delete the adfpublish branch -- and again setup the repos to new project xyz1. </p>

<p>As u can see in this images I tried to change in my ADF but not able to publish it.</p>

<p><a href=""https://i.stack.imgur.com/1z8tn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1z8tn.png"" alt=""enter image description here""></a></p>

<p>Thank
Anuj</p>
","<deployment><continuous-integration><azure-data-factory><paas>","2020-03-18 12:42:12","7801","1","2","73117958","<p>In ADF, click on <strong>Manage</strong> tab -&gt; Git configuration -&gt; click on <strong>disconnect</strong></p>
<p><a href=""https://i.stack.imgur.com/FXHZC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FXHZC.png"" alt=""enter image description here"" /></a></p>
"
"60736253","Azure Data Factory copy from folder onwards","<p>I am trying to create a copy activity between two Azure Data Lakes GEN1. I don't need to copy all the folders from the source Data Lake, for example if I have the following directory structure:</p>

<pre><code>rootFolder/subfolder/2015
rootFolder/subfolder/2016
rootFolder/subfolder/2017
rootFolder/subfolder/2018
rootFolder/subfolder/2019
rootFolder/subfolder/2020
</code></pre>

<p>I would just want to copy the data from folders from 2017 onwards.</p>

<p>Is there a way to implement this automatically without specifying the field as a parameter and setting it when the pipeline run? </p>
","<azure><azure-data-lake><azure-data-factory>","2020-03-18 08:59:50","1203","3","1","60750278","<p>Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get MetaData</a> Activity,<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">For Each Activity</a>,<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">If Condition Activity</a> may implement your requirement.Please refer to my idea:</p>

<p>Firstly, my test files resides in the ADLS as below:</p>

<p><a href=""https://i.stack.imgur.com/mVssy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mVssy.png"" alt=""enter image description here""></a></p>

<p><code>test1.json</code> in 2016, <code>test2.json</code> in 2017, <code>test3.json</code> in 2018</p>

<p>In ADF, 1st layer:</p>

<p><a href=""https://i.stack.imgur.com/2MG4J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2MG4J.png"" alt=""enter image description here""></a></p>

<p>Dataset for Get Metadata Activity:</p>

<p><a href=""https://i.stack.imgur.com/SrnvE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SrnvE.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/CEdjp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CEdjp.png"" alt=""enter image description here""></a></p>

<p>Configuration for For Each Activity:</p>

<p><a href=""https://i.stack.imgur.com/Np2Yk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Np2Yk.png"" alt=""enter image description here""></a></p>

<p>Then,2nd layer:</p>

<p><a href=""https://i.stack.imgur.com/JuOFq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JuOFq.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/snsW5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/snsW5.png"" alt=""enter image description here""></a></p>

<p>Finally,3rd layer:</p>

<p><a href=""https://i.stack.imgur.com/jgx84.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jgx84.png"" alt=""enter image description here""></a></p>

<p>Source Dataset in copy activity:</p>

<p><a href=""https://i.stack.imgur.com/r0qIh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r0qIh.png"" alt=""enter image description here""></a></p>

<p>Test result,only <code>test1</code> and <code>test2</code> was pulled out.</p>

<p><a href=""https://i.stack.imgur.com/6ZlHt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6ZlHt.png"" alt=""enter image description here""></a></p>

<p>So,it does works for me.Any concern,pls let me know.</p>
"
"60733562","Connecting to On-Premises Oracle DB from Azure Data Factory","<p>Trying to connecting to On-Premises Oracle DB from Azure Data Factory.</p>

<p>Current Setup:</p>

<p><a href=""https://i.stack.imgur.com/aVppt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aVppt.png"" alt=""enter image description here""></a></p>

<ol>
<li>Oracle db - On Premises</li>
<li>VM with Self Hosted Integration Runtime on Azure - Tested with other MSSQL Server Databases - can connect successfully - This VM is managed by ITP (our Cloud Hosting Team), developers don't have access to it.</li>
<li>Azure Data Factory - configured to consume above Self Hosted Integration Runtime.</li>
</ol>

<p>I can connect to Oracle db from my local machine, using Taod for Oracle.  When passed same credential details - connection fails from ADF.</p>

<p>Do I need to ask ITP to install ODAC on VM for the connectivity?</p>
","<oracle><azure-data-factory>","2020-03-18 04:29:06","2590","0","1","60749643","<p>Congratulations that @Sreedhar have figured out the error by himself:</p>

<p>""Figured out, just need to install Oracle Data Access Components (ODAC) on the VM and everything works.""</p>

<p>I help him post it as answer and this can be beneficial to other community members. </p>
"
"60726722","Is there any way on Azure to get the metadata of different file types?","<p>I am trying to get metadata of different file types ( .xml , .mov , .mp3 , .mp4 , .jpeg , .png etc ). Is there any way in Azure data factory to do so? it's unclear how to set a get metadata task data set without specifying the file type (More specifically unstructured data).</p>
","<azure><azure-data-factory>","2020-03-17 16:46:09","67","0","1","60731662","<p>You should not need to specify the file type to get its metadata. Please have a look at <a href=""https://www.mssqltips.com/sqlservertip/6246/azure-data-factory-get-metadata-example/"" rel=""nofollow noreferrer"">this article</a> that explains step by step how you can do it. Note: The source type is different from the file type. So, the solution in the article is dependent on the source type, but in your case is file-based source.</p>
"
"60723135","How to read a local csv file using Azure Data Factory and a self-hosted runtime?","<p>I have a Windows Server VM with the ADF Integration Runtime installed running under a local account called <code>deploy</code>. This account is a member of the local admins group. The server is not domain-joined.</p>

<p>I created a new linked service (File System) and pointed it to a csv file on the root of the C drive as a test. When I test the connection I get Connection failed.</p>

<pre><code>Error occurred when trying to access the file in Folder 'C:\etr.csv', File filter: ''. The directory name is invalid. Activity ID: 1b892702-7cc3-48d5-83c7-c680d6d15afd.
</code></pre>

<p>Any ideas on a fix?</p>

<p><a href=""https://i.stack.imgur.com/Sb7QO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sb7QO.jpg"" alt=""screenshot""></a></p>
","<azure-data-factory>","2020-03-17 13:11:34","1082","2","2","60724169","<p>The linked service needs to be a folder on the target machine. In your screenshot, change <code>C:\etr.csv</code> to <code>C:\</code> and then define a new dataset that uses the linked service to select <code>etr.csv</code>.</p>
"
"60723135","How to read a local csv file using Azure Data Factory and a self-hosted runtime?","<p>I have a Windows Server VM with the ADF Integration Runtime installed running under a local account called <code>deploy</code>. This account is a member of the local admins group. The server is not domain-joined.</p>

<p>I created a new linked service (File System) and pointed it to a csv file on the root of the C drive as a test. When I test the connection I get Connection failed.</p>

<pre><code>Error occurred when trying to access the file in Folder 'C:\etr.csv', File filter: ''. The directory name is invalid. Activity ID: 1b892702-7cc3-48d5-83c7-c680d6d15afd.
</code></pre>

<p>Any ideas on a fix?</p>

<p><a href=""https://i.stack.imgur.com/Sb7QO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sb7QO.jpg"" alt=""screenshot""></a></p>
","<azure-data-factory>","2020-03-17 13:11:34","1082","2","2","60724331","<p>The dataset represents the structure of the data within the linked data stores, and the linked service defines the connection to the <strong>data source</strong>. So the linked service should point to the folder instead of file. It should be <code>C:\</code> instead of <code>C:\etr.csv</code></p>
"
"60722032","ADF: Sink Directory Ignored in my Data Flow","<p>Anyone had an issue with the Directory setting in the Sink Dataset.  The files are ending up in the locations that only includes the File System value:</p>

<p><a href=""https://i.stack.imgur.com/4uQkO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4uQkO.png"" alt=""enter image description here""></a></p>

<p>So the files end up in /curated
But should end up in /curated/profiledata</p>
","<azure-data-lake-gen2><azure-data-factory>","2020-03-17 12:03:16","878","0","1","60745823","<p>It depends on the File Name Option that you have selected.</p>

<p>""Output to Single File"" will honor that dataset file path folder.</p>

<p>But if you are using ""As data in column"", we start back at the container root in order to allow you to put your files in different multiple folder locations. You can just append your path to the value in the column in a derived column to set your proper path.</p>

<p><a href=""https://i.stack.imgur.com/NlyhS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NlyhS.png"" alt=""enter image description here""></a></p>
"
"60715866","Best practise to export ADF pipelines, datasets, etc to another subsctiption","<p>What's the best practise for exporting ADF pipelines, data sets, etc. to another subscription and ADF?
I know that you can choose the ""ARM Template"" drop down and select Export ARM template, then Import ARM template.
Or if you have source control set up, you could also clone the repo and connect that to a new ADF in a new subscription.
But what's the recommended/best practise way of doing this and how could it be automated?</p>
","<azure-devops><azure-data-factory>","2020-03-17 03:27:07","770","-1","1","60716094","<p>It really depends on how many times you are going to do this. If it is just once because you are moving to a different subscription, then just export ARM template and deploy it on the new subscription, thats just it.</p>

<p>If this will be a recurring thing, then the best practice is to automate this with Azure DevOps, as it is clearly explained here in the documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a></p>

<p>I've found this pdf also to be really helpful when doing this for the first time: <a href=""https://azure.microsoft.com/mediahandler/files/resourcefiles/whitepaper-adf-on-azuredevops/Azure%20data%20Factory-Whitepaper-DevOps.pdf"" rel=""nofollow noreferrer"">https://azure.microsoft.com/mediahandler/files/resourcefiles/whitepaper-adf-on-azuredevops/Azure%20data%20Factory-Whitepaper-DevOps.pdf</a></p>

<p>Hope this helped!</p>
"
"60714066","Using ADFv2 Validation activity to check minimum size of Virtual Directory of Azure BLOB dataset","<p>ADFv2 Validation activity using Azure BLOB dataset has a property called <strong>Minimum size</strong>. I would like to validate that a certain virtual directory for a given Azure blob storage has total file size specified in the <strong>Minimum size</strong> field. For that I have tried leaving the 'File' field of the connected dataset as blank but it didn't work. The activity succeeded even though there was an empty file in the virtual directory. Then again made the 'File' field as * and then the validation activity just kept running, never succeeded. How do I achieve this?</p>
","<azure-data-factory>","2020-03-16 22:36:45","381","0","1","60718604","<p>Actually, I tested and found that: <strong>Minimum size</strong> only works for specified file, like bellow:</p>

<p><a href=""https://i.stack.imgur.com/qWpUn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qWpUn.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/PZ6TJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PZ6TJ.png"" alt=""enter image description here""></a></p>

<p>Parameter with dynamic content also doesn't support:</p>

<p><a href=""https://i.stack.imgur.com/TiQvW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TiQvW.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/A8tgX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A8tgX.png"" alt=""enter image description here""></a></p>

<p>If we set the <strong>Minimum size</strong> value is bigger than the file size, the validation activity will always being in progress.</p>

<p>I think there are the limits about the validation activity. So we can't achieve that. We could call the Azure support to get more helps.</p>

<p>Hope this helps. </p>
"
"60709781","Dynamically query Oracle table by date with Azure Data Factory","<p>I am creating a copy job with Azure Data Factory (v2) from our on-premise <strong>Oracle database</strong> to our Azure Data Lake.  Ideally, this copy job is set up as a delta-load, where only the information from the last day is considered.</p>

<p>To do so, we want to filter the column ""<em>load_time</em>"", which is of the format <strong>datetime</strong>, with <strong>dynamic content</strong> functionality of Azure Data Factory.</p>

<p>The dummy query would be:</p>

<pre><code>SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;= {everything from one hour ago}
</code></pre>

<p>When enriching this query with dynamic content, we have something like</p>

<pre><code>SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;= addHours(utcnow(), -1, format='yyyy-MM-dd HH:mm:ss')
</code></pre>

<p>However, we continue to run into an error:</p>

<pre><code>ORA-00904:""ADDHOURS"": invalid identifier
</code></pre>

<p>Does anyone have any experience / insight in what is going wrong here?</p>

<p>Thanks</p>

<p>EDIT-1:  We aim to use the expressions from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">the Dynamic Content in Azure Data Factory</a>, such as ""<em>addHours</em>"" to set the datetime limit.</p>
","<oracle><azure-data-factory>","2020-03-16 16:32:58","1302","2","3","60709993","<p>I believe the ADDHOURS that is in use is from <a href=""https://docs.oracle.com/cd/E85694_01/ODPNT/TimeStampAddHours.htm"" rel=""nofollow noreferrer"">Oracle .Net C# function</a>.  It's not part of the Oracle functions.  You could build your own function in PL/SQL which mimics the same function but the better solution is to rewrite the where clause.  Something like</p>

<pre><code>WHERE sales.load_time &gt;= sysdate - interval '1' hour
</code></pre>
"
"60709781","Dynamically query Oracle table by date with Azure Data Factory","<p>I am creating a copy job with Azure Data Factory (v2) from our on-premise <strong>Oracle database</strong> to our Azure Data Lake.  Ideally, this copy job is set up as a delta-load, where only the information from the last day is considered.</p>

<p>To do so, we want to filter the column ""<em>load_time</em>"", which is of the format <strong>datetime</strong>, with <strong>dynamic content</strong> functionality of Azure Data Factory.</p>

<p>The dummy query would be:</p>

<pre><code>SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;= {everything from one hour ago}
</code></pre>

<p>When enriching this query with dynamic content, we have something like</p>

<pre><code>SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;= addHours(utcnow(), -1, format='yyyy-MM-dd HH:mm:ss')
</code></pre>

<p>However, we continue to run into an error:</p>

<pre><code>ORA-00904:""ADDHOURS"": invalid identifier
</code></pre>

<p>Does anyone have any experience / insight in what is going wrong here?</p>

<p>Thanks</p>

<p>EDIT-1:  We aim to use the expressions from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">the Dynamic Content in Azure Data Factory</a>, such as ""<em>addHours</em>"" to set the datetime limit.</p>
","<oracle><azure-data-factory>","2020-03-16 16:32:58","1302","2","3","60713330","<p>Looks like the issue is with Dynamic expression which is used to form the required query. </p>

<p>Please try updating your dynamic expression as below in your ADF pipeline activity to form a valid query.</p>

<p>I have defined a pipeline parameter named SingleQuotes of type string and value = ' (single quote) -  This is required to form a valid where condition like ""<code>WHERE sales.load_time &gt;='2020-03-16 20:04:04'</code> ""</p>

<p><strong>Dynamic Expression</strong>:</p>

<pre><code>@concat('SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;=', pipeline().parameters.singleQuotes, formatDateTime(addHours(utcnow(), -1), 'yyyy-MM-dd HH:mm:ss'), pipeline().parameters.singleQuotes)
</code></pre>

<p>This dynamic expression will generate a SQL query as below: (I have tried this in T-SQL)</p>

<p>""sqlReaderQuery"": ""<code>SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;='2020-03-16 20:04:04'</code>""</p>

<p>Hope this helps.</p>
"
"60709781","Dynamically query Oracle table by date with Azure Data Factory","<p>I am creating a copy job with Azure Data Factory (v2) from our on-premise <strong>Oracle database</strong> to our Azure Data Lake.  Ideally, this copy job is set up as a delta-load, where only the information from the last day is considered.</p>

<p>To do so, we want to filter the column ""<em>load_time</em>"", which is of the format <strong>datetime</strong>, with <strong>dynamic content</strong> functionality of Azure Data Factory.</p>

<p>The dummy query would be:</p>

<pre><code>SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;= {everything from one hour ago}
</code></pre>

<p>When enriching this query with dynamic content, we have something like</p>

<pre><code>SELECT sales.* FROM schema.sales sales WHERE sales.load_time &gt;= addHours(utcnow(), -1, format='yyyy-MM-dd HH:mm:ss')
</code></pre>

<p>However, we continue to run into an error:</p>

<pre><code>ORA-00904:""ADDHOURS"": invalid identifier
</code></pre>

<p>Does anyone have any experience / insight in what is going wrong here?</p>

<p>Thanks</p>

<p>EDIT-1:  We aim to use the expressions from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">the Dynamic Content in Azure Data Factory</a>, such as ""<em>addHours</em>"" to set the datetime limit.</p>
","<oracle><azure-data-factory>","2020-03-16 16:32:58","1302","2","3","76063112","<p>I had the same issue I have sorted it out via the following method:</p>
<ol>
<li>Create a Set variable activity (Let's say the name - X)</li>
</ol>
<p><a href=""https://i.stack.imgur.com/gxvP6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gxvP6.png"" alt=""enter image description here"" /></a></p>
<pre><code>@formatDateTime('2022-01-01 00:00:00', 'yyyy-MM-dd HH:mm:ss')
</code></pre>
<ol start=""2"">
<li><p>Write the following Code to pipeline expression builder:</p>
<p>@{concat('select * from Table where Column1=''','A',''' AND TO_CHAR(Date, ''',
'YYYY-MM-DD HH24:MI:SS',''') &gt;= ''', variables('X'),'''')}</p>
</li>
</ol>
<p>This will do the trick!</p>
"
"60709053","Azure Data Factory copy between ADLs with a dynamic path","<p>I am trying to create a copy activity between two Azure Data Lakes GEN1. I have to make the copy over a path where one of the subfolders is varible, for example: </p>

<pre><code>rootFolder/subFolder1/*/subFolder3
</code></pre>

<p>where * can take different values, and the copy has to be made automatically for all these possible values, so it was not worth setting that subfolder as a parameter and running the pipeline as many times as the number of possible values.</p>

<p>I would like to know if there is a way to implement this copy activity automatically, I am new to Azure and ADF.</p>
","<azure><azure-data-lake><azure-data-factory>","2020-03-16 15:46:26","894","0","1","60715074","<p>This can be achieved by using Wildcard filtering in the source settings of your Copy Activity.</p>
<p>Set Wildcard Folder path = <code>rootFolder/subFolder1/*/subFolder3</code>
Wildcard File name = * or *.json or *.txt or *.csv.., etc based on your requirement.</p>
<p><strong>For Example</strong>:</p>
<p><a href=""https://i.stack.imgur.com/dCLPg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dCLPg.png"" alt=""enter image description here"" /></a></p>
<p>To know more about resulting behavior of the folder path and file name with wildcard filters, please refer to this MS Doc: <a href=""https://learn.microsoft.com/azure/data-factory/connector-azure-data-lake-store#folder-and-file-filter-examples"" rel=""nofollow noreferrer"">https://learn.microsoft.com/azure/data-factory/connector-azure-data-lake-store#folder-and-file-filter-examples</a></p>
<p><strong>Here are few threads related to similar requirement which might be helpful</strong>:</p>
<p><a href=""https://social.msdn.microsoft.com/Forums/en-US/d2b6c77b-c6de-4369-94ba-5a0c7e31a510/iterate-to-every-single-folder-and-each-files-into-a-single-csv?forum=AzureDataFactory#c9696acd-39d0-491f-8ced-7bf135b82c10"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/d2b6c77b-c6de-4369-94ba-5a0c7e31a510/iterate-to-every-single-folder-and-each-files-into-a-single-csv?forum=AzureDataFactory#c9696acd-39d0-491f-8ced-7bf135b82c10</a></p>
<p><a href=""https://social.msdn.microsoft.com/Forums/en-US/00c93358-0291-44d9-96d8-266fadc7fd47/copy-data-to-separate-directory-based-on-folder-name?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/00c93358-0291-44d9-96d8-266fadc7fd47/copy-data-to-separate-directory-based-on-folder-name?forum=AzureDataFactory</a></p>
"
"60708026","Azure Data Factory unzip and move files","<p>I know this has been asked (in other question as well as) <a href=""https://stackoverflow.com/questions/57261025/how-to-decompress-a-zip-file-in-azure-data-factory-v2"">here</a>, which are exactly my cases. I have downloaded (via ADF) a zip file to Azure Blob and I am trying to decompress it and move the files to another location within the Azure Blob container.</p>

<p>However having tried both of those approaches I only end up with a zipped file moved to another location without it being unzipped.</p>
","<zip><azure-data-factory>","2020-03-16 14:43:16","2621","2","1","60720631","<p>Trying to understand your question - Was your outcome a zip file or the folder name has .zip on it? It sounds crazy, let me explain in detail. In ADF decompressing the zip file using copy activity creates a folder(which has .zip on its name) which has actual file in it.</p>

<p>Example: Let's say you have sample.txt inside abc.zip </p>

<p>Blob sourcepath: container1/abc.zip [Here abc.zip is a compressed file]</p>

<p>Output path will be: container2/abc.zip/sample.txt [Here, abc.zip is the decompressed folder name]</p>

<p> This is achieved when the copy behaviour of sink is ""none"". Hope it helps :) </p>
"
"60707385","Set 'Copy Data' source folder with @item().name in Azure Data Factory","<p>I have the following pipeline:
<a href=""https://i.stack.imgur.com/8JqCg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8JqCg.png"" alt=""enter image description here""></a></p>

<p>Get Metadata1 basically retrieves child items (which is collection of folders i.e. <code>originalFolder1</code>, <code>originalFolder2</code>, etc..).</p>

<p>Inside ForEach1 activity, I put copy data activity.</p>

<p>When defining the source dataset, I would like to use the retrieved folder name as the path. So It would be like this: <code>staticFolder1/staticFolder2/originalFolder1</code>.</p>

<p>I did try using <code>staticFolder1/staticFolder2/@item().name</code> in the folder path but it always throw an error <code>file not found</code></p>

<p>Am I missing something?</p>
","<azure-data-lake><azure-data-factory><dynamic-content>","2020-03-16 14:00:26","2096","0","2","60707897","<p>unfortunately, in ADF you can't mix string and expression, you can only have one or the other </p>

<p>what you can do is:</p>

<pre><code>@concat(variables('SourceFolderName'), '/', string(item().name) )
</code></pre>

<p>say if you have a variable called SourceFolderName, which can be useful if your source folder is different everyday; otherwise make it as a string </p>

<pre><code>@concat('staticFolder1/staticFolder2/', string(item().name) )
</code></pre>

<p>then you can concatenate the ForEach activity's item to form the complete path </p>
"
"60707385","Set 'Copy Data' source folder with @item().name in Azure Data Factory","<p>I have the following pipeline:
<a href=""https://i.stack.imgur.com/8JqCg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8JqCg.png"" alt=""enter image description here""></a></p>

<p>Get Metadata1 basically retrieves child items (which is collection of folders i.e. <code>originalFolder1</code>, <code>originalFolder2</code>, etc..).</p>

<p>Inside ForEach1 activity, I put copy data activity.</p>

<p>When defining the source dataset, I would like to use the retrieved folder name as the path. So It would be like this: <code>staticFolder1/staticFolder2/originalFolder1</code>.</p>

<p>I did try using <code>staticFolder1/staticFolder2/@item().name</code> in the folder path but it always throw an error <code>file not found</code></p>

<p>Am I missing something?</p>
","<azure-data-lake><azure-data-factory><dynamic-content>","2020-03-16 14:00:26","2096","0","2","60719464","<p>The comments provided by @sowmen makes sense, but it's not necessary to convert the item().name to string format. </p>

<p>There is an other interesting concept called String Interpolation, which makes our life more easier. Please see below the code that fits your scenario. Hope it helps :)</p>

<pre><code>staticFolder1/staticFolder2/@{item().name}
</code></pre>
"
"60707323","How to fail Azure Data Factory pipeline based on IF Task","<p>I have a pipeline built on Azure data Factory. It has:</p>

<ol>
<li><p>a ""LookUp"" task that has an SQL query that returns a column [CountRecs]. This columns holds a value 0 or more.</p></li>
<li><p>an ""if"" task to check this returned value. I want to fail the pipeline when the value of [CountRecs]>0 </p></li>
</ol>

<p>Is this possible?</p>
","<azure-data-factory>","2020-03-16 13:56:02","5268","6","2","60712553","<p>You could probably achieve this by having a Web Activity when your IF Condition is true ([CountRecs]>0) in which the web activity should call the below REST API to cancel the pipeline run by using the pipelinerunID (you can get this value by using dynamic expression - <code>@pipeline().RunId</code>)</p>

<p>Sample Dynamic Expression for Condition: <code>@greater(activity('LookupTableRecordCount').output.firstRow.COUNTRECS, 0)</code></p>

<p><strong>REST API to Cancel the Pipeline Run</strong>: POST <a href=""https://management.azure.com/subscriptions/%7BsubscriptionId%7D/resourceGroups/%7BresourceGroupName%7D/providers/Microsoft.DataFactory/factories/%7BfactoryName%7D/pipelineruns/%7BrunId%7D/cancel?api-version=2018-06-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelineruns/{runId}/cancel?api-version=2018-06-01</a></p>

<p>MS Doc related to Rest API:  <a href=""https://learn.microsoft.com/rest/api/datafactory/pipelineruns/cancel"" rel=""nofollow noreferrer"">ADF Pipeline Runs - Cancel</a></p>

<p>One other possible way is to have an invalid URL in your web activity which will fail the Web activity in-turn it will fail the IfCondition activity, which inturn will result in your pipeline to fail. </p>

<p>There is an existing feature request related to the same requirement in ADF user voice forum suggested by other ADF users. I would recommend you please up-vote and/or comment on this feedback which will help to increase the priority of the feature request implementation.</p>

<p>ADF User voice feedback related to this requirement: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/38143873-a-new-activity-for-cancelling-the-pipeline-executi"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/38143873-a-new-activity-for-cancelling-the-pipeline-executi</a></p>

<p>Hope this helps.</p>
"
"60707323","How to fail Azure Data Factory pipeline based on IF Task","<p>I have a pipeline built on Azure data Factory. It has:</p>

<ol>
<li><p>a ""LookUp"" task that has an SQL query that returns a column [CountRecs]. This columns holds a value 0 or more.</p></li>
<li><p>an ""if"" task to check this returned value. I want to fail the pipeline when the value of [CountRecs]>0 </p></li>
</ol>

<p>Is this possible?</p>
","<azure-data-factory>","2020-03-16 13:56:02","5268","6","2","66881104","<p>As a sort-of hack-solution you can create a &quot;Set variable&quot; activity which incurs division by zero if a certain condition is met. I don't like it but it works.</p>
<pre><code>@string( 
  div(
    1 
  , if(
      greater( int(variables('date_diff')), 100 )
     , 0
     , 1
    )
  ) 
)
</code></pre>
"
"60707087","Time Out Expired - Moving Data from managed Intance to DataWarehouse AZure Data Factory","<p>We have huge data to be moved betwwen managed instance and DataWarehouse.We are using a
bulk insert to Move the data.</p>

<p>Issue:
Time Out Expired - Moving Data from managed Intance to DataWarehouse AZure Data Factory.</p>

<p>Attached the screenhots for the same.<a href=""https://i.stack.imgur.com/ThLBR.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure><azure-data-factory>","2020-03-16 13:41:08","30","0","1","60708896","<p>You can configure the timeout in the sink tab of the copy activity. </p>

<p><a href=""https://i.stack.imgur.com/S52wg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S52wg.png"" alt=""enter image description here""></a></p>

<p>By default it is 2 hours, you can modify that if the insert is taking longer than that. The format is HH:mm:ss (timespan). Doc about this here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#sql-server-as-a-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#sql-server-as-a-sink</a></p>

<p>Aditionally, you can increase the number of DIU (data integration units) from the Settings tab of the copy activity. Your image shows 4 DIU being used, try adding more to that.</p>

<p>Hope this helped!</p>
"
"60703396","copy activities queued for long time","<p>We are migrating the data from Oracle data warehouse to Azure SQL Data Warehouse through copy data activity and the status is showing always in Queue for all the pipelines from the last two weeks. Its almost taking 30 minutes to 45 minutes in queue for 4000 records(Azure IR north central US). </p>

<blockquote>
  <p>SUBSCRIPTION ID:2ac8ee00-839a-4148-8460-d5265a560467.</p>
</blockquote>

<p>Please let me know what the issue is.</p>

<p>Thanks in advance.</p>
","<azure-data-factory>","2020-03-16 09:32:07","315","1","1","60712698","<p>The queue time you mentioned is pretty high . Can you please try to see how things works when </p>

<ul>
<li>Let the source be Oracle DW and let the sink be a blob .Do you see any improvement ? if yes the Synapse is having some issue . </li>
<li>Get the blob as the source and try to to pump in the data to Synapse , if you see any improvement , Oracle DW is having an issue .</li>
<li><p>If there is no improvement something else is going wrong . </p></li>
<li><p>Are all the resource in the same region ? </p></li>
<li>Was this thing working before ever ? </li>
</ul>
"
"60700074","Deploying Data Factory through ARM - Data Factory not recognizing Stored Procedure","<p>On deploying my ARM for my Data Factory, it is not recognizing my stored procedure and giving this value as my <strong>'Stored procedure name'</strong>: <code>[object Object]</code></p>

<p><a href=""https://i.stack.imgur.com/h3eFL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h3eFL.png"" alt=""enter image description here""></a></p>

<p>Here is how my stored procedure is declared in <strong>ARM Template</strong>:</p>

<pre class=""lang-js prettyprint-override""><code>{
        ""name"": ""execute_teststoredproc"",
        ""type"": ""SqlServerStoredProcedure"",
        ""dependsOn"": [],
        ""policy"": {
            ""timeout"": ""7.00:00:00"",
            ""retry"": 0,
            ""retryIntervalInSeconds"": 30,
            ""secureOutput"": false,
            ""secureInput"": false
        },
        ""userProperties"": [],
        ""typeProperties"": {
            ""storedProcedureName"": {
                ""value"": ""[[dbo].[teststoredproc]""
            }
        },
        ""linkedServiceName"": {
            ""referenceName"": ""TargetSQLDB"",
            ""type"": ""LinkedServiceReference""
        }
    }
</code></pre>

<p>Anyone knows why this is happening? Thanks.</p>
","<azure><azure-resource-manager><azure-data-factory>","2020-03-16 03:40:29","43","0","1","60701489","<p>Apparently you cannot use the <code>value</code> field for <code>storedProcedureName</code>. Simple yet subtle difference that can make you waste unnecessary time.</p>

<p>Correct code:</p>

<pre class=""lang-js prettyprint-override""><code>    ""typeProperties"": {
        ""storedProcedureName"": ""[[dbo].[teststoredproc]""
    }
</code></pre>

<p>instead of </p>

<pre class=""lang-js prettyprint-override""><code>    ""typeProperties"": {
        ""storedProcedureName"": {
            ""value"": ""[[dbo].[teststoredproc]""
        }
    }
</code></pre>
"
"60698466","ADF: Split a JSON file with an Array of Objects into Single JSON files containing One Element in Each","<p>I'm using Azure Data Factory and trying to convert a JSON file that is an array of JSON objects into separate JSON files each contain one element e.g. the input:</p>

<pre><code>[
{""Animal"":""Cat"",""Colour"":""Red"",""Age"":12,""Visits"":[{""Reason"":""Injections"",""Date"":""2020-03-15""},{""Reason"":""Check-up"",""Date"":""2020-01-02""}]},
{""Animal"":""Dog"",""Colour"":""Blue"",""Age"":1,""Visits"":[{""Reason"":""Check-up"",""Date"":""2020-02-08""}]},
{""Animal"":""Guinea Pig"",""Colour"":""Green"",""Age"":5,""Visits"":[{""Reason"":""Injections"",""Date"":""2019-12-01""},{""Reason"":""Check-up"",""Date"":""2020-02-26""}]}
]
</code></pre>

<p>However, I've tried Data Flow to split this array up into single files containing each element of the JSON array but cannot work it out.  Ideally I would also want to name each file dynamically e.g. Cat.json, Dog.json and ""Guinea Pig.json"".</p>

<p>Is Data Flow the correct tool for this with Azure Data Factory (version 2)?</p>
","<json><azure><azure-data-factory>","2020-03-15 22:50:56","3985","2","1","60704202","<p>Data Flows should do it for you. Your JSON snippet above will generate 3 rows. Each of those rows can be sent to a single sink. Set the Sink as a JSON sink with no filename in the dataset. In the Sink transformation, use the 'File Name Option' of 'As Data in Column'. Add a Derived Column before that which sets a new column called 'filename' with this expression:</p>

<p>Animal + '.json'</p>

<p>Use the column name 'filename' as data in column in the sink.</p>

<p>You'll get a separate file for each row.</p>

<p><a href=""https://i.stack.imgur.com/1wmEq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1wmEq.png"" alt=""enter image description here""></a></p>
"
"60673220","Transfer the output of 'Set Variable' activity into a json file [Azure Data Factory]","<p>In Data Factory, can we have the output from 'Set Variable' activity being logged as a json file?</p>
","<json><azure-data-lake><azure-data-factory>","2020-03-13 15:30:31","14642","6","4","60699405","<p>No built-in easy way for your need as i know.</p>

<p>2 ways as workarounds:</p>

<p>1.Use enable <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">Azure Monitor diagnostic log</a> in ADF to log data into Azure Blob Storage as JSON files.And every activity's <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#schema-of-logs-and-events"" rel=""nofollow noreferrer"">execution details(contains output)</a> could be logged in the file.However,you need to get know the structure of json schema and grab what you want.</p>

<p>2.Use Azure Function or Web Activity after Set Variable Activity to call API(@activity('Set Variable1').output). Save the output into residence as json file in the function method with SDK code.</p>
"
"60673220","Transfer the output of 'Set Variable' activity into a json file [Azure Data Factory]","<p>In Data Factory, can we have the output from 'Set Variable' activity being logged as a json file?</p>
","<json><azure-data-lake><azure-data-factory>","2020-03-13 15:30:31","14642","6","4","61376547","<p>Another simplest way to achieve this requirement is by utilizing  ""<a href=""https://learn.microsoft.com/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy"" rel=""noreferrer"">Add additional columns during copy</a>"" feature as below. </p>

<p>Have a set variable activity and set the value of the variable, followed by a copy activity. In copy activity, <code>Source</code> settings, you have <code>Additional columns</code> property where you can give a name to source variable column. Using Dynamic expression <code>@variables('varInput')</code> you assign the variable value. Then in <code>Mapping</code> section, you can remove unwanted columns and only have the required columns including variable column that you created in <code>Additional columns</code> of <code>Source</code>. Then on the destination side give your desired column name and test it. </p>

<p><a href=""https://i.stack.imgur.com/BvKKl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BvKKl.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/rp1xp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rp1xp.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/uflvx.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uflvx.gif"" alt=""enter image description here""></a></p>

<p><strong>NOTE</strong>: This feature works with the latest dataset model. If you don't see this option from the UI, try creating a new dataset.</p>

<p>Hope this helps. </p>
"
"60673220","Transfer the output of 'Set Variable' activity into a json file [Azure Data Factory]","<p>In Data Factory, can we have the output from 'Set Variable' activity being logged as a json file?</p>
","<json><azure-data-lake><azure-data-factory>","2020-03-13 15:30:31","14642","6","4","70325693","<p>If you want to write the content of a variable of type Array, there is a workaround which works fine.
Goal: write content of your array as 1 line per value of the array into a file</p>
<p>variable : [ a, b, c]<br>
to<br>
file content:<br>
a<br>
b<br>
c<br></p>
<p>Steps:</p>
<ol>
<li>Create an 'empty' file with 1 row , can be a json file or something else with just 1 row</li>
<li>Use the additional column mechanism</li>
<li>Expand-join-with-carriagereturn the array variable with use of @join and @decodeUriComponent -&gt; @join(variable,decodeUriComponent('%0A'))</li>
</ol>
<p>Yes, it is horrible that Microsoft doesn't have an @char(int) function to create a special character.(or I am an idiot and doesn't know the right way to concat an '\n' , which I tried but didn't work.)</p>
"
"60673220","Transfer the output of 'Set Variable' activity into a json file [Azure Data Factory]","<p>In Data Factory, can we have the output from 'Set Variable' activity being logged as a json file?</p>
","<json><azure-data-lake><azure-data-factory>","2020-03-13 15:30:31","14642","6","4","70334207","<p>I generally use the Copy activity for writing files but it is possible to write content to Azure Data Lake (ADLS) Gen 2 using the <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/put-blob"" rel=""nofollow noreferrer"">Blob REST API</a> and <code>PUT</code> command.  The settings in the Web activity are crucial to this working:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Setting</th>
<th>Value</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>URL</td>
<td>some blob</td>
<td>NB this is using the .blob address not the .dfs one.  The path must end in <code>?resource=file</code></td>
</tr>
<tr>
<td>Method</td>
<td>PUT</td>
<td></td>
</tr>
<tr>
<td>Headers</td>
<td></td>
<td></td>
</tr>
<tr>
<td>x-ms-version</td>
<td>2019-07-07</td>
<td></td>
</tr>
<tr>
<td>x-ms-blob-type</td>
<td>BlockBlob</td>
<td></td>
</tr>
<tr>
<td>Content-Type</td>
<td>application/json</td>
<td>This value is for writing json but can be customised eg application/csv</td>
</tr>
<tr>
<td>Body</td>
<td>@variables('varResult')</td>
<td>I'm using a pre-prepared variable with json content but this can be anything</td>
</tr>
<tr>
<td>Authentication</td>
<td>Managed Identity</td>
<td></td>
</tr>
<tr>
<td>Resource</td>
<td><a href=""https://storage.azure.com"" rel=""nofollow noreferrer"">https://storage.azure.com</a></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Note you must set the URL to the file name and folder you want and use the .blob address.  The URL must end with <code>?resource=file</code>:</p>
<p><strong>Example URL / Blob address</strong>
<a href=""https://yourstorage.blob.core.windows.net/yourFilesystem/yourFolder/someFile.json?resource=file"" rel=""nofollow noreferrer"">https://yourstorage.blob.core.windows.net/yourFilesystem/yourFolder/someFile.json?resource=file</a></p>
<p>Note also I'm writing json here but you can amend as required, eg <code>application/csv</code>.  I am using a variable in the Body but this can be anything you like.  The documentation states this only supports files up to 2GB so this is only for small activities.</p>
<p>Screenprint:
<a href=""https://i.stack.imgur.com/xstRy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xstRy.png"" alt=""Web activity settings"" /></a></p>
<p>I wasn't able to get this to work with the <code>.dfs</code> address and/or Data Lake methods but it's fine as long as it works on blob.</p>
"
"60672248","Is Data encrypted during In-Transit in Azure Data Factory and Databricks runtime?","<p>Is Data encrypted during In-Transit in Azure Data Factory while data movement
and Databricks runtime when data transformation.</p>

<p>Please share artifcats if any to understand the flow.</p>

<p>Thanks,
Mahammad khan</p>
","<azure-data-factory><azure-databricks>","2020-03-13 14:27:23","171","0","1","75112548","<p>Yes, Databricks workspaces, and all associated API calls, are only accessible over HTTPS with TLS.</p>
"
"60671018","Is there way a to use join query in Azure Data factory When copying data from Sybase source","<p>I am trying to ingest data from Sybase source in to Azure datalake. I am ingesting several tables using a Watermark table that has tables names from Sybase source. Now process works fine for a full import, however we are trying to Import tables every 15 minutes to feed a dashboard. We don't need to ingest whole table as we don't need all the data from it. </p>

<p>Table doesn't have dateModified or any kind of incremental id to perform an incremental load. Only way of filtering out unwanted data is to perform a join on to another look up table at source and then using ""filter"" value in ""Where"" clause. </p>

<p>Is there a way we can perform this in Azure data factory ? I have attached my current pipeline screenshot just to make it a bit more clear. 
<a href=""https://i.stack.imgur.com/9g9SG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9g9SG.png"" alt=""enter image description here""></a>  </p>
","<azure><azure-data-factory>","2020-03-13 13:05:07","772","0","1","60719087","<p>Many thanks for looking in to this. I have managed to find a solution. I was using a Watermark table to ingest about 40 tables using one pipeline. My only issue was how to use join and ""where"" filter in my query without hard coding it in pipeline. I have achieved this by adding ""Join"" and ""Where"" fields in my Watermark table and then passing it in ""Query"" as @{item ().Join} @{item().Where). It Worked like a magic.</p>
"
"60670104","U-SQL User Defined Function in Azure Data Factory","<p>I'm currently using Azure Data Factory for an ETL job and in the end I want to start a U-SQL job. I've created my datalake.usql script and the UDF's the script uses are in the datalake.usql.cs file, the same structure a U-SQL project has in Visual Studio (which is where I developed the U-SQL job and succesfully ran it).</p>

<p>After that, I uploaded them both to Azure Blob Storage and set up the U-SQL step in Azure Data Factory to use the U-SQL script, but it doesn't see the datalake.usql.cs with my UDF's.</p>

<p>How can I do this?</p>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2020-03-13 12:00:59","415","0","1","60712398","<p>I figured this one out so I'll post the answer here if someone else has this problem in the future.</p>

<p>When you're developing locally, you have a script.usql and a script.usql.cs file. When you run it, Visual Studio does all the heavy lifting, compiles it all in a usable way and your script runs. But when you're trying to run a script from Azure Data Factory, you can't perform that compilation on the fly, like Visual Studio does.</p>

<p>The solution to this problem is to make an Assembly, a .dll file, from your script.usql.cs, upload it to the data storage you're using and register it in the U-SQL Catalog. Then, you can reference this assembly and use it normally, as you would on your local machine.</p>

<p>All the steps needed for this are presented in these short guides:</p>

<p><a href=""https://saveenr.gitbooks.io/usql-tutorial/content/usql-catalog/intro.html"" rel=""nofollow noreferrer"">https://saveenr.gitbooks.io/usql-tutorial/content/usql-catalog/intro.html</a></p>

<p><a href=""https://saveenr.gitbooks.io/usql-tutorial/content/usql-catalog/usql-databases.html"" rel=""nofollow noreferrer"">https://saveenr.gitbooks.io/usql-tutorial/content/usql-catalog/usql-databases.html</a></p>

<p><a href=""https://saveenr.gitbooks.io/usql-tutorial/content/usql-catalog/assemblies.html"" rel=""nofollow noreferrer"">https://saveenr.gitbooks.io/usql-tutorial/content/usql-catalog/assemblies.html</a></p>

<p><a href=""https://www.c-sharpcorner.com/UploadFile/1e050f/creating-and-using-dll-class-library-in-C-Sharp/"" rel=""nofollow noreferrer"">https://www.c-sharpcorner.com/UploadFile/1e050f/creating-and-using-dll-class-library-in-C-Sharp/</a></p>
"
"60669045","Azure data factory how to fetch source count and sink count for data validation","<p>I have very genric question. Like I am copy data from storage to sql. For validation I need to check 100 records in or 100 rows in storage in csv form all rows are moved to sql or not. I.e I need to find out the Source records count and destination records count.</p>

<p><a href=""https://i.stack.imgur.com/PKgLc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>I need to populated those count in monitor section of user property.
<a href=""https://i.stack.imgur.com/St3Al.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure><error-handling><count><azure-data-factory><paas>","2020-03-13 10:47:09","948","0","1","60702054","<p>One possible way you could try is to have an additional Wait Activity chained to your Copy Activity and then configure the User Properties of Wait Activity with dynamic expression as below to monitor the number of rows read and copied using Copy Activity from UI.</p>

<p><strong>User Properties of Wait Activity</strong></p>

<ol>
<li><code>RowsReadFromSource_copyCSVToSQLTable = @string(activity('copyCSVToSQLTable').output.rowsRead)</code></li>
<li><code>RowsCopiedToSink_copyCSVToSQLTable = @string(activity('copyCSVToSQLTable').output.rowsCopied)</code></li>
</ol>

<p><a href=""https://i.stack.imgur.com/rRxLz.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rRxLz.gif"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60669027","Delete data from destination tables before running copy data activity","<p>I am trying to create a pipeline that moves data between some Oracle databases. I have successfully created and tested the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">copy data</a> activities. My problem is I want to empty the destination tables before the ""copy"" runs and I can't see a way to do that (right now it appends data to the tables).</p>

<p>At first I tried using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Stored Procedure</a> activity, but that only supports SQL Server-related sources. None of the other activities offered by data factory work for this use case. Even a generic ""execute SQL"" activity could work for me, but nothing like that appears to be available.</p>

<p>EDIT: I have posted an answer below, but it's not ideal. Please post an answer if you think you have a better solution!</p>
","<sql><oracle><azure><azure-data-factory>","2020-03-13 10:45:09","13479","3","3","60671358","<p>I have found a slightly hacky solution. There is a configuration in the Sink part of the Copy Data activity called <strong>Pre-copy script</strong>. The documentation says to use it for manipulating the incoming data so I thought it would only work on the incoming data, but it's actually SQL that's executed on the target database (seemingly in a transaction, which is committed)</p>

<p>So basically, this works:</p>

<p><a href=""https://i.stack.imgur.com/xQvQp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xQvQp.png"" alt=""sql in copy data activity""></a></p>

<p>I'm not a huge fan of this solution, because it's hard to configure the flow. I would have preferred a separate activity so I could prepare success/failure options, while with this, the whole ""Copy Data"" activity will fail. Please post an answer if you have a better idea!</p>
"
"60669027","Delete data from destination tables before running copy data activity","<p>I am trying to create a pipeline that moves data between some Oracle databases. I have successfully created and tested the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">copy data</a> activities. My problem is I want to empty the destination tables before the ""copy"" runs and I can't see a way to do that (right now it appends data to the tables).</p>

<p>At first I tried using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Stored Procedure</a> activity, but that only supports SQL Server-related sources. None of the other activities offered by data factory work for this use case. Even a generic ""execute SQL"" activity could work for me, but nothing like that appears to be available.</p>

<p>EDIT: I have posted an answer below, but it's not ideal. Please post an answer if you think you have a better solution!</p>
","<sql><oracle><azure><azure-data-factory>","2020-03-13 10:45:09","13479","3","3","60744861","<p>I have always used a Stored Procedure to clear table data as part of my pipelines. I do this because I usually have a specific where clause for the data [that typically lines up with a table partition] rather than truncating the entire table.</p>

<p>As an alternative to the Copy activity, you can use a Data Flow (ADFDF) activity. ADFDF Sink settings show you can truncate the table:</p>

<p><a href=""https://i.stack.imgur.com/xFZip.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xFZip.png"" alt=""enter image description here""></a></p>

<p>This screenshot is for a SQLDW Sink, so I don't know if the ADFDF Sink for Oracle permits this or not.</p>
"
"60669027","Delete data from destination tables before running copy data activity","<p>I am trying to create a pipeline that moves data between some Oracle databases. I have successfully created and tested the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">copy data</a> activities. My problem is I want to empty the destination tables before the ""copy"" runs and I can't see a way to do that (right now it appends data to the tables).</p>

<p>At first I tried using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Stored Procedure</a> activity, but that only supports SQL Server-related sources. None of the other activities offered by data factory work for this use case. Even a generic ""execute SQL"" activity could work for me, but nothing like that appears to be available.</p>

<p>EDIT: I have posted an answer below, but it's not ideal. Please post an answer if you think you have a better solution!</p>
","<sql><oracle><azure><azure-data-factory>","2020-03-13 10:45:09","13479","3","3","60744889","<p>I use ""TRUNCATE TABLE xxx"" in pre-copy script. (Truncate is faster than Delete)</p>

<p>Also, to your point, there is no ""Execute SQL task"", but as a hack, we can use a ""Lookup"" activity to execute ad-hoc scripts. Link to one of my previous answers on the same : <a href=""https://stackoverflow.com/a/59422740/2993606"">https://stackoverflow.com/a/59422740/2993606</a></p>
"
"60667396","Azure DevOps Exception: LinkedAuthorizationFailed","<p>I have developed a release pipeline for Azure data Factory which includes blob trigger. The deployment succeeds with exception: LinkedAuthorizationFailed, which results in triggered not being created. Everything else including pipeline and data sets are deployed.Following is the exception I get..</p>

<p><a href=""https://i.stack.imgur.com/SBsJr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SBsJr.png"" alt=""enter image description here""></a></p>

<p>Not sure what is the cause? Any pointers would be helpful.</p>
","<azure-devops><azure-data-factory>","2020-03-13 08:51:09","3065","1","1","60702114","<p>From the error message, your service principal does not have the permission <code>Microsoft.EventGrid/EventSubscriptions/Write</code> on your storage account.</p>

<p>To fix the issue, follow the steps below.</p>

<p>1.Navigate to the <code>Azure Active Directory</code> in the azure portal -> <code>Enterprise applications</code>-> specify the <code>Application Type</code> with <code>All Applications</code>-> search for the <code>Client ID</code> in the error message(the first GUID in <code>message</code>), copy the <code>Name</code>.</p>

<p><a href=""https://i.stack.imgur.com/57CDP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/57CDP.png"" alt=""enter image description here""></a></p>

<p>2.Navigate to the storage account mentioned in the error message ->  <code>Access control (IAM)</code> -> <code>Add</code> -> <code>Add role assignment</code> -> search for the <code>Name</code> copied in step 1 and add it as a <code>Contributor</code> role, then the error should be fixed.</p>

<p><a href=""https://i.stack.imgur.com/g8KNN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g8KNN.png"" alt=""enter image description here""></a></p>
"
"60666620","Unable to use SSIS package for importing data to Azure Data Warehouse using Data Factory","<p>We are trying to copy the data from on-premise system to Azure Data Warehouse. We want to use SSIS package for copy data from on premise to cloud. Azure Data Factory v1 does not support SSIS package and Azure Data factory v2 supports SSIS packages but it does not supported by Data Warehouse.</p>

<p>So My Question is, How to import data from On-premise database to Azure Data Warehouse using Azure Data Factory SSIS package?</p>

<p>Thanks in Advance,
Vinir Shah</p>

<p><a href=""https://i.stack.imgur.com/xx7Ss.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xx7Ss.png"" alt=""Ref Diagram""></a></p>
","<ssis><azure-data-factory><azure-synapse>","2020-03-13 07:42:28","374","0","1","60715266","<p>I'm afraid we can't copy the data from on-premise system to Azure Data Warehouse with SSIS package running in Data Factory.</p>

<p>To copy the data from on-premise system to Azure Data Warehouse, we could do with two ways:</p>

<ol>
<li>Run the SSIS package local: <a href=""https://learn.microsoft.com/en-us/sql/integration-services/load-data-to-sql-data-warehouse?view=sql-server-ver15#option-1---use-the-sql-dw-upload-task"" rel=""nofollow noreferrer"">Load data into Azure SQL Data Warehouse with SQL Server Integration Services (SSIS)</a>.</li>
<li>Using Copy activity to copy the data from on-premise system to Azure
Data Warehouse with self-host integration runtime.</li>
</ol>

<p>Data Factory SSIS-integration runtime can't get data from the on-premise SQL server. I tested run the package in Data Factory and always get the error bellow:</p>

<pre><code>Error: SSIS Error Code DTS_E_OLEDBERROR.  An OLE DB error has occurred. Error code: 0x80004005. An OLE DB record is available.  Source: ""Microsoft SQL Server Native Client 11.0""  Hresult: 0x80004005  Description: ""Login timeout expired"". An OLE DB record is available.  Source: ""Microsoft SQL Server Native Client 11.0""  Hresult: 0x80004005  Description: ""A network-related or instance-specific error has occurred while establishing a connection to SQL Server. Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online."". An OLE DB record is available.  Source: ""Microsoft SQL Server Native Client 11.0""  Hresult: 0x80004005  Description: ""Named Pipes Provider: Could not open a connection to SQL Server [53]. "".
</code></pre>

<p>As we know about Data Factory, only the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">self-host integration runtime</a> can help us connect to on-premise data source.</p>

<p>Hope this helps.</p>
"
"60655518","Getting pipeline running timestamp in Azure Data Factory","<p>How can I get the value of the following in DataFactory:</p>

<ol>
<li>Last time the pipeline was triggered</li>
<li>Current starting time of the triggered pipeline</li>
</ol>
","<azure><azure-pipelines><azure-data-lake><azure-data-factory>","2020-03-12 13:50:04","2974","2","2","60660632","<ol>
<li><p>There is no easy way. As far as I know, you cannot do that with just data factory, I'd run an Azure Function to look for that using PowerShell or Python's sdk.</p></li>
<li><p>This one is easy, you can get it using:</p>

<p>""@trigger().startTime""</p></li>
</ol>

<p>And that will give you the current starting time. Doc here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a></p>

<p>Hope this helped!</p>
"
"60655518","Getting pipeline running timestamp in Azure Data Factory","<p>How can I get the value of the following in DataFactory:</p>

<ol>
<li>Last time the pipeline was triggered</li>
<li>Current starting time of the triggered pipeline</li>
</ol>
","<azure><azure-pipelines><azure-data-lake><azure-data-factory>","2020-03-12 13:50:04","2974","2","2","60664088","<p>You could get some of this messages in Data Factory <code>monitor/pipeline run</code> page:</p>

<p><a href=""https://i.stack.imgur.com/6XPBz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6XPBz.png"" alt=""enter image description here""></a></p>

<p>It include: <code>last run time(triggered time)</code> and <code>DURACTION</code>.</p>

<p>But for now, we can not export it.</p>

<p>Hope this helps.</p>
"
"60654888","Iterate each folder in Azure Data Factory","<p>In our DataLake storage, we received unspecified amount of folders every day. Each of these folders contain at least one file.</p>

<pre><code>Example of folders:

    FolderA

    |_/2020

       |_/03

          |_/12

              |_fileA.json

        |_/04

           |_/13

               |_fileB.json

    FolderB

    |_/2020

       |_/03

          |_/12

              |_fileC.json
Folder C/...
Folder D/...
So on..
</code></pre>

<p>Now:
1. How do I iterate every folders and get the file(s) inside it?</p>

<ol start=""2"">
<li>I would also like to do 'Copy Data' from each of these files and make a single .csv file from it. What would be the best approach to achieve it?</li>
</ol>
","<azure-data-lake><azure-data-factory>","2020-03-12 13:12:26","423","0","1","60660958","<p>This can be done with a single copy activity using wildcard filtering in the source dataset, as seen here: <a href=""https://azure.microsoft.com/en-us/updates/data-factory-supports-wildcard-file-filter-for-copy-activity/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/data-factory-supports-wildcard-file-filter-for-copy-activity/</a></p>

<p>Then in the sink tab of the copy activity, select Merge Files in the Copy behavior as seen here: </p>

<p><a href=""https://i.stack.imgur.com/s6jEo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s6jEo.png"" alt=""Merge files""></a></p>

<p>If you have extra requirements, another way to do this is by using Mapping Dataflows. Mark Kromer explains a similar scenario here: <a href=""https://kromerbigdata.com/2019/07/05/adf-mapping-data-flows-iterate-multiple-files-with-source-transformation/"" rel=""nofollow noreferrer"">https://kromerbigdata.com/2019/07/05/adf-mapping-data-flows-iterate-multiple-files-with-source-transformation/</a></p>

<p>Hope this helped!</p>
"
"60652650","Filtering on multiple wildcard filenames when copying data in Data Factory","<p>I am doing a simple Copy Data Data Factory pipeline.</p>

<p>I am accessing a folder &amp; want to retrieve all files matching particular patterns.</p>

<p>I have been able to use the ""Wildcard file name"" only to match one of these.</p>

<p>Is there a way to input multiple wildcard file names?</p>

<p>I have tried;</p>

<p>(pattern1*.csv|pattern2*_pat.csv)</p>

<p>which copies nothing to the destination.</p>

<p>Any help much appreciated! </p>

<p>Thanks</p>
","<azure><copy><azure-data-factory>","2020-03-12 10:56:06","1733","0","1","60661155","<p>You can only have 1 pattern, however you should be able to use * and ? to match both: ""Allowed wildcards are: * (matches zero or more characters) and ? (matches zero or single character)"", from the official documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#copy-activity-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#copy-activity-properties</a></p>

<p>If not, you can always have multiple copy activities, each with a different pattern.</p>

<p>Hope this helped!</p>
"
"60642210","Cannot deploy TumblingWindowTrigger in ADF v2 using CI/CD","<p>Currently we have CI/CD release pipeline setup for deploying ADF V2 code from Development to Test Data Factory as per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#set-up-an-azure-pipelines-release"" rel=""nofollow noreferrer"">documentation</a>.  When running the release pipeline in Azure DevOps, it fails with following error</p>

<pre><code>    {
  ""status"": ""Failed"",
  ""error"": {
    ""code"": ""ResourceDeploymentFailure"",
    ""message"": ""The resource operation completed with terminal provisioning state 'Failed'."",
    ""details"": [
      {
        ""code"": ""DeploymentFailed"",
        ""message"": ""At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details."",
        ""details"": [
          {
            ""code"": ""BadRequest"",
            ""message"": ""{\r\n  \""error\"": {\r\n    \""code\"": \""TumblingWindowTriggerStartTimeUpdateNotAllowed\"",\r\n    \""message\"": \""Update of start time for tumbling window trigger is not allowed.null\"",\r\n    \""target\"": null,\r\n    \""details\"": null\r\n  }\r\n}""
          }
        ]
      }
    ]
  }
}
</code></pre>

<p>Is there any workaround to deploy the ARM Template to Test ADF without updating or parameterize the StartTime of tumbling window trigger?</p>

<p>Any help is much appreciated!</p>
","<azure><azure-devops><azure-pipelines-release-pipeline><azure-data-factory>","2020-03-11 18:12:30","814","2","3","64801604","<p>I did not manage to update such a trigger. But to prevent deployment issues I added a condition to the trigger, based on a parameter.</p>
<p>The parameter definition</p>
<pre><code> &quot;isNewDeployment&quot;: {
  &quot;type&quot;: &quot;bool&quot;,
  &quot;defaultValue&quot;: true
}
</code></pre>
<p>The condition</p>
<pre><code>&quot;condition&quot;: &quot;[parameters('isNewDeployment')]&quot;
</code></pre>
<p>And depending on if you want to deploy the trigger or not, adjust the parameter file of your deployment</p>
<pre><code>&quot;isNewDeployment&quot;: {
  &quot;value&quot;: false
}
</code></pre>
"
"60642210","Cannot deploy TumblingWindowTrigger in ADF v2 using CI/CD","<p>Currently we have CI/CD release pipeline setup for deploying ADF V2 code from Development to Test Data Factory as per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#set-up-an-azure-pipelines-release"" rel=""nofollow noreferrer"">documentation</a>.  When running the release pipeline in Azure DevOps, it fails with following error</p>

<pre><code>    {
  ""status"": ""Failed"",
  ""error"": {
    ""code"": ""ResourceDeploymentFailure"",
    ""message"": ""The resource operation completed with terminal provisioning state 'Failed'."",
    ""details"": [
      {
        ""code"": ""DeploymentFailed"",
        ""message"": ""At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details."",
        ""details"": [
          {
            ""code"": ""BadRequest"",
            ""message"": ""{\r\n  \""error\"": {\r\n    \""code\"": \""TumblingWindowTriggerStartTimeUpdateNotAllowed\"",\r\n    \""message\"": \""Update of start time for tumbling window trigger is not allowed.null\"",\r\n    \""target\"": null,\r\n    \""details\"": null\r\n  }\r\n}""
          }
        ]
      }
    ]
  }
}
</code></pre>

<p>Is there any workaround to deploy the ARM Template to Test ADF without updating or parameterize the StartTime of tumbling window trigger?</p>

<p>Any help is much appreciated!</p>
","<azure><azure-devops><azure-pipelines-release-pipeline><azure-data-factory>","2020-03-11 18:12:30","814","2","3","66387450","<p>It was the same issue for me.</p>
<p>Although I wrote another script, which checks for the trigger time in ARM template and then in DataFactory.</p>
<ol>
<li>If the time matches, it deploys!</li>
<li>Else it checks if the trigger was active or not.
-- If active: checks the latest trigger run time, adds in ARM template and deletes the trigger.
-- If inactive: Deletes the trigger and deploys with the new time.</li>
</ol>
<p>NOTE:  The delete is important, else you might not be able to deploy the trigger.
Im using kinda same method: to put parameters i.e. if deployment has to be done or not.</p>
<p>You might have a look in : <a href=""https://github.com/pranayyt/Azure.TumblingWindowTriggerStartTime"" rel=""nofollow noreferrer"">https://github.com/pranayyt/Azure.TumblingWindowTriggerStartTime</a></p>
<p>Atm its running smoothly in several data factory environments.</p>
<p>Hope this helps. Cheers.</p>
"
"60642210","Cannot deploy TumblingWindowTrigger in ADF v2 using CI/CD","<p>Currently we have CI/CD release pipeline setup for deploying ADF V2 code from Development to Test Data Factory as per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#set-up-an-azure-pipelines-release"" rel=""nofollow noreferrer"">documentation</a>.  When running the release pipeline in Azure DevOps, it fails with following error</p>

<pre><code>    {
  ""status"": ""Failed"",
  ""error"": {
    ""code"": ""ResourceDeploymentFailure"",
    ""message"": ""The resource operation completed with terminal provisioning state 'Failed'."",
    ""details"": [
      {
        ""code"": ""DeploymentFailed"",
        ""message"": ""At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details."",
        ""details"": [
          {
            ""code"": ""BadRequest"",
            ""message"": ""{\r\n  \""error\"": {\r\n    \""code\"": \""TumblingWindowTriggerStartTimeUpdateNotAllowed\"",\r\n    \""message\"": \""Update of start time for tumbling window trigger is not allowed.null\"",\r\n    \""target\"": null,\r\n    \""details\"": null\r\n  }\r\n}""
          }
        ]
      }
    ]
  }
}
</code></pre>

<p>Is there any workaround to deploy the ARM Template to Test ADF without updating or parameterize the StartTime of tumbling window trigger?</p>

<p>Any help is much appreciated!</p>
","<azure><azure-devops><azure-pipelines-release-pipeline><azure-data-factory>","2020-03-11 18:12:30","814","2","3","68768844","<p>This might not directly answer your question, but if you want to do yourself a favour, you should try the SQL Player ADF Extension for DevOps: <a href=""https://marketplace.visualstudio.com/items?itemName=SQLPlayer.DataFactoryTools"" rel=""nofollow noreferrer"">https://marketplace.visualstudio.com/items?itemName=SQLPlayer.DataFactoryTools</a></p>
<p>We have been using it in all our projects for CI/CD, and it especially does not suffer from the typical deployment problems with triggers when using ARM.</p>
"
"60638708","This table is a No Primary Index(NOPI) table."" on Teradata tables with a PI","<p>I am currently testing out a pipeline where I am using ""Hash"" partition option to extract from an on-prem Database. I have set up a pipeline that passes a list of tables to a ForEach activity and then I run the extract in parallel:</p>

<p><a href=""https://i.stack.imgur.com/c1icg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c1icg.png"" alt=""image""></a></p>

<p>Inside the ForEach activity I have put the following for the source tab:</p>

<p><a href=""https://i.stack.imgur.com/IV9gk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IV9gk.png"" alt=""image""></a></p>

<p>When I run the pipeline I am getting the following errors on a lot of the Teradata tables:</p>

<p>{
""errorCode"": ""2200"",
""message"": ""ErrorCode=TeradataNoPrimaryIndexTable,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=This table is a No Primary Index(NOPI) table. Please specify the partition column name in copy activity.,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,'"",
""failureType"": ""UserError"",
""target"": ""Copy data1"",
""details"": []
}</p>

<p>What confuses me is that when I check the Teradata tables I can see that there is a Primary Index. For example this is what I see for one of the tables that ""failed"" when I tried to copy it:</p>

<p><a href=""https://i.stack.imgur.com/8EWry.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8EWry.png"" alt=""image""></a></p>

<p>Am I not setting up something correct? Do I also need to create a parameter that would capture the primary index columns? Or are the errors coming from the fact that there is no primary column and the table has only an index?</p>

<p>Thank you</p>
","<azure-data-factory>","2020-03-11 14:46:20","249","0","2","60717225","<p>I think as per the blog PK is required .</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-teradata#parallel-copy-from-teradata"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-teradata#parallel-copy-from-teradata</a></p>

<p><strong>Partition option: Hash.</strong></p>

<p>During execution, Data Factory automatically detects the PK column, applies a hash against it, and copies data by partitions.</p>
"
"60638708","This table is a No Primary Index(NOPI) table."" on Teradata tables with a PI","<p>I am currently testing out a pipeline where I am using ""Hash"" partition option to extract from an on-prem Database. I have set up a pipeline that passes a list of tables to a ForEach activity and then I run the extract in parallel:</p>

<p><a href=""https://i.stack.imgur.com/c1icg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c1icg.png"" alt=""image""></a></p>

<p>Inside the ForEach activity I have put the following for the source tab:</p>

<p><a href=""https://i.stack.imgur.com/IV9gk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IV9gk.png"" alt=""image""></a></p>

<p>When I run the pipeline I am getting the following errors on a lot of the Teradata tables:</p>

<p>{
""errorCode"": ""2200"",
""message"": ""ErrorCode=TeradataNoPrimaryIndexTable,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=This table is a No Primary Index(NOPI) table. Please specify the partition column name in copy activity.,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,'"",
""failureType"": ""UserError"",
""target"": ""Copy data1"",
""details"": []
}</p>

<p>What confuses me is that when I check the Teradata tables I can see that there is a Primary Index. For example this is what I see for one of the tables that ""failed"" when I tried to copy it:</p>

<p><a href=""https://i.stack.imgur.com/8EWry.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8EWry.png"" alt=""image""></a></p>

<p>Am I not setting up something correct? Do I also need to create a parameter that would capture the primary index columns? Or are the errors coming from the fact that there is no primary column and the table has only an index?</p>

<p>Thank you</p>
","<azure-data-factory>","2020-03-11 14:46:20","249","0","2","61064393","<p>So just to give an update on this, the reason why I was having an issue is that Microsoft only applied the auto Partition option to Teradata tables that had a name of 30 characters or less.</p>

<p>The tables that I was using had table names that were greater than 30 characters and thus were failing because of a bug in ADF that could not recognize how to auto-partition it. I have raised this issue with Microsoft and they are planning on putting out a patch (will update once that is done).</p>

<p>The way I solved the above question is that I had to separate out the pipeline where if the name is less or equal to 30 characters it was sent to a ForEach activity with a Copy activity nested inside it that used the Auto-Parition option.</p>

<p>The rest of the tables (which were greater than 30 characters) were sent to a different ForEach activity where I had chosen a ""load_date"" column that we put on all of our incoming tables in Teradata. It was not the best solution, but it at least helped speed up our copying.</p>

<p>Below is a picture of how my pipeline looks:</p>

<p><a href=""https://i.stack.imgur.com/FtPtN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FtPtN.png"" alt=""enter image description here""></a></p>

<p>Hope this helps someone else that might have been having this issue.</p>
"
"60631193","How to get python notebook path from Azure databricks?","<p>I want to get exact python/scala file path which are created at workspace or user level.</p>

<p>In Azure Datafactory Python activity, I want to execute the python notebook which is part of my workspace.</p>

<p>If I upload the .py file in dbfs, ADF pipeline gets executed. </p>

<p>But I don't want to upload file in dbfs. </p>

<p>How to call python files in Python activity of Azure data factory?</p>

<p>Thank you</p>
","<azure><azure-data-factory><azure-databricks>","2020-03-11 07:37:28","921","0","1","60632593","<p>You need to use  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-notebook"" rel=""nofollow noreferrer"">Azure Databricks Notebook Activity</a> in a Data Factory pipeline runs a Databricks notebook (Python/Scala/Sql/R) in your Azure Databricks workspace.</p>

<p><strong>Note:</strong> The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python"" rel=""nofollow noreferrer"">Azure Databricks Python Activity</a> in a Data Factory pipeline runs a Python file in your Azure Databricks cluster.</p>
"
"60621440","Azure ADF - ContentMD5 field in Get Metadata activity is always null","<p>If I manually upload txt or csv files in azure blob storage, when using the get metadata activity, I always get ""contentMD5"": null, while the other fields of the output are always populated. I also tried to copy it from on prem to blob storage using azcopy,but I have same issue. I am using ADF v2.</p>

<p>screen shot here <a href=""https://i.stack.imgur.com/HrQD0.png"" rel=""nofollow noreferrer"">null ContentMD5</a></p>

<p>Any idea why would this happen?
Thanks</p>
","<metadata><azure-data-factory>","2020-03-10 16:02:01","1162","0","1","60628451","<p>Actually, that's not the Data Factory error.</p>

<p>Please check the file property in your Blob Storage. I also have one file without content-MD5:</p>

<p><a href=""https://i.stack.imgur.com/ald4O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ald4O.png"" alt=""enter image description here""></a></p>

<p>That why the result of get metadata contentMD5 will be null.</p>

<p><strong>How to solve this problem?</strong></p>

<p>I just download the <code>test.csv</code> to my computer,  delete it(Also delete blob snapshots) in the container. Then re-upload it, we can see the CONTENT-MD5 now:</p>

<p><a href=""https://i.stack.imgur.com/yZSgE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yZSgE.png"" alt=""enter image description here""></a></p>

<p>Run the get metadata activity, check the out put:</p>

<p><a href=""https://i.stack.imgur.com/ZdnlQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZdnlQ.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60616455","How to set up the high availability for Azure Data Factory with Azure Integration run-time?","<p>We plan to use the Azure Data factory to handle some of our ETL pipelines. We also planning to use Azure integration runtime since all our data sources and sinks available within the Azure and we don't have any requirements or use cases to use the self-hosted integration runtime. </p>

<p>I found a few resources that discuss how to set up HA/DR for Azure Data factory with self-hosted integration runtime but not for Azure integration runtime. I want to understand or know how to set up HA/DR for Azure data factory with Azure Integration runtime. It would be great if someone helps me with this.</p>
","<azure><azure-data-factory>","2020-03-10 11:12:30","2796","0","2","60617286","<p>Azure Integration Runtime is <strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-integration-runtime"" rel=""nofollow noreferrer"">fully managed</a></strong>. Hence I do not think there is a need to do anything about it. </p>

<p><a href=""https://i.stack.imgur.com/vcAoO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vcAoO.png"" alt=""enter image description here""></a></p>
"
"60616455","How to set up the high availability for Azure Data Factory with Azure Integration run-time?","<p>We plan to use the Azure Data factory to handle some of our ETL pipelines. We also planning to use Azure integration runtime since all our data sources and sinks available within the Azure and we don't have any requirements or use cases to use the self-hosted integration runtime. </p>

<p>I found a few resources that discuss how to set up HA/DR for Azure Data factory with self-hosted integration runtime but not for Azure integration runtime. I want to understand or know how to set up HA/DR for Azure data factory with Azure Integration runtime. It would be great if someone helps me with this.</p>
","<azure><azure-data-factory>","2020-03-10 11:12:30","2796","0","2","60628711","<p>If you want improve the performance/high availability of Azure integration runtime, you can scale the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units"" rel=""nofollow noreferrer"">Data Integration Unit(DIU)</a> OF Azure integration runtime.</p>

<p>A Data Integration Unit is a measure that represents the power (a combination of CPU, memory, and network resource allocation) of a single unit in Azure Data Factory. Data Integration Unit only applies to Azure integration runtime, but not self-hosted integration runtime.</p>

<p>For more details, you could reference this data Factory document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">Copy activity performance and scalability guide</a></p>

<p>Hope this helps.</p>
"
"60615315","Integration runtime (self-hosted) node is not registered","<p>I am using Azure data factory V2 and the problem I am facing is the Self-Hosted integration runtime is is getting Deregistered from the node automatically after running some time.</p>

<p><a href=""https://i.stack.imgur.com/lFBpb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lFBpb.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-03-10 10:04:42","4363","0","2","60631045","<p>Make sure you passing the valid ""<strong>Authentication Key</strong>, while registering Integration Runtime"".</p>

<p>There error message clearly says ""Authentication Key is invalid"".</p>

<p><a href=""https://i.stack.imgur.com/ENfLc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ENfLc.png"" alt=""enter image description here""></a></p>

<p>For more details, refer ""<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">Create and configure a self-hosted integration runtime</a>"".</p>
"
"60615315","Integration runtime (self-hosted) node is not registered","<p>I am using Azure data factory V2 and the problem I am facing is the Self-Hosted integration runtime is is getting Deregistered from the node automatically after running some time.</p>

<p><a href=""https://i.stack.imgur.com/lFBpb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lFBpb.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-03-10 10:04:42","4363","0","2","70780094","<p>Ok, i had this same error as per your screenshot.</p>
<ol>
<li>Uninstall the integration runtime on your VM / Laptop</li>
<li>Go to ADF download the manual integration runtime (NOT THE EXPRESS; For some reason it didn't work for me)</li>
<li>Install the latest version of INtegration Runtime
<a href=""https://i.stack.imgur.com/PnodO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PnodO.png"" alt=""enter image description here"" /></a></li>
<li>Restart your machine (recommended for registry purpose i guess!)
<a href=""https://i.stack.imgur.com/uY45W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uY45W.png"" alt=""enter image description here"" /></a></li>
</ol>
"
"60612901","best practice for datasets conversions for usage in AML","<p>What are the best practices for large datasets conversion? In many of the cases I deal with there is always a first step where the input dataset is converted to a format that is consumable by the training (I deal with thousands of images). The conversion script was naively created to work locally (input directory - > output directory), and we run inside an estimator (blob storage - > blob storage). Based on the guidelines here <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-datasets#mount-vs-download"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-datasets#mount-vs-download</a> it looks like is better to do download and then upload rather than mount, am I correct? A part from that what about parallel processing or distributed processing guidelines?</p>

<p>looking at this post: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf</a>, it looks like they are suggesting to use batch for custom parallel processing. If so what is the advantage of using ADF? Why not use an AML pipeline with a first stage that runs batch?</p>
","<azure-data-factory><azure-batch><azure-machine-learning-service>","2020-03-10 07:09:03","317","1","1","60624254","<p>For dataset mount-vs-download, if you are processing all data in your dataset, then download will perform better than mount. For parallel processing, there is a pipeline step specialized in it: <a href=""https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/machine-learning-pipelines/parallel-run"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/machine-learning-pipelines/parallel-run</a></p>

<p>When to use ADF v.s. AzureML for data ingestion<br>
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-ingestion"" rel=""nofollow noreferrer"">Here</a> is an article describe the pros and cons for these 2 approaches. You can use it to evaluate based on your scenario and needs.</p>
"
"60612030","Extract table from SAP BW to Azure Data Lake Gen2 using data factory","<p>I would like to know the procedure to extract table from SAP BW installed on Azure cloud to Azure data lake gen2. I want to use ADF to copy data from SAP BW to Data lake.</p>

<p>Can we connect ADF to SAP directly with SAP connector? Do I have to install Runtime Integration and any VM for this connection? What's the difference between SAP BW Open Hub connector and SAP BW via MDX?</p>

<p>Would like to hear from experts on how to extract data from SAP BW,  when SAP is also hosted on Azure. Thanks.</p>
","<azure><sap><azure-data-factory><sap-bw>","2020-03-10 05:32:48","1721","2","1","63707666","<p>I am not an expert, but the difference was explained to me by a BW person where you can use both, but with OpenHub you can run an extract on a BW query without involvement of a BW person, but performance would not be great. With MDX I believe there is additional development that would need to be set up on BW but the performance is better.</p>
<p>Also keep in mind that when I was running those queries I found it hard to parallelize it and while Microsoft docs did not provide a good example I found that what whatever I pushed to BW it was sent as a single query.</p>
<p>Alternatively my recent use case was to get data out of a table in SAP BW vs a cube so this might work.</p>
<p>I followed instructions listed out for the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sap-table"" rel=""nofollow noreferrer"">&quot;SAP Table&quot; connector</a></p>
<p>For this process to work you will need a self hosted IR (either on your laptop or a VM that is attached to an ADF) and you will need to install the following drivers:</p>
<p><a href=""https://i.stack.imgur.com/yK9Xe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yK9Xe.png"" alt=""SAP Table connector requirements"" /></a></p>
<p>To get those drivers you will probably need to reach out to your Basis team. They will also need to also create an Interface role (esp if this is your first time making this connection and you want a service account to be re-used by other processes).</p>
<p>After all of that you also need to have RFC authorizations added to this Interface. The below ones are the ones that worked for me. Microsoft website does give out a suggested RFC authorization, but those are almost on admin level and our Basis team basically did not want to do that:</p>
<p>S_RFC:
FUGR - RFC1, SYST, SYSU
FUNC - RFCPING, RFC_FUNCTION_SEARCH
ACTVT – 16</p>
<p>In addition to above we had to run a couple of tests and found that depending on the number of tables you want to pull data from they might need to add additional authorizations so that you can only read from that table.</p>
<p>The above process was the one I followed so your's might look a little different, but to make this work you need: Self Hosted IR, SAP drivers installed on those IRs, Firewall rules allowing you to access the BW system id, Interface created by Basis, then also RFC authorizations.</p>
<p>I have opened up an issue on the microsoft github documentation about the incorrect RFC authorization list: <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/60637"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/60637</a></p>
<p>Also keep in mind that the way that ADF pulls the data it first sends query to BW, BW then creates a file on its end collecting that info, the file then is sent back to the Self Hosted IR which then writes the data into a storage account through ADF. What might happen is that if the file is too large then the pipeline can fail, but not because of ADF, but because of limitations on BW side.</p>
<p>Hopefully my experience can help someone else stuck :)</p>
"
"60595027","'int' is a primitive and doesn't support nested properties: Azure Data Factory v2","<p>I am trying to find a substring of a string as a part of an activity in ADF. Say for instance I want to extract out the subsctring <code>'de'</code> out of a string <code>'abcde'</code>. I have tried: </p>

<pre><code>@substring(variables('target_folder_name'), 3, (int(variables('target_folder_name_length'))-3))
</code></pre>

<p>where <code>int(variables('target_folder_name_length'))</code> has a value of 5 and <code>variables('target_folder_name')</code> has a value of <code>'abcde'</code></p>

<p>But it gives me: <code>Unrecognized expression: (int(variables('target_folder_name_length'))-3)</code></p>

<p>On the other hand, if I try this: <code>@substring(variables('target_folder_name'), 2, int(variables('target_folder_name_length'))-3)</code></p>

<p>This gives me: <code>'int' is a primitive and doesn't support nested properties</code></p>

<p>Where am I going wrong?</p>
","<azure-data-factory>","2020-03-09 05:13:01","2915","0","2","60595383","<p>Use <strong>indexof</strong>. See my example below:</p>

<pre><code>@substring(variables('testString'), indexof(variables('testString'), variables('de')), length(variables('de')))
</code></pre>

<p><a href=""https://i.stack.imgur.com/CQODy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CQODy.png"" alt=""enter image description here""></a></p>

<p>Output of 'result' variable:</p>

<p><a href=""https://i.stack.imgur.com/piVfU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/piVfU.png"" alt=""enter image description here""></a></p>
"
"60595027","'int' is a primitive and doesn't support nested properties: Azure Data Factory v2","<p>I am trying to find a substring of a string as a part of an activity in ADF. Say for instance I want to extract out the subsctring <code>'de'</code> out of a string <code>'abcde'</code>. I have tried: </p>

<pre><code>@substring(variables('target_folder_name'), 3, (int(variables('target_folder_name_length'))-3))
</code></pre>

<p>where <code>int(variables('target_folder_name_length'))</code> has a value of 5 and <code>variables('target_folder_name')</code> has a value of <code>'abcde'</code></p>

<p>But it gives me: <code>Unrecognized expression: (int(variables('target_folder_name_length'))-3)</code></p>

<p>On the other hand, if I try this: <code>@substring(variables('target_folder_name'), 2, int(variables('target_folder_name_length'))-3)</code></p>

<p>This gives me: <code>'int' is a primitive and doesn't support nested properties</code></p>

<p>Where am I going wrong?</p>
","<azure-data-factory>","2020-03-09 05:13:01","2915","0","2","60606118","<p>Since your preceding values are static, you can use below dynamic expression to achieve the substring value as per your requirement.</p>

<pre><code>@substring(variables('varInputFolderName'), 3, sub(length(variables('varInputFolderName')), 3))
</code></pre>

<p>Where varInputFolderName = String = abcde as per this sample.</p>

<p>Here is the pipeline JSON payload for this sample. You can play around with it for further testing. </p>

<pre><code>{
    ""name"": ""pipeline_FindSubstring"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""setSubstringValue"",
                ""type"": ""SetVariable"",
                ""dependsOn"": [],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""variableName"": ""varSubstringOutput"",
                    ""value"": {
                        ""value"": ""@substring(variables('varInputFolderName'), 3, sub(length(variables('varInputFolderName')), 3))"",
                        ""type"": ""Expression""
                    }
                }
            }
        ],
        ""variables"": {
            ""varInputFolderName"": {
                ""type"": ""String"",
                ""defaultValue"": ""abcde""
            },
            ""varSubstringOutput"": {
                ""type"": ""String""
            }
        },
        ""annotations"": []
    }
}
</code></pre>
"
"60594428","ADF delete activity with python- Unable to create logging","<p>I am currently creating adfv2 delete activity with python using below code</p>

<p>My python code looks like below</p>

<pre><code>#Create a delete activity 

blob_ls_name = 'AzureBlobLS'
ds_name = 'Dataset_Test'
dataset_name = DatasetReference(reference_name= ds_name)
ds_ls = LinkedServiceReference(reference_name=blob_ls_name)
logsettings = LogStorageSettings(linked_service_name = ds_ls,  path = 'mycontainer/path')
act_name = 'CleanUp_Dest_Folder'
act_delete = DeleteActivity(name = act_name , dataset = dataset_name, log_storage_settings = logsettings)
</code></pre>

<p>Even after creating logsettings, when the delete activity is created the log settings are not properly applied in the activity and it is selecting nothing.</p>
","<python><azure-data-factory>","2020-03-09 03:38:42","118","0","1","60597625","<p>I can reproduce your issue, you missed the <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.deleteactivity?view=azure-python#parameters"" rel=""nofollow noreferrer""><code>enable_logging = True</code></a> in the last line, add it like below, then it will work fine.</p>
<pre><code>act_delete = DeleteActivity(name = act_name , dataset = dataset_name, enable_logging = True, log_storage_settings = logsettings )
</code></pre>
<hr />
<p><strong>My test sample:</strong></p>
<p>It creates a pipeline with a delete activity.</p>
<pre><code>from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *

subscription_id = 'xxxxx'
credentials = ServicePrincipalCredentials(client_id='xxxx', secret='xxxx', tenant='xxx')
adf_client = DataFactoryManagementClient(credentials, subscription_id)

blob_ls_name = 'AzureBlobStorage2'
ds_name = 'Binary1'
dataset_name = DatasetReference(reference_name= ds_name)
ds_ls = LinkedServiceReference(reference_name=blob_ls_name)
logsettings = LogStorageSettings(linked_service_name = ds_ls,  path = 'test/d2')
act_name = 'CleanUp_Dest_Folder'
act_delete = DeleteActivity(name = act_name , dataset = dataset_name, enable_logging = True, log_storage_settings = logsettings )

rg_name = 'xxxx'
df_name = 'joyfactory'
p_name = 'Pipeline1234'
params_for_pipeline = {}
p_obj = PipelineResource(
    activities=[act_delete], parameters=params_for_pipeline)
p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)
print(p.activities[0].log_storage_settings.linked_service_name)
</code></pre>
<p><a href=""https://i.stack.imgur.com/RN7fY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RN7fY.png"" alt=""enter image description here"" /></a></p>
<p>Check in the portal:</p>
<p><a href=""https://i.stack.imgur.com/7DSFJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7DSFJ.png"" alt=""enter image description here"" /></a></p>
"
"60594129","Azure Data Factory: Data Lifecycle Management and cleaning up stale data","<p>I'm working on a requirement to reduce the cost of data storage. It includes the following tasks:</p>

<ol>
<li>Being able to remove files from File Share and blobs from Blob Storage, based on their last modified date.</li>
<li>Being able to change the tier of individual blobs, based on their last modified date.</li>
</ol>

<p>Does Azure Data Factory has built-in activities to take care of these tasks? What's the best approach for automating the clean-up process?</p>
","<data-cleaning><azure-data-factory><azure-blob-storage>","2020-03-09 02:43:46","373","0","1","60598000","<blockquote>
  <p>1.Being able to remove files from File Share and blobs from Blob Storage, based on their last modified date.</p>
</blockquote>

<p>This requirement could be implemented by ADF built-in method: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity"" rel=""nofollow noreferrer"">Delete Activity</a>.</p>

<p>Please create a blob storage dataset and just refer to this example and configure the range of last modify date :<a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity#clean-up-the-expired-files-that-were-last-modified-before-201811"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/delete-activity#clean-up-the-expired-files-that-were-last-modified-before-201811</a></p>

<p>Please consider some back up strategy for some accidents because: <a href=""https://i.stack.imgur.com/wpXTB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wpXTB.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>2.Being able to change the tier of individual blobs, based on their last modified date.</p>
</blockquote>

<p>No built-in feature to complete this in ADF. However,while i notice that your profile shows you are .net maker, so follow this case:<a href=""https://stackoverflow.com/questions/48870035/azure-java-sdk-set-block-blob-to-cool-storage-tier-on-upload/48899909#48899909"">Azure Java SDK - set block blob to cool storage tier on upload</a> so that you could know the Tier could be changed in sdk code. That's easy to create an Azure Function to do such simple task. Moreover,ADF supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a>.</p>
"
"60583806","How to get static IP for Azure Data Factory Pipeline?","<p>I have a workflow/pipeline in Azure which connects to third party FTP (via linked service) and get files on regular basis.</p>
<p>It was all working fine, till third party introduced white listing of IP's, and now they are asking me to provide static IP's or range. Unless white listed, I will not be able to get my pipeline working.</p>
<p>Now my question is. How to provide my IP address?</p>
<p>I know which region my ADF works in (North-Europe) and I know my linked service uses AutoResolve-IR.</p>
<p>Will solution be to go with, self hosted IR? If yes, then how will I know the IP of my IR?</p>
","<azure><ftp><azure-data-factory>","2020-03-08 01:42:32","3605","0","1","60588692","<p>They seem to support static IP addresses for Data Factory recently. Announcement: <a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/azure-data-factory-now-supports-static-ip-address-ranges/ba-p/1117508"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/azure-data-factory-now-supports-static-ip-address-ranges/ba-p/1117508</a></p>

<p>Here is the list of IPs for North Europe as per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses</a></p>

<p><a href=""https://i.stack.imgur.com/rN98p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rN98p.png"" alt=""enter image description here""></a></p>
"
"60571433","Azure Key Vault incorrectly creating Compound Identity","<p>In Azure Key Vault policies I wanted to add ADF.
Given below is the command(executed from pipeline). But after creation it is showing as 'compound identity' with an 'on behalf of' text and it is not working. When I manually add ADF to policies it shows as 'application' and it works. How can I make the powershell create 'Application' identity? </p>

<pre><code>Set-AzKeyVaultAccessPolicy -VaultName $keyVaultName -PermissionsToSecrets get -ApplicationId $appId -ObjectId $objectId
</code></pre>

<p>Full script</p>

<pre><code>$rgName='myRG'
$storageAccountName='MyStorage'
$secretName='myKey'
$keyVaultName='myKv'
$adfName='myADF'

Write-Host ""Adding data lake storage Key to key vault""

$storageAccountKey = (Get-AzStorageAccountKey  -ResourceGroupName $rgName -Name $storageAccountName).Value[0]
$secretVal = ConvertTo-SecureString -String $storageAccountKey -AsPlainText -Force
Set-AzKeyVaultAccessPolicy -VaultName $keyVaultName -PermissionsToSecrets get -ApplicationId $appId -ObjectId $objectId

Write-Host ""completed adding key""

Write-Host ""Adding access policy to key vault""
$objectId=(Get-AzureRmDataFactoryV2 -ResourceGroupName $rgName -Name $adfName).Identity.PrincipalId
$appId = (Get-AzureRmADServicePrincipal -ObjectId $objectId).ApplicationId
</code></pre>

<p><a href=""https://i.stack.imgur.com/3beup.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3beup.png"" alt=""enter image description here""></a></p>
","<azure><azure-pipelines><azure-powershell><azure-data-factory><azure-keyvault>","2020-03-06 20:50:53","966","1","1","60571787","<p>Wrong command was given. Given below is the correct. <a href=""https://learn.microsoft.com/en-us/powershell/module/az.keyvault/set-azkeyvaultaccesspolicy?view=azps-3.5.0"" rel=""nofollow noreferrer"">Explained in 'Example 2: Grant permissions for an application service principal to read and write secrets'</a></p>

<pre><code>Set-AzKeyVaultAccessPolicy -VaultName $keyVaultName -ServicePrincipalName $appId -PermissionsToSecrets Get
</code></pre>
"
"60567336","Compare Table Sizes to Trigger Pipeline in Azure Data Factory","<p>I currently have 2 Lookup Activities that both return total row counts, one for a Temp table and one for a current table that the TEMP will replace. I want to be able to compare these two values, determine if the new table is within 25% of the old tables row count and if it is, to trigger the pipeline to move the tables. I have not been able to do anything with the row counts. I was setting the results at variables, but the dynamic content I add to try and determine the TEMP table size does not work. Any ideas on how I could take the row counts and then turn them in to usable variables in Azure Data Factory?</p>
","<azure-data-factory>","2020-03-06 15:40:39","306","0","1","60594015","<p>Actually, you can compare the two <code>Lookup</code> outputa in one <code>[if-condition][1]</code>, then add the active in the true or false condition:</p>

<p>For example, I compare the row count between table <code>test3</code> and <code>test</code>:</p>

<p>Lookup1: test3 row count:</p>

<p><a href=""https://i.stack.imgur.com/UJWnS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UJWnS.png"" alt=""enter image description here""></a></p>

<p>Lookup2 test row count:</p>

<p><a href=""https://i.stack.imgur.com/tnxxt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tnxxt.png"" alt=""enter image description here""></a></p>

<p>Compare the two output row count in <code>if-condition</code> active:</p>

<p>I just compare the row count expression test3 > test:</p>

<pre><code>@activity('Lookup1').output&gt;activity('Lookup2').output
</code></pre>

<p><a href=""https://i.stack.imgur.com/cfL0L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cfL0L.png"" alt=""enter image description here""></a></p>

<p>Then you could add the active to move the table if condition true or false.</p>

<p>The whole pipeline preview:</p>

<p><a href=""https://i.stack.imgur.com/XjV6X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XjV6X.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60565233","Find String Length in Azure Data Factory v2","<p>I'm starting off with ADFv2 and am facing an issue while trying to find out the length of a string. Although the portal says <code>length('abc')</code> should work, it is actually not working. What is the workaround for this?</p>

<p>PFB error snapshot:
<a href=""https://i.stack.imgur.com/weGFk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/weGFk.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-03-06 13:37:21","4161","1","1","60570850","<p>If the variable is of type String, just convert the value to a string using this: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#string"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#string</a></p>

<pre><code>string(length('abc'))
</code></pre>

<p>Hope this helped!</p>
"
"60558731","get the latest added file in a folder [Azure Data Factory]","<p>Inside the Data Lake, We have a folder that basically contains the files pushed by external source every day. However, we wanted to only process the latest added file in that folder. 
Is there any way to achieve that with Azure Data Factory?</p>
","<azure><azure-data-factory><azure-data-lake><last-modified>","2020-03-06 06:34:39","10794","4","2","60558836","<p>You could set <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#azure-data-lake-store-as-source"" rel=""noreferrer"">modifiedDatetimeStart and modifiedDatetimeEnd</a> to filter the files in the folder when you use ADLS connector in copy activity.</p>

<p>Maybe it has two situations:</p>

<p>1.The data was pushed by external source <strong>in the schedule</strong>,you are suppose to know the schedule time to configure.</p>

<p>2.The frequency is <strong>random</strong>,then maybe you have to log the pushing data time in another residence,then pass the time as parameter into copy activity pipeline before you execute it.</p>

<hr>

<p>I try to provide a flow for you in ADF pipelines as below:</p>

<p>My sample files in same folder:</p>

<p><a href=""https://i.stack.imgur.com/a2RFh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/a2RFh.png"" alt=""enter image description here""></a></p>

<p>Step1,create two variables, maxtime and filename:</p>

<p>maxtime is the critical datetime of specific date, filename is empty string.</p>

<p><a href=""https://i.stack.imgur.com/a2RFh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/a2RFh.png"" alt=""enter image description here""></a></p>

<p>Step2, use GetMetadata Activity and ForEach Activity to get the files under folder.</p>

<p><a href=""https://i.stack.imgur.com/QzjeL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QzjeL.png"" alt=""enter image description here""></a> </p>

<p>GetMetadata 1 configuration:</p>

<p><a href=""https://i.stack.imgur.com/Lor2Z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Lor2Z.png"" alt=""enter image description here""></a></p>

<p>ForEach Activity configuration:</p>

<p><a href=""https://i.stack.imgur.com/3aOAr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3aOAr.png"" alt=""enter image description here""></a></p>

<p>Step3: Inside ForEach Activity,use GetMetadata and If-Condition, the structure as below:</p>

<p><a href=""https://i.stack.imgur.com/JesM6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JesM6.png"" alt=""enter image description here""></a></p>

<p>GetMetadata 2 configuration:</p>

<p><a href=""https://i.stack.imgur.com/HJXdj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/HJXdj.png"" alt=""enter image description here""></a></p>

<p>If-Condition Activity configuration:</p>

<p><a href=""https://i.stack.imgur.com/AgrEl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AgrEl.png"" alt=""enter image description here""></a></p>

<p>Step4: Inside If-Condition True branch,use Set Variable Activity:</p>

<p><a href=""https://i.stack.imgur.com/f8w2w.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/f8w2w.png"" alt=""enter image description here""></a></p>

<p>Set variable1 configuration:</p>

<p><a href=""https://i.stack.imgur.com/ykTV8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ykTV8.png"" alt=""enter image description here""></a></p>

<p>Set variable2 configuration:</p>

<p><a href=""https://i.stack.imgur.com/TULkX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TULkX.png"" alt=""enter image description here""></a></p>

<p>All of above steps aim to finding the latest fileName, the variable fileName is exactly target.</p>

<hr>

<p>Addition for another new dataset in GetMetadata 2</p>

<p><a href=""https://i.stack.imgur.com/Fe97z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Fe97z.png"" alt=""enter image description here""></a></p>
"
"60558731","get the latest added file in a folder [Azure Data Factory]","<p>Inside the Data Lake, We have a folder that basically contains the files pushed by external source every day. However, we wanted to only process the latest added file in that folder. 
Is there any way to achieve that with Azure Data Factory?</p>
","<azure><azure-data-factory><azure-data-lake><last-modified>","2020-03-06 06:34:39","10794","4","2","60558880","<p>You can make use of the <strong>Modified datetime start</strong> and <strong>Modified datetime</strong> end fields as per shown in below screenshot. </p>

<p>The example here shows get files from 24 hours from current datetime.</p>

<p><a href=""https://i.stack.imgur.com/jORvX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jORvX.png"" alt=""enter image description here""></a></p>
"
"60558200","DELETE using PreCopy Script in CopData activity","<p>I have simple copy data activity with source and destination as a table in Azure Data Factory, Before inserting I'm having delete script in the pre-copy script option. The Delete should be done on the basis of parameters passed to the pipeline.
I tried this way but getting error.</p>

<p>DELETE FROM [dbo].[StgMetricLoad] where TransactionKey in(pipeline().parameters.TransactionKey)</p>
","<azure><azure-sql-database><etl><azure-data-factory>","2020-03-06 05:40:51","1335","0","1","60597788","<p>Per my experience,you can't merge pipeline string parameter into sql string like that directly. This should be configured as dynamic content with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">@cancat</a> built-in function.</p>

<p>I tested it in the Set Variable Activity:</p>

<pre><code>@concat('DELETE FROM [dbo].[StgMetricLoad] where TransactionKey in(',
pipeline().parameters.keystring,
')')
</code></pre>

<p><a href=""https://i.stack.imgur.com/E2aKK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E2aKK.png"" alt=""enter image description here""></a></p>

<p>Test Output:</p>

<p><a href=""https://i.stack.imgur.com/u56o8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u56o8.png"" alt=""enter image description here""></a></p>
"
"60556481","Unable to map lookup activity output to Copy Activity Mapping in ADF","<p>I am new to the ADF.</p>

<p>While I am trying to use Copy activity for moving data from  API Call output to Blob Json, I am unable to use Lookup output. I am trying to map the fields explicitly in Mapping using @item().SiteID. But JSON output returns only with input fields (not the derived fields). Can someone help me to let me know how to achieve this?</p>

<p>Can I use Copy activity in For Each activity <code>(@activity('LookupAvailableChannelListForExport').output.value)</code>  to pass Lookup output value <code>(@item().siteID)</code>in mapping between  source and sink? </p>
","<azure><azure-data-factory>","2020-03-06 01:50:02","251","0","1","60751752","<p>As i know, the output of Look Up Activity can't be source data in copy activity,even mapping between source and sink. Acutally, Look Up activity prefers the following usage according to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">official document</a>:</p>

<blockquote>
  <p>Dynamically determine which objects to operate on in a subsequent
  activity, instead of hard coding the object name. Some object examples
  are files and tables.</p>
</blockquote>

<p>I think the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#source-dataset-for-copy-activity"" rel=""nofollow noreferrer"">example</a> from above link is a good interpretation.You could see that the output of Look Up activity is configured as dynamic sql db source dataset table name.Not the data in source.</p>

<p>Then back to your requirement,i think you could configure the source dataset as root folder if the files are stored in the same directory with same schema. And keep this option is selected so that all the data in all files will be grabbed.</p>

<p><a href=""https://i.stack.imgur.com/Ng8HI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ng8HI.png"" alt=""enter image description here""></a></p>

<p>If you want to implement some variant of source data, copy activity can't cover it but <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-data-flow-activity"" rel=""nofollow noreferrer"">data flow activity</a> could.You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived column</a>.Such as <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column#build-schemas-in-output-schema-pane"" rel=""nofollow noreferrer"">resetting the Json structure</a>.</p>
"
"60554954","Why is ADF failing the SSL handshake with mongodb.net?","<p>For the past few days, we've been getting errors during a daily pipeline run. The errors are similar to this:</p>

<blockquote>
  <p>Message:Failure happened on 'Source' side. 
  ErrorCode=UserErrorFailedToConnectOdbcSource,'
  Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
  Message= Error from MongoDB Client: No suitable servers found
  (<code>serverSelectionTryOnce</code> set):  TLS handshake failed:
  error:1416F086:SSL routines:tls_process_server_certificate:
  certificate verify failed calling ismaster on
  '.mongodb.net:27017'</p>
</blockquote>

<p>I am able to connect to the Mongo database using Mongo CLI. I used Wireshark to confirm the SSL handshake is working fine. It shows that the Mongo cert is issued by ‘Let’s Encrypt' (<a href=""https://letsencrypt.org/"" rel=""nofollow noreferrer"">https://letsencrypt.org/</a>). The ‘Let’s Encrypt’ cert is included in the list of certs and it issued by ‘DST Root CA X3’, which is in the trusted root certs on my computer. Interestingly enough, the Mongo cert was just renewed a few days ago.</p>

<p>Just for fun (see '...verify calling ismaster...' in the message above), in the pipeline configuration, I updated the datasource dataset to ‘Allow Self-Signed Server Cert’ and the connection started working.</p>

<p>I’ll set that flag and allow the pipeline to run a few days to see if it fixes it. But I have no explanation for why this would work other than Azure has a weird way of implementing SSL.</p>

<p>Is there a way to see the list of trusted certs on the Azure server? Is there a way to capture the packets of the SSL handshake on the Azure side?</p>
","<mongodb><ssl><azure-data-factory>","2020-03-05 22:35:28","452","0","1","60764501","<p>Microsoft suggested I upgrade the MongoDB Collection dataset type to MongoDB Collection V2. The pipeline I was working with had been developed a while back. The V2 version does not have 'Allow Self-Signed Server Cert'. This turned out to work for me though.</p>
"
"60551744","Selecting 'Merge Files' for 'Copy Behavior' distorting file name in ADF","<p>I am new to Azure Data Factory and am encountering a problem where in the 'Sink' section of my ADF pipeline, if I specify 'Merge Files' as the 'Copy Behavior', my output file name is getting distorted. What would have otherwise taken the same name as my input file, the output file name is now taking a name like 'data_followed_by_some_random_space_separated_number'. How can I fix this? Thanks in advance.</p>
","<azure-data-factory>","2020-03-05 18:17:59","1510","1","1","60561479","<p>Yes, this is expected behavior. You need to know the meaning of this operation, it can copy multiple files into one file. If you want to specify its name, you should specify the final name when setting the copy behavior. I found that you mentioned the word ""distortion"", so I think what you really want to do is actually copy the corresponding file, then you should not choose the copy behavior (that is, specify the copy behavior as ""None"")</p>
"
"60536920","Azure Data Factory WebHook Fails after 1 minute","<p>We have a few Azure Functions that calls an API endpoint that takes >230 seconds (the maximum runtime for Azure Function call from ADF). The work around we found was to use the Webhook activity and using the callBackUri. But for whatever reason, the webhook always fails at 00:01:01 with a BadRequest Error:</p>

<p>BadRequestError:<br>
<a href=""https://i.stack.imgur.com/RRfNF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RRfNF.png"" alt=""BadRequestError""></a></p>

<p>If the function completes within that minute, the callback is working correctly and runs fine.</p>

<p>The WebHook's Timeout is set to 10 minutes (00:10:00), but after 1 minute it will raise a BadRequest error. The Azure function continues to run in the background and will successfully complete it's task, but my pipeline is now broken and not continue to the next step.</p>

<p>I cannot use Durable Azure Functions, as that is not yet supported in Python Azure Functions.</p>
","<azure-data-factory>","2020-03-05 01:17:01","3318","4","2","60628389","<p>After some further invetigation, the 1 min timeout error is expected. Reading the docs, for long running calls, the activity expects a 202 (Accepted) responce within the minute.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity#additional-notes"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity#additional-notes</a></p>

<p>Details on the Asynchronous Request-Reply pattern (and sample c# code) is avalable here:
<a href=""https://learn.microsoft.com/en-us/azure/architecture/patterns/async-request-reply"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/architecture/patterns/async-request-reply</a></p>
"
"60536920","Azure Data Factory WebHook Fails after 1 minute","<p>We have a few Azure Functions that calls an API endpoint that takes >230 seconds (the maximum runtime for Azure Function call from ADF). The work around we found was to use the Webhook activity and using the callBackUri. But for whatever reason, the webhook always fails at 00:01:01 with a BadRequest Error:</p>

<p>BadRequestError:<br>
<a href=""https://i.stack.imgur.com/RRfNF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RRfNF.png"" alt=""BadRequestError""></a></p>

<p>If the function completes within that minute, the callback is working correctly and runs fine.</p>

<p>The WebHook's Timeout is set to 10 minutes (00:10:00), but after 1 minute it will raise a BadRequest error. The Azure function continues to run in the background and will successfully complete it's task, but my pipeline is now broken and not continue to the next step.</p>

<p>I cannot use Durable Azure Functions, as that is not yet supported in Python Azure Functions.</p>
","<azure-data-factory>","2020-03-05 01:17:01","3318","4","2","63392913","<p>I managed to use a work around in the Azure Function itself by using -Timeout param. Details here: <a href=""https://stackoverflow.com/questions/63376930/using-azure-data-factorys-web-hook-activity-to-invoke-poll-azure-powershell-fun/63388120#63388120"">Using Azure Data Factory&#39;s Web Hook Activity to invoke/poll Azure Powershell Function App times out after one minute</a></p>
<p>Sample running code below:</p>
<pre><code>$Body = @{
            callbackUri = $Request.Body.callBackUri;
        } | ConvertTo-Json
try{
    # This API will be responsible for issuing the call back after it has finished the long running process
    $output = Invoke-RestMethod -Method Post -Body $Body -Uri $funcapp2Url -ContentType 'application/json' -TimeoutSec 1
}
catch {
    Write-Output $_
}

# Return HTTP 202 immediately
Push-OutputBinding -Name Response -Value ([HttpResponseContext]@{
    StatusCode = [HttpStatusCode]::Accepted
    Body = &quot;Wait for callback&quot;
}) -Clobber
</code></pre>
"
"60536173","Is there a ""load from Outlook attachment"" pipeline for Azure Data Factory?","<p>Looking for information on how to load data received over email (attached as <code>.xslx</code> or <code>.csv</code>), and load them into data storage with Azure Data Factory.</p>

<p>I've been unable to search for a solution --- all I can find with keywords ""Data Factory"" and ""email"" will direct me to questions like ""how to set up email notifications for data factory.""</p>

<p>Any pointers on which pipeline module to use or any related articles are greatly appreciated.</p>

<p>Thanks!</p>
","<azure><azure-data-factory>","2020-03-04 23:26:13","457","0","1","60539247","<p>Actually,ADF copy activity supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-office-365#approving-new-data-access-requests"" rel=""nofollow noreferrer"">office 365 connector</a>.</p>

<p><a href=""https://i.stack.imgur.com/IPMhl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IPMhl.png"" alt=""enter image description here""></a></p>

<p>You could follow above link to create linkedService and DateSet.</p>

<p><a href=""https://i.stack.imgur.com/9YNI7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9YNI7.png"" alt=""enter image description here""></a></p>

<p>When you create Linkedservice,you need to grant Mail Read permission to sp id and secret.</p>

<p><a href=""https://i.stack.imgur.com/YQJbX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YQJbX.png"" alt=""enter image description here""></a></p>

<p>In dataset,you could define below properties in the structure json.</p>

<p><a href=""https://i.stack.imgur.com/qE9Sl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qE9Sl.png"" alt=""enter image description here""></a></p>

<p>Then you could set blob storage as sink dataset of copy activity which i think you already know.</p>
"
"60536131","Do pipeline variables persist between runs?","<p>I'm doing a simple data flow pipeline between 2 cosmos dbs. The pipeline starts with the dataflow, which grabs the pipeline variable ""LastPipelineStartTime"" and passes that parameter to the dataflow for the query to use in order to get all new data where c._ts >= ""LastPipelineStartTime"". Then, on data flow success, updates the variable via Set Variable to the pipeline.TriggerTime(). Essentially so I'm always grabbing new data between pipeline runs.</p>

<p>My question is: it looks like the variable during each debug run reverts back to its Default Value of 0, and instead grabs everything each time. Am I misunderstanding or using pipeline variables wrong? Thanks!</p>
","<azure-data-factory>","2020-03-04 23:21:38","718","2","1","60540147","<p>As i know,the variable which is set in the <code>Set Variable Activity</code> has it's own life cycle: during current execution of pipeline.Any change of variable can't persist until next execution stage.</p>

<p>To implement your needs,pls refer to my workarounds as below:</p>

<p>1.If you execute ADF pipeline in the schedule,you could just pass the schedule time as parameter into it to make sure you grab new data.</p>

<p>2.If the frequency is random,persist the trigger time into other residence(e.g. simple file in the blob storage),before data flow activity,use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer""><code>LookUp Activity</code></a> to grab that time from blob storage file.</p>
"
"60531480","Increasing concurrency in Azure Data Factory","<p>We have a parent pipeline that gets a list of tables and feeds it into a ForEach. Within the ForEach we then call another pipeline passing in some config, this child pipeline moves the data for the table it is passed as config.</p>

<p>When we run this at scale I often see 20 or so instances of the child pipeline created in the monitor. All but 4 will be ""Queued"", the other 4 are executing as ""In progress"" . I can't seem to find any setting for this limit of 4. We have several hundred pipelines to execute and I really could do with it doing more than 4 at a time. I have set concurrency as 20 throughout the pipelines and tasks, hence we get 20 instances fired up. But I can't figure out what it is I need to twiddle to get more than 4 executing at the same time.</p>

<p>The ForEach looks like this </p>

<p><a href=""https://i.stack.imgur.com/Xdhe8.png"" rel=""nofollow noreferrer"">activities in ForEach loop look like this</a></p>

<p>many thanks</p>
","<azure-data-factory>","2020-03-04 17:20:17","3835","0","2","60541781","<p>It seems max 20 loop iteration can be executed at once in parallel.
The documentation is however a bit unclear. </p>

<p>The BatchCount setting that controls this have max value to 50, default 20. But in the documentation for isSequential it states maximum is 20. </p>

<p>Under Limitations and workarounds, the documentation states:
""The ForEach activity has a maximum batchCount of 50 for parallel processing, and a maximum of 100,000 items.""</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity</a></p>
"
"60531480","Increasing concurrency in Azure Data Factory","<p>We have a parent pipeline that gets a list of tables and feeds it into a ForEach. Within the ForEach we then call another pipeline passing in some config, this child pipeline moves the data for the table it is passed as config.</p>

<p>When we run this at scale I often see 20 or so instances of the child pipeline created in the monitor. All but 4 will be ""Queued"", the other 4 are executing as ""In progress"" . I can't seem to find any setting for this limit of 4. We have several hundred pipelines to execute and I really could do with it doing more than 4 at a time. I have set concurrency as 20 throughout the pipelines and tasks, hence we get 20 instances fired up. But I can't figure out what it is I need to twiddle to get more than 4 executing at the same time.</p>

<p>The ForEach looks like this </p>

<p><a href=""https://i.stack.imgur.com/Xdhe8.png"" rel=""nofollow noreferrer"">activities in ForEach loop look like this</a></p>

<p>many thanks</p>
","<azure-data-factory>","2020-03-04 17:20:17","3835","0","2","60544828","<p>I think I have found it. On the child Pipeline (the one that is being executed inside the ForEach loop) on the General Tab is a concurrency setting. I had this set to 4. When I increased this to 8 I got 8 executing, and when I increased it to 20 I got 20 executing.</p>
"
"60531401","Validate files in a Folder with Control File","<p>I am looking to validate the list of filenames present in a Control File with below mentioned structure and check if those files are present in the Folder using Azure Data Factory.</p>

<p>Control File Structure: SerialNo, FileName, RecordCount.</p>

<p>Folder Path: companysftp.xyz.io</p>

<p>So for example: If the control File contains,</p>

<p>1 data.csv 124</p>

<p>2 productdetails.csv 50 </p>

<p>We need to check if the data.csv and productdetails.csv is present in the folder path mentioned above.</p>

<p>Thanks in advance.
Kind regards,
Arjun Rathinam</p>
","<azure><azure-data-factory>","2020-03-04 17:15:30","122","0","1","60537325","<p>Data Factory doesn't support Control File. Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview#supported-data-stores"" rel=""nofollow noreferrer"">Supported data stores</a>.</p>

<p>In data Factory, only <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get Matadata</a> can help us list all the filename. To see: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#supported-connectors"" rel=""nofollow noreferrer"">Supported connectors</a>.</p>

<ol>
<li><code>Get Matadata</code> all the file name from the source folder.</li>
<li>Using <code>if-condition</code> in Foreach to filter the filename <code>data.csv</code>
and <code>productdetails.csv</code> is exist.</li>
</ol>

<p>For example:</p>

<p>1.Get all the file name in the container backup:
<a href=""https://i.stack.imgur.com/zU5f4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zU5f4.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Q0WtT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q0WtT.png"" alt=""enter image description here""></a></p>

<p>2.Foreach <code>items</code> settings: send all the filename to Foreach:</p>

<pre><code>@activity('Get Metadata1').output.childitems
</code></pre>

<p><a href=""https://i.stack.imgur.com/XY6Dn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XY6Dn.png"" alt=""enter image description here""></a></p>

<p>3  using <code>if-condition</code> expression to filter the filename:</p>

<pre><code>@or(equals(item().name,'data.csv'),equals(item().name,'productdetails.csv'))
</code></pre>

<p>This expression is to filter if the name equsals <code>data.csv</code> or <code>productdetails.csv</code>,return <code>true/false</code>.</p>

<p><a href=""https://i.stack.imgur.com/8EUo1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8EUo1.png"" alt=""enter image description here""></a></p>

<p>3.Then you can add the active in true condition and false condition.</p>

<p>Hope this helps.</p>
"
"60527896","How to copy data from an a csv to Azure SQL Server table?","<p>I have a dataset based on a csv file. This exposes a data as follows:</p>

<pre><code>Name,Age
John,23
</code></pre>

<p>I have an Azure SQL Server instance with a table named: [People]
This has columns</p>

<p>Name, Age</p>

<p>I am using the Copy Data task activity and trying to copy data from the csv data set into the azure table. 
There is no option to indicate the table name as a source. Instead I have a space to input a Stored Procedure name?</p>

<p>How does this work? Where do I put the target table name in the image below?<a href=""https://i.stack.imgur.com/sWKpD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sWKpD.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-03-04 14:05:51","245","0","2","60528368","<p>You should DEFINITELY have a table name to write to.  If you don't have a table, something is wrong with your setup.  Anyway, make sure you have a table to write to; make sure the field names in your table match the fields in the CSV file.  Then, follow the steps outlined in the description below.  There are several steps to click through, but all are pretty intuitive, so just follow the instructions step by step and you should be fine.</p>

<p><a href=""http://normalian.hatenablog.com/entry/2017/09/04/233320"" rel=""nofollow noreferrer"">http://normalian.hatenablog.com/entry/2017/09/04/233320</a></p>
"
"60527896","How to copy data from an a csv to Azure SQL Server table?","<p>I have a dataset based on a csv file. This exposes a data as follows:</p>

<pre><code>Name,Age
John,23
</code></pre>

<p>I have an Azure SQL Server instance with a table named: [People]
This has columns</p>

<p>Name, Age</p>

<p>I am using the Copy Data task activity and trying to copy data from the csv data set into the azure table. 
There is no option to indicate the table name as a source. Instead I have a space to input a Stored Procedure name?</p>

<p>How does this work? Where do I put the target table name in the image below?<a href=""https://i.stack.imgur.com/sWKpD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sWKpD.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-03-04 14:05:51","245","0","2","60559908","<p>You can add records into the SQL Database table directly without stored procedures, by configuring the table value on the <strong>Sink Dataset</strong> rather than the <strong>Copy Activity</strong> which is what is happening.</p>

<p>Have a look at the below screenshot which shows the <strong>Table</strong> field within my dataset.</p>

<p><a href=""https://i.stack.imgur.com/v1CbU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v1CbU.png"" alt=""enter image description here""></a></p>
"
"60526780","foreach actitivity in azure data factory","<p>Is there any method available in Azure data factory to exit the for each loop. I am using a for each loop to iterate through files and to process it. But when the copy activity placed inside the loop fails, the loop executes multiple times to reprocess the failed file. I think it has to do something with the number of files available in the Get meta data array. Can anyone suggest a method to resolve this issue.</p>

<p>Regards,
Sandeep</p>
","<azure-data-factory>","2020-03-04 13:05:02","558","-1","2","60537393","<p>Like @Jay Gong said that Data factory doesn't support break the loop if the inner copy active failed!</p>

<p>Others have created a <a href=""https://feedback.azure.com/forums/270578-data-factory?query=foreach"" rel=""nofollow noreferrer"">user voice</a> for data factory, it have been vote up 14 times.</p>

<p>But still with no response. :
<a href=""https://i.stack.imgur.com/DhFTR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DhFTR.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60526780","foreach actitivity in azure data factory","<p>Is there any method available in Azure data factory to exit the for each loop. I am using a for each loop to iterate through files and to process it. But when the copy activity placed inside the loop fails, the loop executes multiple times to reprocess the failed file. I think it has to do something with the number of files available in the Get meta data array. Can anyone suggest a method to resolve this issue.</p>

<p>Regards,
Sandeep</p>
","<azure-data-factory>","2020-03-04 13:05:02","558","-1","2","72426023","<p>You can't break the ForEach loop, but you can cancel the entire pipeline running that ForEach via the Rest API in case of an error. Will add example code in a blog post next week.
<a href=""https://i.stack.imgur.com/5rb7j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5rb7j.png"" alt=""enter image description here"" /></a></p>
"
"60517074","Log Analytics Search Results in ADF","<p>I am attempting to manipulate Log Analytics Query Results and hitting a road block...not really sure where to ask, so please point me in the correct direction if this is not it. I've been banging my head against the wall for about a week on this one. I figured out how to do this via a series of web activities and 3 nested pipelines, but Web Activities dont support the data volume the copy activity does.</p>

<p>The REST API for Log Analytics output is of the form:</p>

<pre><code>{
""tables"": [
    { 
         ""name"": ""PrimaryResult"",
         ""columns"": [
                         { ""name"": ""Category"", ""type"": ""string"" }, 
                         { ""name"": ""count_"", ""type"": ""long"" }
                    ],
          ""rows"": [
                     [ ""Administrative"", 20839 ],
                     [ ""ServiceHealth"", 11 ]
           ]
      }
   ]
}
</code></pre>

<p>I want to create a JSON object of the form</p>

<p><code>[{""Category"":""Administrative"", ""count_"": 20839},{""Category"":""ServiceHealth"", ""count_"": 11}]</code></p>

<p>Any ideas? Again, if this is the wrong forum, please let me know and I will adjust accordingly.</p>
","<json><rest><azure-data-factory>","2020-03-03 23:35:39","113","0","1","60736453","<p>Per my knowledge,there is no built-in feature in ADF could process the result for you.</p>

<pre><code>[{""Category"":""Administrative"", ""count_"": 20839},{""Category"":""ServiceHealth"", ""count_"": 11}]
</code></pre>

<p>You have to manipulate the result by yourself.I suggest you creating a <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">HTTP Trigger Azure Function</a> to implement this which is supported by ADF Azure Function Activity. Just set it up after Web Activity mentioned in your quesiton.</p>

<p>I wrote test sample for you in Python code. Certainly, you could use any language you know.</p>

<pre><code>import logging

import azure.functions as func

import json

def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    req_body = req.get_json()

    tables = req_body[""tables""]
    returnArray = []
    for table in tables: 
        for row in table[""rows""]:
            info ={}
            for inx,value in enumerate(row):
                info[table[""columns""][inx][""name""]] = value
            returnArray.append(info)   

    if req_body:
        return func.HttpResponse(json.dumps(returnArray ))
    else:
        return func.HttpResponse(
             ""This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response."",
             status_code=200
        )
</code></pre>

<p>Test Output:</p>

<p><a href=""https://i.stack.imgur.com/wH0CV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wH0CV.png"" alt=""enter image description here""></a></p>
"
"60516477","Unable to connect to Salesforce on Azure Data Factory","<p>I'm trying to connect to my Salesforce QA environment on Azure Data Factory but am not having any success even though I entered my username, password, and my security token after resetting it under Salesforce QA>Settings>My Personal Information>Reset My Security Token. Am I missing something?</p>

<p>I tried putting some environment urls but it didn't work so I removed it. Not sure what else I can do at this point... Any pointers would be greatly appreciated!</p>

<p><a href=""https://i.stack.imgur.com/TI0bX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TI0bX.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-03-03 22:30:01","81","0","1","60516537","<p>Nevermind this is resolved! I set the environment url to <a href=""https://test.salesforce.com"" rel=""nofollow noreferrer"">https://test.salesforce.com</a> and it worked. :)</p>
"
"60513921","Can you download a Qualtrics data file to blob storage using Azure Data Factory?","<p>Qualtrics provides an API that allows you to download your survey data as a file.  The API call looks like this:</p>

<pre><code>https://[tenant].qualtrics.com/API/v3/surveys/[Survey_ID]/export-responses/[File_ID]/file
</code></pre>

<p>The response contains ALL the data (in .csv format) and must be written straight to a file (i.e. responses.csv)in blob storage for further processing.</p>

<p>I've tried using a web activity and a copy activity (w/ Rest) with no luck.  Any help greatly appreciated.</p>

<p>Here is what a sample result response from the API call looks like:</p>

<pre><code>RecordedDate,RecipientFirstName,RecipientLastName,CSULBID,Score
2020-02-07 23:21:50,Darth,Vader,00001234,56
2020-02-07 23:21:50,Darth,Maul,00005678,56
</code></pre>
","<rest><api><azure-data-factory><qualtrics>","2020-03-03 19:01:29","646","0","1","60515613","<p>Have you looked into Azure Logic Apps?</p>

<p>It has a very intuitive way to call web API and create CSV out of it.  It's explained here: <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-perform-data-operations"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-perform-data-operations</a></p>

<p>My group tried to use Azure Data Factory, but it seemed like Azure Logic App did a better job, and ended up creating the API calling and CSV making step as a Logic App.</p>
"
"60509596","Data Load from CSV file to SQL DB using ADF","<p>I am having a.CSV file and I am trying to load to SQLDB using azure data factory but in the CSV file there is a column ""Address"" in that column there is comma in between address so ADF is splitting that column into multiple columns because CSV stands for (comma-separated values) and because of that address column is being split into multiple columns is there any way to resolve this issue.</p>
","<azure><csv><etl><azure-data-factory>","2020-03-03 14:37:20","606","0","2","60509859","<p>In your <strong>Dataset</strong>, try changing the <strong>'Column delimiter'</strong> to another character other than ','.</p>

<p><a href=""https://i.stack.imgur.com/Aaohp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aaohp.png"" alt=""enter image description here""></a></p>
"
"60509596","Data Load from CSV file to SQL DB using ADF","<p>I am having a.CSV file and I am trying to load to SQLDB using azure data factory but in the CSV file there is a column ""Address"" in that column there is comma in between address so ADF is splitting that column into multiple columns because CSV stands for (comma-separated values) and because of that address column is being split into multiple columns is there any way to resolve this issue.</p>
","<azure><csv><etl><azure-data-factory>","2020-03-03 14:37:20","606","0","2","60522994","<p>According you description, It's sure that the csv file using the comma as column delimiter.</p>

<p>Change the dataset column delimiter to <code>Pipe(|)</code> doesn't works.</p>

<p>For this question, others have asked the same questions in Stack overflow. Data factory do not support change the csv file schema  during Copy Active or Data Flow.</p>

<p>I also asked Azure Support for helps before, the only way they suggest me is that change the source csv file.</p>

<p>Hope this helps.</p>
"
"60506157","Azure Logic Apps/Data Factory, getting files using FTPS","<p>I'm trying to get a file from a ftp server trough ftps.
This post is updated with more test results and knowledge from replies and my own research.
The ftp server support ftps Implicit SSL/TLS mode on port 990 and Exlicit mode on port 21.
I have tried to get the file from Logic apps and Data factory, both unsuccessful.</p>

<p>Testing Implicit mode.
Port 990, Enable SSL, Binary transport true/false tried both(Logic App), Server Certificate Validation disabled, yields this error:</p>

<p>Data factory error:</p>

<p>""Failed to connect to FTP server. ADF doesn't support SSL/TLS implicit encryption mode. Please make sure you are connecting by explicit encryption mode. The underlying connection was closed: The server committed a protocol violation.""</p>

<p>Logic apps error: </p>

<pre><code>{
  ""error"": {
    ""code"": 502,
    ""source"": ""logic-apis-northeurope.azure-apim.net"",
    ""clientRequestId"": ""c935a165-c725-4d26-824d-d1df3c82a65c"",
    ""message"": ""BadGateway"",
    ""innerError"": {
      ""status"": 502,
      ""message"": ""An FTP protocol violation was detected. This could be caused by an implicit TLS/SSL configuration of the FTP server that is not supported. Please try explicit TLS/SSL.\r\nclientRequestId: c935a165-c725-4d26-824d-d1df3c82a65c"",
      ""error"": {
        ""message"": ""An FTP protocol violation was detected. This could be caused by an implicit TLS/SSL configuration of the FTP server that is not supported. Please try explicit TLS/SSL.""
      },
      ""source"": ""ftp-ne.azconn-ne.p.azurewebsites.net""
    }
  }
}
</code></pre>

<p>As the response below indicate, either Data Factory nor Logig apps supports implicit mode.</p>

<p>Testing Explicit mode.
Port 21, Enable SSL, Binary transport true/false tried both(Logic App), Server Certificate Validation disabled, yields this error:</p>

<p>Data Factory error:</p>

<p>""Failed to connect to FTP server. Please make sure the provided server informantion is correct, and try again. The remote server returned an error: 150 Opening data channel for directory listing of ""/"" . Authentication failed because the remote party has closed the transport stream""</p>

<p>Logic App error:</p>

<pre><code>{
  ""error"": {
    ""code"": 502,
    ""source"": ""logic-apis-northeurope.azure-apim.net"",
    ""clientRequestId"": ""61a1cc3e-74f3-40ac-a9f3-79a1409d81cc"",
    ""message"": ""The response is not in a JSON format."",
    ""innerError"": ""The specified CGI application encountered an error and the server terminated the process.""
  }
}
</code></pre>

<p>Test using FileZilla client. </p>

<p>Both implicit port(990) and explicit mode (port 21) works fine.
My original question where:
Is there any way to get the FTP connection in either Data factory or Logic Apps to work with SSL/TLS implicit mode?
As the response below states. Logic Apps and Data factory does not support Implicit mode. </p>

<p>A work around is to create a Function App, with c# code that gets the file trough implicit mode. And call the Function App from logic apps and/or data factory.</p>

<p>But why isn't explicit mode work?
Any tips?</p>
","<azure><ssl><ftp><azure-logic-apps><azure-data-factory>","2020-03-03 11:22:57","2418","2","2","60520866","<p>Unfortunately neither Azure Data Factory nor Azure Logic Apps support Implicit TLS/SSL Configuration.  This is a known limitation.</p>

<p><strong>Azure Data Factory limitation</strong>: The ADF FTP connector supports accessing FTP server with either no encryption or explicit SSL/TLS encryption; it doesn’t support implicit SSL/TLS encryption. (Ref: <a href=""https://learn.microsoft.com/azure/data-factory/connector-ftp#linked-service-properties"" rel=""nofollow noreferrer"">Copy data from FTP server by using Azure Data Factory</a>)</p>

<ul>
<li>Please consider to share your feedback/suggestion in ADF user voice forum about your requirement: <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory</a> </li>
</ul>

<p><strong>Azure Logic Apps limitaiton</strong>: The Azure Logic Apps FTP connector supports only explicit FTP over SSL (FTPS) and isn't compatible with implicit FTPS (Ref: <a href=""https://learn.microsoft.com/azure/connectors/connectors-create-api-ftp#limitations"" rel=""nofollow noreferrer"">Azure Logic Apps FTP connector limiations</a>)</p>

<ul>
<li>There is already an existing feature request thread in Azure Logic Apps user voice forum related to this requirement, please feel free to up-vote and/or comment to increase the priority of the feature request: <a href=""https://feedback.azure.com/forums/287593-logic-apps/suggestions/15201162-provide-implicit-ftps-capabilities-with-the-ftp-co"" rel=""nofollow noreferrer"">Provide Implicit FTPS Capabilities with the FTP Connector</a></li>
</ul>
"
"60506157","Azure Logic Apps/Data Factory, getting files using FTPS","<p>I'm trying to get a file from a ftp server trough ftps.
This post is updated with more test results and knowledge from replies and my own research.
The ftp server support ftps Implicit SSL/TLS mode on port 990 and Exlicit mode on port 21.
I have tried to get the file from Logic apps and Data factory, both unsuccessful.</p>

<p>Testing Implicit mode.
Port 990, Enable SSL, Binary transport true/false tried both(Logic App), Server Certificate Validation disabled, yields this error:</p>

<p>Data factory error:</p>

<p>""Failed to connect to FTP server. ADF doesn't support SSL/TLS implicit encryption mode. Please make sure you are connecting by explicit encryption mode. The underlying connection was closed: The server committed a protocol violation.""</p>

<p>Logic apps error: </p>

<pre><code>{
  ""error"": {
    ""code"": 502,
    ""source"": ""logic-apis-northeurope.azure-apim.net"",
    ""clientRequestId"": ""c935a165-c725-4d26-824d-d1df3c82a65c"",
    ""message"": ""BadGateway"",
    ""innerError"": {
      ""status"": 502,
      ""message"": ""An FTP protocol violation was detected. This could be caused by an implicit TLS/SSL configuration of the FTP server that is not supported. Please try explicit TLS/SSL.\r\nclientRequestId: c935a165-c725-4d26-824d-d1df3c82a65c"",
      ""error"": {
        ""message"": ""An FTP protocol violation was detected. This could be caused by an implicit TLS/SSL configuration of the FTP server that is not supported. Please try explicit TLS/SSL.""
      },
      ""source"": ""ftp-ne.azconn-ne.p.azurewebsites.net""
    }
  }
}
</code></pre>

<p>As the response below indicate, either Data Factory nor Logig apps supports implicit mode.</p>

<p>Testing Explicit mode.
Port 21, Enable SSL, Binary transport true/false tried both(Logic App), Server Certificate Validation disabled, yields this error:</p>

<p>Data Factory error:</p>

<p>""Failed to connect to FTP server. Please make sure the provided server informantion is correct, and try again. The remote server returned an error: 150 Opening data channel for directory listing of ""/"" . Authentication failed because the remote party has closed the transport stream""</p>

<p>Logic App error:</p>

<pre><code>{
  ""error"": {
    ""code"": 502,
    ""source"": ""logic-apis-northeurope.azure-apim.net"",
    ""clientRequestId"": ""61a1cc3e-74f3-40ac-a9f3-79a1409d81cc"",
    ""message"": ""The response is not in a JSON format."",
    ""innerError"": ""The specified CGI application encountered an error and the server terminated the process.""
  }
}
</code></pre>

<p>Test using FileZilla client. </p>

<p>Both implicit port(990) and explicit mode (port 21) works fine.
My original question where:
Is there any way to get the FTP connection in either Data factory or Logic Apps to work with SSL/TLS implicit mode?
As the response below states. Logic Apps and Data factory does not support Implicit mode. </p>

<p>A work around is to create a Function App, with c# code that gets the file trough implicit mode. And call the Function App from logic apps and/or data factory.</p>

<p>But why isn't explicit mode work?
Any tips?</p>
","<azure><ssl><ftp><azure-logic-apps><azure-data-factory>","2020-03-03 11:22:57","2418","2","2","71475916","<p>Implicit mode FTPS is deprecated, hence the lack of support in Azure services.</p>
<p>The error in explicit mode indicates that you actually connected and authenticated with the server. It tried to open a data channel to send a directory listing but the open request was 'closed'.</p>
<p>This error indicates you are using active mode when you probably should be using passive mode. In active mode the FTP server tries connect back to the client to create the data channel. There is a very strong chance that firewalls will prevent this. Using passive mode requires the client to connect to the server for the data channel. Better chance of firewalls allowing that connecting through.</p>
"
"60505409","ADF V2 - Web POST method using Dynamic Content and Variable","<p><strong>Very short version</strong> </p>

<p>How do I include an ADF Variable inside a JSON POST request, in a Web Activity within ADF?
I feel like this should be a very simple string concatenation, but i can't get it to work</p>

<p><strong>Detail</strong></p>

<p>We have a requirement to run a query / SProc from within ADF, which will return a string containing an error message.  That string is to then be passed via the Web Activity in ADF to a Logic App, in order to fire off an email, containing the error.</p>

<p>The setup of the logic app is copied from here: </p>

<p><a href=""https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification--part-1/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification--part-1/</a></p>

<p>and then here (part 2)</p>

<p><a href=""https://www.mssqltips.com/sqlservertip/5962/send-notifications-from-an-azure-data-factory-pipeline--part-2/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/5962/send-notifications-from-an-azure-data-factory-pipeline--part-2/</a></p>

<p>In ADF, I used the <strong>Lookup</strong> activity, to run a query, which brings back the error (appears to work, the preview returns the correct string)
Then I use the <strong>Set Variable</strong> activity, to take the output of the lookup and store it in a variable.</p>

<p>Last Step is to fire off the POST using the <strong>Web</strong> Activity.</p>

<p>With this code (tweaked slightly to remove personal details) in my Web Activity, everything works fine and I receive an email</p>

<pre><code>{
   ""DataFactoryName"": ""@{pipeline().DataFactory}"",
   ""PipelineName"": ""@{pipeline().Pipeline}"",
   ""Subject"": ""Pipeline finished!"",
   ""ErrorMessage"": ""Everything is okey-dokey!"",
   ""EmailTo"": ""me@myEmail.com""
}
</code></pre>

<p>But any attempt to put the contents of the Variable into the Subject part has failed.</p>

<p>This (for example) sends me an email with the subject literally being <strong>@variables('EmailSubject')</strong></p>

<pre><code>{
   ""DataFactoryName"": ""@{pipeline().DataFactory}"",
   ""PipelineName"": ""@{pipeline().Pipeline}"",
   ""Subject"": ""@variables('EmailSubject')"",
   ""ErrorMessage"": ""Everything is okey-dokey!"",
   ""EmailTo"": ""me@myEmail.com""
}
</code></pre>

<p>But I've also attempted various other solutions that result in errors or the email subject just containing the literal thing that I put in there (e.g.  + @variables('EmailSubject') +). </p>

<p>I also tried storing the entire JSON in the Variable, and then having the Web activity use only the variable, that returned no errors, but also did not send an email.</p>

<p>This attempt:</p>

<pre><code>{
   ""DataFactoryName"": ""@{pipeline().DataFactory}"",
   ""PipelineName"": ""@{pipeline().Pipeline}"",
   ""Subject"": ""@{variables('EmailSubject')}"",
   ""ErrorMessage"": ""Everything is okey-dokey!"",
   ""EmailTo"": ""me@myEmail.com""
}   
</code></pre>

<p>Resulted in this input into the web activity - which actually includes the text of the error, which is a bonus ... (text = Job Duration Warning):</p>

<pre><code>{
    ""url"": ""https://azureLogicAppsSiteHere"",
    ""method"": ""POST"",
    ""headers"": {
        ""Content-Type"": ""application/json""
    },
    ""body"": ""{\n   \""DataFactoryName\"": \""DFNAMEHERE\"",\n   \""PipelineName\"": \""pipeline1\"",\n   \""Subject\"": \""{\""firstRow\"":{\""\"":\""Job Duration Warning\""},\""effectiveIntegrationRuntime\"":\""DefaultIntegrationRuntime (West Europe)\"",\""billingReference\"":{\""activityType\"":\""PipelineActivity\"",\""billableDuration\"":[{\""meterType\"":\""AzureIR\"",\""duration\"":0.016666666666666666,\""unit\"":\""DIUHours\""}]},\""durationInQueue\"":{\""integrationRuntimeQueue\"":0}}\"",\n   \""ErrorMessage\"": \""Everything is okey-dokey!\"",\n   \""EmailTo\"": \""me@myEmail.com\""\n}\t""
}
</code></pre>

<p>But then resulted in this error:</p>

<pre><code>{
    ""errorCode"": ""2108"",
    ""message"": ""{\""error\"":{\""code\"":\""InvalidRequestContent\"",\""message\"":\""The request content is not valid and could not be deserialized: 'After parsing a value an unexpected character was encountered: f. Path 'Subject', line 4, position 17.'.\""}}"",
    ""failureType"": ""UserError"",
    ""target"": ""Web1"",
    ""details"": []
}
</code></pre>

<p>[Edit] The <strong>PREVIEW</strong> from the Lookup Activity is the text: <strong>Job Duration Warning</strong> BUT when I debug the pipeline, it lets me see the actual Output, which is this:</p>

<pre><code>{
    ""count"": 1,
    ""value"": [
        {
            """": ""Job Duration Warning""
        }
    ],
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (West Europe)"",
    ""billingReference"": {
        ""activityType"": ""PipelineActivity"",
        ""billableDuration"": [
            {
                ""meterType"": ""AzureIR"",
                ""duration"": 0.016666666666666666,
                ""unit"": ""DIUHours""
            }
        ]
    },
    ""durationInQueue"": {
        ""integrationRuntimeQueue"": 0
    }
}
</code></pre>

<p>So it appears that the problem is that the Lookup Output isn't what I thought it was, so the variable can't be used in the Web Activity, as it contains unsupported characters or something along those lines.</p>

<p>I just tested this and it worked ok:</p>

<ul>
<li>Create a String Parameter with the value <strong>Job Duration Warning</strong></li>
<li>Set the Variable value to be <strong>@pipeline().parameters.ParamSubject</strong></li>
<li>Include the variable in the web activity with an @ in front of it</li>
</ul>

<p>I then receive my expected email with the right subject.  I just don't know how to get the string output of my query, into a variable / parameter, so that i can use it in the web activity.</p>
","<json><azure-logic-apps><azure-data-factory>","2020-03-03 10:42:22","6440","3","1","60527046","<p>I don't know how well this applies to other people's issues, but I found a solution that has worked for me.</p>

<ul>
<li>In the SELECT query within the Lookup Activity - name the output (in my case, I called that column 'Subject'- i.e. SELECT xyz AS Subject</li>
<li>In the Lookup Activity, turn on the setting 'First Row Only'</li>
<li>In the Set Variable Activity, use the code: @activity('Lookup1').output.firstRow.subject
(where 'Lookup1' is the name of your Lookup Activity and Subject is the name of the column you are outputting)</li>
<li>In the Web Activity, reference the variable as follows:</li>
</ul>

<pre><code>{
   ""DataFactoryName"": ""@{pipeline().DataFactory}"",
   ""PipelineName"": ""@{pipeline().Pipeline}"",
   ""Subject"": ""@{variables('EmailSubject')}"",
   ""ErrorMessage"": ""Everything is okey-dokey!"",
   ""EmailTo"": ""me@myEmail.com""
}
</code></pre>
"
"60502546","Create and save new file in Blob Storage in each pipeline run","<p>Is it possible to create a <em>csv</em> file in blob storage for each run of a pipeline in Azure Data Factory with a custom name?</p>
<p>Each <em>csv</em> has to have the date of the run (which I can easily set with a variable) in its name as it has data related to that date.</p>
<p>I don't have any idea how to do that since the sink property in the &quot;Copy&quot; activity allows to select an already created &quot;Sink dataset&quot; but not to create a new one in each pipeline run.</p>
","<azure-pipelines><azure-blob-storage><azure-data-factory>","2020-03-03 07:58:23","1313","0","1","60502923","<blockquote>
  <p>I don't have any idea how to do that since the sink property in the
  ""Copy"" activity allows to select an already created ""Sink dataset"" but
  not to create a new one in each pipeline run.</p>
</blockquote>

<p>Yes,the source and sink dataset should be specific exactly.You can't set dynamic sink dataset when you execute copy activity.</p>

<p>According to your description,you want to log the execution date of every ADF pipeline into different files separately.Per my knowledge,no direct option to implement that.Provide below possible workaround for your reference:</p>

<p>Enable diagnostic logs with your ADF account and set the residence(Blob Storage) of log files. You could view the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#schema-of-logs-and-events"" rel=""nofollow noreferrer"">properties</a> in that log file:<code>pipelineName ,start,end</code>.</p>

<p>Use Azure Function Blob Trigger to get the entire data from the file.And create the files separately as you want into another blob storage path with code.</p>
"
"60501334","Copy data from Azure ""FILE SHARES"" to BLOB containers via data factory","<p>I have setup two datalake Gen2 in one subscription. I am uploading data into FILE SHARES in one of the storage account and try to copy the data to another storage account (Blob container) via data factory. I am not able to set up linked service for data for FILE SHARES from data factory. It only points to Blob containers of the Data Lake. </p>

<p>How do I achieve this? I am trying via data factory as I have to schedule and automatically run this copying once every day. Thanks.</p>
","<azure><azure-data-factory><azure-data-lake-gen2>","2020-03-03 06:35:01","1018","0","2","60502654","<p>For file shares, <strong>Source dataset</strong> should be <strong>Azure File Storage</strong>.</p>

<p><a href=""https://i.stack.imgur.com/mEDty.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mEDty.png"" alt=""enter image description here""></a></p>

<p>For linked service, there will be a prompt to ask for <strong>Host</strong>, <strong>User name</strong>, <strong>Password</strong>, to point to your fileshare.
<a href=""https://i.stack.imgur.com/g7JiX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7JiX.png"" alt=""enter image description here""></a></p>

<p>These <strong>Host</strong>, <strong>Username</strong> and <strong>Password</strong> fields can be obtained when you click on 'Connect' in the Azure file share window.</p>

<p><a href=""https://i.stack.imgur.com/xH9Pj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xH9Pj.png"" alt=""enter image description here""></a></p>

<p>Hope it helps.</p>
"
"60501334","Copy data from Azure ""FILE SHARES"" to BLOB containers via data factory","<p>I have setup two datalake Gen2 in one subscription. I am uploading data into FILE SHARES in one of the storage account and try to copy the data to another storage account (Blob container) via data factory. I am not able to set up linked service for data for FILE SHARES from data factory. It only points to Blob containers of the Data Lake. </p>

<p>How do I achieve this? I am trying via data factory as I have to schedule and automatically run this copying once every day. Thanks.</p>
","<azure><azure-data-factory><azure-data-lake-gen2>","2020-03-03 06:35:01","1018","0","2","60503345","<p>I could create Linked service for SFTP as below.
<a href=""https://i.stack.imgur.com/UJfPv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UJfPv.png"" alt=""enter image description here""></a></p>
"
"60492457","ADF data flow, pass ForEach activity's @item() to sink's settings(?)","<p>In my data flows I have a lot of scenarios where I just upsert data from one database to another(names of tables are the same in both of them). I would like to avoid duplicated operations and just use ADF ForEach for that. </p>

<p>So I have defined pipeline's variable with my table names. Then in ForEach activity, I am using @item() as a parameter for my data flow which is nested inside of it. However, my problem lies here:</p>

<p><a href=""https://i.stack.imgur.com/HtLyd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HtLyd.png"" alt=""enter image description here""></a></p>

<p>My key column is always name of the table + ""ID"", so I have wanted to do something like concat(@item(), ""ID"") to make it work. However, this ""add dynamic content"" doesn't allow to do that. </p>

<p>Is there some way to reference items from ForEach activity in this scenario?</p>

<p>Thank you in advance!</p>
","<azure><dataflow><azure-data-factory>","2020-03-02 15:55:31","2332","0","1","60494811","<ol>
<li><p>Create a parameter in your data flow. Make the data type of the parameter the same data type as the field you'll use for the key column. Let's call it ""dynakey"" for this sample.</p></li>
<li><p>In the pipeline activity, set the parameter ""dynakey"" for that data flow as the concat expression you have above, as a parameter expression.</p></li>
<li><p>In Key Columns, choose ""add dynamic content"". Set the value for the field to the parameter ""dynakey"".</p></li>
</ol>
"
"60488293","Send two GetMetadataDB activity output values to Foreach activity","<p>I am trying to get output from two different GetMetadataDB activities and want to pass as item names to foreach activity.</p>

<p><a href=""https://i.stack.imgur.com/QhDdY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QhDdY.jpg"" alt=""enter image description here""></a></p>

<p>Pipeline Code Snippet</p>

<pre><code>{
    ""name"": ""V1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Get Metadata From Source"",
                ""type"": ""GetMetadata"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""dataset"": {
                        ""referenceName"": ""DelimitedSourceFile"",
                        ""type"": ""DatasetReference""
                    },
                    ""fieldList"": [
                        ""childItems"",
                        ""itemName""
                    ],
                    ""storeSettings"": {
                        ""type"": ""AzureBlobStorageReadSettings"",
                        ""recursive"": true
                    },
                    ""formatSettings"": {
                        ""type"": ""DelimitedTextReadSettings""
                    }
                }
            },
            {
                ""name"": ""ForEach source and target"",
                ""type"": ""ForEach"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Get Metadata from Target"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    },
                    {
                        ""activity"": ""Get Metadata From Source"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""items"": {
                        ""value"": ""@union(activity('Get Metadata From Source').output.childItems, activity('Get Metadata From Target').output.childItems)"",
                        ""type"": ""Expression""
                    },
                    ""activities"": [
                        {
                            ""name"": ""ValidationNotebook"",
                            ""type"": ""DatabricksNotebook"",
                            ""dependsOn"": [],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""userProperties"": [],
                            ""typeProperties"": {
                                ""notebookPath"": ""/GetFilenames"",
                                ""baseParameters"": {
                                    ""sourcestgpath"": {
                                        ""value"": ""@concat('abfss://',activity('Get Metadata from Source').output.itemName,'@dqstg.dfs.core.windows.net/',item().name)"",
                                        ""type"": ""Expression""
                                    },
                                    ""targetstgpath"": {
                                        ""value"": ""@concat('abfss://',activity('Get Metadata from Target').output.itemName,'@dqstg.dfs.core.windows.net/',item().name)"",
                                        ""type"": ""Expression""
                                    }
                                }
                            },
                            ""linkedServiceName"": {
                                ""referenceName"": ""AzureDatabricks1"",
                                ""type"": ""LinkedServiceReference""
                            }
                        }
                    ]
                }
            },
            {
                ""name"": ""Get Metadata from Target"",
                ""type"": ""GetMetadata"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""dataset"": {
                        ""referenceName"": ""DelimitedTargetFiles"",
                        ""type"": ""DatasetReference""
                    },
                    ""fieldList"": [
                        ""childItems"",
                        ""itemName""
                    ],
                    ""storeSettings"": {
                        ""type"": ""AzureBlobStorageReadSettings"",
                        ""recursive"": true
                    },
                    ""formatSettings"": {
                        ""type"": ""DelimitedTextReadSettings""
                    }
                }
            }
        ],
        ""annotations"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>Pipeline fails with error: </p>

<blockquote>
  <p>{""error"":{""code"":""BadRequest"",""message"":""ErrorCode=InvalidTemplate, ErrorMessage=The expression 'activity('Get Metadata From Source').output @activity('Get Metadata from Target').output' is not valid: the string character '@' at position '44' is not expected.\"""",""target"":""pipeline/V1/runid/1457"",""details"":null}}</p>
</blockquote>

<p>How can this be achieved?</p>

<p>Thank you.</p>
","<azure-pipelines><azure-data-factory>","2020-03-02 11:52:31","350","0","1","60489476","<p>Please try:</p>

<pre><code>@union(activity('Get Metadata From Source').output.childItems, activity('Get Metadata From Target').output.childItems)
</code></pre>
"
"60482638","Pass parameter to Azure Data Factory-ADF activity based on trigger","<p>I have Azure Data pipeline where I have to pass a parameter to Databricks activity. I have multiple Event based triggers (Updation of blob folder) added for that activity. When specific trigger gets activated, it should pass a parameter to Databricks activity and based on that notebook should run. Is there any way to pass parameter from Event based trigger to Databricks notebook activity?</p>
","<azure><azure-pipelines><databricks><azure-data-factory>","2020-03-02 04:42:25","3580","2","1","60487315","<p>Trigger gives out 2 parameters.</p>

<ul>
<li><p>@triggerBody().fileName</p></li>
<li><p>@triggerBody().folderPath</p></li>
</ul>

<p>You will have to add this to JSON code of trigger</p>

<pre><code>        ""parameters"": {
            ""FPath"": ""@triggerBody().folderPath""
        }
</code></pre>

<p>Use this parameter as Pipeline variable <code>@triggerBody().FPath</code> and use that variable with other activities. Please refer to link below for detailed explanation</p>

<p><a href=""https://www.mssqltips.com/sqlservertip/6063/create-event-based-trigger-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://www.mssqltips.com/sqlservertip/6063/create-event-based-trigger-in-azure-data-factory/</a></p>
"
"60469091","How to parameterise Dataset definition filename in Azure Data factory","<p>I have a blob storage container folder (source) that gets several csv files. My task is to pick the csv files starting with ""file"". See example filename below::</p>

<p><strong>file12345.csv</strong></p>

<p>The numeric part varies every time.</p>

<p>I have set the ""fixed"" Container and Directory names in the image below but it seems the File parameter does not accept wildcard ""File*.csv"". 
<a href=""https://i.stack.imgur.com/jZt7g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jZt7g.png"" alt=""enter image description here""></a></p>

<p>How can I pass a wildcard to the Dataset definition?</p>

<p>Thanks</p>
","<azure-data-factory><azure-blob-storage>","2020-02-29 19:57:00","2236","0","1","60481581","<p>You can't do that operation in Soure dataset.</p>

<p>Just choose the container or folder in the dataset like bellow:</p>

<p><a href=""https://i.stack.imgur.com/WYaSX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WYaSX.png"" alt=""enter image description here""></a></p>

<p>Choose the <code>Wildcard file path</code> in Source settings:</p>

<p><a href=""https://i.stack.imgur.com/xHZK4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xHZK4.png"" alt=""enter image description here""></a></p>

<p>The will help you filter the filename wildcard ""File*.csv"".</p>

<p>Ref: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#copy-activity-properties"" rel=""nofollow noreferrer"">Copy activity properties</a>:</p>

<p><a href=""https://i.stack.imgur.com/hwMrE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hwMrE.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60464027","ADF Mapping Data Flows, is it possible to execute SQL on a source?","<p>So I continue to rewrite my lovely SSIS packages to ADF Data Flows. However, there is a lot of cases where I have some OLE DB Source with quite complicated SQL statement followed by other transformations. </p>

<p>Let's say there is a SQL statement which joins 10 different tables. As far as I know I can execute SQL statement <strong>only</strong> on my sink. So to get the very same dataset that is being used later, I have to create 10 different sources and 10 join operations. Is that correct?</p>

<p>It is possible but it doesn't seem to be very efficient. The only other thing that comes to my mind is to re-think our whole DWH logic but it would be a lot of added work, so I would rather avoid that.</p>

<p>Thank you in advance!</p>
","<azure><azure-data-factory><dataflow>","2020-02-29 10:18:30","1422","2","1","60481820","<p>Actually, it's possible to execute SQL query on Source(only can do sql query).</p>

<p>For example, I do a SQL query in Source Azure SQL database.</p>

<p>Here's the data in my table <code>test4</code> and <code>test6</code>:</p>

<p><a href=""https://i.stack.imgur.com/fsHnC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fsHnC.png"" alt=""enter image description here""></a></p>

<p>Don't specify the table in Source dataset:</p>

<p><a href=""https://i.stack.imgur.com/l6P70.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l6P70.png"" alt=""enter image description here""></a></p>

<p>Data Flow Source setting:</p>

<p><a href=""https://i.stack.imgur.com/BhZQm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BhZQm.png"" alt=""enter image description here""></a></p>

<p>Source Options, execute a SQL query joined two tables:</p>

<pre><code>select a.id, a.tname,b.tt from test6 as a left join test4 as b  on a.id=b.id
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZfuMB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZfuMB.png"" alt=""enter image description here""></a></p>

<p>Import the schema of the query result:</p>

<p><a href=""https://i.stack.imgur.com/Xbori.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xbori.png"" alt=""enter image description here""></a></p>

<p>Data Preview:
<a href=""https://i.stack.imgur.com/8fvNG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8fvNG.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60458538","'The term 'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet' error in Pipeline Azure Devops","<p>Azure Powershell AZ module still does not work in azure devops pipeline?</p>

<p>I get this error when I try powershell version 4+ and the agent is 2017 (also tried windows 2019)</p>

<pre><code>#Install-Module -Name Az -AllowClobber -Scope CurrentUser (is this needed? I tried with and #without and it fails)
Set-AzureRmDataFactoryV2 -ResourceGroupName ""myRG"" -Name ""LLmenADF1"" -Location ""North Europe""
</code></pre>

<p><a href=""https://i.stack.imgur.com/1iyvb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1iyvb.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-02-28 20:03:23","471","0","2","60458842","<p>If you are using Powershell Az module, then the cmdlet is: </p>

<pre><code>Set-AzDataFactoryV2
</code></pre>

<p>Your cmdlet makes reference to the old library, AzureRM. To get the same cmdlet when migrating from one library to another, just replace AzureRM with Az.</p>

<p>Hope this helped!</p>
"
"60458538","'The term 'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet' error in Pipeline Azure Devops","<p>Azure Powershell AZ module still does not work in azure devops pipeline?</p>

<p>I get this error when I try powershell version 4+ and the agent is 2017 (also tried windows 2019)</p>

<pre><code>#Install-Module -Name Az -AllowClobber -Scope CurrentUser (is this needed? I tried with and #without and it fails)
Set-AzureRmDataFactoryV2 -ResourceGroupName ""myRG"" -Name ""LLmenADF1"" -Location ""North Europe""
</code></pre>

<p><a href=""https://i.stack.imgur.com/1iyvb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1iyvb.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2020-02-28 20:03:23","471","0","2","60459991","<p>@Martin Esteban Zurita has the correct recommended solution to migrate to Az commands</p>

<p>For those that can't/won't for any reason, just select Version 3 of the Azure Powershell task to work with AzureRM commands.</p>
"
"60451190","How to send group email from Azure Log Analytics Query result?","<p><strong>My Requirement:</strong> I need to send log analytics query result to Group Email basically we are using Azure Data Factories to log all ""<strong>Pipeline Metrics</strong>"" and ""<strong>Activity Metrics</strong>"".
To send Email I setup one rule and one ""Action Group"" and for this ""Action Group"" initially i setup my Emailid then i Received Email Successfully.However when i give 
Group Email Id(basically i created one group in outloook) it is not sending. Otherway i tried the below solution to send group Email</p>

<p>1.If i select ""<strong>Contributor</strong>"" Role for  ""<strong>Action Group</strong>"" then it is sending email to all contributor roles .<br/>
2. But i want to send email to these roles ""<strong>Monitoring Contributor</strong>"" or ""<strong>Monitoring Reader</strong>"" or ""<strong>Reader</strong>"" . I tried, but didn't Received mails.</p>

<p>we tried Logic Apps to send Group Email. It worked basically But we don't want use Logic Apps(cost issue).</p>

<p>So, I want solution to send Group Email.</p>

<p>Query :</p>

<pre><code>
ADFPipelineRun
|join
(
    ADFActivityRun
    |where Status == ""Succeeded""  or Status == ""Failed""
    |project ActivityStatus = Status,PipelineRunId,ActivityRunId ,ActivityName,ActivityStartTime=Start ,ActivityEndTime=End,FailureReason=ErrorMessage 
)
on $left.CorrelationId == $right.PipelineRunId
|project PipelineName,ActivityName,ActivityStatus,ActivityStartTime,ActivityEndTime,FailureReason

</code></pre>

<p>Here is image for action group role selection:
<a href=""https://i.stack.imgur.com/rG8jH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rG8jH.png"" alt=""enter image description here""></a></p>

<p>Please help me out from this problem. Thanks in advance!</p>
","<azure-data-factory><azure-log-analytics><azure-monitoring><azure-alerts>","2020-02-28 11:53:10","1001","0","1","60711992","<p>Not very sure if adding an (O365) Group email to an Action group would land the alert notification email correctly.</p>

<p>But I've tested the same with an <a href=""https://support.office.com/en-us/article/distribution-groups-e8ba58a8-fab2-4aaf-8aa1-2a304052d2de#bkmk_create"" rel=""nofollow noreferrer"">Outlook Distribution Group</a> and verified that the alert notification email was successfully sent when the alert was fired.</p>

<p>Unless you have a specific reason to do so, I'd suggest to use an appropriate DG for alert configuration.</p>
"
"60450930","Which is the best suitable service to log data whether Application Insights or Log analytics?","<p>I am new to azure data factory.</p>

<p>For Azure Data Factory(<strong>ADF</strong>) What is the best suitable service to monitor logs i.e whether i have to use <strong>Application Insights</strong> or <strong>Log analytics</strong> if so why and what is the difference between these two.</p>

<p>Many of the blogs said, For ADF, log analytics is best suitable. <br/>
Can't we use application insights for ADF. If no why?, if yes Why? and how?</p>

<p>Here is my scenario:</p>

<p>Currently, i am using <strong>log analytics</strong> for azure data factory when pipeline is failed or success. <br/>
Can't we use application insights for the above scenario?</p>

<p>Please help me out from this problem. Thanks in Advance!</p>
","<azure-application-insights><azure-data-factory><azure-log-analytics><azure-monitoring>","2020-02-28 11:37:05","1636","0","2","60712366","<p>@ZakiMa's comments in the Q&amp;A above cover the fundamental differences between Azure Log Analytics and Application Insights. Another similar discussion happened in <a href=""https://stackoverflow.com/a/55133921/10805761"">this</a> Stack Overflow thread.</p>

<p>Both of these services are under the <a href=""https://learn.microsoft.com/en-gb/azure/azure-monitor/overview"" rel=""nofollow noreferrer"">Azure Monitor</a> umbrella now:</p>

<p><a href=""https://i.stack.imgur.com/vG6NV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vG6NV.png"" alt=""Azure Monitor overview""></a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-portal"" rel=""nofollow noreferrer"">Log Analytics</a> is the primary tool in the Azure portal for writing log queries and interactively analyzing their results. All log data collected by Azure Monitor is stored in a Log Analytics workspace. A workspace is essentially a container where log data is collected from a variety of sources. You may have a single Log Analytics workspace for all your monitoring data or may have requirements for multiple workspaces.</p>

<p><a href=""https://learn.microsoft.com/en-gb/azure/azure-monitor/app/app-insights-overview#how-does-application-insights-work"" rel=""nofollow noreferrer"">Application Insights</a> is an extensible Application Performance Management (APM) service for developers and DevOps professionals that can be used to monitor live applications. It will automatically detect performance anomalies, and includes powerful analytics tools to help diagnose issues and to understand what users actually do with the app. It's designed to help in continuously improving performance and usability.</p>

<p>There is also a detailed <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/faq"" rel=""nofollow noreferrer"">FAQ</a> document that goes over some common questions in this area that may interest you.</p>
"
"60450930","Which is the best suitable service to log data whether Application Insights or Log analytics?","<p>I am new to azure data factory.</p>

<p>For Azure Data Factory(<strong>ADF</strong>) What is the best suitable service to monitor logs i.e whether i have to use <strong>Application Insights</strong> or <strong>Log analytics</strong> if so why and what is the difference between these two.</p>

<p>Many of the blogs said, For ADF, log analytics is best suitable. <br/>
Can't we use application insights for ADF. If no why?, if yes Why? and how?</p>

<p>Here is my scenario:</p>

<p>Currently, i am using <strong>log analytics</strong> for azure data factory when pipeline is failed or success. <br/>
Can't we use application insights for the above scenario?</p>

<p>Please help me out from this problem. Thanks in Advance!</p>
","<azure-application-insights><azure-data-factory><azure-log-analytics><azure-monitoring>","2020-02-28 11:37:05","1636","0","2","60712818","<p>It is not either or. But more 'and', as both are needed. AI is more visual and in the beginning it gives more 'insights' and then you are more likely to go and use the K-query in AI. That is the start to Log Analytics(you are matured enough to use LA). in LA you just use different set of tables to query. For eg: If you want to know why your api's are blocked then you will have to query log analytics for firewall logs, not app insights.</p>

<p>So in short both AI and LA have their own tables to query from. AI gives default visualizations which LA fails to do. You can expand AI visualization by making Dashboards inherited from it. Onto this dashboard you can add query visualizations from LA. So you have it all in one dashboard. </p>
"
"60445790","How to load files from blob to sql dw by using azure data factory?","<p>I have tried so many ways to load data from :</p>
<ol>
<li><p>Azure blob to azure SQL synapse.</p>
<p>My requirement is :<br />
Description:</p>
<pre><code> (Input)Blob storage ---&gt;  Azure sql synapse(Output)
                emp_dummy.csv----&gt; emp_dummy table
                dept_dummy.csv -----&gt; dept_dummy table
                sales_dummy.csv-----&gt; sales_dummy table   and so on
</code></pre>
</li>
</ol>
<p>...</p>
<p>We have files starting with different names but the format is .csv only.
I have been trying this in various ways by using <code>getmetadata</code> activity or lookup activity.</p>
<p>When I tried with the below activity, facing the error:
[ADF pipeline][1]
[1]: https://i.stack.imgur.com/RynIb.png
Error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=UserErrorMissingPropertyInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Required property 'fileName' is missing in payload.,Source=Microsoft.DataTransfer.ClientLibrary,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy data1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>I hope, I mention all details, if need some more, let me know.</p>
","<azure-data-factory><azure-synapse>","2020-02-28 05:32:37","1202","0","2","60500785","<p>I figured it out.</p>

<p>Here's my example steps: load two csv files to ADW, and auto create table with the same name with csv filename . </p>

<p><strong>Csv files in blob storage:</strong></p>

<p><a href=""https://i.stack.imgur.com/kNoOw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kNoOw.png"" alt=""enter image description here""></a></p>

<p>Get all the filename in the blob container 'backup':</p>

<p><a href=""https://i.stack.imgur.com/Q3nSC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q3nSC.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/CgZeG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CgZeG.png"" alt=""enter image description here""></a></p>

<p><strong>Foreach item settings:</strong></p>

<pre><code>@activity('Get Metadata2').output.childItems
</code></pre>

<p><a href=""https://i.stack.imgur.com/PPefL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PPefL.png"" alt=""enter image description here""></a></p>

<p><strong>Copy active in Foreach:</strong></p>

<p>In copy active, using another blob source, add parameter to choose the file:
<a href=""https://i.stack.imgur.com/pOt1i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pOt1i.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/2iEwi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2iEwi.png"" alt=""enter image description here""></a></p>

<p>Source settings:</p>

<p><a href=""https://i.stack.imgur.com/y7McN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y7McN.png"" alt=""enter image description here""></a></p>

<p>Sink dataset(ADW):
<a href=""https://i.stack.imgur.com/KQuXF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KQuXF.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/yjkhb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yjkhb.png"" alt=""enter image description here""></a></p>

<p>Sink settings:</p>

<p><a href=""https://i.stack.imgur.com/HtdSq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HtdSq.png"" alt=""enter image description here""></a></p>

<p>table name expression: <code>@split(item().name, '.')[0]</code></p>

<p><strong>Note:</strong> get metadata will get the full file name like 'test.csv', when we set the table name, we need split it and set table name as 'test'.</p>

<p><strong>Execute pipeline:</strong>
<a href=""https://i.stack.imgur.com/OjITf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OjITf.png"" alt=""enter image description here""></a></p>

<p><strong>Check data in ADW</strong>:
<a href=""https://i.stack.imgur.com/YDyJR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YDyJR.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60445790","How to load files from blob to sql dw by using azure data factory?","<p>I have tried so many ways to load data from :</p>
<ol>
<li><p>Azure blob to azure SQL synapse.</p>
<p>My requirement is :<br />
Description:</p>
<pre><code> (Input)Blob storage ---&gt;  Azure sql synapse(Output)
                emp_dummy.csv----&gt; emp_dummy table
                dept_dummy.csv -----&gt; dept_dummy table
                sales_dummy.csv-----&gt; sales_dummy table   and so on
</code></pre>
</li>
</ol>
<p>...</p>
<p>We have files starting with different names but the format is .csv only.
I have been trying this in various ways by using <code>getmetadata</code> activity or lookup activity.</p>
<p>When I tried with the below activity, facing the error:
[ADF pipeline][1]
[1]: https://i.stack.imgur.com/RynIb.png
Error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=UserErrorMissingPropertyInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Required property 'fileName' is missing in payload.,Source=Microsoft.DataTransfer.ClientLibrary,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy data1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>I hope, I mention all details, if need some more, let me know.</p>
","<azure-data-factory><azure-synapse>","2020-02-28 05:32:37","1202","0","2","60518890","<p>I did a Google search for you.  I found several really bad tutorials out there.  The two links below looks pretty darn on point.</p>

<p><a href=""https://intellipaat.com/blog/azure-data-factory-tutorial/"" rel=""nofollow noreferrer"">https://intellipaat.com/blog/azure-data-factory-tutorial/</a></p>

<p><a href=""https://medium.com/@adilsonbna/using-azure-data-lake-to-copy-data-from-csv-file-to-a-sql-database-712c243db658"" rel=""nofollow noreferrer"">https://medium.com/@adilsonbna/using-azure-data-lake-to-copy-data-from-csv-file-to-a-sql-database-712c243db658</a></p>

<p>Remember, when you're copying data from file stores by using Azure Data Factory, you can now configure wildcard file filters to let Copy Activity pick up only files that have the defined naming pattern—for example, ""*.csv"" or ""???20180504.json"".  </p>

<p>For reference, look at the image below.</p>

<p><a href=""https://i.stack.imgur.com/8HcrH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8HcrH.png"" alt=""enter image description here""></a></p>

<p>If you wanted to iterate through all the files, in different folders, in a Blob environment, instead of setting the File or Folder to this: </p>

<p><code>adfv2/SalesJan2009.csv</code></p>

<p>You can set the File or Folder to this:</p>

<p><code>adfv2/Sales*2009.csv</code></p>

<p>That will merge all Sales data from 2009 into a single dataframe, which you can them load to SQL Server (Data Warehouse, Synapse, etc).</p>
"
"60441940","Add more tables to the existing dataset","<p>How can I add more tables to an existing dataset? I have created a Data Factory to copy table data for 3 tables (Azure SQL Database).</p>

<p>Now I want to add two more tables to this. How do I do this?</p>
","<azure-sql-database><azure-data-factory>","2020-02-27 21:21:17","185","0","4","60442183","<p>First, you will need to create 2 new datasets for your 2 new tables. Then add Copy Activity (2) in your pipeline referencing output dataset as your newly created dataset.</p>
"
"60441940","Add more tables to the existing dataset","<p>How can I add more tables to an existing dataset? I have created a Data Factory to copy table data for 3 tables (Azure SQL Database).</p>

<p>Now I want to add two more tables to this. How do I do this?</p>
","<azure-sql-database><azure-data-factory>","2020-02-27 21:21:17","185","0","4","60456049","<p>You can only have 1 table for 1 dataset. Hence you have to create more datasets for more tables.</p>
"
"60441940","Add more tables to the existing dataset","<p>How can I add more tables to an existing dataset? I have created a Data Factory to copy table data for 3 tables (Azure SQL Database).</p>

<p>Now I want to add two more tables to this. How do I do this?</p>
","<azure-sql-database><azure-data-factory>","2020-02-27 21:21:17","185","0","4","60458520","<p>You can use parameters to have only one dataset pointing to a database, and make it represent different tables, depending on the execution context.</p>

<p>Cathrine Wilhelmsen does a really good job explaining parameters in her blog: <a href=""https://www.cathrinewilhelmsen.net/2019/12/20/parameters-azure-data-factory/"" rel=""nofollow noreferrer"">https://www.cathrinewilhelmsen.net/2019/12/20/parameters-azure-data-factory/</a></p>

<p>Hope this helped!</p>
"
"60441940","Add more tables to the existing dataset","<p>How can I add more tables to an existing dataset? I have created a Data Factory to copy table data for 3 tables (Azure SQL Database).</p>

<p>Now I want to add two more tables to this. How do I do this?</p>
","<azure-sql-database><azure-data-factory>","2020-02-27 21:21:17","185","0","4","60484297","<p>When you the copy data active executed, you can edit the pipeline to add the tables in dataset as the pipeline parameter.</p>

<p>For example, I copy two tables(<code>test</code> and <code>test4</code>) between two SQL database with COPY DATA. </p>

<p>When the pipeline execute successfully, editor the pipeline we can find that all the table name/schema mapping are set as parameter:
<a href=""https://i.stack.imgur.com/6Ib2U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Ib2U.png"" alt=""enter image description here""></a></p>

<pre><code>[
{""source"":{""table"":""test""},
""destination"":{""table"":""test""},
""copyActivity"":{""translator"":{""type"":""TabularTranslator"",""mappings"":[{""source"": 
{""name"":""id"",""type"":""Int32""},""sink"":{""name"":""id"",""type"":""Double""}},{""source"": 
{""name"":""tt"",""type"":""String""},""sink"":{""name"":""name"",""type"":""String""}}]}}},

{""source"":{""table"":""test4""},
""destination"":{""table"":""test4""},
""copyActivity"":{""translator"":{""type"":""TabularTranslator"",""mappings"":[{""source"": 
{""name"":""id"",""type"":""Int32""},""sink"":{""name"":""id"",""type"":""Int32""}},{""source"": 
{""name"":""tt"",""type"":""DateTime""},""sink"":{""name"":""tt"",""type"":""DateTime""}}]}}}

]
</code></pre>

<p>If you want add more tables, just editor the parameter json and  add a source json like bellow:</p>

<pre><code>{""source"":{""table"":""test4""},
        ""destination"":{""table"":""test4""},
        ""copyActivity"":{""translator"":{""type"":""TabularTranslator"",""mappings"":[{""source"": 
        {""name"":""id"",""type"":""Int32""},""sink"":{""name"":""id"",""type"":""Int32""}},{""source"": 
        {""name"":""tt"",""type"":""DateTime""},""sink"":{""name"":""tt"",""type"":""DateTime""}}]}}}
</code></pre>

<p>So the new parameter should be:</p>

<pre><code>  [
    {""source"":{""table"":""test""},
    ""destination"":{""table"":""test""},
    ""copyActivity"":{""translator"":{""type"":""TabularTranslator"",""mappings"":[{""source"": 
    {""name"":""id"",""type"":""Int32""},""sink"":{""name"":""id"",""type"":""Double""}},{""source"": 
    {""name"":""tt"",""type"":""String""},""sink"":{""name"":""name"",""type"":""String""}}]}}},

    {""source"":{""table"":""test4""},
    ""destination"":{""table"":""test4""},
    ""copyActivity"":{""translator"":{""type"":""TabularTranslator"",""mappings"":[{""source"": 
    {""name"":""id"",""type"":""Int32""},""sink"":{""name"":""id"",""type"":""Int32""}},{""source"": 
    {""name"":""tt"",""type"":""DateTime""},""sink"":{""name"":""tt"",""type"":""DateTime""}}]}}}

    {""source"":{""table"":""test6""},
    ""destination"":{""table"":""test6""},
    ""copyActivity"":{""translator"":{""type"":""TabularTranslator"",""mappings"":[{""source"": 
    {""name"":""id"",""type"":""Int32""},""sink"":{""name"":""id"",""type"":""Int32""}},{""source"": 
    {""name"":""tt"",""type"":""DateTime""},""sink"":{""name"":""tt"",""type"":""DateTime""}}]}}}

    {""source"":{""table"":""test7""},
    ""destination"":{""table"":""test7""},
    ""copyActivity"":{""translator"":{""type"":""TabularTranslator"",""mappings"":[{""source"": 
    {""name"":""id"",""type"":""Int32""},""sink"":{""name"":""id"",""type"":""Int32""}},{""source"": 
    {""name"":""tt"",""type"":""DateTime""},""sink"":{""name"":""tt"",""type"":""DateTime""}}]}}}

    ```

]
</code></pre>

<p>Hope this helps.</p>
"
"60435922","Azure Data Factory GetMetadata Activity","<p>I have a metadata activity in one of my azure data factory pipeline and its connected to a data lake to get the files. Is there any method available in Azure data factory to sort the files available in the metadata activity based on the file name?</p>

<p>Sample output for the Metadata activity is given below
  ""childitems"" :[
{
 ""name"":""File_20200101.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200501.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200301.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200201.csv"",
 ""type"": ""File""
}
]</p>

<p>I need to get the files in the below-given order.</p>

<p>""childitems"" :[
{
 ""name"":""File_20200101.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200201.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200301.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200501.csv"",
 ""type"": ""File""
}
]</p>

<p>Regards,
sandeep</p>
","<json><azure-data-lake><azure-data-factory>","2020-02-27 14:53:06","1607","1","2","60437194","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#metadata-options"" rel=""nofollow noreferrer"">GetMetadata Activity doc</a>, no sort feature for childItems. So,i'm afraid you have to sort the childItemsby yourself.</p>

<p>In ADF environment,you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> after GetMetadata Activity.Pass the <code>childItems</code> as an array param into Azure Function.Inside azure function,it's easy to sort elements in an array by one element which is common requirement so that you could write code as you want.</p>
"
"60435922","Azure Data Factory GetMetadata Activity","<p>I have a metadata activity in one of my azure data factory pipeline and its connected to a data lake to get the files. Is there any method available in Azure data factory to sort the files available in the metadata activity based on the file name?</p>

<p>Sample output for the Metadata activity is given below
  ""childitems"" :[
{
 ""name"":""File_20200101.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200501.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200301.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200201.csv"",
 ""type"": ""File""
}
]</p>

<p>I need to get the files in the below-given order.</p>

<p>""childitems"" :[
{
 ""name"":""File_20200101.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200201.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200301.csv"",
 ""type"": ""File""
},
{
 ""name"":""File_20200501.csv"",
 ""type"": ""File""
}
]</p>

<p>Regards,
sandeep</p>
","<json><azure-data-lake><azure-data-factory>","2020-02-27 14:53:06","1607","1","2","60543028","<p>I have used a SQL server table to store the array values and then used a lookup activity with a order by file name query  inside another loop to get the sorted filenames. This helped me to solve the sorting problem</p>
"
"60435859","Unable to copy data from Azure Germany Storage account using Azure Data Factory","<p>I have a Storage account in Azure Germany (the sovereign version, not the public cloud) and I want to copy its data to Azure global public storage account using Azure Data Factory.</p>

<p>But Azure Data Factory throws this error:</p>

<blockquote>
  <p>Invalid storage connection string provided to '<strong><em>UnknownLocation</em></strong>'. Check the storage connection string in configuration. No valid combination of account information found. </p>
</blockquote>

<p>I can't use ""AzCopy"" because I want schedule this process.</p>

<p>Scenario - Copy data from ""<em>.blob.core.cloudapi.de"" to ""</em>.blob.core.windows.net"" using Azure Data Factory</p>
","<azure><azure-data-factory><azure-storage-account>","2020-02-27 14:50:08","166","0","1","60461497","<p>I assume the connection to the German sovereign cloud is the one failing. If so try editing that linked service in ADF and adding the following to the end of the connection string:</p>
<pre><code>;EndpointSuffix=core.cloudapi.de;
</code></pre>
<p>This answer is based upon ADF <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?#account-key-authentication"" rel=""nofollow noreferrer"">documentation</a> and storage <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string?#create-a-connection-string-with-an-endpoint-suffix"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"60430850","Move a specfic CSV file prefixed by a word(TCE*) from blob to SQL table","<p>i would like to know how do we copy a specfic patter named files (prefixed by tce)from bob to SQL tabel.</p>

<p>if its a single file im able to load it,but i would like to all the files tht starts with tce.</p>
","<azure><azure-data-factory>","2020-02-27 10:16:12","19","0","1","60437365","<p>You could set wildcard in blob source dataset when you set your copy activity.</p>

<p>Firstly,follow the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#dataset-properties"" rel=""nofollow noreferrer"">statement</a> in blob storage connector, don't set fileName in blob storage dataset setting:</p>

<p><a href=""https://i.stack.imgur.com/yDhHu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yDhHu.png"" alt=""enter image description here""></a></p>

<p>Then set the wildcard in copy activity source.</p>

<p><a href=""https://i.stack.imgur.com/J6cDz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J6cDz.png"" alt=""enter image description here""></a></p>
"
"60430808","How to list and delete empty folders on Azure Data Lake Store Gen1","<h3>Question 1:</h3>
<p>Is there a way to list and delete empty folders on Azure Data Lake Store Gen 1?</p>
<h3>Scenario:</h3>
<p>We require to periodically run a job to delete all empty folders recursively under a root folder in our data lake storage.</p>
<blockquote>
<p>Folder paths cannot be hard coded as there can be 100 s of empty folders.</p>
</blockquote>
<h3>Question 2:</h3>
<p>Can we use Data Factory or Data bricks to perform this operation?</p>
<p>Thanks.</p>
","<azure-data-factory><azure-data-lake><azure-databricks>","2020-02-27 10:14:01","4675","1","2","60504181","<p>Rinks.I implemented your requirement with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">GetMetadata Activity</a>,If-Condition Activity,For each Activity and Delete Activity. Please see my detailed steps:</p>

<p>Step1, i created 2 empty folders and 1 folder contains one csv file in the root path.</p>

<p><a href=""https://i.stack.imgur.com/KVojJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KVojJ.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/rH5Z8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rH5Z8.png"" alt=""enter image description here""></a></p>

<p>Step2, create GetMetadata Activity in the ADF pipeline and output the <code>childItems</code>.</p>

<p><a href=""https://i.stack.imgur.com/lwiqp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lwiqp.png"" alt=""enter image description here""></a></p>

<p>Step3, loop the output by ForEach Activity:<code>@activity('Get Metadata1').output.childItems</code></p>

<p><a href=""https://i.stack.imgur.com/UR6k9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UR6k9.png"" alt=""enter image description here""></a></p>

<p><strong><em>Total structure like:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/s10O0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s10O0.png"" alt=""enter image description here""></a></p>

<p>Step4,Inside ForEach Activity,use another GetMetaData Activity and If-condition Activity:</p>

<p>Set the Directory as <code>@item().name</code></p>

<p><a href=""https://i.stack.imgur.com/nOvyh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nOvyh.png"" alt=""enter image description here""></a></p>

<p>Set the condition expression as <code>@empty(activity('Get Metadata2').output.childItems)</code></p>

<p><strong><em>Total structure like:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/0hlUJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0hlUJ.png"" alt=""enter image description here""></a></p>

<p>Step5,set Delete Activity as the Failed Activity of For each Activity. Set the <code>@item.name()</code> as directory of Delete Activity Dataset.</p>

<p>Test result,<code>test2</code> and <code>test3</code> folder are deleted:</p>

<p><a href=""https://i.stack.imgur.com/9gsVl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9gsVl.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/CZtVk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CZtVk.png"" alt=""enter image description here""></a></p>

<p>Any concern,pls let me know.</p>
"
"60430808","How to list and delete empty folders on Azure Data Lake Store Gen1","<h3>Question 1:</h3>
<p>Is there a way to list and delete empty folders on Azure Data Lake Store Gen 1?</p>
<h3>Scenario:</h3>
<p>We require to periodically run a job to delete all empty folders recursively under a root folder in our data lake storage.</p>
<blockquote>
<p>Folder paths cannot be hard coded as there can be 100 s of empty folders.</p>
</blockquote>
<h3>Question 2:</h3>
<p>Can we use Data Factory or Data bricks to perform this operation?</p>
<p>Thanks.</p>
","<azure-data-factory><azure-data-lake><azure-databricks>","2020-02-27 10:14:01","4675","1","2","60547431","<p>Answered at <a href=""https://social.msdn.microsoft.com/Forums/en-US/526006aa-f378-4766-9aba-532223a44814/how-to-list-and-delete-empty-folders-on-azure-data-lake-store-gen1?forum=AzureDataLake"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/526006aa-f378-4766-9aba-532223a44814/how-to-list-and-delete-empty-folders-on-azure-data-lake-store-gen1?forum=AzureDataLake</a></p>

<p>After mounting in databricks and getting through any permissions issues, one potential (python3) solution:</p>

<p>def recur(item):
  good_to_delete_me = True
  contents = dbutils.fs.ls(item)
  for i in contents:
    if not i.isDir():
      good_to_delete_me = False
    else:
      can_delete_child = recur(i.path)
      good_to_delete_me = good_to_delete_me and can_delete_child
      if can_delete_child:
        test= i.path
        dbutils.fs.rm(test)
  return good_to_delete_me</p>
"
"60430359","Data factory : type of csv column are all string, cannot change them","<p>I am creating a dataflow with datafactory, and currently I am getting a csv from blob.</p>

<p>3 column should be consider as double. When I am doing ""Import schema"", here is the result : </p>

<p><a href=""https://i.stack.imgur.com/fdpoL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fdpoL.png"" alt=""enter image description here""></a></p>

<p>But in Data preview :
<a href=""https://i.stack.imgur.com/FtFHi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FtFHi.png"" alt=""enter image description here""></a></p>

<p>And if I change the type to string, it is working :</p>

<p><a href=""https://i.stack.imgur.com/UAR17.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UAR17.png"" alt=""enter image description here""></a></p>

<p>Why to consider as double is impossible? I need to do a aggregation and sum the value so string are totally incompatible</p>

<p>I try to specify the format, it don't change anything
(here it is a test csv file with only 2 row, so no problem of having string or something like that in other rows)
thanks</p>

<p>Edit : what I don't understand is that the import schema understand the column as double, so why it fail in the data preview then?</p>
","<string><types><double><azure-data-factory>","2020-02-27 09:49:31","429","0","1","60444436","<p>The error is caused by the option <code>Validate schema</code>, please don't choose it.</p>

<p>For example:</p>

<p>This is my csv data:</p>

<p><a href=""https://i.stack.imgur.com/dkOuq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dkOuq.png"" alt=""enter image description here""></a></p>

<p>When click the <code>Validate schema</code>:</p>

<p><a href=""https://i.stack.imgur.com/f5vTJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f5vTJ.png"" alt=""enter image description here""></a>.</p>

<p><a href=""https://i.stack.imgur.com/jRC2L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jRC2L.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/WwHer.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WwHer.png"" alt=""enter image description here""></a></p>

<p>If we don't choose <code>Validate schema</code>, the Data Preview works well:</p>

<p><a href=""https://i.stack.imgur.com/hubG7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hubG7.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/HDjbo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HDjbo.png"" alt=""enter image description here""></a></p>

<p>According my experience, if we choose the <code>Validate schema</code> option in Data Flow, Data Preview often has the error.</p>

<p>Hope this helps. </p>
"
"60426364","I am getting issue while making post request to durable functions (extension of Azure Functions) from ADF and from Postman","<p>I am able to call azure functions from ADF and postman but when I call durable functions from ADF or postman it gives me error:</p>

<pre><code>Operation on target Azure Function1 failed: Call to provided Azure function '' failed with status-'NotFound' and message - 'Invoking Azure function failed with HttpStatusCode - NotFound.'.
</code></pre>

<p>I have tried everything but dont know why this is happening. I have created durable function through portal in functionApp as follow:</p>

<p><strong>DurableFunctionsHttpStart1:</strong></p>

<pre><code>#r ""Microsoft.Azure.WebJobs.Extensions.DurableTask""
#r ""Newtonsoft.Json""

using System.Net;

public static async Task&lt;HttpResponseMessage&gt; Run(
    HttpRequestMessage req,
    DurableOrchestrationClient starter,
    ILogger log)
{
    // Function input comes from the request content.
    dynamic eventData = await req.Content.ReadAsAsync&lt;object&gt;();

    // Pass the function name as part of the route 
    string instanceId = await starter.StartNewAsync(""DurableFunctionsOrchestrator1"", null);

    log.LogInformation($""Started orchestration with ID = '{instanceId}'."");

    return starter.CreateCheckStatusResponse(req, instanceId);
}
</code></pre>

<p><strong>DurableFunctionsOrchestrator1:</strong></p>

<pre><code>/*
 * This function is not intended to be invoked directly. Instead it will be
 * triggered by an HTTP starter function.
 * 
 * Before running this sample, please:
 * - create a Durable activity function (default name is ""Hello"")
 * - create a Durable HTTP starter function
 */

#r ""Microsoft.Azure.WebJobs.Extensions.DurableTask""

public static async Task&lt;List&lt;string&gt;&gt; Run(DurableOrchestrationContext context)
{
    var outputs = new List&lt;string&gt;();

    // Replace ""Hello"" with the name of your Durable Activity Function.
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(""Hello1"", ""Tokyo""));
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(""Hello1"", ""Seattle""));
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(""Hello1"", ""London""));

    // returns [""Hello Tokyo!"", ""Hello Seattle!"", ""Hello London!""]
    return outputs;
}
</code></pre>

<p><strong>Hello1:</strong></p>

<pre><code>/*
 * This function is not intended to be invoked directly. Instead it will be
 * triggered by an orchestrator function.
 * 
 * Before running this sample, please:
 * - create a Durable orchestration function
 * - create a Durable HTTP starter function
 */

#r ""Microsoft.Azure.WebJobs.Extensions.DurableTask""

using System;
using System.Threading;

public static string Run(string name)
{
    Thread.Sleep(260000);
    return $""Hello {name}!"";
}
</code></pre>

<p><strong>From ADF side I am calling DurableFunctionsHttpStart1 function, which will call orchestrator function and activity will be called. Please guide</strong></p>
","<azure-functions><http-status-code-404><azure-data-factory><azure-durable-functions>","2020-02-27 04:42:03","1504","2","1","60430921","<p><strong>Update:</strong>
Reproduce your error on ADF, this is what you are facing now:</p>

<p><a href=""https://i.stack.imgur.com/9sAXd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9sAXd.png"" alt=""enter image description here""></a></p>

<p>If your function in ADF is like this, you will get the above error.
<a href=""https://i.stack.imgur.com/ybQCg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ybQCg.png"" alt=""enter image description here""></a>
(Also the method need to be post)</p>

<p>This is the configuration of the azure function in ADF should be:</p>

<p><a href=""https://i.stack.imgur.com/RXPqB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RXPqB.png"" alt=""enter image description here""></a></p>

<p>Then it works fine on my ADF.</p>

<p><strong>Original Answer:</strong></p>

<p>These code work fine on my side:</p>

<p><strong>DurableFunctionsHttpStart1</strong></p>

<pre><code>#r ""Microsoft.Azure.WebJobs.Extensions.DurableTask""
#r ""Newtonsoft.Json""

using System.Net;

public static async Task&lt;HttpResponseMessage&gt; Run(
    HttpRequestMessage req,
    DurableOrchestrationClient starter,
    string functionName,
    ILogger log)
{
    // Function input comes from the request content.
    dynamic eventData = await req.Content.ReadAsAsync&lt;object&gt;();

    // Pass the function name as part of the route 
    string instanceId = await starter.StartNewAsync(""DurableFunctionsOrchestrator1"", eventData);

    log.LogInformation($""Started orchestration with ID = '{instanceId}'."");

    return starter.CreateCheckStatusResponse(req, instanceId);
}
</code></pre>

<p><strong>DurableFunctionsOrchestrator1</strong></p>

<pre><code>/*
 * This function is not intended to be invoked directly. Instead it will be
 * triggered by an HTTP starter function.
 * 
 * Before running this sample, please:
 * - create a Durable activity function (default name is ""Hello"")
 * - create a Durable HTTP starter function
 */

#r ""Microsoft.Azure.WebJobs.Extensions.DurableTask""

public static async Task&lt;List&lt;string&gt;&gt; Run(DurableOrchestrationContext context)
{
    var outputs = new List&lt;string&gt;();

    // Replace ""Hello"" with the name of your Durable Activity Function.
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(""Hello1"", ""Tokyo""));
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(""Hello1"", ""Seattle""));
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(""Hello1"", ""London""));

    // returns [""Hello Tokyo!"", ""Hello Seattle!"", ""Hello London!""]
    return outputs;
}
</code></pre>

<p><strong>Hello1</strong></p>

<pre><code>/*
 * This function is not intended to be invoked directly. Instead it will be
 * triggered by an orchestrator function.
 * 
 * Before running this sample, please:
 * - create a Durable orchestration function
 * - create a Durable HTTP starter function
 */

#r ""Microsoft.Azure.WebJobs.Extensions.DurableTask""
using System;
using System.Threading;
public static string Run(string name)
{
    Thread.Sleep(260000);
    return $""Hello {name}!"";
}
</code></pre>

<p>And all the function triggered well, at last:</p>

<p><a href=""https://i.stack.imgur.com/1HYTL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1HYTL.png"" alt=""enter image description here""></a></p>
"
"60425079","Process every item in a Cosmos DB using Azure Data Factory","<p>I am hoping this is an appropriate usecase for Azure Data Factory.</p>

<p>I have a Cosmos DB that has ~200k records, and I would like to iterate over the entire database, passing each record into a Logic App.  Is there an easy way to foreach over every record?  I thought that Azure Data Factory would have this capability, but the ""Lookup + Foreach"" combo doesn't like the number of records I have.  My attempts at creating a while loop with the ""Lookup + Foreach"" pipeline also feels slightly clunky.</p>

<p>I don't feel that 200k records is a large dataset. Am I missing something? Is there a better way?</p>
","<azure-cosmosdb><azure-data-factory>","2020-02-27 01:31:53","214","1","1","60425302","<p>I believe the ideal way is for you to use <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/change-feed"" rel=""nofollow noreferrer"">Change feed</a> mechanism.  This would be a perfect use-case for it.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/change-feed-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/change-feed-functions</a></p>
"
"60424706","Import CSV with variable columns into Sql Database using Azure Data Factory","<p>I am trying to import csv files from blob storage into a sql server database using Azure Data Factory.  These csv files do not have a consistent format.  Not all csv files have the same number of columns.  </p>

<p>How can I check to see if a column exists in the csv file and if it doesn't just insert a NULL value for that column into the SQL Server database?</p>
","<azure-data-factory>","2020-02-27 00:41:02","1589","0","3","60428047","<p>I am assuming you are using ADF v2.</p>

<p>You can see the column presence by clicking on the ADF activity and then going to Mapping tab, it will enable you to map the columns present in CSV file with the input dataset and from there you will be able to know if there are any missing columns as the mapping would fall short for columns from input file.</p>

<p>If you don't see any mapping present already, then you need to click on Import Schemas to map the file columns with input dataset.</p>

<p>To insert a NULL value in database, you can write a stored procedure and call that via ADF or you can keep the database column with a default value as NULL, so if the data is not present in the file for that column, then automatically it will take value as NULL.</p>

<p>Let me know if you have any further questions.</p>

<p>Thanks and Regards,</p>

<p>Pratik Somaiya</p>
"
"60424706","Import CSV with variable columns into Sql Database using Azure Data Factory","<p>I am trying to import csv files from blob storage into a sql server database using Azure Data Factory.  These csv files do not have a consistent format.  Not all csv files have the same number of columns.  </p>

<p>How can I check to see if a column exists in the csv file and if it doesn't just insert a NULL value for that column into the SQL Server database?</p>
","<azure-data-factory>","2020-02-27 00:41:02","1589","0","3","73562158","<p>somaiya,</p>
<p>I didn't understand how I could apply your solution in the translator</p>
<pre><code>you can tweak it according to your use case: @json(activity('FetchingColumnMapping').output.firstRow.ColumnMapping)
</code></pre>
<p>In my scenario, CSV file has rows with 10 columns and other rows with 23 columns. Thus, when I run the pipeline, I got the error below:</p>
<p>UserErrorInvalidColumnMappingColumnNotFound error</p>
<p>So, how would I CSV add all 23 columns to the output table (i.e. sink), assigning &quot;null&quot; to the columns of the rows, which have only 8 total?</p>
<p>The translatos chunk (json) has only 10 columns</p>
<pre><code> &quot;translator&quot;: {
        &quot;type&quot;: &quot;TabularTranslator&quot;,
        &quot;mappings&quot;: [
            {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;ordinal&quot;: 1
                },
                &quot;sink&quot;: {
                    &quot;name&quot;: &quot;Prop_0&quot;,
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;physicalType&quot;: &quot;nvarchar&quot;
                }
            },
            ...
            {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;ordinal&quot;: 10
                },
                &quot;sink&quot;: {
                    &quot;name&quot;: &quot;Prop_9&quot;,
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;physicalType&quot;: &quot;nvarchar&quot;
                }
            }
        ],
</code></pre>
<p>The CSV looks like as following</p>
<p><a href=""https://i.stack.imgur.com/caM2N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/caM2N.png"" alt=""enter image description here"" /></a></p>
"
"60424706","Import CSV with variable columns into Sql Database using Azure Data Factory","<p>I am trying to import csv files from blob storage into a sql server database using Azure Data Factory.  These csv files do not have a consistent format.  Not all csv files have the same number of columns.  </p>

<p>How can I check to see if a column exists in the csv file and if it doesn't just insert a NULL value for that column into the SQL Server database?</p>
","<azure-data-factory>","2020-02-27 00:41:02","1589","0","3","73562373","<p>@pratik-somaiya
So far, I figured out how to add apply it, in my piepline activity.</p>
<pre><code>...
&quot;translator&quot;: {
                &quot;value&quot;: &quot;@json(activity('FetchingColumnMapping').output.firstRow.ColumnMapping)&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            }
....
</code></pre>
<p>However, it returns an error as following:</p>
<p>&quot;The output of activity 'FetchingColumnMapping' can't be referenced since it is either not an ancestor to the current activity or does not exist.&quot;</p>
<p><a href=""https://i.stack.imgur.com/RluEN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RluEN.png"" alt=""enter image description here"" /></a></p>
<p>Pipeline code has been attached below:</p>
<pre><code>{
&quot;name&quot;: &quot;pipeline2&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;FetchingColumnMapping&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;,
                        &quot;skipLineCount&quot;: 3
                    }
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSink&quot;,
                    &quot;writeBehavior&quot;: &quot;insert&quot;,
                    &quot;sqlWriterUseTableLock&quot;: false
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;value&quot;: &quot;@json(activity('FetchingColumnMapping').output.firstRow.ColumnMapping)&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;CSV_DelimitedText&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;AzureSqlBlob&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        }
    ],
    &quot;folder&quot;: {
        &quot;name&quot;: &quot;DEV/IURI&quot;
    },
    &quot;annotations&quot;: []
 }
}
</code></pre>
"
"60421130","How to find which activity called another activity in my ADF Pipeline","<p>I have created a pipeline (LogPipeline) that logs other pipelines' status to a database. The idea is that every pipeline will call the LogPipeline at the start and at the end by passing pipeline name and pipeline ID along with other parameters like started/ended/failed. </p>

<p>The last parameter is ""Reason"" where I want to capture the error message of why a pipeline may have failed.</p>

<p>However, in a given pipeline there are multiple activities that could fail. So I want to direct any and all failed activities to my Execute Pipeline activity and pass the error message. </p>

<p>But on the Execute Pipeline when filling out the parameters, I can only reference an activity by its name, e.g. <code>Reason = @activity['Caller Activity'].Error.Message.</code><br>
However, since multiple activities are calling this Execute Pipeline, is there a way to say 
<code>Reason = @activity[activityThatCalledExecutePipeline].Error.Message?</code></p>
","<azure-data-factory>","2020-02-26 19:26:40","2106","2","1","60425148","<p>If my understanding is correct,there are multiple activities call the LogPipeline and you want to get those failed activities' names so that you could know the names inside LogPipeline. Per my knowledge,your requirement is not supported in ADF.</p>

<p>I'm not sure why you have to construct such complex scenario,even though you just want to log the specific fail activities and error messages anyway which is common requirement.There are many monitor ways supported by ADF,please follow below links:</p>

<p>1.<a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#alerts"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#alerts</a></p>

<p>2.<a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically</a></p>

<p>I would suggest you getting an idea of <code>Alerts and Monitor</code> in ADF portal:</p>

<p><a href=""https://i.stack.imgur.com/7ZGHI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7ZGHI.png"" alt=""enter image description here""></a></p>

<p>And you could set the Target Criteria</p>

<p><a href=""https://i.stack.imgur.com/bJ8sA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bJ8sA.png"" alt=""enter image description here""></a></p>

<p>It includes:</p>

<p><a href=""https://i.stack.imgur.com/JZLWp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JZLWp.png"" alt=""enter image description here""></a></p>
"
"60419859","How to load multiple files from azure blob to azure synapse(Dw) by using azuredatafactory?","<p>I am trying to load multiple files from azure blob to azure sql dw by using azure data factory.Below is my code.And I am facing the highlighted error.Could anyone suggest.
I am pasting my adf code json here.</p>

<p>I am getting below Highlighted at copy activity stage.</p>

<pre><code>{
    ""name"": ""DelimitedText11"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage2"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""FileName"": {
                ""type"": ""string""
            }
        },
        ""annotations"": [],
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""fileName"": {
                    ""value"": ""@dataset().FileName"",
                    ""type"": ""Expression""
                },
                ""container"": ""input""
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": """",
            ""firstRowAsHeader"": true,
            ""quoteChar"": """"
        },
        ""schema"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>Error:</p>

<pre><code>{
        ""errorCode"": ""2200"",
        ""message"": ""ErrorCode=UserErrorMissingPropertyInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Required property 'fileName' is missing in payload.,Source=Microsoft.DataTransfer.ClientLibrary,'"",
        ""failureType"": ""UserError"",
        ""target"": ""Copy data1"",
        ""details"": []
    }
</code></pre>
","<azure-data-factory><azure-container-service><azure-synapse>","2020-02-26 18:00:37","1929","0","1","60425666","<p>If you want load multiple files from azure blob to Azure SQL Data Warehouse, please must set the <code>Wildcard file path</code> in Source dataset. Or you will always get the error 2200:</p>

<pre><code>{
        ""errorCode"": ""2200"",
        ""message"": ""ErrorCode=UserErrorMissingPropertyInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Required property 'fileName' is missing in payload.,Source=Microsoft.DataTransfer.ClientLibrary,'"",
        ""failureType"": ""UserError"",
        ""target"": ""Copy data1"",
        ""details"": []
    }
</code></pre>

<p>For example,  I have two csv files with same schema and load them to my Azure SQL Data Warehouse table <code>test</code>.</p>

<p><strong>Csv files:</strong></p>

<p><a href=""https://i.stack.imgur.com/q8Znb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q8Znb.png"" alt=""enter image description here""></a></p>

<p><strong>Source Dataset:</strong></p>

<p><a href=""https://i.stack.imgur.com/1A8Wj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1A8Wj.png"" alt=""enter image description here""></a></p>

<p><strong>Source setting: choose all the csv files in source container:</strong></p>

<p><a href=""https://i.stack.imgur.com/XEiTj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XEiTj.png"" alt=""enter image description here""></a>.</p>

<p><strong>Sink dataset:</strong></p>

<p><a href=""https://i.stack.imgur.com/zM0xM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zM0xM.png"" alt=""enter image description here""></a></p>

<p><strong>Sink settings:</strong></p>

<p><a href=""https://i.stack.imgur.com/KraB0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KraB0.png"" alt=""enter image description here""></a></p>

<p><strong>Mapping:</strong>
<a href=""https://i.stack.imgur.com/xq40j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xq40j.png"" alt=""enter image description here""></a></p>

<p><strong>Settings:</strong>
<a href=""https://i.stack.imgur.com/DI2MB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DI2MB.png"" alt=""enter image description here""></a></p>

<p><strong>Execute the pipeline and check the data in the ADW:</strong>
<a href=""https://i.stack.imgur.com/kVtIC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kVtIC.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/9kFvG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9kFvG.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"60418287","Data Factory DevOps SSIS-IntegrationRuntime","<p>We're planning to use CI/CD pipelines for Data Factory. 
In one of our pipelines we use SSIS packages that needs to be called. To call SSIS packages you need to specify an Azure-SSIS IR that must be used. </p>

<p><a href=""https://i.stack.imgur.com/5cAH9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5cAH9.png"" alt=""enter image description here""></a></p>

<p>The Azure-SSIS IR has a different naming on every environment.
Now, it is not possible to set this value dynamic (the option ""Add dynamic content [Alt+P]"" is not available on this field)
Is there a simple solution to change the Azure-SSIS IR during the deployment?</p>

<p>Thanks in advance</p>
","<azure><ssis><azure-data-factory>","2020-02-26 16:22:40","80","1","1","60428551","<p>Your linked services aren't named by environment are they? (they most definitley should not be)</p>

<p>The default out of the box cloud runtime is <em>also</em> not named by environment.</p>

<p><em>Your runtimes should not be named by environment either.</em></p>

<p>IMHO your naming convention is incorrect. You should challenge it - there's no reason to include an environment designator in any runtime names.</p>

<p>Yes, your parent data factory should definitely have a different name per environment. That's where the distinction is made. Your runtimes should not.</p>

<p>In direct answer to your question, the way I have dealt with this in the past is added a powershell script task to the build part of DevOps that transforms the deployment asset and basically find/replaces the name the  delivers the result as a build artifact</p>
"
"60416323","Mapping data flows, schema for parametrised sink and source","<p>I am currently trying to rewrite some of my SSIS packages to ADF mapping data flows. I want to use parameters in order to avoid creating hundreds of different sinks and sources. However, I have encountered one problem on the way.</p>

<p>Let's take one sink as an example:
<a href=""https://i.stack.imgur.com/lFzrg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lFzrg.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Oz0hd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oz0hd.png"" alt=""enter image description here""></a></p>

<p>It doesn't have any schema defined. Then if I try to use it in my dataflow(and provide a value for my parameter), I get:</p>

<p><a href=""https://i.stack.imgur.com/JwVUv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JwVUv.png"" alt=""enter image description here""></a></p>

<p>I can avoid this error by hardcoding schema on my sink but then this schema is used wherever I decide to use this sink, so I cannot really use it mutiple times...</p>

<p>Thank you for any advice!</p>
","<azure><azure-data-factory><dataflow>","2020-02-26 14:37:11","170","0","1","60421083","<p>If are attempting to maximize reuse of a single data flow, you should consider just using auto-mapping for the sink and set ""allow schema drift"" to on so that you can gain maximum flexibility in your mappings.</p>
"
"60415960","Stored Procedure Activity procedure parameter with int data type in Azure Data Factory","<p>I am working on a stored procedure activity to load data into one of my table. The procedure expects an int parameter to be passed and the value for this comes from the lookup output firstrow field. But when I pass the lookup output value for the parameter the pipeline is not getting executed. I have tried to convert the values as well and even used a variable to store. Any option to process the same?</p>

<p>Regards,
Sandeep</p>
","<stored-procedures><azure-data-factory>","2020-02-26 14:19:27","168","1","1","60428105","<p>Can you send the detailed error when you try to execute the pipeline?</p>

<p>Thanks,</p>

<p>Pratik</p>
"
"60407986","Can we extract a zip file in copy activity - Azure Datafactory","<p>i have zip file i would like to uncproesss the file and get the csv file and push it to the blob.i can achive in.gz but .zip file we are not able to.</p>

<p>could you please assit here.</p>

<p>Thanks
Richard</p>
","<azure><azure-data-factory><azure-data-explorer>","2020-02-26 06:28:38","1888","1","1","60409502","<p>You could set binary format as source and sink dataset in ADF copy activity.Select Compression type as <code>ZipDefalte</code> following this link: <a href=""https://social.msdn.microsoft.com/Forums/en-US/a46a62f2-e211-4a5f-bf96-0c0705925bcf/working-with-zip-files-in-azure-data-factory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/a46a62f2-e211-4a5f-bf96-0c0705925bcf/working-with-zip-files-in-azure-data-factory</a></p>

<p>Source:</p>

<p><a href=""https://i.stack.imgur.com/ydt3r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ydt3r.png"" alt=""enter image description here""></a></p>

<p>Sink:</p>

<p><a href=""https://i.stack.imgur.com/X2HOQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X2HOQ.png"" alt=""enter image description here""></a></p>

<p>Test result in sink path:</p>

<p><a href=""https://i.stack.imgur.com/YWWFz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YWWFz.png"" alt=""enter image description here""></a></p>
"
"60388878","How to use the build pipeline to deploy azure data factory resources?","<p>I have a Azure data factory to deploy using azure devops. So I have created a build pipeline using the ""Publish Build Artifacts"" task and created the artifacts folder named ""drop"" which has the resources i want to deploy.</p>

<p>I need help with the release pipeline for the same. Which task should i use in my release pipeline to deploy this artifact folder ""drop""?</p>

<p>I initially tried with ARM template deployment but it doesn't make use of the drop folder and deploys everything i.e, the entire data factory everytime. So i created a build folder wherein i have only limited things to deploy. But i am now stuck with the release pipeline task for the same.</p>

<p>Any help would be great. Thanks</p>
","<azure><azure-devops><azure-data-factory>","2020-02-25 06:45:39","123","0","1","60428190","<p>You need to use version 2 of RG deployment:</p>

<p><a href=""https://i.stack.imgur.com/ZiRzj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZiRzj.png"" alt=""enter image description here""></a></p>

<p>And then you need to select, Create or Update Resource Group.</p>

<p>You need to provide path of ADF ARM templates in the template path text box.</p>

<p>Thanks,</p>

<p>Pratik</p>
"
"60383115","Azure Data Factory - Multiply sources - Transform and output as JSON only","<p>Hi I am still new to the ADF scene, my question is if I have multiple sources (REST API, on-prem SQL server) and all I want to do is to fetch the data and transform it into a specific JSON format and POST it  to another REST API endpoint and I do not need to store the data on the azure cloud. </p>

<pre><code>Source (REST API, on-prem SQL) -&gt; Transform to specific JSON format -&gt; POST JSON to another REST API endpoint
</code></pre>

<p>What would be the correct approach on ADF? Do I just use Azure custom activity to run a python script for transforming? Appreciate some pointers.</p>
","<azure><azure-data-factory>","2020-02-24 20:04:05","378","1","1","60411319","<p>According to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">copy activity document</a>,you only could get data from REST API,not support for sending request to REST API.Additional,copy activity doesn't support multiple source mapping to single sink(which is mentioned in your question,you have REST api and SQL 
 DB source dataset).</p>

<p>So,my rough idea is using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> to get data from multiple source in the azure function method and return the json format as you want.Then execute <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> to call your REST API,set the json in the request body.</p>

<p>In doing so, I don't feel very different from custom activity. Custom activity is also more flexible.You could balance above solutions.</p>
"
"60381510","Error in triggering mail using web activity in Azure data factory","<p>I am trying to setup mail notification in event of copy activity fails(using For logic apps). Unfortunately I am getting the below error, however when I do the same (copy --> web activity) outside foreach activity, works fine and mail gets triggered in the event of failure.</p>

<p><strong>Error Message from Web Activity:</strong></p>

<blockquote>
  <p>""message"": ""The expression 'activity('Copy SQL to Blob').Error.message' cannot be evaluated because property 'Error' cannot be selected.</p>
</blockquote>

<p><strong>Screenshot from adf:</strong>
<a href=""https://i.stack.imgur.com/R1wFT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R1wFT.png"" alt=""enter image description here""></a></p>

<p><strong>Body details of web activity:</strong></p>

<pre><code>{""DataFactoryName"":""@{pipeline().DataFactory}"",""PipelineName"":""@{pipeline().Pipeline}"",""Subject"":""Data Load: @{pipeline().Pipeline} An Error has occurred!!"",""ErrorMessage"":""@{activity('Copy SQL to Blob').Error.message}"",""Activity Name"": ""Copy Activity"",""EmailTo"":""user@domain.com""}
</code></pre>

<p><strong>Error Message from Copy Activity:</strong></p>

<p><a href=""https://i.stack.imgur.com/0XfrT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0XfrT.png"" alt=""enter image description here""></a>
Like said above, the same web activity works fine outside the iterator. Any inputs greatly appreciated?</p>
","<azure-logic-apps><azure-data-factory>","2020-02-24 17:57:48","608","0","1","60418799","<p>When i want to catch the error from a copy activity i always use the expression:
@activity('Copy Data').output.errors[0].message and that works fine even in a loop</p>
"
"60377323","How to create azure datafactory pipeline?","<p>Could anyone suggest, How to create pipeline for specific tables from azure synapse to azure blob storage.</p>

<p>Example:I have 10 tables like 
table1,
table2,
table3,
...,
...,
I want to fetch only table1, table5, table 7,table 8,table 9, table 10 and need to load into azure blob stoarge.</p>

<p>My synapse and blob both in azure cloud.</p>
","<azure-data-factory>","2020-02-24 13:50:33","70","0","1","60386457","<p>This is what I suggest .</p>

<ol>
<li>Create a variable of type array and load the table names.</li>
<li>Use a for each activity </li>
<li>Inside the foreach activity use a copy activity , add the source as the synapse and you will have to use parameterized dataset to pass the table name . The sink should be blob .</li>
</ol>

<p>This article does the reverse , but it will be give you some idea of the logic . 
<a href=""https://social.technet.microsoft.com/wiki/contents/articles/53367.azure-data-factory-v2-copy-content-of-multiple-blob-to-different-sql-tables.aspx"" rel=""nofollow noreferrer"">https://social.technet.microsoft.com/wiki/contents/articles/53367.azure-data-factory-v2-copy-content-of-multiple-blob-to-different-sql-tables.aspx</a></p>

<p>Let me know how it goes .</p>
"
"60374242","problem reading time stamp data from api in Azure Data Factory","<p>While reading data from http using rest api as source In copy data’s import schema functionality in Azure Data Factory I am unable to read proper format data in time stamp values. Can anyone please let me know how do u fix this problem?</p>

<p>Thanks  </p>
","<azure-data-factory>","2020-02-24 10:47:21","46","-1","2","60386492","<p>If you can share some data as to what is that you are seeing and what is expected , that will be great . My hunch is that the API you are referring is not a public facing and so helping without more data is really tough .</p>
"
"60374242","problem reading time stamp data from api in Azure Data Factory","<p>While reading data from http using rest api as source In copy data’s import schema functionality in Azure Data Factory I am unable to read proper format data in time stamp values. Can anyone please let me know how do u fix this problem?</p>

<p>Thanks  </p>
","<azure-data-factory>","2020-02-24 10:47:21","46","-1","2","60482645","<p>Here is the tag as read from ADF and actual tag from URL.</p>

<p>ADF Read:
""modified"": 1452859829000,</p>

<p>Actual URL data:
 2016-01-15T05:10:29-07:00</p>
"
"60371565","Is there a way to deploy Azure Data factory other than using ARM templates?","<p>I want to deploy my azure data factory using Azure DevOps. I have worked with the deployment using ARM templates with resource group deployment tasks during the release pipeline, but I want to know any other way for the ADF deployment apart from ARM templates?</p>

<p>The reason being that ARM templates deploy everything but I have lot of sample pipelines in my DEV environment which cant be deployed in other QA or Production. But ARM templates are restricting me to do so.</p>
","<azure><azure-data-factory><azure-deployment><azure-rm-template>","2020-02-24 07:55:52","736","0","1","60372001","<ol>
<li><a href=""https://www.terraform.io/docs/providers/azurerm/r/data_factory.html"" rel=""nofollow noreferrer"">Terraform</a></li>
<li><a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/?view=azps-3.5.0"" rel=""nofollow noreferrer"">Azure Powershell</a></li>
<li><a href=""https://docs.ansible.com/ansible/latest/modules/azure_rm_resource_module.html#azure-rm-resource-module"" rel=""nofollow noreferrer"">Ansible</a> resource module \ Azure Cli \ SDK's \ Rest Api </li>
</ol>

<p>Last one requires bit of creativity, you'd have to construct API call on your own.</p>
"
"60368475","Azure Datafactory Pipeline execution status","<p>It is kind of annoying we cannot change the logical order(AND/OR) of the Activity dependencies. however, I have got another issue. having said that I have activities for on failure to log the error messages in DB, since the logging activity succeeds, the entire pipeline  succeeds too! is there any workaround to say if any activities failed the entire pipeline and the parent pipeline, if it is called from another pipeline, should be failed either? </p>

<p>In my screenshot, i have selected the on completion dependencies to log the successful or error.    </p>

<p><a href=""https://i.stack.imgur.com/FawCx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FawCx.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-02-24 01:22:07","822","0","1","60402862","<p>I see that you defined ""On Success"" of the copy activity to run ""usp_postexecution"" . Please define a ""On failure""  of the copy activity and add any activity ( may be a set variable for testing  ) and execute the pipeline . The pipeline will fail .</p>

<p>Just to give you more context what i tried .
I have a variable name ""test"" of the type boolean and I am failing it deliberately ( by assigning to a non-boolean value of true1 ) </p>

<p>Pipeline will fail when I define both success and failure scenarios .</p>

<p><a href=""https://i.stack.imgur.com/fwi3d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fwi3d.png"" alt=""enter image description here""></a></p>

<p>The pipeline will succeed  when you have only ""Failure"" defined </p>

<p><a href=""https://i.stack.imgur.com/CUDy0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CUDy0.png"" alt=""enter image description here""></a></p>
"
"60364819","Copy table data from Azure SQL Database to another Azure SQL Database?","<p>I have to copy table data from one Azure SQL Database to another Azure SQL Database which are under same Azure server.</p>

<p>Is there any way to do this using Azure data factory? Also, this needs to be scheduled as a daily feed.</p>

<p>Edit : How can we add more tables to the existing dataset ? I have created this for 3 tables, now i want to add two more tables to this, how ?</p>
","<sql-server><azure><azure-data-factory><azure-table-storage><azure-blob-storage>","2020-02-23 17:20:27","3012","3","2","60364880","<p>Did you have a look at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server"" rel=""nofollow noreferrer"">Copy data to and from SQL Server by using Azure Data Factory</a>?. </p>

<blockquote>
  <p>In Azure Data Factory, you can use the Copy activity to copy data
  among data stores located on-premises and in the cloud. After you copy
  the data, you can use other activities to further transform and
  analyze it</p>
</blockquote>

<p>You can have a look at the steps from here on how to configured a <a href=""https://ajitpatra.com/2018/10/23/azure-copy-data-from-one-database-to-another-using-azure-data-factory-ii/"" rel=""nofollow noreferrer""><code>triggered pipeline.</code></a></p>

<p>One important thing to remember is that you'll have to define the data set (with or without schema) for all tables that require copy for any source-destination combination.</p>
"
"60364819","Copy table data from Azure SQL Database to another Azure SQL Database?","<p>I have to copy table data from one Azure SQL Database to another Azure SQL Database which are under same Azure server.</p>

<p>Is there any way to do this using Azure data factory? Also, this needs to be scheduled as a daily feed.</p>

<p>Edit : How can we add more tables to the existing dataset ? I have created this for 3 tables, now i want to add two more tables to this, how ?</p>
","<sql-server><azure><azure-data-factory><azure-table-storage><azure-blob-storage>","2020-02-23 17:20:27","3012","3","2","60406883","<p>you can think of elastic queries(preview)-for cross database queries and elastic jobs (preview) for job scheduling.</p>

<ol>
<li>Utilize Elastic query for bringing result from another database on the same server. <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-elastic-query-getting-started-vertical"" rel=""nofollow noreferrer"">Read more on Elastic Query</a>. The advantage is it is coming as free with Azure SQL. </li>
</ol>

<blockquote>
  <p>Elastic database query (preview) for Azure SQL Database allows you to
  run T-SQL queries that span multiple databases using a single
  connection point.</p>
</blockquote>

<ol start=""2"">
<li>Schedule Elastic job(currently in preview) which can be used to schedule job in a Azure SQL database. <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-job-automation-overview"" rel=""nofollow noreferrer"">Read more on Elastic jobs</a></li>
</ol>

<blockquote>
  <p>Elastic Database Jobs (preview) are Job Scheduling services that
  execute custom jobs on one or many Azure SQL Databases.</p>
</blockquote>
"
"60362140","Azure Data Factory: common logic before/after all activities","<h2>Objective</h2>

<p>I'm going to implement custom auditing on top of ADF pipelines. My ADF pipelines consist of activities of different types: ingesting, Databricks, loading results to data warehouse, etc. At the beggining and the end of each activity I would like to write some information to the auditing database.</p>

<h2>The issue</h2>

<p>I don't like to wrap each pipeline activity with extra auditing activies. I would like to do some kind of aspects/advices common for all activities. But I suppose that isn't a way ADF is supposed to be used.</p>

<p>Of course, it's easy to wrap custom Databricks/Python code with auditing, but how about ingesting/uploading activities?</p>

<h2>Question</h2>

<p><em>What is the best way to implement custom logic before/after all ADF pipeline activities?</em> </p>

<h2>P.S.</h2>

<p>In Apache NiFi it's possible to access NiFi's logs and build a separate parallel pipeline that will parse and write logs to audit database. See <a href=""https://community.cloudera.com/t5/Community-Articles/NiFi-Easy-custom-logging-of-diverse-sources-in-mere-seconds/ta-p/247947"" rel=""nofollow noreferrer"">the article</a>. As far as I know, I can configure ADF to write logs to Azure Storage. Than read logs from there and write to audit database. But this sounds to me like a bad design. Any better options? </p>
","<azure><azure-data-factory>","2020-02-23 12:22:29","445","1","1","60369960","<p>As far as I know, there is no direct way to do this apart from the approach you have mentioned i.e. to wrap each activity between pre and post processing logic and get it audited by logging your information to your sink, however I think it would be a good feature if activities natively starts supporting pre and post processing events in ADF itself and allow to invoke webhooks so that any pre and post processing data can be sent over to webhooks. </p>

<p>Further, You can submit an idea or suggestion to the team at
<a href=""http://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">feedback.azure.com/forums/270578-data-factory</a> </p>
"
"60360674","How can I reference a JSON source for a derived column action in Azure Data Factory","<p>I'm new to Azure Data Factory. I've been able to generate a set of JSON files from a REST API source using a Pipeline. Each file consists of one top level JSON object with an array of up to 100 child objects. The output is saved to an Azure Blob Storage container.</p>

<p>I now want to use a Mapping Data Flow to modify the JSON before I write it to Azure SQL, however I'm struggling with the syntax. I've configured the source to point to the directory containing the JSON files. The Source Projection tab displays the correct schema. I can preview the data and I see a row for each file and I can expand the child objects to see the full structure. </p>

<p>However, when I add a Derived Column action, the Input Schema is blank in the Expression Builder. I can refer to the top level elements in the source using the byName and byPosition functions, but I don't know how I can reference the child elements. </p>

<p>The examples that I have been able to find online use a SQL table or CSV file as a source. I can't find any examples that use hierarchical data as the source for a derived column.</p>

<p>Am I missing something? Is this scenario supported?</p>
","<json><azure><azure-data-factory>","2020-02-23 09:06:43","1142","0","1","60443783","<p>I found a way to achieve what I want. This may not be the best approach, but it works. </p>

<p>It seems that it is difficult to deal with JSON that has multiple hierarchies as a source for copy data activities. You can choose one level of repeating data to map to a table structure (the Collection Reference property on the Mapping tab).</p>

<p>In my scenario, there was additional repeating data within the data I was mapping to my table. I updated the mapping to write the child JSON data to a text field in my SQL table. To do this, I needed to use the Azure Data Factory JSON editor for my pipeline. You can access this from the ""Code"" link in the top right corner of the pipeline visual editor.</p>

<p>I added the following line after the closing bracket for the ""mappings"" array for my copy activity:
""mapComplexValuesToString"": true</p>

<p>The full path to the mapping array in the activity definition is typeProperties - translator - mappings. Make sure your commas are correct after you add the new element.</p>

<p>With this approach, I had a row in my SQL table for each array item in my Collection Reference. The scalar child elements in the array items are mapped to table columns and the child JSON element is written to a data column in the same table.  </p>

<p>To extract the values I need within the child JSON, I created a SQL view that uses the CROSS APPLY OPENJSON syntax. This allows me to treat the JSON in the data field similar to a related table. You can specify the structure that your JSON is in. If you have nested data in your JSON, you can apply the same approach for each level. </p>

<p>The OPENJSON command is only supported by more recent versions of SQL Server. I'm using Azure SQL, so that works for me.  </p>
"
"60357168","Sink must be binary when source is binary dataset","<p>I am new to the Azure Data Factory scene, trying out the copy data tutorial where I have an InputDataset with emp.txt with the following information:</p>

<pre><code>firstname, lastname
John, Doe
Jane, Doe
</code></pre>

<p>And I want to have an OutputDataset in json format.</p>

<pre><code>{
 ""firstname"" : John,
 ""lastname"" : Doe
}
</code></pre>

<p>How can I set it up correctly in the Pipeline? It keeps telling me sink must be binary when source is binary dataset.</p>
","<azure><azure-data-factory>","2020-02-22 21:49:26","4071","2","1","60368243","<p>Your requirement is very common,it could be done in ADF copy activity exactly.Please don't use binary format, use DelimitedText as source dataset and Json as sink dataset instead.</p>

<p>Please see my example:</p>

<p>DelimitedText dataset configuration:</p>

<p><a href=""https://i.stack.imgur.com/AFf4n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AFf4n.png"" alt=""enter image description here""></a></p>

<p>And you could import Schema to check the key-value:</p>

<p><a href=""https://i.stack.imgur.com/3CLST.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3CLST.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/nUV9j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nUV9j.png"" alt=""enter image description here""></a></p>

<p>Json dataset configuration:</p>

<p><a href=""https://i.stack.imgur.com/K8b9J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K8b9J.png"" alt=""enter image description here""></a></p>

<p>Select <code>Array of Objects</code> in Json Sink:</p>

<p><a href=""https://i.stack.imgur.com/VOUNc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VOUNc.png"" alt=""enter image description here""></a></p>

<p>Test Output:</p>

<p><a href=""https://i.stack.imgur.com/VE2Y4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VE2Y4.png"" alt=""enter image description here""></a></p>
"
"60342953","Failed to create for each activity in azure data factory using python","<p>I am trying to create azure data factory pipelines and resources using python. I was successful with certain ADF activities like Lookup, Copy .. but the problem I am facing here is I am trying to copy few tables from SQL to blob using FOR Each activity and it is throwing the below error</p>

<p>How would you create activities inside for each activity? Any inputs is greatly appreciated. Thanks!</p>

<p>Ref: <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.foreachactivity?view=azure-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.foreachactivity?view=azure-python</a></p>

<p><strong>Error Message</strong></p>

<blockquote>
  <p>TypeError: 'CopyActivity' object is not iterable</p>
</blockquote>

<p><strong>Code Block</strong></p>

<pre><code>## Lookup Activity
ls_sql_name = 'ls_'+project_name+'_'+src_svr_type+'_dev'
linked_service_name =LinkedServiceReference(reference_name=ls_sql_name)
lkp_act_name ='Get Table Names'
sql_reader_query = ""SELECT top 3 name from sys.tables where name like '%dim'""
source = SqlSource(sql_reader_query= sql_reader_query)
dataset= {""referenceName"": ""ds_sql_Dim_input"",""type"": ""DatasetReference""}
LookupActivity_ = LookupActivity(name =lkp_act_name, linked_service_name= linked_service_name, source = source, dataset = dataset
                                ,first_row_only =False)


#create copy activity
ds_name = 'ds_sql_dim_input' #these datasets already created
dsOut_name ='ds_blob_dim_output' #these datasets already created

copy_act_name = 'Copy SQL to Blob(parquet)'
sql_reader_query =  {""value"": ""@item().name"",""type"": ""Expression""}
sql_source = SqlSource(sql_reader_query=sql_reader_query)
blob_sink = ParquetSink()
dsin_ref = DatasetReference(reference_name=ds_name)
dsOut_ref = DatasetReference(reference_name=dsOut_name)
copy_activity = CopyActivity(name=copy_act_name,inputs=[dsin_ref], outputs=[dsOut_ref], source=sql_source, sink=blob_sink)

## For Each Activity
pl_name ='pl_Test'
items= {""value"": ""@activity('Get Table Names').output.value"",""type"": ""Expression""}
dependsOn = [{""activity"": ""Get Table Names"",""dependencyConditions"": [""Succeeded""]}]
ForEachActivity_= ForEachActivity(name = 'Copy tables in loop',items=items,depends_on=dependsOn ,activities =copy_activity)

params_for_pipeline = {}
p_obj = PipelineResource(activities=[LookupActivity_,ForEachActivity_], parameters=params_for_pipeline)
p = adf_client.pipelines.create_or_update(rg_name, df_name, pl_name, p_obj)
</code></pre>
","<python><azure><azure-data-factory>","2020-02-21 16:41:21","342","1","1","60343836","<p>Activities needs to be a list of Activity, and you are passing a single one. Try creating a list and add the copy activity to it, and the pass that list in the activities parameter.</p>
"
"60339506","ADF V2 The required Blob is missing wildcard folder path and wildcard file name","<p>I am trying to use a wild card folder path that is being supplied by  getmetadata and foreach. The actual file name within these folders ends with _Problem_1.csv however i get the following error. can anyone advise me on where i am going wrong?</p>

<p>{ ""errorCode"": ""2200"", ""message"": ""ErrorCode=UserErrorSourceBlobNotExist,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The
 required Blob is missing. Folder path: client-uploads/[{\""name\"":\""A001\"",\""type\"":\""Folder\""},{\""name\"":\""A002\"",\""type\"":\""Folder\""},{\""name\"":\""A004\"",\""type\"":\""Folder\""},{\""name\"":\""A006\"",\""type\"":\""Folder\""},{\""name\"":\""A623\"",\""type\"":\""Folder\""}]/.,Source=Microsoft.DataTransfer.ClientLibrary,'"",
 ""failureType"": ""UserError"", ""target"": ""Copy data1"", ""details"": [] }</p>

<p><a href=""https://i.stack.imgur.com/Bo1r4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bo1r4.jpg"" alt=""fa""></a></p>
","<azure-blob-storage><azure-data-factory>","2020-02-21 13:21:12","3856","0","1","60345639","<p>You can try having your Copy activity inside the forEach activity and having for each items dynamic expression as below, which will get the list of all Folder names (This will also include file names if any exists in the folder you are pointing in getmetadata activity). </p>

<p><strong>ForEach Activity Dynamic expression</strong>:
Items : @activity('getFolderNames').output.childItems</p>

<p><a href=""https://i.stack.imgur.com/dwAMf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dwAMf.png"" alt=""enter image description here""></a></p>

<p><strong>Here are child Items from getMetaData</strong>:</p>

<pre><code>{
""childItems"": [
    {
        ""name"": ""A001"",
        ""type"": ""Folder""
    },
    {
        ""name"": ""A002"",
        ""type"": ""Folder""
    }
],
""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (West US)"",
""executionDuration"": 0,
""durationInQueue"": {
    ""integrationRuntimeQueue"": 0
},
""billingReference"": {
    ""activityType"": ""PipelineActivity"",
    ""billableDuration"": {
        ""Managed"": 0.016666666666666666
    }
}
</code></pre>

<p>Than you have to use the ""item().name"" in the wild card file path expression field of copy activity, to get the name of  folder per iteration of forEach activity. </p>

<p>In my sample, I have tried below concat expression to point to the correct folder path name for each iteration. </p>

<p><strong>Wildcard Folder path</strong>: @{Concat('input/MultipleFolders/', item().name)}</p>

<p><strong>This will return:</strong></p>

<p>For  Iteration 1: input/MultipleFolders/A001 </p>

<p>For  Iteration 2: input/MultipleFolders/A002</p>

<p><a href=""https://i.stack.imgur.com/s0j5f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s0j5f.png"" alt=""enter image description here""></a></p>

<p>Hope this helps..</p>
"
"60337202","Is there an update lately with Azure DataFactory V1 for Credentials' expiring date?","<p>I've been using Azure Datafactory V1 for a long time now, but lately, I've been having troubles with credentials expiring.
A slice of the error message I get is:</p>

<p>""Credential operation error: The authentication token for your linked service has expired.
 Please reauthorize the linked service and re-run the slice/activity window.""
Even though I don't have any expiration constraint.</p>

<p>I have to redeploy all of my Pipelines within my DataFactory in order to make it work again.
This happened twice this week (6 days interval).</p>

<p>I don't know if this is due to some updates, or a way to force migration to Datafactory V2.</p>

<p>Any help would be appreciated.</p>
","<azure><azure-data-factory>","2020-02-21 10:56:16","36","0","1","60345061","<p>You can change the authentication type from OAuth to Service Principle.  OAuth tokens expire.  Service Principle fetches new OAuth tokens on its own.  Do you need instructions for this change?</p>

<p>On another note, I <em>strongly</em> encourage you to migrate to V2.</p>
"
"60319385","how to change default parameter values at pipeline level dynamically in azure data factory while moving from dev to prod","<p>I have few parameters specified at pipeline level in ADF and i have used default values in dev environment.Now i want to move this pipeline to prod environment and want to change the parameter values according to the production.</p>

<p>Earlier is SSIS we used to have configurations(sql,xml...) to do such changes without changing anything in the SSIS package.</p>

<p>can we do the same thing in ADF i:e without changing the default values manually in the package,can we use values stored in sql table to pass as pipeline parameters.</p>
","<azure-data-factory>","2020-02-20 11:56:28","1366","0","1","60322644","<p>You don't need to worry about the values defined in a pipeline parameter as long as you are going to have a trigger on it. Just make sure to publish different versions of triggers in dev and prod repositories and pass different values to the pipeline parameters.</p>

<p>If however you want to change parameters, you can do so by invoking the pipeline from a parent pipeline through an execute pipeline activity. The values you pass as parameters to the execute pipeline activity can be coming from a lookup (over some configuration file or table).</p>
"
"60313157","How to filter the CRM data source as Source to pick up the Last Modified Records after Each Run of Pipeline?","<p>I am Integrating from CRM to Azure SQL DB but I want to set the Net Change as <code>Last ModifiedOn</code>. This will help to update/insert only those records modified since the previous run. I am trying to fetch the Latest modified on date-time from SQL but  I am unable to pass it Fetch XML query for CRM Source. The Query I am using is as follows:</p>

<pre class=""lang-xml prettyprint-override""><code>    &lt;fetch&gt;
      &lt;entity name=""msdyn_project""&gt;
        &lt;all-attributes /&gt;

        &lt;filter type=""and""&gt;
          &lt;condition attribute=""modifiedon"" operator=""on-or-after"" 
    value=""@{activity(\'LookupOldWaterMarkactivity\').output.firstRow.Prop_0}""/&gt;

        &lt;/filter&gt;
      &lt;/entity&gt;
    &lt;/fetch&gt;
</code></pre>

<p>Please suggest if there is some other workaround for this.</p>
","<azure><azure-data-factory>","2020-02-20 04:58:17","188","0","1","60334095","<p>Per my experience,you can't set <code>@{activity(...)}</code> in the Fetch XML query SQL directly.If you want to use <code>@{activity(...)}</code>,it belongs to dynamic content:</p>

<p><a href=""https://i.stack.imgur.com/n4spI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n4spI.png"" alt=""enter image description here""></a></p>

<p>So,you could try to use <code>@cancat</code> to complete the query SQL:</p>

<pre><code>@concat(
'&lt;fetch&gt;&lt;entity name=""""&gt;.....&lt;condition attribute=""modifiedon"" operator=""on-or-after""  value=',
@{activity(""LookupOldWaterMarkactivity"").output.firstRow.Prop_0, 
'/&gt;....&lt;/fetch&gt;')
</code></pre>
"
"60300293","How to Map JSON data from a REST API to Azure SQL using Data Factory","<p>I have a new pipeline in azure data factory.
I created the dataset, one from the rest api (a public one):
<a href=""https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&amp;symbol=MSFT&amp;apikey=demo"" rel=""noreferrer"">https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&amp;symbol=MSFT&amp;apikey=demo</a></p>

<p>and then I created an azure sql table with columns shown in the screenshot</p>

<p><a href=""https://i.stack.imgur.com/mkcub.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mkcub.png"" alt=""enter image description here""></a></p>

<p>The problem, is that I dont know how to do the mapping, as this is a complex JSON object, I am limited with the Mapping Designer:</p>

<p>How do I map the date?</p>

<p><a href=""https://i.stack.imgur.com/lan7J.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/lan7J.png"" alt=""f""></a></p>
","<json><azure><azure-data-factory>","2020-02-19 12:22:05","2361","5","2","60322586","<p>Does the data have a structure? If so, you can generate a dummy file, place it in sink and do a one time mapping. If not, you can Lookup on the file, iterate over the content in a For Each Loop Container and insert details on to a SQL table.</p>

<p>E.g. </p>

<pre><code>insert &lt;&lt;your table&gt;&gt; 
select '@item().name', '@item().address.city', @item().value
</code></pre>

<p>The important thing to remember is to iterate at the correct array. Let me know if it's not clear. Not in front of a system right now, so can't add screenshots.</p>
"
"60300293","How to Map JSON data from a REST API to Azure SQL using Data Factory","<p>I have a new pipeline in azure data factory.
I created the dataset, one from the rest api (a public one):
<a href=""https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&amp;symbol=MSFT&amp;apikey=demo"" rel=""noreferrer"">https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&amp;symbol=MSFT&amp;apikey=demo</a></p>

<p>and then I created an azure sql table with columns shown in the screenshot</p>

<p><a href=""https://i.stack.imgur.com/mkcub.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mkcub.png"" alt=""enter image description here""></a></p>

<p>The problem, is that I dont know how to do the mapping, as this is a complex JSON object, I am limited with the Mapping Designer:</p>

<p>How do I map the date?</p>

<p><a href=""https://i.stack.imgur.com/lan7J.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/lan7J.png"" alt=""f""></a></p>
","<json><azure><azure-data-factory>","2020-02-19 12:22:05","2361","5","2","60352494","<p>I tend to use an ELT approach for these, calling the REST API with a Web task and storing the JSON in a SQL table and then shredding the JSON using SQL functions like <a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/openjson-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">OPENJSON</a>. </p>

<p>Example pipeline:</p>

<p><a href=""https://i.stack.imgur.com/87ti6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/87ti6.png"" alt=""Example ADF pipeline""></a></p>

<p>The key to getting this approach to work is the expression on the stored procedure parameter.  This takes the whole JSON output from the Web task and passes it in to the proc.  This is a simple logging proc which inserts the record into a logging table:</p>

<pre><code>@string(activity('Web1').output)
</code></pre>

<p>I log to a table and then shred the JSON or you could use <code>OPENJSON</code> directly on the stored proc parameter, eg</p>

<pre><code>--INSERT INTO ...
SELECT
    CAST( [key] AS DATE ) AS timeSeriesDate,
    JSON_VALUE ( x.[value], '$.""1. open""' ) AS [open],
    JSON_VALUE ( x.[value], '$.""2. high""' ) AS [high],
    JSON_VALUE ( x.[value], '$.""3. low""' ) AS [low],
    JSON_VALUE ( x.[value], '$.""4. close""' ) AS [close],
    JSON_VALUE ( x.[value], '$.""5. volume""' ) AS [volume]

FROM dbo.myLog
    CROSS APPLY OPENJSON(logDetails , '$.""Time Series (Daily)""' ) x
--WHERE logId = 23333;
</code></pre>

<p>My results:</p>

<p><a href=""https://i.stack.imgur.com/VLVGT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VLVGT.png"" alt=""My results""></a></p>
"
"60298020","Error while creating Azure IR:Check the linked service configuration is correct","<p>Error while creating an Azure IR data factory.</p>
<blockquote>
<p>Cannot connect to SQL Database:
'mysqlserversowmya.database.windows.net', Database: 'master', User:
'adminuser'. Check the linked service configuration is correct, and
make sure the SQL Database firewall allows the integration runtime to
access.</p>
</blockquote>
","<azure-sql-database><azure-data-factory>","2020-02-19 10:19:41","2322","0","1","60313302","<p>Make sure you have open the SQL database firewall: <strong>add client ip</strong> and <strong>Allow Azure services and resources to access this server</strong> On portal:</p>

<p><a href=""https://i.stack.imgur.com/Y5ldZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y5ldZ.png"" alt=""enter image description here""></a></p>

<p>To see: <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-server-level-firewall-rule"" rel=""nofollow noreferrer"">Server-level IP firewall rules</a></p>

<p>Hope this helps.</p>
"
"60292214","I am trying to download data from REST API to azure data lake via azure data factory. How can I pass query parameters for API in azure data factory?","<p>I am doing this using ‘Copy Activity’. Is there any other activity that can better perform this operation?
Example REST API URL:<a href=""https://xzy.com/servicename?name=tom&amp;age=50&amp;city=boston&amp;"" rel=""nofollow noreferrer"">https://xzy.com/servicename?name=tom&amp;age=50&amp;city=boston&amp;</a>......
I am putting '<a href=""https://xzy.com/servicename/"" rel=""nofollow noreferrer"">https://xzy.com/servicename/</a>' as the base URL for the linked service.
How to pass the query parameters 'name=tom' and 'age=50' and 'city=boston' to the base url in the azure data factory to get the response? 
Will the parameters be passed at linked service, dataset or pipeline level?</p>
","<rest><parameters><azure-data-factory><azure-data-lake><query-string>","2020-02-19 02:39:10","617","0","1","60294144","<p>It seems that your request method is <code>Get</code>. You could only set the query parameters behind base url directly.</p>

<p>For example,i test rest api from this sample <a href=""https://jsonplaceholder.typicode.com/"" rel=""nofollow noreferrer"">website</a>:</p>

<pre><code>https://jsonplaceholder.typicode.com/comments?postId=1
</code></pre>

<p>Test result:</p>

<p><a href=""https://i.stack.imgur.com/xYNU8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xYNU8.png"" alt=""enter image description here""></a></p>

<p>If your request method is <code>Post</code>,i suppose you have to define the parameters in the <code>Body</code> part.</p>

<p><a href=""https://i.stack.imgur.com/07CRK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/07CRK.png"" alt=""enter image description here""></a></p>
"
"60290314","Create a generic data factory with multiple linked services","<p>Use Case: To create a generic data factory which can read data from different azure blob containers which has flat files into Azure SQL. I have created a data pipeline which uses stored procedures to populate the Azure SQL tables.</p>

<p>Issue: The trouble that I have is that I want to execute this data factory from my code and change the database and blob container on the fly and execute the same data factory with this new parameters. The Table names will remain the same on the Azure SQL side and the File name will also remain same in the blob storage. The change will the the Container or the folder name inside the Container which will be know before hand. </p>

<p>Please help me out or point me in the direction as to what could help me achieve this and if this can be at all be achieved or not.</p>
","<azure><azure-sql-database><azure-blob-storage><azure-data-factory><azure-sdk-.net>","2020-02-18 22:25:09","565","0","1","60299316","<p>You would need to use the parameterized datasets and linked services. Define parameters on your data factory pipeline (which you want to pass from your code e.g. container name or the folder name, connection string for SQL azure and connection string for blob storage). Once this is defined - you would need to pass these values downstream all the way till the linked service </p>

<p>i.e. something like this
Pipeline Parameters > Dataset Parameters > Linked Service Parameters </p>

<p><a href=""https://i.stack.imgur.com/ZdxbE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZdxbE.png"" alt=""enter image description here""></a></p>
"
"60289113","Cannot Create Variable Activity on data Factory","<p>I am trying to create data factory resources using python and I am getting error while calling few classes</p>

<p>example- here I am trying to create ""Set Variable Activity"" but I am getting error</p>

<p>ref: <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.setvariableactivity?view=azure-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.setvariableactivity?view=azure-python</a></p>

<p><strong>Error Message</strong></p>

<blockquote>
  <p>NameError: name 'SetVariableActivity' is not defined</p>
</blockquote>

<p><strong>Code</strong></p>

<pre><code>from azure.mgmt.datafactory.models import *

variable_name = 'vGetDate'
var_activity = SetVariableActivity(name = 'Get Date',variable_name= variable_name, value = '@utcnow()' )
</code></pre>

<p>I am getting NameError with DatasetFolder Class as well. I am not sure what went wrong, I thought I have imported all the required libraries and I am using on 3.7 v</p>

<p>Any input is greatly appreciated.</p>
","<python><azure><azure-data-factory>","2020-02-18 20:39:35","92","1","1","60294360","<p>I test the code on my side, it works fine, please try the same version of the libraries with me.</p>

<pre><code>azure-common==1.1.23
azure-mgmt-datafactory==0.9.0
</code></pre>

<p>My sample creates a pipeline with the <code>Set variable</code> activity:</p>

<pre><code>from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *

subscription_id = '&lt;subscription-id&gt;'
credentials = ServicePrincipalCredentials(client_id='&lt;client-id&gt;', secret='&lt;client-secret&gt;', tenant='&lt;tenant-id&gt;')
adf_client = DataFactoryManagementClient(credentials, subscription_id)

variable_name = 'vGetDate'
var_activity = SetVariableActivity(name = 'Get Date',variable_name= variable_name, value = '@utcnow()' )

rg_name = '&lt;resource-group-name&gt;'
df_name = 'joyfactory'
p_name = 'Pipeline123'
params_for_pipeline = {}
p_obj = PipelineResource(
    activities=[var_activity], parameters=params_for_pipeline)
p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)
print(p)
</code></pre>

<p><a href=""https://i.stack.imgur.com/0wRX4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0wRX4.png"" alt=""enter image description here""></a></p>

<p><strong>Check in the portal</strong>:</p>

<p><a href=""https://i.stack.imgur.com/MyFlB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MyFlB.png"" alt=""enter image description here""></a></p>
"
"60283222","Handling big JSONs in Azure Data Factory","<p>I'm trying to use ADF for the following scenario:</p>

<ul>
<li>a JSON is uploaded to a Azure Storage Blob, containing an array of similar objects</li>
<li>this JSON is read by ADF with a Lookup Activity and uploaded via a Web Activity to an external sink</li>
</ul>

<p>I cannot use the Copy Activity, because I need to create a JSON payload for the Web Activity, so I have to lookup the array and paste it like this (payload of the Web Activity):</p>

<pre><code>{
   ""some field"": ""value"",
   ""some more fields"": ""value"",
   ...
   ""items"": @{activity('GetJsonLookupActivity').output.value}
}
</code></pre>

<p>The Lookup activity has a known limitation of an upper limit of 5000 rows at a time. If the JSON is larger, only 5000 top rows will be read and all else will be ignored.</p>

<p>I know this, so I have a system that chops payloads into chunks of 5000 rows before uploading to storage. But I'm not the only user, so there's a valid concern that someone else will try uploading bigger files and the pipeline will silently pass with a partial upload, while the user would obviously expect all rows to be uploaded.</p>

<p>I've come up with two concepts for a workaround, but I don't see how to implement either:</p>

<ol>
<li><p>Is there any way for me to check if the JSON file is too large and fail the pipeline if so? The Lookup Activity doesn't seem to allow row counting, and the Get Metadata Activity only returns the size in bytes. </p></li>
<li><p>Alternatively, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#limitations-and-workarounds"" rel=""nofollow noreferrer"">MSDN docs</a> propose a workaround of copying data in a foreach loop. But I cannot figure out how I'd use Lookup to first get rows 1-5000 and then 5001-10000 etc. from a JSON. It's easy enough with SQL using <code>OFFSET N FETCH NEXT 5000 ROWS ONLY</code>, but how to do it with a JSON?</p></li>
</ol>
","<json><azure><azure-blob-storage><azure-data-factory>","2020-02-18 14:32:08","2892","1","1","60296443","<p>You can't set any index range(<code>1-5,000,5,000-10,000</code>) when you use LookUp Activity.The workaround mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#limitations-and-workarounds"" rel=""nofollow noreferrer"">doc</a> doesn't means you could use LookUp Activity with  pagination,in my opinion.</p>

<p>My workaround is writing an azure function to get the total length of json array before data transfer.Inside azure function,divide the data into different sub temporary files with pagination like <code>sub1.json,sub2.json...</code>.Then output an array contains file names.</p>

<p>Grab the array with ForEach Activity, execute lookup activity in the loop. The file path could be set as dynamic value.Then do next Web Activity.</p>

<p>Surely,my idea could be improved.For example,you get the total length of json array and it is under 5000 limitation,you could just return <code>{""NeedIterate"":false}</code>.Evaluate that response by <code>IfCondition</code> Activity to decide which way should be next.It the value is false,execute the LookUp activity directly.All can be divided in the branches.</p>
"
"60277081","How to check current status of a Data Factory pipelinerun with just Pipeline name?","<p>Is it possible to check the current status of a Azure Data Factory pipelinerun with just Pipeline name either through Powershell or API ?</p>

<p>I have seen that you can use Get-AzDataFactoryV2PipelineRun but that requires you to have the pipelinerunid.</p>

<p>My goal is to build a script that will first check if a pipeline run is running and if not trigger it. I want to avoid the script to trigger a pipelinerun so that there will be multiple pipeline runs running at the same time.</p>
","<powershell><azure-data-factory>","2020-02-18 08:57:15","3144","2","2","60283845","<p>I've done it with PowerShell, although I'm sure there are better ways, this is the solution I've come up with.</p>

<p>It basically gets all the pipeline run ids from a week ago to this day, iterates through each one and if it finds one that is ""InProgress"", it will not do anything. If no pipeline run id is in that state, then execute it.</p>

<p>You obviously need to be authenticated previously to run this:</p>

<pre><code>$before = get-date
$after = (get-date).AddDays(-7)
$runIds = Get-AzDataFactoryV2PipelineRun -DataFactoryName $DataFactoryName -ResourceGroupName $ResourceGroupName -LastUpdatedAfter $after -LastUpdatedBefore 
$before
$shouldRun = 1

$runIds | ForEach-Object {
    ## Check for all statuses
    if($_.Status -eq ""InProgress""){
        $shouldRun = 0
    }
}

if($shouldRun){
    ## Logic to run pipeline, this is just an example.
    Invoke-AzDataFactoryV2Pipeline -PipelineName $PipelineName -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName
} 
</code></pre>
"
"60277081","How to check current status of a Data Factory pipelinerun with just Pipeline name?","<p>Is it possible to check the current status of a Azure Data Factory pipelinerun with just Pipeline name either through Powershell or API ?</p>

<p>I have seen that you can use Get-AzDataFactoryV2PipelineRun but that requires you to have the pipelinerunid.</p>

<p>My goal is to build a script that will first check if a pipeline run is running and if not trigger it. I want to avoid the script to trigger a pipelinerun so that there will be multiple pipeline runs running at the same time.</p>
","<powershell><azure-data-factory>","2020-02-18 08:57:15","3144","2","2","60423305","<p>Using .Net Core SDK</p>

<pre><code>ServiceClientCredentials cred = new TokenCredentials(accessToken);
using (var client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId })
{
    RunQueryFilter filter1 = new RunQueryFilter(""PipelineName"", ""Equals"", new List&lt;string&gt; { ""ActualPipelineName"" });
    RunQueryFilter filter2 = new RunQueryFilter(""Status"", ""Equals"", new List&lt;string&gt; { ""Queued"" });
    DateTime before = DateTime.UtcNow;
    DateTime after = before.AddHours(-4);
    RunFilterParameters param = new RunFilterParameters(after, before, null, new List&lt;RunQueryFilter&gt; { filter1, filter2 }, null);
    PipelineRunsQueryResponse pipelineResponse = client.PipelineRuns.QueryByFactory(resourceGroupName, azureDataFactoryName, param);
    int? QueuedPipelines = pipelineResponse?.Value?.Count;
}
</code></pre>

<p>In filter 1, you can use ""In"" operator to query for more than one PipelineName.</p>

<p>In filter2, you can use any valid status of ADF (Success, InProgress, Queued etc.)</p>

<p>This same thing can also be achieved using REST API.</p>

<p>P.S- you might need to use Continuation token if the count of pipelines is more than 100 (for that particular status).</p>
"
"60270029","Create linked service with key Vault using python","<p>Here is my problem, I am trying to create linked service using python sdk and I was successful if I provide the storage account name and key. But I would like to create Linked service with key vaults reference, the below runs fine and creates the linked service. However when I go to datafactory and test connection.. it fails.. Please help!</p>

<pre><code>store = LinkedServiceReference(reference_name ='LS_keyVault_Dev')
storage_string = AzureKeyVaultSecretReference( store=store, secret_name = 'access_key')

ls_azure_storage = AzureStorageLinkedService(connection_string=storage_string)
ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_azure_storage)

</code></pre>

<p><strong>Error Message</strong></p>

<pre><code>Invalid storage connection string provided to 'AzureTableConnection'. Check the storage connection string in configuration. No valid combination of account information found.
</code></pre>
","<python><azure><azure-data-factory><azure-keyvault>","2020-02-17 20:35:39","454","0","1","60295563","<p>I test your code, it created the linked service successfully, and I navigate to the portal to <code>Test connection</code>, it also works, you could follow the steps below.</p>

<p>1.Navigate to the azure keyvault in the portal -> <code>Secrets</code> -> <code>Create a secret</code>, I'm not sure why you can use <code>access_key</code> as the name of the secret, pey my test, it is invalid. So in my sample, I use <code>accesskey</code> as the name of the secret, then store the <code>Connection string</code> of the storage account.</p>

<p><a href=""https://i.stack.imgur.com/t0omu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t0omu.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/UEvNf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UEvNf.png"" alt=""enter image description here""></a></p>

<p>2.Navigate to the <code>Access policies</code> of the keyvault, add the MSI of your data factory with correct secret permission. If you did not enable the MSI of the data factory, follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">link</a> to generate it, this is used to for the <code>Azure Key Vault</code> linked service to access your keyvault secret.</p>

<p><a href=""https://i.stack.imgur.com/wndjP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wndjP.png"" alt=""enter image description here""></a></p>

<p>3.Navigate to the <code>Azure Key Vault</code> linked service of your data factory, make sure the connection is successful.</p>

<p><a href=""https://i.stack.imgur.com/2IHCk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2IHCk.png"" alt=""enter image description here""></a></p>

<p>4.Use the code below to create the storage linked service.</p>

<p>Version of the libraries:</p>

<pre><code>azure-common==1.1.23
azure-mgmt-datafactory==0.9.0
</code></pre>

<hr>

<pre><code>from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *

subscription_id = '&lt;subscription-id&gt;'
credentials = ServicePrincipalCredentials(client_id='&lt;client-id&gt;', secret='&lt;client-secret&gt;', tenant='&lt;tenant-id&gt;')
adf_client = DataFactoryManagementClient(credentials, subscription_id)

rg_name = '&lt;resource-group-name&gt;'
df_name = 'joyfactory'
ls_name = 'storageLinkedService'

store = LinkedServiceReference(reference_name ='AzureKeyVault1') # AzureKeyVault1 is the name of the Azure Key Vault linked service
storage_string = AzureKeyVaultSecretReference( store=store, secret_name = 'accesskey')

ls_azure_storage = AzureStorageLinkedService(connection_string=storage_string)
ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_azure_storage)
print(ls)
</code></pre>

<p><a href=""https://i.stack.imgur.com/y4PXK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y4PXK.png"" alt=""enter image description here""></a></p>

<p>5.Go back to the linked service page, refresh and test the connection, it works fine.</p>

<p><a href=""https://i.stack.imgur.com/cmJY9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cmJY9.png"" alt=""enter image description here""></a></p>
"
"60268610","Failed Activity not running","<p>I have an Azure Data Factory v2 Pipeline with a copy data activity.  If the activity fails a Lookup activity should be run.  Unfortunately the Lookup never runs.  Why doesn't it run on failure of the copy data activity?  How do I get this to work?</p>

<p><a href=""https://i.stack.imgur.com/J7qmI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J7qmI.png"" alt=""Pipeline Copy Activity Fails, Nothing else runs""></a></p>

<p>I'm expecting the ""Set load of file to failed"" activity to run because the Load Zipped File to Import Destination"" activity failed.  In fact in the output you can see the Status is ""Failed"" but no other activity is run.</p>

<p>Later I updated the Copy Activity to skip incompatible rows which caused the Copy data activity to succeed.  The expected number of rows loaded now doesn't match the total number of rows loaded, so the If Condition activity goes to the failure route.  Why would the Lookup run from the If Condition only triggering the failure Activity vs the Copy Data activity?</p>

<p><a href=""https://i.stack.imgur.com/QCjjk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QCjjk.png"" alt=""Set load of file to failed works from only If condition""></a></p>
","<azure-data-factory>","2020-02-17 18:40:00","984","0","1","60273931","<p>Activity dependencies are a logical AND. The lookup activity <em>Set load of file to failed</em> will only execute if <strong>both</strong> the Copy data activity <strong>and</strong> the If condition fail. It's not one or the other - it's both. I blogged about this <a href=""https://datasavvy.me/2018/10/02/data-factory-v2-activity-dependencies-are-a-logical-and/"" rel=""nofollow noreferrer"">here</a>. </p>

<p>It's common to redesign this as: 
A. Use multiple failure activities. Instead of having the one set load of file to failed at the end, copy that activity and have the copy data activity link to the new one on failure. 
B. Create a parent pipeline and use an execute pipeline activity. Then add a single failure dependency from the execute pipeline activity to <em>Set load of file to failed</em> activity emphasized text.</p>
"
"60263618","Pagination with oauth azure data factory","<p>Inside Azure data factory i make a call to microsoft graph through a REST copy activity utilizing rest to get an access token to the service. The Graph api returns max 200 results, and therefore i am interested in using the pagination rules that can be created in the source. In post man i can see that my response structure is</p>

<pre><code>{
   ""@odata.context"" : &lt;some context&gt;,
   ""@odata.nextLink"" : &lt;the link to next page&gt;
   ""value"" : [&lt;the response data on current page&gt;]
}
</code></pre>

<p>I have read in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">documentation</a> that i set the pagination rules by adding the key</p>

<pre><code>AbsoluteUrl
</code></pre>

<p>and then the path to the next page given in the response as the value.
How do i tell azure data factory how to find this?</p>
","<azure><oauth-2.0><azure-data-factory>","2020-02-17 13:32:51","1447","2","1","60263619","<p>Since your repsonse is a json structure the value of </p>

<pre><code>AbsoluteUrl
</code></pre>

<p>should be </p>

<pre><code>$['@odata.nextLink']
</code></pre>
"
"60244364","How to get Pipeline Folder Path in Azure Data Factory?","<p>Is there a way/function to fetch complete folder path of a pipeline in Azure data factory? </p>

<p>If I have a data factory job like in <code>str</code>:</p>

<pre><code>**datafactory-name/root-folder/etl-pipeline**
</code></pre>

<p>I can use <strong>@pipeline().Pipeline</strong> function to fetch and store pipeline name in a variable which in this case would be <strong>etl-pipeline</strong>.</p>

<p>Is there a similar function to retrieve the name of folder in which the pipeline is? I would like to store <strong>root-folder</strong> also in a variable</p>

<p>Please help.</p>
","<azure><azure-data-factory><azure-deployment>","2020-02-16 00:14:08","761","0","1","60245805","<p>Don't think that there is any direct way to achieve this, however curious to know the use case why you would need to capture the name of the pipeline folder in a variable while executing it and why not parameterize the pipeline which would let you pass the name of the pipeline folder whenever you invoke the pipeline - this way you could still achieve what you are intending to do with pipeline parameter instead of a runtime initialized variable.</p>
"
"60229978","JSON Headers is missing in Azure Data Factory (v2)","<p>In the pipeline, I used copy data activity to fetch data from REST API into a .txt file</p>

<p>This is the preview of source data:
<a href=""https://i.stack.imgur.com/sJIZ6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sJIZ6.png"" alt=""enter image description here""></a></p>

<p>However after running the pipeline, I received the following output:
<a href=""https://i.stack.imgur.com/8MN93.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8MN93.png"" alt=""enter image description here""></a></p>

<p>Now where is the column header? Is there something that I might have missed in configuring during creating the copy data activity?</p>

<p>EDIT: Here's the sink configuration:
<a href=""https://i.stack.imgur.com/vgMa9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vgMa9.png"" alt=""enter image description here""></a></p>
","<json><azure><cloud><azure-data-factory>","2020-02-14 16:18:49","694","1","1","60244052","<p>You have probably missed the option ""First row as header"" when configuring the sink dataset. Check that you have this enabled in the sink dataset of your copy activity:</p>

<p><a href=""https://i.stack.imgur.com/q8eMI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q8eMI.png"" alt=""First row as header""></a></p>

<p>Hope this helped!!</p>
"
"60229319","Using Azure Data Factory to ingest incoming data from a REST API","<p>Is there a way to create an Azure ADF Pipeline to ingest the <strong>incoming</strong> POST requests? I have this gateway app (outside Azure) that is able to publish data via REST as it arrives from the application and this data needs to be ingested into a Data Lake. I am utilizing the REST calls from another pipeline to pull the data but this basically needs to do the reverse - the data will be pushed and i need to be constantly 'listening' to those calls...</p>

<p>Is this something an ADF pipeline should do or maybe there are any other Azure components able to do it?</p>
","<azure><rest><api><azure-data-factory>","2020-02-14 15:37:03","1073","0","3","60229414","<p>Azure Data Factory is a batch data movement service.  If you want to push the data over HTTP, you can implement a simple Azure Function to accept the data and write it to the Azure Data Lake.</p>

<p>See <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">Azure Functions HTTP triggers and bindings overview</a></p>
"
"60229319","Using Azure Data Factory to ingest incoming data from a REST API","<p>Is there a way to create an Azure ADF Pipeline to ingest the <strong>incoming</strong> POST requests? I have this gateway app (outside Azure) that is able to publish data via REST as it arrives from the application and this data needs to be ingested into a Data Lake. I am utilizing the REST calls from another pipeline to pull the data but this basically needs to do the reverse - the data will be pushed and i need to be constantly 'listening' to those calls...</p>

<p>Is this something an ADF pipeline should do or maybe there are any other Azure components able to do it?</p>
","<azure><rest><api><azure-data-factory>","2020-02-14 15:37:03","1073","0","3","60229903","<p>Previous comment is right and is one of the approach to get it working but would need bit of coding (for azure function). </p>

<p>There could also be an alternate solution to cater to your requirement is with Azure Logic Apps and Azure data factory. </p>

<p>Step 1:  Create a HTTP triggered logic app which would be invoked by your gateway app and data will be posted to this REST callable endpoint. </p>

<p>Step 2: Create ADF pipeline with a parameter, this parameter holds the data that needs to be pushed to the data lake. It could be raw data and can be transformed as a step within the pipeline before pushing it to the data lake.</p>

<p>Step 3: Once logic app is triggered, you can simply use Azure data factory actions to invoke the data factory pipeline created in step 2 and pass the posted data as a pipeline parameter to your ADF pipeline. </p>

<p><a href=""https://i.stack.imgur.com/RoSZi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RoSZi.png"" alt=""enter image description here""></a></p>

<p>This should be it, with this - you can spin up your code-less solution.</p>
"
"60229319","Using Azure Data Factory to ingest incoming data from a REST API","<p>Is there a way to create an Azure ADF Pipeline to ingest the <strong>incoming</strong> POST requests? I have this gateway app (outside Azure) that is able to publish data via REST as it arrives from the application and this data needs to be ingested into a Data Lake. I am utilizing the REST calls from another pipeline to pull the data but this basically needs to do the reverse - the data will be pushed and i need to be constantly 'listening' to those calls...</p>

<p>Is this something an ADF pipeline should do or maybe there are any other Azure components able to do it?</p>
","<azure><rest><api><azure-data-factory>","2020-02-14 15:37:03","1073","0","3","60330403","<p>If your outside application is already pushing via REST, why not have it make calls directly to the Data Lake REST APIs?  This would cut out the middle steps and bring everything under your control.</p>
"
"60226168","Find the number of files available in Azure data lake directory using azure data factory","<p>I am working on a pipeline where our data sources are csv files stored in Azure data lake. I was able to process all the files using get meta data and for each activity. Now I need to find the number of files available in the Azure data lake? How can we achieve that. I couldn't find any itemcount argument in the Get Meta Data activity. I have noticed that the input of For each activity contains an itemscount value. Is there anyway to access this?</p>

<p>Regards,
Sandeep</p>
","<azure-data-factory><azure-data-lake>","2020-02-14 12:13:37","631","1","1","60330467","<p>Since the output of a child_items Get Metadata activity is a list of objects, why not just get the length of this list?</p>

<p><code>@{length(activity('Get Metadata1').output.childItems)}</code></p>
"
"60220162","How to setup seemly circular but not actually circular dependencies in Azure Data Factory V2","<p>Below is the scenario :</p>

<p>Pipeline A triggered by Tumbling Window Trigger A - Runs Every Hour.</p>

<p>Pipeline B triggered by Tumbling Windows Trigger B - Runs Every Week.</p>

<p>I want A to run only after :</p>

<p>a. last run of A is successful. (self dependency - doable).</p>

<p>b. last run of B is successful.  ( This is also doable but can get into circular dependency because of point b in below lines)</p>

<p>I want B to run only after :</p>

<p>a. last run of B is successful. ( self dependency - doable)</p>

<p>b. last run of A is successful. ( This gets into circular dependency issue when we publish and later when it generates ARM templates)</p>

<p>Here A is having frequency of 1 hour and B is having frequency of 1 week.</p>

<p>consider B is to next execute on 9th Feb 12:00 AM for window of 2nd Feb - 9th Feb.</p>

<p>Also, A is about to execute on 9th Feb 12:00 Am for window of 8th Feb 11:00 PM - 12:00 PM.</p>

<p>In this case both runs will collide at 9th Feb 12:00 AM. Can we setup dependency so that it does not collide.</p>
","<azure><azure-data-factory>","2020-02-14 04:50:10","738","1","1","60276851","<p>I provide below ideas for your reference:</p>

<p>1.Firstly,put the pipelines into <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">Execute Pipeline Activity</a> because you may have to do some steps before and after execution of A and B pipelines.</p>

<p>2.Secondly,since the pipelines have to be executed with some conditions,i think you could persist the results of A and B pipelines execution anyway.For example,after executing pipeline,use an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> to send the result of pipeline into some residences. The aim is logging the latest execute result of A and B.</p>

<p>3.Finally,before the execution of A and B pipelines,you could use an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-until-activity"" rel=""nofollow noreferrer"">Until Activity</a> to evaluate whether the pipeline could be executed.</p>
"
"60218160",".net Core : How to pass parameters and run Azure data factory pipeline from C# Code?","<p>I am using <strong><em>Microsoft.Azure.Management.DataFactories</em></strong> .net core package.<br/></p>

<p>I am using following code to get the required token for accessing azure data factory pipeline in C# .net core :</p>

<pre><code>public static void RunDataFactoryPipeline()
    {
        try
        {
            var context = new AuthenticationContext("""" + """");
            var credentials = new ClientCredential(clientId: """", clientSecret: """");
            AuthenticationResult result = context.AcquireTokenAsync("""", credentials).Result;

            if (result == null)
            {
                throw new InvalidOperationException(""Failed to acquire Token"");
            }

            var token = result.AccessToken;
            var serviceClientCredentials = new TokenCloudCredentials("""",result.AccessToken);
            var client = new DataFactoryManagementClient(serviceClientCredentials);

            StartPipeline(""name"", ""name"", ""name"", client);
        }
        catch (Exception ex)
        {
            throw;
        }
    }
public static void StartPipeline(string resourceGroup, string dataFactory, string pipeLineName, DataFactoryManagementClient client, )
        {
            var pipeLine = client.Pipelines.Get(resourceGroup, dataFactory, pipeLineName);
}
</code></pre>

<blockquote>
  <p>But i don't find any method using which i can run the pipeline in
  factory.</p>
</blockquote>
","<c#><azure><asp.net-core><.net-core><azure-data-factory>","2020-02-13 23:53:09","966","2","1","60227404","<p>Seems you are using incorrect nuget package, use this one and you should have methods available to run pipelines on the IPipelineOperations instance</p>

<p><strong>Install-Package Microsoft.Azure.Management.DataFactory -Version 4.7.0</strong></p>

<pre><code>public static void StartPipeline(string resourceGroup, string dataFactory, string pipeLineName, DataFactoryManagementClient client )
{
   var pipeLine = client.Pipelines.Get(resourceGroup, dataFactory, pipeLineName);
   client.Pipelines.CreateRun(resourceGroup, dataFactory, pipeLineName);
}
</code></pre>
"
"60212029","How to convert xml data into csv/excel/table in Azure","<p>we need to convert XML data into csv/excel/table in Azure cloud.</p>

<p>below is the sample xml code.</p>

<pre><code>&lt;SOAP-ENV:Envelope
   xmlns:SOAP-ENV=""http://schemas.xmlsoap.org/soap/envelope/""&gt;

&lt;SOAP-ENV:Body&gt;
           &lt;ns2:getProjectsResponse
               xmlns:ns2=""http://www.logic8.com/eq/webservices/generated""&gt;
               &lt;ns2:Project&gt;
                   &lt;ns2:fileName&gt;P10001&lt;/ns2:fileName&gt;
                   &lt;ns2:alias&gt;project1&lt;/ns2:alias&gt;
               &lt;/ns2:Project&gt;
               &lt;ns2:Project&gt;
                   &lt;ns2:fileName&gt;P10002&lt;/ns2:fileName&gt;
                   &lt;ns2:alias&gt;project2&lt;/ns2:alias&gt;
               &lt;/ns2:Project&gt;
       &lt;ns2:Project&gt;
                   &lt;ns2:fileName&gt;P10003&lt;/ns2:fileName&gt;
                   &lt;ns2:alias&gt;project3&lt;/ns2:alias&gt;
               &lt;/ns2:Project&gt;
           &lt;/ns2:getProjectsResponse&gt;
       &lt;/SOAP-ENV:Body&gt;
   &lt;/SOAP-ENV:Envelope&gt;
</code></pre>

<p>Expected output:</p>

<p><a href=""https://i.stack.imgur.com/TvPSS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TvPSS.png"" alt=""output""></a></p>

<p>Can anyone help me on this.</p>
","<azure><azure-functions><azure-logic-apps><azure-data-factory><azure-databricks>","2020-02-13 16:03:48","2300","1","2","60214977","<p>Do you have any experience with Azure?  I ask because from your question it sounds like you’re not sure where to start or which service to use.  I’m also curious if you were given a requirement to use Azure or you thought Azure might be the solution yourself.  Also where is this XML coming from?  It looks like a SOAP request.</p>

<p>If you are a developer I’d consider authoring a Web App in .Net, it can use MVC, Core, Web APIs, and use it to consume this SOAP request and translate it and save the file.
For this I’d consider using an XMLDocument class to load the XML and parse through it.</p>

<p>But if you absolutely need to use Azure, the closest thing that would help automate this is Azure Logic Apps.  It offers many “no-code” solutions to plug in connectors that can transform and save data.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-transform#how-to-use-a-transform"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-transform#how-to-use-a-transform</a></p>

<p>If you elaborate on your situation I’d be happy to offer further solutions</p>
"
"60212029","How to convert xml data into csv/excel/table in Azure","<p>we need to convert XML data into csv/excel/table in Azure cloud.</p>

<p>below is the sample xml code.</p>

<pre><code>&lt;SOAP-ENV:Envelope
   xmlns:SOAP-ENV=""http://schemas.xmlsoap.org/soap/envelope/""&gt;

&lt;SOAP-ENV:Body&gt;
           &lt;ns2:getProjectsResponse
               xmlns:ns2=""http://www.logic8.com/eq/webservices/generated""&gt;
               &lt;ns2:Project&gt;
                   &lt;ns2:fileName&gt;P10001&lt;/ns2:fileName&gt;
                   &lt;ns2:alias&gt;project1&lt;/ns2:alias&gt;
               &lt;/ns2:Project&gt;
               &lt;ns2:Project&gt;
                   &lt;ns2:fileName&gt;P10002&lt;/ns2:fileName&gt;
                   &lt;ns2:alias&gt;project2&lt;/ns2:alias&gt;
               &lt;/ns2:Project&gt;
       &lt;ns2:Project&gt;
                   &lt;ns2:fileName&gt;P10003&lt;/ns2:fileName&gt;
                   &lt;ns2:alias&gt;project3&lt;/ns2:alias&gt;
               &lt;/ns2:Project&gt;
           &lt;/ns2:getProjectsResponse&gt;
       &lt;/SOAP-ENV:Body&gt;
   &lt;/SOAP-ENV:Envelope&gt;
</code></pre>

<p>Expected output:</p>

<p><a href=""https://i.stack.imgur.com/TvPSS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TvPSS.png"" alt=""output""></a></p>

<p>Can anyone help me on this.</p>
","<azure><azure-functions><azure-logic-apps><azure-data-factory><azure-databricks>","2020-02-13 16:03:48","2300","1","2","60224315","<p>You could try this way, firstly <a href=""https://learn.microsoft.com/zh-cn/archive/blogs/mioteg/converting-xml-to-json-with-azure-logic-apps?utm_content=bufferadeeb&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer#converting-to-json"" rel=""nofollow noreferrer"">convert xml to json</a> then use create csv table action to implement it.  The below is my test flow.</p>

<p><a href=""https://i.stack.imgur.com/fQufu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fQufu.png"" alt=""enter image description here""></a></p>

<p>I use blob to get the xml content. The <code>Compose</code> action input is <code>json(xml(body('Get_blob_content')))</code>, then will get thejson data. Then is the <code>Create CSV table</code> From, ause the from data should be array, so it should be <code>outputs('Compose')['SOAP-ENV:Envelope']['SOAP-ENV:Body']['ns2:getProjectsResponse']['ns2:Project']</code>.</p>

<p>The last thing is customize the header and the value, the <code>ProjectID</code> value should be <code>item()['ns2:fileName']</code> and the <code>ProjectDescription</code> should be <code>item()['ns2:alias']</code>.</p>

<p>And here is the flow output, suppose this is what you want, hope this could help you.</p>

<p><a href=""https://i.stack.imgur.com/Tm3vp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tm3vp.png"" alt=""enter image description here""></a> </p>
"
"60210625","Specify order of tables to copy using the Copy Data tool in Azure Data Factory to respect foreign keys","<p>The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""nofollow noreferrer"">Copy Data</a> tool lets me select the tables I want to copy from the source to the destination, but the tables are copied in <strong>alphabetical</strong> order. Since I have foreign keys defined, this cannot work. I would like to manually change the order.</p>
","<sql-server><azure><azure-sql-database><azure-data-factory>","2020-02-13 14:54:58","798","1","2","60219433","<p>Unfortunately,as i know,<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#dataset-properties"" rel=""nofollow noreferrer"">ADF copy activity SQL DB connector</a> only could transfer tables in the default order.It can't scan your constraint policy and execute in the optimal order.</p>

<p>So,i'm afraid that you have to figure out how your constraint policy set up and make the right order manually.After getting the table name sorting list,create the copy activity one by one for every table. </p>

<p>Surely,don't worry about this part.Every element could be created by <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">ADF SDK</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-powershell"" rel=""nofollow noreferrer"">Powershell script</a>.All you need to do is looping the list and pass it into snippet of code or script.Only the table name need to be changed per activity.</p>
"
"60210625","Specify order of tables to copy using the Copy Data tool in Azure Data Factory to respect foreign keys","<p>The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""nofollow noreferrer"">Copy Data</a> tool lets me select the tables I want to copy from the source to the destination, but the tables are copied in <strong>alphabetical</strong> order. Since I have foreign keys defined, this cannot work. I would like to manually change the order.</p>
","<sql-server><azure><azure-sql-database><azure-data-factory>","2020-02-13 14:54:58","798","1","2","60229464","<p>Here is a simple pipeline that copies data from one database to another - it has a ForEach with 2 activities inside it.  It copies each table and then runs a stored procedure after each table.  The column names in the tables are the same.  It has a variable called tableMapping that is a json array that defines the 'from' and 'to' table names as they can be different.  The ForEach has this setting ""batchCount"": 1, so that it runs one at a time.  In my testing just now it processes the tables in the sequence in the variable tableMapping.  If you don't specify the batchCount = 1 then it will run them in parallel.</p>

<pre><code>{
    ""name"": ""TowWorks to Azure SQL DB"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""ForEach1"",
                ""type"": ""ForEach"",
                ""dependsOn"": [],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""items"": {
                        ""value"": ""@variables('tableMapping')"",
                        ""type"": ""Expression""
                    },
                    ""batchCount"": 1,
                    ""activities"": [
                        {
                            ""name"": ""Copy from BI to Azure"",
                            ""type"": ""Copy"",
                            ""dependsOn"": [],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""userProperties"": [],
                            ""typeProperties"": {
                                ""source"": {
                                    ""type"": ""SqlServerSource"",
                                    ""queryTimeout"": ""02:00:00""
                                },
                                ""sink"": {
                                    ""type"": ""AzureSqlSink"",
                                    ""preCopyScript"": {
                                        ""value"": ""@{concat('truncate table raw.', item().to)}"",
                                        ""type"": ""Expression""
                                    },
                                    ""disableMetricsCollection"": false
                                },
                                ""enableStaging"": false
                            },
                            ""inputs"": [
                                {
                                    ""referenceName"": ""BI_TW_Raw_NoTable"",
                                    ""type"": ""DatasetReference"",
                                    ""parameters"": {
                                        ""tableName"": {
                                            ""value"": ""@item().from"",
                                            ""type"": ""Expression""
                                        }
                                    }
                                }
                            ],
                            ""outputs"": [
                                {
                                    ""referenceName"": ""TW_Azure_DB_noTable"",
                                    ""type"": ""DatasetReference"",
                                    ""parameters"": {
                                        ""schema"": ""raw"",
                                        ""table"": {
                                            ""value"": ""@item().to"",
                                            ""type"": ""Expression""
                                        }
                                    }
                                }
                            ]
                        },
                        {
                            ""name"": ""Stored Procedure1"",
                            ""type"": ""SqlServerStoredProcedure"",
                            ""dependsOn"": [
                                {
                                    ""activity"": ""Copy from BI to Azure"",
                                    ""dependencyConditions"": [
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""userProperties"": [],
                            ""typeProperties"": {
                                ""storedProcedureName"": {
                                    ""value"": ""@concat('raw.Update', item().to)"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""linkedServiceName"": {
                                ""referenceName"": ""TowWorksAzureSqlDB"",
                                ""type"": ""LinkedServiceReference""
                            }
                        }
                    ]
                }
            }
        ],
        ""variables"": {
            ""tableMapping"": {
                ""type"": ""Array"",
                ""defaultValue"": [
                    {
                        ""from"": ""WORK_TASKS"",
                        ""to"": ""WorkTask""
                    },
                    {
                        ""from"": ""Invoice"",
                        ""to"": ""Invoice""
                    },
                    {
                        ""from"": ""ASSET"",
                        ""to"": ""Asset""
                    },
                    {
                        ""from"": ""InvoiceDistribution"",
                        ""to"": ""InvoiceDistribution""
                    },
                    {
                        ""from"": ""InvoiceEvent"",
                        ""to"": ""InvoiceEvent""
                    },
                    {
                        ""from"": ""InvoiceTransaction"",
                        ""to"": ""InvoiceTransaction""
                    },
                    {
                        ""from"": ""LOGISTICS_ORDERS"",
                        ""to"": ""LogisticOrder""
                    },
                    {
                        ""from"": ""LOGISTICS_ORDERS_LINE_ITEMS"",
                        ""to"": ""LogisticOrderLineItem""
                    },
                    {
                        ""from"": ""WORK_ORDERS"",
                        ""to"": ""WorkOrder""
                    },
                    {
                        ""from"": ""WORK_ORDERS_STATUSES"",
                        ""to"": ""WorkOrderStatus""
                    }
                ]
            },
            ""from"": {
                ""type"": ""String""
            },
            ""to"": {
                ""type"": ""String""
            }
        },
        ""folder"": {
            ""name"": ""TowWorks""
        },
        ""annotations"": []
    }
}
</code></pre>
"
"60208791","How to work around 'lastindexof' is a primitive and doesn't support nested properties in Azure Data Factory","<p>I'm trying to take an existing filename in data factory and using dynamic content rearrange it so it has a timestamp appended to it. However I seem to be getting the following error:</p>

<blockquote>
  <p>Position 172 'lastindexof' is a primitive and doesn't support nested properties. </p>
</blockquote>

<p>It seems I can't put calculations within arguments to other functions which is really restrictive. Is there any work around for this? Should I be looking at another component in Data Factory to achieve this more easily?</p>

<p>Please see the dynamic content expression below:</p>

<p><code>@concat(substring(item().name, 0, lastindexof(item().name, '.')), '_', formatDateTime(utcnow(),'yyyyMMddhhmmss'), '.', substring(item().name, lastindexof(item().name, '.') + 1, length(item().name) - lastindexof(item().name, '.') - 1))</code></p>

<p>A basic test of what I want to achieve is to take the input filename <code>abc.csv</code> or <code>xyz.xlsx</code> and convert this to <code>abc_20200213131301.csv</code> or <code>xyz_20200213131301.xlsx</code>.</p>
","<azure><azure-data-factory>","2020-02-13 13:17:36","1704","2","2","60329557","<p>The thing which you are trying will work , i have done that but there will be too many ""("" and "",""</p>

<p>You can try implement by using 2 Set variables activity &amp; Split function  .</p>

<p>1st Activity </p>

<p>to use the split function with : @split(variables('FileName'),'.')</p>

<p>2nd Activity </p>

<p>@concat(variables('SplitFileName')[0],'_',formatDateTime(utcnow(),'yyyyMMddhhmmss'),'.',variables('SplitFileName')<a href=""https://i.stack.imgur.com/VTcM8.gif"" rel=""nofollow noreferrer"">1</a>)</p>

<p><a href=""https://i.stack.imgur.com/VTcM8.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VTcM8.gif"" alt=""enter image description here""></a></p>
"
"60208791","How to work around 'lastindexof' is a primitive and doesn't support nested properties in Azure Data Factory","<p>I'm trying to take an existing filename in data factory and using dynamic content rearrange it so it has a timestamp appended to it. However I seem to be getting the following error:</p>

<blockquote>
  <p>Position 172 'lastindexof' is a primitive and doesn't support nested properties. </p>
</blockquote>

<p>It seems I can't put calculations within arguments to other functions which is really restrictive. Is there any work around for this? Should I be looking at another component in Data Factory to achieve this more easily?</p>

<p>Please see the dynamic content expression below:</p>

<p><code>@concat(substring(item().name, 0, lastindexof(item().name, '.')), '_', formatDateTime(utcnow(),'yyyyMMddhhmmss'), '.', substring(item().name, lastindexof(item().name, '.') + 1, length(item().name) - lastindexof(item().name, '.') - 1))</code></p>

<p>A basic test of what I want to achieve is to take the input filename <code>abc.csv</code> or <code>xyz.xlsx</code> and convert this to <code>abc_20200213131301.csv</code> or <code>xyz_20200213131301.xlsx</code>.</p>
","<azure><azure-data-factory>","2020-02-13 13:17:36","1704","2","2","61617458","<p>You could try something like this.</p>

<pre><code>@concat(substring(item().name,0,lastindexof(item().name,'.')),'_',formatDateTime(utcnow(),'yyyyMMddhhmmss'), replace(item().name,substring(item().name,0,lastindexof(item().name,'.')),''))
</code></pre>

<p>Another option is to use <code>add</code> or <code>sub</code> functions. To find the remaining characters after the last ""."", I used the <code>sub</code> function.</p>

<pre><code>@concat(substring(item().name,0,lastindexof(item().name,'.')),'_',formatDateTime(utcnow(),'yyyyMMddhhmmss'), substring(item().name, lastindexof(item().name,'.'), sub(length(item().name), lastindexof(item().name, '.'))))
</code></pre>
"
"60206709","Event handling in Azure Data Factory","<p>Is there any method available in azure data factory to track the events of a pipeline? I have an event log table and I need to log all the events associated with the pipelines into the same table. Any best practice templates /methods available to achieve the same?</p>

<p>Regards,
Sandeep</p>
","<stored-procedures><azure-data-factory><azure-data-lake>","2020-02-13 11:26:25","443","2","1","60255501","<p>1.You could use Azure Monitor in ADF and enable diagnostic log and store the logs into Azure Blob Storage.Then analysis logs and transfer them into tables as you want.</p>

<p>2.Another choice,you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#net"" rel=""nofollow noreferrer"">ADF monitor SDK</a> or <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/activityruns/querybypipelinerun"" rel=""nofollow noreferrer"">REST API</a> to retrieve the activities run details by <code>Pipeline Run Id</code>:</p>

<pre><code>List&lt;ActivityRun&gt; activityRuns = client.ActivityRuns.ListByPipelineRun(
resourceGroup, dataFactoryName, runResponse.RunId, DateTime.UtcNow.AddMinutes(-10), DateTime.UtcNow.AddMinutes(10)).ToList(); 
if (pipelineRun.Status == ""Succeeded"")
    Console.WriteLine(activityRuns.First().Output);
else
    Console.WriteLine(activityRuns.First().Error);
</code></pre>

<p>Then you could get all the activities details into tables as you want.</p>
"
"60196649","Azure Function written in Powershell that returns a JObject","<p>I am writing an Azure Function in Powershell that will be invoked from Azure Data Factory. ADF requires that Functions return a JObject. Can someone please supply (or point me to) a short, complete Function script that does this? </p>

<p>I tried adding this line to the starter template that Azure generates, but this is from C# and obviously not correct for Powershell.</p>

<pre><code>return new OkObjectResult( new { StatusCode = $status, Body = $body });
</code></pre>

<p>Thanks!</p>
","<powershell><azure-functions><azure-data-factory>","2020-02-12 20:51:03","1432","1","2","60206003","<p>Suppose you want powershell function return a json object, you could just use <code>ConvertFrom-Json</code> or <code>ConvertTo-Json</code> to convert between object and string.</p>

<p>Actually if your body is in json-format string, it will be OK.</p>

<pre><code>$body = '{ ""key1"":""value1"", ""Key2"":""value2"" }'

Push-OutputBinding -Name Response -Value ([HttpResponseContext]@{
    StatusCode = $status
    Body = $body
})
</code></pre>

<p><a href=""https://i.stack.imgur.com/iqSWv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iqSWv.png"" alt=""enter image description here""></a></p>

<p>If I misunderstand, please let me know, hope this could help you.</p>
"
"60196649","Azure Function written in Powershell that returns a JObject","<p>I am writing an Azure Function in Powershell that will be invoked from Azure Data Factory. ADF requires that Functions return a JObject. Can someone please supply (or point me to) a short, complete Function script that does this? </p>

<p>I tried adding this line to the starter template that Azure generates, but this is from C# and obviously not correct for Powershell.</p>

<pre><code>return new OkObjectResult( new { StatusCode = $status, Body = $body });
</code></pre>

<p>Thanks!</p>
","<powershell><azure-functions><azure-data-factory>","2020-02-12 20:51:03","1432","1","2","60213432","<p>George Chen got me over the hump. My final solution is below. I verified that this works with Data Factory for both success and error responses. </p>

<pre><code>$body = $('{ ""functionReturnMsg"":""'  + $msg + '"" }')

Push-OutputBinding -Name Response -Value ([HttpResponseContext]@{ 
    StatusCode = $status
    Body = $body   # Data Factory wants this in JSON
    })
</code></pre>

<p>The ADF documentation makes this sound more complicated than it really is.</p>
"
"60190440","Azure Data Factory -> Copy from SQL to Table Storage (boolean mapping)","<p>I am adding pipeline in Azure Data factory to migrate data from SQL to Table storage.
All seems working fine, however i observed that bit column is not getting copies as expected.
I have a filed &quot;IsMinor&quot; in SQL DB.</p>
<p>If i don't add explicit mapping for bit column as is then, it is copied as <code>null</code>
If i set it as 'True' Or 'False' from SQL, it is copied as <code>String</code> instead of Boolean in TableStorage.</p>
<p>I also tried to specify type while mapping the field i.e. &quot;<code>IsMinor (Boolean)</code>&quot;, however it didn't worked as well.
Following is my sample table</p>
<p><a href=""https://i.stack.imgur.com/FDTLz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FDTLz.png"" alt=""enter image description here"" /></a></p>
<p>I want the bit value above to be copied as &quot;Boolean&quot; in Table storage instead of String.</p>
","<azure-table-storage><azure-data-factory>","2020-02-12 14:19:18","843","1","1","60200145","<p>I tried copy the boolean data from my SQL database to table Storage, it works. </p>

<p>As you know, SQL server does't support boolean data type, so I create table like this:
<a href=""https://i.stack.imgur.com/vfUQe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vfUQe.png"" alt=""enter image description here""></a></p>

<p>All the data preview look well in Source dataset:
<a href=""https://i.stack.imgur.com/T95Rh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T95Rh.png"" alt=""enter image description here""></a></p>

<p>I just create a table <code>test1</code> in Table storage, let the data factory create the PartitionKey and RowKey automatically.</p>

<p>Run the pipeline and check the data in <code>test1</code> with Storage Explorer:
<a href=""https://i.stack.imgur.com/wlvcG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wlvcG.png"" alt=""enter image description here""></a></p>

<p>From the document <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/understanding-the-table-service-data-model"" rel=""nofollow noreferrer"">Understanding the Table service data model</a>, Table storage do support Boolean property types.</p>

<p>Hope this help.</p>
"
"60189990","Is there a way to SkipLinesAtEnd in a TextFormat Azure Data Factory","<p>We receive Text files from a external partner.
They claim to be csv but have some difficult pre-header and footers.</p>

<p>In a ADF TextFormat I can use ""skipLineCount"": 6, But at the end i'm running in troubles ...</p>

<p>Any suggestions ?</p>

<p>Can't find something like SkipLinesAtEnd ....</p>

<p>This is the Sample</p>

<pre><code>TITLE : Liste de NID_C_BG_NPIG configuré.
FILE NAME : Ines_bcn_npig_net_f.csv
CREATION DATE : 09/10/2019 23:18:43
ENVIRONMENT : Production 12c
&lt;Begin of file&gt;
""NID_C"";""NID_BG"";""N_PIG""
""253"";""0"";""0""
""253"";""0"";""1""
""253"";""1"";""0""
""253"";""1"";""1""
""253"";""2"";""0""
""253"";""2"";""1""
""253"";""3"";""0""
&lt;End of file&gt;
</code></pre>
","<azure-data-factory>","2020-02-12 13:56:06","123","1","2","60199940","<p>It seems that you are using <code>skipLineCount</code> setting in Data Flow.No feature like <code>skipLinesAtEnd</code> in ADF.</p>

<p>You could follow suggestion mentioned by @Joel that using Alter Row.</p>

<p><a href=""https://i.stack.imgur.com/3sV2p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3sV2p.png"" alt=""enter image description here""></a></p>

<p>However,based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row"" rel=""nofollow noreferrer"">official document</a>,it only supports database mode sink.</p>

<p><a href=""https://i.stack.imgur.com/ldZpb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ldZpb.png"" alt=""enter image description here""></a></p>

<p>So,if you are limited by that,i would suggest you parse the file first before copy job.For example,add an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> to cut the extra rows if you know the specific location of header and foot.Inside the Azure Function,just use the code to alter the file.</p>
"
"60189990","Is there a way to SkipLinesAtEnd in a TextFormat Azure Data Factory","<p>We receive Text files from a external partner.
They claim to be csv but have some difficult pre-header and footers.</p>

<p>In a ADF TextFormat I can use ""skipLineCount"": 6, But at the end i'm running in troubles ...</p>

<p>Any suggestions ?</p>

<p>Can't find something like SkipLinesAtEnd ....</p>

<p>This is the Sample</p>

<pre><code>TITLE : Liste de NID_C_BG_NPIG configuré.
FILE NAME : Ines_bcn_npig_net_f.csv
CREATION DATE : 09/10/2019 23:18:43
ENVIRONMENT : Production 12c
&lt;Begin of file&gt;
""NID_C"";""NID_BG"";""N_PIG""
""253"";""0"";""0""
""253"";""0"";""1""
""253"";""1"";""0""
""253"";""1"";""1""
""253"";""2"";""0""
""253"";""2"";""1""
""253"";""3"";""0""
&lt;End of file&gt;
</code></pre>
","<azure-data-factory>","2020-02-12 13:56:06","123","1","2","60217124","<p>Jay &amp; Joel are correct in pointing you toward Data Flows to solve this problem. Use Copy  Activity in ADF for data movement-focused operations and Data Flows for data transformation.</p>

<p>You'll find the price for data movement similar to that of data transformation.</p>

<p>I would solve this in Data Flow and use a Filter transformation to filter out any row that has the string """" in it.</p>

<p>Should not need an Alter Row in this case.  HTH!!</p>
"
"60188640","Add headers in csv file using azure data factory while moving to sink","<p>How can we add headers to the files existing in the blob/ azure data lake using azure data factory.
I am using a copy activity to move the header less files to the sink, but while moving the files should have default headers like ""Prop_0"" or ""Column_1"". Any method available to achieve the same?</p>

<p>Any help would be appreciated.
Thanks and Regards,
Sandeep </p>
","<azure-data-factory><azure-data-lake>","2020-02-12 12:42:53","5232","2","3","60199620","<p>In usually, Data factory will using the default header <code>Prop_0, Prop_1...Prop_N</code>  for the less header csv file  to help us copy the data, if we don't set the <code>first row as header</code>.</p>

<p>This is to help us do the column mapping but won't change the csv file.</p>

<p>According my experience and know about Data Factory, it doesn't support us do the schema change of the csv file. It's impossible to add the header of the csv files at least for now.</p>

<p>Hope this helps</p>
"
"60188640","Add headers in csv file using azure data factory while moving to sink","<p>How can we add headers to the files existing in the blob/ azure data lake using azure data factory.
I am using a copy activity to move the header less files to the sink, but while moving the files should have default headers like ""Prop_0"" or ""Column_1"". Any method available to achieve the same?</p>

<p>Any help would be appreciated.
Thanks and Regards,
Sandeep </p>
","<azure-data-factory><azure-data-lake>","2020-02-12 12:42:53","5232","2","3","65829544","<p>In ADF, create a new Data Flow. Add your CSV source with a no header dataset. Then add your Sink with a dataset that writes to ADLS G2 folder as a text delimited file WITH headers. In the Sink Mapping, you can name your columns:
<a href=""https://i.stack.imgur.com/Q0K81.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q0K81.png"" alt=""enter image description here"" /></a></p>
"
"60188640","Add headers in csv file using azure data factory while moving to sink","<p>How can we add headers to the files existing in the blob/ azure data lake using azure data factory.
I am using a copy activity to move the header less files to the sink, but while moving the files should have default headers like ""Prop_0"" or ""Column_1"". Any method available to achieve the same?</p>

<p>Any help would be appreciated.
Thanks and Regards,
Sandeep </p>
","<azure-data-factory><azure-data-lake>","2020-02-12 12:42:53","5232","2","3","70246291","<p>I tried a different solution. Used the 'no delimiter' option to keep all of them as one column. Then, In the derived column action, I split the single column into multiple columns and provided a proper name for each column. Now we can map the columns into the target table.</p>
"
"60187395","decompress a .Z file in Azure Data Factory","<p>How to decompress a .Z file in Azure Data Factory while pulling the file from source (SFTP) to ADLS. I've tried all the availabe options in Dataset but it's not working. I'm able to decompress a zip file but not able to do for .Z file.</p>

<p>Error - message"": </p>

<blockquote>
  <p>""Failure happened on 'Sink' side.
  ErrorCode=UserErrorUnzipInvalidFile,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The
  file 'nz.drstores.cdp.csv.Z' is not a valid Zip file with Deflate
  compression
  method.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.IO.InvalidDataException,Message=End
  of Central Directory record could not be
  found.,Source=Microsoft.DataTransfer.ClientLibrary,</p>
</blockquote>
","<azure><azure-data-factory>","2020-02-12 11:31:36","940","1","1","60203899","<p>ADF deflate compression only supports below format of compress files which is stated in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-binary#dataset-properties"" rel=""nofollow noreferrer"">official document</a> clearly:</p>

<p><a href=""https://i.stack.imgur.com/kYCYl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kYCYl.png"" alt=""enter image description here""></a></p>

<p>So no built-in feature in ADF,you may need to convert .Z file into supported format by yourself.Or you could submit feedback <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">here</a> to ask improvement of ADF.</p>
"
"60177536","Azure Data Flow / Data factory Error Handling","<p>What I want to achieve is to include error handling in azure data Flow if an error occurs while transferring the rows it should not fail it process the other rows and save the id of the row which occurred error in a text file or log </p>

<p>Example:</p>

<p>Let suppose we have 10 rows to sink in a table and somehow we got an error on row 5, but I want the data-flow to skip that row but insert remaining 9 rows and save the id of the row which created an error in a text file or a SQL Table</p>
","<azure><azure-data-factory>","2020-02-11 21:21:06","1587","1","1","60328994","<p>You can set that fault tolerance option in the copy activity to achieve this .</p>

<p><a href=""https://i.stack.imgur.com/eGzAs.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eGzAs.gif"" alt=""enter image description here""></a></p>

<p>In addition to above, you can also achieve the same in Mapping Data Flow of Azure Data Factory with error row handling concept using Conditional split in your Data Flow. </p>

<p>Please refer to this MS doc which has a sample on how to split/log the error rows based on specific condition : <a href=""https://learn.microsoft.com/azure/data-factory/how-to-data-flow-error-rows"" rel=""nofollow noreferrer"">Error Row handling in Mapping Data Flow of Azure Data Factory</a></p>

<p>Also please refer to below MSDN thread where a similar implementation was explained for handling error rows in Data flow.</p>

<p>MSDN thread: <a href=""https://social.msdn.microsoft.com/Forums/en-US/f7a14806-8e2a-4d82-8311-bea702848f70/mapping-data-flows-error-handling?forum=AzureDataFactory"" rel=""nofollow noreferrer"">Mapping Data Flows Error Handling</a></p>
"
"60168856","How can we identify if a file contains header or not using powershell / azure data factory","<p>Is there any option available to identify the header of a text/csv file dynamically using power shell /azure data factory ?   The solution required to search the file and then need to identify if any header row is there in the file. If it exists then it should remove the header and if not no action. </p>

<p>Any help would be appreciated.</p>

<p>Thanks and Regards,
Sandeep</p>
","<powershell><azure-data-factory><azure-powershell><azure-data-lake>","2020-02-11 12:31:36","434","0","1","60177247","<p>Here is an example using ADF: <a href=""https://kromerbigdata.com/2019/09/28/adf-dynamic-skip-lines-find-data-with-variable-headers/"" rel=""nofollow noreferrer"">https://kromerbigdata.com/2019/09/28/adf-dynamic-skip-lines-find-data-with-variable-headers/</a></p>
"
"60164812","Azure Data Factory: Dataset Dynamic DB Table name not resolving in Data Wrangling Flow","<p>I created a DataSet which points to a table in my database. The name of the table is set as dynamic content: <code>@concat(dataset().db_prefix, '_Baseline_CIs')</code>. This works when checking in the ﻿Dataset through 'Preview Data'. The table contents are shown.</p>

<p>BUT: When using the dataset in the Data Warngling Flow, the M-query fails with the following error:</p>

<pre><code>Expression.Error: The key didn't match any rows in the table.
AdfDoc = Sql.Database(""oedudigital.database.windows.net"", ""IntegratedEnvironments""),
  InputTable = AdfDoc{[Schema = ""dbo"", Item = ""undefined""]}[Data]
</code></pre>

<p>As you can see, the table name concatenation has returned 'undefined'. Is this a bug?</p>

<p>BR, Denis</p>
","<azure-data-factory><dataflow><data-wrangling>","2020-02-11 08:40:47","440","0","1","60328907","<p>If I understand it right you have the DataSet which is parameter , at least that was the case on my side . Under the AdFResouce you will see the dataset name . You will have to pass the table name as  </p>

<p>AdfDoc{[Schema = ""dbo"", Item = ""<strong>TableName</strong>""]}[Data]
and then it will bring in the records .</p>

<p><a href=""https://i.stack.imgur.com/ZDzXS.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZDzXS.gif"" alt=""enter image description here""></a></p>
"
"60162129","How to get first row and first column value of CSV in DataFactory?","<p>We are loading data from CSV to SQL table using data factory and CSV are stored in azure blob storage and if the loading fails I need to update the log the file loading  is failed with file Id.</p>

<p>File ID is present in first column of CSV .
How can I get that in Datafactory?</p>
","<azure><azure-sql-database><azure-data-factory><azure-blob-storage>","2020-02-11 04:39:26","1167","0","1","60180544","<p>Hi，Veeramalla Sandeep，I tried a lot ways but all failed.</p>

<p>Data Factory doesn't support do query or filter with the csv file.</p>

<p>That means that we can't not get the first row or column of the csv files.</p>

<p>Hope this helps.</p>
"
"60153111","azure data factory lookup activity - parameterize sql query","<p>How do I parameterize the where condition in a lookup activity query in azure data factory? I have created a pipeline parameter and tried to pass it to the lookup activity query as given below.</p>

<p>select max(dt) as dt from tab1 where col='@pipeline.parameters.parama1'</p>

<p>I have tried with quotes, without quotes, curly brackets, but still not firing. Any help would be appreciated.</p>

<p>Regards,
Sandeep</p>
","<azure-data-factory>","2020-02-10 15:06:08","6899","5","1","60156906","<p>Official doc here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a></p>

<p><em>Expressions can also appear inside strings, using a feature called string interpolation where expressions are wrapped in @{ ... }.</em></p>

<p>Taking this into consideration, this may work for you:</p>

<pre><code>select max(dt) as dt from tab1 where col=@{pipeline().parameters.param}
</code></pre>

<p>Hope this helped!</p>
"
"60152283","Retrieve Datafactory ManagedID in ARM template","<p>I have a nested template to create a <code>datafactory</code> and I want to retrieve its managed identity to the master template. However, I'm having a problem using the reference function as shown below. </p>

<p>It expects 1-2 arguments but the online documentation asks to use the format I am using - see image</p>

<p><img src=""https://i.stack.imgur.com/WDDva.png"" alt=""https://i.stack.imgur.com/WDDva.png""></p>
","<azure><azure-data-factory><azure-rm-template><azure-managed-identity>","2020-02-10 14:15:07","104","0","1","60174860","<p>Please provide the online documentation you are referring to.  Based on your snippet I believe it is because you are using <code>reference(concat(</code>it should be<code>reference(resourceId(</code> as really you want the actually resourceID.</p>
"
"60152051","MIME type not supported on Azure Data Factory ODATA","<p>I decorated my Controller class with [Produces(""application/json"")] to enforce JSON output, but did not help.</p>

<p>Did you find a solution to this issue? I get the same error with the same scenario, but intermittently. The user however get the same error consistently.</p>

<p><a href=""https://stackoverflow.com/questions/59173492/how-to-handle-not-supported-mime-type-on-azure-datactory"">How to handle not supported MIME type on azure datactory?</a></p>
","<odata><azure-data-factory>","2020-02-10 14:01:40","266","0","2","60180235","<p>According my experience and I searched a lot, we can't find any solutions to solve the problem. I'm afraid we can't answer you.</p>

<p>So I suggest that you can call the Azure Support for help , maybe they could some more effective suggestions or solution. </p>

<p>You could follow these steps On Portal:</p>

<p><a href=""https://i.stack.imgur.com/v1aFL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v1aFL.png"" alt=""enter image description here""></a></p>

<p>New support request:</p>

<p><a href=""https://i.stack.imgur.com/XNtDT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XNtDT.png"" alt=""enter image description here""></a></p>

<p>Fill the details about you problem:
<a href=""https://i.stack.imgur.com/SLTds.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SLTds.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Fqcy7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fqcy7.png"" alt=""enter image description here""></a></p>

<p>Then create the request, Azure Support will email or call you later.</p>

<p>Hope this helps.</p>
"
"60152051","MIME type not supported on Azure Data Factory ODATA","<p>I decorated my Controller class with [Produces(""application/json"")] to enforce JSON output, but did not help.</p>

<p>Did you find a solution to this issue? I get the same error with the same scenario, but intermittently. The user however get the same error consistently.</p>

<p><a href=""https://stackoverflow.com/questions/59173492/how-to-handle-not-supported-mime-type-on-azure-datactory"">How to handle not supported MIME type on azure datactory?</a></p>
","<odata><azure-data-factory>","2020-02-10 14:01:40","266","0","2","60195807","<p>Thanks Leon. I was able to resolve the issue. My ODATA API (Multi-Tenant App) was in a different Tenant and the Data Factory was in a different Tenant. In my API, the ValidateIssuers was set to true and the ValidIssuers was assigned correctly - array of valid tenants. Even then this was breaking it. I set the ValidateIssuers to false and wrote my custom logic to validate the issuers. This pretty much fixed it. There was another error related to the Audience. The AAD resource property in the Data Factory Linked Service (Connection) should match the ODATA Api AD applications Application ID URI – it is case sensitive. In all these scenarios, the error was always the MIME error which was throwing me off – completely misleading. I had to do some logging in my ODATA Api to get to the actual issue.</p>
"
"60151916","Azure Data Factory GitHub Integration Private Repo","<p>I only seem able to integrate my ADFv2 instance with <strong><em>public</em></strong> repos in my organisation, which is undesirable.</p>

<p>In the documentation (<a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/source-control.md#author-with-github-integration"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/source-control.md#author-with-github-integration</a>), it states that:</p>

<pre><code>..can use both public and private GitHub repositories with Data Factory as long you have read and write permission to the repository..
</code></pre>

<p>So, i complete the following steps:</p>

<ol>
<li>Navigate to my GitHub Org.</li>
<li>Create a new ""Private"" repo, initialised w/ README.MD.</li>
<li>Create a new ""Internal"" repo, initialised w/ README.MD. (this is GitHub Enterprise Cloud)</li>
<li>In the Azure Portal, create a new ADFv2 instance without Git integration.</li>
<li>Click ""Set up code repository"" in the ADFv2 instance.</li>
<li>Choose, RepoType: <code>GitHub</code> - Authenticate w. GitHub pop-up.</li>
<li>Enter org name in to the GitHub Account section.</li>
<li>Git repo name: <strong><em>No results found</em></strong></li>
</ol>

<p><a href=""https://i.stack.imgur.com/FyQpb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FyQpb.png"" alt=""enter image description here""></a></p>

<p>🤷🏼‍♂️</p>
","<azure><github><azure-data-factory><github-enterprise>","2020-02-10 13:53:55","1295","0","1","60196806","<p>The solution turned out to be the need to grant access as an Authorized OAuth App to the organisation. I did not have permission to do this.</p>

<p>You can then find it under 'Applications' once done.. but i think when you first connect up your ADF to GitHub, it prompts you if you would like to grant it.</p>

<p>I was not seeing this due to permissions on the Org.</p>
"
"60149069","Azure Data Factory Copy Data SFTP","<p>I'm trying to copy data from a .CSV file on an SFTP server to a Azure SQL server using Azure Data Factory.</p>

<p>When configuring the SFTP and mapping the file to the Azure SQL server table, everything is fine and it connects fine (I can preview data from .CSV file on SFTP and auto map columns), but when triggering the pipeline, it fails to connect to the SFTP and I get the following error.</p>

<pre><code>Operation on target SFTP failed: ErrorCode=UserErrorNetworkIssue,
'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
Message=Meet network issue when connect to Sftp server 'sftp.websitename.co.uk', 
SocketErrorCode: 'TimedOut'.,Source=Microsoft.DataTransfer.ClientLibrary.SftpConnector,
''Type=System.Net.Sockets.SocketException,
Message=A connection attempt failed because the connected party did not properly respond after a period of time, 
or established connection failed because connected host has failed to respond,Source=Renci.SshNet,'
</code></pre>
","<azure><sftp><azure-data-factory>","2020-02-10 11:06:20","1106","1","1","60153997","<p>Disable SSH Key Validation and tried?</p>

<p><a href=""https://i.stack.imgur.com/jVusz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jVusz.png"" alt=""enter image description here""></a></p>
"
"60139933","Importing pipeline with linked services in azure portal","<p>I have exported my all pipelines (azure data factory) and it is in .zip folder. Now it contains all my pipeline information (json format) with my linked services. When I want to import the zipped folder, I selected <strong>pipeline from template</strong> option, after import I am having all my pipelines but there is no linked services imported, can any one let me know the issue. </p>
","<azure><pipeline><azure-data-factory><azureportal>","2020-02-09 18:30:58","393","0","1","60144045","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services"" rel=""nofollow noreferrer"">Linked Services</a> are not technically defined as part of the pipeline but rather as part of the Data Factory itself. </p>

<p>Try exporting the Data Factory from either Azure Portal (Export Template button on the data factory) or <a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/get-azdatafactoryv2linkedservice?view=azps-3.4.0"" rel=""nofollow noreferrer"">PowerShell</a> for the linked services information.</p>

<p>This is common practice with a lot of ETL or ELT like process where the connection string information is defined and shared by the main instance and not defined in the jobs locally.</p>
"
"60117356","Connect ADF to ServiceNow URL","<p>I am fairly new to Azure, but I have been doing ETL for quite some time now. I want to connect ADF to ServiceNow to bring in lists to our SQL data warehouse. Does anyone have any good articles or what the settings are on how to achieve this?</p>
","<azure-data-factory>","2020-02-07 16:21:23","1060","0","1","60143111","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">copy activity</a> in ADF which supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-servicenow"" rel=""nofollow noreferrer"">Service Now</a> as input source and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse"" rel=""nofollow noreferrer"">Azure Synapse Analytics</a>(formerly Azure SQL Data Warehouse) as output sink. </p>

<p>Since you are new to ADF,based on above tutorials,i'm afraid that there are 3 elements you should get know when you execute copy activity.</p>

<p>1.Linked Service:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services</a></p>

<p>2.Dataset:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services</a></p>

<p>3.Pipeline:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</a></p>

<p>If you want to execute pipeline in the schedule, you also could add time trigger onto specific pipeline.</p>
"
"60109363","Problem retrieving Dynamics CRM custom fields for integration in ADF","<p>I am doing data integration from Dynamics CRM to Azure SQL Database using Azure Data Factory. I am unable to fetch the custom fields 
created in Dynamics CRM for doing the mapping in integration. Please help me with the solution for the above.</p>
","<dynamics-crm><azure-data-factory>","2020-02-07 08:06:17","166","0","1","60264451","<p>We ran pretty hard into the limitations of the Dynamics connector of Azure Data Factory v2. The ADF Dynamics connector does not expand the fields and requires a way too big rewrite of our current logic. Either with an entity selection or FetchXML, same results.</p>

<p>We were using SSIS + Cozyroc that did expand a lot of fields automatically. So we are back to using SSIS + Cozyroc. And since running custom components on SSIS-IR is... challenging we are also back to running SSIS on-premise.</p>
"
"60109058","Purge file through ADF V2 before 60 days from ADLS","<p>I want to delete all the heavy files which are older than 60 days through my ADF pipeline but want to retain the files for the first 6 working days of the month (irrespective of the last 60 days).
Also, the files should be deleted on the name basis like Account, customer service, etc
My files are in ADLS V1.
Could you pl help me to get the approach here.I have gone through multiple blogs but cant find anything concrete.</p>

<p>Thanks</p>
","<azure><azure-data-factory><azure-data-lake>","2020-02-07 07:43:01","298","0","1","60145402","<p>Only the Data Factory <a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity"" rel=""nofollow noreferrer"">Delete active</a> can help you delete all  files which are older than 60 days.</p>

<p>Delete the old files by the filter ""Modified datetime"":
<a href=""https://i.stack.imgur.com/8fw7t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8fw7t.png"" alt=""enter image description here""></a></p>

<p>But for now, Data Factory can not help you delete all the heavy files which are older than 60 days through my ADF pipeline but want to retain the files for the first 6 working days of the month. </p>

<p>Hope this helps.</p>
"
"60102025","Adjusting Batch read size using Teradata as a Source in ADF pipeline","<p>I was wondering if anyone had a chance to use Teradata as a Source and if so do you know if we can set batch read size for it? I am currently connecting to an on-prem Teradata and was hoping to improve performance to move files to a blob storage account by adjusting batch size on the Teradata source side.</p>

<p>Thank you</p>
","<azure-data-factory>","2020-02-06 19:12:33","134","0","1","60103863","<p>The batch write size  is the property which is on the sink side and if I understand that you are reading the data from Terradata which is the source . I think you are looking for a Read Batch Size property which to my understanding does not exist at this time . </p>

<p>But I think since Terradata support TOP N in the query , you can loop through all the records with that , by reading top n records at a time . </p>

<p>I am not Terradata literate , but I think we can achieve this in SQL .</p>
"
"60096929","Cannot save file to Blob Storage using PowerShell","<p>Xposted question..</p>

<p>I am setting up a script from Azure Batch Services and have it injected to Azure Data Factory using Custom Batch Service. </p>

<ul>
<li><p>From On-Prem (from my local computer): I can save the file to blob using ""Set-AzureStorageBlobContent""  though PowerShell just fine. (Set-AzureStorageBlobContent is only for uploading files from local to Azure, not Azure to Azure)</p></li>
<li><p>From Cloud Shell, I can run $data | export-CSV -path ($home + ""/clouddrive/"" + $SubName + ""result.csv"") –NoType</p></li>
</ul>

<p>However, from Cloud Shell. but it won't let me save the CSV to my blob storage, this is the steps:</p>

<hr>

<ol>
<li><p>I use cd to go to the blob storage container, where all my file resides e.g. azure\storageaccount\blob\container\xxxx\</p></li>
<li><p>Within the very directory.....I ran: $data | export-csv .\result.csv -notype</p></li>
<li><p>I get this error:</p></li>
</ol>

<p>Export-Csv : Cannot open file because the current provider (SHiPS\SHiPS) cannot open a file.</p>

<p>I know it must have to do with the way I specify the out-path of the CSV file, or is it just not possible to export a file directly from Cloud Shell to blob storage? </p>
","<azure><azure-data-factory><azure-powershell>","2020-02-06 14:13:06","481","0","1","60204963","<p>If your requirement is to send a file directly from Cloud Shell to Azure Blob Storage then it is possible as illustrated in below screenshots.</p>
<p><a href=""https://i.stack.imgur.com/Pm3b6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pm3b6.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/glBit.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/glBit.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/N4cyi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N4cyi.png"" alt=""enter image description here"" /></a></p>
"
"60085081","JSON Copy activity from Azure Table to Data LAke Gen2 failing in Data Factory","<p>I am facing an error when trying to do a JSON copy activity with ‘Source’ as an Azure Table and ‘Sink’ as ADLS Gen 2 Storage.   </p>

<p>The error is:</p>

<blockquote>
  <p>Operation on target TableToADLSPipeline failed:
  ErrorCode=UserErrorColumnMappingNotAllowed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
  mapping is not allowed for copy between 'AzureTableStorage' and
  'AzureBlobFS(JsonFormatV2)'.,Source=Microsoft.DataTransfer.ClientLibrary,'</p>
</blockquote>

<p>However, when they  save the file in CSV format and it works fine.  Is there some sort of restriction inherent to copying data from Azure Table Storage to Azure Blob Storage in JSON format?  What are other options besides the CSV files for the ADLS Blob (Container) storage for copying data from Azure Data Table?  The client may not want the CSV format.  Is that my only alternative?</p>

<p>Thanks!</p>
","<azure-data-factory>","2020-02-05 22:33:09","356","0","1","60328356","<p>I was able to repro the issue and I think you have not set the mapping . Setting the mapping is very easy you can do that from the UI .See the screen shot below .
<a href=""https://i.stack.imgur.com/tay9A.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tay9A.gif"" alt=""enter image description here""></a></p>
"
"60074432","Getting ""Error converting data type VARCHAR to DATETIM""E while copying data from Azure blob to Azure DW through Polybase","<p>I am new to the Azure environment and i am using data factory while trying to copy data present in the CSV file on Azure blob storage which has three columns (id,age,birth date) to a table in Azure data warehouse. The birth date is of the format ""MM/dd/yyyy"" and i am using polybase to copy the data from blob to my table in azure DW. The columns of the table are defined as(int,int,datetime).</p>

<p>I can copy my data if i use ""Bulk Insert"" option in data factory but it gives me an error when i choose the Polybase copy. Also changing the dateformat in the pipleine does not do any good either.
Polybase copies successfully if i change the date format in my file to ""yyyy/MM/dd"".</p>

<p>Is there a way i can copy data from my blob to my table without having to change the date format in the source file to ""yyyy/MM/dd"".</p>
","<azure><azure-data-factory><polybase>","2020-02-05 11:07:18","1508","0","2","60115424","<p>I assume you have created an external file format which you reference in your external table? </p>

<p>The <code>CREATE EXTERNAL FILEFORMAT</code> has an option to define how a date is represented: <code>DATE_FORMAT</code>, and you set that to how your source data represents datetime.</p>

<p>So something like so:</p>

<pre><code>CREATE EXTERNAL FILE FORMAT your-format  
WITH 
(  
  FORMAT_TYPE = DELIMITEDTEXT,  
  FORMAT_OPTIONS (  
      FIELD_TERMINATOR = '|',  
      DATE_FORMAT = 'MM/dd/yyyy' ) 
);  
</code></pre>

<p>You can find more about this at: <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql?view=sql-server-ver15</a></p>
"
"60074432","Getting ""Error converting data type VARCHAR to DATETIM""E while copying data from Azure blob to Azure DW through Polybase","<p>I am new to the Azure environment and i am using data factory while trying to copy data present in the CSV file on Azure blob storage which has three columns (id,age,birth date) to a table in Azure data warehouse. The birth date is of the format ""MM/dd/yyyy"" and i am using polybase to copy the data from blob to my table in azure DW. The columns of the table are defined as(int,int,datetime).</p>

<p>I can copy my data if i use ""Bulk Insert"" option in data factory but it gives me an error when i choose the Polybase copy. Also changing the dateformat in the pipleine does not do any good either.
Polybase copies successfully if i change the date format in my file to ""yyyy/MM/dd"".</p>

<p>Is there a way i can copy data from my blob to my table without having to change the date format in the source file to ""yyyy/MM/dd"".</p>
","<azure><azure-data-factory><polybase>","2020-02-05 11:07:18","1508","0","2","60376019","<p>Seems like this error is resolved now. I was giving the date-format as 'MM/dd/yyyy' whereas the data factory expected it to be just MM/dd/yyyy without any quotes.</p>

<p>So as per my understanding i will summarize what i learned while copying data from Azure blob to Azure SQL Data Warehouse with a 'MM/dd/yyy' date format, in a few points here :</p>

<p>1) If you are using azure portal to copy data from blob to azure sql data warehouse using Data Factory copy option.</p>

<ul>
<li>Create a copy data pipe line using data factory.</li>
<li>Specify your input data source and your destination data store.</li>
<li>Under filed mappings,choose datetime in the column that contains the
date, click on the little icon on its right to bring the custom date
format field and enter your date format without quotes e.g.
MM/dd/yyyy  as in my case.</li>
<li>Run your pipleline and it should successfully complete.</li>
</ul>

<p>2) You can use polybase directly by:</p>

<ul>
<li>Creating external data source that specifies the location of your
input file e.g. csv file on blob storage in my case.</li>
<li>An external file format that specifies the delimiter and custom date format e.g. MM/dd/yyyy in your
input file.  </li>
<li>External table that defines all the columns present in your source
file and uses the external data storage and file format which you
defined above.</li>
<li>You can then create your custom tables as select using the external
table(CTAS).Something which Niels stated in his answer above.I used
Microsoft SQL Server Management Studio for this process.</li>
</ul>
"
"60069889","Onpremise Databases to Azure SQL Databases and Sync continuously","<p>My requirements are as below : </p>

<ol>
<li>Move 3 SAP local databases to 3 Azure SQL DB. </li>
<li>Then Sync daily transactions or data to azure every night. If transactions of local DB are already exists in azure, update process will do on these transactions if not insert process will do.</li>
<li>Local systems will not stop after moving to azure. They will still goes about 6 months.</li>
</ol>

<p>Note : </p>

<ol>
<li><strong>We are not compatible with Azure Data Sync process because of it's
    limitations - only support 500 tables, can't sync no primary keys
    table, no views and no procedure. It also increase database size on
    both(local and azure).</strong></li>
<li><strong>Azure Data Factory Pipeline can fulfill my requirements but I have
to create pipeline and procedure manually for each table. (SAP has
over 2000 tables, not good for me)</strong></li>
<li><strong>We don't use azure VM and Manage Instance</strong></li>
</ol>

<p>Can you guide me the best solution to move and sync? I am new to azure.</p>

<p>Thanks all.</p>
","<azure><azure-data-factory><azure-data-sync>","2020-02-05 06:10:38","70","0","1","60097241","<p>Since you mentioned that ADF basically meets your needs, I will try to start from ADF. Actually,you don't need to manually create each table one by one.The creation could be done in the ADF sdk or powershell script or REST api. Please refer to the official document:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/</a></p>

<p>So,if you could get the list of SAP table names(i found this thread:<a href=""https://answers.sap.com/questions/7375575/how-to-get-all-the-table-names.html"" rel=""nofollow noreferrer"">https://answers.sap.com/questions/7375575/how-to-get-all-the-table-names.html</a>) ,you could loop the list and execute the codes to create pipelines in the batch.Only <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sap-table#dataset-properties"" rel=""nofollow noreferrer"">table name</a> property need to be set.</p>
"
"60069848","selection of task in release pipeline in azure devops","<p>I have a build pipeline which I created and I have the artifacts in a drop folder. 
Now I want to deploy these artifacts. The drop folder contains my data factory resources like pipelines, datasets. </p>

<p>I have 2 questions:</p>

<ol>
<li><p>What task for the agent should I select in the release pipeline to deploy this folder?</p></li>
<li><p>How do I specify that in this environment it should go ? i.e. If I suppose have a QA environment, then how do I specify that thing?</p></li>
</ol>

<p>I am actually confused over this selection of task for the agent.</p>
","<azure><azure-devops><azure-data-factory><azure-pipelines-release-pipeline>","2020-02-05 06:06:33","191","0","1","60087318","<blockquote>
  <p>What task for the agent should I select in the release pipeline to
  deploy this folder?</p>
</blockquote>

<p>It's based on where and how you want to deploy the artifacts.
If you just want to deploy them in a local machine where you can access, then you just need to add a <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/copy-files?view=azure-devops&amp;tabs=yaml"" rel=""nofollow noreferrer"">Copy Files task</a> to copy the artifacts to the target folder.</p>

<p>If you want to deploy to other remote sites or third party services, then you need to select other tasks based on your requirements, and some of them we need to create <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/library/service-endpoints?view=azure-devops&amp;tabs=yaml"" rel=""nofollow noreferrer"">services connections</a> to access them. 
For example <a href=""https://learn.microsoft.com/en-us/azure/devops/deploy-azure/?view=azure-devops"" rel=""nofollow noreferrer"">Deploy to Azure</a>(e.g. <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/apps/cd/deploy-webdeploy-webapps?view=azure-devops"" rel=""nofollow noreferrer"">Deploy a web app to Azure App Services</a>)</p>

<blockquote>
  <p>How do I specify that in this environment it should go ? i.e. If I
  suppose have a QA environment, then how do I specify that thing?</p>
</blockquote>

<p>In release pipeline we can defined multiple environments/stages, in each environment we can add different tasks to do the actions based on our requirements. We need to add the artifacts sources to deploy them in the release pipeline. Please see <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/release/?view=azure-devops"" rel=""nofollow noreferrer"">Release pipelines</a> for details. You can reference below links to do that:</p>

<ul>
<li><a href=""https://azuredevopslabs.com/labs/vstsextend/azuredevopsprojectdotnet/#exercise-2-examine-the--cicd-pipelines-configured-by-azure-devops-project"" rel=""nofollow noreferrer"">Reference Exercise 2: Examine the CI/CD pipelines configured by
Azure DevOps Project</a> </li>
<li><a href=""https://www.youtube.com/watch?v=A0gpZCwJA5Q"" rel=""nofollow noreferrer"">Azure DevOps - creating a release pipeline- Video Guide</a></li>
</ul>
"
"60069125","Can we extract delta data from RestAPi through Azure DataFactory?","<p>My Source is Rest API (comma separated value format)..how can I extract delta data in Azure Data factory?</p>
","<azure><azure-data-factory>","2020-02-05 04:39:44","160","0","1","60089187","<p>One way to do so would be to copy the data from the REST API to a staging table in Azure SQL and then set up incremental copy from the staging table to your final sink dataset.</p>

<p>Ref - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview</a></p>

<p>Hope this helps.</p>
"
"60064394","Is there a way to add forced empty values to columns on the sink side in a copy data flow?","<p>I have a JSON file  as my data source. I am trying to convert this file to a .csv format. The sink will contain several empty columns I would like to add. The problem is, it seems I am unable to add a column which does not exist in the source JSON file. I have tried several different sink types (database table, csv, etc.) I always seem to get a mapping issue, where I cannot add the empty columns to the schema on the sink side. Is there a way to accomplish this:</p>

<p><img src=""https://i.stack.imgur.com/BURSM.png"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/c9duA.png"" alt=""Working flow for single JSON""></p>

<p><img src=""https://i.stack.imgur.com/DD4K7.png"" alt=""can&#39;t see input columns to build expression""></p>
","<json><csv><azure-data-factory>","2020-02-04 19:33:03","429","0","1","60068017","<p>Jason.Actually copy activity has some limitations so i'm afraid you can't add a column which does not exist in the source side.Because this is illegal in copy activity.Please see the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">error conditions</a> in copy activity:</p>

<ul>
<li>Source data store query result does not have a column name that is
specified in the input dataset ""structure"" section.</li>
<li>Sink data store (if with pre-defined schema) does not have a column
name that is specified in the output dataset ""structure"" section.</li>
<li>Either fewer columns or more columns in the ""structure"" of sink
dataset than specified in the mapping.</li>
<li>Duplicate mapping.</li>
</ul>

<p>However,your needs could be implemented in Data Flow Activity which is similar to copy activity.Please get an idea of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">Mapping Data Flow</a>.You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern"" rel=""nofollow noreferrer"">Derived Column</a> to create some empty columns which does not exist in the source data.</p>

<p><a href=""https://i.stack.imgur.com/57WrK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/57WrK.png"" alt=""enter image description here""></a></p>
"
"60058243","Azure Data Factory - Why does my ForEach block not count my stored procedure as an activity?","<p>I am trying to create the following:</p>

<ul>
<li>Retrieve file names form an Azure blob container</li>
<li>For each file name write a record into a table on my Azure SQL database</li>
</ul>

<p>What I expected would work:</p>

<p>ROOT LEVEL PIPELINE</p>

<p><a href=""https://i.stack.imgur.com/ZrMyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZrMyX.png"" alt=""Root level pipeline""></a></p>

<p>FOREACH LEVEL</p>

<p><a href=""https://i.stack.imgur.com/8ro9I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8ro9I.png"" alt=""ForEach level""></a></p>

<p>However, when I try to validate or debug this configuration ADF complains that I have no activities, even though I have already configured the stored procedure in the <code>ForEach</code> block. </p>

<p><strong>Solution</strong>: Insert a dummy SetVariable into the ForEach level. Now it recognises my stored procedure and <code>SetVariable</code> and my code runs without any issues. </p>

<p>Surely there must be a better way for my ForEach to work without having to insert a dummy SetVariable. Or am I doing something wrong?</p>

<p>Thanks in advance!</p>
","<azure><stored-procedures><foreach><azure-data-factory>","2020-02-04 13:14:29","205","0","1","60058303","<p>Seems like hitting the refresh button was not was not enough. After closing my browser and trying again it finally recognized my stored procedure. </p>
"
"60053997","Archive the file in adf with timestamp using copy activity in adf","<p>My requirement is to Copy specific file based on wild card from a container/folder in datalake to azure database using copy activity and then copy the file into a different folder with timestamp at the end of the file. </p>

<p>I used getmetadata and filter activities to get the specific file name from the datalake/blob folder to be loaded.But copy activities to database and the file movement with timestamp are failing.</p>

<p>Please find the attachment for the steps that was followed.
Can you please help. </p>

<p>Thanks</p>
","<azure-data-lake><azure-data-factory>","2020-02-04 09:11:41","998","0","1","60090758","<p>Found a solution for this. After filter activity used foreach activity and inside that used setvariable. Using this setvariable able to archive the source file with timestamp. </p>
"
"60046834","use output of azure function is azure data factory","<p>There is a possibility to use an Azure function immediatelly in Azure Data Factory and you can send parameters to the Azure function as inputs. But now I want to use the output of my Azure function immediatelly in my Azure Data Factory. Is this possible or do I need to save the result of my Azure function in a database/blob?
With the Azure function, I'm doing some manipulation on data and after that, I need to join the result of the Azure function with some data of another database. I would prefer to join both data sets directly in Azure Data Factory instead of in the Azure function for readability reasons</p>
","<azure><azure-functions><azure-data-factory>","2020-02-03 20:39:44","2101","1","1","60047394","<p>Azure Data Factory pipelines can connect with Azure Functions, in any step of the pipeline.</p>

<p>Calling an Azure Function is via a POST. The pipeline may also capture data returned from the Azure Function (as the return of the call to the Function), and may then use that data in a follow-on pipeline step.</p>

<p>If your Azure Function is producing something substantive (e.g. generating a large CSV), you might want to consider storing your Function output somewhere (like a blob) and returning the URI of this output, in your Function return. You can then work with that data in a subsequent step in your pipeline.</p>
"
"60044698","System.InvalidOperationException loading mySQL information","<p>I was toying around with the Azure Data Factory using the <a href=""https://dev.mysql.com/doc/sakila/en/"" rel=""nofollow noreferrer"">Sakila Dataset</a>. I set up a Maria DB (5.5.64) on a private centos7.7-vm. I also ran into the same issue when I was using MySQL 8 instead of MariaDB.</p>

<p>I run a parameterized load pipeline in Azure Data Factory. I repeatedly get this error inside a foreach loop in the Azure Data Factory. I get the error every time with a different source table. </p>

<p>Error from Azure Data Factory: </p>

<pre><code>{     
   “errorCode”: “2100”,
   “message”: “’Type=System.InvalidOperationException,Message=Collection was modified; enumeration operation may not execute.,Source=mscorlib,’”,
   “failureType”: “UserError”,
   “target”: “GET MAX MySQL”, 
   “details”: [] 
}
</code></pre>

<p>Parameterized query running in the lookup activity:</p>

<pre><code>SELECT MAX(@{item().WatermarkColumn}) as maxd FROM @{item().SRC_tab}
</code></pre>

<p>becomes</p>

<pre><code>SELECT MAX(last_update) as maxd FROM sakila.actor
</code></pre>

<p>Please note that the error appeared the last time in the staff and the category table, I was using the MariaDB connector. After I switched to the MySQL connector, the error disappeared. However in the past when I used the MySQL connector, and switched to the MariaDB connector the error also persisted. </p>

<p>Have any of you experienced a similar behaviour? If yes, what were your workarounds? </p>
","<mysql><azure><azure-sql-database><azure-data-factory>","2020-02-03 17:59:01","93","0","1","60104014","<p>Apologizes , but we need more clarity here . As I understand is this issue still with and MariaDB connection and MySQL or only with MySQL ?</p>

<p>Just to let you know ADF team regularly deploys changes and it may happen that the issues which you experienced and is not repro-able at this time , a fix may have been deployed for that . </p>
"
"60040909","Azure Data Factory :Get MetaData: Can filter the meta data by Folder Type and Pass the Folder Names to a For Loop","<p>Problem: Can we get the only the folder Names from a blob using meta data or any other activity. <a href=""https://i.stack.imgur.com/1BzZg.png"" rel=""nofollow noreferrer"">Example</a></p>

<p>I am trying to get the folder names and trying to use a filter with item type equals 'Folder', but the values include all the files present in the blob. <a href=""https://i.stack.imgur.com/vPDh4.png"" rel=""nofollow noreferrer"">Example</a></p>
","<azure><azure-data-factory>","2020-02-03 14:09:28","3882","1","1","60043184","<p>You can use a Filter activity after the Get Metadata.  Here is the JSON for one that filters to only files. 
 Change the filter below to just get files with this line: ""value"": ""@equals(item().type, 'Folder')"",</p>

<pre><code>    {
        ""name"": ""FilterFiles"",
        ""description"": ""Only files will be selected, the sub-folders will not be selected."",
        ""type"": ""Filter"",
        ""dependsOn"": [
            {
                ""activity"": ""GetFileList"",
                ""dependencyConditions"": [
                    ""Succeeded""
                ]
            }
        ],
        ""userProperties"": [],
        ""typeProperties"": {
            ""items"": {
                ""value"": ""@activity('GetFileList').output.childItems"",
                ""type"": ""Expression""
            },
            ""condition"": {
                ""value"": ""@equals(item().type, 'File')"",
                ""type"": ""Expression""
            }
        }
    }
</code></pre>
"
"60036761","Azure Data Factory complex JSON source (nested arrays) to Azure Sql Database?","<p>I have a JSON source document that will be uploaded to Azure blob storage regularly. The customer wants to have this input written to Azure Sql Database using Azure Data Factory. The JSON is however complex with many nested arrays and so  far I have not be able to find a way to flatten the document. Perhaps this is not supported/possible?</p>

<pre><code>[
{
""ActivityId"": 1,
    ""Header"": {},
    ""Body"": [{
        ""1stSubArray"": [{
            ""Id"": 456,
            ""2ndSubArray"": [{
                ""Id"": ""abc"",
                ""Descript"": ""text"",
                ""3rdSubArray"": [{
                    ""Id"": ""def"",
                    ""morefields"": ""text""
                },
                {
                    ""Id"": ""ghi"",
                    ""morefields"": ""sample""
                }]
            }]
        }]
    }]
}
]
</code></pre>

<p>I need to flatten it:</p>

<pre><code>ActivityId, Id, Id, Descript, Id, morefields
1, 456, abc, text1, def, text
1, 456, abc, text2, ghi, sample
1, 456, xyz, text3, jkl, textother
1, 456, xyz, text4, mno, moretext 
</code></pre>

<p>There could be 8+ flat records per ActivityId. Anyone out there that has seen this and found a way to resolve using Azure Data Factory Copy Data?</p>
","<azure><azure-data-factory>","2020-02-03 09:53:09","11803","4","2","60089772","<p>In the past,you could follow this <a href=""https://medium.com/@gary.strange/flattening-json-in-azure-data-factory-2f2130794258"" rel=""nofollow noreferrer"">blog</a> and my previous case:<a href=""https://stackoverflow.com/questions/57553533/loosing-data-from-source-to-sink-in-copy-data/57686714#57686714"">Loosing data from Source to Sink in Copy Data</a> to set <code>Cross-apply nested JSON array</code> option in Blob Storage Dataset. However,it disappears now.</p>
<p>Instead,<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#explicit-mapping"" rel=""nofollow noreferrer"">Collection Reference</a> is applied for array items schema mapping in copy activity.</p>
<p><a href=""https://i.stack.imgur.com/8sfRa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8sfRa.png"" alt=""enter image description here"" /></a></p>
<p>But based on my test,only one array can be flattened in a schema. Multiple arrays can be referenced—returned as one row containing all of the elements in the array. However, only one array can have each of its elements returned as individual rows. This is the current limitation with <code>jsonPath settings</code>.</p>
<p><a href=""https://i.stack.imgur.com/ZmXse.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZmXse.png"" alt=""enter image description here"" /></a></p>
<p>As workaround,you can first convert json file with nested objects into CSV file using Logic App and then you can use the CSV file as input for Azure Data factory. Please refer this <a href=""https://adatis.co.uk/converting-json-with-nested-arrays-into-csv-in-azure-logic-apps-by-using-array-variable/"" rel=""nofollow noreferrer"">doc</a> to understand how Logic App can be used to convert nested objects in json file to CSV. Surely,you could also make some efforts on the sql database side,such as SP which is mentioned in the comment by @GregGalloway.</p>
<hr />
<p>Just for summary,unfortunately,the &quot;Collection reference&quot; only works for one level down in the array structure which is not suitable for @Emrikol. Finally,@Emrikol  abandoned Data Factory and has built an app to the work.</p>
"
"60036761","Azure Data Factory complex JSON source (nested arrays) to Azure Sql Database?","<p>I have a JSON source document that will be uploaded to Azure blob storage regularly. The customer wants to have this input written to Azure Sql Database using Azure Data Factory. The JSON is however complex with many nested arrays and so  far I have not be able to find a way to flatten the document. Perhaps this is not supported/possible?</p>

<pre><code>[
{
""ActivityId"": 1,
    ""Header"": {},
    ""Body"": [{
        ""1stSubArray"": [{
            ""Id"": 456,
            ""2ndSubArray"": [{
                ""Id"": ""abc"",
                ""Descript"": ""text"",
                ""3rdSubArray"": [{
                    ""Id"": ""def"",
                    ""morefields"": ""text""
                },
                {
                    ""Id"": ""ghi"",
                    ""morefields"": ""sample""
                }]
            }]
        }]
    }]
}
]
</code></pre>

<p>I need to flatten it:</p>

<pre><code>ActivityId, Id, Id, Descript, Id, morefields
1, 456, abc, text1, def, text
1, 456, abc, text2, ghi, sample
1, 456, xyz, text3, jkl, textother
1, 456, xyz, text4, mno, moretext 
</code></pre>

<p>There could be 8+ flat records per ActivityId. Anyone out there that has seen this and found a way to resolve using Azure Data Factory Copy Data?</p>
","<azure><azure-data-factory>","2020-02-03 09:53:09","11803","4","2","60175144","<p>Azure SQL Database has some capable JSON shredding abilities including <a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/openjson-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">OPENJSON</a> which shreds JSON, and <a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/json-value-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">JSON_VALUE</a> which returns scalar values from JSON.  Being as you already have Azure SQL DB in your architecture, it would make sense to use it rather than add additional components.</p>

<p>So why not adopt an ELT pattern where you use Data Factory to insert the JSON into a table in Azure SQL DB and then call a stored procedure task to shred it?  Some sample SQL based on your example:</p>

<pre><code>DECLARE @json NVARCHAR(MAX) = '[
{
  ""ActivityId"": 1,
  ""Header"": {},
  ""Body"": [
    {
      ""1stSubArray"": [
        {
          ""Id"": 456,
          ""2ndSubArray"": [
            {
              ""Id"": ""abc"",
              ""Descript"": ""text"",
              ""3rdSubArray"": [
                {
                  ""Id"": ""def"",
                  ""morefields"": ""text""
                },
                {
                  ""Id"": ""ghi"",
                  ""morefields"": ""sample""
                }
              ]
            },
            {
              ""Id"": ""xyz"",
              ""Descript"": ""text"",
              ""3rdSubArray"": [
                {
                  ""Id"": ""jkl"",
                  ""morefields"": ""textother""
                },
                {
                  ""Id"": ""mno"",
                  ""morefields"": ""moretext""
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}
]'

--SELECT @json j

-- INSERT INTO yourTable ( ...
SELECT
    JSON_VALUE ( j.[value], '$.ActivityId' ) AS ActivityId,
    JSON_VALUE ( a1.[value], '$.Id' ) AS Id1,
    JSON_VALUE ( a2.[value], '$.Id' ) AS Id2,
    JSON_VALUE ( a2.[value], '$.Descript' ) AS Descript,
    JSON_VALUE ( a3.[value], '$.Id' ) AS Id3,
    JSON_VALUE ( a3.[value], '$.morefields' ) AS morefields

FROM OPENJSON( @json ) j
    CROSS APPLY OPENJSON ( j.[value], '$.""Body""' ) AS m
        CROSS APPLY OPENJSON ( m.[value], '$.""1stSubArray""' ) AS a1
            CROSS APPLY OPENJSON ( a1.[value], '$.""2ndSubArray""' ) AS a2
                CROSS APPLY OPENJSON ( a2.[value], '$.""3rdSubArray""' ) AS a3;
</code></pre>

<p>As you can see, I've used <code>CROSS APPLY</code> to navigate multiple levels.  My results:</p>

<p><a href=""https://i.stack.imgur.com/Ednpy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ednpy.png"" alt=""My results""></a></p>
"
"60033172","Is there a way to reuse a single running databricks cluster in multiple mapping data flows","<p>Is there a way to reuse a databricks cluster that is started by a web activity before 
we run the mapping data flows and use the same running cluster in all of the data flows instead of letting all the data flow instances spin up their 
own clusters which takes around 6 minutes for setting up each cluster?</p>
","<azure-data-factory><azure-databricks>","2020-02-03 04:40:45","1407","4","1","60034382","<p>Yes. Set the TTL in the Azure Integration Runtime under ""Data Flow Properties"" to an amount of time that there is a gap in between data flow job executions. This way, we can set-up a VM pool for you and reuse those resource to minimize the cluster start-up time: <a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/adf-adds-ttl-to-azure-ir-to-reduce-data-flow-activity-times/ba-p/878380"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/adf-adds-ttl-to-azure-ir-to-reduce-data-flow-activity-times/ba-p/878380</a>.</p>

<p>To start the cluster, don't use a web activity. Use a ""dummy"" data flow as I demonstrate here: <a href=""https://youtu.be/FFCbU4ujCiY?t=533"" rel=""nofollow noreferrer"">https://youtu.be/FFCbU4ujCiY?t=533</a>.</p>

<p>In ADF, you cannot access the underlying compute engines (Databricks in this case), so you have to kick-off a dummy data flow to warm it up.</p>

<p>That cluster start-up will take 5-6 mins. But now, if you use that same Azure IR in your subsequent activities, as long as they are scheduled to execute within that TTL window, ADF can grab existing VM resources to spin-up the Spark clusters and marshall your data flow definition to the Spark job execution.</p>

<p>End-to-end that process should now take just 2 mins.</p>
"
"60018956","Azure Data Factory - Error on Delete Activity","<p>I am creating a test Azure Data Factory Pipeline for learning purposes.</p>

<p>For this specific pipeline, I want to move files from one blob to the other. In ADF world, this involves creating a pipeline with the following activities:</p>

<ol>
<li>Get Metadata: retrieve file list from Blob 1</li>
<li>Filter: from the output of previous activity, filter out folders, and output list of files</li>
<li>Move files: get output from previous activity, move files from Blob 1 to Blob 2</li>
</ol>

<p>ADF does not implement a Move File activity, so activity 2. above is actually a ForEach activity composed by two sub-activities:</p>

<p>2.1 Copy Files: Copy file from Blob 1 to Blob 2
2.2 Delete Files:Delete file from Blob 1</p>

<p>When debugging this pipeline, I am getting the following error for activity 2.2:</p>

<blockquote>
  <p>Failed to execute delete activity with data source 'AzureBlobStorage'
  and error 'The required Blob is missing. Folder path:
  sensor-sink-aws/test - Copy.json/.'</p>
</blockquote>

<p>But the file exists in the container:</p>

<p><a href=""https://i.stack.imgur.com/jG1oH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jG1oH.png"" alt=""enter image description here""></a></p>

<p>In my DeleteFile activity, I have the following data for Source:</p>

<p><a href=""https://i.stack.imgur.com/jPGfI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jPGfI.png"" alt=""enter image description here""></a></p>

<p>Which basically references the Source Dataset that I created for this pipeline - it is a reusable dataset, meaning that I'm able to pass the container name and file name in a dynamic way.</p>

<p>Any idea on what could be going wrong?</p>
","<azure><etl><azure-data-factory>","2020-02-01 16:37:55","3362","0","1","60105010","<p>From the path in the  error it looks that the path of the blob is not correct ""sensor-sink-aws/test - Copy.json/.'"" </p>

<p>The file name should had been at the last , but as you can see that it ""/."" . I think you should check Delete activity -> Source ->Dataset -> Open-> Connection -> It should be some like the below screenshot ( please pay  attention that the file name is at the end , my guess is you have put that in the middle text box ) </p>

<p><a href=""https://i.stack.imgur.com/G3ZTS.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G3ZTS.gif"" alt=""enter image description here""></a></p>
"
"60017021","How to start databricks cluster using web activity in azure data factory?","<p>When I run the Web activity, it redirects this to login page and I get below as response from the web activity .. Suggestions please?</p>

<p>POST URL  : <a href=""https://abc.azuredatabricks.net/2.0/clusters/start"" rel=""nofollow noreferrer"">https://abc.azuredatabricks.net/2.0/clusters/start</a></p>

<p>Body : {""cluster_id"":""1234-567890-stoke123""}</p>

<p><em>{
    ""Response"": ""\n\n\n\n    \n    \n    Databricks - Sign In\n    \n    \n    \n\n    \n\n\n\n\n\n"",
    ""ADFWebActivityResponseHeaders"": {
        ""x-databricks-reason-phrase"": ""OK"",
        ""vary"": ""Accept-Encoding;User-Agent"",
        ""strict-transport-security"": ""max-age=31536000; includeSubDomains; preload"",
        ""x-content-type-options"": ""nosniff"",
        ""Cache-Control"": ""no-store, must-revalidate, no-cache"",
        ""Date"": ""Sat, 01 Feb 2020 10:00:44 GMT"",
        ""Server"": ""databricks""
    },
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (North Central US)"",
    ""executionDuration"": 0,
    ""durationInQueue"": {
        ""integrationRuntimeQueue"": 0
    },
    ""billingReference"": {
        ""activityType"": ""ExternalActivity"",
        ""billableDuration"": {
            ""Managed"": 0.016666666666666666
        }
    }
}</em></p>
","<azure-data-factory><azure-databricks>","2020-02-01 12:50:02","772","0","1","60040468","<p>You need to pass the bearer token in the header: </p>

<pre><code>Authorization = Bearer yourtoken
</code></pre>

<p>Where ""Authorization"" is the key and ""Bearer dapi1234567890"" is the value.
See this article for how to generate a new token:</p>

<p><a href=""https://docs.databricks.com/dev-tools/api/latest/authentication.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/api/latest/authentication.html</a></p>
"
"60011842","What triggers a Retry when using a Copy data activity whose source is a REST service","<p>I have a pipeline that includes a Copy data activity whose source is a REST service. The service sometimes times out - about 1 out of 10 times. I would like to have the activity retry the call but I don't know what triggers a retry. Does the service just need to return a 'success' key with a value of 'false'? </p>

<p>How does this affect the pagination rules? Does the activity start over from the first page? or does the activity use the current QueryParameter value?</p>
","<azure-data-factory>","2020-01-31 22:03:11","814","2","1","60042998","<p>The 'Retry' and 'Retry Interval' parameters are properties of the Activity object and have nothing to do with other properties of the given activity, like the source property as referenced by my question. In this case, my source was returning a failure and the activity would retry up to the number specified by the 'Retry' parameter. In my case, I had the Retry parameter set to 1 and when I drilled down to the activity under the Monitor tab, I saw the activity retry after the Retry Interval.</p>

<p>The Pagination Rules have nothing to do with the Retry; pagination is a function of the REST service. When the activity fails, the Retry function will effectively start over from the 1st page.</p>
"
"60005056","How to merge multiple Activitiy into one in Azure Data Factory","<p>Is there a way to merge all activity into a single activity ? 
For e.g: I am having two pipeline (A &amp; B) and both pipeline contain 10 - 12 activity each.
Now the problem is need to create dependency in between those two pipeline.
Once the first pipeline is successfully executed then only my second pipeline should trigger, and i didn't want those two pipeline should present in two different window.</p>

<p>If I am creating in single window then it looks complex, so i am planning to keep all activity of pipeline A in one box and all activity in pipeline B in another box.</p>

<p>So that in a single window i can see only 2 box A and B.</p>

<p>Can anyone know how to achieve this.</p>

<p>Please note: Here Box means activity which are available in Azure Data Factory.</p>
","<azure-data-factory>","2020-01-31 13:41:25","636","0","1","60035753","<p>Haven't herd any way to merge all actives into a single activity.</p>

<p>You purpose when the pipeline A runs succeed, then the pipeline b running.</p>

<p>I have some suggestions that:</p>

<ol>
<li>Create a new pipeline,  using two <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">Execute Pipeline activity</a> to run pipeline A and B. 
<a href=""https://i.stack.imgur.com/Vbnrc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vbnrc.png"" alt=""enter image description here""></a></li>
</ol>

<p>This is similar with you want ""in a single window i can see only 2 box A and B""</p>

<p>For example, run the new pipeline, you can find that execute pipeline 3 is running after pipeline 2 is finished. </p>

<p><a href=""https://i.stack.imgur.com/wooGy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wooGy.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"59997456","Manual Azure Backup Cosmos DB","<p>Tried to export data in CosmosDB but it was not successful. According to <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/storage-explorer"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/storage-explorer</a>, by using this tool I can export the data inside the cosmosdb, but no option to export. Tried to do the instructions here <a href=""https://azure.microsoft.com/en-us/updates/documentdb-data-migration-tool/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/documentdb-data-migration-tool/</a> and <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#JSON"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#JSON</a>, but error is being encountered.</p>

<p>Can you help me how to do this in <strong>Data Factory</strong> or <strong>any steps just to manual backup cosmos DB</strong>?</p>

<p>i tried doing the backup through azure data factory but data factory can't seem to connect to cosmos db, it's so weird 'cause the primary string/secondary string that I used is in the details of the cosmos db</p>

<p>Thank you.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-01-31 03:22:15","673","-1","3","59999450","<blockquote>
  <p>Can you help me how to do this in Data Factory</p>
</blockquote>

<p>According to your description,it seems you have trouble with export data,not import data. You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Copy activity</a> in ADF which supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db"" rel=""nofollow noreferrer"">cosmos db connector</a>.For you needs,cosmos db is source dataset and please add one more sink dataset(destination). Such as some json files in the blob storage.Just make sure you configure right authentication information with your cosmos db account.</p>

<p>ADF is more suitable for the batch back up or daily back up.</p>

<blockquote>
  <p>or any steps just to manual backup cosmos DB</p>
</blockquote>

<p>Yes,Storage Explorer is not for exporting data from cosmos db,Data migration tool is the suitable option.Please install the tool and refer to some details from this link:<a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#export-to-json-file"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#export-to-json-file</a></p>

<p>DMT is more suitable for single back up.Surely,it also supports execution in the batch if you use command line to execute it.</p>
"
"59997456","Manual Azure Backup Cosmos DB","<p>Tried to export data in CosmosDB but it was not successful. According to <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/storage-explorer"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/storage-explorer</a>, by using this tool I can export the data inside the cosmosdb, but no option to export. Tried to do the instructions here <a href=""https://azure.microsoft.com/en-us/updates/documentdb-data-migration-tool/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/documentdb-data-migration-tool/</a> and <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#JSON"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#JSON</a>, but error is being encountered.</p>

<p>Can you help me how to do this in <strong>Data Factory</strong> or <strong>any steps just to manual backup cosmos DB</strong>?</p>

<p>i tried doing the backup through azure data factory but data factory can't seem to connect to cosmos db, it's so weird 'cause the primary string/secondary string that I used is in the details of the cosmos db</p>

<p>Thank you.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-01-31 03:22:15","673","-1","3","59999773","<p>Cosmos DB Data Migration tool can be used to export data from Cosmos DB. </p>

<p>Refer <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#export-to-json-file"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#export-to-json-file</a></p>
"
"59997456","Manual Azure Backup Cosmos DB","<p>Tried to export data in CosmosDB but it was not successful. According to <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/storage-explorer"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/storage-explorer</a>, by using this tool I can export the data inside the cosmosdb, but no option to export. Tried to do the instructions here <a href=""https://azure.microsoft.com/en-us/updates/documentdb-data-migration-tool/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/documentdb-data-migration-tool/</a> and <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#JSON"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/import-data#JSON</a>, but error is being encountered.</p>

<p>Can you help me how to do this in <strong>Data Factory</strong> or <strong>any steps just to manual backup cosmos DB</strong>?</p>

<p>i tried doing the backup through azure data factory but data factory can't seem to connect to cosmos db, it's so weird 'cause the primary string/secondary string that I used is in the details of the cosmos db</p>

<p>Thank you.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-01-31 03:22:15","673","-1","3","60929150","<p>this one worked for me... since my SSL in my Macbook did not work, I did these steps from the Azure VM that I created.</p>

<p>Steps:
Download MongoDB Community Server Client tool as per your OS version and MongoDB compatible version.
(Or you can download [v3.2.22 for Windows X64] directly at here, please don’t download the version beyond 4.2 as it’s incompatible)</p>

<p>After installing the MongoDB client tools, go to the installation directory -> go to the subfolder “bin” containing the mongoexport.exe, then issue below command to export your data:
mongoexport --host=: -u= -p= --db=  --collection= --ssl --sslAllowInvalidCertificates --out=</p>

<p>Note 1: You can find the , ,  and  in Cosmos DB Portal – “Connection String” </p>
"
"59997308","How to migrate datalake gen1 datasets to datalake gen2 in azure data factory?","<p>Recently for our solution we are requested to upgrade to datalake gen 2 from gen 1
However i am able to load the folders and files from gen 1 to gen2 using azure data factory but in existing data factory we have multiple datasets associated with datalake gen1.</p>

<p>Can anyone suggest me how to change datasets from gen 1 to gen 2 in azure data factory? </p>
","<azure><azure-data-lake><azure-data-factory><azure-data-lake-gen2>","2020-01-31 02:59:57","351","1","1","59998159","<p>You can be able to load the folders and files from gen 1 to gen2 using azure data factory.</p>

<p>In Data Factory, you can change the exist datasets like this:</p>

<p>Data Factory-->Author-->Dataset-->Choose the exist dataset-->New:</p>

<p><a href=""https://i.stack.imgur.com/qwppH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qwppH.png"" alt=""enter image description here""></a> </p>

<p>New linked service:
<a href=""https://i.stack.imgur.com/JB14E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JB14E.png"" alt=""enter image description here""></a></p>

<p>Then the dataset will changed linked to Azure Data Lake Gen 2, just choose the file you have loaded from Gen 1.</p>

<p>Don't forget Publish the changes.</p>

<p><a href=""https://i.stack.imgur.com/abScQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/abScQ.png"" alt=""enter image description here""></a></p>

<p>Some limits are that you must edit the multiple datasets one by one for now.</p>

<p>Hope this helps.</p>
"
"59991635","Copy files out of Azure Data Lake Storage","<p>I've setup pipeline in Azure Data Factory to
1) Copy files from Storage to Lake
2) u-sql to merge / process the copied files and output to single file
3) open &amp; process this merged file (insert to DB).</p>

<p>Whatever I try, permissions wise, step 3 fails.  All demos and tutorials for Azure data lake stop at producing the output file claiming success . job done etc.. </p>

<p>I'm finding the docs.microsoft on this quite convoluted (it could be due to Gen1/Gen2 Lake??).  Surely, what I'm trying to do is a common scenario, take some data files, merge and output , process the output.</p>

<p>It seems that the file created by the u-sql process has different owner from other files, so the most common error is a 403.  When setting up the pipelines in ADF, I can browse to the folders on lake storage etc to configure, but can't open the file without setting all permissions on file in Lake storage.  When I debug / run the pipeline in ADF, the new file output doesn't have these permissions so the process output file step in the pipeline fails.</p>

<p>All of these resources are setup in same azure subscription.</p>
","<azure><azure-data-factory><azure-data-lake>","2020-01-30 17:42:48","74","0","1","60002311","<p>I've sorted this permissions issue now.
If anyone is interested, some information &amp; guidance here...
<a href=""https://www.sqlservercentral.com/stairways/stairway-to-u-sql"" rel=""nofollow noreferrer"">https://www.sqlservercentral.com/stairways/stairway-to-u-sql</a></p>

<p>also, this course is quite good introduction.</p>

<p><a href=""https://app.pluralsight.com/library/courses/u-sql-azure-data-lake/"" rel=""nofollow noreferrer"">https://app.pluralsight.com/library/courses/u-sql-azure-data-lake/</a></p>
"
"59990627","Run lot of piplines in the same time","<p>There is a solution if i want to run those piplines in the same time instead of doing it for each pipline<a href=""https://i.stack.imgur.com/W03Hr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W03Hr.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-01-30 16:41:54","67","0","2","59996964","<p>Just add a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#triggers"" rel=""nofollow noreferrer"">trigger</a> at same time for all of your pipelines.</p>

<p>In the ADF portal:</p>

<p><a href=""https://i.stack.imgur.com/dCmUf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dCmUf.png"" alt=""enter image description here""></a></p>

<p>Set the same time for trigger configuration:</p>

<p><a href=""https://i.stack.imgur.com/2vELF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2vELF.png"" alt=""enter image description here""></a></p>

<p>If you want to execute them in the queue,you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">execute pipeline activity</a> which allows you to invoke another pipeline.</p>
"
"59990627","Run lot of piplines in the same time","<p>There is a solution if i want to run those piplines in the same time instead of doing it for each pipline<a href=""https://i.stack.imgur.com/W03Hr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W03Hr.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2020-01-30 16:41:54","67","0","2","60352987","<p>You could also leverage the lookup activity to lookup the pipeline using meta data or a pipeline parameter table and then use the set the for each loop to parallel processing so that it will process upto 50 pipelines at once.</p>

<p>See ForEach Article for more info: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#parallel-execution"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#parallel-execution</a></p>
"
"59983655","Azure service principal name for Linked services","<p>I have few linked services used to copy data from on premise server to Azure BLOB and then loading that data into Azure DB using ADF pipeline, these linked services are using a user account to authenticate, I want to replace this user account with a Service Principal name. Is it possible? If yes then how.</p>

<p>Thanks.</p>
","<azure><azure-data-factory>","2020-01-30 10:15:38","322","0","1","59997206","<p>Yes,it is possible.I checked <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#service-principal-authentication"" rel=""nofollow noreferrer"">blob storage linked service</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#service-principal-authentication"" rel=""nofollow noreferrer"">azure sql db linked service</a> documents,then all support service principal authenticate ways. You could choose that way when you create linked services:</p>

<p><a href=""https://i.stack.imgur.com/GOyr9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GOyr9.png"" alt=""enter image description here""></a></p>
"
"59981124","How to copy files from multiple source folders to target folders in azure data lake storage gen 2","<p>I have multiple folders in ADLS and each folder have delta files in it. 
My goal is to convert all the delta files into parquet format, and place it in a different folder. And I want to maintain same folder names in target as in source. </p>

<p>I have written a script which takes in folder name as parameter, then converts all delta files in that folder to parquet and move to target directory with same name as source folder. Below is snippet of code. </p>

<pre><code>var loadDelta = spark.read.format(""delta"").load(deltaPath)
loadDelta.write.format(""parquet"").mode(SaveMode.Overwrite).save(parquetPath)
</code></pre>

<p>Here I have to define deltaPath(Source folder) and parquetPath(Target folder) for each folder...which is counter-productive if I have to scale it up for 100s of folders. I was wondering if there is a way that files of all the folders present in Source directory can be converted using the code and place in target directory in similar folder structure. </p>

<p>E.g.
This is the Source directory and it have 4 folder/tables. Each folder have delta files in it.
<a href=""https://i.stack.imgur.com/o8SO0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o8SO0.png"" alt=""SourceDirectory""></a></p>

<p>Aim is to create same 4 folder names in test directory(target) without providing any specific folder name as input, and use the conversion code to convert files to parquet format. </p>

<p><a href=""https://i.stack.imgur.com/gAem9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gAem9.png"" alt=""DesiredTargetDirectory""></a></p>

<p>How can the conversion process be performed recursively. Any leads/suggestions would be appreciated. </p>
","<azure><azure-data-factory><databricks><azure-data-lake><azure-databricks>","2020-01-30 07:39:49","568","1","1","59982302","<p>You can create a DataFactory pipeline with copyData activities. 
You have just to define your data folders target in Source input (in copyData activity configs) and your target data folders in Sink with a Parquet format  (in copyData activity configs).</p>
"
"59970778","mapping Date in AZure Data Factory","<p>Im working with Data actory this time this why i ask lot of question about that</p>

<p>My new problem is that my SOURCE(CSV file contains a column DeleveryDate  full of Date dd/MM/YYYY) and my table SQl where i specify DElevry date as DateTime but when I map btw source and sink in Data preview source
<a href=""https://i.stack.imgur.com/ZBfRO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZBfRO.png"" alt=""enter image description here""></a></p>

<p>duplicate columns like in the picture below but in data preview sink the columns always NULL the same in my table NULL.</p>

<p><a href=""https://i.stack.imgur.com/vNw2d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vNw2d.png"" alt=""enter image description here""></a>
Thanks</p>
","<sql><azure-data-factory>","2020-01-29 15:50:28","4209","1","2","59978642","<p>You said column DeleveryDate full of Date dd/MM/YYYY), can you tell me why the column DeleveryDate has the values like '3', '1' in your screenshot? String '3' or '1' are not the date string with format <code>dd/MM/YYYY</code>.</p>

<p>If you want to do some data convert in Data Factory, I still suggest your to learn more about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data Flow</a>.</p>

<p>For now, we can not convert date format from <code>dd/MM/YYYY</code> to datetime <code>yyyy-MM-dd HH:mm:ss.SSS</code> directly, we must do some other converts. </p>

<p>Look at bellow, I have a csv file contained a column with date format <code>dd/MM/YYYY</code> string, I still using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">DerivedColumn</a> this time:</p>

<p><a href=""https://i.stack.imgur.com/BoPgu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BoPgu.png"" alt=""enter image description here""></a></p>

<p><strong>Add DerivedColumn</strong>:</p>

<p><a href=""https://i.stack.imgur.com/XW7YB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XW7YB.png"" alt=""enter image description here""></a></p>

<p>Firstly, using this bellow expression to substring and convert <code>dd/MM/YYYY</code> to <code>YYYY-MM-dd</code>:</p>

<pre><code>substring(Column_2, 7, 4)+'-'+substring(Column_2, 4, 2)+'-'+substring(Column_2, 1,2)
</code></pre>

<p><a href=""https://i.stack.imgur.com/xtynH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xtynH.png"" alt=""enter image description here""></a></p>

<p>Then using <code>toTimestamp()</code> to convert it:</p>

<pre><code>toTimestamp(substring(Column_2, 7, 4)+'-'+substring(Column_2, 4, 2)+'-'+substring(Column_2, 1,2), 'yyyy-MM-dd')
</code></pre>

<p><a href=""https://i.stack.imgur.com/SZ3fD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SZ3fD.png"" alt=""enter image description here""></a></p>

<p><strong>Sink settings and preview</strong>：</p>

<p>My Sink table column tt data type is datetime:</p>

<p><a href=""https://i.stack.imgur.com/RHjjF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RHjjF.png"" alt=""enter image description here""></a></p>

<p><strong>Execute the pipeline:</strong> </p>

<p><a href=""https://i.stack.imgur.com/wwBjS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wwBjS.png"" alt=""enter image description here""></a></p>

<p><strong>Check the data in sink table:</strong></p>

<p><a href=""https://i.stack.imgur.com/v2KTO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v2KTO.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"59970778","mapping Date in AZure Data Factory","<p>Im working with Data actory this time this why i ask lot of question about that</p>

<p>My new problem is that my SOURCE(CSV file contains a column DeleveryDate  full of Date dd/MM/YYYY) and my table SQl where i specify DElevry date as DateTime but when I map btw source and sink in Data preview source
<a href=""https://i.stack.imgur.com/ZBfRO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZBfRO.png"" alt=""enter image description here""></a></p>

<p>duplicate columns like in the picture below but in data preview sink the columns always NULL the same in my table NULL.</p>

<p><a href=""https://i.stack.imgur.com/vNw2d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vNw2d.png"" alt=""enter image description here""></a>
Thanks</p>
","<sql><azure-data-factory>","2020-01-29 15:50:28","4209","1","2","67966125","<p>Please try this-
This is a trick which was a blocker for me, but try this-</p>
<ol>
<li>Go to sink</li>
<li>Mapping</li>
<li>Click on output format</li>
<li>Select the data format or time format you prefer to store the data into the sink.
<a href=""https://i.stack.imgur.com/Vkasq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vkasq.png"" alt=""enter image description here"" /></a></li>
</ol>
"
"59956537","I can't do my mapping In Azure data facotry with NULL values","<p>I have a problem with azure data factory
So they told me to do a mapping (i think that it will be a simple mapping) btw sql database tempory tables and a file source so my problem is that there is some colums( NO NULL) so i can't do the mapping if the colum doesn't exist in my source file or it exsist but NULL</p>

<p>below a table from my BD, for exmple  COmpanyTypeCode is empty in my file! any help plz? maybe i can use something to skip NULL 
thnks</p>
","<sql><azure-data-factory>","2020-01-28 20:28:39","2195","0","2","59959660","<p>Yes, you can ship the null values of the source file.</p>

<p>But the solution is that you must specify a not null value to replace the empty <code>COmpanyTypeCode</code> column.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data Flow</a>  <code>[DerIvedCloumn][2]</code> can help you do that. For example:</p>

<p>Here's an empty 'null' column2 of my source dataset:</p>

<p><a href=""https://i.stack.imgur.com/ROlRY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ROlRY.png"" alt=""enter image description here""></a></p>

<p>DerivedColumn expression:</p>

<p><a href=""https://i.stack.imgur.com/itEAU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/itEAU.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/71rcZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/71rcZ.png"" alt=""enter image description here""></a></p>

<p>Then mapping the columns in the Sink settings.</p>

<p>Hope this helps.</p>
"
"59956537","I can't do my mapping In Azure data facotry with NULL values","<p>I have a problem with azure data factory
So they told me to do a mapping (i think that it will be a simple mapping) btw sql database tempory tables and a file source so my problem is that there is some colums( NO NULL) so i can't do the mapping if the colum doesn't exist in my source file or it exsist but NULL</p>

<p>below a table from my BD, for exmple  COmpanyTypeCode is empty in my file! any help plz? maybe i can use something to skip NULL 
thnks</p>
","<sql><azure-data-factory>","2020-01-28 20:28:39","2195","0","2","73379411","<p>If you have Ident flds like [FLD_ID] [int] IDENTITY(1,1) NOT NULL, they will cause this issue.  You can't simply delete the mapping.</p>
"
"59951429","how to load csv file data into table storage of azure datalake using data factory","<p>how to load csv file data into table storage of azure data-lake using data factory.</p>

<p>Lets say i have a file with few columns and few rows and i want to upload this data into azure table storage of azure data-lake gen-2 using data factory.</p>

<p>I used copy activity of azure data factory and a file in the container of azure data-lake as source but for sink i couldn't see the option of selecting azure data-lake table storage.</p>

<p>Can you please let me know if i am missing anything here.</p>
","<azure-table-storage><azure-data-factory><azure-data-lake-gen2>","2020-01-28 14:54:47","128","0","1","59960665","<p>On the sink side you need to look for sink dataset as  ""Azure Table Storage"" and then you can navigate that from the UI itself .</p>
"
"59941770","Azure Datalake on-premise or hybrid stack","<p>We are trying to evaluate a good fit for our solution. We want to process big-data, for that we want build the solution around Hadoop stack. We wanted to know how azure can help in these situations. The solution we are building is a SAAS. But some of our clients have confidential data which they want to hold only in their premise.</p>

<ol>
<li>So can we run azure data lake on premise for those clients?</li>
<li>Can we have a hybrid model where storage used will be on premise but the processing done will be on cloud.</li>
</ol>

<p>The reason we are asking this is to answer the question of scalability and reliability.</p>

<p>I know this is vague but if you need more clarification please let us know.</p>
","<azure><azure-data-factory><azure-data-lake><azure-databricks><azure-stack>","2020-01-28 03:27:59","911","1","1","59945108","<p>Azure Data Lake (gen2) Hierarchical Filesystem support in Azure Stack would enable you to use it natively for your storage requirements. Unfortunately currently Azure Stack does not support Azure Data Lake. </p>

<p>You can find the list of available services here:  <a href=""https://azure.microsoft.com/en-gb/overview/azure-stack/keyfeatures/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-gb/overview/azure-stack/keyfeatures/</a>. Some of the big data ecosystem Azure tools are in development but not yet generally available. </p>

<p>There is a feature request to add this support. You can vote this up to help Microsoft prioritize this. <a href=""https://feedback.azure.com/forums/344565-azure-stack/suggestions/38222872-support-adls-gen2-on-azure-stack"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/344565-azure-stack/suggestions/38222872-support-adls-gen2-on-azure-stack</a></p>
"
"59938374","Converting XML files to JSON or CSV?","<p>I have complex XML files with nested elements. I built a process to handle using SSIS and T-SQL. We utilize Azure Data Factory and I'd like to explore converting XML files to JSON or CSV, since those are supported by ADF and XML is not. </p>

<p>It appears logic apps is one option. Has anyone had other luck with taking XML and converting within a pipeline?</p>

<p>Current Workflow:
pick up XML files from folder, drop to on network drives, bulk insert XML into a staging row, parse XML to various SQL tables for analysis.</p>

<p>Sample:</p>

<pre><code>&lt;HEADER&gt;
&lt;SurveyID&gt; 1234 &lt;/SURVEYID&gt;
  &lt;RESPONSES&gt;
      &lt;VAR&gt;Question1&lt;/VAR&gt;
      &lt;VALUE&gt;Answer1&lt;/VALUE&gt;
  &lt;/RESPONSES&gt;
  &lt;RESPONSES&gt;
      &lt;VAR&gt;Question2&lt;/VAR&gt;
      &lt;VALUE&gt;Answer2&lt;/VALUE&gt;
  &lt;/RESPONSES&gt;
&lt;SurveyID&gt; 1234 &lt;/SURVEYID&gt;
 &lt;RESPONSES&gt;
      &lt;VAR&gt;Question1&lt;/VAR&gt;
      &lt;VALUE&gt;DifferentAnswer&lt;/VALUE&gt;
  &lt;/RESPONSES&gt;
&lt;/HEADER&gt;
</code></pre>

<p>Note: I don't need to know how to parse XML, that is done. I also know that you can execute SSIS within ADF. I am looking at alternatives to the overall process. </p>
","<xml><azure-logic-apps><azure-data-factory>","2020-01-27 20:21:20","4616","5","1","59938808","<p>I'm not sure why this question got downvoted - I had a similar need a few months ago. The situation was exacerbated by the fact the XML we receive is poorly formatted and wouldn't even parse correctly. To solve this, I wrote a .NET console app and deployed it to Azure Batch. It reads the XML from Blob Storage, corrects the formatting errors, then parses the XML and outputs it to a JSON file back in Blob Storage. ADF supports Azure Batch through the &quot;Custom&quot; activity, and so this plugs right into our pipeline. Depending on your data structure, you could output it to CSV if that is more appropriate.</p>
<p>The tricky bits of using Azure Batch from ADF are in passing and processing parameter data. In the ADF configuration, these are listed under &quot;Extended properties&quot;:
<a href=""https://i.stack.imgur.com/8mTPl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8mTPl.png"" alt=""enter image description here"" /></a></p>
<p>These properties are available to the Batch job at runtime in a JSON file named &quot;activity.json&quot;:In the Console app, you will need to access the JSON file to read the extended properties:</p>
<pre><code>var activity_json = File.ReadAllText(&quot;activity.json&quot;);
dynamic activity = JsonConvert.DeserializeObject(activity_json);
            
parameters.Add(&quot;alertId&quot;, activity.typeProperties.extendedProperties.AlertId.ToString());
parameters.Add(&quot;hashKey&quot;, activity.typeProperties.extendedProperties.HashKey.ToString());
parameters.Add(&quot;startTime&quot;, activity.typeProperties.extendedProperties.StartTime.ToString());
parameters.Add(&quot;endTime&quot;, activity.typeProperties.extendedProperties.EndTime.ToString());
</code></pre>
<p>The property names are Case Sensitive. [Note that in this example I am writing them to a &quot;parameters&quot; Dictionary - I do that so I can run the Console app either locally or in Azure Batch.] There are a few other &quot;interesting&quot; aspects to using Azure Batch, but this is the biggest hurdle in my opinion.</p>
"
"59937155","Can Azure Data Factory invoke an Azure Virtual Machine?","<p>I have an Azure Data Factory pipeline. I want the pipeline to run a command on an existing Azure Virtual Machine, perhaps with ssh. I cannot find any mention of this in the ADF documentation. The closest thing is to use Azure Batch to access some (new) VMs that hold the software I want. But I would like to do this without Batch, just have ADF control an existing Azure VM.</p>

<p>Sounds simple and obvious. Does anyone know how to do this, or knows that it is not possible?</p>
","<azure><azure-data-factory>","2020-01-27 18:50:42","2707","0","2","59937340","<p>All of the supported Activities (as both <em>source</em> and <em>sink</em>) are fully documented. There is no specific way of talking directly to a VM; you'd need to set up your own listener on your VM (since generic HTTP &amp; REST are supported <em>sinks</em>). There is no <code>ssh</code> <em>sink</em>.</p>

<p>More details on Activities: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#data-movement-activities"" rel=""nofollow noreferrer"">here</a>.</p>
"
"59937155","Can Azure Data Factory invoke an Azure Virtual Machine?","<p>I have an Azure Data Factory pipeline. I want the pipeline to run a command on an existing Azure Virtual Machine, perhaps with ssh. I cannot find any mention of this in the ADF documentation. The closest thing is to use Azure Batch to access some (new) VMs that hold the software I want. But I would like to do this without Batch, just have ADF control an existing Azure VM.</p>

<p>Sounds simple and obvious. Does anyone know how to do this, or knows that it is not possible?</p>
","<azure><azure-data-factory>","2020-01-27 18:50:42","2707","0","2","59937377","<p>You can run an Azure Funcion activity from an Azure Data Factory:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity</a></p>

<p>Azure functions call be written in Powershell:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference-powershell</a></p>

<p>You can then use the az vm run command to execute a script on a virtual machine</p>

<p><a href=""https://learn.microsoft.com/en-us/cli/azure/vm/run-command?view=azure-cli-latest#az-vm-run-command-invoke"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/cli/azure/vm/run-command?view=azure-cli-latest#az-vm-run-command-invoke</a></p>
"
"59933994","Can I force flush a Databricks Delta table, so the disk copy has latest/consistent data?","<p>I am accessing Databricks Delta tables from Azure Data Factory, which does not have a native connector to Databricks tables. So, as a workaround, I create the tables with the LOCATION keyword to store them in Azure Data Lake. Then, since I know the table file location, I just read the underlying Parquet files from Data Factory. This works fine.</p>

<p>But... what if there is cached information in the Delta transaction log that has not yet been written to disk? Say, an application updated a row in the table, and the disk does not yet reflect this fact. Then my read from Data Factory will be wrong.</p>

<p>So, two questions...</p>

<ul>
<li>Could this happen? Are changes held in the log for a while before being written out?</li>
<li>Can I force a transaction log flush, so I know the disk copy is updated?</li>
</ul>
","<azure-data-factory><databricks><delta-lake>","2020-01-27 15:20:24","382","3","2","59937189","<p>Azure Data Factory has built in delta lake support (this was not the case at the time the question was raised).</p>
<p>Delta is  available as an inline dataset in a Azure Data Factory data flow activity. To get column metadata, click the Import schema button in the Projection tab. This will allow you to reference the column names and data types specified by the corpus (see also the docs <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delta"" rel=""nofollow noreferrer"">here</a>).</p>
"
"59933994","Can I force flush a Databricks Delta table, so the disk copy has latest/consistent data?","<p>I am accessing Databricks Delta tables from Azure Data Factory, which does not have a native connector to Databricks tables. So, as a workaround, I create the tables with the LOCATION keyword to store them in Azure Data Lake. Then, since I know the table file location, I just read the underlying Parquet files from Data Factory. This works fine.</p>

<p>But... what if there is cached information in the Delta transaction log that has not yet been written to disk? Say, an application updated a row in the table, and the disk does not yet reflect this fact. Then my read from Data Factory will be wrong.</p>

<p>So, two questions...</p>

<ul>
<li>Could this happen? Are changes held in the log for a while before being written out?</li>
<li>Can I force a transaction log flush, so I know the disk copy is updated?</li>
</ul>
","<azure-data-factory><databricks><delta-lake>","2020-01-27 15:20:24","382","3","2","65987366","<p>ADF supports Delta Lake format as of July 2020:</p>
<p><a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/adf-adds-connectors-for-delta-lake-and-excel/ba-p/1515793"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/adf-adds-connectors-for-delta-lake-and-excel/ba-p/1515793</a></p>
<blockquote>
<p>The Microsoft Azure Data Factory team is enabling .. and a data flow connector for data transformation using Delta Lake</p>
</blockquote>
<p>Delta is currently available in ADF as a public preview in data flows as an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-source#inline-datasets"" rel=""nofollow noreferrer"">inline dataset</a>.</p>
"
"59932495","Azure Data Factory Data Flow CSV schema drift to parquet static destination dropping columns. Is it possible?","<p>Trying to write a Azure Data Factory Data Flow that would handle two similar versioned CSV files. Version 1 file has 48 columns. Version 2 file has 50 columns – same 48 columns as version 1 but has 2 additional columns appended to the end. I’d like to create a destination parquet file that contains all 50 columns to load into my SQLDW via polybase. Historically we have over 6 thousand files in the same blob source with no easy way to identify files with 48 columns vs 50 columns. Below is the closest I’ve come to a solution.</p>

<ol>
<li>Source CSV with Allow schema drift enabled. No schema defined on the CSV data set</li>
<li>MapDrifted derived columns – i.e toString(byName('Manufacturer')) all 50 columns</li>
<li>Sink  – data set is parquet with schema defined by parquet template file which contains all 50 columns. Sink partition is set by sourcefilename. Each file coming in will have a parquet file generated in the output.</li>
</ol>

<p>This solution works with a set of two test files. One with 48 columns and one with 50 columns. Two parquet files are created with 50 columns. One file is populated up to the 48th column the other file is populated with all 50 columns. If I add more source files with 48 columns to the test. The file with 50 columns looses the last two columns of data and only ends up with 48 columns in it? I thought this would be a common issue that ADF could solve. i.e file version changes over time. Any suggestions? Below is a a script of my ADF</p>

<pre><code>source(allowSchemaDrift: true,
    validateSchema: false,
    rowUrlColumn: 'sourcefilename',
    inferDriftedColumnTypes: true,
    multiLineRow: true,
    wildcardPaths:['avail/archive_csv2/*.csv']) ~&gt; SRCAvailCSV
SRCAvailCSV derive(Manufacturer = toString(byName('Manufacturer')),
        SKU = toString(byName('SKU')),
        {Partner Name} = toString(byName('Partner Name')),
        {Partner Part Number} = toString(byName('Partner Part Number')),
        {Search Date} = toString(byName('Search Date')),
        {Search Result Description} = toString(byName('Search Result Description')),
        {1st Line Description} = toString(byName('1st Line Description')),
        {2nd Line Description} = toString(byName('2nd Line Description')),
        {Product Category} = toString(byName('Product Category')),
        {Product Category 1} = toString(byName('Product Category 1')),
        {Product Category 2} = toString(byName('Product Category 2')),
        {Product Category 3} = toString(byName('Product Category 3')),
        {Product Category 4} = toString(byName('Product Category 4')),
        {UNSPSC Code} = toString(byName('UNSPSC Code')),
        Pricing = toString(byName('Pricing')),
        Currency = toString(byName('Currency')),
        {Availability Qty} = toString(byName('Availability Qty')),
        {Availability Status} = toString(byName('Availability Status')),
        {Average Rating} = toString(byName('Average Rating')),
        {Total Reviews} = toString(byName('Total Reviews')),
        Brand = toString(byName('Brand')),
        Model = toString(byName('Model')),
        {Product Line} = toString(byName('Product Line')),
        {Partner Site} = toString(byName('Partner Site')),
        {Product URL} = toString(byName('Product URL')),
        Warranty = toString(byName('Warranty')),
        {Product Length} = toString(byName('Product Length')),
        {Product Width} = toString(byName('Product Width')),
        {Product Height} = toString(byName('Product Height')),
        {Product Depth} = toString(byName('Product Depth')),
        {Product Weight} = toString(byName('Product Weight')),
        {Fullfilling Partner} = toString(byName('Fullfilling Partner')),
        {Date First Available} = toString(byName('Date First Available')),
        {Frequently Bought Together 1} = toString(byName('Frequently Bought Together 1')),
        {Frequently Bought Together 1 Part Number} = toString(byName('Frequently Bought Together 1 Part Number')),
        {Frequently Bought Together 2} = toString(byName('Frequently Bought Together 2')),
        {Frequently Bought Together 2 Part Number} = toString(byName('Frequently Bought Together 2 Part Number')),
        {Frequently Bought Together 3} = toString(byName('Frequently Bought Together 3')),
        {Frequently Bought Together 3 Part Number} = toString(byName('Frequently Bought Together 3 Part Number')),
        {Frequently Bought Together 4} = toString(byName('Frequently Bought Together 4')),
        {Frequently Bought Together 4 Part Number} = toString(byName('Frequently Bought Together 4 Part Number')),
        {From the Manufacturer} = toString(byName('From the Manufacturer')),
        {Bestesellers Rank 1} = toString(byName('Bestesellers Rank 1')),
        {Bestsellers Rank 2} = toString(byName('Bestsellers Rank 2')),
        {Bestsellers Rank 3} = toString(byName('Bestsellers Rank 3')),
        {Bestsellers Rank 4} = toString(byName('Bestsellers Rank 4')),
        Endpoint = toString(byName('Endpoint')),
        {Related StarTech.com SKU} = toString(byName('Related StarTech.com SKU')),
        {Search SKU} = toString(byName('Search SKU')),
        {Search Manufacturer} = toString(byName('Search Manufacturer')),
        sourcefilename = sourcefilename) ~&gt; MapDrifted1
MapDrifted1 sink(input(
        FileName as string,
        Manufacturer as string,
        SKU as string,
        PartnerName as string,
        PartnerPartNumber as string,
        SearchDate as string,
        SearchResultDescription as string,
        {1stLineDescription} as string,
        {2ndLineDescription} as string,
        ProductCategory as string,
        ProductCategory1 as string,
        ProductCategory2 as string,
        ProductCategory3 as string,
        ProductCategory4 as string,
        UNSPSCCode as string,
        Pricing as string,
        Currency as string,
        AvailabilityQty as string,
        AvailabilityStatus as string,
        AverageRating as string,
        TotalReviews as string,
        Brand as string,
        Model as string,
        ProductLine as string,
        PartnerSite as string,
        ProductURL as string,
        Warranty as string,
        ProductLength as string,
        ProductWidth as string,
        ProductHeight as string,
        ProductDepth as string,
        ProductWeight as string,
        FullfillingPartner as string,
        DateFirstAvailable as string,
        FrequentlyBoughtTogether1 as string,
        FrequentlyBoughtTogether1PartNumber as string,
        FrequentlyBoughtTogether2 as string,
        FrequentlyBoughtTogether2PartNumber as string,
        FrequentlyBoughtTogether3 as string,
        FrequentlyBoughtTogether3PartNumber as string,
        FrequentlyBoughtTogether4 as string,
        FrequentlyBoughtTogether4PartNumber as string,
        FromtheManufacturer as string,
        BestesellersRank1 as string,
        BestsellersRank2 as string,
        BestsellersRank3 as string,
        BestsellersRank4 as string,
        Endpoint as string,
        RelatedStarTechcomSKU as string,
        SearchSKU as string,
        SearchManufacturer as string
    ),
    allowSchemaDrift: false,
    validateSchema: false,
    format: 'parquet',
    rowUrlColumn:'sourcefilename',
    mapColumn(
        FileName = sourcefilename,
        Manufacturer,
        SKU,
        PartnerName = {Partner Name},
        PartnerPartNumber = {Partner Part Number},
        SearchDate = {Search Date},
        SearchResultDescription = {Search Result Description},
        {1stLineDescription} = {1st Line Description},
        {2ndLineDescription} = {2nd Line Description},
        ProductCategory = {Product Category},
        ProductCategory1 = {Product Category 1},
        ProductCategory2 = {Product Category 2},
        ProductCategory3 = {Product Category 3},
        ProductCategory4 = {Product Category 4},
        UNSPSCCode = {UNSPSC Code},
        Pricing,
        Currency,
        AvailabilityQty = {Availability Qty},
        AvailabilityStatus = {Availability Status},
        AverageRating = {Average Rating},
        TotalReviews = {Total Reviews},
        Brand,
        Model,
        ProductLine = {Product Line},
        PartnerSite = {Partner Site},
        ProductURL = {Product URL},
        Warranty,
        ProductLength = {Product Length},
        ProductWidth = {Product Width},
        ProductHeight = {Product Height},
        ProductDepth = {Product Depth},
        ProductWeight = {Product Weight},
        FullfillingPartner = {Fullfilling Partner},
        DateFirstAvailable = {Date First Available},
        FrequentlyBoughtTogether1 = {Frequently Bought Together 1},
        FrequentlyBoughtTogether1PartNumber = {Frequently Bought Together 1 Part Number},
        FrequentlyBoughtTogether2 = {Frequently Bought Together 2},
        FrequentlyBoughtTogether2PartNumber = {Frequently Bought Together 2 Part Number},
        FrequentlyBoughtTogether3 = {Frequently Bought Together 3},
        FrequentlyBoughtTogether3PartNumber = {Frequently Bought Together 3 Part Number},
        FrequentlyBoughtTogether4 = {Frequently Bought Together 4},
        FrequentlyBoughtTogether4PartNumber = {Frequently Bought Together 4 Part Number},
        FromtheManufacturer = {From the Manufacturer},
        BestesellersRank1 = {Bestesellers Rank 1},
        BestsellersRank2 = {Bestsellers Rank 2},
        BestsellersRank3 = {Bestsellers Rank 3},
        BestsellersRank4 = {Bestsellers Rank 4},
        Endpoint,
        RelatedStarTechcomSKU = {Related StarTech.com SKU},
        SearchSKU = {Search SKU},
        SearchManufacturer = {Search Manufacturer}
    )) ~&gt; sink1
</code></pre>
","<azure-data-factory>","2020-01-27 13:48:46","1659","0","2","59937413","<p>Do you <em>always</em> wish to output a Parquet file with the same schema? I.e. 50 cols, regardless of schema of incoming file?</p>

<p>If so, what you can do is create a data flow with a ""canonical model"" that defines this 50-column structure.</p>

<p>You would build your target schema definition using a Derived Column and map your incoming source columns there. If you do not have a matching column, you can just set to NULL.</p>

<p>With this method, you would not need to define a dataset format in the sink. You could just use Auto Map with a blank dataset and output Parquet files.</p>

<p>The Parquet file output schema would match your Derived Column model which would define the friendly alias names you used in your Sink mapping above.</p>

<p>Here's a video I made to help explain this method: <a href=""https://www.youtube.com/watch?v=K5tgzLjEE9Q"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=K5tgzLjEE9Q</a>.</p>

<p>I hope it helps.</p>
"
"59932495","Azure Data Factory Data Flow CSV schema drift to parquet static destination dropping columns. Is it possible?","<p>Trying to write a Azure Data Factory Data Flow that would handle two similar versioned CSV files. Version 1 file has 48 columns. Version 2 file has 50 columns – same 48 columns as version 1 but has 2 additional columns appended to the end. I’d like to create a destination parquet file that contains all 50 columns to load into my SQLDW via polybase. Historically we have over 6 thousand files in the same blob source with no easy way to identify files with 48 columns vs 50 columns. Below is the closest I’ve come to a solution.</p>

<ol>
<li>Source CSV with Allow schema drift enabled. No schema defined on the CSV data set</li>
<li>MapDrifted derived columns – i.e toString(byName('Manufacturer')) all 50 columns</li>
<li>Sink  – data set is parquet with schema defined by parquet template file which contains all 50 columns. Sink partition is set by sourcefilename. Each file coming in will have a parquet file generated in the output.</li>
</ol>

<p>This solution works with a set of two test files. One with 48 columns and one with 50 columns. Two parquet files are created with 50 columns. One file is populated up to the 48th column the other file is populated with all 50 columns. If I add more source files with 48 columns to the test. The file with 50 columns looses the last two columns of data and only ends up with 48 columns in it? I thought this would be a common issue that ADF could solve. i.e file version changes over time. Any suggestions? Below is a a script of my ADF</p>

<pre><code>source(allowSchemaDrift: true,
    validateSchema: false,
    rowUrlColumn: 'sourcefilename',
    inferDriftedColumnTypes: true,
    multiLineRow: true,
    wildcardPaths:['avail/archive_csv2/*.csv']) ~&gt; SRCAvailCSV
SRCAvailCSV derive(Manufacturer = toString(byName('Manufacturer')),
        SKU = toString(byName('SKU')),
        {Partner Name} = toString(byName('Partner Name')),
        {Partner Part Number} = toString(byName('Partner Part Number')),
        {Search Date} = toString(byName('Search Date')),
        {Search Result Description} = toString(byName('Search Result Description')),
        {1st Line Description} = toString(byName('1st Line Description')),
        {2nd Line Description} = toString(byName('2nd Line Description')),
        {Product Category} = toString(byName('Product Category')),
        {Product Category 1} = toString(byName('Product Category 1')),
        {Product Category 2} = toString(byName('Product Category 2')),
        {Product Category 3} = toString(byName('Product Category 3')),
        {Product Category 4} = toString(byName('Product Category 4')),
        {UNSPSC Code} = toString(byName('UNSPSC Code')),
        Pricing = toString(byName('Pricing')),
        Currency = toString(byName('Currency')),
        {Availability Qty} = toString(byName('Availability Qty')),
        {Availability Status} = toString(byName('Availability Status')),
        {Average Rating} = toString(byName('Average Rating')),
        {Total Reviews} = toString(byName('Total Reviews')),
        Brand = toString(byName('Brand')),
        Model = toString(byName('Model')),
        {Product Line} = toString(byName('Product Line')),
        {Partner Site} = toString(byName('Partner Site')),
        {Product URL} = toString(byName('Product URL')),
        Warranty = toString(byName('Warranty')),
        {Product Length} = toString(byName('Product Length')),
        {Product Width} = toString(byName('Product Width')),
        {Product Height} = toString(byName('Product Height')),
        {Product Depth} = toString(byName('Product Depth')),
        {Product Weight} = toString(byName('Product Weight')),
        {Fullfilling Partner} = toString(byName('Fullfilling Partner')),
        {Date First Available} = toString(byName('Date First Available')),
        {Frequently Bought Together 1} = toString(byName('Frequently Bought Together 1')),
        {Frequently Bought Together 1 Part Number} = toString(byName('Frequently Bought Together 1 Part Number')),
        {Frequently Bought Together 2} = toString(byName('Frequently Bought Together 2')),
        {Frequently Bought Together 2 Part Number} = toString(byName('Frequently Bought Together 2 Part Number')),
        {Frequently Bought Together 3} = toString(byName('Frequently Bought Together 3')),
        {Frequently Bought Together 3 Part Number} = toString(byName('Frequently Bought Together 3 Part Number')),
        {Frequently Bought Together 4} = toString(byName('Frequently Bought Together 4')),
        {Frequently Bought Together 4 Part Number} = toString(byName('Frequently Bought Together 4 Part Number')),
        {From the Manufacturer} = toString(byName('From the Manufacturer')),
        {Bestesellers Rank 1} = toString(byName('Bestesellers Rank 1')),
        {Bestsellers Rank 2} = toString(byName('Bestsellers Rank 2')),
        {Bestsellers Rank 3} = toString(byName('Bestsellers Rank 3')),
        {Bestsellers Rank 4} = toString(byName('Bestsellers Rank 4')),
        Endpoint = toString(byName('Endpoint')),
        {Related StarTech.com SKU} = toString(byName('Related StarTech.com SKU')),
        {Search SKU} = toString(byName('Search SKU')),
        {Search Manufacturer} = toString(byName('Search Manufacturer')),
        sourcefilename = sourcefilename) ~&gt; MapDrifted1
MapDrifted1 sink(input(
        FileName as string,
        Manufacturer as string,
        SKU as string,
        PartnerName as string,
        PartnerPartNumber as string,
        SearchDate as string,
        SearchResultDescription as string,
        {1stLineDescription} as string,
        {2ndLineDescription} as string,
        ProductCategory as string,
        ProductCategory1 as string,
        ProductCategory2 as string,
        ProductCategory3 as string,
        ProductCategory4 as string,
        UNSPSCCode as string,
        Pricing as string,
        Currency as string,
        AvailabilityQty as string,
        AvailabilityStatus as string,
        AverageRating as string,
        TotalReviews as string,
        Brand as string,
        Model as string,
        ProductLine as string,
        PartnerSite as string,
        ProductURL as string,
        Warranty as string,
        ProductLength as string,
        ProductWidth as string,
        ProductHeight as string,
        ProductDepth as string,
        ProductWeight as string,
        FullfillingPartner as string,
        DateFirstAvailable as string,
        FrequentlyBoughtTogether1 as string,
        FrequentlyBoughtTogether1PartNumber as string,
        FrequentlyBoughtTogether2 as string,
        FrequentlyBoughtTogether2PartNumber as string,
        FrequentlyBoughtTogether3 as string,
        FrequentlyBoughtTogether3PartNumber as string,
        FrequentlyBoughtTogether4 as string,
        FrequentlyBoughtTogether4PartNumber as string,
        FromtheManufacturer as string,
        BestesellersRank1 as string,
        BestsellersRank2 as string,
        BestsellersRank3 as string,
        BestsellersRank4 as string,
        Endpoint as string,
        RelatedStarTechcomSKU as string,
        SearchSKU as string,
        SearchManufacturer as string
    ),
    allowSchemaDrift: false,
    validateSchema: false,
    format: 'parquet',
    rowUrlColumn:'sourcefilename',
    mapColumn(
        FileName = sourcefilename,
        Manufacturer,
        SKU,
        PartnerName = {Partner Name},
        PartnerPartNumber = {Partner Part Number},
        SearchDate = {Search Date},
        SearchResultDescription = {Search Result Description},
        {1stLineDescription} = {1st Line Description},
        {2ndLineDescription} = {2nd Line Description},
        ProductCategory = {Product Category},
        ProductCategory1 = {Product Category 1},
        ProductCategory2 = {Product Category 2},
        ProductCategory3 = {Product Category 3},
        ProductCategory4 = {Product Category 4},
        UNSPSCCode = {UNSPSC Code},
        Pricing,
        Currency,
        AvailabilityQty = {Availability Qty},
        AvailabilityStatus = {Availability Status},
        AverageRating = {Average Rating},
        TotalReviews = {Total Reviews},
        Brand,
        Model,
        ProductLine = {Product Line},
        PartnerSite = {Partner Site},
        ProductURL = {Product URL},
        Warranty,
        ProductLength = {Product Length},
        ProductWidth = {Product Width},
        ProductHeight = {Product Height},
        ProductDepth = {Product Depth},
        ProductWeight = {Product Weight},
        FullfillingPartner = {Fullfilling Partner},
        DateFirstAvailable = {Date First Available},
        FrequentlyBoughtTogether1 = {Frequently Bought Together 1},
        FrequentlyBoughtTogether1PartNumber = {Frequently Bought Together 1 Part Number},
        FrequentlyBoughtTogether2 = {Frequently Bought Together 2},
        FrequentlyBoughtTogether2PartNumber = {Frequently Bought Together 2 Part Number},
        FrequentlyBoughtTogether3 = {Frequently Bought Together 3},
        FrequentlyBoughtTogether3PartNumber = {Frequently Bought Together 3 Part Number},
        FrequentlyBoughtTogether4 = {Frequently Bought Together 4},
        FrequentlyBoughtTogether4PartNumber = {Frequently Bought Together 4 Part Number},
        FromtheManufacturer = {From the Manufacturer},
        BestesellersRank1 = {Bestesellers Rank 1},
        BestsellersRank2 = {Bestsellers Rank 2},
        BestsellersRank3 = {Bestsellers Rank 3},
        BestsellersRank4 = {Bestsellers Rank 4},
        Endpoint,
        RelatedStarTechcomSKU = {Related StarTech.com SKU},
        SearchSKU = {Search SKU},
        SearchManufacturer = {Search Manufacturer}
    )) ~&gt; sink1
</code></pre>
","<azure-data-factory>","2020-01-27 13:48:46","1659","0","2","60049189","<p>I threw together a very quick, very simple demo here: <a href=""http://youtu.be/DhscTC6VwwI?hd=1"" rel=""nofollow noreferrer"">http://youtu.be/DhscTC6VwwI?hd=1</a>.</p>

<p>I added it to our collection of ADF data flow videos because this is a common pattern and leverages schema drift and flexible schema handling.</p>

<p>The key to make this work is to build your output model inside the data flow with derived column. Keep your datasets schemaless.</p>

<p>Let me know if you have any questions.</p>
"
"59919834","Data Load Error Using ADF V2 to Sql Server on Azure VM","<p>I'm facing below error while trying to data load from On-Prem Db2 system to Sql Server on Azure Vm using ADF V2</p>

<pre><code>A transport-level error has occurred when receiving results from the server. (provider: TCP Provider, error: 0 - An existing connection was forcibly closed by the remote host.),Source=.Net SqlClient Data Provider,SqlErrorNumber=10054,Class=20,ErrorCode=-2146232060,State=0,Errors=[{Class=20,Number=10054,State=0,Message=A transport-level error has occurred when receiving results from the server. (provider: TCP Provider, error: 0 - An existing connection was forcibly closed by the remote host.),},],
</code></pre>

<p>This happens only when i'm trying to load very large source table to target Sql server. Because when i changed the target to Azure SQL PaaS table got loaded successfully.</p>

<p>I'm not sure how to fix this data load issue and where should i start to trouble shoot this error. </p>

<p>Sql Server Error Log details:</p>

<pre><code>"" Message A fatal error occurred while reading the input stream from the network. The session will be terminated (input error: 10054, output error: 0).""
</code></pre>

<p>Please help here.</p>

<p>Note: I can't switch to Azure Sql PaaS offering as its not supporting any distributed transaction yet.</p>
","<azure><azure-sql-database><azure-data-factory><azure-sql-server>","2020-01-26 15:32:33","1345","0","1","59920889","<blockquote>
  <p>where should i start to trouble shoot this error.</p>
</blockquote>

<p>You start with the SQL Server Error Log.  This is either a network issue, or a severe server-side error.  </p>

<p>The client error message:</p>

<blockquote>
  <p>A transport-level error has occurred when receiving results from the server.</p>
</blockquote>

<p>and SQL Server's error message </p>

<blockquote>
  <p>"" Message A fatal error occurred while reading the input stream from the network. The session will be terminated (input error: 10054, output error: 0).""</p>
</blockquote>

<p>both agree that the network connection between the client and server failed.  If your ADF Integration Runtime is running on-premises inquire about any firewalls or network security devices that might be terminating the connection.</p>
"
"59893328","Azure data factory convert timestamp taking into account DST","<p>I have a data flow in Azure Data Factory
The source contains timestamps in local time 
The sink is a SQL that is running on UTC</p>

<p>In the DataFlow, I can convert the timestamp to UTC using the following command</p>

<p><code>toUTC(timestamp, ""Romance Standard Time"")</code></p>

<p>But that does not take into account DST.</p>

<p>Is there a way to do this directly in the dataflow ? </p>
","<azure><timezone><azure-data-factory><dst>","2020-01-24 09:11:23","2835","3","1","59901562","<p>The <code>toUTC</code> function should indeed take into account DST, as well as other historical changes to time zone offsets.</p>

<p>According <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#toutc"" rel=""nofollow noreferrer"">to the documentation</a>, the supported time zones are those used by Java, which would be the regular <a href=""https://en.wikipedia.org/wiki/List_of_tz_database_time_zones"" rel=""nofollow noreferrer"">IANA time zones</a> (plus a few extra abbreviations for Java legacy purposes).</p>

<p><code>""Romance Standard Time""</code> is a <em>Windows</em> time zone identifier.  An equivalent IANA zone that should work with ADF would be <code>""Europe/Paris""</code>.  Refer to <a href=""https://github.com/unicode-org/cldr/blob/master/common/supplemental/windowsZones.xml"" rel=""nofollow noreferrer"">the CLDR mapping here</a>.</p>

<p>(If you need to do this mapping in .NET code, use my <a href=""https://github.com/mj1856/TimeZoneConverter"" rel=""nofollow noreferrer"">TimeZoneConverter</a> library.)</p>
"
"59891790","Azure DF: Get metadata of millions of files located in a VM and call a store procedure to update file details in a DB","<p>I have created a Getmeta data activity in azure pipeline to fetch the file details located in a VM and I am iterating the output of Getmeta data activity using For each loop.
In the for each loop , I am calling a store procedure to update file details in the database.
If I have 2K files in the VM, the store procedure is called 2K times and which I feel not a good practice.</p>

<p>Is there any method to update all the file details in one shot ?</p>
","<azure><azure-functions><azure-data-factory>","2020-01-24 07:03:33","76","0","1","59942473","<p>Per my knowledge, i think you could use GetMetadata Activity to get the output then pass it into <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a>.</p>

<p>Inside azure function,you could loop the output and use sdk (such as <a href=""https://docs.oracle.com/javase/10/docs/api/java/sql/package-summary.html"" rel=""nofollow noreferrer"">java sql lib</a>) to update the tables as you want in the batch.</p>
"
"59887083","Merging multiple files into one JSON file in Azure Data Factory","<p>I’m working on a pipeline in ADF and i would like to get business metadata from one source and technical metadata from Oracle. </p>

<p>How can i merge those two files into one model.JSON in Azure Data Factory? </p>
","<json><azure><azure-data-factory>","2020-01-23 21:09:24","1182","0","1","59889682","<p>You could Union those 2 sources together as a single model using data flows and design your new combined output schema using a Derived Column</p>
"
"59881794","Azure Data Factory - How to initialize variable with null value","<p>I am calling a stored procedure from the Azure Data Factory. And part of one parameter I need to pass <code>Null</code>. However, the problem is, if I set null, it takes <code>""Null""</code> as a string. </p>

<p>Is there a way I can initialize a variable with null?</p>
","<azure><azure-data-factory>","2020-01-23 15:19:48","3950","0","1","59886533","<p>You can pass a NULL value to a Stored Procedure from the Stored procedure activity using the ""Treat as null"" checkbox:
<a href=""https://i.stack.imgur.com/IwKTt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IwKTt.png"" alt=""enter image description here""></a></p>

<p>It seems that the real question though is how to set a Variable's value to NULL. The only way I can see right off the bat is to (mis)use the coalesce function:
<a href=""https://i.stack.imgur.com/KGyHS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KGyHS.png"" alt=""enter image description here""></a></p>

<p>NOTE: I have not tested this, so YMMV.</p>
"
"59875102","Missing data while copying data cubes from SAP BW to Azure with OpenHub","<p>I need to copy data cubes from SAP BW to some Azure storage (SQL Database, Data Lake, SQL DW - anything which can connect to PowerBI).</p>

<p>I first tried to use a SAP via MDX connector and to copy to a SQL Database - but the tables (in preview) had missing data - ex: I had sales by month and by country, but the combination sales in Jan in UK were missing (and this data is in the cubes)</p>

<p>Now I tried Open Hub connection in SAP but I can see only some basic data (no master data - like name of the country only a code). </p>

<p>What do you advice me to do? I believe this sort of copy activity is quite common so it should be easy to solve.</p>
","<azure><azure-data-factory><data-integration><sap-bw>","2020-01-23 09:22:34","342","0","1","59924495","<p>Congratulations you have solved the issue:</p>

<p>""Fortunately I was able to solve it like this:</p>

<ol>
<li>look in SAP BW for the tables which were holding the master data
(country)</li>
<li>imported them in Azure using connector SAP via Table</li>
<li>merged the data extracted via Openhub with the master data in a data
flow (ADF)""</li>
</ol>

<p>I help you post it as answer and This can be beneficial to other community members. </p>
"
"59864397","Data Factory v2 - connecting using REST","<p>The aim is to connect to a public REST api using ADF. It's my first stab at sending requests to a REST api in ADF. It is the Companies House ('CH') governement website's  API in England. </p>

<p>I have created an account and obtained a key.  Apparently, it is basic authentication and the user name is the API key and password will be ignored (<a href=""https://developer.companieshouse.gov.uk/api/docs/index/gettingStarted/apikey_authorisation.html"" rel=""nofollow noreferrer"">CH note on authentication</a>)</p>

<p>I want to explore the contents of the 'Search all' API (<a href=""https://developer.companieshouse.gov.uk/api/docs/search/search.html"" rel=""nofollow noreferrer"">CH note on Search All</a>) and want to copy the results to Blob Storage.</p>

<p>I therefore set the linked service to use REST as below, the obfuscated User Name is the key I obtained from CH, the password is jsut the key repeated as their documentation states they ignore the password:
[<img src=""https://i.stack.imgur.com/ET1WT.png"" alt=""REST Linked Service[3]""></p>

<p>I then have added a REST dataset referencing this linked service:
<a href=""https://i.stack.imgur.com/ZUAnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZUAnW.png"" alt=""REST DataSet""></a></p>

<p>And the testing of the connection works fine.</p>

<p>Problems then arise in the copy data task, I am getting an error when previewing and also when I attempt a copy to blob of 'Invalid Authorization Header':</p>

<p><a href=""https://i.stack.imgur.com/scVHj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/scVHj.png"" alt=""enter image description here""></a></p>

<p>I'd be grateful for pointers on where I'm going wrong.</p>
","<rest><azure-data-factory>","2020-01-22 16:39:39","528","0","1","59874104","<p>I can't reproduce your Auth error but i notice that you want to add some parameters with your <code>GET</code> request in the Request Body.</p>

<p><a href=""https://i.stack.imgur.com/AQaBB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AQaBB.png"" alt=""enter image description here""></a></p>

<p>I think you need to add parameters in <code>relativeUrl</code> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#dataset-properties"" rel=""nofollow noreferrer"">property</a>:</p>

<blockquote>
  <p>A relative URL to the resource that contains the data. When this
  property isn't specified, only the URL that's specified in the linked
  service definition is used. The HTTP connector copies data from the
  combined URL: [URL specified in linked service]/[relative URL
  specified in dataset].</p>
</blockquote>

<p>Also i suggest you checking the correct REST API format of Search Api you are using.There is no other special features in the ADF REST connector. Just make sure the GET request works locally and duplicate it.</p>
"
"59862122","Jira Rest API Calls in Azure Data Factory","<p>Good Day</p>

<p>I configured a Pipeline Copy Data job in Azure Data Factory to extract data from Jira with an API call using the rest API connector in Azure.</p>

<ol>
<li>When i configure and test the connection it is successful.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/66eSM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/66eSM.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>Now when i try to preview the data in the Copy container i get the following error.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/FDkF4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FDkF4.png"" alt=""enter image description here""></a></p>

<p>Does anyone know what this error means and how do i bypass it?</p>

<p>I believe i am not the first one trying to extract data from Jira via Rest API.</p>

<p>Thank you and Regards</p>

<p>Rayno</p>
","<azure><jira><azure-data-factory><jira-rest-api>","2020-01-22 14:38:32","1572","0","1","59874319","<blockquote>
  <p>Error occurred when deserializing source JSON file "".Check if the data
  is in valid JSON object format.Unexpected character encountered while
  parsing value:&lt;.Path"".....</p>
</blockquote>

<p>I think the error already indicates the root cause.You data format is invalid JSON format,you could try to simulate rest api invoke to make sure if the situation exists.ADF can't help you handle the illegal deserialization.</p>

<p>In addition,according to the connector <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-jira"" rel=""nofollow noreferrer"">doc</a>,ADF supports <code>JIRA connector</code>.Maybe you could try to have a try on that.</p>
"
"59857442","Unchanged Azure Dataflow Shows Validation Error On Flow Expression","<p>We were trying to copy and transform data from one cosmos collection to another using data flows. We are using query to select the data from the collection, all of a sudden the data factory show validation error:</p>
<blockquote>
<p>Data flow expression uses functions/parameters/columns which are not present in current context</p>
</blockquote>
<p>when the <code>validate all</code> option is selected.</p>
<p>We did not make any modifications and nothing published this occurred. Also when we try to debug it shows the same error.</p>
<p>How should this be resolved?</p>
","<azure><azure-data-factory>","2020-01-22 10:20:30","2590","2","2","59861320","<p>Yes i have faced this error before @Akhilesh.</p>

<ol>
<li>Check if the query you have used is valid.</li>
<li>Check all the input fields within the dataflow transformation if there is any field highlighted in red.</li>
<li>If above doesnt work, then delete and create a new dataflow and add your expressions or queries without any errors. Validate everytime you add any transformation to get a clear answer where you are making the error.</li>
</ol>

<p>Debug will not run if you have any invalid fields within the dataflow.
There might be any expression field which is invalid and hence u r getting validation error.</p>
"
"59857442","Unchanged Azure Dataflow Shows Validation Error On Flow Expression","<p>We were trying to copy and transform data from one cosmos collection to another using data flows. We are using query to select the data from the collection, all of a sudden the data factory show validation error:</p>
<blockquote>
<p>Data flow expression uses functions/parameters/columns which are not present in current context</p>
</blockquote>
<p>when the <code>validate all</code> option is selected.</p>
<p>We did not make any modifications and nothing published this occurred. Also when we try to debug it shows the same error.</p>
<p>How should this be resolved?</p>
","<azure><azure-data-factory>","2020-01-22 10:20:30","2590","2","2","66320161","<p>I don't know if you're still having this issue, but for the people coming here in the future the issue is resolved by importing the schema on the step before the filter/select.</p>
<ol>
<li><p>Start Debug session</p>
</li>
<li><p>Select the Source &gt; Projection &gt; Import Projection</p>
</li>
</ol>
<p>This will update the schema to allow for your query to view the column</p>
"
"59857404","How to get pipeline run id from within ADFv2 pipeline","<p>Is there any way to access run id of a running pipeline from within that very pipeline? I haven't found any system variable or function that would return this value. </p>
","<azure-data-factory>","2020-01-22 10:18:33","7682","2","1","59858045","<p>@pipeline().RunId   variable should help you. Refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables#pipeline-scope"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables#pipeline-scope</a>  for all variables which you can use in a pipeline. </p>
"
"59856895","Datafactory Mapping Dataflow cannot format datetime to yyyy/MM/dd","<p>I am trying to convert date from '2019-12-12' to '2019/12/12' in my mapping dataflow.
But i cannot find dataflow expression which can convert to this format.</p>

<p>I want a function similar to formatDateTime() which is available in datafactory expression and not in dataflow expression.</p>

<p>Tried toDate()  ->doesnt take yyyy/MM/dd</p>

<p>Tried toTimestamp()   -> doesnt take yyyy/MM/dd</p>

<p><a href=""https://i.stack.imgur.com/u9YBJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u9YBJ.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/lPScH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lPScH.png"" alt=""enter image description here""></a></p>
","<azure><datetime><azure-data-factory>","2020-01-22 09:54:30","5017","1","3","59870751","<p>You could using this expression:</p>
<pre><code>toString(toDate('2019-12-12'), 'yyyy/MM/dd')
</code></pre>
<p>Result:
<a href=""https://i.stack.imgur.com/7q5uJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7q5uJ.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps</p>
"
"59856895","Datafactory Mapping Dataflow cannot format datetime to yyyy/MM/dd","<p>I am trying to convert date from '2019-12-12' to '2019/12/12' in my mapping dataflow.
But i cannot find dataflow expression which can convert to this format.</p>

<p>I want a function similar to formatDateTime() which is available in datafactory expression and not in dataflow expression.</p>

<p>Tried toDate()  ->doesnt take yyyy/MM/dd</p>

<p>Tried toTimestamp()   -> doesnt take yyyy/MM/dd</p>

<p><a href=""https://i.stack.imgur.com/u9YBJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u9YBJ.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/lPScH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lPScH.png"" alt=""enter image description here""></a></p>
","<azure><datetime><azure-data-factory>","2020-01-22 09:54:30","5017","1","3","59881230","<p>Your first conversion results in a timestamp which doesn't have a format.  To output with your desired format use the below which wraps your code with an additional toString() to format as desired.</p>

<pre><code>toString(toTimestamp(toString(byName('PostedTransactionDate')), 'yyyy-MM-dd') , 'yyyy/MM/dd')
</code></pre>
"
"59856895","Datafactory Mapping Dataflow cannot format datetime to yyyy/MM/dd","<p>I am trying to convert date from '2019-12-12' to '2019/12/12' in my mapping dataflow.
But i cannot find dataflow expression which can convert to this format.</p>

<p>I want a function similar to formatDateTime() which is available in datafactory expression and not in dataflow expression.</p>

<p>Tried toDate()  ->doesnt take yyyy/MM/dd</p>

<p>Tried toTimestamp()   -> doesnt take yyyy/MM/dd</p>

<p><a href=""https://i.stack.imgur.com/u9YBJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u9YBJ.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/lPScH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lPScH.png"" alt=""enter image description here""></a></p>
","<azure><datetime><azure-data-factory>","2020-01-22 09:54:30","5017","1","3","67064092","<p>I used this approach to get a date column format, Used derived column and used this the expression.</p>
<pre><code> coalesce(toDate(YourDate,'MM/dd/yyyy'),toDate(YourDate,'yyyy/MM/dd'),toDate(YourDate,'dd/MM/yyyy'),toDate(YourDate,'MMddyyyy'),toDate(YourDate,'yyyyddMM'),toDate(YourDate,'MMddyyyy'),toDate(YourDate,'yyyyMMdd'))
</code></pre>
<p>I hope it works for you as well, Cheers</p>
"
"59839722","How Modifying Azure Analysis services roles using a logic app?","<p>With Azure Data Factory I have built a pipeline to orchestrate the processing of my Azure Analysis Services model trough a dedicated Logic App as explicated in this <a href=""https://jorgklein.com/2018/01/30/process-azure-analysis-services-objects-from-azure-data-factory-v2-using-a-logic-app/"" rel=""nofollow noreferrer"">article</a>, and it works properly.</p>

<p>Now, always using Azure Data Factory (through Logic App), I wish I could also update the list of the user in a specific roles.</p>

<p>In the article mentioned above, to process the Azure Analysis Services models, the Logic App calls a specific API that has the following format:</p>

<pre><code>https:// &lt;rollout&gt;.asazure.windows.net/servers/&lt;serverName&gt;/models/&lt;resource&gt;/refreshes
</code></pre>

<p>but this API doesn't seem to work for update the model's roles.</p>

<p>Is there anyone who knows the correct method to be able to update model roles using a specific Logic App?</p>

<p>Thanks for any suggestions</p>
","<azure><azure-active-directory><azure-data-factory><azure-logic-apps><azure-analysis-services>","2020-01-21 11:03:06","368","3","2","60052854","<p>If you don't necessarily need to use the logic app for this, I think it might be possible using Azure automation and the powershell cmdlets for managing azure analysis services:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-refresh-azure-automation"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-refresh-azure-automation</a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-powershell</a></p>

<p><a href=""https://learn.microsoft.com/en-us/powershell/module/sqlserver/Add-RoleMember?view=sqlserver-ps"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/sqlserver/Add-RoleMember?view=sqlserver-ps</a></p>
"
"59839722","How Modifying Azure Analysis services roles using a logic app?","<p>With Azure Data Factory I have built a pipeline to orchestrate the processing of my Azure Analysis Services model trough a dedicated Logic App as explicated in this <a href=""https://jorgklein.com/2018/01/30/process-azure-analysis-services-objects-from-azure-data-factory-v2-using-a-logic-app/"" rel=""nofollow noreferrer"">article</a>, and it works properly.</p>

<p>Now, always using Azure Data Factory (through Logic App), I wish I could also update the list of the user in a specific roles.</p>

<p>In the article mentioned above, to process the Azure Analysis Services models, the Logic App calls a specific API that has the following format:</p>

<pre><code>https:// &lt;rollout&gt;.asazure.windows.net/servers/&lt;serverName&gt;/models/&lt;resource&gt;/refreshes
</code></pre>

<p>but this API doesn't seem to work for update the model's roles.</p>

<p>Is there anyone who knows the correct method to be able to update model roles using a specific Logic App?</p>

<p>Thanks for any suggestions</p>
","<azure><azure-active-directory><azure-data-factory><azure-logic-apps><azure-analysis-services>","2020-01-21 11:03:06","368","3","2","60065836","<p>One alternative approach might be to have fixed AD groups as members of the tabular model roles and add / remove members from those AD groups.  Therefore the tabular model roles would not need to be refreshed, it would simply be a matter of adding or removing members from the AD groups as part of your governance process.</p>

<p>A second approach would be to use dynamic row-level security.  Adding records to a Azure SQL DB table is perfectly possible with Logic Apps and could be used to drive security, depending on your requirements.  You can then refresh your security dimension with the Logic App.  See here for more details:</p>

<p><a href=""https://learn.microsoft.com/en-us/power-bi/desktop-tutorial-row-level-security-onprem-ssas-tabular"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/power-bi/desktop-tutorial-row-level-security-onprem-ssas-tabular</a></p>

<p>To answer your question however, the Azure Analysis Services REST API is useful but is not that fully featured, ie it does not contain all possible operations for tabular models or the service.  One other missing example I found was backups, ie although it is possible to trigger a pause or resume of the service, it is not possible to trigger a backup of a tabular model via the REST API.  I do not believe it is possible to alter role members or at least, the operation is not listed in the REST API, although happy to be corrected if I am wrong.  To be more specific, <code>Roles</code> is not mentioned in the list of available objects which can be passed in to the Objects array using the <code>POST / Refreshes</code> eg <a href=""https://learn.microsoft.com/en-us/archive/blogs/analysisservices/asynchronous-refresh-with-the-rest-api-for-azure-analysis-services#post-refreshes"" rel=""nofollow noreferrer"">here</a>.  <code>table</code> and <code>partition</code> are the only ones I'm aware of.</p>

<p>There are also no examples on the MS github site:</p>

<p><a href=""https://github.com/microsoft/Analysis-Services"" rel=""nofollow noreferrer"">https://github.com/microsoft/Analysis-Services</a></p>

<p>Finally, consider calling TMSL via Powershell in an Azure Function, which you can call from Azure Data Factory.</p>

<p>HTH</p>
"
"59824825","How to output json data as an array and not set of objects in Mapping Dataflow Datafactory?","<p><a href=""https://i.stack.imgur.com/UFhMI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UFhMI.png"" alt=""enter image description here""></a></p>

<p>I am trying to output my data after transformations in mapping dataflow to a json file. But the records ends up as separate set of json objects and not joined by commas enclosed in array as shown below:</p>

<p>file contents:
{k1:v1,k2:v2}
{k1:v3,k2:v4}</p>

<p><a href=""https://i.stack.imgur.com/ddJ9z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ddJ9z.png"" alt=""enter image description here""></a></p>

<p>Expected:
[{k1:v1,k2:v2},
{k1:v3,k2:v4}]</p>

<p><a href=""https://i.stack.imgur.com/YwwCa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YwwCa.png"" alt=""enter image description here""></a></p>

<p>This is causing a problem while reading as it is not a valid json. The same problem can be fixed in copy activity as there is a setting to output as array of objects and not set of objects.</p>

<p>Can anyone help pls..</p>
","<json><azure><azure-data-factory>","2020-01-20 13:58:16","1151","4","1","62585466","<p>Not sure if you are still stuck into this, i will answer it anyway.</p>
<p>You can use the Azure Data Factory Data Flow Flatten mapping, unroll by and unroll root. more information found in the following link:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-flatten"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-flatten</a></p>
"
"59818624","subtract 2 day current date in ADF","<p>I am usig azure data factory and basically I need to add a dynamic content (date function) to do this:</p>

<p>SELECT DATEADD(DAY, -2, GETDATE())</p>

<p>any idea?</p>
","<azure><azure-functions><azure-data-factory>","2020-01-20 07:26:52","3859","0","3","59819298","<p>I guess you can do the below using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#logical-functions"" rel=""nofollow noreferrer"">Logical Function</a></p>

<pre><code>@equals(formatDateTime(addDays(utcnow(),-2),'yyyy-MM-dd'),formatDateTime(activity('Your Metadata Activity Name').output.lastmodifieddate,'yyyy-MM-dd'))
</code></pre>
"
"59818624","subtract 2 day current date in ADF","<p>I am usig azure data factory and basically I need to add a dynamic content (date function) to do this:</p>

<p>SELECT DATEADD(DAY, -2, GETDATE())</p>

<p>any idea?</p>
","<azure><azure-functions><azure-data-factory>","2020-01-20 07:26:52","3859","0","3","66543136","<p>You can use just addDays() function to get date differences. Detailed document on Azure Data Factory expressions and function is available : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">here</a><br/>
I used <code>@{formatDateTime(convertfromutc(addDays(utcNow(),-6),'Eastern Standard Time'), 'yyyy-MM-dd' )}</code> Here I wanted to get 6 days old date (from current date).</p>
"
"59818624","subtract 2 day current date in ADF","<p>I am usig azure data factory and basically I need to add a dynamic content (date function) to do this:</p>

<p>SELECT DATEADD(DAY, -2, GETDATE())</p>

<p>any idea?</p>
","<azure><azure-functions><azure-data-factory>","2020-01-20 07:26:52","3859","0","3","73231661","<p>If Date contains timestamp then you should try below code.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@string(addhours('2022-08-03 06:20:51',-2, 'yyyy-MM-dd HH:mm'))</code></pre>
</div>
</div>
</p>
"
"59810720","Can we use Azure DevOps Test Plans for Datalake testing","<p>I'm working on a Data Lake project and I'm using Azure Databricks (writing pyspark code) for ETL purpose and Azure DevOps for CICD and source control purpose. I have noticed Test Plans n Devops: my query is can I use test plans for Data Lake testing? I went through the internet browsing related test plans but I did not find anything about plans related to Data Lake, Database or Data warehousing.</p>
","<python-3.x><pyspark><azure-devops><azure-data-factory><azure-databricks>","2020-01-19 13:57:24","514","0","1","59818790","<p>You can use Azure Devops Test Plans for Datalake testing. If your Data Lake test cases are written in c#, you can use Azure devops Test plans for Datalake testing just like any other c# test projects.</p>

<p>1,Here is the example for set up Datalake test case, <a href=""https://learn.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-cicd-test"" rel=""nofollow noreferrer"">Test your Azure Data Lake Analytics</a> code.</p>

<p>2,Then you can <a href=""https://learn.microsoft.com/en-us/azure/devops/boards/work-items/view-add-work-items?view=azure-devops&amp;tabs=browser#add-a-work-item"" rel=""nofollow noreferrer"">create test workitems</a> in your azure devops boards.</p>

<p>3,After your test items are created in azure devops, you can associate them to your test cases in visual studio test project. Please check the detailed steps 
<a href=""https://learn.microsoft.com/en-us/azure/devops/test/associate-automated-test-with-test-case?view=azure-devops"" rel=""nofollow noreferrer"">Associate automated tests with test cases</a>.</p>

<p>4, In the <strong>Test plans</strong> of your azure devops project, <a href=""https://learn.microsoft.com/en-us/azure/devops/test/create-a-test-plan?view=azure-devops#create-a-test-plan"" rel=""nofollow noreferrer"">Create test plans for your test work items
</a></p>

<p>5, Then you can <a href=""https://learn.microsoft.com/en-us/azure/devops/test/run-automated-tests-from-test-hub?view=azure-devops"" rel=""nofollow noreferrer"">run your automated tests from the test hub in azure devops</a></p>
"
"59807525","Manage Azure BlobStorage file append with ADF","<p>I've azure data factory pipeline which stores data with some operation by calling Azure data flow.
Here file name in blob storage should be the pipeline-run-id.</p>

<p>Pipeline copy activity has 'Copy Behavior', I can not find a related option in the sink stream in a data flow?</p>

<p>Now I have a situation where I'm going to call the same azure data flow in the same pipeline execution more than one time. And because of that my file get overwritten in the blob. But I want to append new data to the same file if it exists.</p>

<p>Ex. If pipeline run id '9500d37b-70cc-4dfb-a351-3a0fa2475e32' and data flow call from that pipeline execution 2 times. In that case, 9500d37b-70cc-4dfb-a351-3a0fa2475e32.csv only has data with 2nd azure data flow process.</p>
","<azure><blob><azure-data-factory>","2020-01-19 05:52:50","874","0","1","59816248","<p>Data Flow doesn't support <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#legacy-copy-activity-sink-model"" rel=""nofollow noreferrer"">copyBehavior</a>.  It means that Data Flow doesn't support merge/append files.
<a href=""https://i.stack.imgur.com/dGBlF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dGBlF.png"" alt=""enter image description here""></a></p>

<p>Every time you call the Data Flow, it will create a new file '9500d37b-70cc-4dfb-a351-3a0fa2475e32.csv' and replace the exist '9500d37b-70cc-4dfb-a351-3a0fa2475e32.csv'.</p>

<p>Hope this helps.</p>
"
"59801996","Self Hosted IR unavailable after ARM deployment","<p>We are trying to use self hosted integration runtime to extract data from on-prem fileshare. To implement CI/CD, I have created arm templates from the data factory where IR is successfully working and enabled sharing on for the Data Factory in which I am going to deploy my pipelines using ARM templates. I can successfully deploy pipeline and self hosted IR and linked services but IR is not available in the new data factory connections. </p>

<p><a href=""https://i.stack.imgur.com/nWdo6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nWdo6.png"" alt=""enter image description here""></a></p>

<p>Is it normal? Because to use CI/CD with Data Factory, as soon as ARM gets deployed we should be ready to run pipelines without manual changes? And if I am correct then can anyone help why IR in the new Data Factory isn't available which is making the pipeline failed when I am trying to run it. </p>
","<deployment><continuous-integration><azure-rm-template><azure-data-factory>","2020-01-18 15:28:38","869","1","2","61660232","<p>Self Hosted Integration Runtime are tied to the ADF it is created in.
To use CI/CD with Self Hosted IR you need to do following steps:</p>

<ol>
<li>Create a new Data Factory other than the ones you using in CI/CD
process,then create the Self hosted Integration Runtime their.(This
ADF doesn't need to contain any of your pipeline or Dataset).</li>
<li>Go to the newly created Integration Runtime and click on edit or pencil
icon. Go to sharing tab of opened window. </li>
<li>Click on Grant Permission to other Data factory.(Search and Give Permission to all ADF
involved in CI/CD Process). </li>
<li>Copy the resource id Displayed. Go to the DEV Data Factory and create new Self hosted runtime of type linked.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/v2Zz6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v2Zz6.png"" alt=""enter image description here""></a></p>

<p>5.Enter the Resource ID when asked and click create.</p>

<p>6.Then proceed to setup CI/CD process through DEV Data Factory.</p>

<p>Since through ARM template in all other Data factory linked Self Hosted IR will be created and if you provided permission then everything will work.</p>
"
"59801996","Self Hosted IR unavailable after ARM deployment","<p>We are trying to use self hosted integration runtime to extract data from on-prem fileshare. To implement CI/CD, I have created arm templates from the data factory where IR is successfully working and enabled sharing on for the Data Factory in which I am going to deploy my pipelines using ARM templates. I can successfully deploy pipeline and self hosted IR and linked services but IR is not available in the new data factory connections. </p>

<p><a href=""https://i.stack.imgur.com/nWdo6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nWdo6.png"" alt=""enter image description here""></a></p>

<p>Is it normal? Because to use CI/CD with Data Factory, as soon as ARM gets deployed we should be ready to run pipelines without manual changes? And if I am correct then can anyone help why IR in the new Data Factory isn't available which is making the pipeline failed when I am trying to run it. </p>
","<deployment><continuous-integration><azure-rm-template><azure-data-factory>","2020-01-18 15:28:38","869","1","2","63893419","<p>A Self-Hosted Integration Runtime, is 'owned' by exactly one Data Factory instance.  The way the 'owner' and the 'sharer' factories define the IR are different.  When you deployed one over the other, the type changed and you ended up with either two 'owners' or two 'sharers'.  Since there can only be one 'owner' or a 'sharer' points to an 'owner', things break.</p>
"
"59790782","COPY command runs but no data being copied from Teradata (on-prem)","<p>I am running into an issue where I have a set up a pipeline that gets a list of tables from Teradata using a Lookup activity and then passes those items to a ForEach activity that then copies the data in parallel and saves them as a gzipped file. The requirement is to essentially archive some tables that are no longer being used.</p>

<p>For this pipeline I am not using any partition options as most of the tables are small and I kept it to be flexible.</p>

<p><strong>Pipeline</strong></p>

<p><a href=""https://i.stack.imgur.com/V9Zfs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V9Zfs.png"" alt=""enter image description here""></a></p>

<p><strong>COPY activity within ForEach activity</strong>
<a href=""https://i.stack.imgur.com/wBii7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wBii7.png"" alt=""enter image description here""></a></p>

<p>99% of the tables ran without issues and were copied as gz files into blob storage, but two tables in particular run for long time (apprx 4 to 6 hours) without any of the data being written into a blob storage account.</p>

<p><a href=""https://i.stack.imgur.com/Ib5RF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ib5RF.png"" alt=""image""></a></p>

<p>Note that the image above says ""Cancelled"", but that was done by me. Before that I had a run time as described above, but still no data being written. This is affecting only 2 tables.</p>

<p>I checked with our Teradata team and those tables are not being used by any one (hence its not locked). I also looked at ""Teradata Viewpoint"" (admin tool) and looked at the query monitor and saw that the query was running on Teradata without issues.</p>

<p>Any insight would be greatly apreciated.</p>
","<azure-data-factory>","2020-01-17 15:56:32","73","0","2","59803281","<p>Onlooking issue mention it look the data size of table is more than a blob can store ( As you are not using any partition options ) </p>

<p>Use partition option for optimize performance and hold the data</p>

<p><a href=""https://www.red-gate.com/simple-talk/cloud/cloud-data/working-azure-blob-storage-service/"" rel=""nofollow noreferrer"">Link</a></p>
"
"59790782","COPY command runs but no data being copied from Teradata (on-prem)","<p>I am running into an issue where I have a set up a pipeline that gets a list of tables from Teradata using a Lookup activity and then passes those items to a ForEach activity that then copies the data in parallel and saves them as a gzipped file. The requirement is to essentially archive some tables that are no longer being used.</p>

<p>For this pipeline I am not using any partition options as most of the tables are small and I kept it to be flexible.</p>

<p><strong>Pipeline</strong></p>

<p><a href=""https://i.stack.imgur.com/V9Zfs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V9Zfs.png"" alt=""enter image description here""></a></p>

<p><strong>COPY activity within ForEach activity</strong>
<a href=""https://i.stack.imgur.com/wBii7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wBii7.png"" alt=""enter image description here""></a></p>

<p>99% of the tables ran without issues and were copied as gz files into blob storage, but two tables in particular run for long time (apprx 4 to 6 hours) without any of the data being written into a blob storage account.</p>

<p><a href=""https://i.stack.imgur.com/Ib5RF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ib5RF.png"" alt=""image""></a></p>

<p>Note that the image above says ""Cancelled"", but that was done by me. Before that I had a run time as described above, but still no data being written. This is affecting only 2 tables.</p>

<p>I checked with our Teradata team and those tables are not being used by any one (hence its not locked). I also looked at ""Teradata Viewpoint"" (admin tool) and looked at the query monitor and saw that the query was running on Teradata without issues.</p>

<p>Any insight would be greatly apreciated.</p>
","<azure-data-factory>","2020-01-17 15:56:32","73","0","2","60101586","<p>Just in case someone else comes across this, the way I solved this was to create a new data store connection called ""TD_Prod_datasetname"". The purpose of this dataset is to not point to a specific table, but to just accept a ""item().TableName"" value.</p>

<p>This datasource contains two main values. 1st is the <strong>@dataset().TeradataName</strong></p>

<p><strong>Dataset property</strong>
<a href=""https://i.stack.imgur.com/3QVrC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3QVrC.png"" alt=""enter image description here""></a></p>

<p>I only came up with that after doing a little bit of digging in Google.</p>

<p>I then created a parameter called <strong>""TeradataTable""</strong> as String.</p>

<p><a href=""https://i.stack.imgur.com/u8KBG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u8KBG.png"" alt=""enter image description here""></a></p>

<p>I then updated my pipeline. As above the main two activities remain the same. I have a lookup and then a ForEach Activity (where for each will get the item values):</p>

<p><a href=""https://i.stack.imgur.com/zBrDX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zBrDX.png"" alt=""enter image description here""></a></p>

<p>However, in the COPY command inside the ForEach activity I updated the source. Instead of getting ""item().Name"" I am passing through @item().TableName:</p>

<p><a href=""https://i.stack.imgur.com/ia1Ug.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ia1Ug.png"" alt=""enter image description here""></a></p>

<p>This then enabled me to then select the ""Table"" option and because I am using Table instead of query I can then use the ""Hash"" partition. I left it blank because according to Microsoft documentation it will automatically find the Primary Key that will be used for this. </p>

<p>The only issue that I ran into when using this was that if you run into a table that does not have a Primary Key then this item will fail and will need to be run through either a different process or manually outside of this job.</p>

<p>Because of this change the previously files that just hung there and did not copy now copied successfully into our blob storage account.</p>

<p>Hope this helps someone else that wants to see how to create parallel copies using Teradata as a source and pass through multiple table values.</p>
"
"59774270","Azure Data Factory, How get output from scala (jar job)?","<p>We have a Azure Data Factory pipeline and one step is a jar job that should return output used in the next steps.
It is possible to get output from notebook with dbutils.notebook.exit(....)
I need the similar feature to retrieve output from main class of jar.
Thanks!</p>

<p><a href=""https://i.stack.imgur.com/D6xAj.jpg"" rel=""nofollow noreferrer"">Image of my pipeline</a></p>
","<azure><apache-spark><azure-data-factory>","2020-01-16 16:43:33","1080","0","1","59782067","<p><strike>
Actually,there is no built-in feature to execute jar job directly as i know.However, you could implement it easily with Azure Databricks Service.</p>

<p>Two ways in Azure Databricks workspace:</p>

<p><a href=""https://i.stack.imgur.com/DIZYQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DIZYQ.png"" alt=""enter image description here""></a></p>

<p>If your jar is executable jar,then just use <code>Set JAR</code> which could set main class and parameters:</p>

<p><a href=""https://i.stack.imgur.com/k0KE4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k0KE4.png"" alt=""enter image description here""></a></p>

<p>Conversely,you could try to use <code>Notebook</code> to execute <a href=""https://docs.databricks.com/notebooks/notebook-workflows.html#notebook-workflows"" rel=""nofollow noreferrer"">dbutils.notebook.exit(....)</a> or something else.</p>

<p>Back to ADF, ADF has <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"" rel=""nofollow noreferrer"">Databricks Activity</a> and you can get output of it for next steps.Any concern,please let me know.</strike></p>

<hr>

<p>Updates:</p>

<p>There is no similar feature to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-notebook#passing-parameters-between-notebooks-and-data-factory"" rel=""nofollow noreferrer"">dbutils.notebook.exit(....)</a> in Jar activity as i know.So far i just provide a workaround here: storing the parameters into specific file which resides in the (for example)blob storage inside the jar execution.Then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity"" rel=""nofollow noreferrer"">LookUp activity</a> after jar activity to get the params for next steps.</p>

<hr>

<p>Updates at 1.21.2020</p>

<p>Got some updates from MSFT in the github link: <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/46347"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/46347</a></p>

<blockquote>
  <p>Sending output is a feature that only notebooks support for notebook
  workflows and not jar or python executions in databricks. This should
  be a feature ask for databricks and only then ADF can support it.</p>
  
  <p>I would recommend you to submit this as a product feedback on Azure
  Databricks feedback forum.</p>
</blockquote>

<p>It seems that output from jar execution is not supported by azure databricks,ADF only supports features of azure databricks naturally. Fine...,you could push the related progress by contacting with azure databricks team. I just shared all my knowledges here.</p>
"
"59772474","Azure Data Factory Copy Activity fails converting MongoDB ISODate to DateTime v2 connector","<p>I am running into an issue with the MongoDB V2 connector in the Copy Activity. Converting an ISODate() field in MongoDB to a SQL Stored Procedure fails with the following error</p>

<pre><code>Column 'createdAt' contains an invalid value '1578842185255'. Cannot convert '1578842185255' to type 'DateTime'.
</code></pre>

<p><strike>It very much feels like a <strong>.net error</strong> and not an issue with SQL at all </strike></p>

<p>We've been running Copy Pipelines for more than a year with the <strong>MongoDB V1 connector</strong> and never had this issue</p>

<p>We're using MongoDB 3.6, DataFactory v2 and SQL Server on Azure</p>

<p>Anyone running into this?</p>

<p>Updates:
I found a work around by having the sql table type / stored procedure accepting a <code>bigint</code> and then converting the timestamp to a date in SQL</p>

<p>Question remains is why the data factory broke this functionality with the V2 Connector</p>
","<sql-server><mongodb><azure><azure-data-factory>","2020-01-16 15:06:07","501","1","1","59886576","<p>V1 connector used a different protocol to get data from MongoDB but V2 connector is using a different driver to get the original data from MongoDB. That was the reason you are seeing a difference between V1 and V2 connectors. </p>

<p>If you have any feedback/suggestion regarding this connector, I would recommend you to please share it ADF uservoice forum. All the feedback shared in this forum will be actively monitored and reviewed by ADF engineering team. </p>

<p>ADF uservoice forum: <a href=""https://feedback.azure.com/forums/270578-azure-data-factory"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-azure-data-factory</a></p>

<p>Reference: The same topic was discussed in MSDN forum: <a href=""https://social.msdn.microsoft.com/Forums/en-US/aa0ea926-9b97-4324-89c9-134179cdf37d/copy-activity-fails-converting-mongodb-isodate-to-datetime-v2-connector?forum=AzureDataFactory"" rel=""nofollow noreferrer"">Copy Activity fails converting MongoDB ISODate to DateTime v2 connector</a></p>
"
"59772356","Orchestrating Talend Jobs/ Tasks with Azure Data Factory","<p>is it posssible to orchestrate Talend Jobs  with Azure Data factory? On the talend website, there is an API called Metaservlet. Has any one used it with Data factory API to orchestrate/ trigger Talend Jobs? </p>
","<azure><talend><azure-data-factory>","2020-01-16 14:59:43","207","0","1","59785387","<p>If you can make RESTful calls via Azure Data Factory, you should be able to use the Metaservlet API (which isn't actually RESTful, but uses a more RPC style). </p>

<p>However, in my experience the Metaservlet API isn't very secure (or implemented well for that matter) as you have to send your JSON payload via a Base64 encoded URL query parameter. Most of the time, you'll be sending a username and password within that Base64 encoded string. This is pretty much like sending the username and password in cleartext - which isn't the sort of thing you should be doing anywhere, let alone over the Internet.</p>

<p>As an alternative, you could implement your own more secure REST wrapper as a proxy for the Metaservlet API.</p>
"
"59754642","How to perform data factory transformations on large datasets in Azure data warehouse","<p>We have Data warehouse tables that we perform transformations using ADF.</p>

<p>If I have a group of ADW tables, and I need to perform transformations on them to land them back onto ADW, should I save the transformations into Azure Blob Storage? or go direct into the target table.  </p>

<p>The ADW tables are in excess of 100Mil records.</p>

<p>Is it an acceptable practice to use Blob Storage as the middle piece.</p>
","<azure><azure-blob-storage><azure-table-storage><azure-data-factory>","2020-01-15 15:29:51","611","0","2","59763913","<p>I can think of two ways to do this (they do not require moving the data into blob storage),</p>

<ul>
<li>Do the transformation within SQL DW using stored procedure and use ADF to orchestrate the stored procedure call</li>
<li>Use ADF's data flow to apply the transformation to read from SQL DW and write back to SQL DW</li>
</ul>
"
"59754642","How to perform data factory transformations on large datasets in Azure data warehouse","<p>We have Data warehouse tables that we perform transformations using ADF.</p>

<p>If I have a group of ADW tables, and I need to perform transformations on them to land them back onto ADW, should I save the transformations into Azure Blob Storage? or go direct into the target table.  </p>

<p>The ADW tables are in excess of 100Mil records.</p>

<p>Is it an acceptable practice to use Blob Storage as the middle piece.</p>
","<azure><azure-blob-storage><azure-table-storage><azure-data-factory>","2020-01-15 15:29:51","611","0","2","59764328","<p>Yes, you'd better using the use Blob Storage as the middle piece.</p>

<p>You can not copy the tables from SQL DW(Source) to the same SQL DW(Sink) directly! If you have tried this， you will have the problems:</p>

<ol>
<li>Copy data: has the error in data mapping, copy data to the same table, not create new tales.</li>
<li>Copy Active: Table is required for Copy activity.</li>
</ol>

<p>If you want to copy the data from SQL DW tables to  new tables with Data Factor, you need at least two steps:</p>

<ol>
<li>copy the data from the SQL DW tables to Blob storage(create the csv
files).</li>
<li>Load these csv files to SQL DW and create new tables.</li>
</ol>

<p>Reference tutorials:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse"" rel=""nofollow noreferrer"">Copy and transform data in Azure Synapse Analytics (formerly Azure
SQL Data Warehouse) by using Azure Data Factory</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">Copy and transform data in Azure Blob storage by using Azure Data
Factory</a></li>
</ul>

<p>Data Factory is good at transfer big data. Reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#copy-performance-and-scalability-achievable-using-adf"" rel=""nofollow noreferrer"">Copy performance of Data Factory</a>. I think it may faster than <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/select-into-clause-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">SELECT - INTO Clause (Transact-SQL)</a>.</p>

<p>Hope this helps.</p>
"
"59749301","Near real-time ETL of Oracle data to Azure SQL","<p>I have an Oracle DB with data that I need to load and transform into an Azure SQL Database. I have no control over either the DB nor the application that updates its data.</p>

<p>I'm looking at Azure Data Factory, but I really need data changes in Oracle to be reflected as near to real-time as possible.</p>

<p>I would appreciate any suggestions / insights.</p>

<p>Is ADF the correct tool for the job? If so, what is a good approach to use? If not suitable, what should I consider using instead?</p>
","<azure><azure-sql-database><azure-data-factory>","2020-01-15 10:17:01","1687","0","3","59762528","<p>I don't think Data Factory is not good for you. Yes you can copy data from Oracle to Azure SQL database with it. But like @Thiago Custodio said, we need need to do it to each table you have. That's too complicated.</p>

<p>Just reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle"" rel=""nofollow noreferrer"">Copy data from and to Oracle by using Azure Data Factory</a>.</p>

<p>As you said, you really need data changes in Oracle to be reflected as near to real-time as possible.</p>

<p>The migration/copy time must be very short. Then the data in Oracle and Azure SQL database could be same before the Oracle data changed next time. I searched a lot and didn't find any real-time copy tools. Actually, I think you want the copy could be  something like 'data sync'.</p>

<p>I found this link <a href=""https://stackoverflow.com/questions/12384554/sync-oracle-database-with-sql-azure"">Sync Oracle Database with SQL Azure</a>, hope it could give some good ideas for you.</p>

<p>About the data migration or copy, You can using bellow ways:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/sql/ssma/oracle/sql-server-migration-assistant-for-oracle-oracletosql?view=sql-server-ver15"" rel=""nofollow noreferrer"">SQL Server Migration Assistant for Oracle (OracleToSQL)</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/dms/dms-overview"" rel=""nofollow noreferrer"">Azure Database Migration Service (DMS)</a></li>
</ol>

<p>Reference tutorial:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/sql/ssma/oracle/migrating-oracle-databases-to-sql-server-oracletosql?view=sql-server-ver15"" rel=""nofollow noreferrer"">Migrating Oracle Databases to SQL Server (OracleToSQL)</a>: SQL Server Migration Assistant (SSMA) for Oracle is a comprehensive environment that helps you quickly migrate Oracle databases to Azure SQL database.</li>
<li><a href=""https://azure.microsoft.com/en-us/resources/videos/how-to-migrate-oracle-to-sqldb-online/"" rel=""nofollow noreferrer"">How to migrate Oracle to Azure SQL Database with minimum downtime</a>: </li>
</ol>

<p>Hope this helps.</p>
"
"59749301","Near real-time ETL of Oracle data to Azure SQL","<p>I have an Oracle DB with data that I need to load and transform into an Azure SQL Database. I have no control over either the DB nor the application that updates its data.</p>

<p>I'm looking at Azure Data Factory, but I really need data changes in Oracle to be reflected as near to real-time as possible.</p>

<p>I would appreciate any suggestions / insights.</p>

<p>Is ADF the correct tool for the job? If so, what is a good approach to use? If not suitable, what should I consider using instead?</p>
","<azure><azure-sql-database><azure-data-factory>","2020-01-15 10:17:01","1687","0","3","59771271","<p>For real-time you don't really want an ELT/ETL tool like ADF. Consider a replication agent like <a href=""https://www.qlik.com/us/products/attunity-replicate"" rel=""nofollow noreferrer"">Attunity</a> or (gulp at the licensing costs) <a href=""https://www.oracle.com/uk/middleware/technologies/goldengate.html"" rel=""nofollow noreferrer"">GoldenGate</a>. </p>
"
"59749301","Near real-time ETL of Oracle data to Azure SQL","<p>I have an Oracle DB with data that I need to load and transform into an Azure SQL Database. I have no control over either the DB nor the application that updates its data.</p>

<p>I'm looking at Azure Data Factory, but I really need data changes in Oracle to be reflected as near to real-time as possible.</p>

<p>I would appreciate any suggestions / insights.</p>

<p>Is ADF the correct tool for the job? If so, what is a good approach to use? If not suitable, what should I consider using instead?</p>
","<azure><azure-sql-database><azure-data-factory>","2020-01-15 10:17:01","1687","0","3","69680407","<p>For the record, we went with a product named QLik Replicate (aka Attunity) and it is working very well!</p>
"
"59748128","Use Unmanaged table in Delta lake on Top of ADLS Gen2","<p>I use ADF to ingest the data from SQL server to ADLS GEN2 in a Parquet Snappy format, But the size of the file in sink goes upto 120 GB, The size causes me a lot of problem when I read this file in Spark and join the data from this file with many other Parquet files.</p>

<p>I am thinking to use Delta lake's unmanage table with the location pointing to the ADLS location, I am able to create an UnManaged table if I don't specify any partition using this</p>

<p>"" <strong>CONVERT TO DELTA parquet.<code>PATH TO FOLDER CONTAINING A PARQUET FILE(S)</code></strong>""</p>

<p>But if I would want to partition this file for query optimization</p>

<p><strong>"" CONVERT TO DELTA parquet.<code>PATH TO FOLDER CONTAINING A PARQUET FILE(S)</code>, PARTITIONED_COLUMN DATATYPE""</strong></p>

<p>It gives me error like the one mentioned in the screenshot (find the attachment).</p>

<p>Error in Text :- 
org.apache.spark.sql.AnalysisException: Expecting 1 partition column(s): [<code>&lt;PARTITIONED_COLUMN&gt;</code>], but found 0 partition column(s): [] from parsing the file name: abfss://mydirectory@myADLS.dfs.core.windows.net/level1/Level2/Table1.parquet.snappy;</p>

<p>There is no way that I can create this Parquet file using ADF with partition details (Am open for suggestions)</p>

<p>Am I giving a wrong Syntax or this can be even done?</p>
","<apache-spark><pyspark><azure-data-factory><delta-lake>","2020-01-15 09:11:01","774","0","1","59784834","<p>Ok, I found the answer to this. While you convert parquet files to delta using the above approach, Delta would look for the correct directory structure with partition information along with the name of the column mentioned in ""Partitioned By"" clause.</p>

<p>For E.g, I have a folder called /Parent, inside this I have a directory structure with partition information, the partitioned parquet files are kept one level further inside the partitioned folders, the folder names are like this</p>

<p>/Parent/Subfolder=0/part-00000-62ef2efd-b88b-4dd1-ba1e-3a146e986212.c000.snappy.parquet
/Parent/Subfolder=1/part-00000-fsgvfabv-b88b-4dd1-ba1e-3a146e986212.c000.snappy.parquet
/Parent/Subfolder=2/part-00000-fbfdfbfe-b88b-4dd1-ba1e-3a146e986212.c000.snappy.parquet
/Parent/Subfolder=3/part-00000-gbgdbdtb-b88b-4dd1-ba1e-3a146e986212.c000.snappy.parquet</p>

<p>in this case, subfolder is the partitions created inside parent.</p>

<p>CONVERT TO DELTA  parquet.<code>/Parent/</code> partitioned by (Subfolder INT)</p>

<p>will just take this directory structure and convert the whole partitioned data to delta and will store the partitioned information in metastore.</p>

<p>Summary:- This command is only to utilize already created partitioned Parquet files. To create partition on single Parquet file you would have to take different route, Which I can explain you later if you are interested ;)</p>
"
"59741084","Calling a singular Logic app from multiple steps in a pipeline","<p>I have created an ADF pipeline which has several components to in it; execute a stored procedure, perform a copy data task.  There are 14 in total (7 pair) and I want to trigger a failure that will send out an email with the error message.</p>

<p>I've gone and created a logic app to send an email as described in this link
<a href=""http://microsoft-bitools.blogspot.com/2018/03/add-email-notification-in-azure-data.html"" rel=""nofollow noreferrer"">http://microsoft-bitools.blogspot.com/2018/03/add-email-notification-in-azure-data.html</a></p>

<p>In the 'Web Activity' component-> Settings-> Body, I have the following:</p>

<pre><code>    ""DataFactoryName"":
        ""@{pipeline().DataFactory}"",
    ""PipelineName"":
        ""@{pipeline().Pipeline}"",
    ""ErrorMessage"":
        ""@{activity('Execute Package').error.message}"",
    ""EmailTo"":
        ""@{pipeline().parameters.EmailTo}""
}
</code></pre>

<p>The 'Execute Package' is the name of the Stored Procedure or Copy Data activity.<a href=""https://i.stack.imgur.com/L8gVD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L8gVD.png"" alt=""enter image description here""></a></p>

<p>The problem is that it only works for the named activity 'Execute Package'.  I haven't been able to find anywhere that it can dynamically get where it is coming from.
Is there any way to just have a singular web activity?  I don't want to create 14 more things in my pipeline each to handle a different possible failure.
The call to SendCompletionEmail works fine with the logic app since only one thing is calling it.</p>
","<azure-pipelines><azure-logic-apps><azure-data-factory>","2020-01-14 20:10:38","168","1","1","59744141","<p>I'd like to take a moment to point out that after solving the immediate issue (how to get the error out of multiple sources), there is another issue you will face: The FailureSendEmail activity will not run when you expect.</p>

<p>First issue (how to get error of whichever is failing):
Assuming that your pipeline is linear, and you only expect one activity to fail, and all subsequent to not run,  I recommend you use the coalesce function.  Coalesce grabs the first non-null argument.  Here is an example I worked out using 2 stored procs:
<code>@string(coalesce(activity('Stored Procedure1').Error,activity('Stored Procedure2').Error))</code>
Coalesce takes an arbitrary number of arguments, so you can expand this as you like.</p>

<p>The other issue is all the 'on failure' connections.  While what you built makes sense, please let me explain.  When an activity has multiple dependency connections coming into it, it will not execute unless all the dependencies report in.  These are AND'ed, not OR'ed.</p>

<p>Fortunately, you do not need a direct connection in order to reference the activity outputs/errors.  An indirect connection is all you need.  If your pipeline logic is linear, then you already have this.  Remove all the lines going into <code>FailureSendEmail</code>.  Then add an 'on failure' and a 'skipped' dependency connection from your last 'Copy Data' activity.  The logic goes like this:</p>

<p>Assuming all the activities in your 'happy path' are connected by success dependencies,
If one activity fails early in the pipeline, then the subsequent activities are skipped.  This fulfills the skipped dependency.
If the last copy activity fails, this fulfills the failure dependency.</p>
"
"59736805","field a colums with the substring of the namefile in DataFlow","<p>I want to field a colums name ManagingCountryCode with the name of File , this colums existe just in my Output (BD)
I've try to do a substring like that but always wrong!</p>

<p>I chose an option to store the fileName in a colums 
[</p>
","<mapping><azure-data-factory><dataflow>","2020-01-14 15:22:16","47","0","1","59742863","<p>like i say, i store the filesName in filenamecolum which is an option in (SourceOption) after that i did'nt get the error below(in picture) because he can define what is 'fileName' , after that i add mapping simple and i mapp the new colums with the others in output file.</p>
"
"59736316","Azure ADF how to ensure that the same files that are copied, are also deleted?","<p>Using Azure ADF and currently my setup is as follows:</p>

<p>Event based triggerd on a input BLOB on file upload. File upload triggers a copy activity to  output BLOB, and this action is followed by a delete operation on the input BLOB. The input BLOB can take 1 or many files at once (not sure how often the file is scanned/how quickly the event triggers the pipeline). Reading up on the delete function documentation it says:</p>

<pre><code>Make sure you are not deleting files that are being written at the same time.
</code></pre>

<p>Would my current setup delete files that are being written? </p>

<p>Event based trigger on file upload >> Write from input Blob to Output Blob >> Delete input Blob</p>

<p>I've made an alternative solution which does a get metadata activity based on event in the beginning of the pipeline, and then does a for loop which deletes the files at the end, not sure if this is necessary though. Would my original solution suffice in an unlikely event where I'm receiving files every 15seconds or so?</p>

<p>Also while I'm at it, in a get metadata activity how can I get the actual path to the file, not just the file name?</p>

<p>Thank you for the help.</p>
","<azure><azure-data-factory><azure-data-lake>","2020-01-14 14:55:59","1366","1","2","59744622","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity"" rel=""nofollow noreferrer"">Delete Active</a> says:</p>

<ul>
<li>Make sure you are not deleting files that are being written at the
same time.</li>
</ul>

<p>Your settings are:</p>

<ul>
<li>Event based trigger on file upload >> Write from input Blob to Output
Blob >> Delete input Blob</li>
</ul>

<p>Only after the active <code>Write from input Blob to Output Blob</code> finished(the deleting files are not being written),  then the <code>Delete input Blob</code> can works.</p>

<p>Your questions: Would my current setup delete files that are being written?  </p>

<p>So did you test these steps? You must test by yourself and you will get the answer.</p>

<p>Please notice:</p>

<p><strong>Delete activity does not support deleting list of folders described by wildcard.</strong></p>

<p>Any other suggestions:</p>

<p>You don't need to using delete actives to delete the input blob after <code>Write from input Blob to Output Blob</code> finished.</p>

<p>You can learn from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data flow</a>, it's Source settings support delete the source file(input blob) after the copy active completed.
<a href=""https://i.stack.imgur.com/pILZF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pILZF.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"59736316","Azure ADF how to ensure that the same files that are copied, are also deleted?","<p>Using Azure ADF and currently my setup is as follows:</p>

<p>Event based triggerd on a input BLOB on file upload. File upload triggers a copy activity to  output BLOB, and this action is followed by a delete operation on the input BLOB. The input BLOB can take 1 or many files at once (not sure how often the file is scanned/how quickly the event triggers the pipeline). Reading up on the delete function documentation it says:</p>

<pre><code>Make sure you are not deleting files that are being written at the same time.
</code></pre>

<p>Would my current setup delete files that are being written? </p>

<p>Event based trigger on file upload >> Write from input Blob to Output Blob >> Delete input Blob</p>

<p>I've made an alternative solution which does a get metadata activity based on event in the beginning of the pipeline, and then does a for loop which deletes the files at the end, not sure if this is necessary though. Would my original solution suffice in an unlikely event where I'm receiving files every 15seconds or so?</p>

<p>Also while I'm at it, in a get metadata activity how can I get the actual path to the file, not just the file name?</p>

<p>Thank you for the help.</p>
","<azure><azure-data-factory><azure-data-lake>","2020-01-14 14:55:59","1366","1","2","69285127","<p>I could not use Leon Yue <a href=""https://stackoverflow.com/a/59744622/1843853"">solution</a> because my source dataset was a sftp one, which is not supported by Azure dataflows.</p>
<p>To deal with this problem, I used the Filter by last modified of the dataset. I set the End Time to the time the pipeline has started.</p>
<p><a href=""https://i.stack.imgur.com/j2VhD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j2VhD.png"" alt=""Filter by last modified End Time"" /></a></p>
<p>With this solution, only the files added to the source before the pipeline started will be consumed by both the copy and delete activities.</p>
"
"59734969","Azure Data Factory - Data flow activity changing file names","<p>I am running a data flow activity using Azure Data Factory. 
Source data source - Azure bolb
Destination data source - Azure Data Lake Gen 2</p>

<p>For Eg. I have a file named <strong>""test_123.csv""</strong> in Azure blob. When I create a data flow activity to filter some data and copy to Data Lake it is changing the file name to <strong>""part-00.csv""</strong> in Data Lake. </p>

<p><strong>I want to keep my original filename?</strong> </p>
","<azure><filenames><azure-blob-storage><azure-data-factory>","2020-01-14 13:41:11","196","0","1","59846766","<p>Yes you can do that , please look at the screenshot below . Please do let me know how it goes .<a href=""https://i.stack.imgur.com/YxOeH.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YxOeH.gif"" alt=""enter image description here""></a></p>
"
"59733920","Parametrization using Azure Data Factory","<p>I have a Pipeline job in Azure Data Factory which I want to use to run the pipeline job but pass all files for a specific month through for example. </p>

<p>I have a folder called 2020/01 inside this folder is numerous files with different names.</p>

<p>The question is: Can one pass a parameter through to only extract and load the files for 2020/01/01 and 2020/01/02 if that makes sense?</p>
","<azure><parameters><azure-data-factory>","2020-01-14 12:36:29","133","1","2","59744166","<blockquote>
  <p>The question is: Can one pass a parameter through to only extract and
  load the files for 2020/01/01 and 2020/01/02 if that makes sense?</p>
</blockquote>

<p>You did't mention which connector you are using in pipeline job,but you mentioned <code>folder</code> in your question.As i know,the majority folder path could be parametrization in ADF copy activity configuration.</p>

<p>You could create a param :</p>

<p><a href=""https://i.stack.imgur.com/dZQyL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dZQyL.png"" alt=""enter image description here""></a></p>

<p>Then apply it in the <code>wildcard folder path</code>:</p>

<p><a href=""https://i.stack.imgur.com/BQGqs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BQGqs.png"" alt=""enter image description here""></a></p>

<p>Even if your files' names have same prefix,you could apply <code>01*.json</code> on the <code>wildcard file name property</code>.</p>
"
"59733920","Parametrization using Azure Data Factory","<p>I have a Pipeline job in Azure Data Factory which I want to use to run the pipeline job but pass all files for a specific month through for example. </p>

<p>I have a folder called 2020/01 inside this folder is numerous files with different names.</p>

<p>The question is: Can one pass a parameter through to only extract and load the files for 2020/01/01 and 2020/01/02 if that makes sense?</p>
","<azure><parameters><azure-data-factory>","2020-01-14 12:36:29","133","1","2","59751943","<p>Excellent, Thanks Jay it worked and i can now run my pipeline jobs passing through the month or even day level.</p>

<p>Really appreciate your response, have a fantastic day.</p>

<p>Regards</p>

<p>Rayno</p>
"
"59732972","Azure Data Factory Integration runtimes will not start","<p>I have an issue where Azure Data Factory Integration runtimes will not start.</p>

<p>When I trigger the pipeline I get the following error in Monitor -> Pipeline runs ""InternalServerError executing request""</p>

<p><a href=""https://i.stack.imgur.com/FoulU.png"" rel=""nofollow noreferrer"">Image 1</a></p>

<p>In ""view activity run"" I can see that it's the Data Flow that failed with the error </p>

<pre><code>{
    ""errorCode"": ""1006"",
    ""message"": ""Hit unexpected exception and execution failed."",
    ""failureType"": ""SystemError"",
    ""target"": ""data_wrangling_ks"",
    ""details"": []
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/ctZC2.png"" rel=""nofollow noreferrer"">Image 2</a></p>

<p>(the two successful runs are from a Self-Hosted IR)</p>

<p>When i try to start ""Data flow debug"" it will just disappear without any information. </p>

<p>This issue started earlier today without any changes in Data Factory config or the pipeline.</p>

<p>Please help and thank you for your time.</p>

<hr>

<p>SOLVED:
I changed the <em>Compute type</em> from <strong>General Purpose</strong> to <strong>Compute Optimized</strong> and that solved the problem. </p>
","<azure><dataflow><azure-data-factory>","2020-01-14 11:35:09","1938","1","1","59848064","<p>By looking at the error message, it seems like this issue has occurred due ADF related service outage in West Europe region. The issue has been resolved by the product team. Please open a MSDN thread if you ever encounter this issue.   </p>

<p>Ref: <em><a href=""https://social.msdn.microsoft.com/Forums/en-US/662f53db-0a7c-492f-a094-52ea2e59397a/azure-data-factory-pipeline-failed-while-running-data-flows-with-error-message-hit-unexpected?forum=AzureDataFactory"" rel=""nofollow noreferrer"">Azure Data Factory Pipeline failed while running data flows with error message : Hit unexpected exception and execution failed</a></em></p>
"
"59724020","Processing tables in parallel using Azure Data Factory, single pipeline, single Databricks Notebook?","<p>I want to transform a list of tables in parallel using Azure Data Factory and one single Databricks Notebook. </p>

<p>I already have an Azure Data Factory (ADF) pipeline that receives a list of tables as a parameter, sets each table from the table list as a variable, then calls one single notebook (that performs simple transformations) and passes each table in series to this notebook. The problem is that it transforms the tables in series (one after the other) and not in parallel (all tables at the same time). I need the tables to be processed in parallel.</p>

<p><strong>So, my questions are: 
1) Is it possible to trigger the same Databricks notebook multiple times at the exact same point in time (each time with a different table as a parameter) from Azure Data Factory?
2) If yes, then what do I need to change in my pipeline or notebook to make it work?</strong> </p>

<p>Thanks in advance :)</p>

<p><strong>Parameters</strong></p>

<p><a href=""https://i.stack.imgur.com/8D2XJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8D2XJ.jpg"" alt=""ADF Parameters""></a></p>

<p><strong>Variables</strong></p>

<p><a href=""https://i.stack.imgur.com/hIkQb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hIkQb.jpg"" alt=""variables""></a></p>

<p><strong>Set Table Variables and Notebook</strong></p>

<p><a href=""https://i.stack.imgur.com/DGVjB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DGVjB.jpg"" alt=""enter image description here""></a></p>

<p><strong>Configure Sequential</strong></p>

<p><a href=""https://i.stack.imgur.com/24dVp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/24dVp.jpg"" alt=""Configure Sequential""></a></p>

<p><strong>Sequential Unchecked with Batch Count = blank</strong></p>

<p>When configured as ""sequential"" and Batch Count = blank, and pass two tables, the pipeline runs ""successfully"" but only one table is transformed (even if I add multiple tables in the table list). ""Set variable"" correctly shows twice, once for each table. But Orchestrate shows twice for the same table.</p>

<p><a href=""https://i.stack.imgur.com/wIQVx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wIQVx.jpg"" alt=""enter image description here""></a></p>

<p><strong>Sequential Unchecked with Batch Count = 2</strong></p>

<p>When configured as ""sequential"" and Batch Count = 2, and pass two tables, the pipeline fails on the second iteration, but it also tries transforming the same table two times. ""Set variable"" correctly shows twice, once for each table. But Orchestrate shows twice for the same table.</p>

<p><a href=""https://i.stack.imgur.com/ufPTz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ufPTz.jpg"" alt=""Sequential Unchecked with Batch Count = 2""></a></p>

<p><strong>Sequential Checked or Batch Count =1</strong></p>

<p>If I leave Sequential Checked or Batch Count =1, then the pipeline runs correctly and performs transformations on all tables, but the processing occurs in series (as expected). Example below for 5 tables.</p>

<p><a href=""https://i.stack.imgur.com/d7fes.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d7fes.jpg"" alt=""Sequential Checked or Batch Count =1""></a></p>

<p><a href=""https://i.stack.imgur.com/SqFMZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SqFMZ.jpg"" alt=""Example of Sequential Checked or Batch Count =1""></a></p>

<p><strong>Set Variable Task</strong></p>

<p><a href=""https://i.stack.imgur.com/f6Tvl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f6Tvl.png"" alt=""Set Variable Task Overview""></a></p>

<p>Variable table passed with value @item()</p>

<p><a href=""https://i.stack.imgur.com/B8KtY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B8KtY.jpg"" alt=""Variable table passed with value @item()""></a></p>

<p>Variable ""table"" defined as string</p>

<p><a href=""https://i.stack.imgur.com/TQvlv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TQvlv.jpg"" alt=""Variable &quot;table&quot;""></a></p>

<p>Parameter ""table_list""</p>

<p><a href=""https://i.stack.imgur.com/BwOmu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BwOmu.jpg"" alt=""Parameter &quot;table_list&quot;""></a></p>

<p>Pipeline Run Parameters</p>

<p><a href=""https://i.stack.imgur.com/1lXS2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1lXS2.jpg"" alt=""Pipeline Run Parameters""></a></p>
","<azure><azure-data-factory><azure-databricks><spark-notebook>","2020-01-13 20:55:19","1364","2","1","59740677","<p>I solved it using ""Lookup"" to a SQL tables instead of ""Set Variable"". The picture below shows a run of 5 tables in parallel using one single notebook.</p>

<p><a href=""https://i.stack.imgur.com/lPIoB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lPIoB.jpg"" alt=""enter image description here""></a></p>
"
"59715294","Automated Deployment of ADF Pipelines using Azure DevOps CI/CD Pipelines","<p>I have automated the Azure ADF Pipeline Deployment process using Azure DevOps CI/CD pipelines with the help of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> (i.e) Deploying pipelines from DEV to PROD environment ADF. I am using ARM Templates of the ADF to deploy pipelines from one environment to another. Hence I will be having a separate ARM_Parameter.json corresponding to each environment(Dev/Prod).
   The Problem is each ADF pipeline may have few base parameres along with it, which is not parameterized and hence it will not be available in parameter.json. Can you guys help me to replace the Dev Values with the PROD Values in Base Parameter section under each ADF Pipelines in an automated way during this automated ADF pipeline deployment process using CI/CD Pipelines?</p>
","<azure><azure-devops><cloud><azure-data-factory>","2020-01-13 10:59:24","1318","1","4","59729450","<p>I see two options:</p>

<ol>
<li>If it's only for this RUN_ENVIRONMENT parameter, you could change your parameter to variable and use the system variable @pipeline().DataFactory to determine what environment you're running in. </li>
<li>Otherwist, you can configure the Data Factory to generate ARM Parameters for your pipeline parameter default values, but you'll have to create a custom  arm-template-parameters-definition.json file. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">Check the documentation here</a> </li>
</ol>
"
"59715294","Automated Deployment of ADF Pipelines using Azure DevOps CI/CD Pipelines","<p>I have automated the Azure ADF Pipeline Deployment process using Azure DevOps CI/CD pipelines with the help of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> (i.e) Deploying pipelines from DEV to PROD environment ADF. I am using ARM Templates of the ADF to deploy pipelines from one environment to another. Hence I will be having a separate ARM_Parameter.json corresponding to each environment(Dev/Prod).
   The Problem is each ADF pipeline may have few base parameres along with it, which is not parameterized and hence it will not be available in parameter.json. Can you guys help me to replace the Dev Values with the PROD Values in Base Parameter section under each ADF Pipelines in an automated way during this automated ADF pipeline deployment process using CI/CD Pipelines?</p>
","<azure><azure-devops><cloud><azure-data-factory>","2020-01-13 10:59:24","1318","1","4","59729525","<blockquote>
  <p>Replace the Dev Values with the PROD Values in Base Parameter section</p>
</blockquote>

<p>Based on your screenshot, the <code>RUN_ENVIRONMENT</code> is the parameter of pipeline, which means while convert to ARM template, its format like:</p>

<pre><code> ""resources"": [
    {
      ....
      ....
      ""properties"": {
        ""parameters"": {
          ""RUN_ENVIRONMENT"": {
            ""type"": ""string"",
            ""defaultValue"": ""pro""
          }
        },...      
      },...
    }
  ]
</code></pre>

<p>It can not be replaced by using <code>Override template parameters</code> in ARM deploy task. Because it will prompt <code>The template parameters 'environment' in the parameters file are not valid; they are not present in the original template and can therefore not be provided at deployment time.</code> </p>

<hr>

<p>To around this error, just install one <a href=""https://marketplace.visualstudio.com/items?itemName=qetza.replacetokens&amp;targetId=3ec66d6c-317d-423e-a056-b014031f2871&amp;utm_source=vstsproduct&amp;utm_medium=ExtHubManageList"" rel=""nofollow noreferrer"">extension</a> and add the <code>Replace token</code> task into the pipeline which before ARM deploy task. And this task will replace the value of content during the build runtime:</p>

<p><a href=""https://i.stack.imgur.com/c6lbj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c6lbj.png"" alt=""enter image description here""></a></p>

<p>For how to apply this task in our pipeline, you could have a refer to my <a href=""https://stackoverflow.com/questions/59684532/can-i-create-a-file-or-replace-content-of-a-file-using-kubernetes/59710379#59710379"">answer1</a> and <a href=""https://stackoverflow.com/questions/57877200/how-can-i-pass-a-variable-group-in-jmeter-using-azure-pipeline/57887868#57887868"">answer2</a></p>
"
"59715294","Automated Deployment of ADF Pipelines using Azure DevOps CI/CD Pipelines","<p>I have automated the Azure ADF Pipeline Deployment process using Azure DevOps CI/CD pipelines with the help of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> (i.e) Deploying pipelines from DEV to PROD environment ADF. I am using ARM Templates of the ADF to deploy pipelines from one environment to another. Hence I will be having a separate ARM_Parameter.json corresponding to each environment(Dev/Prod).
   The Problem is each ADF pipeline may have few base parameres along with it, which is not parameterized and hence it will not be available in parameter.json. Can you guys help me to replace the Dev Values with the PROD Values in Base Parameter section under each ADF Pipelines in an automated way during this automated ADF pipeline deployment process using CI/CD Pipelines?</p>
","<azure><azure-devops><cloud><azure-data-factory>","2020-01-13 10:59:24","1318","1","4","60644158","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">Custom parameter with ARM template</a>. 
The custom parameter for Pipeline could look like this:</p>

<pre><code>""Microsoft.DataFactory/factories/pipelines"": {
    ""properties"": {
        ""parameters"": {
                ""RUN_ENVIRONMENT"": ""=:-:string""
        }
    }
},
</code></pre>
"
"59715294","Automated Deployment of ADF Pipelines using Azure DevOps CI/CD Pipelines","<p>I have automated the Azure ADF Pipeline Deployment process using Azure DevOps CI/CD pipelines with the help of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> (i.e) Deploying pipelines from DEV to PROD environment ADF. I am using ARM Templates of the ADF to deploy pipelines from one environment to another. Hence I will be having a separate ARM_Parameter.json corresponding to each environment(Dev/Prod).
   The Problem is each ADF pipeline may have few base parameres along with it, which is not parameterized and hence it will not be available in parameter.json. Can you guys help me to replace the Dev Values with the PROD Values in Base Parameter section under each ADF Pipelines in an automated way during this automated ADF pipeline deployment process using CI/CD Pipelines?</p>
","<azure><azure-devops><cloud><azure-data-factory>","2020-01-13 10:59:24","1318","1","4","61729756","<p>There is another approach to publish ADF, from master (collaboration) branch.
You can define (replace) value for every single node (property) in JSON file (ADF object).
It will resolve your problem as you can provide a separate CSV config file per each environment (stage).</p>

<p>Example of CSV config file (<code>config-stage-UAT.csv</code>):</p>

<pre><code>type,name,path,value
pipeline,PL_CopyMovies,activities[0].outputs[0].parameters.BlobContainer,UAT
</code></pre>

<p>Then just run such cmdlet in PowerShell:</p>

<pre><code>Publish-AdfV2FromJson -RootFolder ""$RootFolder"" -ResourceGroupName ""$ResourceGroupName"" -DataFactoryName ""$DataFactoryName"" -Location ""$Location"" -Stage ""stage-UAT""
</code></pre>

<p>Check this out:
<a href=""https://www.powershellgallery.com/packages/azure.datafactory.tools/"" rel=""nofollow noreferrer"">azure.datafactory.tools</a> (PowerShell module)</p>
"
"59714804","Copy activity queued for a long time","<p>I have a pipeline with a Copy activity which copies data from a folder in ADLS Gen2 with Parquet files to a table in Azure SQL Database. Sometimes the Copy activity is in status Queued for several minutes before the actual copy of data occurs and then succeeds. The Copy activity has a pre-copy script setting for the Sink that truncates the table before copying the data. Why is the Copy activity queued for a long time before copying the data? Can it be due to some lock in the database, i.e. that some query is reading from the table?</p>
","<azure-data-factory>","2020-01-13 10:31:26","4938","2","1","59795583","<p>Below is similar issue with Azure IR, after looking at the logs internally, it is addressed as an issue occurred due to ADF related service outage in West Europe. Internal team has confirmed that the issue has been resolved. </p>

<p>Could you please re-run the pipeline and let us know if you still notice the issue.</p>

<p>Related MSDN case: <a href=""https://social.msdn.microsoft.com/Forums/en-US/db83d241-a66b-40fc-82e0-eee13162befa/copy-activities-on-azure-ir-has-long-queue-wait-times?forum=AzureDataFactory"" rel=""nofollow noreferrer"">copy activities on Azure IR has long queue wait times</a></p>
"
"59710982","Azure Data factory - Data flow","<p>I need to handle Minus operation in Azure Data factory - Data flows. Is this feature supported in Data flow or is there any work around for this. Pls provide the inputs. I know it can be done using SP activity or Databricks transformation activity or in other ways as well, but want to know if it can be done using Data flows itself in ADF. I have two tables one having yesterday data and another having today data, in Databases using minus i can find the differences between two tables. For eg In Table A a record exists as A, 123 and Table B record exists as A, 456. When i do a minus operation it will fetch this record.</p>
","<azure><azure-data-factory>","2020-01-13 05:27:37","606","0","2","59712751","<p>You could use data flow DerivedColumn to do the Minus operation between the two table.</p>

<p>For example, I create two tables for the test: <code>test7</code> and <code>test8</code>:
<a href=""https://i.stack.imgur.com/VE4Xb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VE4Xb.png"" alt=""enter image description here""></a></p>

<p><strong>Data flow overview</strong>:</p>

<p><a href=""https://i.stack.imgur.com/JXuLz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JXuLz.png"" alt=""enter image description here""></a></p>

<p><strong>Join settings and data preivew</strong>:</p>

<p><a href=""https://i.stack.imgur.com/sgxMM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sgxMM.png"" alt=""enter image description here""></a></p>

<p><strong>DerivedColumn settings and data preview:</strong>
<a href=""https://i.stack.imgur.com/xXsvr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xXsvr.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/j8reW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j8reW.png"" alt=""enter image description here""></a></p>

<p><strong>Sink settings, mappings:</strong>
<a href=""https://i.stack.imgur.com/f4QJm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f4QJm.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"59710982","Azure Data factory - Data flow","<p>I need to handle Minus operation in Azure Data factory - Data flows. Is this feature supported in Data flow or is there any work around for this. Pls provide the inputs. I know it can be done using SP activity or Databricks transformation activity or in other ways as well, but want to know if it can be done using Data flows itself in ADF. I have two tables one having yesterday data and another having today data, in Databases using minus i can find the differences between two tables. For eg In Table A a record exists as A, 123 and Table B record exists as A, 456. When i do a minus operation it will fetch this record.</p>
","<azure><azure-data-factory>","2020-01-13 05:27:37","606","0","2","59722078","<p>You could put that query in the Source query in any Source transformation in ADF Data Flows. Just use EXCEPT rather than MINUS:</p>

<p>select * from dbo.ProdFromSales2 except select * from dbo.DimProducts</p>
"
"59710724","Azure Data factory Transformation Pipeline- Screen resolution issue","<p>I am facing Azure Data factory screen resolution issue. my screen resolution is 50% still I am not able to see the expression I am trying to build.
if anyone from the team has faced this issue before, please help us resolve<a href=""https://i.stack.imgur.com/IwVCe.jpg"" rel=""nofollow noreferrer"">enter image description here</a> this issue.</p>
","<azure><azure-data-factory>","2020-01-13 04:47:33","59","-1","2","59710850","<p>This is Daniel from the product group. Sorry you are facing this issue! To help us diagnose the this, can you answer the following questions:</p>

<ol>
<li>What browser are you using?</li>
<li>When did you start experiencing this?</li>
</ol>

<p>While this is probably not helpful, please refresh the page and see if this issue still occurs. </p>

<p>Any additional information provided will be of great assistance!</p>
"
"59710724","Azure Data factory Transformation Pipeline- Screen resolution issue","<p>I am facing Azure Data factory screen resolution issue. my screen resolution is 50% still I am not able to see the expression I am trying to build.
if anyone from the team has faced this issue before, please help us resolve<a href=""https://i.stack.imgur.com/IwVCe.jpg"" rel=""nofollow noreferrer"">enter image description here</a> this issue.</p>
","<azure><azure-data-factory>","2020-01-13 04:47:33","59","-1","2","59712206","<p>Please try Edge browser!</p>

<p>It works.</p>
"
"59688434","How do I scale Azure Data Factory Dataflow?","<p>I was able to setup a SCD Type 2 process quite easily using the ADF UI for one table BUT I don't see an easy way to scale to the 1000s of datasources we've. I don't see any Java APIs that will allow me to write ADF Pipelines/Dataflow and configure &amp; trigger them dynamically. No UI to allow which tables to choose from a particular database etc. I looked at Azure Datalake Gen 2, Azure Databricks etc. I don't see any tool in Azure that will allow us to replace the UI driven Data Lake ingestion process we've built in house. Am I missing something?</p>

<p>On a side note, we've an old Data lake application that ingests data from thousands of datasources such as Databases, log files, web applications etc and stores data on HDFS (a typical architecture) using technologies as Java, Spark, Kafka etc. We're evaluating Azure Active Data Factory to replace it. </p>
","<azure><azure-data-lake><azure-data-factory><azure-databricks>","2020-01-10 20:14:35","964","0","2","59689897","<p>There is a generic SCD (Type 1, but you can retrofit to Type 2) example built into ADF. Go to New > Pipeline from template > Transform with Data flows > Generic SCD Type 1.</p>

<p>This pattern is outlined here: <a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/create-generic-scd-pattern-in-adf-mapping-data-flows/ba-p/918519"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/create-generic-scd-pattern-in-adf-mapping-data-flows/ba-p/918519</a>.</p>

<p>You can also iterate over schemaless table datasets for Foreach inside a pipeline, calling the same data flow on every iteration.</p>

<p>Lastly, if you still wish to stamp-out data flows programmatically, the .NET and PowerShell SDKs are listed in the references section of the online Azure docs.</p>
"
"59688434","How do I scale Azure Data Factory Dataflow?","<p>I was able to setup a SCD Type 2 process quite easily using the ADF UI for one table BUT I don't see an easy way to scale to the 1000s of datasources we've. I don't see any Java APIs that will allow me to write ADF Pipelines/Dataflow and configure &amp; trigger them dynamically. No UI to allow which tables to choose from a particular database etc. I looked at Azure Datalake Gen 2, Azure Databricks etc. I don't see any tool in Azure that will allow us to replace the UI driven Data Lake ingestion process we've built in house. Am I missing something?</p>

<p>On a side note, we've an old Data lake application that ingests data from thousands of datasources such as Databases, log files, web applications etc and stores data on HDFS (a typical architecture) using technologies as Java, Spark, Kafka etc. We're evaluating Azure Active Data Factory to replace it. </p>
","<azure><azure-data-lake><azure-data-factory><azure-databricks>","2020-01-10 20:14:35","964","0","2","59691322","<p>You could leverage the REST API from Java to build out pipelines using code.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api</a></p>
"
"59686273","Migrating existing Cosmosdb collections to Autopilot","<p>I have a number of existing collection with manual RU provisioning which I would like to migrate to be Autopilot managed to better automatically deal with varying levels of demand.</p>

<p>The collections contain many GB of historical timeseries data, and I cannot have any downtime where new or historical data is not available to customers.  I must also ensure no data is lost during the migration</p>

<p>Once a day, a new day of data is bulk uploaded to the cosmosdb collection, and the collections can be queried at any time by the customer-facing service in front of them.</p>

<p>For migration, I was considering the following:
1. Create new autopilot collection
2. Modify service to query both old and new collection and deduplicate any data present in both
3. Redirect data upload to new collection
4. Use ADF (Azure data factory) to copy the contents of the old collection to the new Autopilot one
5. Update service to only query the new collection
6. Drop old collection.</p>

<p>Is this the best migration strategy, or is there an alternative approach which would provide a better customer experience, or be less work?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-01-10 17:20:08","158","0","1","59688441","<p>While in Preview you will need to manually migrate data to AutoPilot containers. Once we GA we are planning to allow customers to seamlessly migrate containers from regular to AutoPilot throughput. </p>

<p>For the scenario you describe I find it easier to use ChangeFeed when I need to do a near zero downtime migration. Create a new AutoPilot configured container, then create an Azure Function using the Cosmos DB bindings to read from the source container and write to the new AutoPilot container to allow data to stay in sync. </p>

<p>Rewrite your consuming apps to use the new container and your bulk load scripts to write to the new container. Once that is done, deploy the changes. I like using slots for Web Apps (or whatever you choose) for zero or near zero downtime.</p>

<p>One thing to keep an eye on is since you are bulk loading this data, Azure Functions will likely fall far behind keeping the data in sync. You'll want to monitor to see how long that takes just so you know when you can flip the switch on the migration.</p>

<p>Hope that helps.</p>
"
"59684489","Azure Data Factory Pipeline Alert Email Send Everytime","<p>I would like to send an email via an alert once an Azure Data Factory Pipeline has either finished successfully or failed. This is fairly simple to setup and I get the email once! However after the first time the email is sent it never sends again.</p>

<p>I understand that this is because the alert is in a fired state and will not send again unless this is changed. There is no way to change the state of the alert so the alert will never be triggered again.</p>

<p>Is there anyway to make it so the notification sends an email everytime the pipeline runs?</p>
","<azure><azure-data-factory>","2020-01-10 15:21:37","566","1","1","59725783","<p>Your understanding is correct , that once an alert is fired ( for what ever reason ) it needs to be deactivated and then only the other one can be fired again . My understanding is the alerts are configured for ""not the happy path"" and so the logic makes sense .</p>

<p>As it appears that just want to an email alert when your pipeline is executed . I suggest to use a logic App  to do that . You can execute this as the  last activity in the pipeline and call the logic app  from the pipeline . Configuring its is very easy . </p>
"
"59684353","Azure - PGP/GPG decryption of files","<p>The client sends files through SFTP (using <a href=""https://learn.microsoft.com/en-us/samples/azure-samples/sftp-creation-template/sftp-on-azure/"" rel=""nofollow noreferrer"">SFTP on Azure</a>) to File Share, then I transfer it to Blob using Data Factory. Files are encrypted using GPG or PGP. I looking for way to decrypt it on the server. I was thinking about additional step in Data Factory which would trigger a python script. Is this task doable in Azure?</p>
","<azure><azure-functions><azure-data-factory>","2020-01-10 15:12:23","7002","2","1","59690772","<p>You can create an Azure function that use the blob storage trigger to decrypt the files as soon as they arrive.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function</a></p>
"
"59684311","Approach to connect SAP BW to Azure?","<p>Having a big database in SAP BW and wanting to move it to Azure for storing and querying, what is the approach you would take?</p>

<p>Connect SAP BW to Azure Data Factory -> Data Lake / Azure SQL Warehouse / Azure SQL Database??
What are your suggestions? Any extra details / steps would help a lot. Thanks</p>

<p>My final goal is then to create dashboards in PowerBI retrieving data from Azure.</p>
","<powerbi><azure-data-factory><azure-data-lake><azure-synapse><sap-bw>","2020-01-10 15:09:51","998","0","1","64488456","<p>It's very subjective and hard to answer.
First you must check if you have any licence issues. You can extract data from BW (Netweaver) if you have OpenHub licence. You can use HANA connection if your HANA is Enterprise.</p>
<p>Secondlly, about the data lake. Dou you need it? Do you have non or semi structured data, do you have data from other sources like IoT? A datalake must be carefully planned or you will end with a bigger problem in your hands.
Yes, let's do it: here are a lot of articles talking about structuring your data Lake like this one (<a href=""https://medium.com/@Nicholas_Hurt/building-your-data-lake-on-adls-gen2-3f196fc6b430"" rel=""nofollow noreferrer"">https://medium.com/@Nicholas_Hurt/building-your-data-lake-on-adls-gen2-3f196fc6b430</a>). You can ingest your data to a &quot;raw&quot; stage using data Factory, treat it to a cleansed/curated area using DataBricks or Data Factory, depending on the volume, concorrency you could need Azure Synapse/SSAS, finally you can access your data using PowerBI.</p>
<p>No, I only have small data: You can acess your data directly from PowerBI to BW using a PowerBI Gateway, it's the simplest way to consume BW data using PowerBI. You can Integrate it to an ADLS Gen2, isert your data on a SQLDb and finally Access using PowerBI.</p>
"
"59683341","How do you execute a python Wheel Class/Method(not a script) in Azure Data Factory using an Azure Databricks activity?","<p>Is it possible to execute a python Wheel Class/Method(not a script) in Azure Data Factory using an Azure Databricks activity like you would execute if it were a java packaged method in a .jar?  Unlike a script, this would have the ability to return a value(s), without doing something like burying them stdout.   </p>

<p>I haven't been able to search anything and I tried using the jar activity with no luck which didn't surprise me but worth a try.</p>

<p>If not, what I am looking for is a way to use Azure Databricks compute and return a small set of values back from the python job.  I have successfully used the ADF activity for databricks python script.</p>

<p>TIA!</p>
","<python><interop><azure-data-factory><azure-databricks><python-wheel>","2020-01-10 14:07:29","1122","3","2","59688488","<p>Yes. Add the wheel as a library on the cluster. Then create a .py file that imports the library and calls the method you need. 
Save the py file onto the dbfs volume. </p>

<p>Create a data factory pipeline that uses the python task and point it at your py file. You can pass in arguments as well. </p>

<p>You could also do this with a notebook that imports the library. </p>

<p>This blog post (and the series it is in) should help <a href=""https://datathirst.net/blog/2019/9/20/building-pyspark-applications-as-a-wheel"" rel=""nofollow noreferrer"">https://datathirst.net/blog/2019/9/20/building-pyspark-applications-as-a-wheel</a></p>
"
"59683341","How do you execute a python Wheel Class/Method(not a script) in Azure Data Factory using an Azure Databricks activity?","<p>Is it possible to execute a python Wheel Class/Method(not a script) in Azure Data Factory using an Azure Databricks activity like you would execute if it were a java packaged method in a .jar?  Unlike a script, this would have the ability to return a value(s), without doing something like burying them stdout.   </p>

<p>I haven't been able to search anything and I tried using the jar activity with no luck which didn't surprise me but worth a try.</p>

<p>If not, what I am looking for is a way to use Azure Databricks compute and return a small set of values back from the python job.  I have successfully used the ADF activity for databricks python script.</p>

<p>TIA!</p>
","<python><interop><azure-data-factory><azure-databricks><python-wheel>","2020-01-10 14:07:29","1122","3","2","73373817","<p>Here is the link that has helped me to organize content in the wheel (.whl).</p>
<p><a href=""https://community.databricks.com/s/question/0D53f00001jxOsgCAE/databricks-job-package-name-and-entrypoint-parameters-for-the-python-wheel-file"" rel=""nofollow noreferrer"">https://community.databricks.com/s/question/0D53f00001jxOsgCAE/databricks-job-package-name-and-entrypoint-parameters-for-the-python-wheel-file</a></p>
<p>File structure:</p>
<pre><code>| - setup.py
| - src
|  - __init__.py
|  - test1.py
|  - test2.py
</code></pre>
<p>Your setup.py content</p>
<pre><code> setup(    
      name=&quot;src&quot;,    
      packages = ['.src'],    
      description=&quot;demo&quot;,    
      version=&quot;0.1&quot;,
      ....
    )

 
</code></pre>
<p>And from the Databricks UI:</p>
<pre><code>PackageName: src    
Entry_Point : print_name (Function inside __init__.py)
</code></pre>
"
"59678416","Azure Data Flow: How to map csv to sql following table structure?","<p>I have a raw csv in azure blob as follows:</p>

<p><a href=""https://i.stack.imgur.com/nEzJ9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nEzJ9.png"" alt=""csv in blob""></a></p>

<p>I want to transform it as follows so that I can map it to the table:</p>

<p><a href=""https://i.stack.imgur.com/h5C6c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h5C6c.png"" alt=""sql table structure""></a></p>

<p>If this is not possible in Azure data flow, then at least can we convert CSV to the expected SQL table format using excel functions?</p>
","<azure><azure-pipelines><azure-data-factory>","2020-01-10 09:02:49","118","0","1","59679670","<p>Based on my knowledge, there is no built-in feature to implement transpose.I suggest you using <code>azure function activity</code> before <code>data flow activity</code> to convert the data into the format as you want.</p>

<pre><code>import csv
# import numpy as np

with open('D:/test1.csv', 'r') as csv_file:  # Opens the file in read mode
    csv_reader = csv.reader(csv_file)
    table = [row for row in csv_reader]
    header = table[0][0:2]+['Date','Value']
    dates = table[0][2:]
    newTable = [header]+[row[0:2]+[date, value] for row in table[1:] for date, value in zip(dates,row[2:])]
    print(newTable)
    # print(np.array(newTable))
</code></pre>

<p>Output as below:</p>

<p><a href=""https://i.stack.imgur.com/OaZO0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OaZO0.png"" alt=""enter image description here""></a></p>
"
"59674105","How to pass complex parameters to ADF via Powershell","<p><a href=""https://stackoverflow.com/questions/53756570/how-to-pass-arguments-to-adf-pipeline-using-powershell"">This answer</a> helped me get partway through what I need to do, but one of the parameters I need to pass to my Azure Data Factory pipeline is an array. </p>

<p>In the UI, I can just type it into the ""default parameter"" slot with [""x"", ""y"", ""z""] but I am not sure how to translate this syntactically to Powershell. </p>

<p>I also want to pass an object in, which in the ADF UI is of the format {""x"":{""y"":[""z""],""a"":""b"",""c"":""d""}}. </p>

<p>How can I pass non-primitive parameters to my pipeline?</p>

<p>Thank you in advance.</p>
","<powershell><azure-data-factory>","2020-01-10 00:43:38","475","0","2","59674443","<p>Not familiar with ADF, but for translate a string to an object, I suggest you use json string and <code>ConvertFrom-Json</code>. Here is a sample:</p>

<pre><code>$json = '{ ""X"":{ ""Y"":[ ""1"",""2"",""3""]}, ""a"" : ""b"", ""c"":""d""}'
$obj = $json | ConvertFrom-Json
$obj.X
$obj.X.Y
$obj.a
$obj.c
</code></pre>

<p>And the output:</p>

<pre><code>PS C:\WINDOWS\system32&gt; $obj.X

Y        
-        
{1, 2, 3}

PS C:\WINDOWS\system32&gt; $obj.X.Y
1
2
3

PS C:\WINDOWS\system32&gt; $obj.a
b
</code></pre>

<p>Hope it would be helpful to you. </p>
"
"59674105","How to pass complex parameters to ADF via Powershell","<p><a href=""https://stackoverflow.com/questions/53756570/how-to-pass-arguments-to-adf-pipeline-using-powershell"">This answer</a> helped me get partway through what I need to do, but one of the parameters I need to pass to my Azure Data Factory pipeline is an array. </p>

<p>In the UI, I can just type it into the ""default parameter"" slot with [""x"", ""y"", ""z""] but I am not sure how to translate this syntactically to Powershell. </p>

<p>I also want to pass an object in, which in the ADF UI is of the format {""x"":{""y"":[""z""],""a"":""b"",""c"":""d""}}. </p>

<p>How can I pass non-primitive parameters to my pipeline?</p>

<p>Thank you in advance.</p>
","<powershell><azure-data-factory>","2020-01-10 00:43:38","475","0","2","59794219","<p>I'm not sure if the other answer works or not, but I was able to get it working by using the -ParameterFile flag instead of the -Parameter flag, and passing in a path to a JSON file. This was a cleaner solution for me anyways as it allows git tracking of our parameter file rather than manually passing in parameters. Thanks!</p>
"
"59672446","load two files in the same DB table with Azure Data Flow","<p>How can i Load two files(csv) the same shemas into my sqlDatabase with AzureDatafactory flow?</p>

<p>I've created one flow with two input and the same output but i get just the one table data the other one (NULL) .</p>
","<sql><azure><azure-data-factory>","2020-01-09 21:34:51","49","0","1","59675092","<p>As you said, the two csv file have the same schema, you could put them in same folder or container, make sure the container or folder only have the two files.</p>

<p>Then you could follow my steps:</p>

<p><strong>My Container</strong> :</p>

<p><a href=""https://i.stack.imgur.com/5aOXw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5aOXw.png"" alt=""enter image description here""></a></p>

<p><strong>Data and file schema</strong>:
<a href=""https://i.stack.imgur.com/9dGVo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9dGVo.png"" alt=""enter image description here""></a></p>

<p><strong>Data FLOW Source dataset settings:</strong></p>

<p>Just choose the container or folder, all the csv files in it will be chosen. When we preview  data, the data of the two csv files will be merged together
<a href=""https://i.stack.imgur.com/Q5Qjt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q5Qjt.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/RyIJY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RyIJY.png"" alt=""enter image description here""></a></p>

<p><strong>Sink dataset settings and data preview(the data will be inserted to the Sink table):</strong>
<a href=""https://i.stack.imgur.com/qi64j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qi64j.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/0ak2I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0ak2I.png"" alt=""enter image description here""></a></p>

<p><strong>Run the pipeline:</strong></p>

<p><a href=""https://i.stack.imgur.com/ArNL8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ArNL8.png"" alt=""enter image description here""></a></p>

<p><strong>Check the data in Sink table:</strong></p>

<p><a href=""https://i.stack.imgur.com/twien.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/twien.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"59672364","Azure Data Factory: Using ORC file as source or sink in data flow with ADLS gen2?","<p>I'm trying to create a Azure Data Factory dataflow from an Avro file source with some transforms, landing as an ORC file, both in ADLS gen2.  However, ORC does not seem to be an option for a dataflow sink.  I also tried as a dataflow source, and it was grayed out (see image below, ORC option is grayed out)</p>

<p><a href=""https://i.stack.imgur.com/7ni33.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7ni33.png"" alt=""enter image description here""></a></p>

<p>I've successfully used it in a Copy activity as a sink.  Haven't been able to find documentation though, other than that ORC is a compatible file type with ADF.  Has anyone been able to use ORC in a dataflow as either a source or sink, and if so, how?</p>
","<azure-data-factory><orc><azure-data-lake-gen2>","2020-01-09 21:27:44","387","0","1","59674884","<p>Data Flow doesn't support ORC format file now.</p>

<p>You could reference the doucument <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-source"" rel=""nofollow noreferrer"">Supported source connectors in mapping data flow</a>:
<a href=""https://i.stack.imgur.com/7AZb4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7AZb4.png"" alt=""enter image description here""></a></p>

<p>We can not use ORC as the Source or Sink dataset.</p>

<p>Hope this helps.</p>
"
"59656353","MERGE data in a Dataflow of Azure Data Factory into an existing table","<p>I am trying to create a DataFlow under Azure Data Factory that inserts &amp; updates rows into a table after performing some transformations. When I am trying to write the modified data into a 'Sink' I am selecting both checkboxes, 'Allow Inserts' &amp; 'Allow Updates'. A message pops up telling me to create 'Add Alter Row'. What I want to do is simply update if the primary columns match; otherwise insert rows but I cannot figure out how to do that under 'Add Alter Row'.</p>
<p>To summarize, I want to write all the rows back to the table. If PK columns match then update row; otherwise insert row. How do I do that? Unfortunately, truncating the table is not a solution I can use.</p>
<p>Essentially, I need to perform a 'MERGE'.</p>
","<azure><azure-data-factory>","2020-01-09 02:17:32","10231","5","1","59661901","<p>I solve it and it works. I will merge my two tables <code>TEST2</code> and <code>TEST3</code>, all the rows will write to <code>TEST3</code>.</p>

<p>Data in my table <code>TEST2</code> and <code>TEST3</code>:</p>

<p><a href=""https://i.stack.imgur.com/reAec.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/reAec.png"" alt=""enter image description here""></a></p>

<p><strong>Here's my Data FLOW</strong>:
<a href=""https://i.stack.imgur.com/4BWhn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4BWhn.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/eSCHj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eSCHj.png"" alt=""enter image description here""></a></p>

<p><strong>Firstly, using JOIN to get the data from <code>TEST2</code> and <code>TEST3</code></strong>:</p>

<p><a href=""https://i.stack.imgur.com/YzSQI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YzSQI.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/hUQkN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hUQkN.png"" alt=""enter image description here""></a></p>

<p><strong>Alter Row settings and Data Preview:</strong></p>

<p><a href=""https://i.stack.imgur.com/vQ0Fv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vQ0Fv.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/bBdPj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bBdPj.png"" alt=""enter image description here""></a></p>

<p><strong>Sink settings and Data Preview:</strong></p>

<p><a href=""https://i.stack.imgur.com/mc17T.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mc17T.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/RCOjm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RCOjm.png"" alt=""enter image description here""></a></p>

<p><strong>Check the data in TEST3:</strong></p>

<p><a href=""https://i.stack.imgur.com/xHl23.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xHl23.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"59649463","Data factory Copy to temporary table","<p>I'm trying to follow the simple best practice instructions from MS on how to copy bulk data into azure sql using data factory:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#best-practice-for-loading-data-into-sql-server"" rel=""nofollow noreferrer"">Best Practice</a></p>

<p>""Option 1: When you have a large amount of data to copy, use the following approach to do an upsert:</p>

<p>First, use a temporary table to bulk load all records by using the copy activity. Because operations against temporary tables aren't logged, you can load millions of records in seconds.
...
for example, ##UpsertTempTable, as the table name in the dataset. ""</p>

<p>I've followed those instructions and it fails to copy the data. If I use a real table, rather than a temp table it's fine. The real table gets created on the fly and the data imports successfully</p>

<p>Here is the JSON for the dataset</p>

<pre><code>{
""name"": ""UserTempTable"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""AzureSqlDatabase1"",
        ""type"": ""LinkedServiceReference""
    },
    ""annotations"": [],
    ""type"": ""AzureSqlTable"",
    ""schema"": [],
    ""typeProperties"": {
        ""table"": ""usertemptable""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>If I replace ""usertemptable"" with ""##usertemptable""</p>

<p>it fails        </p>

<p>Any ideas?</p>
","<azure-data-factory>","2020-01-08 15:53:35","6116","0","2","59656777","<p>Data Factory doesn't support auto create temporary table. We can not set ""##usertemptable"" as the table name:
<a href=""https://i.stack.imgur.com/yaeaB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yaeaB.png"" alt=""enter image description here""></a></p>

<p><strong>ERROR</strong>:</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed. Please search error to get more details.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.InvalidOperationException,Message=Cannot access destination table '[dbo].[##temptest]'.,Source=System.Data,''Type=System.Data.SqlClient.SqlException,Message=Invalid object name '##temptest'.,Source=.Net SqlClient Data Provider,SqlErrorNumber=208,Class=16,ErrorCode=-2146232060,State=0,Errors=[{Class=16,Number=208,State=0,Message=Invalid object name '##temptest'.,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy data1"",
    ""details"": []
}
</code></pre>

<p>For Azure SQL database, the temporary tables are in TempDB, but we can not see and access it in System Database. We also can not choose the temporary table as dataset in Data Factory.</p>

<blockquote>
  <p>Global temporary tables are automatically dropped when the session that created the table ends and all other tasks have stopped referencing them. The association between a task and a table is maintained only for the life of a single Transact-SQL statement. This means that a global temporary table is dropped at the completion of the last Transact-SQL statement that was actively referencing the table when the creating session ended.</p>
</blockquote>

<p>You can reference this link <a href=""https://stackoverflow.com/questions/56119601/how-to-create-temp-tables-in-sql-to-be-used-in-several-adf-activities"">How to create temp tables in SQL to be used in several ADF activities?</a>. It give you a lot of helps and suggestions.</p>

<p>You also can referece this blog: <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/35449"" rel=""nofollow noreferrer"">Using global temporary table in copy activity not working</a>. Microsoft MSFT gives you an another way that you can insert the data to temporary table with stored procedure.</p>

<p>Some notice: Don't close the SQL connection session when the temporary table created.</p>

<p>If all of this don't work for you, you may need think about why you insist to use temporary table since Copy Active have good performance to copy data. The final choice may be that using the real table rather than temporary table.</p>

<p>Hope this helps.</p>
"
"59649463","Data factory Copy to temporary table","<p>I'm trying to follow the simple best practice instructions from MS on how to copy bulk data into azure sql using data factory:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#best-practice-for-loading-data-into-sql-server"" rel=""nofollow noreferrer"">Best Practice</a></p>

<p>""Option 1: When you have a large amount of data to copy, use the following approach to do an upsert:</p>

<p>First, use a temporary table to bulk load all records by using the copy activity. Because operations against temporary tables aren't logged, you can load millions of records in seconds.
...
for example, ##UpsertTempTable, as the table name in the dataset. ""</p>

<p>I've followed those instructions and it fails to copy the data. If I use a real table, rather than a temp table it's fine. The real table gets created on the fly and the data imports successfully</p>

<p>Here is the JSON for the dataset</p>

<pre><code>{
""name"": ""UserTempTable"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""AzureSqlDatabase1"",
        ""type"": ""LinkedServiceReference""
    },
    ""annotations"": [],
    ""type"": ""AzureSqlTable"",
    ""schema"": [],
    ""typeProperties"": {
        ""table"": ""usertemptable""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>If I replace ""usertemptable"" with ""##usertemptable""</p>

<p>it fails        </p>

<p>Any ideas?</p>
","<azure-data-factory>","2020-01-08 15:53:35","6116","0","2","67308749","<p>There is new feature available in sql so that instead of creating temp table use table type. Its quite easy to implement as well.</p>
"
"59635685","Azure ADF V2 ForEach File CopyData from Blob Storage to SQL Table","<p>I need to design an ADF pipeline to copy a CSV file created on a particular Blob Store folder path named ""Current"" to a SQL table. After successful copy, i'll have to move the file to an archive folder. </p>

<p><strong>Things i've accomplished</strong>: I'm using a Copy Data Activity that copies the CSV file and loads into my SQL table. On success, another copy data activity will copy the CSV file from ""Current"" folder to ""Archive"", after this i have a Delete activity that deletes the file from Current  Folder. </p>

<p><strong>Issue</strong>: This is totally fine where there is only one file at a time. But i want redesign this to handle multiple files imports. I want make use of ForEach activity. Please help. </p>
","<azure><azure-blob-storage><azure-data-factory>","2020-01-07 20:33:34","7555","1","1","59637281","<p>You need to add a Get Metadata activity before the for-each.  The Get Metadata activity will get the files in the current directory and pass them to the For-Each.  You connect it to your Blob storage folder and specify the file mask.   The output of the activity will need to be the input for your for-each loop.  Something like this..@activity('Get Metadata').output.
  By the way how are you triggering this ADF pipeline?  Another way to accomplish this would be to setup an event trigger to run this pipeline whenever a file is created in that blob directory.
  Here are the steps to use the For-Each on files in a storage container.</p>

<ol>
<li>Set the Get Metadata argument to ""Child Items""
<a href=""https://i.stack.imgur.com/kvJHP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kvJHP.png"" alt=""child_items-screenshot""></a></li>
<li>In your For-Each set the Items to @activity('Get Metadata1').output.childitems</li>
</ol>

<p><a href=""https://i.stack.imgur.com/d8Zdw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d8Zdw.png"" alt=""for-each-image""></a></p>

<ol start=""3"">
<li>In the Source Dataset used in your Copy Activity create a parameter named FileName.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/66vbC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/66vbC.png"" alt=""filename Parameter""></a></p>

<ol start=""4"">
<li><p>Set the file path to use the parameter
<a href=""https://i.stack.imgur.com/rpRSx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rpRSx.png"" alt=""file path parameter""></a></p></li>
<li><p>On the Copy Activity set the FileName parameter to @item().name
<a href=""https://i.stack.imgur.com/Lfjuh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lfjuh.png"" alt=""enter image description here""></a></p></li>
</ol>

<p>To finish you'll need to set the sink to whatever you need.  That should do it.</p>
"
"59632050","Storage Accoung V2 access with firewall, VNET to data factory V2","<p>I have blob containers in storage account V2 having firewall settings with VNet and only allowed access to Microsoft Trusted Services. Now, as long as the firewall is restricting other services, I am unable to get test connection succeeded for data factory V2 while I am trying to set up Linked Service and I am getting an error:</p>

<p>Connection failed
ADLS Gen2 operation failed for: Operation returned an invalid status code 'Forbidden'. Account: 'mufgpresales'. FileSystem: 'filesystem'. ErrorCode: 'AuthorizationFailure'. Message: 'This request is not authorized to perform this operation.'. RequestId: 'fdc2149f-401f-0027-0b8f-c464ff000000'.. Operation returned an invalid status code 'Forbidden' Activity ID: 439d7a8c-254b-4af6-8697-1ff8770e1c40.</p>

<p>I read many posts about using Managed Identity and integration runtime. So, I used Managed Identity Application ID of Data factory and given all permissions on 1 blob container and still, I cannot get it to succeed. </p>

<p>It's hard for me to sale Integration runtime bit and an extra VM to host it. I am looking for a solution using AzureRuntime which I believe is possible but don't know exactly how. Any suggestions?</p>
","<azure-data-factory><azure-data-lake><azure-managed-identity><azure-storage-account><azure-data-lake-gen2>","2020-01-07 16:09:54","1488","0","1","59638992","<p><strong>Did you face the error like this?</strong></p>
<p><a href=""https://i.stack.imgur.com/yXisc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yXisc.png"" alt=""enter image description here"" /></a></p>
<p><strong>Cause:
The reason is your Data Factory don't have the access permission to your Storage Account.</strong></p>
<p><strong>Solution:</strong></p>
<p><a href=""https://i.stack.imgur.com/FjaVR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FjaVR.png"" alt=""enter image description here"" /></a></p>
<p><strong>Then click 'Add role assignment',</strong></p>
<p><a href=""https://i.stack.imgur.com/uvsyq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uvsyq.png"" alt=""enter image description here"" /></a></p>
<p><strong>With these steps, the connection must be work.</strong></p>
<p><a href=""https://i.stack.imgur.com/EZ7CY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EZ7CY.png"" alt=""enter image description here"" /></a></p>
<p><strong>Let me know if you have more doubts.</strong></p>
"
"59625210","Azure Data Factory automatically re-trigger failed pipeline","<p>I want to automatically Re-trigger a failed pipeline using the If Condition Activity (dynamic content).</p>

<p>Process :</p>

<ul>
<li>Pipeline 1 running at a schedule time with trigger 1 - works</li>
<li>If pipeline 1 fails, scheduled trigger 2 will run pipeline 2 - works </li>
<li><strong>Pipeline 2 should contain if condition to check if pipeline 1 failed</strong> - This is the Issue</li>
<li>If pipeline 1 failed, then rerun else ignore - need to fix this</li>
</ul>

<p>How can this be done?</p>

<p>All help is appreciated.</p>

<p>Thank you.</p>
","<azure><if-statement><pipeline><azure-data-factory>","2020-01-07 09:07:33","2324","2","2","59642205","<p>I can give you an idea,</p>

<p>For example. Your pipeline1 failed by some reasons. At this time, you can create a file to Azure Storage Blob.</p>

<p><a href=""https://i.stack.imgur.com/5dVeT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5dVeT.png"" alt=""enter image description here""></a></p>

<p>(Here is an example, you can use the activities what you want to use.)</p>

<p>Then create the trigger2 triggered by a blob been created.</p>
"
"59625210","Azure Data Factory automatically re-trigger failed pipeline","<p>I want to automatically Re-trigger a failed pipeline using the If Condition Activity (dynamic content).</p>

<p>Process :</p>

<ul>
<li>Pipeline 1 running at a schedule time with trigger 1 - works</li>
<li>If pipeline 1 fails, scheduled trigger 2 will run pipeline 2 - works </li>
<li><strong>Pipeline 2 should contain if condition to check if pipeline 1 failed</strong> - This is the Issue</li>
<li>If pipeline 1 failed, then rerun else ignore - need to fix this</li>
</ul>

<p>How can this be done?</p>

<p>All help is appreciated.</p>

<p>Thank you.</p>
","<azure><if-statement><pipeline><azure-data-factory>","2020-01-07 09:07:33","2324","2","2","67866994","<p>can't you do it with &quot;Execute Pipeline&quot; activity?
For example:</p>
<p>You create a simple pipeline named &quot;Rerun main pipeline&quot; and use &quot;Execute Pipeline&quot;  inside that and link it to &quot;main pipeline&quot;. Then in main pipeline, you add failure output and link it to &quot;Rerun main pipeline&quot;.</p>
"
"59618100","Get HD Insight Cluster on demand cluster name","<p>I've created a cluster On Demand in HD-Insight (Using Azure Data Factory). Therefore the name of the cluster is: prefix + timestamp. According to <a href=""https://stackoverflow.com/questions/33795090/how-to-avoid-hdinsight-on-demand-in-azure-data-factory-from-creating-a-new-conta"">this thread</a> there is no new functionality to fix the cluster name.</p>

<p>I want to submit a Livy job in a power shell as script in Azure Data Factory. But in order to do that i need to provide the cluster name. Exist any possibility to get the current name of the cluster in order to use to submit a job with Livy. </p>
","<azure><azure-data-factory>","2020-01-06 19:49:31","108","0","2","59725824","<p>I think the ask is to get the on demand cluster name , so that the cluster name can be passed on to other activities . I did reached out to some internals teams and as per them it does not look to be possible at this time .</p>
"
"59618100","Get HD Insight Cluster on demand cluster name","<p>I've created a cluster On Demand in HD-Insight (Using Azure Data Factory). Therefore the name of the cluster is: prefix + timestamp. According to <a href=""https://stackoverflow.com/questions/33795090/how-to-avoid-hdinsight-on-demand-in-azure-data-factory-from-creating-a-new-conta"">this thread</a> there is no new functionality to fix the cluster name.</p>

<p>I want to submit a Livy job in a power shell as script in Azure Data Factory. But in order to do that i need to provide the cluster name. Exist any possibility to get the current name of the cluster in order to use to submit a job with Livy. </p>
","<azure><azure-data-factory>","2020-01-06 19:49:31","108","0","2","66288003","<p>I am not sure whether you already got your solution and this is for reference for others.
There is no need to get the cluster name, you can create a pipeline in ADF(with on-demand HDI linked service) and then store your jar/script in Azure storage</p>
<p><a href=""https://i.stack.imgur.com/KN6Hc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KN6Hc.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/LoLU8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LoLU8.png"" alt=""enter image description here"" /></a></p>
<p>You can also trigger the pipeline using rest-api</p>
<pre><code>https://management.azure.com/subscriptions/{{sub_id}}/resourceGroups/{{resource_group_name}}/providers/Microsoft.DataFactory/factories/{{factory_name}}/pipelines/{{pipeline_name}}/createRun?api-version=2018-06-01
</code></pre>
<p>Or if you still need the Cluster name,</p>
<p>Then you can write a custom Powershell(using <code>Get-AzHDInsightCluster</code> along with String operations or if-else) to print the exact cluster name(you can add a prefix to the cluster so that it will be easy to use with String operations).You can automate it using Custom activity in ADF
<a href=""https://i.stack.imgur.com/XXENT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XXENT.png"" alt=""enter image description here"" /></a></p>
<p>Sample Powershell command</p>
<pre><code>(Get-AzHDInsightCluster|out-string) -split &quot;`n&quot; | select-string &lt;Cluster Name Prefix&gt; 
</code></pre>
<p><strong>Or</strong>,</p>
<pre><code> (Get-AzHDInsightCluster|out-string) -split &quot;`n&quot; | select-string -pattern &quot;Name                       :&quot; -caseSensitive
</code></pre>
"
"59590293","Long Running Azure Function returns 502, But the function is actually did the work","<p>I have a Azure Function which copies the data from MongoDB to Azure SQL  DB. When I invoke the Azure function from Azure Data Factory it keep on running. So I tried it form the Postman and it gives the below result after 8 minutes. But actually all the data are copied and if I check the application insight of the Azure Function it completed successfully. Why the Data Factory shows me that the Azure Function is keep on running even though it completed the execution successfully? how to avoid this?</p>

<pre><code>&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1- 
strict.dtd""&gt;
&lt;html xmlns=""http://www.w3.org/1999/xhtml""&gt;
&lt;head&gt;
    &lt;meta http-equiv=""Content-Type"" content=""text/html; charset=iso-8859-1""/&gt;
    &lt;title&gt;502 - Web server received an invalid response while acting as a gateway or proxy server.&lt;/title&gt;
    &lt;style type=""text/css""&gt;
        &lt;!--
 body{margin:0;font-size:.7em;font-family:Verdana, Arial, Helvetica, sans-serif;background:#EEEEEE;}
 fieldset{padding:0 15px 10px 15px;} 
h1{font-size:2.4em;margin:0;color:#FFF;}
h2{font-size:1.7em;margin:0;color:#CC0000;} 
h3{font-size:1.2em;margin:10px 0 0 0;color:#000000;} 
#header{width:96%;margin:0 0 0 0;padding:6px 2% 6px 2%;font-family:""trebuchet MS"", Verdana, sans-serif;color:#FFF;
background-color:#555555;}
#content{margin:0 0 0 2%;position:relative;}
.content-container{background:#FFF;width:96%;margin-top:8px;padding:10px;position:relative;}
--&gt;

    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div id=""header""&gt;
        &lt;h1&gt;Server Error&lt;/h1&gt;
    &lt;/div&gt;
    &lt;div id=""content""&gt;
        &lt;div class=""content-container""&gt;
            &lt;fieldset&gt;
                &lt;h2&gt;502 - Web server received an invalid response while acting as a gateway or proxy server.&lt;/h2&gt;
                &lt;h3&gt;There is a problem with the page you are looking for, and it cannot be displayed. When the Web server (while acting as a gateway or proxy) contacted the upstream content server, it received an invalid response from the content server.&lt;/h3&gt;
            &lt;/fieldset&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Thanks,</p>

<p>Joe</p>
","<azure><azure-functions><azure-data-factory>","2020-01-04 11:38:02","1543","1","1","59590312","<p>This should be your answer: <a href=""https://learn.microsoft.com/en/azure/data-factory/control-flow-azure-function-activity#timeout-and-long-running-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en/azure/data-factory/control-flow-azure-function-activity#timeout-and-long-running-functions</a></p>

<blockquote>
  <p>Azure Functions times out after 230 seconds regardless of the
  functionTimeout setting you've configured in the settings. For more
  information, see this article. To work around this behavior, follow an
  async pattern or use Durable Functions. The benefit of Durable
  Functions is that they offer their own state-tracking mechanism, so
  you won't have to implement your own.</p>
</blockquote>
"
"59586715","Azure ADF V2 - Activity on completion","<p>I'm working on creating a small project to track ETL logs. I've created a stored procedure with parameters and a custom SQL table to load the ETL logs. </p>

<p>Inside the ADF I have multiple activities. At the end I'm using stored procedure  activity with parameters mapped to ADF system variables like pipeline name, error details etc to log in the SQL table. </p>

<p><strong>Issue</strong>: whenever there's an error on an activity in middle, the pipeline fails and not touching the stored procedure activity. Like, say I have Copy1, Copy2, Copy3 and at last <code>ETLLog_StoredProcedure</code>. If Copy2 fails, the pipeline run stops there at Copy2 and stored procedure activity is not run. </p>

<p>I have connected all the Copy activities to <code>ETLLog_StoredProcedure</code> using Activity-On-Completion connections. Take a look at the picture below. </p>

<p><strong>Expectation</strong>: I need to call the stored procedure activity even if the pipeline fails/succeeds so that I can log the status of the pipeline.  </p>

<p><a href=""https://i.stack.imgur.com/j3Qrb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j3Qrb.png"" alt=""Sample ADF ETLLog""></a></p>
","<sql-server><azure><azure-sql-database><azure-data-factory>","2020-01-03 23:57:20","2230","0","2","59592520","<p>Data factory dependencies are used as an AND condition. This means that the stored procedure will be run once ALL of the 3 activities are &quot;completed&quot; (success or failure). But in your scenario, the second activity is failing and the third one is never running (not even failing) and that's why the Stored Procedure activity is not running.</p>
<p>You can achieve what you are looking for with this, changing the parameters for the stored procedure depending where the failure happened (or to show a success in the last one), for example:</p>
<p><a href=""https://i.stack.imgur.com/HOSOd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HOSOd.png"" alt=""Logging errors in ADF"" /></a></p>
<p>There are other ways to achieve this, but they require a bit more understanding of ADF variables and functions, this one is the simplest in my opinion.</p>
<p>Hope this helped!!</p>
"
"59586715","Azure ADF V2 - Activity on completion","<p>I'm working on creating a small project to track ETL logs. I've created a stored procedure with parameters and a custom SQL table to load the ETL logs. </p>

<p>Inside the ADF I have multiple activities. At the end I'm using stored procedure  activity with parameters mapped to ADF system variables like pipeline name, error details etc to log in the SQL table. </p>

<p><strong>Issue</strong>: whenever there's an error on an activity in middle, the pipeline fails and not touching the stored procedure activity. Like, say I have Copy1, Copy2, Copy3 and at last <code>ETLLog_StoredProcedure</code>. If Copy2 fails, the pipeline run stops there at Copy2 and stored procedure activity is not run. </p>

<p>I have connected all the Copy activities to <code>ETLLog_StoredProcedure</code> using Activity-On-Completion connections. Take a look at the picture below. </p>

<p><strong>Expectation</strong>: I need to call the stored procedure activity even if the pipeline fails/succeeds so that I can log the status of the pipeline.  </p>

<p><a href=""https://i.stack.imgur.com/j3Qrb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j3Qrb.png"" alt=""Sample ADF ETLLog""></a></p>
","<sql-server><azure><azure-sql-database><azure-data-factory>","2020-01-03 23:57:20","2230","0","2","65250072","<p>I added the picture for better understanding
<img src=""https://i.stack.imgur.com/c6UFa.png"" alt=""ADFPicturePipelineRun"" /></p>
<p>To have only one Stored Procedure call in the pipeline you can just add the option &quot;Skipped&quot;.</p>
<p>So in general the Activity &quot;Copy data3&quot; has 2 options to full fill the condition to execute Activity &quot;Stored procedure1&quot;, Completion OR Skipped.
As &quot;Copy data1&quot; and &quot;Copy data2&quot; both completed and &quot;Copy data3&quot; Skipped &quot;Stored procedure1&quot; is executed.</p>
"
"59582819","exclude azure data factory connections and integration runtime from azure devops sync","<p>So we have configured ADF to use GIT under DevOps.
Problem is our connection details are getting synced between dev\qa\master branches which are causing issues as each environment has its own SQL Servers.
Is there any way to keep connections and IR out of sync operation between branches?</p>
","<azure><azure-data-factory>","2020-01-03 17:28:06","917","2","2","59583578","<p>Connections rather have to be parameterized than removed from a deployment pipeline.</p>

<p>Parameterization can be done by using ""pipeline"" and ""variable groups"" <strong>variables</strong> </p>

<p>As an example, a pipeline variable <code>adf-keyvault</code> can be used to point to a rigt KeyVault instance  that belongs to a certain environment:</p>

<pre><code>adf-keyvault = ""adf-kv-yourProjectName-$(Environment)""
</code></pre>

<p>Variable <code>$Environment</code> is declared on a variable groups level, so each environment has own value mapped, for instance:</p>

<pre><code>$Environment = 'dev' #development
$Environment = 'stg' #staging
$Environment = 'prd' #production
</code></pre>

<p>Therefore the final value of <code>adf-keyvault</code>, depending on environment, resolves into:</p>

<pre><code>adf-keyvault = ""adf-kv-yourProjectName-dev"" 
adf-keyvault = ""adf-kv-yourProjectName-stg"" 
adf-keyvault = ""adf-kv-yourProjectName-prd"" 
</code></pre>

<p>And each Key Vault stores connection string to a database server in secret with the same name across environments. For instance:</p>

<pre><code>adf-sqldb-connectionstring = Server=123.123.123.123;Database=adf-sqldb-dev;User Id=myUsername;Password=myPassword;
</code></pre>

<p>Because an initial setup of CI/CD pipelines in Azure Data Factory can be complex in a first glance, I blogged recently a step-by-step guide about this topic: <a href=""https://www.alexvolok.com/2020/adf-devops-setting-up-ci-cd/"" rel=""nofollow noreferrer"">Azure Data Factory &amp; DevOps – Setting-up Continuous Delivery Pipeline</a></p>
"
"59582819","exclude azure data factory connections and integration runtime from azure devops sync","<p>So we have configured ADF to use GIT under DevOps.
Problem is our connection details are getting synced between dev\qa\master branches which are causing issues as each environment has its own SQL Servers.
Is there any way to keep connections and IR out of sync operation between branches?</p>
","<azure><azure-data-factory>","2020-01-03 17:28:06","917","2","2","59585247","<p><a href=""https://stackoverflow.com/a/59503805/8446413"">Look at this similar post</a> Which also asks how to use parameters for SQL connection information in ADF.</p>

<p>Your solution should also leverage Managed Identities for creating the access policies in the Key Vault this can be done via ARM.</p>

<p>One additional comment would be that the Linked Services would be where the parameter substitutions of these values would occur.</p>
"
"59582745","Connection failure to Salesforce Marketing Cloud using ADF","<p>We want to ingest data from SFMC using the in-built connector from Azure Data Factory.</p>

<p>The connector needs two things: <code>client ID</code> and <code>client secret</code>. The team managing SFMC has given me these values but when I try the, I get an error: </p>

<p><code>Authentication failed: Malformed response received from the authentication server that does not include the following required parameters for the next authentication step: ""Auth_AccessToken"" located at ""accessToken"" from the server response; ""Auth_expires"" located at ""expiresIn"" from the server response.</code></p>

<p>The SFMC dev team does not what the problem is. We have also raised Salesforce help ticket but they don't know what the problem is either.</p>

<p><strong>EDIT-1</strong></p>

<p>I created a POST request using an example <a href=""https://developer.salesforce.com/docs/atlas.en-us.mc-app-development.meta/mc-app-development/access-token-s2s.htm"" rel=""nofollow noreferrer"">here</a> and I am getting the response where I can see <code>access_token</code> and <code>expires_in</code>. But ADF is looking for <code>accessToken</code> and <code>expiresIn</code>.</p>
","<azure><salesforce><azure-data-factory><salesforce-marketing-cloud>","2020-01-03 17:21:41","823","0","1","59606137","<p>Based on the statement in the MS <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce-marketing-cloud#supported-capabilities"" rel=""nofollow noreferrer"">doc</a>:</p>

<blockquote>
  <p>The Salesforce Marketing Cloud connector supports OAuth 2
  authentication. It is built on top of the Salesforce Marketing Cloud
  REST API.</p>
</blockquote>

<p>Only client id and client secret need to be set in the ADF connector which is obtained from Marketing Cloud | Installed Packages. Please refer to this <a href=""https://developer.salesforce.com/docs/atlas.en-us.mc-apis.meta/mc-apis/index-api.htm"" rel=""nofollow noreferrer"">link</a>: You do not need a Marketing Cloud user to call the APIs, but you do need a Marketing Cloud user when creating an API integration in Installed Packages. The Marketing Cloud user must have the Installed Package | Administer permission.</p>

<p>According to your issue,it seems that the generation of access token fails.You need to check the above steps or settings of Admin Permission of Installed Packages for ADF.More details,please refer to:</p>

<p>1.<a href=""https://developer.salesforce.com/docs/atlas.en-us.mc-apis.meta/mc-apis/your-subdomain-tenant-specific-endpoints.htm"" rel=""nofollow noreferrer"">https://developer.salesforce.com/docs/atlas.en-us.mc-apis.meta/mc-apis/your-subdomain-tenant-specific-endpoints.htm</a></p>

<p>2.<a href=""https://developer.salesforce.com/docs/atlas.en-us.mc-app-development.meta/mc-app-development/install-packages.htm"" rel=""nofollow noreferrer"">https://developer.salesforce.com/docs/atlas.en-us.mc-app-development.meta/mc-app-development/install-packages.htm</a></p>
"
"59578308","Azure Mapping Data Flow : Not able to use blob storage dataset as a source","<p>I added a Azure Blob <code>dataset</code> as a source to a Azure mapping data flow, but am not able to view the preview as it is showing the below error :</p>
<blockquote>
<p>Dataset is using 'AzureStorage' linked service type, which is not supported in data flow.</p>
</blockquote>
<p>Given below is the dataset JSON :</p>
<pre><code>{
&quot;name&quot;: &quot;PIT_Input&quot;,
&quot;properties&quot;: {
    &quot;linkedServiceName&quot;: {
        &quot;referenceName&quot;: &quot;data_staging&quot;,
        &quot;type&quot;: &quot;LinkedServiceReference&quot;
    },
    &quot;annotations&quot;: [],
    &quot;type&quot;: &quot;DelimitedText&quot;,
    &quot;typeProperties&quot;: {
        &quot;location&quot;: {
            &quot;type&quot;: &quot;AzureBlobStorageLocation&quot;,
            &quot;container&quot;: &quot;dataflowpoc&quot;
        },
        &quot;columnDelimiter&quot;: &quot;,&quot;,
        &quot;escapeChar&quot;: &quot;\\&quot;,
        &quot;firstRowAsHeader&quot;: true,
        &quot;quoteChar&quot;: &quot;\&quot;&quot;
    },
    &quot;schema&quot;: []
  }
}
</code></pre>
<p><strong>data_staging</strong> is a linked service of type <strong>Azure Storage</strong>.</p>
<p>The documentation states that Azure Blob datasets can used as a source.
Please tell me what I'm doing wrong here.</p>
","<azure><azure-data-factory>","2020-01-03 12:03:17","1524","1","1","59606192","<p>According you <code>dataset</code> <code>JSON</code>, you just choose the container <code>dataflowpoc</code>, didn't specify the file.</p>
<p>You only could preview the file data which type is &quot;DelimitedText&quot;:
<a href=""https://i.stack.imgur.com/Yq9qC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yq9qC.png"" alt=""enter image description here"" /></a></p>
<p><code>Dataset</code>  Preview data, we can not preview all the data if files in the container with different schema:
<a href=""https://i.stack.imgur.com/qu118.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qu118.png"" alt=""enter image description here"" /></a></p>
<p>Dataset JSON:</p>
<pre><code>{
    &quot;name&quot;: &quot;DelimitedText1&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AzureBlobStorage1&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;DelimitedText&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobStorageLocation&quot;,
                &quot;container&quot;: &quot;containerleon&quot;
            },
            &quot;columnDelimiter&quot;: &quot;,&quot;,
            &quot;escapeChar&quot;: &quot;\\&quot;,
            &quot;firstRowAsHeader&quot;: false,
            &quot;quoteChar&quot;: &quot;\&quot;&quot;
        },
        &quot;schema&quot;: []
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p>But in Data Flow Data Preview, we can see all the data in the files:
<a href=""https://i.stack.imgur.com/lHAtf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lHAtf.png"" alt=""enter image description here"" /></a></p>
<p>I think your error just happened by accident, please refresh the Data Factory and try again.</p>
<p><strong>Update:</strong></p>
<p>The error is solved: &quot;I changed the type of the Linked Service from Azure storage to Azure blob storage, and it worked.&quot;</p>
<p>Hope this helps.</p>
"
"59575886","Can Azure Data Factory read data from Delta Lake format?","<p>We were able to read the files by specifiying the delta file source as a parquet dataset in ADF. Although this reads the delta file, it ends up reading all versions/snapshots of the data in the delta file instead of specifically picking up the most recent version of the delta data.</p>

<p>There is a similar question here - <a href=""https://stackoverflow.com/questions/57917908/is-it-possible-to-connect-to-databricks-deltalake-tables-from-adf?noredirect=1&amp;lq=1"">Is it possible to connect to databricks deltalake tables from adf</a></p>

<p>However, I am looking to read the delta file from an ADLS Gen2 location. Appreciate any guidance on this.</p>
","<azure-data-factory><delta-lake>","2020-01-03 09:19:41","4016","1","2","60394636","<p>I don't think you can do it as easily as reading from Parquet files today, because the Delta Lake files are basically transaction log files + snapshots in Parquet format. Unless you VACUUM every time before you read from a Delta Lake directory, you are going to end up readying the snapshot data like you have observed. </p>

<p>Delta Lake files do not play very nicely OUTSIDE OF Databricks. </p>

<p>In our data pipeline, we usually have a Databricks notebook that exports data from Delta Lake format to regular Parquet format in a temporary location. We let ADF read the Parquet files and do the clean up once done. Depending on the size of your data and how  you use it, this may or may not be an option for you. </p>
"
"59575886","Can Azure Data Factory read data from Delta Lake format?","<p>We were able to read the files by specifiying the delta file source as a parquet dataset in ADF. Although this reads the delta file, it ends up reading all versions/snapshots of the data in the delta file instead of specifically picking up the most recent version of the delta data.</p>

<p>There is a similar question here - <a href=""https://stackoverflow.com/questions/57917908/is-it-possible-to-connect-to-databricks-deltalake-tables-from-adf?noredirect=1&amp;lq=1"">Is it possible to connect to databricks deltalake tables from adf</a></p>

<p>However, I am looking to read the delta file from an ADLS Gen2 location. Appreciate any guidance on this.</p>
","<azure-data-factory><delta-lake>","2020-01-03 09:19:41","4016","1","2","62867828","<p>Time has passed and now ADF Delta support for Data Flow is in preview... hopefully it makes it into ADF native soon.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delta"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-delta</a></p>
"