QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"76138907","Filter to copy specific sets of data using Azure Data Factory","<p>I am starting to learn about Azure Data Factory. I was wondering if I can create a pipeline to copy only specific records from the origin data source to the destination.</p>
<p>Imagine it is only on-premise SQL database to Azure SQL Database, but instead of copying all the rows in a given dataset, I would like to pass a parameter to the pipeline and use it in a sort of where clause to decide which records I want to copy to the destination.</p>
<p>Is that something I can do with ADF, parameters and triggers or do I need something else?</p>
<p>Thank you in advance for your time</p>
","<azure><azure-data-factory><replication><database-replication>","2023-04-29 23:24:38","12","0","1","76139010","<p>You can definitely use <code>Copy Data</code> tool to specify the query for the source. Generally ADF is powerful enough to let you manipulate data in many ways.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-hello-world-copy-data-tool"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-hello-world-copy-data-tool</a></p>
"
"76133214","Automatically navigated to the specific branch of Azure Data Factory","<p>I have already setup Git repository and it contains more than  one branch.</p>
<p>I want to directly open specific branch of  ADF instance.</p>
<p>Is this possible ?  if so, Can  you please let me know the solution.</p>
<p> </p>
<p>For example  <a href=""https://adf.azure.com/en/home?factory=subscriptions/%7B%7Bsubscription_Id%7D%7D/resourceGroups/%7B%7BResourceGroup_Name%7D%7D/providers/Microsoft.DataFactory/factories/%7B%7BFactory_Name%7D%7D/Branch=%7B%7BBranch_Name%7D%7D"" rel=""nofollow noreferrer"">https://adf.azure.com/en/home?factory=subscriptions/{{subscription_Id}}/resourceGroups/{{ResourceGroup_Name}}/providers/Microsoft.DataFactory/factories/{{Factory_Name}}/Branch={{Branch_Name}}</a></p>
<p>I tried the below url path it is landing home page  instead of specific branch.
<a href=""https://adf.azure.com/en/home?factory=subscriptions/%7B%7Bsubscription_Id%7D%7D/resourceGroups/%7B%7BResourceGroup_Name%7D%7D/providers/Microsoft.DataFactory/factories/%7B%7BFactory_Name%7D%7D"" rel=""nofollow noreferrer"">https://adf.azure.com/en/home?factory=subscriptions/{{subscription_Id}}/resourceGroups/{{ResourceGroup_Name}}/providers/Microsoft.DataFactory/factories/{{Factory_Name}}</a></p>
","<azure><azure-data-factory>","2023-04-28 20:53:01","18","0","1","76133652","<p>You don't think you can reach to a specific branch directly.</p>
"
"76129215","What compression type should we choose to unzip .gz files using Copy activity on ADF?","<p>I'm trying to use ADF Copy Activity to unzip .gz files I have on my ADLS Gen2, which compression type should we choose from the drop-down list below? All of them failed to work for me.</p>
<p><a href=""https://i.stack.imgur.com/V6gbJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V6gbJ.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-04-28 11:40:54","41","0","1","76135578","<p>I agree with @<strong>Joel Cochran</strong>'s comment.</p>
<p>To Unzip the file of <code>.gz</code> type, give <code>gzip</code> for the <strong>Compression type</strong> in the source dataset of the copy activity.</p>
<p><img src=""https://i.imgur.com/VibabH0.png"" alt=""enter image description here"" /></p>
<p>And in sink dataset, give the target location and Compression type as <strong>None</strong>.</p>
<p><img src=""https://i.imgur.com/bZPrkC0.png"" alt=""enter image description here"" /></p>
<p><strong>My Unzipped files in target location after copy activity execution:</strong></p>
<p><img src=""https://i.imgur.com/BSoXCmH.png"" alt=""enter image description here"" /></p>
"
"76125536","How to change the type, to an array, of an item I'm passing to identify key column(s) in Data Factory?","<p>I have a lookup where I'm gathering tables from a couple of schemas. In that same lookup, I've got a column that identifies key columns needed for the Upsert operation on the Sink. I'm then doing a &quot;For Each&quot; and a copy activity inside.</p>
<p>Here's how I've set up things in there for my sink:
<a href=""https://i.stack.imgur.com/sfnbe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sfnbe.png"" alt=""enter image description here"" /></a></p>
<p>I tried running this, however, key columns needs to be in type Array. How would I accomplish changing the type here?</p>
<p>Sample output from lookup:</p>
<pre><code>+--------------+------------+-------------+
| table_schema | table_name | key_columns |
+--------------+------------+-------------+
| schema1      | table1     | key1        |
+--------------+------------+-------------+
| schema2      | table2     | key2, key3  |
+--------------+------------+-------------+
</code></pre>
","<azure-data-factory>","2023-04-28 00:48:05","33","0","1","76127232","<p>When you are upserting data in SQL and passing key columns, it should be array format and from you sample data it is in the comma separate column names to pass them as an array you need to use dynamic expression as <code>@split(item().key_columns,',' )</code> .</p>
<p>It will take string value of <code>key_columns</code> value and split it where it found a <strong>comma</strong> and create array of that values.</p>
<p><strong>See below images for reference:</strong></p>
<p><strong>My lookup Output:</strong></p>
<p><img src=""https://i.imgur.com/C3hhCG5.png"" alt=""enter image description here"" /></p>
<p><strong>Copy activity sink settings and output:</strong></p>
<p><img src=""https://i.imgur.com/vllOTPy.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/KPtQYoP.png"" alt=""enter image description here"" /></p>
"
"76124948","Whats the Easiest Way to Extract All Columns From a Complex Type Using ADF Mapping Data Flow","<p>I have a json file I am transforming with an ADF Data Flow.   The file has has some complex type properties which have a 1 to 1 relationship with the main row, it is not an array.  I know I can extract columns from the complex type using a derived column such as MyDerivedColumn1 = ComplexType.Column1, MyDerivedColumn2 = ComplexType.Column2, etc..; however there are a lot of columns in the complex type and the creation of a derived column for each is a bit tedious.  I was hoping someone could enlighten me to some magical button or function which could do all that work for me similar to when transforming a complex type in Power BI.  In PBI all I have to do is click a button at the column header of the complex type and it adds a Table.ExpandRecordColumn function to the M code and all the complex type columns are promoted to the top level record.</p>
<p>A simplified version of the json would look like this...</p>
<pre><code>{
    &quot;Col1&quot;: &quot;Col1Value&quot;,
    &quot;Col2&quot;: &quot;Col2Value&quot;,
    &quot;Col3&quot;: &quot;Col3Value&quot;,
    &quot;ComplexType&quot;: {
        &quot;ComplexCol1&quot;: &quot;ComplexType1Value&quot;,
        &quot;ComplexCol2&quot;: &quot;ComplexType2Value&quot;,
        &quot;ComplexCol3&quot;: &quot;ComplexType3Value&quot;
    }
}
</code></pre>
","<azure-data-factory>","2023-04-27 22:00:18","39","0","1","76129310","<p>You can use <strong>select</strong> transformation to extract these dynamically.</p>
<p>This is sample data from the above JSON:</p>
<p><img src=""https://i.imgur.com/ynszxjN.png"" alt=""enter image description here"" /></p>
<p>In select transformation, Go to <strong>Add mapping -&gt; Rule-based mapping</strong> and give it like below. Delete the original mapping for the <code>ComplexType</code> object so that you won't get that JSON in your output.</p>
<p><img src=""https://i.imgur.com/jaEP1xL.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/oG9ONQy.png"" alt=""enter image description here"" /></p>
"
"76123903","Azure data factory Error : InvalidTemplate, ErrorMessage=The template function 'linkedService' is not defined or not valid","<p>I need to use different Keyvaults for different environments (Dev/Test/UAT/Prod).</p>
<p>So, the name of the keyvault to be used is stored the Global Parameters, and is propagated from pipelines to other resources by means of parametrizing those resources.
It works for cases like
<strong>Pipeline-&gt;Linked Service</strong>, <strong>Pipeline-&gt;Dataset-&gt;Linked service</strong>,</p>
<p>But, I can't get it to work, when I have to access the Key vault from Linked service.  That is, for example, when a REST type of linked service needs to read the API Key form KeyVault secret.</p>
<p>I have defined the Linked service to the keyvault
<a href=""https://i.stack.imgur.com/eLyVD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eLyVD.png"" alt=""Linked service to azure Keyvaukt"" /></a></p>
<p>This way, the user will be able to obtain a secret from the Keyvault by making a web request
to <strong><a href=""https://kevaultName.vault.azure.net/"" rel=""nofollow noreferrer"">https://kevaultName.vault.azure.net/</a></strong>..</p>
<p>They 'kevaultName' name will be in the Global parameters and pipeline needing it would pass it as an argument for parameter.</p>
<p>But, what if I need a Linked Service to a database that needs to read the database password from the Key vault ?  I define a Linked service in the following manner and it also works as expected.  The Keyvault name will be provided as a parameter to this SQL DB linked service, propagated from the Pipeline. Where it is taken from Global parameter sections.</p>
<p><a href=""https://i.stack.imgur.com/06SZe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/06SZe.png"" alt=""Linked Service to Azure SQL Database"" /></a></p>
<p>But, here is where I am stuck.
I need a linked service to a REST api, where API Key would be taken from the Key vault secret.
So, when defining Linked Service, I provide that 'Authorization' header to be read from Key Vault accessed by linked service defined in the first picture above (The one that works correct in case of Pipeline linking to it by web activity).<br />
Here is the definition of the Linked Service:
<a href=""https://i.stack.imgur.com/7Jj54.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Jj54.png"" alt=""REST Linked service"" /></a></p>
<p>In order for it to work, I need to HARDCODE the key vault name in the &quot;KeyVaultLink&quot; linked service - like this
<a href=""https://i.stack.imgur.com/JkXHl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JkXHl.png"" alt=""enter image description here"" /></a>
But, I have no control over the name of a Keyvault in other environments.  So, I wanted to do pass a Linked Service Parameter to itlike this:</p>
<pre><code>@linkedService().AZKeyVaultName
</code></pre>
<p><a href=""https://i.stack.imgur.com/BatnX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BatnX.png"" alt=""enter image description here"" /></a></p>
<p>However, doing so - I get the the error message</p>
<pre><code>The parameters and expression cannot be resolved for schema operations. Error Message: {
&quot;message&quot;: &quot;ErrorCode=InvalidTemplate, ErrorMessage=The template function 'linkedService' is not defined or not valid.&quot;
}
</code></pre>
","<azure><azure-data-factory><azure-keyvault><linked-service>","2023-04-27 19:13:34","24","0","1","76136286","<blockquote>
<p>Azure data factory Error : InvalidTemplate, ErrorMessage=The template function 'linkedService' is not defined or not valid.</p>
</blockquote>
<p>The issue in might be causing because of incorrect referencing and add the vale within the curl braces <code>{ }</code>  so it will take it as string interpretation.</p>
<p>Rest API lined service:</p>
<p><img src=""https://i.imgur.com/hyHOaSu.png"" alt=""enter image description here"" /></p>
<p>key vault linked service:</p>
<p><img src=""https://i.imgur.com/JenLK2K.png"" alt=""enter image description here"" /></p>
<p>If you are still facing the same issue please raise a <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">support ticket</a> for the deeper investigation.</p>
"
"76122397","How to add a new column with non default delimiter using Azure Data Flows","<p>I have a text file with this format</p>
<p>A~B</p>
<p>E~F~G~H</p>
<p>K~L~P</p>
<p>How can I add a new string to every row like this</p>
<p>A~B~XXX</p>
<p>E~F~G~H~XXX</p>
<p>K~L~P~XXX</p>
<p>I am doing this so I will have XXX as the new column when I read the file later</p>
<p>I tried to add a new column using Derived Column, and the file looks like this</p>
<p>A~B,XXX</p>
<p>E~F~G~H,XXX</p>
<p>K~L~P,XXX</p>
<p>but later I can't read the file with two different delimiter (~ and , )</p>
","<azure><azure-functions><azure-data-factory>","2023-04-27 15:53:45","57","0","1","76129821","<blockquote>
<p>I want the above result in a csv file 1 column only.</p>
</blockquote>
<p>Use the derived column for the same input column.</p>
<p>This is source data from text file.</p>
<p><img src=""https://i.imgur.com/UAbO0Dg.png"" alt=""enter image description here"" /></p>
<p>Here, if we didn't provided any header, by default it will give the header as <code>Column_1</code>.
Use the same column in the derived column like below rather than creating new column.
To add the string use <code>concat(Column_1, '~XXX')</code> in it.</p>
<p><img src=""https://i.imgur.com/OWKeABp.png"" alt=""enter image description here"" /></p>
<p>Result in csv file after dataflow execution using pipeline.</p>
<p><img src=""https://i.imgur.com/cSynC0u.png"" alt=""enter image description here"" /></p>
"
"76119766","How to Replicate the mentioned Logic in Azure Data flow using Window function","<p><strong>I have input table like</strong><a href=""https://i.stack.imgur.com/UjNtF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UjNtF.png"" alt=""enter image description here"" /></a></p>
<p><strong>My requirement is to fetch the behind One row value from current row if the current row value is null .Iam doing this by mentioning</strong> <a href=""https://i.stack.imgur.com/1xv5E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1xv5E.png"" alt=""enter image description here"" /></a></p>
<p>In Sort ,am giving a surrogate key from 1 . This is giving me like the following :<a href=""https://i.stack.imgur.com/vMSVH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vMSVH.png"" alt=""enter image description here"" /></a></p>
<p><strong>BUT I need The Above output</strong>
:<a href=""https://i.stack.imgur.com/FEI7D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FEI7D.png"" alt=""enter image description here"" /></a></p>
<p><strong>So the window function should look my behind row in real time not from the existing Column like</strong> <a href=""https://i.stack.imgur.com/lIshY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lIshY.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/1vfGE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1vfGE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ALOll.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ALOll.png"" alt=""enter image description here"" /></a></p>
<p><strong><strong>I can achieve this by using</strong>  <strong>windows column: year = coalesce(year, last(year, true()))</strong> But ,this is not feasible if I want to look behind &gt;=2 Rows OR look Following &gt;=2 rows .Can anyone suggest how to achieve this .</strong></p>
","<azure><azure-data-factory><azure-databricks><azure-data-lake-gen2>","2023-04-27 11:20:03","29","0","1","76128506","<p>You can use a combination of conditional split transformation, window functions and join to get the desired result. Lets stay I have the starting data as shown below:</p>
<pre><code>year,month,product,sales,dummy,sk
2012,1,p1,100,1,1
2012,2,p2,200,1,2
,3,p3,300,1,3
,4,p4,400,1,4
,5,p5,500,1,5
,6,p6,600,1,6
,7,p7,700,1,7
,8,p8,800,1,8
,9,p9,900,1,9
,10,p10,1000,1,10
,11,p11,1100,1,11
,12,p12,1200,1,12
2013,1,q1,100,1,13
2013,2,q2,200,1,14
,3,q3,300,1,15
,4,q4,400,1,16
,5,q5,500,1,17
,6,q6,600,1,18
,7,q7,700,1,19
,8,q8,800,1,20
,9,q9,900,1,21
,10,q10,1000,1,22
,11,q11,1100,1,23
,12,q12,1200,1,24
</code></pre>
<p><img src=""https://i.imgur.com/QNVVuSD.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have applied transformations as shown in the below dataflow JSON to get the desired results:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow2&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                },
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source2&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;split1&quot;
                },
                {
                    &quot;name&quot;: &quot;window1&quot;
                },
                {
                    &quot;name&quot;: &quot;join1&quot;
                },
                {
                    &quot;name&quot;: &quot;select1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          year as short,&quot;,
                &quot;          month as short,&quot;,
                &quot;          product as string,&quot;,
                &quot;          sales as short,&quot;,
                &quot;          dummy as boolean,&quot;,
                &quot;          sk as short&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source1&quot;,
                &quot;source(output(&quot;,
                &quot;          year as short,&quot;,
                &quot;          month as short,&quot;,
                &quot;          product as string,&quot;,
                &quot;          sales as short,&quot;,
                &quot;          dummy as boolean,&quot;,
                &quot;          sk as short&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source2&quot;,
                &quot;source1 split(not(isNull(year)),&quot;,
                &quot;     disjoint: false) ~&gt; split1@(notNulls, nulls)&quot;,
                &quot;split1@notNulls window(over(dummy),&quot;,
                &quot;     asc(sk, true),&quot;,
                &quot;     new_rn = lead(sk,1,9999)) ~&gt; window1&quot;,
                &quot;source2, window1 join(source2@sk &gt;= split1@notNulls@sk&quot;,
                &quot;     &amp;&amp; source2@sk &lt;= minus(new_rn,1),&quot;,
                &quot;     joinType:'inner',&quot;,
                &quot;     matchType:'exact',&quot;,
                &quot;     ignoreSpaces: false,&quot;,
                &quot;     broadcast: 'left')~&gt; join1&quot;,
                &quot;join1 select(mapColumn(&quot;,
                &quot;          month = source2@month,&quot;,
                &quot;          product = source2@product,&quot;,
                &quot;          sales = source2@sales,&quot;,
                &quot;          year = split1@notNulls@year&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select1&quot;,
                &quot;select1 sink(validateSchema: false,&quot;,
                &quot;     partitionFileNames:['output.csv'],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     store: 'cache',&quot;,
                &quot;     format: 'inline',&quot;,
                &quot;     output: true,&quot;,
                &quot;     saveOrder: 1,&quot;,
                &quot;     partitionBy('hash', 1)) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
<ul>
<li>Here, both source1 and source2 use the same dataset i.e., the file data given above. The final output after applying these transformations is as shown below:</li>
</ul>
<pre><code>&quot;month&quot;,&quot;product&quot;,&quot;sales&quot;,&quot;year&quot;
&quot;1&quot;,&quot;p1&quot;,&quot;100&quot;,&quot;2012&quot;
&quot;2&quot;,&quot;p2&quot;,&quot;200&quot;,&quot;2012&quot;
&quot;3&quot;,&quot;p3&quot;,&quot;300&quot;,&quot;2012&quot;
&quot;4&quot;,&quot;p4&quot;,&quot;400&quot;,&quot;2012&quot;
&quot;5&quot;,&quot;p5&quot;,&quot;500&quot;,&quot;2012&quot;
&quot;6&quot;,&quot;p6&quot;,&quot;600&quot;,&quot;2012&quot;
&quot;7&quot;,&quot;p7&quot;,&quot;700&quot;,&quot;2012&quot;
&quot;8&quot;,&quot;p8&quot;,&quot;800&quot;,&quot;2012&quot;
&quot;9&quot;,&quot;p9&quot;,&quot;900&quot;,&quot;2012&quot;
&quot;10&quot;,&quot;p10&quot;,&quot;1000&quot;,&quot;2012&quot;
&quot;11&quot;,&quot;p11&quot;,&quot;1100&quot;,&quot;2012&quot;
&quot;12&quot;,&quot;p12&quot;,&quot;1200&quot;,&quot;2012&quot;
&quot;1&quot;,&quot;q1&quot;,&quot;100&quot;,&quot;2013&quot;
&quot;2&quot;,&quot;q2&quot;,&quot;200&quot;,&quot;2013&quot;
&quot;3&quot;,&quot;q3&quot;,&quot;300&quot;,&quot;2013&quot;
&quot;4&quot;,&quot;q4&quot;,&quot;400&quot;,&quot;2013&quot;
&quot;5&quot;,&quot;q5&quot;,&quot;500&quot;,&quot;2013&quot;
&quot;6&quot;,&quot;q6&quot;,&quot;600&quot;,&quot;2013&quot;
&quot;7&quot;,&quot;q7&quot;,&quot;700&quot;,&quot;2013&quot;
&quot;8&quot;,&quot;q8&quot;,&quot;800&quot;,&quot;2013&quot;
&quot;9&quot;,&quot;q9&quot;,&quot;900&quot;,&quot;2013&quot;
&quot;10&quot;,&quot;q10&quot;,&quot;1000&quot;,&quot;2013&quot;
&quot;11&quot;,&quot;q11&quot;,&quot;1100&quot;,&quot;2013&quot;
&quot;12&quot;,&quot;q12&quot;,&quot;1200&quot;,&quot;2013&quot;
</code></pre>
<p><img src=""https://i.imgur.com/a7CGv1i.png"" alt=""enter image description here"" /></p>
"
"76119175","ADF Pipeline to delete certain folders/subfolders based on regex","<p>I have several folders in azure storage explorer under this form.</p>
<pre><code>batch-input/bab/20203042_000000/**files
batch-input/bab/20202314_000000/**files
batch-input/bab/20211212_000000/**files
batch_input/aba/20203212_000000/**files
...
</code></pre>
<p>I am trying to create a pipeline in azure data factory to delete certain folders and everything inside them based on several user parameters. Currently I have a Delete activity linked to the corresponding dataset, with the container name: batch-input.</p>
<p>So far so good, but the part I'm having trouble with is the FolderAndSubFolderName value. I tried this:</p>
<pre><code>@concat(pipeline().parameters.ParamDomain, '/', pipeline().parameters.ParamDate, '*')
</code></pre>
<p>The <code>ParamDomain</code> in this case would be <code>bab</code> for example and <code>ParamDate</code> would be <code>2020</code>.</p>
<p>In theory this would be able to delete the 2 folders and all the files within from the exaple I posted above. In practive what happens is that when I check the execution I see this as my parameter value instead:</p>
<p><code>&quot;folderPath&quot;: &quot;batch-input/bab/2020^*&quot;</code></p>
<p>I'm not sure where the '^' comes from and tried multiple ways to remove it but without success.</p>
","<azure-data-factory>","2023-04-27 10:04:50","23","0","1","76119835","<p>Instead of putting the <code>*</code> in the absolute file path, better to go with one additional parameter declared in the dataset for the wild card i.e. <code>*</code>, and use it separately in <strong>file_name</strong> section instead of concatenating it.</p>
<p><a href=""https://i.stack.imgur.com/TlQSR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TlQSR.png"" alt=""enter image description here"" /></a></p>
"
"76118470","Create dynamic folders and sub folders inside blob via Azure dynamic expression","<p>I wanted to create folders and subfolders inside my container per this way inside the copyActivity. Everytime i run this i wanted the pipeline to create new folders per the date in the landing container.</p>
<p><code>size/oracle/finance/ddmmyyyy/ddmmyyyyhhmmss/file.csv</code></p>
<p>Here are the dataset variables that i declared.</p>
<p>landing conatiner = size</p>
<p>directory = /oracle/finance/</p>
<p>filename = <code>@concat('/oracle/finance/', formatDateTime(utcnow(), 'ddmmyyyy'), '/', formatDateTime(utcnow(), 'yyyyMMdd-HHmmss'), '/hey_', formatDateTime(utcnow(), 'ddmmyyyyhhmmss'), '.csv')</code></p>
<p>The adf dynamic expression is not producing the right results. I wanted the expression to create datestamp folder and a subfolder inside it with date time stamp and a csv file inside it.</p>
<p>what i tried</p>
<p><code>@concat('/oracle/finance/', formatDateTime(utcnow(), 'yyyyMMdd'), '/', formatDateTime(utcnow(), 'yyyyMMdd-HHmmss'), '/hey_', formatDateTime(utcnow(), 'yyyyMMdd-HHmmss'), '.csv')</code></p>
<p>what i was expecting</p>
<p>size/oracle/finance/ddmmyyyy/ddmmyyyyhhmmss/finance.csv
size/oracle/account/ddmmyyyy/ddmmyyyyhhmmss/account.csv</p>
<p>Every time the activity runs, it should create separate folders and sub folders based on time inside.</p>
","<azure><kotlin><azure-data-factory>","2023-04-27 08:47:04","29","0","1","76119855","<p>If you want <strong>to create datestamp folder and a subfolder inside it with date time stamp and a csv file inside it.</strong> you need to pass the values like</p>
<pre><code>landing container = size
directory = oracle/finance/
filename = @concat(formatDateTime(utcnow(), 'ddMMyyyy'), '/', formatDateTime(utcnow(), 'yyyyMMddHHmmss'), '/hey_', formatDateTime(utcnow(), 'ddMMyyyyhhmmss'), '.csv')
</code></pre>
<p><strong>There is no need to again include directory in file name</strong> see below reference:</p>
<p><img src=""https://i.imgur.com/6jP1hUT.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/0cMzsCA.png"" alt=""enter image description here"" /></p>
"
"76118332","How to do join query similar operation in azure datafactory dataflow component","<p>I am using dataflow component to perform the csv file transformation and we implemented source operation and select component to query the data. we would like to implement on top the component to implement similar SQL queryoperation.</p>
<p><a href=""https://i.stack.imgur.com/HeHof.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HeHof.png"" alt=""enter image description here"" /></a></p>
<pre><code>select col1,id,col1,col1,col1,col1,
col1,col1,col1,col1,col1
from tableA a
join tableB b
on a.id = b.id
left join tableC c
on a.id = c.id
</code></pre>
<p>I needs to implement the similar SQL operation in adf data flow.</p>
","<azure-data-factory>","2023-04-27 08:29:23","37","0","1","76120201","<ul>
<li>Take the join transformation in dataflow after source transformations.</li>
<li>Enter the left stream and right stream.</li>
<li>Enter the join type.</li>
<li>In join conditions, give the matching conditions.</li>
</ul>
<p>Refer the below screenshot.</p>
<p><img src=""https://i.imgur.com/boWGdJT.png"" alt=""enter image description here"" /></p>
<p>Reference: <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/data-factory/data-flow-join.md"" rel=""nofollow noreferrer"">azure-docs/data-flow-join.md at main · MicrosoftDocs/azure-docs (github.com)</a></p>
"
"76116418","Can I create a new column that gets its values by averaging values based on specified criteria in Azure Data Factory?","<p>I am trying to create a new column in my data set that is currently in Azure that gets its values from averaging all rows in a specified column if values in that row meet certain conditions, conditions that change based on which column the calculated value is going into.  I was able to do this in googlesheets however I cant seem to find a way to accomplish this in Azure Data Factory.</p>
<p>This is a simple example of what my google sheets equation did if that equation was in cell E2.</p>
<p>=averageifs(D:D,C:C,C2,A:A,A2,B:B,&quot;&lt;&quot;&amp;B2)</p>
<p>Essentially what my data is, is game statistics from all NFL Games in the past 10 years with each row being a specific game, and I am trying to create a calculated row that gives (in this case) the average number of points a team is scoring per game going into that game. So obviously the season needs to match the season of the current row, the week needs to be less than the current rows week, and the team name needs to match the current rows team name.</p>
<p>I have tried using Power Query however I am not super knowledgeable in the Power Query M Language and I couldnt find any great resources online for an AverageIf statement of this nature.</p>
<p>I have also tried using Azure Data Flows with a window transformation and utilizing the AvgIF() function however Im not sure how to reference other values in the current row. For example, I can hard code AvgIF(AND(season == 2022, week &lt;= 7, team_name == &quot;Broncos&quot;), points) however I dont know how to make the &quot;2022&quot;,&quot;7&quot;, and &quot;Broncos&quot; so they change depending on what values are in the current row.</p>
<p>Im more than willing to test out other cloud platforms or services if they are cheap and beginner-ish friendly, I just want to automate a data pipeline for running these transformations then inserting the data in an ML logistic regression model.</p>
","<azure><azure-data-factory><powerquery><data-transform>","2023-04-27 03:01:10","24","0","1","76118192","<p>In order to get the running average points of each team, you can use the <strong>windows transformation</strong> in dataflow. Below is the approach.</p>
<ul>
<li>Sample data is taken in source transformation.</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Season</th>
<th>week</th>
<th>team</th>
<th>point</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>RCB</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>RR</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>MI</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>RCB</td>
<td>4</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>RR</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>MI</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>RCB</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>RR</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>MI</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>RCB</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>RR</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>MI</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>RCB</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>RR</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>MI</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>RCB</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>RR</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>MI</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>Table:1 Sample input data</p>
<ul>
<li>Then <strong>Windows transformation</strong> is taken and Windows settings are given as,</li>
</ul>
<pre><code>1. over: season.team
2. sort: team ascending
3. range by: range: unbounded to current row
4. windows column: point_avg=avg(point)
</code></pre>
<p><img src=""https://user-images.githubusercontent.com/113445679/234786304-47ca583f-904e-4c56-8fbf-32c605d94dff.gif"" alt=""gif1"" /></p>
<p><strong>Output:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Season</th>
<th>week</th>
<th>team</th>
<th>point</th>
<th>point_avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>MI</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>MI</td>
<td>2</td>
<td>2.5</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>MI</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>RCB</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>RCB</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>RCB</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>RR</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>RR</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>RR</td>
<td>3</td>
<td>2.333333333</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>MI</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>MI</td>
<td>2</td>
<td>2.5</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>MI</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>RCB</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>RCB</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>RCB</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>RR</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>RR</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>RR</td>
<td>3</td>
<td>2.333333333</td>
</tr>
</tbody>
</table>
</div>
<p>table:2 window transformation result.</p>
"
"76111615","In Azure Data Factory how do I specify an SQL filter to select documents one day old","<p>In Azure Data Factory I have a copy activity. I have as a source a Cosmos NoSQL database. I am trying to select from the source files that are one day old using an SQL filter:</p>
<pre><code>@concat('SELECT * FROM c where c.createdDate&gt;&quot;',subtractFromTime(utcNow(),1,'Day'),'&quot;')
</code></pre>
<p><a href=""https://i.stack.imgur.com/UxBwu.png"" rel=""nofollow noreferrer"">Screenshot</a></p>
<p>However there are no documents selected</p>
","<azure-data-factory><azure-cosmosdb>","2023-04-26 13:53:52","63","0","1","76120189","<p>I tried with the following sample data in cosmos db</p>
<pre class=""lang-json prettyprint-override""><code>[
    {
        &quot;id&quot;: &quot;1&quot;,
        &quot;name&quot;: &quot;Rakesh&quot;,
        &quot;createdDate&quot;: &quot;2023-04-27T11:31:16.5438142Z&quot;
    },
    {
        &quot;id&quot;: &quot;2&quot;,
        &quot;name&quot;: &quot;Laddu&quot;,
        &quot;createdDate&quot;: &quot;2023-04-25T11:31:16.5438142Z&quot;
    },
    {
        &quot;id&quot;: &quot;3&quot;,
        &quot;name&quot;: &quot;Chinna&quot;,
        &quot;createdDate&quot;: &quot;2023-04-28T11:31:16.5438142Z&quot;
    },
    {
        &quot;id&quot;: &quot;4&quot;,
        &quot;name&quot;: &quot;Virat&quot;,
        &quot;createdDate&quot;: &quot;2023-04-20T11:31:16.5438142Z&quot;
    }
]
</code></pre>
<p>When I used the same query as yours, you can see I am able to get the desired result in the preview of the source.</p>
<p><img src=""https://i.imgur.com/pidVXCR.png"" alt=""enter image description here"" /></p>
<p>Make sure your data is correct as per the query. If still it results same, try with <strong>String interpolation</strong> in the query like below.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM c
where c.createdDate&gt;&quot;@{subtractFromTime(utcNow(),1,'Day')}&quot;
</code></pre>
"
"76111183","Change lookup object output to integer in Azure data factory","<p>Hei,</p>
<p>I have a lookup that has a number as output PeriodeID_var. I want to use it as a variable in a stored procedure called by a Copy activity. I get the following error:</p>
<p>The variable 'PeriodeID_var' of type 'String' cannot be initialized or updated with value of type 'Object'. The variable 'PeriodeID_var' only supports values of types 'String'.</p>
<p>Is is possible to change the type from object to string? And if so, how?</p>
<p>I have tried int but it didn't work.</p>
<p><a href=""https://i.stack.imgur.com/Aj9J1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aj9J1.png"" alt=""This is my pipeline"" /></a></p>
","<types><azure-data-factory>","2023-04-26 13:13:48","19","0","1","76111532","<p>Use below expression in set variable activity:</p>
<pre><code>@activity('Lookup1').output.firstRow.VarPeriodId
</code></pre>
"
"76110445","Is it possible to install Integration Runtime on virtual AWS server?","<p>I need to connect Azure data factory to an AWS virtual server which is on a private network. It is using Windows 2008R2 with SQl Server 2008 R2.
Therefore, it looks like I need to install Microsoft’s Integration Runtime service behind the firewall to allow self-hosted integrated runtime to run on the data factory.
I can't find anything online about whether this can be installed onto an AWS virtual server. Can this be done? and can anyone point me towards documentation / tutorials about how to do this?</p>
","<amazon-web-services><azure-data-factory>","2023-04-26 11:55:26","19","-1","1","76115107","<p>According with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory"" rel=""nofollow noreferrer"">Create and configure a self-hosted integration runtime</a> it is possible to install <strong>Integration Runtime</strong> on a virtual AWS server. Azure Data Factory supports deploying the Integration Runtime on a virtual machine in a customer-managed environment. You can create an EC2 Instance (following <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#prerequisites"" rel=""nofollow noreferrer"">Prerequisites</a> in AWS and then install the Integration Runtime on that machine.</p>
<p>Before you proceed with the installation, you should make sure that the virtual machine meets the Integration Runtime's system requirements. You will also need to configure the necessary ports on the NSG (following <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#ports-and-firewalls"" rel=""nofollow noreferrer"">Ports and firewall</a>) to ensure that it can communicate with the Azure Data Factory and any other data sources that you want to connect to.</p>
<p>Once you have configured the virtual machine, you can download and install the Integration Runtime software onto the virtual machine, and then register it with your Azure Data Factory.</p>
<p>Hoping it helps</p>
"
"76110275","Incremental Data copy for Postgres DB using Azure Data Factory","<p>I have a requirement, where i need to copy incremental data from one postgres DB to another postgres DB.</p>
<p>I thought i will use watermark column approach and then use look up and for each to achieve this , however problem comes because in postgres database sink , i don't get an option for upsert .</p>
<p>any idea how i can achieve incremental data load , if my target is postgres DB?</p>
<p>I created a pipelien with watermark look and used foreach to copy the data , however i am not sure what to do when sink is Postgress DB, I don't have upsert option avaiable</p>
","<azure><azure-data-factory>","2023-04-26 11:38:19","35","0","1","76130523","<p>There is no upsert option in azure SQL database for PostgreSQL sink dataset. For inserting the new records in the sink, you can follow the same approach you followed. In order to update the changes of the existing records, you can delete the existing record and insert the record in the sink. Below is the detailed approach.</p>
<ul>
<li><p>Source table and target table are taken as in below image.
<img src=""https://i.imgur.com/FIEji79.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Watermark table is created, and its initial value is given as  <code>1900-01-01</code>.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/vYrdJ93.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then in ADF, lookup activity is taken and source dataset in the lookup activity is for watermark table.</li>
</ul>
<p><img src=""https://i.imgur.com/qbNau6I.png"" alt=""enter image description here"" /></p>
<ul>
<li>Again lookup activity is taken to take values of id field (primary key column) from the source table. This is done because if the same ids are there in sink, those rows are to be deleted and new rows for those ids are to be inserted.</li>
</ul>
<p><img src=""https://i.imgur.com/G1dij1J.png"" alt=""enter image description here"" /></p>
<p>Query in Lookup2:</p>
<pre class=""lang-sql prettyprint-override""><code>select distinct id from src_employee where created_date&gt; '@{activity('Lookup1').output.firstRow.watermark_column}'
</code></pre>
<ul>
<li>The result of lookup2 is stored in a variable v1 of string type using set variable activity.</li>
</ul>
<p><img src=""https://i.imgur.com/P9gS4an.png"" alt=""enter image description here"" /></p>
<ul>
<li>Another variable v2 is set using set variable activity. Value for variable v2 is given as <code>@{replace(replace(replace(replace(variables('v1'),'{&quot;id&quot;:',''),'}',''),'[','('),']',')')}</code>.
This expression removes the characters <code>&quot;{&quot;id&quot;:</code>, <code>&quot;}&quot;</code> and replaces the characters  <code>[</code>,<code>]</code>  with <code>(</code>, <code>)</code> respectively.</li>
</ul>
<p><img src=""https://i.imgur.com/rIdFaCq.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then copy activity is taken and in source dataset, query is given as,</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>select distinct id from src_employee where created_date&gt; '@{activity('Lookup1').output.firstRow.watermark_column}'
</code></pre>
<ul>
<li>In sink dataset settings, sink dataset is given and pre-copy activity is written as</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>delete from tgt_employee where id in @{variables('v2')}
</code></pre>
<ul>
<li>Then you can update the watermark table with the new value using lookup activity or script activity based on the dataset you store the watermark table.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>update watermark_table
set watermark_column=(select max(created_date) from tgt_employee)
</code></pre>
<p><strong>Result:</strong></p>
<ol>
<li><p>When pipeline is run for the first time, data is copied from source to sink and watermark_column of watermark_table is updated as  <code>2023-04-26 00:00:00.0000000</code></p>
</li>
<li><p>Then data is changed in the source table. Row with id=3 is updated and new row with id=5 is inserted.
<img src=""https://i.imgur.com/NzmWxtD.png"" alt=""enter image description here"" /></p>
</li>
</ol>
<p><img src=""https://i.imgur.com/BFWxBTP.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/nQehV0q.png"" alt=""enter image description here"" /></p>
<p>Data is changed in sink table as per source.</p>
"
"76109943","Extract nested information from the runOutput parameter of databricks activity","<p>I have an ADF pipeline that contains a Databricks activity. The runOutput of this activity is as follows:</p>
<pre><code>&quot;runOutput&quot;: {
    &quot;index&quot;: {
        &quot;0&quot;: 0,
        &quot;1&quot;: 0,
        &quot;2&quot;: 1
    },
    &quot;t0&quot;: {
        &quot;0&quot;: &quot;2030-04-26 11:50:30.594&quot;,
        &quot;1&quot;: &quot;2030-04-26 11:50:30.594&quot;,
        &quot;2&quot;: &quot;2030-04-26 11:50:30.594&quot;
    },
    &quot;t1&quot;: {
        &quot;0&quot;: &quot;2030-04-26 11:55:30.594&quot;,
        &quot;1&quot;: &quot;2030-04-26 11:55:30.594&quot;,
        &quot;2&quot;: &quot;2030-04-26 11:55:30.594&quot;
    },
    &quot;name&quot;: {
        &quot;0&quot;: &quot;all&quot;,
        &quot;1&quot;: &quot;Alt&quot;,
        &quot;2&quot;: &quot;Ass&quot;
    },
    &quot;CA&quot;: {
        &quot;0&quot;: 6710065.65,
        &quot;1&quot;: 257580.69,
        &quot;2&quot;: 171109.65
    },
    &quot;nb_ligne&quot;: {
        &quot;0&quot;: 170506,
        &quot;1&quot;: 6500403,
        &quot;2&quot;: 4142539
    },
    &quot;nb_tickets&quot;: {
        &quot;0&quot;: 444766,
        &quot;1&quot;: 164764,
        &quot;2&quot;: 111471
    },
    &quot;CAparPanier&quot;: {
        &quot;0&quot;: 145.04,
        &quot;1&quot;: 1745.48,
        &quot;2&quot;: 145.41
    },
    &quot;CAparLigne&quot;: {
        &quot;0&quot;: 23.94,
        &quot;1&quot;: 35.96,
        &quot;2&quot;: 74.14
    },
    &quot;ligneParPanier&quot;: {
        &quot;0&quot;: 23.82,
        &quot;1&quot;: 34.91,
        &quot;2&quot;: 32.73
    }
}
</code></pre>
<p>My goal is to extract all the values for each key (index, t0, t1, name, CA, etc.) and append each set of value to an array, (ex: CA=[6710065.65,257580.69,171109.65]).</p>
<p>for more details:
I can access statically to each value of this output for example to get the first value of CA this is the corresponding expression:</p>
<pre><code>@activity('landing to raw mailing').output.runOutput.CA['0']
</code></pre>
<p>but i need dynamic solution.</p>
<p>To achieve this, I am considering two scenarios:</p>
<ol>
<li>The first scenario involves using a ForEach activity to loop through each key in the runOutput object. Within this activity, I would use a pipeline to invoke another ForEach activity to extract all the values for each key and append every one to an array using an Append Variable activity.</li>
<li>The second scenario involves storing the runOutput of the Databricks activity in a JSON file. I would then parse the JSON file to extract the desired data.</li>
</ol>
<p>However, I am unsure of how to implement the two scenarios.
example for the first scenario the expression of the foreach return 1 item (i must got 10). So how can i loop on each key?
this is the foreach expression:</p>
<pre><code>@array(activity('landing to raw mailing').output['runOutput'])
</code></pre>
","<json><azure><azure-data-factory><databricks>","2023-04-26 10:58:30","57","1","1","76119628","<p>I am able to achieve your requirement using your 1st method like below which will only work in this case.</p>
<p>I took the above JSON from lookup. For you, it will be databricks notebook <code>runOutout</code>JSON.</p>
<ul>
<li><p>Then, I converted the JSON into string and took substring by skipping first <code>{</code>.</p>
<p><img src=""https://i.imgur.com/Fj8jrRb.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then I have used split on the above string with <code>&quot;},&quot;</code>.
<img src=""https://i.imgur.com/HvgbNMs.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Give this array to ForEach and inside ForEach, get the key and append to an array.</p>
</li>
<li><p>After that, call another pipeline using Execute pipeline activity and pass the array using the key from the JSON as an array parameter to the child pipeline.
<img src=""https://i.imgur.com/Ya6rVIf.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In Child pipeline, create an array variable with values <code>[&quot;0&quot;,&quot;1&quot;,&quot;1&quot;]</code> and pass this to a ForEach.</p>
</li>
<li><p>Inside that ForEach, append the values of the array(array of key) to an array variable using the above keys.
<img src=""https://i.imgur.com/c6cI62g.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Outside the ForEach, use a <strong>set variable return</strong> to return the array from child to parent pipeline.</p>
</li>
<li><p>Now, in the parent pipeline ForEach activity, concat the array with key to build a key value pair where key is our key and value will be the array of it.</p>
</li>
</ul>
<p>After concatenation, it will give a string and outside ForEach, add <code>{</code> and <code>}</code> to it.</p>
<p><strong>Result</strong>:</p>
<p><img src=""https://i.imgur.com/clvpoWM.png"" alt=""enter image description here"" /></p>
<p>Variables in ADF do not support of object type currently. So, I am showing the output as string type here. When using it, you can convert it to object using <code>json()</code> function.</p>
<p>You can see the key value pair above. you can access the array by the keys list(give this to ForEach) we got from the append variable inside first ForEach.</p>
<p><strong>My Parent pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;JsonSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;JsonReadSettings&quot;
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Jsongen2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;convert json to string&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;jsonasstring&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@substring(string(activity('Lookup1').output.value[0]),1,sub(length(string(activity('Lookup1').output.value[0])),1))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;split string to array&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;convert json to string&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;splitarray&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@split(variables('jsonasstring'),'},')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;split string to array&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@variables('splitarray')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Append variable1&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;keys&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@replace(split(item(),':')[0],'\&quot;','')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Execute Pipeline1&quot;,
                            &quot;type&quot;: &quot;ExecutePipeline&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Append variable1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;pipeline&quot;: {
                                    &quot;referenceName&quot;: &quot;child&quot;,
                                    &quot;type&quot;: &quot;PipelineReference&quot;
                                },
                                &quot;waitOnCompletion&quot;: true,
                                &quot;parameters&quot;: {
                                    &quot;json&quot;: {
                                        &quot;value&quot;: &quot;@activity('Lookup1').output.value[0][replace(split(item(),':')[0],'\&quot;','')]&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;concat to temp&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Execute Pipeline1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;temp&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@concat(variables('final_string'),'\&quot;',replace(split(item(),':')[0],'\&quot;',''),'\&quot;:',activity('Execute Pipeline1').output.pipelineReturnValue.myarr,',')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;assign temp to final_str&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;concat to temp&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;final_string&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@variables('temp')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;final_json&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@concat('{',variables('final_string'),substring(variables('final_string'),0,sub(length(variables('final_string')),1)),'}')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;jsonasstring&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;splitarray&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;keys&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;final_string&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;temp&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;final_json&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>Child Pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;child&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@variables('numbers')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Append variable1&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;arr&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@pipeline().parameters.json[item()]&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;numbers&quot;,
                    &quot;value&quot;: [
                        &quot;0&quot;,
                        &quot;1&quot;,
                        &quot;2&quot;
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;return&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;pipelineReturnValue&quot;,
                    &quot;value&quot;: [
                        {
                            &quot;key&quot;: &quot;myarr&quot;,
                            &quot;value&quot;: {
                                &quot;type&quot;: &quot;Expression&quot;,
                                &quot;content&quot;: &quot;@variables('arr')&quot;
                            }
                        }
                    ],
                    &quot;setSystemVariable&quot;: true
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;json&quot;: {
                &quot;type&quot;: &quot;object&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;numbers&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;arr&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;temp&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
"
"76109666","How to Map the column with header csv file data in azure data factory","<p>I am reading the csv files data to using azure data factory dataflow but the csv files does not have any headers so that its loading as below like that data in my source projection column.</p>
<p><a href=""https://i.stack.imgur.com/0BLYK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0BLYK.png"" alt=""enter image description here"" /></a></p>
<p>I would like to map some of the column like as below set of position, I need to put decided column name in dataflow source projection column.</p>
<blockquote>
<p>['_c0', '_c2', '_c8', '_c9', '_c10', '_c11', '_c12', '_c6']</p>
</blockquote>
<blockquote>
<p>_c0 -&gt; customerName _c2 -&gt; cusid ..... _c12-&gt;depid</p>
</blockquote>
<p>I would like to add specfic column name needs to edit or rename dynamically. kindly assist, how do we implement.</p>
","<azure-data-factory>","2023-04-26 10:22:29","34","0","1","76110868","<p>I am showing you two ways of renaming the columns.</p>
<p>First is,</p>
<p><img src=""https://i.imgur.com/DKALHq7.png"" alt=""enter image description here"" /></p>
<p>This is my original dataset columns.</p>
<p>Add <strong>Select</strong> transformation to your dataflow,
and you will get option as below.</p>
<p><img src=""https://i.imgur.com/Z5jNyoN.png"" alt=""enter image description here"" /></p>
<p>Here, add your required names in <strong>Name as</strong> field, as shown above.
You will get result in data preview.</p>
<p><img src=""https://i.imgur.com/wqH2iLZ.png"" alt=""enter image description here"" /></p>
<p>The other way is using <strong>Derived Column</strong> schema modifier and <strong>Select</strong> transformation.</p>
<p>Add <strong>Derived Column</strong> to dataflow and you will get options as below.
<img src=""https://i.imgur.com/uxbOHA9.png"" alt=""enter image description here"" /></p>
<p>Here, you can find <strong>Add</strong> option, click on that and add required column name in <strong>Column</strong> field and wanted column from source dataflow in  <strong>Expression</strong> field as shown below.</p>
<p><img src=""https://i.imgur.com/55mj7Wk.png"" alt=""enter image description here"" /></p>
<p>After this add <strong>Select</strong> transformation and select the renamed derived columns as below.</p>
<p><img src=""https://i.imgur.com/dANtEVD.png"" alt=""enter image description here"" /></p>
<p>Here is the result.</p>
<p><img src=""https://i.imgur.com/YKfW4cP.png"" alt=""enter image description here"" /></p>
"
"76109116","ADF :: How to substitute files in Blob Storage","<p>In the Azure Cost Management &gt; Export, there is an export called <code>Daily export of month-to-date</code>:</p>
<p><a href=""https://i.stack.imgur.com/O7HFt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O7HFt.png"" alt=""enter image description here"" /></a></p>
<p>Such export happens daily and it's exported to a Blob Storage.</p>
<p><strong>PROBLEM:</strong> Because it's a month-to-day export, if today it's the 26 or April, today it will create a .csv file with all the Cost Management information from the 1th of April to the 26th.</p>
<p>But tomorrow another .csv file will be generated with all the costs from the 1th of April to the 27th.</p>
<p>This way I will have double informations!</p>
<p><strong>GOAL:</strong> an ideal solution would be that as soon as a new file is exported to that Storage Account the old file is deleted.</p>
<p>So there is always only 1 .csv file that contains all the data from month-to-date.</p>
<p><strong>SCOPE:</strong> Everything can be in scope:</p>
<ul>
<li>Azure Data Factory</li>
<li>Logic Apps</li>
<li>Automation Accounts</li>
<li>Power Automate</li>
</ul>
<p>...whatever works.</p>
","<azure-data-factory><azure-logic-apps><power-automate><azure-automation><azure-workflow-automation>","2023-04-26 09:22:28","56","0","2","76109558","<p>With blob storage, writing a file with the same name to the same location will automatically overwrite the existing one. I'm not too familiar with cost management, but you can use an ADF pipeline and set the sink as your blob storage account container, and ensure the file is the same name so it will automatically overwrite the existing e.g. fileApril.csv will overwrite fileApril.csv.
You can also set parameters in ADF to change naming convention to retain versioning e.g. using dynamic date() parameter should allow you to keep a file per month based on when the pipeline was run.</p>
"
"76109116","ADF :: How to substitute files in Blob Storage","<p>In the Azure Cost Management &gt; Export, there is an export called <code>Daily export of month-to-date</code>:</p>
<p><a href=""https://i.stack.imgur.com/O7HFt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O7HFt.png"" alt=""enter image description here"" /></a></p>
<p>Such export happens daily and it's exported to a Blob Storage.</p>
<p><strong>PROBLEM:</strong> Because it's a month-to-day export, if today it's the 26 or April, today it will create a .csv file with all the Cost Management information from the 1th of April to the 26th.</p>
<p>But tomorrow another .csv file will be generated with all the costs from the 1th of April to the 27th.</p>
<p>This way I will have double informations!</p>
<p><strong>GOAL:</strong> an ideal solution would be that as soon as a new file is exported to that Storage Account the old file is deleted.</p>
<p>So there is always only 1 .csv file that contains all the data from month-to-date.</p>
<p><strong>SCOPE:</strong> Everything can be in scope:</p>
<ul>
<li>Azure Data Factory</li>
<li>Logic Apps</li>
<li>Automation Accounts</li>
<li>Power Automate</li>
</ul>
<p>...whatever works.</p>
","<azure-data-factory><azure-logic-apps><power-automate><azure-automation><azure-workflow-automation>","2023-04-26 09:22:28","56","0","2","76109814","<p>I have reproduced in my environment and got expected results as below:</p>
<p><strong>Design:</strong></p>
<p><img src=""https://i.imgur.com/vDr7nrN.png"" alt=""enter image description here"" />
Clearly:
<img src=""https://i.imgur.com/dMUlc9y.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/y73Kp93.png"" alt=""enter image description here"" />
You can reproduce the above design with below code:</p>
<p><strong>Logic app code:</strong></p>
<pre><code>{
    &quot;definition&quot;: {
        &quot;$schema&quot;: &quot;https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#&quot;,
        &quot;actions&quot;: {
            &quot;Delete_blob_(V2)&quot;: {
                &quot;inputs&quot;: {
                    &quot;headers&quot;: {
                        &quot;SkipDeleteIfFileNotFoundOnServer&quot;: false
                    },
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['azureblob']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;delete&quot;,
                    &quot;path&quot;: &quot;/v2/datasets/@{encodeURIComponent(encodeURIComponent('AccountNameFromSettings'))}/files/@{encodeURIComponent(encodeURIComponent('/rithwik/',variables('ammo')))}&quot;
                },
                &quot;runAfter&quot;: {
                    &quot;For_each&quot;: [
                        &quot;Succeeded&quot;,
                        &quot;Failed&quot;
                    ]
                },
                &quot;type&quot;: &quot;ApiConnection&quot;
            },
            &quot;For_each&quot;: {
                &quot;actions&quot;: {
                    &quot;Append_to_array_variable&quot;: {
                        &quot;inputs&quot;: {
                            &quot;name&quot;: &quot;emo&quot;,
                            &quot;value&quot;: &quot;@triggerBody()?['LastModified']&quot;
                        },
                        &quot;runAfter&quot;: {
                            &quot;Compose&quot;: [
                                &quot;Succeeded&quot;
                            ]
                        },
                        &quot;type&quot;: &quot;AppendToArrayVariable&quot;
                    },
                    &quot;Append_to_array_variable_2&quot;: {
                        &quot;inputs&quot;: {
                            &quot;name&quot;: &quot;vammo&quot;,
                            &quot;value&quot;: &quot;@items('For_each')?['DisplayName']&quot;
                        },
                        &quot;runAfter&quot;: {
                            &quot;Append_to_array_variable&quot;: [
                                &quot;Succeeded&quot;
                            ]
                        },
                        &quot;type&quot;: &quot;AppendToArrayVariable&quot;
                    },
                    &quot;Compose&quot;: {
                        &quot;inputs&quot;: &quot;@items('For_each')?['LastModified']&quot;,
                        &quot;runAfter&quot;: {},
                        &quot;type&quot;: &quot;Compose&quot;
                    },
                    &quot;Condition&quot;: {
                        &quot;actions&quot;: {
                            &quot;Append_to_string_variable&quot;: {
                                &quot;inputs&quot;: {
                                    &quot;name&quot;: &quot;ammo&quot;,
                                    &quot;value&quot;: &quot;@variables('vammo')[0]&quot;
                                },
                                &quot;runAfter&quot;: {},
                                &quot;type&quot;: &quot;AppendToStringVariable&quot;
                            }
                        },
                        &quot;else&quot;: {
                            &quot;actions&quot;: {
                                &quot;Append_to_string_variable_2&quot;: {
                                    &quot;inputs&quot;: {
                                        &quot;name&quot;: &quot;ammo&quot;,
                                        &quot;value&quot;: &quot;@variables('vammo')[1]&quot;
                                    },
                                    &quot;runAfter&quot;: {},
                                    &quot;type&quot;: &quot;AppendToStringVariable&quot;
                                }
                            }
                        },
                        &quot;expression&quot;: {
                            &quot;and&quot;: [
                                {
                                    &quot;less&quot;: [
                                        &quot;@ticks(variables('emo')[0])&quot;,
                                        &quot;@ticks(variables('emo')[1])&quot;
                                    ]
                                }
                            ]
                        },
                        &quot;runAfter&quot;: {
                            &quot;Append_to_array_variable_2&quot;: [
                                &quot;Succeeded&quot;
                            ]
                        },
                        &quot;type&quot;: &quot;If&quot;
                    }
                },
                &quot;foreach&quot;: &quot;@body('Lists_blobs_(V2)')?['value']&quot;,
                &quot;runAfter&quot;: {
                    &quot;Initialize_variable_3&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;Foreach&quot;
            },
            &quot;Initialize_variable&quot;: {
                &quot;inputs&quot;: {
                    &quot;variables&quot;: [
                        {
                            &quot;name&quot;: &quot;emo&quot;,
                            &quot;type&quot;: &quot;array&quot;
                        }
                    ]
                },
                &quot;runAfter&quot;: {
                    &quot;Lists_blobs_(V2)&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;InitializeVariable&quot;
            },
            &quot;Initialize_variable_2&quot;: {
                &quot;inputs&quot;: {
                    &quot;variables&quot;: [
                        {
                            &quot;name&quot;: &quot;vammo&quot;,
                            &quot;type&quot;: &quot;array&quot;
                        }
                    ]
                },
                &quot;runAfter&quot;: {
                    &quot;Initialize_variable&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;InitializeVariable&quot;
            },
            &quot;Initialize_variable_3&quot;: {
                &quot;inputs&quot;: {
                    &quot;variables&quot;: [
                        {
                            &quot;name&quot;: &quot;ammo&quot;,
                            &quot;type&quot;: &quot;string&quot;
                        }
                    ]
                },
                &quot;runAfter&quot;: {
                    &quot;Initialize_variable_2&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;InitializeVariable&quot;
            },
            &quot;Lists_blobs_(V2)&quot;: {
                &quot;inputs&quot;: {
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['azureblob']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;get&quot;,
                    &quot;path&quot;: &quot;/v2/datasets/@{encodeURIComponent(encodeURIComponent('AccountNameFromSettings'))}/foldersV2/@{encodeURIComponent(encodeURIComponent('JTJmcml0aHdpaw=='))}&quot;,
                    &quot;queries&quot;: {
                        &quot;nextPageMarker&quot;: &quot;&quot;,
                        &quot;useFlatListing&quot;: false
                    }
                },
                &quot;metadata&quot;: {
                    &quot;JTJmcml0aHdpaw==&quot;: &quot;/rithwik&quot;
                },
                &quot;runAfter&quot;: {},
                &quot;type&quot;: &quot;ApiConnection&quot;
            }
        },
        &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
        &quot;outputs&quot;: {},
        &quot;parameters&quot;: {
            &quot;$connections&quot;: {
                &quot;defaultValue&quot;: {},
                &quot;type&quot;: &quot;Object&quot;
            }
        },
        &quot;triggers&quot;: {
            &quot;When_a_blob_is_added_or_modified_(properties_only)_(V2)_2&quot;: {
                &quot;evaluatedRecurrence&quot;: {
                    &quot;frequency&quot;: &quot;Second&quot;,
                    &quot;interval&quot;: 3
                },
                &quot;inputs&quot;: {
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['azureblob']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;get&quot;,
                    &quot;path&quot;: &quot;/v2/datasets/@{encodeURIComponent(encodeURIComponent('AccountNameFromSettings'))}/triggers/batch/onupdatedfile&quot;,
                    &quot;queries&quot;: {
                        &quot;checkBothCreatedAndModifiedDateTime&quot;: false,
                        &quot;folderId&quot;: &quot;JTJmcml0aHdpaw==&quot;,
                        &quot;maxFileCount&quot;: 1
                    }
                },
                &quot;metadata&quot;: {
                    &quot;JTJmcml0aHdpaw==&quot;: &quot;/rithwik&quot;
                },
                &quot;recurrence&quot;: {
                    &quot;frequency&quot;: &quot;Second&quot;,
                    &quot;interval&quot;: 3
                },
                &quot;splitOn&quot;: &quot;@triggerBody()&quot;,
                &quot;type&quot;: &quot;ApiConnection&quot;
            }
        }
    },
    &quot;parameters&quot;: {
        &quot;$connections&quot;: {
            &quot;value&quot;: {
                &quot;azureblob&quot;: {
                    &quot;connectionId&quot;: &quot;/subscriptions/b83c1ed3-c5b6-74c23f/resourceGroups/rbojja-/providers/Microsoft.Web/connections/azureblob&quot;,
                    &quot;connectionName&quot;: &quot;azureblob&quot;,
                    &quot;id&quot;: &quot;/subscriptions/b8b-b5ba-2074c23f/providers/Microsoft.Web/locations/eastus/managedApis/azureblob&quot;
                }
            }
        }
    }
}
</code></pre>
<p>Firstly have 1 blob like below:</p>
<p><img src=""https://i.imgur.com/yBNYvC4.png"" alt=""enter image description here"" /></p>
<p>Blob can be of any type.</p>
<p>Then Uploaded new Blob:</p>
<p><img src=""https://i.imgur.com/YJxrcnf.png"" alt=""enter image description here"" /></p>
<p>Then blob got deleted:</p>
<p><img src=""https://i.imgur.com/mNAtSzY.png"" alt=""enter image description here"" /></p>
<p>Output:</p>
<p><img src=""https://i.imgur.com/XCsxYhv.png"" alt=""enter image description here"" /></p>
<p><strong>Also added some other new steps to be more accurate:</strong>
<img src=""https://i.imgur.com/RWCjDAX.png"" alt=""enter image description here"" />
<strong>Code view:</strong></p>
<pre><code>{
    &quot;definition&quot;: {
        &quot;$schema&quot;: &quot;https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#&quot;,
        &quot;actions&quot;: {
            &quot;Delete_blob_(V2)&quot;: {
                &quot;inputs&quot;: {
                    &quot;headers&quot;: {
                        &quot;SkipDeleteIfFileNotFoundOnServer&quot;: false
                    },
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['azureblob']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;delete&quot;,
                    &quot;path&quot;: &quot;/v2/datasets/@{encodeURIComponent(encodeURIComponent('AccountNameFromSettings'))}/files/@{encodeURIComponent(encodeURIComponent('/rithwik/',variables('xyz')))}&quot;
                },
                &quot;runAfter&quot;: {
                    &quot;Initialize_variable_4&quot;: [
                        &quot;Succeeded&quot;,
                        &quot;Failed&quot;
                    ]
                },
                &quot;type&quot;: &quot;ApiConnection&quot;
            },
            &quot;For_each&quot;: {
                &quot;actions&quot;: {
                    &quot;Append_to_array_variable&quot;: {
                        &quot;inputs&quot;: {
                            &quot;name&quot;: &quot;emo&quot;,
                            &quot;value&quot;: &quot;@triggerBody()?['LastModified']&quot;
                        },
                        &quot;runAfter&quot;: {
                            &quot;Compose&quot;: [
                                &quot;Succeeded&quot;
                            ]
                        },
                        &quot;type&quot;: &quot;AppendToArrayVariable&quot;
                    },
                    &quot;Append_to_array_variable_2&quot;: {
                        &quot;inputs&quot;: {
                            &quot;name&quot;: &quot;vammo&quot;,
                            &quot;value&quot;: &quot;@items('For_each')?['DisplayName']&quot;
                        },
                        &quot;runAfter&quot;: {
                            &quot;Append_to_array_variable&quot;: [
                                &quot;Succeeded&quot;
                            ]
                        },
                        &quot;type&quot;: &quot;AppendToArrayVariable&quot;
                    },
                    &quot;Compose&quot;: {
                        &quot;inputs&quot;: &quot;@items('For_each')?['LastModified']&quot;,
                        &quot;runAfter&quot;: {},
                        &quot;type&quot;: &quot;Compose&quot;
                    },
                    &quot;Condition&quot;: {
                        &quot;actions&quot;: {
                            &quot;Append_to_string_variable&quot;: {
                                &quot;inputs&quot;: {
                                    &quot;name&quot;: &quot;ammo&quot;,
                                    &quot;value&quot;: &quot;@variables('vammo')[0]&quot;
                                },
                                &quot;runAfter&quot;: {},
                                &quot;type&quot;: &quot;AppendToStringVariable&quot;
                            }
                        },
                        &quot;else&quot;: {
                            &quot;actions&quot;: {
                                &quot;Compose_2&quot;: {
                                    &quot;inputs&quot;: &quot;@variables('vammo')[1]&quot;,
                                    &quot;runAfter&quot;: {},
                                    &quot;type&quot;: &quot;Compose&quot;
                                },
                                &quot;Set_variable&quot;: {
                                    &quot;inputs&quot;: {
                                        &quot;name&quot;: &quot;ammo&quot;,
                                        &quot;value&quot;: &quot;@{outputs('Compose_2')}&quot;
                                    },
                                    &quot;runAfter&quot;: {
                                        &quot;Compose_2&quot;: [
                                            &quot;Succeeded&quot;
                                        ]
                                    },
                                    &quot;type&quot;: &quot;SetVariable&quot;
                                }
                            }
                        },
                        &quot;expression&quot;: {
                            &quot;and&quot;: [
                                {
                                    &quot;less&quot;: [
                                        &quot;@ticks(variables('emo')[0])&quot;,
                                        &quot;@ticks(variables('emo')[1])&quot;
                                    ]
                                }
                            ]
                        },
                        &quot;runAfter&quot;: {
                            &quot;Append_to_array_variable_2&quot;: [
                                &quot;Succeeded&quot;
                            ]
                        },
                        &quot;type&quot;: &quot;If&quot;
                    }
                },
                &quot;foreach&quot;: &quot;@body('Lists_blobs_(V2)')?['value']&quot;,
                &quot;runAfter&quot;: {
                    &quot;Initialize_variable_3&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;Foreach&quot;
            },
            &quot;Initialize_variable&quot;: {
                &quot;inputs&quot;: {
                    &quot;variables&quot;: [
                        {
                            &quot;name&quot;: &quot;emo&quot;,
                            &quot;type&quot;: &quot;array&quot;
                        }
                    ]
                },
                &quot;runAfter&quot;: {
                    &quot;Lists_blobs_(V2)&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;InitializeVariable&quot;
            },
            &quot;Initialize_variable_2&quot;: {
                &quot;inputs&quot;: {
                    &quot;variables&quot;: [
                        {
                            &quot;name&quot;: &quot;vammo&quot;,
                            &quot;type&quot;: &quot;array&quot;
                        }
                    ]
                },
                &quot;runAfter&quot;: {
                    &quot;Initialize_variable&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;InitializeVariable&quot;
            },
            &quot;Initialize_variable_3&quot;: {
                &quot;inputs&quot;: {
                    &quot;variables&quot;: [
                        {
                            &quot;name&quot;: &quot;ammo&quot;,
                            &quot;type&quot;: &quot;string&quot;
                        }
                    ]
                },
                &quot;runAfter&quot;: {
                    &quot;Initialize_variable_2&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;InitializeVariable&quot;
            },
            &quot;Initialize_variable_4&quot;: {
                &quot;inputs&quot;: {
                    &quot;variables&quot;: [
                        {
                            &quot;name&quot;: &quot;xyz&quot;,
                            &quot;type&quot;: &quot;string&quot;,
                            &quot;value&quot;: &quot;@variables('ammo')&quot;
                        }
                    ]
                },
                &quot;runAfter&quot;: {
                    &quot;For_each&quot;: [
                        &quot;Succeeded&quot;,
                        &quot;Failed&quot;
                    ]
                },
                &quot;type&quot;: &quot;InitializeVariable&quot;
            },
            &quot;Lists_blobs_(V2)&quot;: {
                &quot;inputs&quot;: {
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['azureblob']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;get&quot;,
                    &quot;path&quot;: &quot;/v2/datasets/@{encodeURIComponent(encodeURIComponent('AccountNameFromSettings'))}/foldersV2/@{encodeURIComponent(encodeURIComponent('JTJmcml0aHdpaw=='))}&quot;,
                    &quot;queries&quot;: {
                        &quot;nextPageMarker&quot;: &quot;&quot;,
                        &quot;useFlatListing&quot;: false
                    }
                },
                &quot;metadata&quot;: {
                    &quot;JTJmcml0aHdpaw==&quot;: &quot;/rithwik&quot;
                },
                &quot;runAfter&quot;: {},
                &quot;type&quot;: &quot;ApiConnection&quot;
            }
        },
        &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
        &quot;outputs&quot;: {},
        &quot;parameters&quot;: {
            &quot;$connections&quot;: {
                &quot;defaultValue&quot;: {},
                &quot;type&quot;: &quot;Object&quot;
            }
        },
        &quot;triggers&quot;: {
            &quot;When_a_blob_is_added_or_modified_(properties_only)_(V2)_2&quot;: {
                &quot;evaluatedRecurrence&quot;: {
                    &quot;frequency&quot;: &quot;Second&quot;,
                    &quot;interval&quot;: 3
                },
                &quot;inputs&quot;: {
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['azureblob']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;get&quot;,
                    &quot;path&quot;: &quot;/v2/datasets/@{encodeURIComponent(encodeURIComponent('AccountNameFromSettings'))}/triggers/batch/onupdatedfile&quot;,
                    &quot;queries&quot;: {
                        &quot;checkBothCreatedAndModifiedDateTime&quot;: false,
                        &quot;folderId&quot;: &quot;JTJmcml0aHdpaw==&quot;,
                        &quot;maxFileCount&quot;: 1
                    }
                },
                &quot;metadata&quot;: {
                    &quot;JTJmcml0aHdpaw==&quot;: &quot;/rithwik&quot;
                },
                &quot;recurrence&quot;: {
                    &quot;frequency&quot;: &quot;Second&quot;,
                    &quot;interval&quot;: 3
                },
                &quot;splitOn&quot;: &quot;@triggerBody()&quot;,
                &quot;type&quot;: &quot;ApiConnection&quot;
            }
        }
    },
    &quot;parameters&quot;: {
        &quot;$connections&quot;: {
            &quot;value&quot;: {
                &quot;azureblob&quot;: {
                    &quot;connectionId&quot;: &quot;/subscriptions/b8074c23f/resourceGroups/bojja/providers/Microsoft.Web/connections/azureblob&quot;,
                    &quot;connectionName&quot;: &quot;azureblob&quot;,
                    &quot;id&quot;: &quot;/subscriptions/b83c1ed3-c574c23f/providers/Microsoft.Web/locations/eastus/managedApis/azureblob&quot;
                }
            }
        }
    }
}
</code></pre>
<p><img src=""https://i.imgur.com/ykGrIuW.png"" alt=""enter image description here"" /></p>
"
"76108719","Delete folders specific folder in adf","<p>I have the follwing set of activities ( see below image) which deletes the folders which are older than 5 days. My path looks like this bronze/D365/Snapshot/salestable/&quot;todaydate&quot;/*.parquet. Below pipeline deletes folders from <code>salestable</code> folder.</p>
<p><a href=""https://i.stack.imgur.com/cZkJh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cZkJh.png"" alt=""enter image description here"" /></a></p>
<p>In the get metadataactivity I have following settings:
<a href=""https://i.stack.imgur.com/EffU6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EffU6.png"" alt=""enter image description here"" /></a></p>
<p>where the folder_path is only pointing( hardcoded) SalesTable.</p>
<p>My goal is to make this dynamic and pass all the table names as I dont have only salestable there.</p>
<p>I tired to but the entire pipeline under ForEach loop and before for each loop another Getmetada activty which gives all the table names. Like this:</p>
<p><a href=""https://i.stack.imgur.com/5Y0J3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Y0J3.png"" alt=""enter image description here"" /></a></p>
<p>my problem is that I dont know how to pass the value from the Getmetadata_Of_ALL inside ForEach activity for get metadata activity dataset. So dataset filepath will become dynamic</p>
","<azure><azure-data-factory>","2023-04-26 08:32:05","34","0","1","76110544","<ul>
<li><p>You can use get metadata on <code>bronze/D365/Snapshot/</code> path as use field list as <code>child items</code> to get list of folders inside this path (since you have multiple folders along with salesTable).</p>
</li>
<li><p>The following is the demo of the same. I have folder structure as <code>data/a/b</code> inside which I have 3 folders (like salesTable and inside which I have todays date folder). I have used get Metadata on these and got the result as shown below:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/lfrvMo6.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now iterate through each of these folders. Inside for each, use another get metadata activity with dataset JSON as below:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;csv2&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;adls1&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;parameters&quot;: {
            &quot;folder_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;DelimitedText&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
                &quot;folderPath&quot;: {
                    &quot;value&quot;: &quot;a/b/@{dataset().folder_name}&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;fileSystem&quot;: &quot;data&quot;
            },
            &quot;columnDelimiter&quot;: &quot;,&quot;,
            &quot;escapeChar&quot;: &quot;\\&quot;,
            &quot;quoteChar&quot;: &quot;\&quot;&quot;
        },
        &quot;schema&quot;: []
    }
}
</code></pre>
<ul>
<li>Pass the value for folder_name parameter from the get metadata activity as <code>@item().name</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/Bc451nD.png"" alt=""enter image description here"" /></p>
<ul>
<li>This would give the contents of each of the folders returned in the first get metadata activity which can be further used.</li>
</ul>
<p><img src=""https://i.imgur.com/vBwVxMg.png"" alt=""enter image description here"" /></p>
"
"76107642","Json to Postgres Azure Datafactory","<p>I need to ingest the following in Postgres table. I have a Copy data set up but Groups and Questions are getting null values. What am I missing in my mapping? Checklists can have one or more groups. And, each group can have one or more qeustions.</p>
<p>The source of the data is an Api. Is there a better way to flatten the json string and then ingest it in the table?</p>
<p><a href=""https://i.stack.imgur.com/br8oV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/br8oV.png"" alt=""enter image description here"" /></a></p>
<pre><code>{
&quot;checklists&quot;: [
    {
        &quot;id&quot;: &quot;71Pbxb42&quot;,
        &quot;name&quot;: &quot;Emergency Planning Checklist&quot;,
        &quot;checklist_template_id&quot;: &quot;2QyMrEy2&quot;,
        &quot;modified_date&quot;: &quot;2022-12-12T03:31:14Z&quot;,
        &quot;created_date&quot;: &quot;2022-12-12T03:31:14Z&quot;,
        &quot;total&quot;: 13.00,
        &quot;max_total&quot;: 13.00,
        &quot;franchisee&quot;: {
            &quot;id&quot;: &quot;GYNnZEAW&quot;,
            &quot;name&quot;: &quot;Kindergartens - Huntly&quot;,
            &quot;external_id&quot;: &quot;CNIKA_5179&quot;
        },
        &quot;groups&quot;: [
            {
                &quot;id&quot;: &quot;WoNgLAQG&quot;,
                &quot;checklist_template_group_id&quot;: &quot;73evDRBW&quot;,
                &quot;name&quot;: &quot;Early childhood services and Mātauranga Ake &quot;,
                &quot;attachments&quot;: [],
                &quot;group_score&quot;: 13.00,
                &quot;max_score&quot;: 13.00,
                &quot;questions&quot;: [
                    {
                        &quot;id&quot;: &quot;GzRlJk4G&quot;,
                        &quot;question_type&quot;: &quot;select&quot;,
                        &quot;question&quot;: &quot;Team roles and responsibilities are clarified in the Emergency Procedure &quot;,
                        &quot;select&quot;: &quot;Yes&quot;,
                        &quot;score&quot;: 1.00,
                        &quot;max_score&quot;: 1.00,
                        &quot;comment&quot;: &quot;&quot;,
                        &quot;checklist_template_question_id&quot;: &quot;G66bDbmG&quot;,
                        
                            },
                    {
                        &quot;id&quot;: &quot;LzRlJm4G&quot;,
                        &quot;question_type&quot;: &quot;select&quot;,
                        &quot;question&quot;: &quot;Team roles and responsibilities are clarified in the Emergency Procedure &quot;,
                        &quot;select&quot;: &quot;Yes&quot;,
                        &quot;score&quot;: 1.00,
                        &quot;max_score&quot;: 1.00,
                        &quot;comment&quot;: &quot;&quot;,
                        &quot;checklist_template_question_id&quot;: &quot;G66bDbmG&quot;,
                        
                            }
                        ]
                    }
                ]
            }
        ]
    }
]
</code></pre>
<p>}</p>
","<azure-data-factory>","2023-04-26 06:05:21","31","0","1","76109004","<p>In copy activity, you cannot flatten the nested objects which are in the nested Json. In this case, you can use dataflow to flatten the data. Below is the approach.</p>
<ul>
<li>You can use rest dataset as a source in data flow source transformation.</li>
<li>Then take the flatten transformation and give the unroll by <code>[]checklists.group.question</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/43fblOM.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can take the input columns and map it with the output columns in mapping of flatten settings. In this demo, I took four input columns and mapped it to output columns. Below is the mapping settings.</li>
</ul>
<pre><code>checklistsid = checklists.id,
franchiseid = checklists.franchisee.id,
groupsid = checklists.groups.id,
questionid = checklists.groups.questions.id
</code></pre>
<p><img src=""https://i.imgur.com/CMeJ0pu.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Take the PostgreSQL dataset as a sink dataset.</p>
</li>
<li><p>By this way, you can flatten the Json and copy it to PostgreSQL.</p>
</li>
</ul>
"
"76107052","If using Azure Synapse Analytics in a new project - is it best to use Synapse Pipelines over ADF, or are there major considerations?","<p>We're moving from on-premise to Azure and will be using Azure Synapse. Our transformations will largely be lift-and-shift of SQL stored procs to start.</p>
<p>Looking at the high-level documentation, I'm not seeing any differences between ADF &amp; Azure Synapse Pipelines that would matter for our use case: <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/concepts-data-factory-differences"" rel=""nofollow noreferrer"">MS Overview</a></p>
<p>Given we'll be using Azure Synapse Analytics, does it make the most sense to use Synapse Pipelines over ADF, just to be a bit more streamlined, or are there other considerations?</p>
<p>i.e. could pricing be totally different, documentation/support (I assume ADF would be better...but maybe they're so similar it's a moot point), etc.</p>
","<azure><azure-data-factory><azure-synapse-pipeline>","2023-04-26 03:52:54","36","0","1","76107205","<p>Everything depends on how you want to design and architect because there are still couple of discrepancies between Synapse pipelines and ADF:</p>
<ol>
<li>IRs cannot be shared between Synapse workspaces unlike ADFs</li>
<li>AIRFlow integration and Power Query are in ADf and still not in synapse</li>
</ol>
<p><a href=""https://azurede.com/2021/03/27/difference-between-synapse-pipelines-and-azure-data-factory-pipelines/"" rel=""nofollow noreferrer"">https://azurede.com/2021/03/27/difference-between-synapse-pipelines-and-azure-data-factory-pipelines/</a></p>
"
"76105505","ADF Azure Data-Factory pipeline Concatenating syntax","<p>Basically I want to delete some data before loading new data into a table in the ADF pipeline. So I have to use below script as Pre-copy script of my copy data - sink.</p>
<p>DELETE FROM HCP_CALL_CNT
WHERE call_mo = '2023-04'</p>
<p>Instead of hard coding '2023-04', I want to use year &amp; date of utcNow() to construct this, but my below statement does not work.</p>
<p>DELETE FROM HCP_CALL_CNT
WHERE call_mo = @concat('@{utcNow('yyyy')}', '-', '@{utcNow('MM')}')</p>
<p>I get an error:
Failure happened on 'Sink' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42000] SQL compilation error:
syntax error line 2 at position 16 unexpected '@concat'.
syntax error line 2 at position 24 unexpected ''2023''.,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,''Type=System.Data.Odbc.OdbcException,Message=ERROR [42000] SQL compilation error:
syntax error line 2 at position 16 unexpected '@concat'.
syntax error line 2 at position 24 unexpected ''2023''.,Source=SnowflakeODBC_sb64.dll,'</p>
<p>Can you please help? Thanks in advance!
Anu</p>
<p>Tried:
DELETE FROM HCP_CALL_CNT
WHERE call_mo = @concat('@{utcNow('yyyy')}', '-', '@{utcNow('MM')}')</p>
<p>Expecting:
DELETE FROM HCP_CALL_CNT
WHERE call_mo = '2023-04'</p>
","<azure-pipelines><azure-data-factory><string-concatenation><copy-data>","2023-04-25 21:20:33","25","-1","1","76107026","<p>please use below logic :
DELETE FROM HCP_CALL_CNT WHERE call_mo = '@{utcnow('yyyy')}-@{utcnow('MM')}'</p>
<p><a href=""https://i.stack.imgur.com/5EDu2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5EDu2.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/UJ00H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UJ00H.png"" alt=""enter image description here"" /></a></p>
"
"76103941","How do I create a SQL Linked Service for Azure Datafactory using the Azure CLI commands?","<p>I am trying to get the Azure CLI commands working for creating linked services for Azure Datafactory. I was able to see what the request JSON looks like when it's being sent over the network via the Azure Portal / GUI and I get the following.</p>
<pre><code>{
    &quot;name&quot;: &quot;sqltest_script&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;SqlServer&quot;,
        &quot;typeProperties&quot;: {
            &quot;connectionString&quot;: &quot;Integrated Security=False;Data Source=placeholderServer;Initial Catalog=placeholderDB;User ID=placeholderUser;Password=placeholderPass&quot;
        },
        &quot;connectVia&quot;: {
            &quot;referenceName&quot;: &quot;placeholderRef&quot;,
            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
        }
    }
}
</code></pre>
<p>Using the <a href=""https://learn.microsoft.com/en-us/cli/azure/datafactory/linked-service?view=azure-cli-latest#az-datafactory-linked-service-create"" rel=""nofollow noreferrer"">Documentation</a>, I was able to see that we need to pass properties as a string when we are creating the Linked Service. So I took from the code block above and kept the &quot;type&quot;, &quot;typeProperties&quot;, and &quot;connectVia&quot; sections.</p>
<p>Following the format used in the documentation example, I converted the string to escape the quote characters.</p>
<pre><code>&quot;{\&quot;type\&quot;: \&quot;SqlServer\&quot;,\&quot;typeProperties\&quot;: {\&quot;connectionString\&quot;: \&quot;Integrated Security=False;Data Source=placeholderServer;Initial Catalog=placeholderDB;User ID=placeholderUser;Password=placeholderPass\&quot;},\&quot;connectVia\&quot;: {\&quot;referenceName\&quot;: \&quot;placeholderRef\&quot;,\&quot;type\&quot;: \&quot;IntegrationRuntimeReference\&quot;}}&quot;
</code></pre>
<p>Then using that string with our --properties part of the command the final command that is entered into the CLI is,</p>
<pre><code>az datafactory linked-service create --factory-name &quot;placeholderFactory&quot; --properties &quot;{\&quot;type\&quot;: \&quot;SqlServer\&quot;,\&quot;typeProperties\&quot;: {\&quot;connectionString\&quot;: \&quot;Integrated Security=False;Data Source=placeholderServer;Initial Catalog=placeholderDB;User ID=placeholderUser;Password=placeholderPass\&quot;},\&quot;connectVia\&quot;: {\&quot;referenceName\&quot;: \&quot;placeholderRef\&quot;,\&quot;type\&quot;: \&quot;IntegrationRuntimeReference\&quot;}}&quot; --name &quot;scriptTest&quot; --resource-group &quot;placeholderResource&quot;
</code></pre>
<p>When I try to run the command I get the following error,
<a href=""https://i.stack.imgur.com/FOwyY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FOwyY.png"" alt=""ParserError Image for Azure CLI command"" /></a></p>
<p>I believe I haven't created the --properties string correctly but am unsure of how to proceed. I was able to find some more <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/linked-services/create-or-update?tabs=HTTP#sqlserverlinkedservice"" rel=""nofollow noreferrer"">documentation</a> on the parameters that get passed in but I think my formatting for the string is incorrect.</p>
<h1>UPDATE:</h1>
<p>I was able to get the creation working when I pass in a file containing the JSON. The file contents are,</p>
<pre><code>{
    &quot;type&quot;: &quot;SqlServer&quot;,
    &quot;typeProperties&quot;: {
        &quot;connectionString&quot;: &quot;Integrated Security=False;Data Source=placeholderServer;Initial Catalog=placeholderDB;User ID=placeholderUser;Password=placeholderPass&quot;
    },
    &quot;connectVia&quot;: {
        &quot;referenceName&quot;: &quot;placeholderRef&quot;,
        &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
    }
}
</code></pre>
<p>and is named <strong>createProperties.json</strong>, then the new command that I am using is,</p>
<pre><code>az datafactory linked-service create --factory-name &quot;placeholderFactory&quot; --properties createProperties.json --name &quot;scriptTest&quot; --resource-group &quot;placeholderResource&quot;
</code></pre>
<p><strong>I still need help with getting the properties string working in the command</strong> but I have figured out how to use a file as input.</p>
","<json><azure><powershell><azure-data-factory><azure-cli>","2023-04-25 17:31:56","61","0","1","76109759","<p>Try below code:</p>
<pre><code>az datafactory linked-service create --factory-name &quot;datafactory-name&quot; --properties '{&quot;type&quot;: &quot;SqlServer&quot;,&quot;typeProperties&quot;: {&quot;connectionString&quot;: &quot;Integrated Security=False;Data Source=&lt;server name&gt;;Initial Catalog=&lt;database name&gt;;User ID=&lt;user id&gt;;Password=&lt;password&gt;&quot;}}' --name &quot;script&quot; --resource-group &quot;resource-name&quot;
</code></pre>
<p>This should work,</p>
<p>Result,</p>
<p><img src=""https://i.imgur.com/zWV7SqJ.png"" alt=""enter image description here"" /></p>
<p>If you still face any issue, try in <strong>Azure Bash</strong></p>
"
"76102448","How to set default parameter with data type object in ADF","<p>I have a generic pipeline where I want the cluster sizes to be configurable. The idea is to use an <strong>optional</strong> parameter where I can set a default pipeline in the generic pipeline, so I don't have to change many other pipelines that reference this generic pipeline. The parameter definition is shown here below. <a href=""https://i.stack.imgur.com/DH9Xs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DH9Xs.png"" alt=""Parameter overview of generic pipeline"" /></a></p>
<p>We now want to use this Object parameter in order to select the cluster size corresponding to our current environment. This is done as follows:</p>
<p><a href=""https://i.stack.imgur.com/1FBbu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1FBbu.png"" alt=""Use parameter in generic pipeline"" /></a></p>
<p>This goes well if I pass an explicit value for the Parameter from the outer (calling) pipeline, like so: <a href=""https://i.stack.imgur.com/gRVcp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gRVcp.png"" alt=""Call generic pipeline with explicit value configured"" /></a>.</p>
<p>I also want other (unchanged) pipelines that call the generic pipeline to continue running with a default value. Those are called as follows:</p>
<p><a href=""https://i.stack.imgur.com/invqz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/invqz.png"" alt=""Call generic pipeline without explicit value configured"" /></a></p>
<p>but that results in the following error:</p>
<p><a href=""https://i.stack.imgur.com/3HOrV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3HOrV.png"" alt=""enter image description here"" /></a></p>
<p>Please note: it is not possible to use Dynamic Content to set the default value for a Pipeline Parameter of type Object. Which adaptations should I make in order to correctly set the default value for clusterSizes <strong>as a json object</strong>?</p>
","<parameters><azure-data-factory>","2023-04-25 14:41:24","49","0","1","76108932","<p>I tried the above scenario with the below JSON as object type parameter in a child pipeline.</p>
<pre><code>{&quot;name&quot;:[&quot;Rakesh&quot;],&quot;Age&quot;:[22],&quot;nickname&quot;:[&quot;Laddu&quot;],&quot;luck&quot;:[]}
</code></pre>
<p>I have used <code>@pipeline().parameters.param1[variables('one')]</code> expression in the child pipeline and called it from Parent pipeline.</p>
<p><img src=""https://i.imgur.com/MVXas4y.png"" alt=""enter image description here"" /></p>
<p>And I got the same error.</p>
<p><img src=""https://i.imgur.com/yoPwbth.png"" alt=""enter image description here"" /></p>
<p>This is because, when we are not passing any values for the child pipeline parameters from parent pipelines and expecting to take default parameter values for it, <strong>it is taking the type of that value as a String</strong>.</p>
<p>Input for the Execute pipeline activity.</p>
<p><img src=""https://i.imgur.com/Rybt4Fc.png"" alt=""enter image description here"" /></p>
<p>You can see it took the value as string even though we defined it as an object type.</p>
<p>When I tried with other data types, it took correct types for <code>int</code> and <code>bool</code> types.</p>
<p><img src=""https://i.imgur.com/QhJGQR8.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/CXr1ATT.png"" alt=""enter image description here"" /></p>
<p>But, for the <code>Object</code>, <code>array</code> and <code>float</code> types it took type as string only. This might be a bug in ADF.</p>
<p>As you want the other pipelines to continue with default values, changing the generic pipeline might be the <strong>workaround</strong> in this case.</p>
<blockquote>
<p>Which adaptations should I make in order to correctly set the default value for clusterSizes <strong>as a json object</strong>?</p>
</blockquote>
<p>In your generic pipeline, when using that parameter convert that to object with the below expression.</p>
<pre><code>@json(string(pipeline().parameters.param1))[variables('one')]
</code></pre>
<p>With this expression, <strong>the pipelines which requires default values will be converted to JSON object</strong> and <strong>the pipelines which uses the dynamic content parameter values will be converted to string and again converted to JSON object</strong>.</p>
"
"76102376","How to populate a parameter dynamically?","<p>In Azure Data Factory, I created a Pipeline with the assistant, that copy data from multiple tables on SQL Server on-premise into Azure Storage Account.
Each table is created into a parquet file.
But when a new table is created on-premise, I must change manually the JSON parameter in the Pipeline to integrate the new Table.</p>
<p>How to make this dynamically?</p>
<p>I know that the process is something like this:</p>
<ol>
<li>Get the list of the tables on-premise</li>
<li>Store it into an Array in ADF</li>
<li>Use that Array in the ForEach component</li>
</ol>
<p>But I can't figure it out how to do it.</p>
","<azure-data-factory>","2023-04-25 14:33:35","24","0","1","76106953","<p>Good day!</p>
<p>Assuming you are using on Prem SQL server, you can have a lookup activity prior to For each activity and via Lookup activity, you can query Select table_schema,table_name from information_schema.tables where table_type='base table' to get the list of table names dynamically rather than manually maintaining a list of tables.</p>
"
"76102329","How do we get the Blob Index Tag of a file in Azure blob using Logic Apps or Azure data factory","<p>I am trying to segregate blobs(files) using blob Index tag. I need to know how to do it in Logic Apps and Azure data factory.</p>
<p>Based on this Blob Index Tag I need to decide whether to copy the file to destination Container. Please help.</p>
","<azure><azure-blob-storage><azure-data-factory><azure-logic-apps>","2023-04-25 14:29:29","55","0","1","76120082","<p>There is no direct action for this in logic apps. You can use Azure function which has HTTP trigger where you can perform <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/get-blob-tags?tabs=azure-ad"" rel=""nofollow noreferrer"">Get Blob Tags (REST API)</a>. After following the below steps I could able to get the desired results.</p>
<p>Here is the logic app flow that worked for me</p>
<p><img src=""https://i.imgur.com/fipniEx.png"" alt=""enter image description here"" /></p>
<p>For the flow inside for each loop, I have used XML to JSON conversion to apply the required conditions as below.</p>
<p><img src=""https://i.imgur.com/EyD9yr2.png"" alt=""enter image description here"" /></p>
<p>Code view of for condition inside for each loop</p>
<pre><code> &quot;expression&quot;: {
                            &quot;and&quot;: [
                                {
                                    &quot;not&quot;: {
                                        &quot;equals&quot;: [
                                            &quot;@body('Xml_to_Json_2')['Tags']['TagSet']&quot;,
                                            &quot;@null&quot;
                                        ]
                                    }
                                },
                                {
                                    &quot;equals&quot;: [
                                        &quot;@body('Xml_to_Json_2')['Tags']['TagSet']['Tag']['Value']&quot;,
                                        &quot;sample&quot;
                                    ]
                                },
                                {
                                    &quot;equals&quot;: [
                                        &quot;@body('Xml_to_Json_2')['Tags']['TagSet']['Tag']['Key']&quot;,
                                        &quot;id&quot;
                                    ]
                                }
                            ]
                        }
</code></pre>
<p>Below is the sample function code I used.</p>
<pre><code>import datetime
import hmac
import hashlib
import base64
import azure.functions as func
import requests

def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    name = req.params.get('name')
    if not name:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            name = req_body.get('name')
    STORAGE_ACCOUNT_NAME = '&lt;STORAGEACCOUNTNAME&gt;'
    STORAGE_ACCOUNT_KEY = &quot;&lt;STORAGEACCOUNTKEY&gt;&quot;
    CONTAINER_NAME = '&lt;CONTAINERNAME&gt;'
    REQUEST_VERSION = '2020-04-08'
    REQUEST_DATE = datetime.datetime.utcnow().strftime('%a, %d %b %Y %H:%M:%S GMT')
    CANONICALIZED_HEADERS = f'x-ms-date:{REQUEST_DATE}\nx-ms-version:{REQUEST_VERSION}\n'
    CANONICALIZED_RESOURCE = f'/{STORAGE_ACCOUNT_NAME}/{CONTAINER_NAME}/{name}\ncomp:tags'

    VERB = 'GET'
    Content_Encoding = ''
    Content_Language = ''
    Content_Length = ''
    Content_MD5 = ''
    Content_Type = ''
    Date = ''
    If_Modified_Since = ''
    If_Match = ''
    If_None_Match = ''
    If_Unmodified_Since = ''
    Range = ''
    CanonicalizedHeaders = CANONICALIZED_HEADERS
    CanonicalizedResource = CANONICALIZED_RESOURCE

    STRING_TO_SIGN = (VERB + '\n' + Content_Encoding + '\n' + Content_Language + '\n' +
                Content_Length + '\n' + Content_MD5 + '\n' + Content_Type + '\n' +
                Date + '\n' + If_Modified_Since + '\n' + If_Match + '\n' +
                If_None_Match + '\n' + If_Unmodified_Since + '\n' + Range + '\n' +
                CanonicalizedHeaders + CanonicalizedResource)

    signature = base64.b64encode(hmac.new(base64.b64decode(STORAGE_ACCOUNT_KEY), msg=STRING_TO_SIGN.encode('utf-8'), digestmod=hashlib.sha256).digest()).decode()
    auth_header = f'SharedKey {STORAGE_ACCOUNT_NAME}:{signature}'

    request_url = f'https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/{CONTAINER_NAME}/{name}?comp=tags'

    request_headers = {
    'x-ms-date': REQUEST_DATE,
    'x-ms-version': REQUEST_VERSION,
    'Authorization': auth_header
    }

    response = requests.get(request_url, headers=request_headers).text
    
    return func.HttpResponse(response)
</code></pre>
<p>RESULTS:</p>
<p><img src=""https://i.imgur.com/8S1eWaR.png"" alt=""enter image description here"" /></p>
"
"76101923","Capture ADF user name in SQL Server when ADF user makes changes to table","<p>I have a pipeline in Azure Data Factory (ADF) that I use to update a table in SQL Server.</p>
<p>Now I want to capture the details like timestamp &amp; user name who ran the pipeline that caused the change in the database table.</p>
<p>Please help me doing so.</p>
<p><strong>NOTE</strong>: I tried to add triggers in SQL Server, but that is not capturing user details if the data is changed using an ADF pipeline. It only captures user name if it's done manually</p>
","<sql-server><azure-data-factory>","2023-04-25 13:49:12","26","0","1","76102267","<p>As such ADF doesnt have out of box feature to identify who manually trigerred the ADF pipeline.
But to identify it in an automated way , you can use the REST API to get the Activity Logs at ADF level/log analytics enablement.</p>
<p>Sample reference:
<a href=""https://stackoverflow.com/questions/66678256/find-who-triggered-azure-data-factory-pipeline-adf#:%7E:text=Go%20to%20particular%20ADF%20Resource,by%20whom%20it%20was%20done.&amp;text=Great"">https://stackoverflow.com/questions/66678256/find-who-triggered-azure-data-factory-pipeline-adf#:~:text=Go%20to%20particular%20ADF%20Resource,by%20whom%20it%20was%20done.&amp;text=Great</a>!</p>
"
"76101174","Access denied to server in File System Linked Service in Data Factory","<p>I'm trying to create a File System Linked Service in Data Factory in order to access a folder (\\serverName\c$\folder) using Self-Hosted Integration Runtime, even though I inserted an user who has &quot;Full Control&quot; permission on this specific folder, when I test connection, it returns the following error message:</p>
<p><em>Please consider I changed &quot;serverName&quot; with the proper server name, which is the same one I installed the IR.</em></p>
<blockquote>
<p>The value of the property '' is invalid: 'Access to serverName is
denied, resolved IP address is ::1, network type is OnPremise'. Access
to serverName is denied, resolved IP address is ::1, network type is
OnPremise</p>
</blockquote>
<p>What really concerns me is that this message says the &quot;property '' is invalid&quot;, how am I supposed to solve this?</p>
<p>Thanks in advance.</p>
","<security><server><filesystems><azure-data-factory><linked-service>","2023-04-25 12:28:20","73","0","2","76110968","<blockquote>
<p>The value of the property '' is invalid: 'Access to serverName is denied, resolved IP address is ::1, network type is OnPremise'.</p>
</blockquote>
<p>Looking at the error message. The Self-Hosted Integration Runtime cannot access the servername. multiple factors, including wrong permissions, connectivity problems on the network or configuration settings, might be causing for this.</p>
<p>To resolve this, you can try below solutions:</p>
<ul>
<li><p>Check to see if the user account you are using in linked service has the access on that particular folder/file. check if you are able to open it with particular user.</p>
</li>
<li><p>Make that the Self-Hosted Integration Runtime is installed on a machine that can connect to the server. To validate, you can try pinging the server from the device with <code>ping servername</code> if you are getting <code>Ping request could not find host serverName. Please check the name and try again.</code> there might be connectivity issue with server.</p>
</li>
<li><p>Make sure your servername username and password is correct.</p>
</li>
</ul>
<p>Also, Check this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/security-and-access-control-troubleshoot-guide?tabs=data-factory#invalid-or-empty-authentication-key-issue-after-public-network-access-is-disabled"" rel=""nofollow noreferrer"">microsoft documentation</a> regarding connectivity errors in self-hosted integration runtime</p>
<p>If still issue persist you can raise a support ticket <a href=""https://azure.microsoft.com/en-in/support/create-ticket/"" rel=""nofollow noreferrer"">here</a> for more investigation.</p>
"
"76101174","Access denied to server in File System Linked Service in Data Factory","<p>I'm trying to create a File System Linked Service in Data Factory in order to access a folder (\\serverName\c$\folder) using Self-Hosted Integration Runtime, even though I inserted an user who has &quot;Full Control&quot; permission on this specific folder, when I test connection, it returns the following error message:</p>
<p><em>Please consider I changed &quot;serverName&quot; with the proper server name, which is the same one I installed the IR.</em></p>
<blockquote>
<p>The value of the property '' is invalid: 'Access to serverName is
denied, resolved IP address is ::1, network type is OnPremise'. Access
to serverName is denied, resolved IP address is ::1, network type is
OnPremise</p>
</blockquote>
<p>What really concerns me is that this message says the &quot;property '' is invalid&quot;, how am I supposed to solve this?</p>
<p>Thanks in advance.</p>
","<security><server><filesystems><azure-data-factory><linked-service>","2023-04-25 12:28:20","73","0","2","76112606","<p>I just had a similar issue: Accessing a (shared -&gt; only read access) folder from an Azure VM on which I had installed the SHIR.</p>
<ul>
<li>According to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#sample-linked-service-and-dataset-definitions"" rel=""nofollow noreferrer"">Microsoft documentation</a> you have to enable the access to the local machine for copying files. For this I opened the PowerShell on the VM and entered the following two commands (using IR version 5.0; for details to this command see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#set-up-an-existing-self-hosted-ir-via-local-powershell"" rel=""nofollow noreferrer"">here</a>) to run the dmgcmd.exe:</li>
</ul>
<pre><code> cd &quot;C:\Program Files\Microsoft Integration Runtime\5.0\Shared\&quot;
 .\dmgcmd.exe -DisableLocalFolderPathValidation
</code></pre>
<ul>
<li>Then I changed the path from \serverName\c$\folder to C:/folder in the linked service</li>
</ul>
<p>Maybe that works for you too.</p>
"
"76099564","Mapping Data Flow Azure","<p>I am flattening a Json File to CSV file using mapping data flow in Azure data factory, the issue is that files is placed in storage container which is behind firewall. As it looks Only Auto Resolve integration runtime is supported in Mapping Data flow, while I cant create a linked service with AutoResolve integration runtime to connect  storage container, Am I missing something here, do we have a workaround for this.</p>
","<azure-data-factory><linked-service><azure-mapping-data-flow>","2023-04-25 09:23:14","28","0","2","76100756","<p>Azure Dataflows do not support Self hosted IR's. The following image is the reference of the same:</p>
<p><img src=""https://i.imgur.com/c16RwTO.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>As a work around, you can store the csv file in a storage container which does not have private firewall settings. One this is done, you can copy this file to required private container using copy data with Self Hosted IR.</p>
</li>
<li><p>Another alternative would be to use another service for transformations like Databricks notebook to complete the operation directly.</p>
</li>
</ul>
"
"76099564","Mapping Data Flow Azure","<p>I am flattening a Json File to CSV file using mapping data flow in Azure data factory, the issue is that files is placed in storage container which is behind firewall. As it looks Only Auto Resolve integration runtime is supported in Mapping Data flow, while I cant create a linked service with AutoResolve integration runtime to connect  storage container, Am I missing something here, do we have a workaround for this.</p>
","<azure-data-factory><linked-service><azure-mapping-data-flow>","2023-04-25 09:23:14","28","0","2","76103701","<p>Use the managed vnet option in the Azure IR which will allow you to connect to the storage account using data flows by creating a private link from ADF to your storage.</p>
"
"76099317","Handling special characters in data in Azure Data Factory Copy Activity while writing data into a CSV file","<p>Problem Summary: Records are breaking into multiple lines after ingestion into CSV due to special characters present in data.</p>
<p>I am trying to ingest data from a SQL Database to ADLS using a copy activity in ADF.
The data contains special characters like &lt; &quot;,$,/n,/r,\ &gt;</p>
<p>The resultant CSV file has few records broken into multiple lines.
I have tried changing the column delimiter and the row delimiter in the sink dataset, but nothing helped.</p>
<p><a href=""https://i.stack.imgur.com/FUGMx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUGMx.png"" alt=""Dataset Settings"" /></a></p>
","<azure-data-factory>","2023-04-25 08:57:41","22","0","1","76099934","<p>I tried with following data from SQL.</p>
<p><img src=""https://i.imgur.com/DzAmIAT.png"" alt=""enter image description here"" /></p>
<p>I didn't changed anything in the dataset configurations and these are the configurations.</p>
<p><img src=""https://i.imgur.com/zhnae3x.png"" alt=""enter image description here"" /></p>
<p>I got the data with mutliple columns when I preview in the storage account.</p>
<p><img src=""https://i.imgur.com/beuxNeJ.png"" alt=""enter image description here"" /></p>
<p>This is because of the <code>,</code> as Column delimiter. While copying the data, you can change it to other if you want.</p>
<p>But while previewing the same dataset with above configurations from ADF after copy, we can see it showed the correct values.</p>
<p><img src=""https://i.imgur.com/3O48VkZ.png"" alt=""enter image description here"" /></p>
<p>So, when you are using the result data in future in ADF or any other, give the same configurations as above for it to get the same data.</p>
"
"76097673","Azure Data Factory Read Csv Files from multiple portioned folders","<p>I am trying to read csv file from partitioned folders using dataflow activity ((yyyy/mm/dd/timestamp). how to read the csv files dynamically?</p>
<p><a href=""https://i.stack.imgur.com/niofg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/niofg.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/nZwUa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nZwUa.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/cXio7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cXio7.png"" alt=""enter image description here"" /></a></p>
<p>How to read single short for all the sub folders? above folder structure before start YYYY (year), the table name as a folder path.</p>
<p>eg : test/dwh/tablename/2023/01/15/100543/files.csv</p>
<p><a href=""https://i.stack.imgur.com/KMPoh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KMPoh.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-data-lake-gen2>","2023-04-25 04:38:23","59","0","1","76098746","<p>You can use <strong>Wildcard paths</strong> in source options in dataflow as below.
<code>test/dwh/tablename/**/**/**/**/*.csv</code></p>
<p><img src=""https://i.imgur.com/d3L4wx6.png"" alt=""enter image description here"" /></p>
<p>Here, you can give the <strong>Partition root path</strong>, that tablename in your case.</p>
<p>In my case following is the path
<code>sampletable/2022/01/12/102344/part1.csv</code>
In each csv files there are <code>6627</code> records.</p>
<p>And this is the number of rows written.</p>
<p><img src=""https://i.imgur.com/HZ2XTM8.png"" alt=""enter image description here"" />.</p>
"
"76095809","Pagination rule for Post API call in data factory","<p>I have a url that returns a response json based on following details:</p>
<pre><code>https://testapi/dev?user=reportuser

Call type: POST

Body: {
    &quot;name&quot;: &quot;Employee Data&quot;,
    &quot;pageSize&quot;: &quot;10000&quot;,
    &quot;nextTableName&quot;: &quot;EMPLOYEE&quot;,
    &quot;nextKey&quot;: &quot;1&quot;,
    &quot;tables&quot;: [
        {
            &quot;tableName&quot;: &quot;EMPLOYEE&quot;,
            &quot;columns&quot;: [
                &quot;FIRST_NAME&quot;,
                &quot;LAST_NAME&quot;,
                &quot;ADDRESS&quot;,
        ]
    }
]
</code></pre>
<p>}</p>
<pre><code>Response JSON:
    {
     &quot;data&quot;: {
        &quot;EMPLOYEE&quot;: [
            {
                &quot;FIRST_NAME&quot;:&quot;Rahul&quot;,
                &quot;LAST_NAME&quot;:&quot;Sharma&quot;,
                &quot;ADDRESS&quot;:&quot;UK&quot;
            },
            {
                &quot;FIRST_NAME&quot;:&quot;Albert&quot;,
                &quot;LAST_NAME&quot;:&quot;Wintermeyer&quot;,
                &quot;ADDRESS&quot;:&quot;Australia&quot;
            },
            ...
            ...
         ],
         &quot;pagination&quot;: [
            {
                &quot;nextTableName&quot;: &quot;EMPLOYEE&quot;,
                &quot;nextKey&quot;: &quot;10001&quot;
            }
        ]
}
</code></pre>
<p>}</p>
<p>To explain how the url works, it returns first 10000 records starting from the 1st when (because of nextKey:1).</p>
<p>If I want to query the next 10000 records, I replace nextKey in body with 10001 and it will return all records from 10001 to 20001.</p>
<p>One keeps calling these till nextKey in response returns 0.</p>
<p>I want to move this data to Azure SQL database and I am using azure data factory for this. I tried with Web activity, but there is a payload limit of 4MB. Because of this endless number of calls need to be made to reach the end.</p>
<p>All the articles I have gone through suggest the direct way of using paginaton rule, i.e. in response json I will get link to the next set of data called nextLink. In my case I just get a number (nextKey).</p>
<p>How do I use pagination rule of copy data activity in a way that I can send the whole body with new nextKey value and url again.</p>
<p>Note: Its a POST call, hence body has to be sent everytime (with new nextkey value).</p>
","<azure><api><rest><azure-sql-database><azure-data-factory>","2023-04-24 20:20:54","40","0","1","76100014","<blockquote>
<p>I use pagination rule of copy data activity in a way that I can send the whole body with new nextKey value and url again.</p>
</blockquote>
<p>AFAIK, you can't directly fetch API response to API body. you check this similar <a href=""https://learn.microsoft.com/en-us/answers/questions/1188835/how-to-configure-adf-copy-data-rest-api-pagination"" rel=""nofollow noreferrer"">issue</a> also.</p>
<p>You can follow below approach to achieve your scenario:</p>
<ul>
<li>First create a set variable to store <strong>next key</strong> with value 1.
<img src=""https://i.imgur.com/Ea3BPZq.png"" alt=""enter image description here"" /></li>
<li>Then take a until activity to check the end condition as <strong>if <code>nextkey</code> is equal to 0 the stop the iteration</strong>.
<img src=""https://i.imgur.com/o7u4f55.png"" alt=""enter image description here"" /></li>
<li>Then inside until loop take a copy activity to copy data from rest Api to SQL and pass body as</li>
</ul>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;name&quot;: &quot;Employee Data&quot;,
    &quot;pageSize&quot;: &quot;10000&quot;,
    &quot;nextTableName&quot;: &quot;EMPLOYEE&quot;,
    &quot;nextKey&quot;: &quot;@{variables('nextkey')}&quot;,
    &quot;tables&quot;: [
        {
            &quot;tableName&quot;: &quot;EMPLOYEE&quot;,
            &quot;columns&quot;: [
                &quot;FIRST_NAME&quot;,
                &quot;LAST_NAME&quot;,
                &quot;ADDRESS&quot;
                ]
        }
    ]
}
</code></pre>
<p>so it will take value of next key from set variable and then call the response and copy it in sql table.
<img src=""https://i.imgur.com/6gSgic6.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then again call the same API with the web activity to get the <code>nextkey</code> value from API response with same request body as above.
<img src=""https://i.imgur.com/h0TcHcA.png"" alt=""enter image description here"" /></li>
<li>After this take another set variable to update the <code>nextkey</code> value which we set earlier with web activity responses <code>nextkey</code> .
<img src=""https://i.imgur.com/LLEhIMo.png"" alt=""enter image description here"" /></li>
<li>So, it will update the <code>nextkey</code> value and iterate based on that value again in until loop till it match with condition.</li>
</ul>
"
"76091861","Azure Data Factory - Site-to-Site VPN Connection","<p>I have 2 cloud environments AWS and Azure. In AWS, I have a SQL Server instance inside a VPC (192.168.0.0/24). In Azure, I have a VNet (10.0.0.0/24). Both these 2 Virtual Network are connected through Site-2-Site VPN. I have tested connectivity to SQL server on AWS (192.168.0.234) from Azure VM inside Azure VNet. In Azure I have Azure Data Factory instance. From ADF, i need to access the SQL server instance within that AWS VM to run a Data Flow. How should I achieve this?</p>
<p>I already created a site-2-site VPN and test the SQL server conenctivity from Azure to AWS via SQLCmd utility running on Azure VM inside Azure VNet.
Created internal load balancer (frontend IP: 10.0.0.4) while pointing traffic to AWS VM as a backend pool. Load balancer doesn't route the traffic to AWS VM SQL server.</p>
<p>This what I'm trying to achieve similarly based on Microsoft Document. Instead On-prem SQL server, I have AWS SQL server behind a VPC.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-managed-virtual-network-on-premise-sql-server"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-managed-virtual-network-on-premise-sql-server</a></p>
<p><a href=""https://i.stack.imgur.com/zfyqY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zfyqY.png"" alt=""enter image description here"" /></a></p>
<p>What's already being tried:</p>
<ol>
<li>Azure Self-Hosted Integration Runtime - This way i can install self-hosted IR on the AWS environment, which allow me to securely connect ADF to SQL Server DB in AWS VNet through SQL connector. But it only support simple data movements. Copy Tasks. ADF data flows does not support.</li>
<li>WhiteListing Azure IPs on AWS VNet - This way i can allow all the Azure ADF used IP ranges in AWS VNet. With this ADF IR environment can access the AWS SQL server DB. But i don't think enabling such number of Azure Public IPs in Prod environment is practical and secure.</li>
</ol>
","<azure><azure-data-factory><aws-site-to-site>","2023-04-24 12:25:28","48","0","2","76095082","<p>It seems like you have created a load balancer in Azure.</p>
<p>The backend pool in Azure has some <a href=""https://learn.microsoft.com/en-us/azure/load-balancer/backend-pool-management#limitations"" rel=""nofollow noreferrer"">limitations</a>, one being that backend resources must be in the same VNET as the LB.</p>
<p>I suggest instantiating the load balancer on the AWS side, which should solve your connectivity issue.</p>
"
"76091861","Azure Data Factory - Site-to-Site VPN Connection","<p>I have 2 cloud environments AWS and Azure. In AWS, I have a SQL Server instance inside a VPC (192.168.0.0/24). In Azure, I have a VNet (10.0.0.0/24). Both these 2 Virtual Network are connected through Site-2-Site VPN. I have tested connectivity to SQL server on AWS (192.168.0.234) from Azure VM inside Azure VNet. In Azure I have Azure Data Factory instance. From ADF, i need to access the SQL server instance within that AWS VM to run a Data Flow. How should I achieve this?</p>
<p>I already created a site-2-site VPN and test the SQL server conenctivity from Azure to AWS via SQLCmd utility running on Azure VM inside Azure VNet.
Created internal load balancer (frontend IP: 10.0.0.4) while pointing traffic to AWS VM as a backend pool. Load balancer doesn't route the traffic to AWS VM SQL server.</p>
<p>This what I'm trying to achieve similarly based on Microsoft Document. Instead On-prem SQL server, I have AWS SQL server behind a VPC.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-managed-virtual-network-on-premise-sql-server"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-managed-virtual-network-on-premise-sql-server</a></p>
<p><a href=""https://i.stack.imgur.com/zfyqY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zfyqY.png"" alt=""enter image description here"" /></a></p>
<p>What's already being tried:</p>
<ol>
<li>Azure Self-Hosted Integration Runtime - This way i can install self-hosted IR on the AWS environment, which allow me to securely connect ADF to SQL Server DB in AWS VNet through SQL connector. But it only support simple data movements. Copy Tasks. ADF data flows does not support.</li>
<li>WhiteListing Azure IPs on AWS VNet - This way i can allow all the Azure ADF used IP ranges in AWS VNet. With this ADF IR environment can access the AWS SQL server DB. But i don't think enabling such number of Azure Public IPs in Prod environment is practical and secure.</li>
</ol>
","<azure><azure-data-factory><aws-site-to-site>","2023-04-24 12:25:28","48","0","2","76122577","<p>Agreed with Nick comment. You won't be able to achieve such a design with an Azure LB and backend outside of that vnet.</p>
<p>A schema as asked by Nico would be great because i don't see the point of pointing an LB on Azure side to take care of a VM on the AWS side.</p>
<p>I'm not an expert of ADF but is this what you are trying to achieve :
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server?tabs=data-factory</a></p>
<p>Worth looking at the integration runtime ?
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory</a></p>
"
"76083320","How ADF Dataflow will read columns and rows from separate arrays and generate csv file","<p>Below is my API response, where I get columns details an row details in separate array.</p>
<pre><code>[    
    {
        &quot;FrameType&quot;: &quot;DataTable&quot;,
        &quot;TableId&quot;: 1,
        &quot;TableKind&quot;: &quot;PrimaryResult&quot;,
        &quot;TableName&quot;: &quot;PrimaryResult&quot;,
        &quot;Columns&quot;: [
            {
                &quot;ColumnName&quot;: &quot;RecordID&quot;,
                &quot;ColumnType&quot;: &quot;string&quot;
            },
            {
                &quot;ColumnName&quot;: &quot;RecordNumber&quot;,
                &quot;ColumnType&quot;: &quot;integer&quot;
            },
            {
                &quot;ColumnName&quot;: &quot;IsValid&quot;,
                &quot;ColumnType&quot;: &quot;boolean&quot;
            },
            {
                &quot;ColumnName&quot;: &quot;Remarks&quot;,
                &quot;ColumnType&quot;: &quot;string&quot;
            }
        ],
        &quot;Rows&quot;: [
            [
                &quot;record1&quot;,
                123,
                true,
                &quot;test1&quot;
            ],
            [
                &quot;record2&quot;,
                456,
                false,
                &quot;test2&quot;
            ]
        ]
    }
]
</code></pre>
<p>Now, using Azure Data Factory, I would like to generate a csv into blob based on above response.</p>
<p>I have tried a data flow with derived columns and some flatten transformations. But I am not soo successful in this attempt. Below is the JSON for my dataflow.</p>
<pre><code>{
    &quot;name&quot;: &quot;df_ADX_REST_TagsInfo&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;linkedService&quot;: {
                        &quot;referenceName&quot;: &quot;ls_ADX_REST_API&quot;,
                        &quot;type&quot;: &quot;LinkedServiceReference&quot;
                    },
                    &quot;name&quot;: &quot;ADXRestApi&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;linkedService&quot;: {
                        &quot;referenceName&quot;: &quot;linkservice_cloudshellstorageforbh&quot;,
                        &quot;type&quot;: &quot;LinkedServiceReference&quot;
                    },
                    &quot;name&quot;: &quot;WriteColumnstoCSV&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;FilterAPIResponse&quot;
                },
                {
                    &quot;name&quot;: &quot;flatten1&quot;
                },
                {
                    &quot;name&quot;: &quot;derivedColumn1&quot;
                },
                {
                    &quot;name&quot;: &quot;derivedColumn2&quot;
                },
                {
                    &quot;name&quot;: &quot;select1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;parameters{&quot;,
                &quot;     access_token as string&quot;,
                &quot;}&quot;,
                &quot;source(output(&quot;,
                &quot;          body as (Cancelled as boolean, Columns as (ColumnName as string, ColumnType as string)[], FrameType as string, HasErrors as boolean, IsProgressive as boolean, Rows as string[][], TableId as short, TableKind as string, TableName as string, Version as string),&quot;,
                &quot;          headers as [string,string]&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     format: 'rest',&quot;,
                &quot;     timeout: 30,&quot;,
                &quot;     requestInterval: 0,&quot;,
                &quot;     entity: '/v2/rest/query',&quot;,
                &quot;     headers: ['Content-Type' -&gt; 'application/json; charset=utf-8', 'Authorization' -&gt; ($access_token), 'Accept' -&gt; 'application/json', 'Host' -&gt; 'help.kusto.windows.net'],&quot;,
                &quot;     httpMethod: 'POST',&quot;,
                &quot;     body: ('{\&quot;db\&quot;:\&quot;bhTestDB\&quot;,  \&quot;csl\&quot;:\&quot;TagsInfo | summarize arg_max(ingestion_time(),*) by TimeseriesId\&quot;}'),&quot;,
                &quot;     paginationRules: ['supportRFC5988' -&gt; 'true'],&quot;,
                &quot;     responseFormat: ['type' -&gt; 'json', 'documentForm' -&gt; 'documentPerLine']) ~&gt; ADXRestApi&quot;,
                &quot;ADXRestApi filter(body.TableKind=='PrimaryResult') ~&gt; FilterAPIResponse&quot;,
                &quot;FilterAPIResponse foldDown(unroll(body.Columns),&quot;,
                &quot;     mapColumn(&quot;,
                &quot;          Columns = body.Columns.ColumnName&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: false,&quot;,
                &quot;     skipDuplicateMapOutputs: false) ~&gt; flatten1&quot;,
                &quot;FilterAPIResponse derive(Tags = toString(unfold(split(toString(body.Rows), '],[')))) ~&gt; derivedColumn1&quot;,
                &quot;derivedColumn1 derive(Tags = replace(replace(Tags,'[[',''),']]','')) ~&gt; derivedColumn2&quot;,
                &quot;derivedColumn2 select(mapColumn(&quot;,
                &quot;          Tags&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select1&quot;,
                &quot;flatten1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     input(&quot;,
                &quot;          {{} as string&quot;,
                &quot;     ),&quot;,
                &quot;     format: 'delimited',&quot;,
                &quot;     container: 'data',&quot;,
                &quot;     columnDelimiter: ',',&quot;,
                &quot;     escapeChar: '\\\\',&quot;,
                &quot;     quoteChar: '\\\&quot;',&quot;,
                &quot;     columnNamesAsHeader: true,&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     saveOrder: 1) ~&gt; WriteColumnstoCSV&quot;
            ]
        }
    }
}
</code></pre>
<p>Can anyone here suggest me an approach to achieve this ?</p>
","<azure><azure-data-factory>","2023-04-23 05:46:37","51","1","1","76088944","<p>Using the above transformations, you get the data as shown in the below image.</p>
<p><img src=""https://i.imgur.com/vKekti8.png"" alt=""enter image description here"" /></p>
<ul>
<li>This file has 4 columns with 2 rows but no header. To integrate this with columns names, you can use the following. Extract the <code>ColumnNames</code> after <code>flatten</code> transformation to get data like in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/9P6OYss.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now write this to a sink CSV file with the dataset configurations as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/7yrNQHB.png"" alt=""enter image description here"" /></p>
<ul>
<li>This would give a file data as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/CqFxuUx.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I have 2 files, <code>rows.csv and cols.csv</code>. Using the following dataflow, I was able to get the complete csv file.</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow2&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;s2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                },
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;s3&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source2&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;d1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;select1&quot;
                },
                {
                    &quot;name&quot;: &quot;union1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          RecordID as string,&quot;,
                &quot;          RecordNumber as string,&quot;,
                &quot;          IsValid as string,&quot;,
                &quot;          Remarks as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source1&quot;,
                &quot;source(output(&quot;,
                &quot;          Column_1 as string,&quot;,
                &quot;          Column_2 as string,&quot;,
                &quot;          Column_3 as string,&quot;,
                &quot;          Column_4 as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source2&quot;,
                &quot;source1 select(mapColumn(&quot;,
                &quot;          RecordID,&quot;,
                &quot;          RecordNumber,&quot;,
                &quot;          IsValid,&quot;,
                &quot;          Remarks&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select1&quot;,
                &quot;select1, source2 union(byName: false)~&gt; union1&quot;,
                &quot;union1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     partitionFileNames:['final.csv'],&quot;,
                &quot;     umask: 0022,&quot;,
                &quot;     preCommands: [],&quot;,
                &quot;     postCommands: [],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     partitionBy('hash', 1)) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
<p><img src=""https://i.imgur.com/5C4PUXX.png"" alt=""enter image description here"" /></p>
"
"76082913","How to arrange a connection between Azure Data Factory and Azure Data Explorer when both are in private VNETs?","<p>I am completely new to Azure Data Explorer and Azure Data Factory. So, I would like to seek your suggestions in implementing below requirement.</p>
<ul>
<li>My data source is an API. My destination is Azure Data Explorer.</li>
<li>I need to use Azure Data Factory for data transformations. So, I built a Pipeline in Azure Data Factory to process and transform the API response.</li>
<li>Transformed API response should be ingested into Azure Data Explorer.</li>
</ul>
<p>Now, my problem is both my Azure Data Explorer and Azure Data Factory is private access and lies behind a virtual network (of course, both are in same subnet).</p>
<p>As per the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Microsoft documentation on ADF connectors</a>, ADF does not support connection to ADX using Managed Private Endpoint.</p>
<p><strong>My Approach:</strong></p>
<ul>
<li>Azure Data Factory has to process the API response and create csv
files in Azure storage blob.</li>
<li>Create Event Grid (Blob Storage) data connection in Azure Data
Explorer. So that when csv files are created in storage account, data
will be ingested to Azure Data Explorer tables.</li>
</ul>
<p>Do we have any better approach to implement this scenario? (FYI. during each API invocation, I many need to process 30K to 40K records and need to do this for hundreds of customers. This pipeline will process millions of records in each run).</p>
","<azure><azure-data-factory><azure-data-explorer><azure-virtual-network>","2023-04-23 02:52:02","35","0","1","76109238","<p>here is what you could do.</p>
<ol>
<li><p>create a managed integration runtime in ADF and make sure that it gets deployed in a line of sight to ADX and your source (same vnet as ADX and the source)</p>
</li>
<li><p>a) In case ADX is vnet injected (not recommended): connect to ADX using private- OR
b) in case there is a private endpoint for ADX: connect to ADX using the regular url of the cluster</p>
</li>
</ol>
"
"76081172","How to get full filepath and to path them to foreach activity in Azure Synapse Analytics?","<p>I need to process all the data contained within ADLS Gen2.
The data is stored in parquet format, not in databases, and the folder structure looks like this.
<a href=""https://i.stack.imgur.com/BzjCq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BzjCq.png"" alt=""folda structure"" /></a></p>
<p>The process is the same for all files, but the processed files must be overwritten in the same location.
I intend to accomplish this in the following ways:</p>
<ol>
<li>Get full filepath of all files using &quot;Get Metadata&quot; activity.</li>
<li>After passing the file paths one by one to the foreach activity, the files are processed one by one and stored in their original locations.</li>
</ol>
<p>However, the Get Metadata activity could not successfully retrieve the file path because wildcards are not allowed in the source data set.
How can this be accomplished?</p>
","<azure><azure-data-factory><azure-synapse><azure-synapse-analytics>","2023-04-22 18:03:18","67","0","1","76090948","<p><strong>Get Meta data activity</strong> won't give the full folder path even though you defined a wild card placeholder.</p>
<p>If you have less row count on overall, you can use copy activity addional column for file path and merge all files and save it in a temporary location. Later using lookup activity, get that column and you can get unique list using union of it. But lookup won't give the output for more than 5000 rows.</p>
<p>So, I tried the below approach to get all file paths in an array.</p>
<p>First, I have generated <strong>array for folder paths(with dates)</strong> using lookup activity SQL script.</p>
<p>If you want to avoid SQL database script, use <strong>until activity</strong> by initializing start date and end date to get the list of folder paths of dates.</p>
<p>First Go through your folder paths and identify the <strong>start date</strong> and <strong>end date</strong> for the folders. Give those values to parameters like this.</p>
<p>Here this is my folder structure:</p>
<pre><code>inputdata
    year=2023
        month=04
            day=20
                File01.parquet
            day=21
                File01.parquet
                File02.parquet
            day=22
                File01.parquet
                File02.parquet
            day=23
                File01.parquet
                File02.parquet
            day=24
                File01.parquet
                File02.parquet
        
</code></pre>
<p>According to my folder structure <code>2023/04/20</code> and <code>2023/04/24</code> are start date and end date respectively.</p>
<p><img src=""https://i.imgur.com/GcMGVUw.png"" alt=""enter image description here"" /></p>
<p>I have used the below SLQ script from this <a href=""https://stackoverflow.com/a/65277680"">SO answer</a> by @<strong>Joseph Xu</strong> in lookup to get folder paths array.</p>
<pre><code>;with dtable as 
(
    select CONVERT(varchar(100),'@{pipeline().parameters.folders_s_date}', 111) as mydate
union all 
    select CONVERT(varchar(100), DATEADD(day,1,mydate), 111) from dtable
    where datediff(day,CONVERT(varchar(100), DATEADD(day,1,mydate), 111),'@{pipeline().parameters.folders_end_date}')&gt;=0
) 

select concat('inputdata/year=',year(mydate),'/month=',PARSENAME( REPLACE(mydate,'/','.'),2),'/day=',PARSENAME( REPLACE(mydate,'/','.'),1)) as path from dtable option(MAXRECURSION 0)

</code></pre>
<p>which will give the folder paths array like this:</p>
<p><img src=""https://i.imgur.com/5jedPhQ.png"" alt=""enter image description here"" /></p>
<p>Now, pass this array to ForEach and inside ForEach use a Get Meta data activity with parquet dataset. With dataset parameters give <code>@item().path</code> in the Get meta data activity to get the Child Items(parquet files).</p>
<p>Here, nested for Each are not supported in ADF currently, so to iterate over the child items I have used another pipeline with <strong>Execute pipeline</strong> activity and passed these child items array to it.</p>
<p>In the child pipeline, append the child items file name to the folder path and generate an array. Join this array with <code>_</code> and return the result as string.</p>
<p><img src=""https://i.imgur.com/rzBq6Yc.png"" alt=""enter image description here"" /></p>
<p>After Execute pipeline activity, I have concatenated the return result with a string variable like below.</p>
<p><img src=""https://i.imgur.com/GeMExcI.png"" alt=""enter image description here"" /></p>
<p>Outside the ForEach, I have converted the <strong>string of paths seperated by <code>_</code></strong> into an array using split and stored in a variable.</p>
<p><strong>Parent Pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup to get dates structure&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;AzureSqlSource&quot;,
                        &quot;sqlReaderQuery&quot;: {
                            &quot;value&quot;: &quot;;with dtable as \n(\n    select CONVERT(varchar(100),'@{pipeline().parameters.folders_s_date}', 111) as mydate\nunion all \n    select CONVERT(varchar(100), DATEADD(day,1,mydate), 111) from dtable\n    where datediff(day,CONVERT(varchar(100), DATEADD(day,1,mydate), 111),'@{pipeline().parameters.folders_end_date}')&gt;=0\n) \n\nselect concat('inputdata/year=',year(mydate),'/month=',PARSENAME( REPLACE(mydate,'/','.'),2),'/day=',PARSENAME( REPLACE(mydate,'/','.'),1)) as path from dtable option(MAXRECURSION 0)\n&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        },
                        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                        &quot;partitionOption&quot;: &quot;None&quot;
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;SQLdataset&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;Itertate through folder paths&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup to get dates structure&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Lookup to get dates structure').output.value&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Get Metadata1&quot;,
                            &quot;type&quot;: &quot;GetMetadata&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;dataset&quot;: {
                                    &quot;referenceName&quot;: &quot;sourcefiles&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;folderpath&quot;: {
                                            &quot;value&quot;: &quot;@item().path&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                },
                                &quot;fieldList&quot;: [
                                    &quot;childItems&quot;
                                ],
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                    &quot;enablePartitionDiscovery&quot;: false
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;ParquetReadSettings&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Child pipeline for file paths&quot;,
                            &quot;type&quot;: &quot;ExecutePipeline&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Get Metadata1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;pipeline&quot;: {
                                    &quot;referenceName&quot;: &quot;child&quot;,
                                    &quot;type&quot;: &quot;PipelineReference&quot;
                                },
                                &quot;waitOnCompletion&quot;: true,
                                &quot;parameters&quot;: {
                                    &quot;folderpath&quot;: {
                                        &quot;value&quot;: &quot;@item().path&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    },
                                    &quot;childitems&quot;: {
                                        &quot;value&quot;: &quot;@activity('Get Metadata1').output.childItems&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;store return and concat in temp&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Child pipeline for file paths&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;temp&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@concat(variables('paths_string'),'_',activity('Child pipeline for file paths').output.pipelineReturnValue.returns)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;assign temp to path strings&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;store return and concat in temp&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;paths_string&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@variables('temp')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;split paths to array&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Itertate through folder paths&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;final_paths&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@skip(split(variables('paths_string'),'_'),1)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;folders_s_date&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;2023/04/20&quot;
            },
            &quot;folders_end_date&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;2023/04/24&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;final_paths&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;temp&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;paths_string&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>Child Pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;child&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.childitems&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Append variable1&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;child_paths&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@concat(pipeline().parameters.folderpath,'/',item().name)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;pipelineReturnValue&quot;,
                    &quot;value&quot;: [
                        {
                            &quot;key&quot;: &quot;returns&quot;,
                            &quot;value&quot;: {
                                &quot;type&quot;: &quot;Expression&quot;,
                                &quot;content&quot;: &quot;@join(variables('child_paths'),'_')&quot;
                            }
                        }
                    ],
                    &quot;setSystemVariable&quot;: true
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;folderpath&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;childitems&quot;: {
                &quot;type&quot;: &quot;array&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;child_paths&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>Paths array:</strong></p>
<p><img src=""https://i.imgur.com/jCr7M9i.png"" alt=""enter image description here"" /></p>
<p>You can pass this array For Each to achieve your requirement.</p>
"
"76078949","How to pass array of Json as a parameter to Dataflow?","<p>I am getting the output of lookup activity as below.</p>
<p>&quot;value&quot;: [
{
&quot;PO_id&quot;: &quot;XXX&quot;,
&quot;SO_number&quot;: &quot;YYYY&quot;
},
{
&quot;PO_id&quot;: &quot;AAA&quot;,
&quot;SO_number&quot;: &quot;BBBB&quot;
}
]</p>
<p>I want to pass this lookup activity output as a paremter to dataflow
and convert them into the tabular format</p>
<p>PO_id,SO_number
&quot;XXX&quot;,&quot;YYYY&quot;
&quot;AAA&quot;,&quot;BBBB&quot;</p>
<p>There is no Json[] type in dataflow parameter. I tried with string[] parameter, but getting error</p>
<p>'Array elements must be all of same type'. How can I solve this?</p>
","<azure><azure-data-factory>","2023-04-22 09:37:18","56","1","1","76083427","<p>You can pass the Json array as a string to dataflow parameter and then in dataflow use parse transformation to convert them into rows and column.</p>
<ul>
<li>Create a dataflow parameter of type string.</li>
</ul>
<p><img src=""https://i.imgur.com/cGlyTLW.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>In pipeline, instead of passing the value for dataflow parameter as <code>@activity('Lookup1').output.value</code> , you can pass the value as <code>@string(activity('Lookup1').output.value)</code>.</p>
</li>
<li><p>In Derived column transformation, add the new column <code>new_col</code> and define the expression for the column as
<code>split(replace(replace(replace($parameter1, '[', ''),']',''),'},','}},'),'},')</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/jnmGlGM.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/6Okklkz.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then add a flatten transformation and unroll by the column <code>new_col</code>.</li>
<li>Then add the parse transformation. Choose Json as format in parse settings. And then choose <code>single document</code> in Json settings as a document form. In column settings give the below expression</li>
</ul>
<pre><code>column: new_col
expression:new_col
Output column type:(name as string,dept as string)
</code></pre>
<p>[Replace name as string,dept as string with required columns and their type]</p>
<p><img src=""https://user-images.githubusercontent.com/113445679/233781460-339a3bc3-6dbd-4a43-bda1-f2b4a08df98c.gif"" alt=""gif1"" /></p>
<p>By this way, you can pass the Json array in the dataflow and parse them as rows and column data.</p>
"
"76078474","How to use output on one lookup activity to another lookup activity in ADF?","<p>I have two SQL servers and both server has same table under the name of &quot;[TABLE-A]&quot;.
What I need to do is, I want to maintain the same exact records in [SERVER2].[TABLE-A] as it is in the [SERVER1].[TABLE-A]. Whenever any record has been inserted or deleted [SERVER1].[TABLE-A] I need to do the same thing in [SERVER2].[TABLE-A]. So I had created two lookup activity under the name of 'Lookup1' and 'Lookup2'. From Lookup1 I am getting all the unique Id's which are present in [SERVER1].[TABLE-A] from the below query.</p>
<pre><code>SELECT DISTINCT Id from [SERVER1].[TABLE-A]
</code></pre>
<p>Which gives me a column which contains all 70 distinct Id's.</p>
<p>My Idea is to use this Id's in Lookup2 and delete Id's which are not present in the Lookup1 output.</p>
<pre><code>DELETE FROM [SERVER2].[TABLE-A] WHERE Id NOT IN ('@{activity('Lookup1').output.value[0].Id}')
</code></pre>
<p>But I am not able to put those 70 Id's in ('@{activity('Lookup1').output.value[0].Id}') It takes only 1 value from those 70 since I specified that way. How can I put those 70 Id's in my Lookup2 query?</p>
","<sql><azure><azure-data-factory>","2023-04-22 07:21:16","46","0","1","76078954","<p>Lookup activity output will be of array type. Sample lookup activity output data:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;value&quot;: [
        {
            &quot;id&quot;: 1
        },
        {
            &quot;id&quot;: 3
        },
        {
            &quot;id&quot;: 4
        }
    ]
  }
</code></pre>
<p>The expression <code>@{activity('Lookup1').output.value[0].Id} </code> will give only the first value. In order to give all the values from lookup activity to the sql query, there are few changes to be done in the lookup1 data</p>
<ul>
<li><p><code>{&quot;id&quot;:</code> and <code>}</code> are to be removed from the data and <code>[</code> and <code>]</code> are to be replaced with <code>(</code> and <code>)</code> respectively. Thus, the above data will become like  <code>(1,3,4)</code>.</p>
</li>
<li><p>To do this , a variable v1 of string type is taken and the value for that variable is set as <code>@{activity('Lookup1').output.value}</code>.
<img src=""https://i.imgur.com/skuuczj.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Variable v2(string type) is taken and in this variable all the unwanted data is removed from v1. The value is given as <code>@{replace(replace(replace(replace(variables('v1'),'{&quot;id&quot;:',''),'}',''),'[','('),']',')')}</code>.</p>
</li>
<li><p>Then lookup activity2 is taken and query is given as</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>select * from emp1 where id in @{variables('v2')}
</code></pre>
<p><img src=""https://i.imgur.com/4CN57nh.png"" alt=""enter image description here"" /></p>
<p>This will select the rows from <code>emp1</code> which are available in <code>lookup1</code> table. Delete sql command can also be written similarly.</p>
"
"76078150","azure blob storage to snowflake table data loading error invalid identifier","<p>this is the most asked question but i didn't find any clear answer yet.</p>
<p>i'm trying to load data from azure data blob storage to snowflake table by doing some transformations in azure data factory activities.</p>
<p>my job is failing with this error:</p>
<blockquote>
<p>Job failed due to reason: at Sink 'sink1': SQL compilation error:
error line 1 at position 22 invalid identifier '&quot;Employee_ID&quot;'</p>
</blockquote>
<p>my dataset is a CSV placed in blob storage with this schema.</p>
<p><a href=""https://i.stack.imgur.com/0QlWY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0QlWY.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/wEQ4K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wEQ4K.png"" alt=""enter image description here"" /></a></p>
<p>my table in snowflake has this schema</p>
<p><a href=""https://i.stack.imgur.com/6r5ro.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6r5ro.png"" alt=""enter image description here"" /></a></p>
<p>my sink in data flow has these settings</p>
<p><img src=""https://i.stack.imgur.com/kn7Dr.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/6Ih9D.png"" alt=""enter image description here"" />]<a href=""https://i.stack.imgur.com/6Ih9D.png"" rel=""nofollow noreferrer"">5</a></p>
<p>This is longer going on issue I couldn't fix it, if there is any help on this i can stop following copy command in cmd which is manual, mentioned below.</p>
<pre><code>COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
</code></pre>
<p>every time my pipeline copying file from blob storage to snowflake table, this error is poping  `</p>
<blockquote>
<p>&quot;Job failed due to reason: at Sink 'sink1': SQL compilation error:
error line 1 at position 22 invalid identifier '&quot;Employee_ID&quot;'</p>
</blockquote>
<p>all the scenario I've mentioned in the above I've followed as process of correctly setting up my datasets, table creation step in snowflake, still I'm not able to load, I'm curious whether snowflake is including header of the datasets, while inserting?</p>
","<azure><snowflake-cloud-data-platform><azure-blob-storage><azure-data-factory><snowflake-schema>","2023-04-22 05:26:43","34","0","1","76078255","<blockquote>
<p>just want to understand why snowflake is considering header as input data while inserting into any table, anything with detailed explanation.</p>
</blockquote>
<p>I think it is useful feature to define if your source csv file have a header or not.
Different data sources produce different kind of csv files some with header, some without, some with semicolon as delimiter, some with comma as delimeter etc.</p>
<p>It is a good design to include this parameters to make the data loading process flexible otherwise you need an additional step to prepare the file for uploading.</p>
<p>skip_header just means skip the first row. Snowflake doesn’t know if the file have a header.There is no auto-detect.</p>
"
"76074919","RestAPI data having multiple pages appends into single file during ADF pagination results in invalid json format","<p>I have extracted the restAPI data which is in 10 pages using pagination in ADF (copy data activity). The problem is ADF seems to have is that when there are multiple pages, ADF just appends the different responses into one file.In my case I have 'results' as collection array and during pagination(10 pages), I can see the result which is json root('results') is getting repeated 10 times and this leads to an invalid JSON file. How to overcome this problem.Does ADF supports this?</p>
","<azure><azure-data-factory>","2023-04-21 16:07:30","36","0","1","76088990","<blockquote>
<p>RestAPI data having multiple pages appends into single file during ADF pagination results in invalid json format.</p>
</blockquote>
<p>When you copy Data from Rest API to file with copy activity It will just append it as whole object ang you will get a set of objects which leads to a invalid json format.</p>
<p>To resolve this in sink setting there is <code>File Pattern</code> Option is available by default it is <strong>set of objects</strong> change it to <strong>Array of objects</strong> so it will convert it as array of objects and proper json format.</p>
<p><img src=""https://i.imgur.com/39U71ck.png"" alt=""enter image description here"" /></p>
"
"76074815","time to datetime field in ADF copy activity gets current date","<p>I have an ADF copy activity in an ADF pipeline. Source is a csv file ,target/sink is a Azure SQL db. Source has amongst other fields a field with time only values, e.g. 07:45:00. In the copy activity I want to insert the field in a datetime field in the target so we can easily convert it later. Normally, in SQL Server, if you add a single time value in a datetime field, SQL Server will automatically add 1/1/1900 as a date in front of it. Now however from the copy activity if the time value is copied, automatically the current date is added. Is there a way to prevent this? I prefer not to do transform data here already (except for the string time value to datetime format of course)</p>
<p>I tried to analyse the copy activity and the column mapping json created but I don't see anything else then string to datetime mapping. Is this default behavior of ADF copy activity?</p>
","<datetime><time><azure-data-factory>","2023-04-21 15:52:01","20","0","1","76077639","<p>Copying a time-only value from a CSV to a datetime column in an Azure SQL database the current date is automatically added to the time value. This is the default behavior of ADF copy. Instead you could change the mapping of the source column to a string column instead. Then, in the sink, use CONVERT() to convert the string to a datetime value. e.g:</p>
<pre><code>INSERT INTO mytable (datetimecolumn)
VALUES (CONVERT(datetime, '1900-01-01 ' + timestringcolumn))
</code></pre>
"
"76074718","Additional empty blob created with folder names in Azure storage container-not able to delete","<p>Whenever I create a folder or subfolder inside my azure container,it is creating additional empty blobs in it. It is a blob storage with hierrachial namespace disabled-is that the cause?</p>
<p>Also I am not able to delete them using the delete activity in ADF. The activity shows success but doesnt removes those files.</p>
","<azure><azure-data-factory>","2023-04-21 15:42:01","35","0","1","76089774","<ul>
<li>Azure blob storage does not support having empty folders. Thus, when you try to create folders (or empty folders), there will be a duplicate empty file. The following is a demonstration where I have tried to replicate this issue.</li>
</ul>
<p><img src=""https://i.imgur.com/XG6aJMh.png"" alt=""enter image description here"" /></p>
<ul>
<li>However, I was able to delete that specific file by selecting it in dataset using delete activity.</li>
</ul>
<blockquote>
<p>It is a blob storage with hierrachial namespace disabled-is that the cause?</p>
</blockquote>
<ul>
<li>Yes, enabling hierarchical workspace will enable azure data lake which supports file and directory semantics and therefore which wouldn't create that additional file.</li>
</ul>
"
"76069911","Read the nested array in Azure Data Factory DataFlow","<p>I have a complex json response returned from an API and I need to store some part of response into CSV file in Azure Storage Blob.</p>
<p>After some series of transformations in Data Flow, I finally able to filter the data that I need to store in CSV. Data looks as below:</p>
<pre><code>&quot;Rows&quot;: [
            [
                &quot;record1&quot;,
                123,
                true,
                &quot;test1&quot;
            ],
            [
                &quot;record2&quot;,
                456,
                false,
                &quot;test2&quot;
            ]
        ]
</code></pre>
<p>I was stuck at this point on how to process this kind of nested array. I have added the Derived column and tried with many expressions. But none works out.</p>
<p>I need the csv as below:</p>
<pre><code>record1   123   true   test1
record2   456   false  test2
</code></pre>
<p>Any suggestion on how to achieve this will be helpful for me.</p>
<p>Response From ADX API for your reference:</p>
<pre><code>[    
    {
        &quot;FrameType&quot;: &quot;DataTable&quot;,
        &quot;TableId&quot;: 1,
        &quot;TableKind&quot;: &quot;PrimaryResult&quot;,
        &quot;TableName&quot;: &quot;PrimaryResult&quot;,
        &quot;Columns&quot;: [
            {
                &quot;ColumnName&quot;: &quot;RecordID&quot;,
                &quot;ColumnType&quot;: &quot;string&quot;
            },
            {
                &quot;ColumnName&quot;: &quot;RecordNumber&quot;,
                &quot;ColumnType&quot;: &quot;integer&quot;
            },
            {
                &quot;ColumnName&quot;: &quot;IsValid&quot;,
                &quot;ColumnType&quot;: &quot;boolean&quot;
            },
            {
                &quot;ColumnName&quot;: &quot;Remarks&quot;,
                &quot;ColumnType&quot;: &quot;string&quot;
            }
        ],
        &quot;Rows&quot;: [
            [
                &quot;record1&quot;,
                123,
                true,
                &quot;test1&quot;
            ],
            [
                &quot;record2&quot;,
                456,
                false,
                &quot;test2&quot;
            ]
        ]
    }
]
</code></pre>
","<azure><azure-data-factory><azure-data-explorer>","2023-04-21 04:32:21","50","0","1","76070343","<ul>
<li><p>You can pass the value of <code>Rows</code> from the API response to dataflow and apply transformations to get the desired results.</p>
</li>
<li><p>Since I do not have access to your API, I have stored the response in a JSON file and accessed it using look up.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/vyakv0u.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have passed this value as a string to my dataflow parameter (named as str). I have taken a file with following data as source (any random file with 1 row and 1 column).</li>
</ul>
<p><img src=""https://i.imgur.com/BfR43NT.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, using the transformations as specified in the following dataflow JSON. I was able to achieve your requirement.</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;derivedColumn1&quot;
                },
                {
                    &quot;name&quot;: &quot;derivedColumn2&quot;
                },
                {
                    &quot;name&quot;: &quot;select1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;parameters{&quot;,
                &quot;     str as string ('[[\&quot;record1\&quot;,123,true,\&quot;test1\&quot;],[\&quot;record2\&quot;,456,false,\&quot;test2\&quot;]]')&quot;,
                &quot;}&quot;,
                &quot;source(output(&quot;,
                &quot;          id as integer&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source1&quot;,
                &quot;source1 derive(tp = toString(unfold(split($str, '],[')))) ~&gt; derivedColumn1&quot;,
                &quot;derivedColumn1 derive(tp = replace(replace(tp,'[[',''),']]','')) ~&gt; derivedColumn2&quot;,
                &quot;derivedColumn2 select(mapColumn(&quot;,
                &quot;          tp&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select1&quot;,
                &quot;select1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     partitionFileNames:['op.csv'],&quot;,
                &quot;     umask: 0022,&quot;,
                &quot;     preCommands: [],&quot;,
                &quot;     postCommands: [],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     saveOrder: 1,&quot;,
                &quot;     partitionBy('hash', 1)) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
<ul>
<li>For Sink dataset, I have used the following configurations:</li>
</ul>
<p><img src=""https://i.imgur.com/fVfbEo4.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following is the resultant output file for reference:</li>
</ul>
<p><img src=""https://i.imgur.com/LF7Sgur.png"" alt=""enter image description here"" /></p>
"
"76068585","Azure Data Factory Snowflake ODDBC connection - Cannot create pipeline to run script","<p>I'm trying to run a script on Snowflake from Azure Data Factory.
This is so we can use elevated rights to run these script and we can use the scheduling capabilities of ADF to run it on weekly basis. Seems the ODBC connection supports this task better than the native ADF-Snowflake connector.
The idea is to use an ODBC connection from a SHIR VM to run a Snowflake script.
When creating the pipeline, selecting script as the activity, I don't see the ODBC Linked Service I created for this purpose.
The ODBC connection tests OK in ADF and on the SHIR VM. The steps to create this connection are outlined <a href=""https://medium.com/snowflake/azure-data-factory-connecting-to-snowflake-using-keypair-authentication-906000506345"" rel=""nofollow noreferrer"">here</a>
Perhaps I need to run a different activity?
Any help would be greatly appreciated.</p>
","<azure><snowflake-cloud-data-platform><odbc><azure-data-factory><factory>","2023-04-20 22:11:56","31","0","1","76102140","<p>Finally figured this out, see steps below:</p>
<ol>
<li>Create a Link Service using the ODBC connector pointing your SHIR VM
DSN</li>
<li>Create an ODBC that consumes above, you'll need a connection string for above</li>
<li>Create an ADF pipeline with a Lookup Activity
<ul>
<li>Use the query function to run your script</li>
<li>If more than one command s needed, convert your code to a Store Procedure on Snowflake so you can run a single command</li>
</ul>
</li>
</ol>
"
"76068543","Parameterise IR name in ADF","<p>I need to pass IR names as a parameter in the ADF template during deployment. We have different names for production IR and IR's in lower environment. In order to do this we have changed to names of IR in the linked service by changing the parameter configuration file in ADF. But we cant change IR defined in the pipeline when using dataflows.
For eg:</p>
<pre><code>{
        &quot;name&quot;: &quot;[concat(parameters('factoryName'), '/pipeline_name')]&quot;,
        &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;,
        &quot;apiVersion&quot;: &quot;2018-06-01&quot;,
        &quot;properties&quot;: {
            &quot;activities&quot;: [
                {
                    &quot;name&quot;: &quot;dataflow_name&quot;,
                    &quot;type&quot;: &quot;ExecuteDataFlow&quot;,
                    &quot;dependsOn&quot;: [],
                    &quot;policy&quot;: {
                        &quot;timeout&quot;: &quot;1.00:00:00&quot;,
                        &quot;retry&quot;: 0,
                        &quot;retryIntervalInSeconds&quot;: 30,
                        &quot;secureOutput&quot;: false,
                        &quot;secureInput&quot;: false
                    },
                    &quot;userProperties&quot;: [],
                    &quot;typeProperties&quot;: {
                        &quot;dataflow&quot;: {
                            &quot;referenceName&quot;: &quot;dataflow_name&quot;,
                            &quot;type&quot;: &quot;DataFlowReference&quot;,
                            &quot;parameters&quot;: {
                            },
                            &quot;datasetParameters&quot;: {
                            }
                        },
                        &quot;staging&quot;: {},
                        &quot;integrationRuntime&quot;: {
                            &quot;referenceName&quot;: &quot;IR_NAME&quot;,
                            &quot;type&quot;: &quot;IntegrationRuntimeReference&quot;
                        },
                        &quot;traceLevel&quot;: &quot;Fine&quot;
                    }
                }               
            ],
            &quot;policy&quot;: {
                &quot;elapsedTimeMetric&quot;: {},
                &quot;cancelAfter&quot;: {}
            },
            &quot;parameters&quot;: {
            },
            &quot;annotations&quot;: [],
            &quot;lastPublishTime&quot;: &quot;2022-11-16T10:27:40Z&quot;
        },
        &quot;dependsOn&quot;: [
            &quot;[concat(variables('factoryId'), '/dataflows/dataflow_name')]&quot;,
            &quot;[concat(variables('factoryId'), '/integrationRuntimes/', IR_NAME)]&quot;
        ]
    }
</code></pre>
<p>Here i want the referenceName under integrationRuntime to be parameterised. I tried the following setup with the parameter configuration file in ADF</p>
<pre><code>    &quot;Microsoft.DataFactory/factories/pipelines&quot;: {
    &quot;*&quot;:{
        &quot;properties&quot;:{
            &quot;*&quot;:{
                &quot;properties&quot;:{
                    &quot;typeProperties&quot;:{
                        &quot;integrationRuntime&quot;:{
                            &quot;referenceName&quot;:&quot;=&quot;
                        }
                    }
                }
            }
        }
    }
}
</code></pre>
<p>But this does not seem to be working. Any input on how to solve this?</p>
","<azure><azure-data-factory><etl>","2023-04-20 21:59:55","41","0","1","76073176","<blockquote>
<p>Parameterise IR name in ADF</p>
</blockquote>
<p>AFAIK, there is no such option available to parameterize Integration runtime in ADF see this <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/40699"" rel=""nofollow noreferrer"">issue</a>. You have to select it manually.</p>
<blockquote>
<p>If you try to set the parameterize Integration runtime value you might face issue like this.
<img src=""https://i.imgur.com/fSWrNFC.png"" alt=""enter image description here"" /></p>
</blockquote>
<p><strong>Here I tried to parameterize it, but we have to mention specific Integration runtime name.</strong></p>
<p>The current workaround is to duplicate everything like LinkedServices, Datasets change them accordingly and use it in your pipeline.</p>
<p>You can raise the feature request for Integration runtime parameterization <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">here</a></p>
"
"76067191","How to change datatypes on the fly in Copy activity IN Azure data factory","<p><a href=""https://i.stack.imgur.com/mwSIX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mwSIX.png"" alt=""enter image description here"" /></a></p>
<p>Here am copying from Oracle db to Parquet ,here i dont find any option to change datatype because import schema detected date col as INT96 . So how to change back to Date col ?</p>
","<azure><azure-data-factory>","2023-04-20 18:24:24","57","0","1","76071874","<p>AFAIK, In parquet files the <code>DateTimeOffset</code> means <code>Int_96</code> type only. Thats why it is detecting the type as <code>Int96</code>.</p>
<p>Here for sample I am copying the below SQL table with date data to a parquet file of same schema.</p>
<p><img src=""https://i.imgur.com/LwXTIVB.png"" alt=""enter image description here"" /></p>
<p>when I import the mapping of source and sink in copy activity, you can see I got same.</p>
<p><img src=""https://i.imgur.com/7Srv205.png"" alt=""enter image description here"" /></p>
<p>But, when we see the symbol of <code>INT_96</code> it is date time symbol. It means when data is copying into parquet file the date time type data of source will be copied to date time type(means <code>INT_96</code> in parquet type).</p>
<p>And I am able copy without any issues here.</p>
<p><img src=""https://i.imgur.com/bQAg5L7.png"" alt=""enter image description here"" /></p>
<p>When I interchange the source and sink datasets in the copy activity, it recognized it as Date type.</p>
<p><img src=""https://i.imgur.com/UFvZs4w.png"" alt=""enter image description here"" /></p>
<p>This behavior is might be because of the parquet file type nature. But it indicates that when you use your target parquet file in any source, it will be recognized as Date type only which is your required type.</p>
<p>When sink is parquet, you can see the type as date only for that column in the Pipeline JSON mapping.</p>
<p><a href=""https://i.stack.imgur.com/XkqNl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XkqNl.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>So how to change back to Date col ?</p>
</blockquote>
<p>If you want to change you can edit the <code>physicalType</code> from <code>INT_96</code> to date in mapping of the Pipeline JSON.</p>
<p><strong>mapping After changing pipeline JSON:</strong></p>
<p><a href=""https://i.stack.imgur.com/Ckwds.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ckwds.png"" alt=""enter image description here"" /></a></p>
<p>(Or) you can use dataflows in ADF to change the data type. Use Cast transformation in the data flows.</p>
<p><img src=""https://i.imgur.com/3MQa8Xo.png"" alt=""enter image description here"" /></p>
"
"76064699","Azure Data Flow expression : Unable to parse","<p><strong>Context :</strong></p>
<ul>
<li>Inside Azure Data Factory, I have a pipeline which contains a &quot;Foreach&quot; activity.</li>
<li>I'm trying to write an expression for its &quot;Items&quot; setting.</li>
</ul>
<p>In the expression builder, I have entered this :</p>
<pre><code>@if(not(empty(activity('GetReplies').output.resultSets)), activity('GetReplies').output.resultSets[0].rows, array([]))
</code></pre>
<p>To make it more readable for you, I'm breaking it into several lines here (but not in the expression builder) :</p>
<pre><code>@if(
   //condition
   not(empty(activity('GetReplies').output.resultSets)),
   //true
   activity('GetReplies').output.resultSets[0].rows, 
   //false
   array([])
)
</code></pre>
<p><em>Note: The idea is to check if activity &quot;GetReplies&quot; produced a set of items. If yes, return it. If not, return an empty array.</em></p>
<p><strong>There is no error highlighted in the expression builder.</strong> The pipeline gets saved, validated, published.</p>
<p><strong>Problem :</strong>
When I run the pipeline, I get an error &quot;Unable to parse&quot; on this expression.</p>
<p><a href=""https://i.stack.imgur.com/95UXH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/95UXH.png"" alt=""enter image description here"" /></a></p>
<p>Troubleshooting :
I've tried with the simplest possible expression I could find to generate a set of items :</p>
<pre><code>@array([])
</code></pre>
<p>...It still complains that the expression can't be parsed... (same error message)</p>
<p>What's the issue there???</p>
","<azure-data-factory><expressionbuilder>","2023-04-20 13:33:55","26","0","1","76065160","<p>There were <strong>two kinds</strong> of answers in there:</p>
<ol>
<li>Be very careful with <strong>unwanted carriage returns</strong>. For example if your expression builder contains this :
<a href=""https://i.stack.imgur.com/OCePw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OCePw.png"" alt=""enter image description here"" /></a></li>
</ol>
<p>...then it's not correct because of the carriage return before the leading '@'</p>
<ol start=""2"">
<li>Some <strong>typing</strong> problems are not caught, and later deemed &quot;parse errors&quot; at runtime, which is misleading.
<em>Note: In this case, it was the use of <code>array([])</code> because function <code>array</code> is not supposed to take an array but a single value in parameter.</em></li>
</ol>
<p>Replacing <code>array([])</code> with <code>json(&quot;[]&quot;)</code> fixed the issue.</p>
"
"76063959","Error while ingesting data from azure append blob to kusto database using an azure data factory","<p>I have an azure append blob(sharing.json) which is of content-type: application/json. I am trying to ingest this into a kusto database with an azure data factory(ADF) but the ingestion is always failing. I get the following error on the output of ADF:</p>
<pre><code>&quot;errors&quot;: [
        {
            &quot;Code&quot;: 23302,
            &quot;Message&quot;: &quot;ErrorCode=KustoWriteFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Write to Kusto failed with following error: 'An error occurred for source: 'DataReader'. Error: '''.,Source=Microsoft.DataTransfer.Runtime.KustoConnector,''Type=Kusto.Ingest.Exceptions.IngestClientException,Message=An error occurred for source: 'DataReader'. Error: '',Source=Kusto.Ingest,'&quot;,
            &quot;EventType&quot;: 0,
            &quot;Category&quot;: 5,
            &quot;Data&quot;: {},
            &quot;MsgId&quot;: null,
            &quot;ExceptionType&quot;: null,
            &quot;Source&quot;: null,
            &quot;StackTrace&quot;: null,
            &quot;InnerEventInfos&quot;: []
        }
    ]
</code></pre>
<p>Tried taking help from chatGPT and other online resource but no luck so far.</p>
<p>This is my ADF activity config:</p>
<pre><code>{
    &quot;name&quot;: &quot;CopyPipeline_k0h&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Copy_k0h&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 3,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [
                    {
                        &quot;name&quot;: &quot;Source&quot;,
                        &quot;value&quot;: &quot;sil-xms-load-max-data//sharing.json&quot;
                    },
                    {
                        &quot;name&quot;: &quot;Destination&quot;,
                        &quot;value&quot;: &quot;AggregatedSharingTest_v1&quot;
                    }
                ],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;JsonSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;JsonReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;AzureDataExplorerSink&quot;,
                        &quot;ingestionMappingName&quot;: &quot;&quot;,
                        &quot;additionalProperties&quot;: {
                            &quot;tags&quot;: &quot;drop-by:loadtest&quot;,
                            &quot;format&quot;: &quot;multijson&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;validateDataConsistency&quot;: false,
                    &quot;logSettings&quot;: {
                        &quot;enableCopyActivityLog&quot;: true,
                        &quot;copyActivityLogSettings&quot;: {
                            &quot;logLevel&quot;: &quot;Info&quot;,
                            &quot;enableReliableLogging&quot;: true
                        },
                        &quot;logLocationSettings&quot;: {
                            &quot;linkedServiceName&quot;: {
                                &quot;referenceName&quot;: &quot;LoadTestBlob&quot;,
                                &quot;type&quot;: &quot;LinkedServiceReference&quot;
                            },
                            &quot;path&quot;: &quot;debug-logs&quot;
                        }
                    },
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;mappings&quot;: [
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['deviceId']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;deviceId&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['tenant']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;tenant&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['tagsSerialNo']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;tagsSerialNo&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['metricSum']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;metricSum&quot;,
                                    &quot;type&quot;: &quot;Int64&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['metricCount']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;metricCount&quot;,
                                    &quot;type&quot;: &quot;Int64&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['notMetricCount']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;notMetricCount&quot;,
                                    &quot;type&quot;: &quot;Int64&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['timestamp']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;timestamp&quot;,
                                    &quot;type&quot;: &quot;DateTime&quot;
                                }
                            }
                        ],
                        &quot;collectionReference&quot;: &quot;&quot;
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;SourceDataset_k0h&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;DestinationDataset_k0h&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;annotations&quot;: [],
        &quot;lastPublishTime&quot;: &quot;2023-04-18T11:30:35Z&quot;
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p>This is the destination dataset config on ADF:</p>
<pre><code>{
    &quot;name&quot;: &quot;DestinationDataset_k0h&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;LoadTestDump&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureDataExplorerTable&quot;,
        &quot;schema&quot;: [
            {
                &quot;name&quot;: &quot;deviceId&quot;,
                &quot;type&quot;: &quot;string&quot;
            },
            {
                &quot;name&quot;: &quot;tenant&quot;,
                &quot;type&quot;: &quot;string&quot;
            },
            {
                &quot;name&quot;: &quot;tagsSerialNo&quot;,
                &quot;type&quot;: &quot;string&quot;
            },
            {
                &quot;name&quot;: &quot;metricSum&quot;,
                &quot;type&quot;: &quot;long&quot;
            },
            {
                &quot;name&quot;: &quot;metricCount&quot;,
                &quot;type&quot;: &quot;long&quot;
            },
            {
                &quot;name&quot;: &quot;notMetricCount&quot;,
                &quot;type&quot;: &quot;long&quot;
            },
            {
                &quot;name&quot;: &quot;timestamp&quot;,
                &quot;type&quot;: &quot;datetime&quot;
            }
        ],
        &quot;typeProperties&quot;: {
            &quot;table&quot;: &quot;AggregatedSharingTest_v1&quot;
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p>This is the Azure blob storage config on ADF:</p>
<pre><code>{
    &quot;name&quot;: &quot;SourceDataset_k0h&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;LoadTestBlob&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;Json&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobStorageLocation&quot;,
                &quot;fileName&quot;: &quot;sharing.json&quot;,
                &quot;container&quot;: &quot;sil-xms-load-max-data&quot;
            }
        },
        &quot;schema&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;deviceId&quot;: {
                    &quot;type&quot;: &quot;string&quot;
                },
                &quot;tenant&quot;: {
                    &quot;type&quot;: &quot;string&quot;
                },
                &quot;tagsSerialNo&quot;: {
                    &quot;type&quot;: &quot;string&quot;
                },
                &quot;metricSum&quot;: {
                    &quot;type&quot;: &quot;integer&quot;
                },
                &quot;metricCount&quot;: {
                    &quot;type&quot;: &quot;integer&quot;
                },
                &quot;notMetricCount&quot;: {
                    &quot;type&quot;: &quot;integer&quot;
                },
                &quot;timestamp&quot;: {
                    &quot;type&quot;: &quot;string&quot;
                }
            }
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p>I have tested both the source and destination connections on azure portal and they look good. Not sure what exactly is going wrong since the pipeline runs and run details shows data read and data written but the data is never available on Kusto table for querying and eventually fails with above error</p>
","<azure><azure-blob-storage><azure-data-factory><azure-data-explorer><azureportal>","2023-04-20 12:17:34","116","0","1","76101019","<p>I tried with your input JSON from storage account and your pipeline JSON and ended up with same error.</p>
<p><img src=""https://i.imgur.com/KW7TJgF.png"" alt=""enter image description here"" /></p>
<p><strong>In your case</strong>, the reason for this error is <code>additionalProperties</code> in the copy activity sink.</p>
<p>When I removed the <code>additionalProperties</code>, I am able to copy the data successfully.</p>
<p><img src=""https://i.imgur.com/CLoOPcf.png"" alt=""enter image description here"" /></p>
<p>I have 4 rows data in kustos table and you can see two rows inserted from the source using copy activity after removing <code>additonal properties</code>.</p>
<p><img src=""https://i.imgur.com/0Pi0hze.png"" alt=""enter image description here"" /></p>
<p><strong>Data in target table:</strong></p>
<p><img src=""https://i.imgur.com/3HuNtTZ.png"" alt=""enter image description here"" /></p>
<p><strong>This is my Pipeline JSON for your reference:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline2&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [
                    {
                        &quot;name&quot;: &quot;Source&quot;,
                        &quot;value&quot;: &quot;data//myjson.json&quot;
                    },
                    {
                        &quot;name&quot;: &quot;Destination&quot;,
                        &quot;value&quot;: &quot;table1&quot;
                    }
                ],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;JsonSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;JsonReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;AzureDataExplorerSink&quot;,
                        &quot;ingestionMappingName&quot;: &quot;&quot;
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;logSettings&quot;: {
                        &quot;enableCopyActivityLog&quot;: true,
                        &quot;copyActivityLogSettings&quot;: {
                            &quot;logLevel&quot;: &quot;Info&quot;,
                            &quot;enableReliableLogging&quot;: true
                        },
                        &quot;logLocationSettings&quot;: {
                            &quot;linkedServiceName&quot;: {
                                &quot;referenceName&quot;: &quot;AzureDataLakeStorage2&quot;,
                                &quot;type&quot;: &quot;LinkedServiceReference&quot;
                            },
                            &quot;path&quot;: &quot;data/debug-logs&quot;
                        }
                    },
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;mappings&quot;: [
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['deviceId']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;deviceId&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['tenant']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;tenant&quot;,
                                    &quot;type&quot;: &quot;Guid&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['tagsSerialNo']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;tagsSerialNo&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['metricSum']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;metricSum&quot;,
                                    &quot;type&quot;: &quot;Int64&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['metricCount']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;metricCount&quot;,
                                    &quot;type&quot;: &quot;Int64&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['notMetricCount']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;notMetricCount&quot;,
                                    &quot;type&quot;: &quot;Int64&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;path&quot;: &quot;$['timestamp']&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;timestamp&quot;,
                                    &quot;type&quot;: &quot;DateTime&quot;
                                }
                            }
                        ],
                        &quot;collectionReference&quot;: &quot;&quot;
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;Json1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;AzureDataExplorerTable1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
"
"76063912","Split a json file based on number of records in ADF","<p>I am using Copy data activity in azure data factory to read restapi json and save in blob as json file.
I want to split the file as multiple files if the number of records in file exceeds 200k records. (example: maximum number of records in one file is 200k )How can I achieve this azure data factory copy data activity.</p>
<p>Thanks in advance!</p>
","<json><azure><azure-data-factory>","2023-04-20 12:11:57","53","0","1","76065844","<p>If the sink is of delimited text, you can use copy activity itself to split the larger files into smaller file. In order to do that, in sink settings of copy activity, you can mention the required value for <code>Max rows per file</code>.</p>
<p><img src=""https://i.imgur.com/euWzshZ.png"" alt=""enter image description here"" /></p>
<p>Sink file  will be created in the format <code>&lt;sourcefilename&gt;_&lt;000xx&gt;.txt</code>.</p>
<ul>
<li>To split the file using dataflow, follow the below steps.</li>
</ul>
<ol>
<li>Take the source transformation with the source json dataset.</li>
<li>Then take the surrogate key transformation and row_number to each row. Set start value and step value as 1. In this demo, I have created a column called <code>row_num</code> as a surrogate key.</li>
</ol>
<p><img src=""https://i.imgur.com/UnmP5kw.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>Then add derived column transformation and add a new column. Value for that column is given as <code>toInteger((row_num-1)/10)</code>. Here row_num is the surrogate key column which is created in second step. <code>10</code> denotes that the maximum rows in each file should be of 10. you can replace the value with the required number.</li>
</ol>
<p><img src=""https://i.imgur.com/51PAG4a.png"" alt=""enter image description here"" /></p>
<ol start=""4"">
<li>Then again add derived column transformation to add the filename as a column. Here file_name is set as <code>concat(&quot;sink_&quot;,toString(file_part))</code></li>
</ol>
<p><img src=""https://i.imgur.com/NuPt7SI.png"" alt=""enter image description here"" /></p>
<ol start=""5"">
<li>Then add a sink transformation with json dataset. In the sink settings, give <code>Name file as column data</code> as file name option value. Give the <code>file_name</code> as  value for the column name .</li>
</ol>
<p><img src=""https://i.imgur.com/jy4MWrz.png"" alt=""enter image description here"" /></p>
<p>If the pipeline containing this dataflow is run, large Json file is split into smaller files containing maximum 10 rows.</p>
"
"76062233","Auto create table in ADF copy activity for ODBC Sink Dataset (Not Working)","<p>I am trying to perform copy activity between ADLS dataset and ODBC sync dataset (<strong>POSTGRESQL</strong>) But, table is not creating automatically in target dataset and error coming  like</p>
<blockquote>
<p>Failure happened on 'Sink' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42P01] ERROR: relation &quot;semantic_dev.dim_storage_location&quot; does not exist;\nError while preparing parameters,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=Microsoft.DataTransfer.ClientLibrary.Odbc.Exceptions.OdbcException,Message=ERROR [42P01] ERROR: relation &quot;semantic_dev.dim_storage_location&quot; does not exist;\nError while preparing parameters,Source=PSQLODBC35W.DLL**</p>
</blockquote>
<p>How can I perform copy activity <code>WITH AUTO CREATE TABLE FOR</code> odbc CONNECTOR or any other way I can create linked services for <em>postgresql</em> and sync dataset because I am not able to create sink dataset for postgresql</p>
<p>I tried ODBC sink dataset for my postgresql</p>
<p>Dataset JSON</p>
<pre><code>{
    &quot;name&quot;: &quot;AZR_DS_PSQL_PROC_LAYER&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AZR_LS_PSQL_ODBC_STRL_POC&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;parameters&quot;: {
            &quot;Schema_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;Table_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;OdbcTable&quot;,
        &quot;schema&quot;: [],
        &quot;typeProperties&quot;: {
            &quot;tableName&quot;: {
                &quot;value&quot;: &quot;@concat(dataset().Schema_name,'.',dataset().Table_name)&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            }
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/tAnB5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tAnB5.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/JCa7X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JCa7X.png"" alt=""enter image description here"" /></a></p>
","<azure><odbc><azure-data-factory><etl><azure-data-lake>","2023-04-20 08:52:29","57","0","2","76070667","<p>ODBC as a sink does not support auto create table option in ADF. Also, PostgreSQL database as a sink is not supported in ADF.  You can use precopy activity of ODBC and write the query to create the table.
<img src=""https://i.imgur.com/uS2ZZWd.png"" alt=""enter image description here"" /></p>
<p>This will make sure that the table is created before you load the data to sink.</p>
"
"76062233","Auto create table in ADF copy activity for ODBC Sink Dataset (Not Working)","<p>I am trying to perform copy activity between ADLS dataset and ODBC sync dataset (<strong>POSTGRESQL</strong>) But, table is not creating automatically in target dataset and error coming  like</p>
<blockquote>
<p>Failure happened on 'Sink' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42P01] ERROR: relation &quot;semantic_dev.dim_storage_location&quot; does not exist;\nError while preparing parameters,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=Microsoft.DataTransfer.ClientLibrary.Odbc.Exceptions.OdbcException,Message=ERROR [42P01] ERROR: relation &quot;semantic_dev.dim_storage_location&quot; does not exist;\nError while preparing parameters,Source=PSQLODBC35W.DLL**</p>
</blockquote>
<p>How can I perform copy activity <code>WITH AUTO CREATE TABLE FOR</code> odbc CONNECTOR or any other way I can create linked services for <em>postgresql</em> and sync dataset because I am not able to create sink dataset for postgresql</p>
<p>I tried ODBC sink dataset for my postgresql</p>
<p>Dataset JSON</p>
<pre><code>{
    &quot;name&quot;: &quot;AZR_DS_PSQL_PROC_LAYER&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AZR_LS_PSQL_ODBC_STRL_POC&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;parameters&quot;: {
            &quot;Schema_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;Table_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;OdbcTable&quot;,
        &quot;schema&quot;: [],
        &quot;typeProperties&quot;: {
            &quot;tableName&quot;: {
                &quot;value&quot;: &quot;@concat(dataset().Schema_name,'.',dataset().Table_name)&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            }
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/tAnB5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tAnB5.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/JCa7X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JCa7X.png"" alt=""enter image description here"" /></a></p>
","<azure><odbc><azure-data-factory><etl><azure-data-lake>","2023-04-20 08:52:29","57","0","2","76099818","<p>For Copy activity of ADLS to PostgreSQL, we can't choose an auto-create table on the sync side
for that, I used Precopy Script before running copy activity, parametrising all the columns and after reading from adls. and checking to DB if it is not exist then create.
I solved using this approaches</p>
"
"76058147","Is it possible to add calculated range to excel dataset in Azure Data Factory?","<p>We need to move data from Excel (in Sharepoint) to SqlDB in Azure Data Factory.</p>
<p>We have an excel where row amount increase now and then, but last row is Total which we don't want to take along. Excel has data in A-L columns and there are some other data in N-&gt; columns which also we don't want to take.
<a href=""https://i.stack.imgur.com/ItZDH.png"" rel=""nofollow noreferrer"">Excel looks like this</a></p>
<p>At the moment we have hard-coded range A1:L9999, but that takes also the 'Total' row.
<a href=""https://i.stack.imgur.com/ONzDc.png"" rel=""nofollow noreferrer"">Dataset looks like this</a></p>
<p>We would like to have range which checks in which row has A? = 'Total' and then create range A1:L? It looks like range needs to be in exact format, because when we tried like '@{concat('A1:L','=MATCH(&quot;Total&quot;,A:A,0)')}' we got error:</p>
<p>ErrorCode=ExcelInvalidCellRange,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Invalid excel cell range '@{concat('A1:L','=MATCH(&quot;Total&quot;,A:A,0)')}' with error 'Input string was not in a correct format.'.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.FormatException,Message=Input string was not in a correct format.,Source=mscorlib,</p>
<p>Any ideas?</p>
<p>My first question, sorry about possibly missing details.</p>
<p>EDIT: We probably need this same solution to fit to multiple excels with same or different logics in range (or no range at all).</p>
","<excel><azure><azure-data-factory>","2023-04-19 19:23:39","44","0","1","76062599","<blockquote>
<p>Is it possible to add calculated range to excel dataset in Azure Data Factory?</p>
</blockquote>
<p>As per your information you have last row in your file is <strong>Total</strong> and  you want to get that row number and pass the value dynamically in range.</p>
<p><strong>To achieve this follow below steps:</strong></p>
<ul>
<li><p>You need to first lookup the file and <strong>it will give you count of the rows in file</strong>. you will get output something like below if you have not selected first row as header you will get count as <strong>9</strong> and if you selected first row as header you will get <strong>8</strong> count.
<img src=""https://i.imgur.com/IeBScL7.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then pass this count in range of copy activity by subtracting 1 from it as you don't want las row which has Total to do this create dataset parameter foe <code>lastrange</code>
<img src=""https://i.imgur.com/ezZecco.png"" alt=""enter image description here"" />
pass this parameter in range as <code>@{concat('A1:L',dataset().lastrange)}</code>
<img src=""https://i.imgur.com/eSplxTz.png"" alt=""enter image description here"" />
The pass the lookup count by subtracting 1 from it with expression <code>@sub(int(activity('Lookup1').output.count),1)</code>
<img src=""https://i.imgur.com/hXJ9fwM.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>Output and data preview for above:</strong>
<img src=""https://i.imgur.com/iWCn5lN.png"" alt=""enter image description here"" /></p>
"
"76057693","How can I configure Azure Data Factory to create Table Storage tables during a copy activity (using SAS token)?","<p>I'm creating a data factory pipeline to copy data from blob storage to table storage.  The table may or may not exist in table storage and needs to be created if it doesn't exist.  The Authentication method for my table storage linked service must be a SAS token (the value of which I'm grabbing from Key Vault).</p>
<p>Everything about my linked service configuration and pipeline works fine, except that the pipeline fails if the table sink doesn't exist.  I've tried a similar configuration using Account Key authentication that works, but I'm looking for a way to do this with SAS token authentication.</p>
<h2>Current Configuration</h2>
<p>This is my Linked Service configuration for the table storage account:</p>
<pre><code>{
    &quot;name&quot;: &quot;Table Storage Account&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;parameters&quot;: {
            &quot;StorageAccountName&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;TableName&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureTableStorage&quot;,
        &quot;typeProperties&quot;: {
            &quot;sasUri&quot;: &quot;https://@{linkedService().StorageAccountName}.table.core.windows.net/@{linkedService().TableName}&quot;,
            &quot;sasToken&quot;: {
                &quot;type&quot;: &quot;AzureKeyVaultSecret&quot;,
                &quot;store&quot;: {
                    &quot;referenceName&quot;: &quot;Key Vault&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                },
                &quot;secretName&quot;: {
                    &quot;value&quot;: &quot;@{linkedService().StorageAccountName}-sas&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        }
    }
}
</code></pre>
<p>These are the SAS Token settings:</p>
<pre><code>sv=2021-12-02&amp;ss=t&amp;srt=sco&amp;sp=rwlacu&amp;se=2023-05-02T03:00:00Z&amp;st=2023-04-19T16:09:39Z&amp;spr=https&amp;sig=[REDACTED]
</code></pre>
<p>This is the Data Set configuration used by the Copy Data activity:</p>
<pre><code>{
    &quot;name&quot;: &quot;StorageTable&quot;,
    &quot;properties&quot;: {
        &quot;description&quot;: &quot;Dataset for the azure table account.&quot;,
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;Table Storage Account&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;,
            &quot;parameters&quot;: {
                &quot;StorageAccountName&quot;: {
                    &quot;value&quot;: &quot;@dataset().StorageAccountName&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;TableName&quot;: {
                    &quot;value&quot;: &quot;@dataset().TableName&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        &quot;parameters&quot;: {
            &quot;StorageAccountName&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;TableName&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureTable&quot;,
        &quot;schema&quot;: [],
        &quot;typeProperties&quot;: {
            &quot;tableName&quot;: {
                &quot;value&quot;: &quot;@dataset().TableName&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            }
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p>This is the sink configuration for the Copy Data activity (though nothing seems relevant here):</p>
<pre><code>&quot;sink&quot;: {
    &quot;type&quot;: &quot;AzureTableSink&quot;,
    &quot;azureTableInsertType&quot;: &quot;replace&quot;,
    &quot;azureTablePartitionKeyName&quot;: {
        &quot;value&quot;: &quot;PartitionKey&quot;,
        &quot;type&quot;: &quot;Expression&quot;
    },
    &quot;azureTableRowKeyName&quot;: {
        &quot;value&quot;: &quot;RowKey&quot;,
        &quot;type&quot;: &quot;Expression&quot;
    },
    &quot;writeBatchSize&quot;: 10000
}
</code></pre>
<p>With this configuration, all connections can be validated successfully in the portal, but ADF won't create a table if it doesn't exist.</p>
<h3>Example Error Message</h3>
<p>ErrorCode=FailedStorageOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A storage operation failed with the following error '0:The table specified does not exist.
RequestId:0c4fc844-d002-0001-0d87-738fd1000000
Time:2023-04-20T12:58:38.1032528Z'.,Source=,''Type=Microsoft.WindowsAzure.Storage.StorageException,Message=0:The table specified does not exist.
RequestId:0c4fc844-d002-0001-0d87-738fd1000000
Time:2023-04-20T12:58:38.1032528Z,Source=Microsoft.WindowsAzure.Storage,StorageExtendedMessage=0:The table specified does not exist.
RequestId:0c4fc844-d002-0001-0d87-738fd1000000
Time:2023-04-20T12:58:38.1032528Z,,'</p>
<h3>Other Attempts</h3>
<h4>Update SAS URI</h4>
<p>I changed the &quot;sasUri&quot; to <code>https://@{linkedService().StorageAccountName}.table.core.windows.net</code> (removing the table name), hoping that with the base account URI, data factory would figure out which REST URL paths to use depending on the operation.</p>
<p>This change broke the connection validation in the portal (which I ignored for testing purposes), but the pipelines still worked fine (probably because the Table Name was still provided in the Data Set).  Unfortunately, it still did not create tables that do not exist.</p>
","<azure-data-factory><azure-table-storage><sas-token>","2023-04-19 18:18:06","75","0","1","76072822","<blockquote>
<p>How can I configure Azure Data Factory to create Table Storage tables during a copy activity (using SAS token)?</p>
</blockquote>
<p><em><strong>Follow below process to create Table Storage tables during a copy activity (using SAS token and check if you missed any):</strong></em></p>
<ul>
<li><p>First grab the <code>SAS token</code> with below settings from <strong><code>storage account &gt;&gt; Security +networking &gt;&gt; shared access signature</code></strong>
<img src=""https://i.imgur.com/MekYuEr.png"" alt=""enter image description here"" />
Generate and copy the <code>SAS token</code>
<img src=""https://i.imgur.com/wL3WIug.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Create a key vault to store this <code>SAS token</code> as secret. In this key vault create a secret add <strong>name</strong> as <code>storage_account_name-sas</code> and <strong>secret</strong> as <code>SAS token</code>
<img src=""https://i.imgur.com/i4pA9Cz.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/jQPfjil.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Now created linked service with the above values and some linked service parameters.
As we have created linked service parameters for storage account name and table name here, I provided <strong>SAS URL</strong> as <code>https://@{linkedService().StorageAccountName}.table.core.windows.net/@{linkedService().TableName}</code></p>
</li>
</ul>
<p><strong>linked service settings:</strong></p>
<p><img src=""https://i.imgur.com/qvPwxMU.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Now create dataset for azure table storage with above linked service and then crate a dataset parameter for storage account name and table name in dataset.
<img src=""https://i.imgur.com/7QPXq0B.png"" alt=""enter image description here"" />
Then add these parameters to respective linked service properties and table.
<img src=""https://i.imgur.com/l06x2ai.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Now created pipeline added copy activity in it and selected source as blob file.
<img src=""https://i.imgur.com/EzdMlvu.png"" alt=""enter image description here"" />
Then passed values to dataset properties storage account name and table name as <strong><code>sampleblob5</code> which is not exist in table storage.</strong>
<img src=""https://i.imgur.com/Gd8eCol.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>Pipeline ran successfully:</strong></p>
<p><img src=""https://i.imgur.com/p7ppvxJ.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<hr />
<p><em><strong>Before running pipeline (sampleblob5 table is not exist):</strong></em>
<img src=""https://i.imgur.com/VEV1xps.png"" alt=""enter image description here"" /></p>
<p><em><strong>After running pipeline (Copy activity created sampleblob5 file):</strong></em>
<img src=""https://i.imgur.com/yssi6Gb.png"" alt=""enter image description here"" /></p>
"
"76056563","Azure Synapse Data flow, how to drop a field within an array","<p>Looking to drop a field from an array of objects in synapse data flows.</p>
<p>Example data:</p>
<p>&quot;Animals&quot;:[{&quot;name&quot;:&quot;tiger&quot;,&quot;colour&quot;:&quot;white&quot;,&quot;location&quot;:&quot;zoo&quot;},{&quot;name&quot;:&quot;eagle&quot;,&quot;colour&quot;:&quot;brown&quot;,&quot;location&quot;:&quot;wild&quot;}]</p>
<p>Expected Output:</p>
<p>&quot;Animals&quot;:[{&quot;name&quot;:&quot;tiger&quot;,&quot;location&quot;:&quot;zoo&quot;},{&quot;name&quot;:&quot;eagle&quot;,&quot;location&quot;:&quot;wild&quot;}]</p>
<p>Looking for a way to drop the field. Thanks very much!</p>
","<azure><azure-data-factory><azure-synapse>","2023-04-19 15:55:33","59","0","1","76061121","<ul>
<li>You can use a dataflow and copy activity to achieve your requirement. I have used a flatten transformation with following configuration:</li>
</ul>
<p><img src=""https://i.imgur.com/di9buq1.png"" alt=""enter image description here"" /></p>
<ul>
<li>This would give some resulting data which I am writing to dataflow activity output using sink cache.</li>
</ul>
<p><img src=""https://i.imgur.com/A4z1PMF.png"" alt=""enter image description here"" /></p>
<ul>
<li>In the pipeline, I have taken a copy data activity after the dataflow activity with source file as follows (1 row and 1 column which will be ignored anyway).</li>
</ul>
<p><img src=""https://i.imgur.com/Wh75iru.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, add an additional column as shown below:</li>
</ul>
<pre><code>{&quot;Animals&quot;:@{activity('Data flow1').output.runStatus.output.sink1.Value}}
</code></pre>
<p><img src=""https://i.imgur.com/MyxqGXx.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, choose sink file as delimited file with following configurations:</li>
</ul>
<p><img src=""https://i.imgur.com/JpvEGwU.png"" alt=""enter image description here"" /></p>
<ul>
<li>In the mapping, keep only the above created additional column and delete the rest.</li>
</ul>
<p><img src=""https://i.imgur.com/rfdh2Em.png"" alt=""enter image description here"" /></p>
<ul>
<li>When you run the pipeline, you would get the data as required. You can create a JSON dataset and use this file as a JSON file.</li>
</ul>
<p><img src=""https://i.imgur.com/5iRhX3P.png"" alt=""enter image description here"" /></p>
"
"76055936","An easy way of running code snippets written in Azure Data Flow script / Expression Builder?","<p><strong>Some context :</strong>
When you use Azure Data Factory, you can create &quot;Data Flows&quot; which in turn contain graphical logic bricks, which themselves can transform data with an embedded code editor (the &quot;Dataflow expression builder&quot;) that lets you execute scripts written in a native language called Azure Data Flows script (see picture).</p>
<p><a href=""https://i.stack.imgur.com/ks60h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ks60h.png"" alt=""enter image description here"" /></a></p>
<p><strong>Questions:</strong></p>
<ol>
<li><p>Is that the real name of this language? <strong>Does it have a more common name?</strong> Is this language the twin brother of another, more widely adopted language?</p>
</li>
<li><p><strong>Is there an easy way of executing code snippets of it?</strong> Nowadays you can execute C# or many other languages directly in the web browser, with sites such as &quot;Fiddle&quot; and whatnot. Is there a place where I can test simple scripts of this scripting language without all the hassle of setting up Azure Data Flows, which are unbearably unreadable and unbearably slow?</p>
</li>
</ol>
<p><em>Note: My specific use case is that I quickly want to test some regExp and text replacement, but even the Debug mode is excruciatingly slow, it takes several minutes to run the flow after each change.</em></p>
","<azure-data-factory><expressionbuilder>","2023-04-19 14:52:27","26","0","1","76064698","<p>The language is the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-transformation-functions"" rel=""nofollow noreferrer"">Data Flow Expression</a> language. Data Flows are executed as Spark Notebooks at run time, so the code is ultimately converted to Scala.</p>
<p>Since the conversion is handled internally, there is no way for you to interject custom code snippets. Synapse can more readily support custom notebooks than Data Factory.</p>
"
"76055344","Azure Data Factory Retrieve the status of ForEach Activity","<p><a href=""https://i.stack.imgur.com/ZLbiJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZLbiJ.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/YakOn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YakOn.png"" alt=""enter image description here"" /></a></p>
<p>In Azure Data Factory is it possible to retrieve the status (Succeeded, Failed etc) of a ForEach activity (that iterated over numerous items). We would like to store the status of ForEach in the OnComplete of ForEach in a stored procedure call. When we try to access activity outputs ForEach activity is not available.</p>
","<azure><azure-data-factory>","2023-04-19 13:51:45","32","0","1","76056053","<blockquote>
<p>Azure Data Factory Retrieve the status of ForEach Activity.</p>
</blockquote>
<p><strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">Foreach activity</a> is just a loop it only iterates the items. It does not give any output because of it we can't get foreach activity in activity output outside of it.</strong></p>
<p><img src=""https://i.imgur.com/9TAbi0n.png"" alt=""enter image description here"" /></p>
<p>The only work around to your requirement is to set different set variables for <code>On success</code> and <code>on failure</code> of foreach activity as below:</p>
<p><img src=""https://i.imgur.com/htD2zec.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>We are trying to get the status of the foreach activity (overall).</p>
</blockquote>
<p>As we know ForEach activity fails when the inner activity fails for a single time so based on that we can create logic below.</p>
<ul>
<li><p>create append variable activity and get the status from the copy activity for each run with expression <code>@activity('Copy data1').output.executionDetails[0].status</code>
<img src=""https://i.imgur.com/nWrzgH1.png"" alt=""enter image description here"" /></p>
</li>
<li><p>After this create a set variable outside of the foreach activity and set the value <strong>Failed</strong> if <strong>Failed</strong> is present in <code>activitystatus</code> array otherwise <strong>Succeed</strong> with expression <code>@if(contains(variables('activitystatus'),'Failed'),'Failed','Succeed')</code>
<img src=""https://i.imgur.com/AAaymtc.png"" alt=""enter image description here"" /></p>
</li>
</ul>
"
"76051637","How can i get refresh token from xero custom connetion","<p>How can i get refresh token from xero custom connetion, when i work with previous account i could get this refresh token. but now it it now provided.now when i use this <a href=""https://identity.xero.com/connect/token"" rel=""nofollow noreferrer"">https://identity.xero.com/connect/token</a>  result is only {
&quot;access_token&quot;: &quot;########################################################################&quot;,
&quot;expires_in&quot;: 1800,
&quot;token_type&quot;: &quot;Bearer&quot;,
&quot;scope&quot;: &quot;app.connections marketplace.billing&quot;
}.
refresh token is not included in result</p>
<p>in here &quot;https://devblog.xero.com/introducing-custom-connections-72c32297382&quot; he mention about &quot;Goodbye refresh tokens&quot;. is it means cant we get this refresh token now????</p>
<p>currently i worked on poc, here we used Azure Datafactory, and need to create xero linked service in oauth2.0. but in there refresh token is required</p>
<p>can we get refresh token from xero custom connection(free trail(developer purpose)) oauth2.0</p>
","<azure><oauth-2.0><azure-data-factory><xero-api><linked-service>","2023-04-19 07:08:00","45","0","1","76051752","<p>Custom connections do not use refresh tokens, you just request a new access token.</p>
<p>Most connectors eg Azure Datafactory are built for the standard codeflow web applications and so we would suggest that you stop using the custom connection and create a web app instead.</p>
<p><a href=""https://developer.xero.com/documentation/guides/oauth2/overview/"" rel=""nofollow noreferrer"">https://developer.xero.com/documentation/guides/oauth2/overview/</a></p>
"
"76048215","{""ErrorCode"":""invalid_request"",""Error"":""Unsupported grant type : "",""token_type"":""Bearer""}","<p>I am using the same setup I used in Postman, but I am getting an error in Azure data factory! Help me!</p>
<p>grant_type:client_credentials
client_id:xxxxxxxxxxxxxxxxxxxx
client_secret:xxxxxxxxxxxxxxxxx</p>
<p>The above, I tried inputting in the key value pairs in the body under x-www-form-urlencoded thats the only time it works, not with Bosy-&gt;raw-&gt;json nor raw-&gt;text</p>
<p>I tried the Json format but it doesnt work in both postman and ADF</p>
<p>{
&quot;grant_type&quot;: &quot;client_credentials&quot;,
&quot;client_id&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxx&quot;,
&quot;client_secret&quot;: &quot;xxxxxxxxxxxxxxxxxxxx&quot;
}</p>
<p>grant_type=client_credentials&amp;client_id=xxxxxxxxxxxxxxxxxxx&amp;client_secret=xxxxxxxxxxxxxxxxx</p>
<p>Both say the error is {&quot;ErrorCode&quot;:&quot;invalid_request&quot;,&quot;Error&quot;:&quot;Unsupported grant type : &quot;,&quot;token_type&quot;:&quot;Bearer&quot;}</p>
<p>But when I enter inputting in the key value pairs in the body -&gt; x-www-form-urlencoded thats the only time it works, but when I use application/x-www-form-urlencoded and used the Key value edit to copy paste the format which was
grant_type:client_credentials
client_id:xxxxxxxxxxxxxxxxxxxx
client_secret:xxxxxxxxxxxxxxxxx
It still gives me same error in Azure data Factory but seems to work in postman</p>
<p>I tried content type as both text/plain, and application/x-www-form-urlencoded in headers</p>
","<rest><azure-data-factory>","2023-04-18 19:03:43","41","0","1","76051868","<p><strong>To resolve the error. Please follow the below steps:</strong></p>
<p><strong>Step1:</strong>  Go to the active directory and check whether created AD application has enough API permission or not.</p>
<p><strong>Step2:</strong> Create web activity to get bearer token from ADF.</p>
<p>URL : <code>https://login.microsoftonline.com/&lt;Tenant_Id&gt;/oauth2/token</code></p>
<p><img src=""https://i.imgur.com/ciA7Tux.png"" alt=""enter image description here"" /></p>
<p><strong>Method :</strong>  POST</p>
<p>Note: Make sure to provide <code>resource</code> inside the body.</p>
<p><strong>Body:</strong>  <code>grant_type=client_credentials&amp;client_id=&lt;client_id&gt;&amp;client_secret=&lt;client_secret&gt;&amp;resource=https://graph.microsoft.com/</code></p>
<p><strong>Header:</strong>  <code>Content-Type:application/x-www-form-urlencoded</code></p>
<p><img src=""https://i.imgur.com/LIqnLRq.png"" alt=""enter image description here"" /></p>
<p>For more information <strong>refer</strong> this <a href=""https://stackoverflow.com/questions/75230149/how-to-generate-bearer-token-via-azure-data-factory/75232048#75232048"">SO</a> thread.</p>
"
"76048078","How to dynamically use upsert in ADF to an Azure SQL database","<p>Inside of my pipeline, I'm running a lookup to gather tables from one database to transfer to another.</p>
<pre><code>select table_schema,table_name from information_schema.tables where table_schema = 'schema1' OR TABLE_SCHEMA = 'schema2'
</code></pre>
<p>Then I do a copy activity for each of those tables. And at the moment I've got the sink option set to &quot;insert&quot;. However, I would like to try and use upsert, but I've got to identify &quot;key column(s)&quot; in order to do so. What's the best way to try and get these dynamically? And which columns are best used for this? Columns that will contain a unique value for that row?</p>
","<azure><azure-data-factory>","2023-04-18 18:46:01","90","0","2","76051433","<p>For the key columns, you need to use a column or combination of columns that uniquely identify a row in that table.</p>
<p>You can add this key list to the result of the query above, either by getting them from metadata (if you have a primary key or a unique index on that table), or by keeping them in some management table, and querying that table.</p>
<p>Then you can use dynamic content in ADF to provide the key columns on the sink tab. Please note that the key column should be of type array (because its a list of columns).</p>
"
"76048078","How to dynamically use upsert in ADF to an Azure SQL database","<p>Inside of my pipeline, I'm running a lookup to gather tables from one database to transfer to another.</p>
<pre><code>select table_schema,table_name from information_schema.tables where table_schema = 'schema1' OR TABLE_SCHEMA = 'schema2'
</code></pre>
<p>Then I do a copy activity for each of those tables. And at the moment I've got the sink option set to &quot;insert&quot;. However, I would like to try and use upsert, but I've got to identify &quot;key column(s)&quot; in order to do so. What's the best way to try and get these dynamically? And which columns are best used for this? Columns that will contain a unique value for that row?</p>
","<azure><azure-data-factory>","2023-04-18 18:46:01","90","0","2","76061376","<p>Create database linked services for Look Up, Source and Destination Create pipeline and perform look up activity to get list of tables in source database with query option using below query.</p>
<pre><code>SELECT TABLE_SCHEMA,TABLE_NAME FROM information_schema.TABLES
WHERE TABLE_TYPE = 'BASE TABLE' and TABLE_SCHEMA = 'dbo'
</code></pre>
<p>After successful execution of lookup connect foreach activity enable sequential and add <code>@activity('Lookup1').output.value</code> as dynamic content to the for each add look up activity to foreach with query execution using below query to get key column of tables</p>
<pre><code>SELECT column_name
FROM information_schema.key_column_usage
</code></pre>
<p>after successful execution of lookup connect copy data activity create Source sql dataset with created linked and create two parameters named Schema and tableName to retrieve tables of database and given below values:</p>
<pre><code>schema: @item().TABLE_SCHEMA
tableName: @item().TABLE_NAME
</code></pre>
<p>Create sink dataset and create above parameters and give same value. Enable upsert option and add dynamic content as <code>@createArray(activity('Lookup2').output.firstRow.column_name)</code> for key column</p>
<p><img src=""https://i.imgur.com/sSLUgQN.png"" alt=""enter image description here"" /></p>
<p>Debug the pipeline it executing successfully and updating the tables in target successfully.</p>
<p><img src=""https://i.imgur.com/TesyzyM.png"" alt=""enter image description here"" /></p>
<p>Pipeline Json:</p>
<pre><code>{
    &quot;name&quot;: &quot;Pipeline 2&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;AzureSqlSource&quot;,
                        &quot;sqlReaderQuery&quot;: &quot;SELECT TABLE_SCHEMA,TABLE_NAME FROM information_schema.TABLES\nWHERE TABLE_TYPE = 'BASE TABLE' and TABLE_SCHEMA = 'dbo'&quot;,
                        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                        &quot;partitionOption&quot;: &quot;None&quot;
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;AzureSqlTable3&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Lookup1').output.value&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Lookup2&quot;,
                            &quot;type&quot;: &quot;Lookup&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                                    &quot;sqlReaderQuery&quot;: &quot;SELECT column_name\nFROM information_schema.key_column_usage&quot;,
                                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                                    &quot;partitionOption&quot;: &quot;None&quot;
                                },
                                &quot;dataset&quot;: {
                                    &quot;referenceName&quot;: &quot;AzureSqlTable3&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Copy_3q5&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Lookup2&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [
                                {
                                    &quot;name&quot;: &quot;Source&quot;,
                                    &quot;value&quot;: &quot;files//input.xml&quot;
                                },
                                {
                                    &quot;name&quot;: &quot;Destination&quot;,
                                    &quot;value&quot;: &quot;dbo.input&quot;
                                }
                            ],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                                    &quot;partitionOption&quot;: &quot;None&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;AzureSqlSink&quot;,
                                    &quot;writeBehavior&quot;: &quot;upsert&quot;,
                                    &quot;upsertSettings&quot;: {
                                        &quot;useTempDB&quot;: true,
                                        &quot;keys&quot;: {
                                            &quot;value&quot;: &quot;@createArray(activity('Lookup2').output.firstRow.column_name)&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    },
                                    &quot;sqlWriterUseTableLock&quot;: false,
                                    &quot;disableMetricsCollection&quot;: false
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;translator&quot;: {
                                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                                    &quot;typeConversion&quot;: true,
                                    &quot;typeConversionSettings&quot;: {
                                        &quot;allowDataTruncation&quot;: true,
                                        &quot;treatBooleanAsNumber&quot;: false
                                    }
                                }
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;DestinationDataset_3q5&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;tableName&quot;: {
                                            &quot;value&quot;: &quot;@item().TABLE_NAME&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;schema&quot;: {
                                            &quot;value&quot;: &quot;@item().TABLE_SCHEMA&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;AzureSqlTable2&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;schema&quot;: {
                                            &quot;value&quot;: &quot;@item().TABLE_SCHEMA&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;tableName&quot;: {
                                            &quot;value&quot;: &quot;@item().TABLE_NAME&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;ok&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;lastPublishTime&quot;: &quot;2023-04-19T11:45:57Z&quot;
    },
    &quot;type&quot;: &quot;Microsoft.Synapse/workspaces/pipelines&quot;
}
</code></pre>
"
"76047290","Get data in Azure App Insights from custom tables in Azure Log Analytics","<p>I know that App Insights stores its data in a Log Analytics Workspace where it creates some specific tables. The charts and log tables visible in App Insights pull the data from those specific tables. I just wanted to know if it is also possible to view data in App Insights from other tables in the Log Analytics.</p>
<p>My use case is that we have a bunch of resources that are using App Insights for logging. But Azure Data Factory only supports Log Analytics for logging, so we wanted to find a way to get all the info in one place. Obviously the better approach would be to just use Log Analytics for all, and we might start doing that, but if there is some way then a lot of re-configuration could be avoided.</p>
<p>I have already searched a lot on this, and what I've gathered is that <strong>this is not possible</strong>. Having said that, I haven't actually seen this specifically mentioned anywhere, neither in Microsoft docs nor in any forum. So I just wanted a final concrete statement on this, hence my question.</p>
","<azure><azure-data-factory><azure-application-insights><azure-log-analytics><azure-monitoring>","2023-04-18 17:05:35","41","0","1","76054769","<p>It depends on what you mean with</p>
<blockquote>
<p>[..] to view data in App Insights from other tables in the Log Analytics. [..]</p>
</blockquote>
<p>Azure Monitor &amp; Application Insights support <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/visualize/workbooks-overview"" rel=""nofollow noreferrer"">workbooks</a> with lots of features like:</p>
<ul>
<li>Combine data from several different sources within a single report. You can create composite resource views or joins across resources to gain richer data and insights that would otherwise be impossible. (<a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/visualize/workbooks-overview#data-sources"" rel=""nofollow noreferrer"">source</a>)</li>
</ul>
<p>You can visualize data from all tables in Application Insights and the Log Analytic Workspace. You can also write <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/logs/cross-workspace-query#query-across-log-analytics-workspaces-and-from-application-insights"" rel=""nofollow noreferrer"">cross resource</a> Kusto queries. For example:</p>
<pre><code>let AiRequests = app(&quot;/subscriptions/xxx/resourceGroups/rg-xxx/providers/microsoft.insights/components/gba-insights&quot;).requests 
| project Time = timestamp, Name = name, Url = url;
let wsRequests = workspace(&quot;xxx-xxx-xx-xxx-xxx&quot;).AppRequests 
| project Time = TimeGenerated, Name, Url;
union wsRequests, AiRequests
| order by Time desc 
</code></pre>
<p>shows request from both Application Insights and Log Analytics Workspace in one result set.</p>
"
"76047110","When copying tables from one database and inserting into another with ADF, does the data duplicate when pipeline is run multiple times?","<p>I'm wondering if -when doing copy activity- picking the &quot;insert&quot; option on the sink and running that pipeline multiple times, does that end up duplicating the data, or will it only insert new rows from the source tables into the sink tables after initial run?</p>
","<azure><azure-data-factory>","2023-04-18 16:42:35","25","0","1","76047273","<p>In case of insert, ADF only appends the data as is. So if the pipeline is run multiple times, it would duplicate the data at sink.
To avoid it, you can either use upsert option ( disadvantage is deleted record at source might not be deleted from sink) or use pre copy script at sink to truncate table at sink before running copy activity</p>
"
"76045791","With Azure Data Factory how can I unfold Objects of objects?","<p>I have this data</p>
<p><a href=""https://i.stack.imgur.com/rCiv3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rCiv3.png"" alt=""enter image description here"" /></a></p>
<p>The RuleManifestsProperty contains objects &quot;1&quot;, &quot;Latitude&quot; and &quot;pumpe&quot;. I need a way unpivot these objects so each objectClassId gets every single object.</p>
<p>It is very important that the objects names are applied dynamically. It differs what kind of objects that I am handling.</p>
<p>I have tried with Unpivot but with no luck.</p>
<p>This is the object structure of the objects. They share the same object structure.</p>
<p>(propertyAlias as boolean, propertyModelDtid as boolean, propertyModelLegacy as boolean, propertyType as string, regexExtension as string)</p>
","<azure><azure-data-factory>","2023-04-18 14:24:26","48","0","1","76093029","<p>Unpivot transformation does not support columns with complex data type. In order to achieve the requirement, you can use the flatten transformation followed by parse transformation. Below is the detailed approach.</p>
<p><strong>Sample Input data:</strong></p>
<pre class=""lang-json prettyprint-override""><code>[
    {
    &quot;ObjectClassId&quot;:&quot;aaaa&quot;,
    &quot;RuleManifestsProperty&quot;:{
    &quot;1&quot;:{
        &quot;propertyAlias&quot;:1,
        &quot;propertyModelDtid&quot;:1,
        &quot;propertyModelLegacy&quot;:0,
        &quot;propertyType&quot;:&quot;type1&quot;,
        &quot;regexExtension&quot;:&quot;re1&quot;
        },
    &quot;Latitude&quot;:null,
    &quot;pumpe&quot;:null    
    }
    },
    {
        &quot;ObjectClassId&quot;:&quot;bbbb&quot;,
        &quot;RuleManifestsProperty&quot;:{
        &quot;1&quot;:null,   
        &quot;Latitude&quot;:{
        &quot;propertyAlias&quot;:0,
        &quot;propertyModelDtid&quot;:1,
        &quot;propertyModelLegacy&quot;:0,
        &quot;propertyType&quot;:&quot;type2&quot;,
        &quot;regexExtension&quot;:&quot;re2&quot;
        },
        &quot;pumpe&quot;:null
        }
    },
    {
        &quot;ObjectClassId&quot;:&quot;cccc&quot;,
        &quot;RuleManifestsProperty&quot;:{
        &quot;1&quot;:null,
        &quot;Latitude&quot;:null,
        &quot;pumpe&quot;:{
        &quot;propertyAlias&quot;:0,
        &quot;propertyModelDtid&quot;:0,
        &quot;propertyModelLegacy&quot;:0,
        &quot;propertyType&quot;:&quot;type3&quot;,
        &quot;regexExtension&quot;:&quot;re3&quot;
        }
        }
    }
]
</code></pre>
<ul>
<li>Source transformation is taken with the sample Json dataset.</li>
<li>Derived column transformation (derivedColumn1) is taken to convert the complex datatype of <code>RuleManifestsProperty</code> field to string.
<strong>Expression:</strong><code>toString(RuleManifestsProperty)</code></li>
</ul>
<p><img src=""https://i.imgur.com/MXZzL6D.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then derived column transformation (derivedColumn2) is taken again to convert the string data (which was originally Json of Json object type) to array of Json.</li>
</ul>
<p><strong>Expression</strong>:<code>split(replace(replace(replace(RuleManifestsProperty,'}}','}'),'{&quot;1','&quot;1'),'},','}},'),'},')</code></p>
<p>This expression will remove the characters <code>{</code> and <code>}</code> which are in the beginning and end of each row. Then It will replace  <code>},</code>  with <code>}},</code>. This is done to split the string data into array basis the characters <code>},</code>.</p>
<p><img src=""https://i.imgur.com/s2ehQ3R.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then flatten transformation is taken and Flatten settings is given as</li>
</ul>
<pre><code>Unroll by: RuleManifestsProperty []
Unroll root: RuleManifestsProperty []
</code></pre>
<p><img src=""https://i.imgur.com/lqJqMwh.png"" alt=""enter image description here"" /></p>
<ul>
<li>Derived column transformation (derivedColumn3) is taken to split the &quot;1&quot;,&quot;latitude&quot;,&quot;pumpe&quot; in one column and their value in another column.</li>
</ul>
<p><strong>Expression:</strong></p>
<pre><code>RuleManifestsProperty =  split(replace(RuleManifestsProperty,':{',':{{'),':{')[1]

Value =  split(replace(RuleManifestsProperty,':{',':{{'),':{')[2]
</code></pre>
<p><img src=""https://i.imgur.com/g0J5QYi.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then Parse transformation is taken and settings is given as,</li>
</ul>
<pre><code>format:JSON
Document form: Single document
Columns:
Column:Value
Expression:Value
Output column type: (propertyAlias as boolean, propertyModelDtid as boolean,
propertyModelLegacy as boolean,
propertyType as string,
regexExtension as string)
</code></pre>
<p><img src=""https://i.imgur.com/a4fOGcw.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/WBOsjAi.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ObjectClassId</th>
<th>RuleManifestsProperty</th>
<th>propertyAlias</th>
<th>propertyModelDtid</th>
<th>propertyModelLegacy</th>
<th>propertyType</th>
<th>regexExtension</th>
</tr>
</thead>
<tbody>
<tr>
<td>aaaa</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>type1</td>
<td>re1</td>
</tr>
<tr>
<td>aaaa</td>
<td>Latitude</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>aaaa</td>
<td>pumpe</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>bbbb</td>
<td>1</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>bbbb</td>
<td>Latitude</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>type2</td>
<td>re2</td>
</tr>
<tr>
<td>bbbb</td>
<td>pumpe</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>cccc</td>
<td>1</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>cccc</td>
<td>Latitude</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>cccc</td>
<td>pumpe</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>type3</td>
<td>re3</td>
</tr>
</tbody>
</table>
</div>"
"76043477","Azure Logic App azure_auth - does not have authorization to perform action 'Microsoft.Logic/workflows/listCallbackUrl/action'","<blockquote>
<p>The client 'clientId' with object id 'objectId' does not have
authorization to perform action
'Microsoft.Logic/workflows/listCallbackUrl/action' over scope
'/subscriptions/subscriptionId/resourceGroups/resourceGrpName/providers/Microsoft.Logic/workflows/logicAppName'
or the scope is invalid. If access was recently granted, please
refresh your credentials.</p>
</blockquote>
<p>I'm calling this <a href=""https://learn.microsoft.com/en-us/rest/api/logic/workflows/list-callback-url?tabs=HTTP"" rel=""nofollow noreferrer"">API</a> from ADF to retrieve the Workflows - List Callback Url so that I can use it to execute the logic app and I get the above error.
The &quot;Logic App Contributor&quot; role has been added to the App registration that I'm using.</p>
<p>I think I'm missing the step to get <a href=""https://learn.microsoft.com/en-us/rest/api/logic/workflows/list-callback-url?tabs=HTTP#security"" rel=""nofollow noreferrer"">the bearer token</a> (azure_auth) to Authorise with.</p>
<p>The documentation is sparse on how to that.
A guide would be helpful here.</p>
","<permissions><azure-data-factory><azure-logic-apps><azure-rbac>","2023-04-18 10:24:26","56","0","1","76056266","<p><strong>Yes you need to pass bearer token as authorization to get call back url of logic app.</strong> I have reproduced issue from my side and below are steps I followed,</p>
<ul>
<li><p>Register an application in AD and get client id, client secret and tenant id. Follow this MS <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app"" rel=""nofollow noreferrer"">document</a> to register application in AD.</p>
</li>
<li><p>Once you have required credentials, create a pipeline in ADF as shown below,
<img src=""https://i.imgur.com/g4vGnhV.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Added two set variables in pipeline for getting client id and client secret. You can assign directly or you can get from keyvault as mentioned in <a href=""https://medium.com/analytics-vidhya/azure-data-factory-access-microsoft-graph-api-3bbab165eb40"" rel=""nofollow noreferrer"">document</a>.</p>
</li>
<li><p>Next added a web activity to get token from
<strong>url</strong>:<a href=""https://login.microsoftonline.com/%7Btenantid%7D/oauth2/token"" rel=""nofollow noreferrer"">https://login.microsoftonline.com/{tenantid}/oauth2/token</a></p>
<p><strong>Request type:</strong> Post</p>
<p><strong>Body:</strong>@concat(concat('tenant=tenant-id&amp;client_id=',variables('clientid'),'&amp;client_secret='),variables('client_secret'),'&amp;grant_type=client_credentials&amp;scope=https://graph.microsoft.com/.default')</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/zQ2PkyT.png"" alt=""enter image description here"" /></p>
<ul>
<li>Taken another web activity to get call back url of logic app,
<strong>Url</strong>:<a href=""https://management.azure.com/subscriptions/xxxxx/resourceGroups/xxxxx/providers/Microsoft.Logic/workflows/testlap01/listCallbackUrl?api-version=2016-06-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/xxxxx/resourceGroups/xxxxx/providers/Microsoft.Logic/workflows/testlap01/listCallbackUrl?api-version=2016-06-01</a></li>
</ul>
<p><strong>Request type:</strong> Post</p>
<p><strong>Body:</strong>{}</p>
<p><strong>Headers:</strong>@concat('Bearer ',activity('Web1').output.access_token)</p>
<p><img src=""https://i.imgur.com/iO4OBnD.png"" alt=""enter image description here"" /></p>
<ul>
<li>Debug the pipeline and able to get call back url of logic app,
<img src=""https://i.imgur.com/EoR4Hh1.png"" alt=""enter image description here"" /></li>
</ul>
"
"76043476","Trying to perform ADF pipeline logging in App Insights using Custom Activity","<p>Can anyone help me with how to log each pipeline activity in App Insights. Can we do it from inside the pipeline or through any API calls?</p>
<p>Can custom activity in ADF call a class library in c#?</p>
","<azure><azure-data-factory><azure-application-insights>","2023-04-18 10:24:24","57","0","1","76061353","<blockquote>
<p>There is a custom activity in ADF-I want to know if we can call a c# code from it. So far I only got reference for python code.</p>
</blockquote>
<p>Yes, you can use <strong>Custom activity</strong> to execute C# code from ADF. If you have code to log the activity details, you can make use of Custom activity.</p>
<p>Go through this <a href=""https://blog.devgenius.io/run-your-net-code-in-azure-data-factory-using-azure-batch-ecd2494c16f2"" rel=""nofollow noreferrer"">blog</a> by @<a href=""https://muafzal.medium.com/?source=post_page-----ecd2494c16f2--------------------------------"" rel=""nofollow noreferrer"">Afzal Muhammad</a> to understand more about this.</p>
<p>Or as an alternative you can use <strong>Azure functions also of C# type</strong>. After building the Azure function with your code and call it using http trigger from ADF <strong>Azure function</strong> activity.</p>
<p><strong>My Sample:</strong></p>
<p><img src=""https://i.imgur.com/yTv5KaG.png"" alt=""enter image description here"" /></p>
"
"76043424","Azure data factory concurrency and dependencies","<p>I am building a pipeline in ADF where I want to replicate what I have in a databricks job. <a href=""https://i.stack.imgur.com/Jo8qZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jo8qZ.png"" alt=""databricks job"" /></a> Is has some dependencies and I was wondering how can I do the same in ADF in order to maximize the parallel runs.</p>
<p>Right now I just divided the notebooks in 5 stages <a href=""https://i.stack.imgur.com/dXOWY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dXOWY.png"" alt=""ADF"" /></a> (right now I have only created 3 stages) and was going to run the notebooks on stage 2 only after stage 1 finishes, etc., but I was wondering if I can do something similar to what I have in databricks so I can run notebooks from different stages in parallel and have the same dependencies.</p>
","<azure><concurrency><azure-data-factory><databricks><azure-databricks>","2023-04-18 10:18:46","38","0","1","76051502","<p>You can run activities in parallel in ADF too. Just connect them to all the activities they are dependent on.
<a href=""https://i.stack.imgur.com/jbZFq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jbZFq.png"" alt=""activity dependencies"" /></a></p>
"
"76042156","How to set 2nd row as header in ADF (Azure Data Factory) and skip 3rd and 4th line, while importing data CSV","<p>Setting up pipeline to import data from .csv file into SQL table. Here in below sample file want to make 2nd row as header and skip 3rd and 4th line.
<a href=""https://i.stack.imgur.com/SaC5C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SaC5C.png"" alt=""enter image description here"" /></a></p>
<p>Expected Output in SQL Table should look like below.
<a href=""https://i.stack.imgur.com/mnRba.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mnRba.png"" alt=""enter image description here"" /></a></p>
","<azure><csv><azure-data-factory>","2023-04-18 08:00:33","35","0","1","76043835","<ul>
<li>I have used a copy data activity and dataflow to achieve the requirement. The following is the sample source file data that I have:</li>
</ul>
<p><img src=""https://i.imgur.com/iP5Upiv.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I have used copy data activity with the above file as source. I have selected skip row count value as 1 for this source. The sink has 1st row as header selected.</li>
</ul>
<p><img src=""https://i.imgur.com/rDTFGOl.png"" alt=""enter image description here"" /></p>
<ul>
<li>This would generate the file as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/3EqXggj.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now using dataflow, select source as this above file. Select another source with same file but skip line count value as 2.</li>
</ul>
<p><img src=""https://i.imgur.com/3ncTaTK.png"" alt=""enter image description here"" /></p>
<ul>
<li>This would give data as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/OlNm2zK.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, use filter on the first source to remove all rows and keep only header. Use union operation on this where you union with the second source. Write this file to sink as csv file. The final data would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/Rpf2OYG.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following is the entire pipeline JSON for dataflow:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;copyDataOutputFile&quot;
                },
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;copyDataOutputFileWithSkip&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText3&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;filter1&quot;
                },
                {
                    &quot;name&quot;: &quot;union1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          id as string,&quot;,
                &quot;          first_name as string,&quot;,
                &quot;          date as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; copyDataOutputFile&quot;,
                &quot;source(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false,&quot;,
                &quot;     skipLines: 2) ~&gt; copyDataOutputFileWithSkip&quot;,
                &quot;copyDataOutputFile filter(false()) ~&gt; filter1&quot;,
                &quot;filter1, copyDataOutputFileWithSkip union(byName: false)~&gt; union1&quot;,
                &quot;union1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     partitionFileNames:['op2.csv'],&quot;,
                &quot;     umask: 0022,&quot;,
                &quot;     preCommands: [],&quot;,
                &quot;     postCommands: [],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     partitionBy('hash', 1)) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
"
"76042047","ADF pagination rule to get total row count from rest API and add as end condition","<p>I have created a restAPI connection with base url:<a href=""https://heineken-dev.collibra.com/rest/2.0/assets?offset=%7Boffset%7D"" rel=""nofollow noreferrer"">https://heineken-dev.collibra.com/rest/2.0/assets?offset={offset}</a></p>
<p>and in copy Data activity source pagination rules , I have added as below.
<a href=""https://i.stack.imgur.com/94Jws.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/94Jws.png"" alt=""paginationRule"" /></a></p>
<p>but in the restAPI result set: total: 14518 which is the total number of records.
I want to get this total 14518 into end condition of offset.Can someone help me do that.</p>
<p>Thanks in advance!!</p>
","<azure><azure-data-factory><azure-rest-api>","2023-04-18 07:50:11","52","0","1","76051673","<blockquote>
<p>ADF pagination rule to get total row count from rest API and add as end condition.</p>
</blockquote>
<p><strong>As per you requirement in the rest API result set you are getting <code>total: 14518</code> which is the total number of records.</strong> And you want to use this in the end condition.</p>
<p><strong>To achieve this:</strong></p>
<blockquote>
<p>here I am using this <a href=""https://reqres.in/api/users?page=2"" rel=""nofollow noreferrer"">sample Rest Api</a></p>
</blockquote>
<ul>
<li>First you need to get the result set of api from web activity.
<img src=""https://i.imgur.com/F2gSWQ1.png"" alt=""enter image description here"" />
<strong>Result set:</strong> here you will get that <code>total: 14518</code>
<img src=""https://i.imgur.com/FxlvSfJ.png"" alt=""enter image description here"" /></li>
<li>Now in copy activity set the pagination rule as below:
set Renge as <code>1::1000</code> so it will loop the url infinite time on the offset of 1000 and the set the end condition as <code>MaxRequestNumber : @string(activity('Web1').output.total_pages)</code> so it will end the looping when it reaches the max request number.
<img src=""https://i.imgur.com/W4ISzjO.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>UPDATE</strong></p>
<p>updating answer for other community members. Agreed with @Sukanya by adding <code>$.total</code> the value of total will be taken from the rest API's response in the end condition as below:</p>
<p><img src=""https://i.imgur.com/h2U6I1v.png"" alt=""enter image description here"" /></p>
"
"76038592","Truncate table Sink Pre-copy script","<p>So I am copying 10 different tables from Azure SQLDB to Azure SQLMI, I need to truncate the destination table before it inserts the data. But I am just cannot get to it work. BTW total n00b at Azure ADF.</p>
<p>I have tried a couple of different solutions in other threads but they just do not work for me.</p>
<p>Tried
TRUNCATE TABLE @item().destination.table
TRUNCATE TABLE [@{item().name}]</p>
<p>&quot;Operation on target ForEach_Loop failed: Activity failed because an inner activity failed; Inner activity name: Copy_Tables, Error: The expression 'item().name' cannot be evaluated because property 'name' doesn't exist, available properties are 'source, destination'.&quot;</p>
<p>Help please?</p>
<p><a href=""https://i.stack.imgur.com/9uaj9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9uaj9.png"" alt=""ForEach Loop"" /></a></p>
","<azure-data-factory>","2023-04-17 19:20:26","50","0","1","76041589","<blockquote>
<p>&quot;Operation on target ForEach_Loop failed: Activity failed because an inner activity failed; Inner activity name: Copy_Tables, Error: The expression 'item().name' cannot be evaluated because property 'name' doesn't exist, available properties are 'source, destination'.&quot;</p>
</blockquote>
<p>The error indicates that there are only <code>source</code> and <code>destination</code> keys for <strong>every item</strong> and the tablename must be inside the destination object.</p>
<p>And as per your dynamic content, your ForEach input array can be like this.</p>
<pre><code>[ 
{ 
    &quot;source&quot;: 
        { 
        &quot;table&quot;: &quot;Source1&quot;
         }, 
     &quot;destination&quot;: 
         { 
         &quot;table&quot;: &quot;Target1&quot; 
         } 
 }, 
 {
     &quot;source&quot;: 
         { 
         &quot;table&quot;: &quot;Source2&quot; 
         }, 
     &quot;destination&quot;: 
         { 
         &quot;table&quot;: &quot;Target2&quot; 
         } 
 }, 
 {
     &quot;source&quot;: 
         { 
         &quot;table&quot;: &quot;Source3&quot; 
         }, 
     &quot;destination&quot;: 
         { 
         &quot;table&quot;: &quot;Target3&quot; 
         } 
 }
 ]
</code></pre>
<p>So, give the Truncate query in the pre-copy script using string interpolation  <code>Truncate table @{item().destination.table}</code> as suggested by <strong>@Nandan</strong> in comments.</p>
<p>This is my sample Example using a copy activity.</p>
<p><img src=""https://i.imgur.com/c0scfFw.png"" alt=""enter image description here"" /></p>
<p>You can see the script here:</p>
<p><img src=""https://i.imgur.com/zvSC7ZR.png"" alt=""enter image description here"" /></p>
"
"76036829","Passing parameters from ADF Data flow to Dataset used as source","<p>I need to dynamically pass parameter from pipeline to the dataset that is being used as source from Data Flow.</p>
<p><strong>Details</strong></p>
<p>Creating a Data flow that would be called from the Azure data factory pipeline.
Dataflow takes data from the REST andpoint, so I have a Data Set used as source (The data set will call a REST linked service)</p>
<p>So, my calling sequence looks this</p>
<pre><code>Pipeline -&gt; DataFlow -&gt; REST DataSet -&gt; RestLinkedService
</code></pre>
<p>The data set takes parameter such as a base URL and the endpoint url.  The requirement is that base URL is defined in the ADF Global parameters and not anywhere else.  So, it should get dynamically be passed through the calling sequence.
My data set is:
<a href=""https://i.stack.imgur.com/e6Suw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e6Suw.png"" alt=""enter image description here"" /></a></p>
<p>and it's parameters section is like this:
<a href=""https://i.stack.imgur.com/Dzipv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dzipv.png"" alt=""enter image description here"" /></a></p>
<p>Here is what I do not understand :
I create a parameter in the data source, expecting it be populated with what is comming from the pipeline when it runs the Data Flow Activity.  And I do know how to pass a parameter from pipeline to the data flow being called.
However, when I choose that data set in the Dataflow source definition, I have no way to provide the parameters.  What am missing ?  This is what I see in the Dataflow definition
<a href=""https://i.stack.imgur.com/koLBh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/koLBh.png"" alt=""enter image description here"" /></a></p>
<p>I found that when we run dataflow debug, there is a &quot;Debug Settings&quot; where I can specify the parameter.  And If I use it, it will work and connect.  However !! The problem I have is, that in the debug setting I <strong>HARDCODE</strong> the parameter value.  There is no option to enter dynamic content.  Or am I not looking in the right place ?  And this is <strong>NOT WHAT IS INTENDED</strong>.<br />
<a href=""https://i.stack.imgur.com/GADjR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GADjR.png"" alt=""enter image description here"" /></a></p>
<p>How do I pass the dynamically passed parameter?
Also, is that section &quot;Debug setting&quot; the right place?  Eventually the pipeline will be triggered on the schedule or via the Rest call by a 3rd party in production environment..... So, I won't be the one &quot;debugging&quot; the dataflow.  What concept am I Missing ?</p>
","<azure><azure-data-factory>","2023-04-17 15:31:29","47","0","1","76041058","<p>The purpose of a debug session in data flow is to act as a testing platform for your transformations. It does not write any data to sink. Just it is used to preview the data. In order to pass the dynamic value, debug session is not the correct place. Use the <code>parameters</code> in <code>Execute Data Flow</code> activity in the pipeline to send the parameter value from the pipeline to the data flow activity. Using this, you can define the parameter value in the pipeline before passing it to the data flow activity. Below is the approach.</p>
<ul>
<li><p>In the data flow activity of the pipeline -&gt; under source parameter all the dataset parameters that are created in the dataflow will be listed.</p>
</li>
<li><p>To define the dataset parameter <code>pBaseURL</code> of the dataflow, click add dynamic content and type the dynamic value. <img src=""https://i.imgur.com/0qYHFXv.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Once you defined the value, you can run the pipeline by debug option of pipeline or schedule it by trigger.</p>
</li>
</ul>
"
"76036311","how to connect Azure SQL through Azure data factory both are hosted on different resource group","<p>I am trying to connect Azure SQL through Azure data factory both are hosted on different resource group.</p>
<p>I have tried to add the IP address for SQL firewall setup but struck with dynamic range.</p>
<p>Also got article for adding static IP ranges and given the try to look at the IP address ranges given in the Ms documents for East US 2, but found that my ADF instance generating different IP address than MS document given ranges. got struck here.</p>
<p>Can you please suggest here for the possible solutions. Appreciate all your efforts.</p>
<p>Thanks.</p>
","<azure-active-directory><azure-data-factory>","2023-04-17 14:35:35","33","0","1","76037110","<p>As long as the ADF and Azure SQL database are hosted within the same tenant but different resource groups, Managed Identity authentication is the best security way.
Also w.r.t networking aspect,you can enable allow azure resources access in network property of Azure Server thereby avoiding whitelisting any IP ranges
<a href=""https://i.stack.imgur.com/DqoFZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DqoFZ.png"" alt=""enter image description here"" /></a></p>
<p>Else corresponding to data factory region, you would have to whitelist the IP mentioned in doc:
<a href=""https://learn.microsoft.com/en-us/power-platform/admin/online-requirements"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/power-platform/admin/online-requirements</a></p>
"
"76034918","Azure Data Factory : Pipelines names missing in table when run in Debug","<p>Scenario :</p>
<ul>
<li>Create a Pipeline in Azure Data Factory</li>
<li>Run the pipeline <strong>normally</strong> with an &quot;Add trigger --&gt; Trigger now&quot;</li>
<li>Run the pipeline again in <strong>Debug</strong></li>
<li>Go to the page that shows all the pipelines that have been run or will be run. <strong>(Monitor --&gt; Pipeline runs)</strong></li>
<li>It has two tabs : <strong>&quot;Triggered&quot;</strong> and <strong>&quot;Debug&quot;</strong>. Each of these tabs shows the pipelines you've just run in the previous steps.</li>
</ul>
<p><strong>EXPECTED (CORRECT) :</strong> In both tabs, the &quot;Pipeline name&quot; column is populated</p>
<p><strong>OBSERVED: (BAD) :</strong> In the &quot;Debug&quot; tab, <strong>the &quot;Pipeline name&quot; column is empty.</strong> Everything is accurate expect the name is missing.</p>
<p><a href=""https://i.stack.imgur.com/W8fXk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W8fXk.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-04-17 11:57:08","127","0","1","76058736","<p>Try using an alternate browser - I experienced the same issue in Google Chrome, but the pipeline names did appear when I used Microsoft Edge.</p>
"
"76033862","Is there a way to copy data from Sharepoint On Premise using Azure Datafactory or Azure Synapse Pipelines?","<p>Hello and I hope you are well.</p>
<p>There is this Sharepoint On Premise that stores sa few tables I would like to analyse and transform. I would like to know if it's possible to connect Azure Datafactory or Azure Synapse Pipelines to Sharepoint On Premise in order to copy data to Azure?</p>
<p>I tried to go to linked services to see if there's connector for Sharepoint On Premises, but I only found something for Sharepoint Lists. So I was wondering how else to connect to this data source?</p>
","<sharepoint><azure-data-factory><azure-synapse-pipeline>","2023-04-17 09:55:04","64","0","1","76044921","<p>Yes to copy data from Sharepoint On Premise using Azure Datafactory or Azure Synapse Pipelines. you will need the
<strong>-Application ID
-Application key
-Tenant ID</strong></p>
<p>Open SharePoint Online site link e.g. <code>https://[your_site_url]/_layouts/15/appinv.aspx (replace the site URL).</code></p>
<p>Use the image as reference.
<img src=""https://i.stack.imgur.com/zT5bX.png"" alt=""enter image description here"" /></p>
<p>The above url will allow you grant permission for your application ID.</p>
<p>•   App Domain: contoso.com
•   Redirect URL: <a href=""https://www.contoso.com"" rel=""nofollow noreferrer"">https://www.contoso.com</a>
•   Permission Request XML:
XMLCopy</p>
<pre><code>&lt;AppPermissionRequests AllowAppOnlyPolicy=&quot;true&quot;&gt;
    &lt;AppPermissionRequest Scope=&quot;http://sharepoint/content/sitecollection/web&quot; Right=&quot;Read&quot;/&gt;
&lt;/AppPermissionRequests&gt;
</code></pre>
<p><img src=""https://i.stack.imgur.com/4cWnP.png"" alt=""enter image description here"" /></p>
<p>Go to the Active driectory in the overview section you will get the <strong>Tenant ID</strong>
<img src=""https://i.stack.imgur.com/GXKKq.png"" alt=""enter image description here"" /></p>
<p>Go to app registrations and create a new app registration for <strong>Clinet ID</strong>
<img src=""https://i.stack.imgur.com/5oHpe.png"" alt=""enter image description here"" /></p>
<p>GO to the certificates and secrets for creating a secret value copy all the 3 into a note pad for the future use.
<img src=""https://i.stack.imgur.com/9BuNl.png"" alt=""enter image description here"" />
Go to the ADF or Synapse workspace
Create a Pipeline choose the web activity.
<img src=""https://i.stack.imgur.com/QAW8q.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/5PtCQ.png"" alt=""enter image description here"" />
url as     <code>https://accounts.accesscontrol.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/tokens/Oauth/2</code></p>
<p>Method: <strong>POST</strong></p>
<p>Headers:
<strong>Content-Type: application/x-www-form-urlencoded</strong></p>
<pre><code>Body: grant_type=client_credentials&amp;client_id=[Client-ID]@[Tenant-ID]&amp;client_secret=[Client-Secret]&amp;resource=00000003-0000-0ff1-ce00-000000000000/[Tenant-Name].sharepoint.com@[Tenant-ID]
</code></pre>
<p>Replace the Client ID, Tenant ID, Tenant Name and Clinet Secret with your cofiguration</p>
<p>Next the Url to request the file:
The url looks something like this</p>
<p>https://[site-url]/_api/web/GetFileByServerRelativeUrl('[relative-path-to-file]')/$value
<img src=""https://i.stack.imgur.com/thjMJ.png"" alt=""enter image description here"" />
replace the site url and relative path to the file. the relative path should be the site again and the structre of folder path.</p>
<p>Now go back to the ADF or Synapse pipeline and create a new linked service for HTTP:
<img src=""https://i.stack.imgur.com/EyBOT.png"" alt=""enter image description here"" />
provide the name of the activity like sharepoint</p>
<p>Base url like <code>microsoft.sharepoint.com</code></p>
<p><strong>Disable the certificate</strong></p>
<p>Authenication to <strong>anonymous</strong></p>
<p>Create a Copy activity</p>
<p>Source:
in the source create a dataset it should be a HTTP type and format binary</p>
<p>choose the created linked service for the HTTP.</p>
<p>Relative url will be something like this.</p>
<pre><code>/teams/sharepointaccess/Shared%20Documents/Forms/AllItems.aspx
</code></pre>
<p>In the sink additional header. add dynamic content and concatinate autraization token and output value from the web activity
Like shown in the picture.
<img src=""https://i.stack.imgur.com/xG77w.png"" alt=""enter image description here"" />
<img src=""https://i.stack.imgur.com/ABP3G.png"" alt=""enter image description here"" />
Sink:</p>
<p>In the sink side create a dataset to the ADLS gen 2 in the Binary format
Attach the linked service for the sink
<img src=""https://i.stack.imgur.com/UhI38.png"" alt=""enter image description here"" />
and you can trigger the Pipleline now.</p>
<p><img src=""https://i.stack.imgur.com/E0lqr.png"" alt=""enter image description here"" />
Docs reffered:
You can also use the microsft documentation for your reference.</p>
<ul>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list?tabs=data-factory#prerequisites"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list?tabs=data-factory#prerequisites</a></p>
</li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list?tabs=data-factory#copy-file-from-sharepoint-online"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list?tabs=data-factory#copy-file-from-sharepoint-online</a></p>
</li>
</ul>
"
"76031535","Inserting value stored in a pipeline variable as a new row of sink dataset data","<p>In ADF, I generate pipeline variables such as the pipeline run time, rows copied count, copy status etc. from various activities. I then use Set Variable to create a JSON string consisting of the above values. Now I want to store this string into an existing CosmosDB as a new record.</p>
<p>I cannot use Azure Functions or a Notebook job to insert, as I want it to be possible within ADF without additional dependencies.</p>
<p>I have tried using Data Flow, but it is unable to add a new record from values in a pipeline variable. I cannot use a separate data source to union and add into the CosmosDB as the values I want inserted are generated during runtime.</p>
","<azure-blob-storage><azure-data-factory><azure-cosmosdb><union><pipeline>","2023-04-17 03:28:39","62","0","1","76031808","<p>You can use the individual pipeline variables like <code>pipeline run time</code> , <code>copied rows count</code>, <code>copy status</code> as they are (without creating a json document consisting of all values) and use the copy activity in adf to insert the values in cosmos db container. Below is the approach.</p>
<ul>
<li>Three pipeline variables <code>pipeline run time</code> , <code>copied rows count</code>, <code>copy status</code>  are taken and their values are set using set variable activity here.</li>
</ul>
<p><img src=""https://i.imgur.com/iVDuSlI.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then a copy activity is taken with a dummy source dataset. Source dataset has one row. In source tab of copy activity, New columns are added by clicking <code>+New</code> in additional columns. All three columns are set with the value that is defined using set variable activity.</li>
</ul>
<p><img src=""https://i.imgur.com/9Ejxx8C.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then sink dataset is taken for cosmos dB
with <code>insert</code> as write behavior.</li>
</ul>
<p><img src=""https://i.imgur.com/ajo5Lg0.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then in Mapping section, Schema is imported by giving sample values for the pipeline variables. Then all additional columns that is created in source are mapped to sink columns.</li>
</ul>
<p><img src=""https://i.imgur.com/CTnM7MK.png"" alt=""enter image description here"" /></p>
<ul>
<li>When pipeline is run, data is inserted to cosmos dB.</li>
</ul>
"
"76030095","Adf copy activity","<p>I have 3 files at destination a.txt, b.txt and c.txt. however a.txt and c.txt is already present in my destination (adls) and my copy activity should only copy the remaining b.txt file. How can i achieve this using adf pipeline activities.</p>
<p>I have tried using get metadata for both source and destination if files exist at destination or not. But while copying inside if activity it copies all the files. Not the ones which are not present.</p>
","<foreach><copy><azure-data-factory>","2023-04-16 20:11:24","52","0","2","76031549","<ol>
<li>use 2 get meta data activities in parallel to get the child items list of the source and sink</li>
<li>post success of both get meta data activity, use filter activity with
items as source child items array
and condition as
@not(contains(child items of sink array,item()))</li>
<li>then leverage For each activity with filter activity output as input for iteration and copy the missing files.</li>
</ol>
"
"76030095","Adf copy activity","<p>I have 3 files at destination a.txt, b.txt and c.txt. however a.txt and c.txt is already present in my destination (adls) and my copy activity should only copy the remaining b.txt file. How can i achieve this using adf pipeline activities.</p>
<p>I have tried using get metadata for both source and destination if files exist at destination or not. But while copying inside if activity it copies all the files. Not the ones which are not present.</p>
","<foreach><copy><azure-data-factory>","2023-04-16 20:11:24","52","0","2","76031729","<p>I agree with what <strong>@Nandan</strong> has suggested. The following is the demonstration of the same approach. The following is the output of the get metadata activity on source.</p>
<p><img src=""https://i.imgur.com/p6lmQak.png"" alt=""enter image description here"" /></p>
<ul>
<li>And my sink has files as shown in the image below:</li>
</ul>
<p><img src=""https://i.imgur.com/hqTMw9F.png"" alt=""enter image description here"" /></p>
<ul>
<li>Using filter, get the filenames that are not present in your sink and then use for each to copy these filtered files. The following dynamic content can be used as filter condition:</li>
</ul>
<pre><code>items: @activity('source file list').output.childItems
condition: @not(contains(activity('sink file list').output.childItems,item()))
</code></pre>
<p><img src=""https://i.imgur.com/8Zhg74k.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, you can iterate through this filtered list using <code>@activity('Filter1').output.Value</code> as items for your for each activity. The following is the entire pipeline JSON for the above implementation:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;pipeline2&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;source file list&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;fieldList&quot;: [
                        &quot;childItems&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;sink file list&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;fieldList&quot;: [
                        &quot;childItems&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Filter1&quot;,
                &quot;type&quot;: &quot;Filter&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;source file list&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    },
                    {
                        &quot;activity&quot;: &quot;sink file list&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('source file list').output.childItems&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;condition&quot;: {
                        &quot;value&quot;: &quot;@not(contains(activity('sink file list').output.childItems,item()))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Filter1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Filter1').output.Value&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy data1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                        &quot;recursive&quot;: true,
                                        &quot;enablePartitionDiscovery&quot;: false
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                    }
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                                        &quot;quoteAllText&quot;: true,
                                        &quot;fileExtension&quot;: &quot;.txt&quot;
                                    }
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;translator&quot;: {
                                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                                    &quot;typeConversion&quot;: true,
                                    &quot;typeConversionSettings&quot;: {
                                        &quot;allowDataTruncation&quot;: true,
                                        &quot;treatBooleanAsNumber&quot;: false
                                    }
                                }
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;DelimitedText3&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;DelimitedText4&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<ul>
<li><p>Since the end goal here would result in sink having all the files that source has, another way that you can consider is to use a delete activity inside your for each to delete the file in sink first and then copy it.</p>
</li>
<li><p>First get the list of files present in source using get metadata activity. Iterate through this list, first apply delete operation on the file and then copy.</p>
</li>
</ul>
<p><strong>NOTE:</strong> If the content of the file that already exist in both source and sink is different, go with the approach demonstrated above (as suggest by @Nandan).</p>
"
"76021576","Azure Data Factory File copying from two different datasets to one folder with two subfolders using lookup value","<p>Everyone I'm trying to copy files from two different tables from postgres database, for which I created two datasets, one linked service, one output dataset (parametrized folder name,filename)</p>
<p>step1: My Pipeline is connecting a lookup table which contains column rownumber,lookup e.g. lookup column values ['output1','output2']</p>
<p>step2: I used an iterator (Foreach Activity) to fetch lookup array items</p>
<p>step3: changing data type array items</p>
<p>step4: switch activity, expression if foreach item value match with let's say 'output1' then it will execute copy operation and fetch file from one table and will place in output location which will also dynamically create a folder 'output1' and will dump copied file with name output.csv.</p>
<p>but it seems all operation is running fine only second case from switch operation is not working.
It Seems i couldn't find any reason why is it not copying to second location, if condition is false</p>
<p>here is the expression I've used for switch operation:</p>
<pre><code> @if(equals(variables('var'),'output1'),'True','False')
</code></pre>
<p>switch activity setting
<a href=""https://i.stack.imgur.com/i1GAM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i1GAM.png"" alt=""enter image description here"" /></a>
<strong>Pipeline Inside Loop:</strong>
<a href=""https://i.stack.imgur.com/Jla58.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jla58.png"" alt=""enter image description here"" /></a></p>
<p><strong>Pipeline debug results:</strong>
<a href=""https://i.stack.imgur.com/kC3MJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kC3MJ.png"" alt=""enter image description here"" /></a></p>
","<azure-functions><azure-blob-storage><azure-data-factory><azure-file-copy>","2023-04-15 10:08:37","45","0","1","76035186","<blockquote>
<p>i couldn't find any reason why is it not copying to second location, if condition is false</p>
</blockquote>
<p>I have also reproduced the similar scenario as you and its working fine. Till switch activity your all the values and pipeline is correct.
the issue you are facing with copy activity may be.</p>
<p>As per you ask you want to copy from two different datasets to one folder with two subfolders.</p>
<p>To achieve that you need to specify dynamic content correctly:</p>
<ul>
<li>Here in source of copy activity I added the source of file from where have to copy the file.
<img src=""https://i.imgur.com/uMTzSru.png"" alt=""enter image description here"" /></li>
<li>Here in the sink we added the dynamic expression to copy file in <code>logs</code> folder under that it will create <code>Output1</code> folder and under that it will place the <code>output.csv</code> file with expression <code>@concat('logs/',variables('string'))</code>.
<img src=""https://i.imgur.com/JZGTUt4.png"" alt=""enter image description here"" /></li>
</ul>
<p>I used similar sink for second copy activity also just changed the source as per requirement.</p>
<p><em><strong>Execution:</strong></em></p>
<p><img src=""https://i.imgur.com/lmh3zGE.png"" alt=""enter image description here"" /></p>
<p><em><strong>Output:</strong></em></p>
<p><strong>copy from two different datasets to one folder with two subfolders.</strong></p>
<p><img src=""https://i.imgur.com/jy8QaTQ.png"" alt=""enter image description here"" /></p>
"
"76020690","Azure Data Factory (Expression of type: 'String' does not match the field: 'expression'. Expected type is 'Bool'.)","<p>I'm trying to add in a IF Condition activity two conditions.</p>
<p>I need something like this:</p>
<pre><code>and(@equals(int(variables('IsRunning')),1),@equals(int(variables('IsRunning1')),1))
</code></pre>
<p>And I receive:</p>
<blockquote>
<p>Expression of type: 'String' does not match the field: 'expression'.
Expected type is 'Bool'.</p>
</blockquote>
<p>The problem is that my variables has the following definition:</p>
<pre><code>@string(length(activity('Filter').output.value))
</code></pre>
<p>What causes this type error, and how is it fixed?</p>
","<azure-data-factory>","2023-04-15 06:22:16","73","0","1","76020925","<p>When I used your expression <code>and(@equals(int(variables('IsRunning')),1),@equals(int(variables('IsRunning1')),1))</code> in if condition expression, I got same.</p>
<p><img src=""https://i.imgur.com/jHUXvUf.png"" alt=""enter image description here"" /></p>
<p>Here, your expression is not correct as per if condition. if condition expects a boolean value but as per the syntax of the expression it is a String value.</p>
<p>Change your dynamic content expression like below and it will work for you.</p>
<p><code>@and(equals(int(variables('IsRunning')),1),equals(int(variables('IsRunning1')),1))</code></p>
<p>Here my variables are of String type and values for them are <code>&quot;1&quot;</code> for both.</p>
<p><img src=""https://i.imgur.com/DFddwer.png"" alt=""enter image description here"" /></p>
<p>Now, you can see I am able to execute the if activity.</p>
<p><img src=""https://i.imgur.com/erRgVNW.png"" alt=""enter image description here"" /></p>
<p>Give your activities inside True activities and False activities of if condition as per your requirement.</p>
"
"76020137","Getting the latest file from a hierarchical folder in ADF( Azure Data Factory)","<p>I have a ADLS container, and my data set is stored in hierarchical manner inside folder( YYYY/MM/DD/Module )</p>
<p>Example:
2023
04
10
Organization
File1.Txt
File2.txt</p>
<p>2023
04
10
People
File3.Txt
File4.txt</p>
<p>Ask: I want to pick the latest file from the file container Using Azure Data Factory.</p>
<p>For testing, Currently I have a metadata activity which points to the static folder above  (2023
04
10
Organization)
I want to parametrize it in ADF so that dynamically it traverses through the folder.</p>
<p>Secondly i have one more activity which traverses the file name [ Get Metadata 2] , the problem is i am only able to fetch the folder name, but not the file name. I tried using field list . But seems like it only fetching the folder name.</p>
<p>The Field list has 3 parameters &quot;Last Modified&quot;, &quot;Item name&quot;</p>
<p>Problem Statement:The JSON output for Item name is the folder name instead of file name</p>
<p>Due to data access restrictions i am unable to share the screenshot.</p>
<p>I tried using two metadata activity , one for capturing the count and other [Get Metadata2 ]inside for each which iterates through the list from Get Matadata1.</p>
","<azure><azure-data-factory>","2023-04-15 02:58:39","98","0","1","76051585","<blockquote>
<p>only on a current single day.. Lets say todays date is 30th so i should only check for files on 30th , if the file is not available then i should check for 29th and pick the one which is latest</p>
</blockquote>
<p>If your requirement is only with ADF, you can try the below approach. But it's better to do this using code via Databricks or Functions if you have access to them.
<strong>This is my sample folder structure:</strong></p>
<pre><code>inputdata
    2023
        04
            19
                Module
                    Module191.csv
                    Module192.csv
                People
                    People191.csv
                    People192.csv
            18
                Module
                    Module181.csv
                    Module182.csv
                People
                    People181.csv
                    People182.csv
        03
</code></pre>
<ul>
<li>In this first I got the latest date folder path(<code>YYYY/MM/DD</code>) by checking in <code>utcnow()</code> and <code>utcnow()-1</code>...etc in <strong>Get Meta data exists</strong> condition inside until loop.</li>
<li>Then using that date path, I got the child items(<strong>sub folders</strong>) and given to a ForEach. Inside ForEach for every subfolder I have used another Get Meta for child items(file names inside sub folder).</li>
<li>Here, to iterate these child items, we need another ForEach. But nested loops are not supported in ADF currenlty. So, use another pipeline with Execute pipeline activity.</li>
<li>In another pipeline, pass this child items to for each activity. Inside ForEach use Get Meta data activity( for file names) to get the Last modified date of file name. Then Get the latest file using if activity.</li>
<li>Outside the ForEach, return the last modfied date and file path to parent pipeline.</li>
<li>Here, the child pipeline is used to get the latest file and its date from every sub folder.</li>
<li>Using that return values in every iteration, get the latest file based on thier dates in the parent pipeline.</li>
<li>It means parent pipeline compares the latest files from every subfolder and gives the latest file in the total heirarchy.</li>
</ul>
<p><strong>This is my Parent pipeline(<code>pipeline2</code>) JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline2&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Counter intiliaze to 0&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Flag_to_false&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;counter&quot;,
                    &quot;value&quot;: &quot;0&quot;
                }
            },
            {
                &quot;name&quot;: &quot;Flag_to_false&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;flag&quot;,
                    &quot;value&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;Until gives latest date folders&quot;,
                &quot;type&quot;: &quot;Until&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Counter intiliaze to 0&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;expression&quot;: {
                        &quot;value&quot;: &quot;@variables('flag')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;get_date&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;get_date&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@addDays(utcnow(), int(variables('counter')), 'yyyy/MM/dd')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Get Meta data exists&quot;,
                            &quot;type&quot;: &quot;GetMetadata&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;get_date&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;dataset&quot;: {
                                    &quot;referenceName&quot;: &quot;sourcefiles&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;folder&quot;: {
                                            &quot;value&quot;: &quot;@variables('get_date')&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                },
                                &quot;fieldList&quot;: [
                                    &quot;exists&quot;
                                ],
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                    &quot;enablePartitionDiscovery&quot;: false
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;temp&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;set_flag&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;temp_counter&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@string(sub(int(variables('counter')), 1))&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;increment_counter_using_temp&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;temp&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;counter&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@variables('temp_counter')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;set_flag&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Get Meta data exists&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;flag&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@activity('Get Meta data exists').output.exists&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ],
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;
                }
            },
            {
                &quot;name&quot;: &quot;Get Childitems of date folders&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;min modified date&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;sourcefiles&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;,
                        &quot;parameters&quot;: {
                            &quot;folder&quot;: {
                                &quot;value&quot;: &quot;@variables('get_date')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    &quot;fieldList&quot;: [
                        &quot;childItems&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach ieterates childItems of dates&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Get Childitems of date folders&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Get Childitems of date folders').output.childItems&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Get Metadata for files&quot;,
                            &quot;type&quot;: &quot;GetMetadata&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;dataset&quot;: {
                                    &quot;referenceName&quot;: &quot;sourcefiles&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;folder&quot;: {
                                            &quot;value&quot;: &quot;@concat(variables('get_date'),'/',item().name)&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                },
                                &quot;fieldList&quot;: [
                                    &quot;childItems&quot;
                                ],
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                    &quot;enablePartitionDiscovery&quot;: false
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Execute Pipeline1&quot;,
                            &quot;type&quot;: &quot;ExecutePipeline&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Get Metadata for files&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;pipeline&quot;: {
                                    &quot;referenceName&quot;: &quot;pipeline3&quot;,
                                    &quot;type&quot;: &quot;PipelineReference&quot;
                                },
                                &quot;waitOnCompletion&quot;: true,
                                &quot;parameters&quot;: {
                                    &quot;folder_path&quot;: {
                                        &quot;value&quot;: &quot;@concat(variables('get_date'),'/',item().name)&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    },
                                    &quot;filenames&quot;: {
                                        &quot;value&quot;: &quot;@activity('Get Metadata for files').output.childItems&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;If Condition1&quot;,
                            &quot;type&quot;: &quot;IfCondition&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Execute Pipeline1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;expression&quot;: {
                                    &quot;value&quot;: &quot;@greater(int(split(activity('Execute Pipeline1').output.pipelineReturnValue.file_path,'_')[0]), int(variables('LatestModifiedDate')))&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                },
                                &quot;ifTrueActivities&quot;: [
                                    {
                                        &quot;name&quot;: &quot;Update lastmodified&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;LatestModifiedDate&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@split(activity('Execute Pipeline1').output.pipelineReturnValue.file_path,'_')[0]&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    },
                                    {
                                        &quot;name&quot;: &quot;Update final file path&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [
                                            {
                                                &quot;activity&quot;: &quot;Update lastmodified&quot;,
                                                &quot;dependencyConditions&quot;: [
                                                    &quot;Succeeded&quot;
                                                ]
                                            }
                                        ],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;final_files_path&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@split(activity('Execute Pipeline1').output.pipelineReturnValue.file_path,'_')[1]&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    }
                                ]
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;min modified date&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Until gives latest date folders&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;LatestModifiedDate&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@addDays(utcnow(), -365,'yyyyMMddHHmmss')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;counter&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;flag&quot;: {
                &quot;type&quot;: &quot;Boolean&quot;
            },
            &quot;get_date&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;req_date_path&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;temp_counter&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;final_files_path&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;LatestModifiedDate&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;mod_and_path&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>This is my Child pipeline(<code>pipeline3</code>) JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline3&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;iterates through file names&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;min modified date child&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.filenames&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Get Metadata for modifed date&quot;,
                            &quot;type&quot;: &quot;GetMetadata&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;dataset&quot;: {
                                    &quot;referenceName&quot;: &quot;Childgetmeta&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;filename&quot;: {
                                            &quot;value&quot;: &quot;@concat(pipeline().parameters.folder_path,'/',item().name)&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                },
                                &quot;fieldList&quot;: [
                                    &quot;lastModified&quot;,
                                    &quot;itemName&quot;
                                ],
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                    &quot;enablePartitionDiscovery&quot;: false
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;If Condition1&quot;,
                            &quot;type&quot;: &quot;IfCondition&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Get Metadata for modifed date&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;expression&quot;: {
                                    &quot;value&quot;: &quot;@greater(int(formatDateTime(activity('Get Metadata for modifed date').output.lastModified,'yyyyMMddHHmmss')), int(variables('lmodified')))&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                },
                                &quot;ifTrueActivities&quot;: [
                                    {
                                        &quot;name&quot;: &quot;update lmodified&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;lmodified&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@formatDateTime(activity('Get Metadata for modifed date').output.lastModified,'yyyyMMddHHmmss')&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    },
                                    {
                                        &quot;name&quot;: &quot;concat modified and file path&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [
                                            {
                                                &quot;activity&quot;: &quot;update lmodified&quot;,
                                                &quot;dependencyConditions&quot;: [
                                                    &quot;Succeeded&quot;
                                                ]
                                            }
                                        ],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;mod_and_path&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@concat(variables('lmodified'),'_',pipeline().parameters.folder_path,'/',item().name)&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    }
                                ]
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;min modified date child&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;lmodified&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@addDays(utcnow(), -365,'yyyyMMddHHmmss')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;returning&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;iterates through file names&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;pipelineReturnValue&quot;,
                    &quot;value&quot;: [
                        {
                            &quot;key&quot;: &quot;file_path&quot;,
                            &quot;value&quot;: {
                                &quot;type&quot;: &quot;Expression&quot;,
                                &quot;content&quot;: &quot;@variables('mod_and_path')&quot;
                            }
                        }
                    ],
                    &quot;setSystemVariable&quot;: true
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;folder_path&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;filenames&quot;: {
                &quot;type&quot;: &quot;array&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;lmodified&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;mod_and_path&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>Use the above JSONs to build the pipelines and this is my Result file:</p>
<p><img src=""https://i.imgur.com/ndB2YLw.png"" alt=""enter image description here"" /></p>
"
"76017056","'Exists Condition' for dynamic pattern in Azure Data Factory Dataflow","<p>I am using Azure Data Factory in which a data flow is used. In this dataflow I want to compare two sources, using the 'Exsits' transformation.
Both sources have identical column names. Only datarows in source1 that doesn't exist in source2 should be stored in Sink. The problem comes while configuring the Exits conditions. As I want to use the same pipeline for many datasets I want to use the custom expression field and implement late binding to compare the required columns in both sources. So I created an array input parameter ($primaryKeys) that holds the columns that needs to be compared in both the sources. Here is where I get stuck :</p>
<p><a href=""https://i.stack.imgur.com/7mPdG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7mPdG.png"" alt=""Dataflow Diagram"" /></a></p>
<p>The expression that gives an error:</p>
<p><a href=""https://i.stack.imgur.com/PrlFk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PrlFk.png"" alt=""Exists Condition"" /></a></p>
<p>Any suggestions how I can get the dynamic pattern in the expression here to work?</p>
","<azure><azure-data-factory>","2023-04-14 16:11:40","82","0","1","76041019","<ul>
<li><p>I was able to achieve your requirement using 2 dataflows instead of one where I have used one dataflow to build the custom expression and the other to implement your exists logic.</p>
</li>
<li><p>I have taken the <code>primitiveKeys</code> array parameter in dataflow where I am passing static value from pipeline for demonstration.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/zdyaPey.png"" alt=""enter image description here"" /></p>
<ul>
<li>In the first dataflow, with the above array parameter, take any dataset as source. Create a derived column transformation with the dynamic value as <code>toString(reduce(map($cols,concat('source1@',#item,' == source2@',#item)),'true() ',#acc+' &amp;&amp; '+#item,#result))</code>. The source1 and source2 are the left and right stream names that I will be applying <code>exists</code> transformation on.</li>
</ul>
<p><img src=""https://i.imgur.com/dkddeAm.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, we need this value. So, I have used cache sink and writing the output to activity output.</li>
<li>In the second dataflow, I have created a string parameter <code>custom_expr</code> for which I pass the value generated above.</li>
</ul>
<p><img src=""https://i.imgur.com/XJKv2iD.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now in the dataflow where we use custom expression, I have used the dynamic content as <code>toBoolean(expr(toString($custom_expr)))</code>.</li>
</ul>
<p>This will give the results as desired.  The following are output images for reference.</p>
<ul>
<li>Source 1 data preview:</li>
</ul>
<p><img src=""https://i.imgur.com/OMA2MxO.png"" alt=""enter image description here"" /></p>
<ul>
<li>Source 2 data preview:</li>
</ul>
<p><img src=""https://i.imgur.com/9y4an8o.png"" alt=""enter image description here"" /></p>
<ul>
<li>Does not exist result:</li>
</ul>
<p><img src=""https://i.imgur.com/qhHIbYl.png"" alt=""enter image description here"" /></p>
<ul>
<li>The JSON for first dataflow:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow2&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText6&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;derivedColumn1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;parameters{&quot;,
                &quot;     cols as string[] (['a','b'])&quot;,
                &quot;}&quot;,
                &quot;source(output(&quot;,
                &quot;          id as string,&quot;,
                &quot;          first_name as string,&quot;,
                &quot;          date as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source1&quot;,
                &quot;source1 derive(tp = toString(reduce(map($cols,concat('source1@',#item,' == source2@',#item)),'true() ',#acc+' &amp;&amp; '+#item,#result))) ~&gt; derivedColumn1&quot;,
                &quot;derivedColumn1 sink(validateSchema: false,&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     store: 'cache',&quot;,
                &quot;     format: 'inline',&quot;,
                &quot;     output: true,&quot;,
                &quot;     saveOrder: 1) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
<ul>
<li>The JSON for second dataflow:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText5&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                },
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText6&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source2&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;exists1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;parameters{&quot;,
                &quot;     custom_expr as string ('true()  &amp;&amp; source1@id == source2@id &amp;&amp; source1@first_name == source2@first_name')&quot;,
                &quot;}&quot;,
                &quot;source(output(&quot;,
                &quot;          id as string,&quot;,
                &quot;          first_name as string,&quot;,
                &quot;          date as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source1&quot;,
                &quot;source(output(&quot;,
                &quot;          id as string,&quot;,
                &quot;          first_name as string,&quot;,
                &quot;          date as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source2&quot;,
                &quot;source1, source2 exists(toBoolean(expr(toString($custom_expr))),&quot;,
                &quot;     negate:true,&quot;,
                &quot;     broadcast: 'both')~&gt; exists1&quot;,
                &quot;exists1 sink(validateSchema: false,&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     store: 'cache',&quot;,
                &quot;     format: 'inline',&quot;,
                &quot;     output: true,&quot;,
                &quot;     saveOrder: 1) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
<p><strong>NOTE:</strong>  The only hardcoded values in the above when you try to replicate this would be the left and right stream names to build the expression.</p>
"
"76016779","ADF Copy Activity from Oracle to ASQL - Logging failed error SessionLogFailToWriteLog","<p>I have existing pipeline copying data from Oracle to Azure SQL executing successfully. Then added logging with level = Warning and mode = Reliable, see below.
But pipeline stuck at source and eventually failed, because pipeline timeout set to 15 mins. Error messages received is as below.</p>
<p>Did I miss out some configs? Please provide some advice as to why this is the case, thanks!</p>
<p>ADF logging settings</p>
<p><a href=""https://i.stack.imgur.com/u2xPL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u2xPL.png"" alt=""enter image description here"" /></a></p>
<p>Copy failed at Source</p>
<p><a href=""https://i.stack.imgur.com/S5dkt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S5dkt.png"" alt=""enter image description here"" /></a></p>
<pre><code>Operation on target AC_COPY_SRC_TO_STG failed: ErrorCode=SessionLogFailToWriteLog,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Write log is failed with error message: 'Microsoft.DataTransfer.Common.Shared.HybridDeliveryException: ADLS Gen2 operation failed for: An error occurred while sending the request.. Account: 'storageAccount'. FileSystem: 'raw'.. ---&gt; System.Net.Http.HttpRequestException: An error occurred while sending the request. ---&gt; System.Net.WebException: Unable to connect to the remote server ---&gt; System.Net.Sockets.SocketException: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond 10.41.33.170:443
   at System.Net.Sockets.Socket.InternalEndConnect(IAsyncResult asyncResult)
   at System.Net.Sockets.Socket.EndConnect(IAsyncResult asyncResult)
   at System.Net.ServicePoint.ConnectSocketInternal(Boolean connectFailure, Socket s4, Socket s6, Socket&amp; socket, IPAddress&amp; address, ConnectSocketState state, IAsyncResult asyncResult, Exception&amp; exception)
   --- End of inner exception stack trace ---
   at System.Net.HttpWebRequest.EndGetResponse(IAsyncResult asyncResult)
   at System.Net.Http.HttpClientHandler.GetResponseCallback(IAsyncResult ar)
   --- End of inner exception stack trace ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at Microsoft.Rest.RetryAfterDelegatingHandler.&lt;SendAsync&gt;d__7.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Rest.RetryDelegatingHandler.&lt;&gt;c__DisplayClass15_0.&lt;&lt;SendAsync&gt;b__0&gt;d.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Rest.RetryDelegatingHandler.&lt;SendAsync&gt;d__15.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Azure.Storage.Data.RequestHandler.&lt;SendAsync&gt;d__3.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Azure.Storage.Data.AzureDfsClient.&lt;GetFilesystemPropertiesWithHttpMessagesAsync&gt;d__42.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Azure.Storage.Data.AzureDfsClientExtensions.&lt;GetFilesystemPropertiesAsync&gt;d__14.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Azure.Storage.Data.AzureDfsClientExtensions.GetFilesystemProperties(IAzureDfsClient operations, String filesystem, String resource, String xMsClientRequestId, Nullable`1 timeout, String xMsDate)
   at Microsoft.Azure.Storage.Data.BlobFSClient.&lt;&gt;c__DisplayClass32_0.&lt;IsFileSystemExist&gt;b__1()
   at Microsoft.Rest.TransientFaultHandling.RetryPolicy.ExecuteAction[TResult](Func`1 func)
   at Microsoft.Azure.Storage.Data.BlobFSClient.&lt;&gt;c__DisplayClass32_0.&lt;IsFileSystemExist&gt;b__0()
   at Microsoft.Azure.Storage.Data.ExceptionHandler.ExecuteAction[TResult](Func`1 func, String fileSystem, String path, String actionName)
   --- End of inner exception stack trace ---
   at Microsoft.Azure.Storage.Data.ExceptionHandler.HandleException(Exception ex, String fileSystem, String path, String actionName)
   at Microsoft.Azure.Storage.Data.ExceptionHandler.ExecuteAction[TResult](Func`1 func, String fileSystem, String path, String actionName)
   at Microsoft.Azure.Storage.Data.BlobFSClient.IsFileSystemExist(String filesystem, Boolean swallowForbiddenError, Boolean useListPathApi)
   at Microsoft.Azure.Storage.Data.BlobFSClient.CreateFileSystemIfNotExist(String filesystem, Boolean swallowForbiddenError, Boolean useListPathApiToCheckFileSystemExist)
   at Microsoft.DataTransfer.Runtime.BlobFSMultipartSink..ctor(AzureBlobFSConnection connection, AzureBlobFSWriteInternalSettings writeSettings)
   at Microsoft.DataTransfer.Runtime.AzureBlobFSConnection.CreateSink(IDictionary`2 properties)
   at Microsoft.DataTransfer.Runtime.BinarySinkStageProcessor..ctor(PluginRegistration plugins, IDictionary`2 stageProperties, BinaryTelemetryCollector telemetryCollector)
   at Microsoft.DataTransfer.Runtime.Provider.HybirdRuntimeBinaryWriter.&lt;&gt;c__DisplayClass11_0.&lt;ProcessCommandSettings&gt;b__1()
   at Microsoft.DataTransfer.Runtime.PipelineProcessStage`3.CreateProcessor(Func`1 createNextProcessor)
   at Microsoft.DataTransfer.Runtime.PipelineProcessStageProcessor`2.Process(TInput data)
   at Microsoft.DataTransfer.Runtime.Provider.HybridRuntimeCommand.ExecuteBinaryWriter(IEnumerable`1 data)
   at Microsoft.DataTransfer.Runtime.FileBasedLogHandler.CreateEmptyLogFile(Int32 currFileIndex)'.,Source=Microsoft.DataTransfer.ClientLibrary,'
</code></pre>
<p>I tried disable the logging and the pipeline was success again.</p>
","<azure><azure-data-factory>","2023-04-14 15:39:14","23","0","1","76033912","<p>By looking at the error you are facing it can be because of the data factory is not able to communicate with ADLS. The error message also suggests that the issue may be related to the <strong>inability to connect to a specific server with the IP address &quot;10.41.33.170&quot;</strong>.</p>
<p>To resolve this:</p>
<ul>
<li>Check to see sure the ADLS Gen2 account is set up correctly and is reachable.</li>
<li>Verify the application's and the ADLS Gen2 account's network connectivity.</li>
<li>Check to see if the firewall rules are preventing access to the server at IP 10.41.33.170.</li>
</ul>
<p><strong>You can add This Address in storage account firewall as below:</strong></p>
<p><img src=""https://i.imgur.com/BRO6ykd.png"" alt=""enter image description here"" /></p>
<p><strong>I also tried with similar settings and it's working fine for me.</strong></p>
<p><img src=""https://i.imgur.com/r4djnza.png"" alt=""enter image description here"" /></p>
"
"76016601","to change the tier of blob using web activity","<p>I'm trying to change the tier of a blob in storage account through ADF using Web Activity:</p>
<p>I am refering to this link - <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-tier?tabs=azure-ad"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-tier?tabs=azure-ad</a></p>
<p>The required values used are as below:</p>
<p>URL :<a href=""https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=tier"" rel=""nofollow noreferrer"">https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=tier</a>
(modified to my account and blob names)</p>
<p>ADF JSON:</p>
<p>{
&quot;name&quot;: &quot;changeTIER&quot;,
&quot;properties&quot;: {
&quot;activities&quot;: [
{
&quot;name&quot;: &quot;Web1&quot;,
&quot;type&quot;: &quot;WebActivity&quot;,
&quot;dependsOn&quot;: [],
&quot;policy&quot;: {
&quot;timeout&quot;: &quot;0.12:00:00&quot;,
&quot;retry&quot;: 0,
&quot;retryIntervalInSeconds&quot;: 30,
&quot;secureOutput&quot;: false,
&quot;secureInput&quot;: false
},
&quot;userProperties&quot;: [],
&quot;typeProperties&quot;: {
&quot;url&quot;: &quot;https://myaccount.blob.core.windows.net/mycontainer/combinedOutput.json?comp=tier&quot;,
&quot;method&quot;: &quot;PUT&quot;,
&quot;headers&quot;: {
&quot;x-ms-version&quot;: &quot;2014-02-14&quot;,
&quot;x-ms-access-tier&quot;: &quot;Cool&quot;,
&quot;x-ms-date&quot;: {
&quot;value&quot;: &quot;@utcNow('dddd, dd MMM yyyy HH:mm:ss')&quot;,
&quot;type&quot;: &quot;Expression&quot;
},
&quot;Authorization&quot;: &quot;&quot;
},
&quot;body&quot;: &quot;&quot;tier&quot;: &quot;Cool&quot;&quot;,
&quot;authentication&quot;: {
&quot;type&quot;: &quot;MSI&quot;,
&quot;resource&quot;: &quot;https://storage.azure.com/&quot;
}
}
}
],
&quot;annotations&quot;: [],
&quot;lastPublishTime&quot;: &quot;2023-04-11T05:08:13Z&quot;
},
&quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;</p>
<p>I'm using the date format as this -@utcNow('dddd, dd MMM yyyy HH:mm:ss') and a SAS Key along with MSI Authentication</p>
<p>I am getting the error:
Error calling the endpoint 'https://myaccount.blob.core.windows.net'. Response status code: 'NA - Unknown'. More details: Exception message: 'NA - Unknown [ClientSideException] The format of value ''.
Request didn't reach the server from the client. This could happen because of an underlying issue such as network connectivity, a DNS failure, a server certificate validation or a timeout.</p>
<p>Please help me with the right format and settings to connect and update the tier using the Web Activity in ADF-Am I missing any changes in the BLOB?</p>
<p>I have tried all stackoverflow articles and MS documentations.</p>
","<azure><azure-data-factory>","2023-04-14 15:15:38","75","0","2","76044604","<p>Three way's you can change the Azure Storage account tier.</p>
<blockquote>
<p><strong>Approach 1:</strong></p>
</blockquote>
<p>First, you can manually change the tier in the Azure storage account like this:</p>
<p><img src=""https://i.imgur.com/jWiDfXE.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Approach 2</strong>.</p>
</blockquote>
<p>Use custom activity in Azure data factory run PowerShell script and change the tier.</p>
<pre><code>$StrAccount = &quot;&quot;
$StrKey = &quot;&quot;
$Cont1 = &quot;&quot;

$ctx = New-AzureStorageContext -StorageAccountName $StrAccount -StorageAccountKey $StrKey

# Get all the blobs in container
$blob = Get-AzureStorageBlob -Container $Cont1 -Context $ctx

# Set the tier of all the blobs.
$blob.ICloudBlob.SetStandardBlobTier(&quot;Cool&quot;)
</code></pre>
<p>Upload power shell file in Azure storage account and perform the custom activity in Azure data factory.
<img src=""https://i.imgur.com/ora7Afk.png"" alt=""enter image description here"" /></p>
<p>For more information refer to this <a href=""https://www.youtube.com/watch?v=VuAXA1UhY78"" rel=""nofollow noreferrer"">link</a>.</p>
<blockquote>
<p><strong>Approach 3:</strong></p>
</blockquote>
<p>Use <code>CopyActivity</code>, to  <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-tier"" rel=""nofollow noreferrer"">set blob tier</a>, and also use <strong>Header:</strong> like this</p>
<pre><code>&quot;headers&quot;: { &quot;x-ms-version&quot;: &quot;2019-12-12&quot;, &quot;x-ms-access-tier&quot;: &quot;Cool&quot;, &quot;x-ms-date&quot;: { &quot;value&quot;: &quot;@utcNow('dddd, dd MMM yyyy HH:mm:ss')&quot;, &quot;type&quot;: &quot;Expression&quot; },
</code></pre>
<p>For information refer this <a href=""https://stackoverflow.com/questions/70623207/change-the-access-tier-of-file-blob-storage-from-hot-tier-to-archive-tier-in-adf"">SO</a> thread  by KarthikBhyresh-MT</p>
"
"76016601","to change the tier of blob using web activity","<p>I'm trying to change the tier of a blob in storage account through ADF using Web Activity:</p>
<p>I am refering to this link - <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-tier?tabs=azure-ad"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-tier?tabs=azure-ad</a></p>
<p>The required values used are as below:</p>
<p>URL :<a href=""https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=tier"" rel=""nofollow noreferrer"">https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=tier</a>
(modified to my account and blob names)</p>
<p>ADF JSON:</p>
<p>{
&quot;name&quot;: &quot;changeTIER&quot;,
&quot;properties&quot;: {
&quot;activities&quot;: [
{
&quot;name&quot;: &quot;Web1&quot;,
&quot;type&quot;: &quot;WebActivity&quot;,
&quot;dependsOn&quot;: [],
&quot;policy&quot;: {
&quot;timeout&quot;: &quot;0.12:00:00&quot;,
&quot;retry&quot;: 0,
&quot;retryIntervalInSeconds&quot;: 30,
&quot;secureOutput&quot;: false,
&quot;secureInput&quot;: false
},
&quot;userProperties&quot;: [],
&quot;typeProperties&quot;: {
&quot;url&quot;: &quot;https://myaccount.blob.core.windows.net/mycontainer/combinedOutput.json?comp=tier&quot;,
&quot;method&quot;: &quot;PUT&quot;,
&quot;headers&quot;: {
&quot;x-ms-version&quot;: &quot;2014-02-14&quot;,
&quot;x-ms-access-tier&quot;: &quot;Cool&quot;,
&quot;x-ms-date&quot;: {
&quot;value&quot;: &quot;@utcNow('dddd, dd MMM yyyy HH:mm:ss')&quot;,
&quot;type&quot;: &quot;Expression&quot;
},
&quot;Authorization&quot;: &quot;&quot;
},
&quot;body&quot;: &quot;&quot;tier&quot;: &quot;Cool&quot;&quot;,
&quot;authentication&quot;: {
&quot;type&quot;: &quot;MSI&quot;,
&quot;resource&quot;: &quot;https://storage.azure.com/&quot;
}
}
}
],
&quot;annotations&quot;: [],
&quot;lastPublishTime&quot;: &quot;2023-04-11T05:08:13Z&quot;
},
&quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;</p>
<p>I'm using the date format as this -@utcNow('dddd, dd MMM yyyy HH:mm:ss') and a SAS Key along with MSI Authentication</p>
<p>I am getting the error:
Error calling the endpoint 'https://myaccount.blob.core.windows.net'. Response status code: 'NA - Unknown'. More details: Exception message: 'NA - Unknown [ClientSideException] The format of value ''.
Request didn't reach the server from the client. This could happen because of an underlying issue such as network connectivity, a DNS failure, a server certificate validation or a timeout.</p>
<p>Please help me with the right format and settings to connect and update the tier using the Web Activity in ADF-Am I missing any changes in the BLOB?</p>
<p>I have tried all stackoverflow articles and MS documentations.</p>
","<azure><azure-data-factory>","2023-04-14 15:15:38","75","0","2","76051055","<p>It works with Web Activity also -only error was in format of the date header.
Using this -@concat(utcNow('ddd, dd MMM yyyy HH:mm:ss'),' GMT') is working now.</p>
<p>The correct format should be like this for eg -Wed, 19 Apr 2023 05:15:42 GMT</p>
"
"76006560","Execute Azure Data Factory from Power Automate with Service Principal","<p>In a Power Automate Flow I've configured a Create Pipeline Run step using a Service Principal. The Service Principal is a Contributor on the ADF object. It works fine when an Admin runs the Flow, but when a non-Admin runs the follow the Flow fails on the Create Pipeline Run step with the error:</p>
<p>The client 'username@domain.com' with object id '714b0320-ebaa-46a7-9896-4c146f64fad1' does not have authorization to perform action 'Microsoft.DataFactory/factories/pipelines/CreateRun/action' over scope '/subscriptions/4a1f6bbe-103b-4bc3-a38a-d0923a2bddff/resourcegroups/xx-xx-xxx/providers/Microsoft.DataFactory/factories/xx-xxx-xxxx/pipelines/pipelinename' or the scope is invalid.</p>
<p>The error message indicates the user ('username@domain.com') is running the Flow, not the Service Principal.
Any ideas how to resolve this?</p>
","<azure-data-factory><power-automate><azure-service-principal>","2023-04-13 14:21:57","52","0","3","76007377","<p>I have quite a bit of experience with Azure Logic Apps which are essentially Power Automate framework. When setting up the ADF connection initially it usually prompts you for a credentials that it will use when initiating the API call to ADF. My suggestion would be to use a Service Account that is on your Azure AD tenant. See illustration below:</p>
<p><a href=""https://i.stack.imgur.com/x2cQF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x2cQF.png"" alt=""Credential Input"" /></a></p>
<p>I know when I tried Managed Identity over a year ago there were issues and roadblocks there. But using a Service Account worked well for us. When the API call goes to ADF to create a pipeline run, it will use whoever was authenticated in this initial API connection creation.</p>
<p>This documentation is helpful: <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#default-connection"" rel=""nofollow noreferrer"">ADF Connector</a></p>
<p>specifically this:</p>
<p><a href=""https://i.stack.imgur.com/3P41y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3P41y.png"" alt=""Connection Default"" /></a></p>
"
"76006560","Execute Azure Data Factory from Power Automate with Service Principal","<p>In a Power Automate Flow I've configured a Create Pipeline Run step using a Service Principal. The Service Principal is a Contributor on the ADF object. It works fine when an Admin runs the Flow, but when a non-Admin runs the follow the Flow fails on the Create Pipeline Run step with the error:</p>
<p>The client 'username@domain.com' with object id '714b0320-ebaa-46a7-9896-4c146f64fad1' does not have authorization to perform action 'Microsoft.DataFactory/factories/pipelines/CreateRun/action' over scope '/subscriptions/4a1f6bbe-103b-4bc3-a38a-d0923a2bddff/resourcegroups/xx-xx-xxx/providers/Microsoft.DataFactory/factories/xx-xxx-xxxx/pipelines/pipelinename' or the scope is invalid.</p>
<p>The error message indicates the user ('username@domain.com') is running the Flow, not the Service Principal.
Any ideas how to resolve this?</p>
","<azure-data-factory><power-automate><azure-service-principal>","2023-04-13 14:21:57","52","0","3","76011291","<p>thanks for getting back to me.</p>
<p>So I can't use an embedded Service Principal in this situation?</p>
<p>When the user logs in they get this prompt:</p>
<p><a href=""https://i.stack.imgur.com/7GgwR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7GgwR.png"" alt=""prompt"" /></a></p>
<p>As a test I added the User as a Contributor to ADF and it works. Here is a screenshot of the ADF Activity Log:</p>
<p><a href=""https://i.stack.imgur.com/HUcCX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HUcCX.png"" alt=""log"" /></a></p>
<p>It's showing when I run the Flow it uses the Service Principal but when my test user runs the App is runs under it's name.</p>
<p>I actually moved away from using User based Service Accounts because the security experts recommend to use Service Principals. But now I'm having to wind back many of my changes back to Service Accounts.</p>
"
"76006560","Execute Azure Data Factory from Power Automate with Service Principal","<p>In a Power Automate Flow I've configured a Create Pipeline Run step using a Service Principal. The Service Principal is a Contributor on the ADF object. It works fine when an Admin runs the Flow, but when a non-Admin runs the follow the Flow fails on the Create Pipeline Run step with the error:</p>
<p>The client 'username@domain.com' with object id '714b0320-ebaa-46a7-9896-4c146f64fad1' does not have authorization to perform action 'Microsoft.DataFactory/factories/pipelines/CreateRun/action' over scope '/subscriptions/4a1f6bbe-103b-4bc3-a38a-d0923a2bddff/resourcegroups/xx-xx-xxx/providers/Microsoft.DataFactory/factories/xx-xxx-xxxx/pipelines/pipelinename' or the scope is invalid.</p>
<p>The error message indicates the user ('username@domain.com') is running the Flow, not the Service Principal.
Any ideas how to resolve this?</p>
","<azure-data-factory><power-automate><azure-service-principal>","2023-04-13 14:21:57","52","0","3","76013274","<p>I reverted the connection back to a Service Account but I had the same issue. It's because Azure Data Factory doesn't support Implicit Connections (like SQL Server does). In the end I create a custom Azure Role which grants ADF Execute rights only, and granted this role to the users. I followed this guide:
<a href=""https://sqlkover.com/allow-a-user-to-only-trigger-pipelines-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://sqlkover.com/allow-a-user-to-only-trigger-pipelines-in-azure-data-factory/</a>
Thank you to Koen Verbeeck for posting this, it saved me hours of time.</p>
"
"76005852","can we select only particular columns in ADF Dataflow?","<p>So I have some data, but I only want some particular columns to be selected. Is there any way to do that in ADF dataflow?</p>
<p>I have tried Select activity but it is giving all the columns. How to get only particular columns?</p>
<blockquote>
<p><strong>Edit: got The answer.  Fixed Mapping is the way to do it. If anyone have better solutions , let me know</strong>*</p>
</blockquote>
","<azure><azure-data-factory>","2023-04-13 13:11:22","41","1","1","76011836","<blockquote>
<p>I only want some particular columns to be selected. Is there any way to do that in ADF dataflow?</p>
</blockquote>
<p><strong>Select transformation takes all columns from incoming stream. To select particular column from select transformation you can select a and delete the unwanted column as shown below image.</strong></p>
<p><img src=""https://i.imgur.com/bVIptlU.png"" alt=""enter image description here"" /></p>
<p>And also, there are two mappings to map the columns:</p>
<ul>
<li><strong>Fixed Mapping (default)</strong></li>
<li><strong>Rule based Mapping</strong></li>
</ul>
<p>You can use rule-based mapping to construct your mappings using column patterns if you want to map many columns at once or send drifting columns downstream. <strong>Matching is done using the column names, types, streams, and positions</strong>.</p>
"
"76003781","How to update SQL Server table from ADF?","<p>I need to update the Azure SQL Server table if another SQL Server tables is updated.</p>
<p>For example: I have one Azure SQL table <code>dbo.Azuretable</code> and I also have a SQL Server table <code>dbo.sqlservertable</code>. Both these tables are part of two different SQL Server instances, but the table schema is exactly the same.</p>
<p>What happens is <code>dbo.sqlservertable</code> updates on daily basis, let's say every day one row inserts in <code>dbo.sqlservertable</code>, I want the same row to be inserted into <code>dbo.Azuretable</code> as soon as <code>dbo.sqlservertable</code> gets updated. I want to do it Azure if possible or I am open to any other method as well.</p>
<p>What I have done so far:</p>
<p>I created one copy activity which replicates the changes to <code>dbo.Azuretable</code> what ever happened in <code>dbo.sqlservertable</code>. But I need to manually trigger it because I don't know when my <code>dbo.sqlservertable</code> gets updated. I want to automate this process.</p>
","<sql-server><azure><triggers><azure-sql-database><azure-data-factory>","2023-04-13 09:20:50","76","-1","2","76004270","<p>There are multiple ways to sync the data :</p>
<ol>
<li><p>use data sync in azure sql database to sync from sql server to Azure SQL database
<a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database?view=azuresql"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database?view=azuresql</a></p>
</li>
<li><p>You can create a trigger on the table in SQL server 2017 on insert,update,delete</p>
</li>
</ol>
<p>and via trigger, call a stored procedure which would export data into blob via polybase.
leverage blob trigger in adf pipeline to sync data from sql server to Azure SQL database</p>
<p><a href=""https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16</a></p>
<p>Else you can also trigger a SQL server job wherein you can have a powershell logic to generate a file in blob or trigger ADF REST API for pipeline trigger</p>
<p>sample reference:
<a href=""https://datasharkx.wordpress.com/2022/10/01/event-trigger-data-sync-from-sql-server-to-synapse-via-azure-data-factory-synapse-pipeline/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2022/10/01/event-trigger-data-sync-from-sql-server-to-synapse-via-azure-data-factory-synapse-pipeline/</a></p>
"
"76003781","How to update SQL Server table from ADF?","<p>I need to update the Azure SQL Server table if another SQL Server tables is updated.</p>
<p>For example: I have one Azure SQL table <code>dbo.Azuretable</code> and I also have a SQL Server table <code>dbo.sqlservertable</code>. Both these tables are part of two different SQL Server instances, but the table schema is exactly the same.</p>
<p>What happens is <code>dbo.sqlservertable</code> updates on daily basis, let's say every day one row inserts in <code>dbo.sqlservertable</code>, I want the same row to be inserted into <code>dbo.Azuretable</code> as soon as <code>dbo.sqlservertable</code> gets updated. I want to do it Azure if possible or I am open to any other method as well.</p>
<p>What I have done so far:</p>
<p>I created one copy activity which replicates the changes to <code>dbo.Azuretable</code> what ever happened in <code>dbo.sqlservertable</code>. But I need to manually trigger it because I don't know when my <code>dbo.sqlservertable</code> gets updated. I want to automate this process.</p>
","<sql-server><azure><triggers><azure-sql-database><azure-data-factory>","2023-04-13 09:20:50","76","-1","2","76006715","<p>First, you require to find inserted rows. Use except functions.</p>
<pre><code>select * from dbo.sqlservertable
except
select * from dbo.Azuretable
</code></pre>
<p>Then add  to &quot;dbo.Azuretable&quot;</p>
<pre><code>WITH new_records AS(
    select * from dbo.sqlservertable
    except
    select * from dbo.Azuretable
)

INSERT INTO dbo.Azuretable
SELECT * FROM new_records 
</code></pre>
<p>You can use data factory or synapse pipeline for it. Pipeline-&gt;Activities-&gt;General-&gt; <strong>Script</strong> meets your demand. After that you'll see lightning icon names add trigger, click and new/edit. Bring your mouse to choose trigger and click. Now, click new. There are all schedule settings. I hope this solution helps you.</p>
<p><a href=""https://i.stack.imgur.com/fkPAx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fkPAx.png"" alt=""enter image description here"" /></a></p>
"
"76002971","How to get OData source file updated data into sink file (Azure SQL Server) using Azure Data Factory","<p>How to get an OData source file updated data into sink file (Azure SQL Server) using Azure Data Factory?</p>
<p>How to create pipeline using OData source file to sink for getting updated records, using Azure Data Factory?</p>
<p><img src=""https://i.stack.imgur.com/iO5CZ.png"" alt=""getting this error for "" /></p>
","<azure-sql-database><azure-data-factory><odata><azure-data-lake-gen2>","2023-04-13 07:54:50","81","0","1","76004399","<p>In order to load data incrementally from ODATA source to SQL database, you need to have an incrementing key column in source. Incrementing key is the unique identifier that is added to each row of the table and the value will be increasing whenever new rows are added. This column will be used to identify the rows that are copied already and the rows which are to be copied to sink.</p>
<ul>
<li>Create a watermark table in SQL Database and insert the initial value.</li>
<li>If your incrementing key is date, keep the value as <code>1900-01-01</code>. And if it is a number, start with <code>0</code>.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>create table watermark_table ( watermark_column datetime2)
insert into watermark_table values  ('1900-01-01')
</code></pre>
<p><img src=""https://i.imgur.com/flIeFbe.png"" alt=""enter image description here"" /></p>
<ul>
<li>In Data factory pipeline, add a lookup activity and create a source dataset for the watermark table.</li>
<li>Then add a copy activity. In source dataset add OData connector dataset and in sink, add the dataset for SQL database table.</li>
<li>In source, enter the query as <code>$filter=&lt;incremental-column-name&gt; gt '@{activity('Lookup1').output.firstRow.watermark_column}'</code></li>
</ul>
<p>Replace the <code>&lt;incremental-column-name&gt;</code> with the respective key column.</p>
<p><img src=""https://i.imgur.com/5pMOx3a.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then add the Script activity and add the linked service for SQL database. Enter the script as,</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>update watermark_table
set watermark_column=(select max(&lt;incremental-column-name&gt;) from &lt;sink-table-name&gt;)
</code></pre>
<p>Replace <code>&lt;incremental-column-name&gt;</code> and <code>&lt;sink-table-name&gt;</code> with the respective column name and table name respectively.</p>
<p>This will replace the old watermark value with new value and new value will be the latest value of the row that got loaded to sink.</p>
<p><strong>Reference</strong>: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-template-delta-copy-with-control-table"" rel=""nofollow noreferrer"">Delta copy from a database using a control table - Azure Data Factory | Microsoft Learn</a></p>
"
"76001688","Exception: HttpResponseError: (BadRequest) Entity [pipeline_name] not found","<p>I created a pipeline in Azure Data Factory that takes an Avro file and creates a SQL table from it. I already tested the pipeline in ADF, and it works fine. Now I need to trigger this pipeline from an Azure function: to do this, I'm trying to create a run of the pipeline using the following code within the function:</p>
<pre><code>import logging
from azure.identity import DefaultAzureCredential
from azure.mgmt.datafactory import DataFactoryManagementClient
import azure.functions as func

def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    from .config import suscription_id, resource_group, factory_name, pipeline_name
    client = DataFactoryManagementClient(
        credential=DefaultAzureCredential(),
        subscription_id=suscription_id,
    )
    logging.info('==== ADF client created ====')

    response = client.pipelines.create_run(
        resource_group_name=resource_group,
        factory_name=factory_name,
        pipeline_name=pipeline_name,
    )
    logging.info(response)
    return func.HttpResponse(f&quot;Pipeline {pipeline_name} started&quot;)
</code></pre>
<p>But for some reason, I'm getting an error message saying that finding the resource is impossible, referring to the pipeline. I already verified all the names, and everything seems correct. Do you have any suggestions to debug this?</p>
<pre><code>[2023-04-13T04:34:00.591Z] Executed 'Functions.restoreTable' (Failed, Id=c00a9f36-695d-4806-bb6d-880ac88f1d95, Duration=7398ms)
[2023-04-13T04:34:00.591Z] System.Private.CoreLib: Exception while executing function: Functions.restoreTable. System.Private.CoreLib: Result: Failure
[2023-04-13T04:34:00.591Z] Exception: HttpResponseError: (BadRequest) Entity Restore_backup not found
[2023-04-13T04:34:00.591Z] Code: BadRequest
[2023-04-13T04:34:00.591Z] Message: Entity Restore_backup not found
[2023-04-13T04:34:00.591Z] Target: pipeline/Restore_backup/runid/65350dbc-d9b4-11ed-8e61-acde48001122
[2023-04-13T04:34:00.591Z] Stack:   File &quot;/usr/local/Cellar/azure-functions-core-tools@4/4.0.5095/workers/python/3.9/OSX/X64/azure_functions_worker/dispatcher.py&quot;, line 452, in _handle__invocation_request
[2023-04-13T04:34:00.591Z]     call_result = await self._loop.run_in_executor(
[2023-04-13T04:34:00.591Z]   File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py&quot;, line 52, in run
[2023-04-13T04:34:00.591Z]     result = self.fn(*self.args, **self.kwargs)
[2023-04-13T04:34:00.591Z]   File &quot;/usr/local/Cellar/azure-functions-core-tools@4/4.0.5095/workers/python/3.9/OSX/X64/azure_functions_worker/dispatcher.py&quot;, line 718, in _run_sync_func
[2023-04-13T04:34:00.591Z]     return ExtensionManager.get_sync_invocation_wrapper(context,
[2023-04-13T04:34:00.591Z]   File &quot;/usr/local/Cellar/azure-functions-core-tools@4/4.0.5095/workers/python/3.9/OSX/X64/azure_functions_worker/extension.py&quot;, line 215, in _raw_invocation_wrapper
[2023-04-13T04:34:00.591Z]     result = function(**args)
[2023-04-13T04:34:00.591Z]   File &quot;/Users/jd/Documents/globant-api/restoreTable/__init__.py&quot;, line 14, in main
[2023-04-13T04:34:00.591Z]     response = client.pipelines.create_run(
[2023-04-13T04:34:00.592Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/tracing/decorator.py&quot;, line 76, in wrapper_use_tracer
[2023-04-13T04:34:00.592Z]     return func(*args, **kwargs)
[2023-04-13T04:34:00.592Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/mgmt/datafactory/operations/_pipelines_operations.py&quot;, line 937, in create_run
[2023-04-13T04:34:00.592Z]     raise HttpResponseError(response=response, error_format=ARMErrorFormat)
</code></pre>
<p><strong>UPDATE</strong></p>
<p>I believe there is a conflict with the credentials that my app is trying to use. I already added my Azure account to the Vscode login and installed the Azure CLI to log in there. Still, once I try to hit the endpoint with Postman, a dialog opens up to ask for permission to use the Azure credentials in Keychain:</p>
<p><a href=""https://i.stack.imgur.com/tYVHY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tYVHY.png"" alt=""enter image description here"" /></a></p>
<p>When I decline the request, I get this message</p>
<pre><code>2023-04-13T18:18:56.002Z] DefaultAzureCredential failed to retrieve a token from the included credentials.
[2023-04-13T18:18:56.002Z] Attempted credentials:
[2023-04-13T18:18:56.002Z]      EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.
[2023-04-13T18:18:56.002Z] Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot.this issue.
[2023-04-13T18:18:56.003Z]      ManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.
[2023-04-13T18:18:56.003Z]      SharedTokenCacheCredential: Authentication failed: 
[2023-04-13T18:18:56.003Z] To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.
[2023-04-13T18:18:56.128Z] Executed 'Functions.restoreTable' (Failed, Id=3c3d2f26-1d37-4b0f-a94e-b3d4d4978c08, Duration=21193ms)
[2023-04-13T18:18:56.128Z] System.Private.CoreLib: Exception while executing function: Functions.restoreTable. System.Private.CoreLib: Result: Failure
[2023-04-13T18:18:56.128Z] Exception: ClientAuthenticationError: DefaultAzureCredential failed to retrieve a token from the included credentials.
[2023-04-13T18:18:56.128Z] Attempted credentials:
[2023-04-13T18:18:56.128Z]      EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.
[2023-04-13T18:18:56.128Z] Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot.this issue.
[2023-04-13T18:18:56.128Z]      ManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.
[2023-04-13T18:18:56.128Z]      SharedTokenCacheCredential: Authentication failed: 
[2023-04-13T18:18:56.128Z] To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.
[2023-04-13T18:18:56.128Z] Stack:   File &quot;/usr/local/Cellar/azure-functions-core-tools@4/4.0.5095/workers/python/3.9/OSX/X64/azure_functions_worker/dispatcher.py&quot;, line 452, in _handle__invocation_request
[2023-04-13T18:18:56.128Z]     call_result = await self._loop.run_in_executor(
[2023-04-13T18:18:56.128Z]   File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py&quot;, line 52, in run
[2023-04-13T18:18:56.128Z]     result = self.fn(*self.args, **self.kwargs)
[2023-04-13T18:18:56.128Z]   File &quot;/usr/local/Cellar/azure-functions-core-tools@4/4.0.5095/workers/python/3.9/OSX/X64/azure_functions_worker/dispatcher.py&quot;, line 718, in _run_sync_func
[2023-04-13T18:18:56.128Z]     return ExtensionManager.get_sync_invocation_wrapper(context,
[2023-04-13T18:18:56.128Z]   File &quot;/usr/local/Cellar/azure-functions-core-tools@4/4.0.5095/workers/python/3.9/OSX/X64/azure_functions_worker/extension.py&quot;, line 215, in _raw_invocation_wrapper
[2023-04-13T18:18:56.128Z]     result = function(**args)
[2023-04-13T18:18:56.128Z]   File &quot;/Users/jd/Documents/globant-api/restoreTable/__init__.py&quot;, line 14, in main
[2023-04-13T18:18:56.128Z]     response = client.pipelines.create_run(
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/tracing/decorator.py&quot;, line 76, in wrapper_use_tracer
[2023-04-13T18:18:56.129Z]     return func(*args, **kwargs)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/mgmt/datafactory/operations/_pipelines_operations.py&quot;, line 929, in create_run
[2023-04-13T18:18:56.129Z]     pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py&quot;, line 202, in run
[2023-04-13T18:18:56.129Z]     return first_node.send(pipeline_request)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py&quot;, line 70, in send
[2023-04-13T18:18:56.129Z]     response = self.next.send(request)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py&quot;, line 70, in send
[2023-04-13T18:18:56.129Z]     response = self.next.send(request)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py&quot;, line 70, in send
[2023-04-13T18:18:56.129Z]     response = self.next.send(request)
[2023-04-13T18:18:56.129Z]   [Previous line repeated 2 more times]
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/mgmt/core/policies/_base.py&quot;, line 46, in send
[2023-04-13T18:18:56.129Z]     response = self.next.send(request)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/policies/_redirect.py&quot;, line 156, in send
[2023-04-13T18:18:56.129Z]     response = self.next.send(request)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/policies/_retry.py&quot;, line 448, in send
[2023-04-13T18:18:56.129Z]     response = self.next.send(request)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/policies/_authentication.py&quot;, line 108, in send
[2023-04-13T18:18:56.129Z]     self.on_request(request)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/core/pipeline/policies/_authentication.py&quot;, line 87, in on_request
[2023-04-13T18:18:56.129Z]     self._token = self._credential.get_token(*self._scopes)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/identity/_credentials/default.py&quot;, line 168, in get_token
[2023-04-13T18:18:56.129Z]     return super(DefaultAzureCredential, self).get_token(*scopes, **kwargs)
[2023-04-13T18:18:56.129Z]   File &quot;/Users/jd/Documents/globant-api/.venv/lib/python3.9/site-packages/azure/identity/_credentials/chained.py&quot;, line 101, in get_token
[2023-04-13T18:18:56.129Z]     raise ClientAuthenticationError(message=message)
[2023-04-13T18:18:56.129Z] .
</code></pre>
<p>When I accept the request, I get the initial message that says that I can´t find the resource (the pipeline), and that makes sense if this is using the credentials of a different account that doesn't have my pipeline. How can I force the code to take the Vscode or CLI credentials?</p>
","<python-3.x><azure-functions><azure-data-factory>","2023-04-13 04:43:25","66","0","1","76003807","<p>I used the below code to trigger my ADF pipeline that takes the avro file and creates an SQL Table from it:-</p>
<p><strong>My init.py file:-</strong></p>
<pre class=""lang-py prettyprint-override""><code>import logging
import os
import json
from azure.identity import DefaultAzureCredential
from azure.mgmt.datafactory import DataFactoryManagementClient
import azure.functions as func


def main(req: func.HttpRequest, res: func.Out[func.HttpResponse]) -&gt; None:
    try:
        # Load configuration values from config.json
        with open(os.path.abspath(os.path.join(os.path.dirname(__file__), 'config.json')), 'r') as f:
            config = json.load(f)

        # Create Data Factory Management Client
        client = DataFactoryManagementClient(
            credential=DefaultAzureCredential(),
            subscription_id=config['subscription_id']
        )
        logging.info('ADF client created')

        # Trigger Pipeline
        run_response = client.pipelines.create_run(
            resource_group_name=config['resource_group_name'],
            factory_name=config['factory_name'],
            pipeline_name=config['pipeline_name']
        )

        # Return response
        res.set(func.HttpResponse(f&quot;Pipeline run_id {run_response.run_id} created&quot;))
        
    except Exception as e:
        logging.error(str(e))
        res.set(func.HttpResponse(f&quot;An error occurred: {str(e)}&quot;, status_code=500))
</code></pre>
<p><strong>My config.json file:-</strong></p>
<pre class=""lang-json prettyprint-override""><code>{

&quot;subscription_id&quot;: &quot;&lt;subscription-id&gt;&quot;,

&quot;resource_group_name&quot;: &quot;&lt;resource-group-name&gt;&quot;,

&quot;factory_name&quot;: &quot;&lt;adf-name&gt;&quot;,

&quot;pipeline_name&quot;: &quot;pipeline6&quot;

}
</code></pre>
<p><strong>My function.json</strong></p>
<pre class=""lang-json prettyprint-override""><code>{

&quot;bindings&quot;: [

{

&quot;name&quot;: &quot;req&quot;,

&quot;type&quot;: &quot;httpTrigger&quot;,

&quot;direction&quot;: &quot;in&quot;,

&quot;authLevel&quot;: &quot;anonymous&quot;,

&quot;methods&quot;: [

&quot;post&quot;

]

},

{

&quot;type&quot;: &quot;http&quot;,

&quot;direction&quot;: &quot;out&quot;,

&quot;name&quot;: &quot;res&quot;

}

],

&quot;scriptFile&quot;: &quot;__init__.py&quot;,

&quot;entryPoint&quot;: &quot;main&quot;

}
</code></pre>
<p><strong>My requirements.txt:-</strong></p>
<pre><code>azure-functions

azure-mgmt-datafactory

azure-identity
</code></pre>
<p>I used the HTTP post request that finds the config.json from function directory which has ADF and pipeline details and triggers the pipeline, Refer below:-</p>
<p><strong>Output:-</strong></p>
<p><img src=""https://i.imgur.com/bxHUa3v.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/QZcihrR.png"" alt=""enter image description here"" /></p>
<p><strong>Post request in Postman:-</strong></p>
<p><img src=""https://i.imgur.com/2cHqC41.png"" alt=""enter image description here"" /></p>
<p><strong>Portal:-</strong></p>
<p><em><strong>Avro file to SQL pipeline:-</strong></em></p>
<p><img src=""https://i.imgur.com/HbaklOr.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/m0g48GV.png"" alt=""enter image description here"" /></p>
<p><em><strong>Pipeline triggered successfully via Azure Function:-</strong></em></p>
<p><img src=""https://i.imgur.com/Xp7848z.png"" alt=""enter image description here"" /></p>
<p><em><strong>Avro file got added in SQL table like below:-</strong></em></p>
<p><img src=""https://i.imgur.com/2Xe8Fws.png"" alt=""enter image description here"" /></p>
"
"75998828","How to properly parse of JSON decimal types in Azure Data Factory?","<p>In an Azure SQL database I have a table which contains a column with a JSON string like this:</p>
<pre><code>{&quot;Id&quot;:&quot;4b11c702-d4b2-47f4-a8d6-0b8ab9f53c99&quot;,&quot;Date&quot;:&quot;2021-11-15T00:16:43.7300189+00:00&quot;,&quot;Source&quot;:&quot;AtlasEarth&quot;,&quot;SourceTransactionId&quot;:&quot;5ad129d3-2b79-44b1-b617-2d86d352003a&quot;,&quot;SourceTransactionType&quot;:&quot;Rent&quot;,&quot;Amount&quot;:0.000000000000000111}
</code></pre>
<p>As part of an Azure Data Flow, I'm trying to parse that JSON string and put some of its properties in a complex object. I followed <a href=""https://www.youtube.com/watch?v=r7O7AJcuqoY"" rel=""nofollow noreferrer"">this video</a>.</p>
<p>The parse works for most of parts. However, the <code>Amount</code> column (which came as serialized from a .Net <code>decimal</code> property) is coming with the format not correct.</p>
<p><a href=""https://i.stack.imgur.com/IoxLm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IoxLm.png"" alt=""Data preview"" /></a></p>
<p>So my question is, how can I get that column with the correct format? I tried both float and double and both gives the same result.</p>
<p>Thanks!</p>
","<azure><azure-data-factory>","2023-04-12 18:44:40","44","0","1","76002671","<blockquote>
<p>how can I get that column with the correct format? I tried both float and double and both gives the same result.</p>
</blockquote>
<p><strong>I tried the similar issue in my environment and face same issue.</strong></p>
<p><img src=""https://i.imgur.com/wyqJM2d.png"" alt=""enter image description here"" /></p>
<p><strong>Tried to resolve this but there may be limitation if the value is present after 6 positions in decimal It will take it by converting into scientific notification only.</strong></p>
<ul>
<li>We can't convert it into decimal while directly in parsing that time we have to take it as <code>double</code>.</li>
<li>After this we have to take a derived column and convert the value to decimal using  <code>toDecimal(jsondemo.Amount,20, 20)</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/U9VMinv.png"" alt=""enter image description here"" /></p>
<p><strong>Output:-</strong></p>
<blockquote>
<p>Note it is only convert vale to decimal till E-6 After this it will take values with scientific notation only.</p>
</blockquote>
<p><img src=""https://i.imgur.com/8M7BIpj.png"" alt=""enter image description here"" /></p>
"
"75997605","ADF Copy Data, Sink all JSON Object into SQL Cell","<p>I am using <code>copy data</code> <strong>activity</strong> in <code>Azure data factory (ADF)</code> to make calls to 5 different <code>APIs</code> in the loop and all 5 APIs will have 5 different responses.</p>
<p>The idea is to sink the whole JSON Object into the SQL database. I want to know how can I refer to all API JSON object responses.</p>
<p>For example, here is my API Response:</p>
<pre><code> {
     result:[{
    &quot;elemenName&quot;:&quot;Element Name&quot;,
     &quot;elemenName&quot;:&quot;Element Name&quot;,

   }]
   }
</code></pre>
<p>I do not want to sink the <code>result</code> in the database cell, but instead, I want to be able to sink the whole JSON object in the cell. I am not sure how to refer to this JSON object. my API JSON response is greater than 5MB</p>
","<azure><api><mapping><azure-data-factory><copy-data>","2023-04-12 16:02:08","49","0","1","76001580","<ul>
<li><p>In for-each activity, you can use <strong>lookup activity</strong> to read the json API data and then use the <strong>Script actvity</strong> to insert the json data that is read from lookup activity into the SQL table. Below is the approach.</p>
</li>
<li><p>In Lookup activity, select <code>HTTP</code> as linked service and <code>json</code> as source dataset.</p>
</li>
<li><p>Enter the Base URL and in Relative URL, enter the value from for-each item as dynamic content.</p>
</li>
<li><p>Then add a script activity and add the linked service for SQL database in it.</p>
</li>
<li><p>Enter the query as a dynamic content in query text box.</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>Insert into &lt;sink-table-name&gt; values ('@{activity('Lookup2').output.value}')
</code></pre>
<p><img src=""https://i.imgur.com/oBSwim9.png"" alt=""img"" /></p>
<ul>
<li>When pipeline is run, json data from each api is copied to table as separate rows.</li>
</ul>
<p><img src=""https://i.imgur.com/iVoQexZ.png"" alt=""enter image description here"" /></p>
<p><strong>Update:</strong></p>
<blockquote>
<p>Message=The size of lookup activity result exceeds the limitation`</p>
</blockquote>
<p>To resolve this, Inside for-each, use the copy activity and take the source dataset as delimited text with different column and row delimiter. In this case, I took <strong>(``)</strong> as both column and row delimiter, so that the entire Json response is in a single row.</p>
<p><img src=""https://i.imgur.com/XpBeWnc.png"" alt=""img1"" /></p>
<p>Copy the data to sink database.</p>
"
"75995980","Get records and send them in batches by an api- DataFactory","<p>I have to update the data from a table in sql server with <code>65023</code> records to a post type api which takes 100 records in the body per batch, the endpoint is <code>https://api.local/batch/update</code> the body contains a batch of 100 records, the body is this</p>
<pre><code>{ &quot;inputs&quot;: [
     {
     &quot;id&quot;: &quot;{id[1]}&quot;,
       &quot;properties&quot;: 
       {
         &quot;field1&quot;: &quot;{field1}&quot;,
 &quot;field2&quot;: &quot;{field2}&quot;
       }
     },
     {
     &quot;id&quot;: &quot;{id[2]}&quot;,
       &quot;properties&quot;: 
       {
       &quot;field1&quot;: &quot;{field1}&quot;,
 &quot;field2&quot;: &quot;{field2}&quot;
       }
     }
....
]
}
</code></pre>
<p>where <code>id</code>, <code>field1</code>, <code>field2</code> correspond to the columns <code>id</code>, <code>field1</code>, <code>field2</code>. No more than 100 records can be sent in the body request, and all <code>65023</code> records must be sent How do I do it in azure data factory, thanks.</p>
","<azure><azure-data-factory>","2023-04-12 13:20:21","51","0","1","75996776","<p><strong>If you want to update data in batches from SQL server to Rest Api with post method you need to set <code>writeBatchSize</code> option to write data in batches of particular amount you and set it as <strong>100</strong> for your requirement.</strong></p>
<ul>
<li><p>First select your source as SQL database and table.
<img src=""https://i.imgur.com/67UCRuk.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then select sink as rest Api and give your Api with post method and to write data in batches of particular amount you and set <code>writeBatchSize</code> as <strong>100</strong> for your requirement.
<img src=""https://i.imgur.com/GCfAgMQ.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>This is my Pipeline JSON for your reference:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;name&quot;: &quot;pipeline4&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;AzureSqlSource&quot;,
                        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                        &quot;partitionOption&quot;: &quot;None&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;RestSink&quot;,
                        &quot;httpRequestTimeout&quot;: &quot;00:01:40&quot;,
                        &quot;requestInterval&quot;: 10,
                        &quot;requestMethod&quot;: &quot;POST&quot;,
                        &quot;writeBatchSize&quot;: &quot;100&quot;,
                        &quot;httpCompressionType&quot;: &quot;none&quot;
                    },
                    &quot;enableStaging&quot;: false
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;RestResource1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest?tabs=data-factory#rest-as-sink"" rel=""nofollow noreferrer"">REST as a Sink</a></p>
"
"75992883","Provide dataset schema for ADF CosmosDB dataset via terraform","<p>I want to provide dataset schema with terraform for my <code>azurerm_data_factory_dataset_cosmosdb_sqlapi</code> resource.</p>
<p>I tried to specify schema with <code>schema_column</code> attribute with the following syntax</p>
<pre><code>schema_column {
  name = &quot;id&quot;
  type = &quot;String&quot;
}
</code></pre>
<p>but it doesn't apply any changes.</p>
<p>Providing <code>structure</code> with <code>additional_parameters</code> fails as it expects values as string instead of array.</p>
<pre><code>additional_parameters = {
  &quot;structure&quot;: [
    {
      &quot;name&quot;: &quot;id&quot;,
      &quot;type&quot;: &quot;String&quot;
    }
  ]
}
</code></pre>
<p>What is the correct way to specify schema with terraform?</p>
<p>These are resources I used:</p>
<ol>
<li><p><a href=""https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/data_factory_dataset_cosmosdb_sqlapi"" rel=""nofollow noreferrer"">https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/data_factory_dataset_cosmosdb_sqlapi</a></p>
</li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-documentdb-connector#schema-by-data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-documentdb-connector#schema-by-data-factory</a></p>
</li>
</ol>
","<azure><terraform><azure-data-factory>","2023-04-12 07:20:32","51","0","1","76005707","<p><em>I also got the same error when I tried in my environment as shown.</em></p>
<p><img src=""https://i.imgur.com/Piv9X23.png"" alt=""enter image description here"" /></p>
<p>After a workaround on this, I modified the <code>additional_properities</code> block by passing <code>structure as jsonencode</code>.</p>
<pre class=""lang-bash prettyprint-override""><code>additional_properties = {
    &quot;structure&quot;: jsonencode([
      {
        &quot;name&quot;: &quot;id&quot;,
        &quot;type&quot;: &quot;String&quot;
      }
    ])
    }
</code></pre>
<p><em><strong>Complete tf script attached below:</strong></em></p>
<pre class=""lang-bash prettyprint-override""><code>provider &quot;azurerm&quot;{
features{}
}
data &quot;azurerm_resource_group&quot; &quot;example&quot; {
  name     = &quot;resourcegroup&quot;
}

resource &quot;azurerm_cosmosdb_account&quot; &quot;main&quot; {
  name                = &quot;xxx&quot;
  location            = data.azurerm_resource_group.example.location
  resource_group_name = data.azurerm_resource_group.example.name
  offer_type          = &quot;Standard&quot;
  kind                = &quot;&quot;

  enable_automatic_failover = true

  capabilities {
    name = &quot;&quot;
  }

  capabilities {
    name = &quot;&quot;
  }

  capabilities {
    name = &quot;MongoDBv3.4&quot;
  }

  consistency_policy {
    consistency_level       = &quot;BoundedStaleness&quot;
    max_interval_in_seconds = xxx
    max_staleness_prefix    = xxx
  }

  geo_location {
    location          = &quot;&quot;
    failover_priority = 1
  }

  geo_location {
    location          = &quot;&quot;
    failover_priority = 0
  }
}

resource &quot;azurerm_data_factory&quot; &quot;main&quot; {
  name                = &quot;exampledf&quot;
  location            = data.azurerm_resource_group.example.location
  resource_group_name = data.azurerm_resource_group.example.name
}

resource &quot;azurerm_data_factory_linked_service_cosmosdb&quot; &quot;main&quot; {
  name             = &quot;example&quot;
  data_factory_id  = azurerm_data_factory.main.id
  account_endpoint = azurerm_cosmosdb_account.main.endpoint
  database         = &quot;xxxx&quot;
}

resource &quot;azurerm_data_factory_dataset_cosmosdb_sqlapi&quot; &quot;example&quot; {
  name                = &quot;exampledsc&quot;
  data_factory_id     = azurerm_data_factory.main.id
  linked_service_name = azurerm_data_factory_linked_service_cosmosdb.main.name

  collection_name = &quot;bar&quot;
  additional_properties = {
    &quot;structure&quot;: jsonencode([
      {
        &quot;name&quot;: &quot;id&quot;,
        &quot;type&quot;: &quot;String&quot;
      }
    ])
}
}
</code></pre>
<p><em><strong>Executed</strong></em> <code>terraform init</code>:</p>
<p><img src=""https://i.imgur.com/e02j7NR.png"" alt=""enter image description here"" /></p>
<p><em><strong>Executed</strong></em> <code>terraform plan</code>:</p>
<p><img src=""https://i.imgur.com/1udsQr5.png"" alt=""enter image description here"" /></p>
<p><em><strong>Executed</strong></em> <code>terraform apply</code>:</p>
<p><img src=""https://i.imgur.com/YBDULAl.png"" alt=""enter image description here"" /></p>
<p>Refer <a href=""https://registry.terraform.io/"" rel=""nofollow noreferrer"">terraform registry</a> for the sample structure of the templates.</p>
"
"75992398","Merging json files into one and adding filename in data before data is merged using ADF","<p>I'm using this approach to merge my individual json files into one and it works :</p>
<p>Using ADF copy actitivyt:
Use Wildcard path in source with * in filename.
Now in sink, use merge option
files merged into one json blob.</p>
<p>All the merged data looks like this in the big json:</p>
<pre><code>{data from file1}
.
.
{data from file2}
.
.
{data from file3} 
</code></pre>
<p>The requirement is to have the final format like this:</p>
<pre><code>{&quot;File1.json&quot;:
[{
&lt;file1 data&gt;
}], 
&quot;File2.json&quot;:
[{
&lt;file2 data&gt;
}]
}
</code></pre>
<p>Edit:
Not looking for lookup activity to fetch the data as it has size limit.</p>
<p>Is it possible using ADF , please suggest an alternative if not.</p>
","<json><azure><azure-data-factory>","2023-04-12 06:11:02","80","0","2","75994394","<p>You can use get metadata, for each, look up and copy data activity to get the desired result.</p>
<ol>
<li>First use get metadata activity to get the list of files in your directory.</li>
<li>Iterate through the JSON files returned by this get metadata activity.</li>
<li>Now use an append variable activity to append each of the item as <code>&quot;file_name&quot;:[&lt;file_data&gt;]</code>.</li>
<li>Outside the for each activity, use a set variable activity to get the required data as string.</li>
<li>Finally use this to build the required JSON. The following is the complete JSON flow:</li>
</ol>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Get Metadata1&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Json2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;fieldList&quot;: [
                        &quot;childItems&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Get Metadata1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Get Metadata1').output.childItems&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Lookup1&quot;,
                            &quot;type&quot;: &quot;Lookup&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;JsonSource&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                        &quot;recursive&quot;: true,
                                        &quot;enablePartitionDiscovery&quot;: false
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;JsonReadSettings&quot;
                                    }
                                },
                                &quot;dataset&quot;: {
                                    &quot;referenceName&quot;: &quot;Json3&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                },
                                &quot;firstRowOnly&quot;: false
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Append variable1&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Lookup1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;final&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;\&quot;@{item().name}\&quot;:@{activity('Lookup1').output.value}&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;required&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;{@{join(variables('final'),',')}}&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;additionalColumns&quot;: [
                            {
                                &quot;name&quot;: &quot;required&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@variables('required')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        ],
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;mappings&quot;: [
                            {
                                &quot;source&quot;: {
                                    &quot;name&quot;: &quot;required&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;String&quot;,
                                    &quot;physicalType&quot;: &quot;String&quot;,
                                    &quot;ordinal&quot;: 1
                                }
                            }
                        ],
                        &quot;typeConversion&quot;: true,
                        &quot;typeConversionSettings&quot;: {
                            &quot;allowDataTruncation&quot;: true,
                            &quot;treatBooleanAsNumber&quot;: false
                        }
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;variables&quot;: {
            &quot;final&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;required&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<ul>
<li>In the final copy data activity, take source as delimited text with any sample data (1 column and 1 row).  create an additional column with value as the set variable.</li>
</ul>
<p><img src=""https://i.imgur.com/kTQduRl.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, for sink dataset create a delimited text dataset with the configurations same as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/UvB0RS3.png"" alt=""enter image description here"" /></p>
<ul>
<li>Running the pipeline would give the desired result. The following is how the data would be after the above pipeline is run.</li>
</ul>
<p><img src=""https://i.imgur.com/XgCblTz.png"" alt=""enter image description here"" /></p>
<pre><code>{
   &quot;sample1.json&quot;:[
      {
         &quot;KTYPE&quot;:[
            {
               &quot;name&quot;:&quot;john&quot;,
               &quot;surname&quot;:&quot;elo&quot;
            },
            {
               &quot;name&quot;:&quot;dd&quot;,
               &quot;surname&quot;:&quot;ss&quot;
            }
         ],
         &quot;MTYPE&quot;:[
            {
               &quot;name&quot;:&quot;dsdsd&quot;,
               &quot;id&quot;:&quot;elo&quot;
            },
            {
               &quot;name&quot;:&quot;sdss&quot;,
               &quot;id&quot;:&quot;sds22&quot;
            }
         ]
      }
   ],
   &quot;sample2.json&quot;:[
      {
         &quot;name&quot;:&quot;Привет&quot;
      },
      {
         &quot;name&quot;:&quot;Привет&quot;
      }
   ],
   &quot;sample3.json&quot;:[
      {
         &quot;id&quot;:1,
         &quot;first_name&quot;:&quot;Catlin&quot;,
         &quot;last_name&quot;:&quot;Haysman&quot;,
         &quot;email&quot;:&quot;chaysman0@hostgator.com&quot;,
         &quot;gender&quot;:&quot;Female&quot;,
         &quot;ip_address&quot;:&quot;80.243.124.118&quot;
      },
      {
         &quot;id&quot;:2,
         &quot;first_name&quot;:&quot;Augustin&quot;,
         &quot;last_name&quot;:&quot;Nesbeth&quot;,
         &quot;email&quot;:&quot;anesbeth1@cbc.ca&quot;,
         &quot;gender&quot;:&quot;Male&quot;,
         &quot;ip_address&quot;:&quot;250.126.164.4&quot;
      }
   ],
   &quot;sample4.json&quot;:[
      {
         &quot;id&quot;:3,
         &quot;first_name&quot;:&quot;Layla&quot;,
         &quot;last_name&quot;:&quot;Morant&quot;,
         &quot;email&quot;:&quot;lmorant2@seattletimes.com&quot;,
         &quot;gender&quot;:&quot;Female&quot;,
         &quot;ip_address&quot;:&quot;247.73.128.196&quot;
      },
      {
         &quot;id&quot;:4,
         &quot;first_name&quot;:&quot;Ophelie&quot;,
         &quot;last_name&quot;:&quot;Rape&quot;,
         &quot;email&quot;:&quot;orape3@bloomberg.com&quot;,
         &quot;gender&quot;:&quot;Female&quot;,
         &quot;ip_address&quot;:&quot;148.213.192.8&quot;
      }
   ]
}
</code></pre>
"
"75992398","Merging json files into one and adding filename in data before data is merged using ADF","<p>I'm using this approach to merge my individual json files into one and it works :</p>
<p>Using ADF copy actitivyt:
Use Wildcard path in source with * in filename.
Now in sink, use merge option
files merged into one json blob.</p>
<p>All the merged data looks like this in the big json:</p>
<pre><code>{data from file1}
.
.
{data from file2}
.
.
{data from file3} 
</code></pre>
<p>The requirement is to have the final format like this:</p>
<pre><code>{&quot;File1.json&quot;:
[{
&lt;file1 data&gt;
}], 
&quot;File2.json&quot;:
[{
&lt;file2 data&gt;
}]
}
</code></pre>
<p>Edit:
Not looking for lookup activity to fetch the data as it has size limit.</p>
<p>Is it possible using ADF , please suggest an alternative if not.</p>
","<json><azure><azure-data-factory>","2023-04-12 06:11:02","80","0","2","76128687","<p>This is my complete json for pipeline:</p>
<pre><code>{
&quot;name&quot;: &quot;Merge Json Format&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Get_all_files&quot;,
            &quot;type&quot;: &quot;GetMetadata&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;DS_Json_input_merge&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;Folder&quot;: {
                            &quot;value&quot;: &quot;@pipeline().parameters.Class&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        },
                        &quot;Subfolder&quot;: {
                            &quot;value&quot;: &quot;@pipeline().parameters.ID&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                },
                &quot;fieldList&quot;: [
                    &quot;childItems&quot;
                ],
                &quot;storeSettings&quot;: {
                    &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                    &quot;recursive&quot;: true,
                    &quot;enablePartitionDiscovery&quot;: false
                },
                &quot;formatSettings&quot;: {
                    &quot;type&quot;: &quot;JsonReadSettings&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach_file&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Get_all_files&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: 
 &quot;@activity('Get_all_files').output.childItems&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Lookup_each_file_data&quot;,
                        &quot;type&quot;: &quot;Lookup&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;source&quot;: {
                                &quot;type&quot;: &quot;JsonSource&quot;,
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                                    &quot;recursive&quot;: true,
                                    &quot;wildcardFileName&quot;: &quot;*&quot;,
                                    &quot;enablePartitionDiscovery&quot;: false
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;JsonReadSettings&quot;
                                }
                            },
                            &quot;dataset&quot;: {
                                &quot;referenceName&quot;: &quot;DS_Json_input&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                &quot;parameters&quot;: {
                                    &quot;Folder&quot;: {
                                        &quot;value&quot;: 
 &quot;@pipeline().parameters.Class&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    },
                                    &quot;Subfolder&quot;: {
                                        &quot;value&quot;: 
  &quot;@pipeline().parameters.ID&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            },
                            &quot;firstRowOnly&quot;: false
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Append_data_and_filename&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Lookup_each_file_data&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;final&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: 
  &quot;\&quot;@{item().name}\&quot;:@{activity('Lookup_each_file_data').output.value}&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Set variable1&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;ForEach_file&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;required&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;{@{join(variables('final'),',')}}&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Copy data1&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;additionalColumns&quot;: [
                        {
                            &quot;name&quot;: &quot;required&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@variables('required')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobStorageWriteSettings&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                        &quot;quoteAllText&quot;: true,
                        &quot;fileExtension&quot;: &quot;.txt&quot;
                    }
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;mappings&quot;: [
                        {
                            &quot;source&quot;: {
                                &quot;name&quot;: &quot;required&quot;,
                                &quot;type&quot;: &quot;String&quot;
                            },
                            &quot;sink&quot;: {
                                &quot;type&quot;: &quot;String&quot;,
                                &quot;physicalType&quot;: &quot;String&quot;,
                                &quot;ordinal&quot;: 1
                            }
                        }
                    ],
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;DS_csv&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;storageaccount&quot;: &quot;abc&quot;
                    }
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;DS_delimited_sink&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;StorageAccount&quot;: &quot;abc&quot;,
                        &quot;Container&quot;: &quot;adf&quot;,
                        &quot;Folder&quot;: &quot;Class&quot;,
                        &quot;filename&quot;: &quot;Merged-0d&quot;
                    }
                }
            ]
        }
    ],
    &quot;parameters&quot;: {
        &quot;Class&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;defaultValue&quot;: &quot;Class&quot;
        },
        &quot;ID&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;defaultValue&quot;: &quot;0d7&quot;
        },
        &quot;storageaccount&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;defaultValue&quot;: &quot;abc&quot;
        }
    },
    &quot;variables&quot;: {
        &quot;final&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;required&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;folder&quot;: {
        &quot;name&quot;: &quot;Test_developement&quot;
    },
    &quot;annotations&quot;: [],
    &quot;lastPublishTime&quot;: &quot;2023-04-27T15:39:43Z&quot;
},
&quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
 }
</code></pre>
<p>It is creating the file but it is doing it twice so data inside looks like this:</p>
<pre><code>{&quot;File1.json&quot;:
[{
&lt;file1 data&gt;
}], 
&quot;File2.json&quot;:
[{
&lt;file2 data&gt;
}]
}{&quot;File1.json&quot;:
[{
&lt;file1 data&gt;
}], 
&quot;File2.json&quot;:
[{
&lt;file2 data&gt;
}]
}
</code></pre>
<p>I am not sure why it is pasting data twice. Let me know if anything wrong in the code.</p>
"
"75988810","Azure data factory dataflow activity get request does not add additional headers along with URL","<p>I'm trying to use ADF dataflow activity web source to receive a JSON response. But it seems like the additional headers I have included in the &quot;Source Options&quot; section does not get sent along with the URL. because of that I'm getting a &quot;401&quot; response.</p>
<p>How to resolve this issue.</p>
<p><a href=""https://i.stack.imgur.com/Nnh0h.png"" rel=""nofollow noreferrer"">Source Options</a></p>
<p>I've tried different URLs. Even different ADF environments. same 401 error output since the additional headers won't be included in the get request.</p>
<p>In web activities the particular URL works just fine.</p>
","<azure><azure-data-factory>","2023-04-11 17:58:42","36","0","1","75993695","<blockquote>
<p>But it seems like the additional headers I have included in the &quot;Source Options&quot; section does not get sent along with the URL. because of that I'm getting a &quot;401&quot; response.</p>
</blockquote>
<p><img src=""https://i.imgur.com/uYmi942.png"" alt=""enter image description here"" /></p>
<p>The cause of error would be dataflow is not able to detect headers properly or values you are passing to headers are incorrect.</p>
<p><strong>I tried with sample URL and passed the <code>Authorization</code> token as header and <code>content-type</code> and worked fine for me.</strong></p>
<p><strong>Follow below process:</strong></p>
<ul>
<li><p>Create string parameter in Data flow for <code>Authorization</code> header.
<img src=""https://i.imgur.com/ZpyU64f.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then pass this parameter in header value and pass other headers as your requirement</p>
</li>
</ul>
<pre class=""lang-json prettyprint-override""><code>Authorization : $parameter1
Content-Type : 'application/json'
</code></pre>
<p><img src=""https://i.imgur.com/20nvfw8.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then in pipeline select data flow under parameter  pass the pipeline expression for the parameter as <code>Bearer @{activity('Web1').output.data.Token}</code> as per your web activity result.
<img src=""https://i.imgur.com/mClopAc.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>This will take correct headers and get the data from Rest Api</strong></p>
<p><em><strong>OUTPUT</strong></em></p>
<p><img src=""https://i.imgur.com/8KdwEfc.png"" alt=""enter image description here"" /></p>
"
"75987769","ErrorCode: 'AuthorizationPermissionMismatch'","<p>I have used service principal in Authentication type while creating Linked services.<a href=""https://i.stack.imgur.com/lMMiR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lMMiR.jpg"" alt=""service principal created"" /></a>
but when i try to create a dataset in ADF (i am using adls) i am not able to see my file on container i am getting an error.
<code>ADLS Gen2 failed for forbidden: Storage operation '' on container 'raw-container' get failed with 'Operation returned an invalid status code 'Forbidden''. Possible root causes: (1). It's possible because the service principal or managed identity don't have enough permission to access the data. (2). Please check storage network setting whether public network access is disabled. If disabled, use Managed Virtual Network IR and create Private Endpoint to access. https://docs.microsoft.com/en-us/azure/data-factory/managed-virtual-network-private-endpoint. https://docs.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal-private (3). It's possible because some IP address ranges of Azure Data Factory are not allowed by your Azure Storage firewall settings. Azure Data Factory IP ranges please refer https://docs.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses. If you allow trusted Microsoft services to access this storage account option in firewall, you must use https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#managed-identity. For more information on Azure Storage firewalls settings, see https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal.. Account: 'storage1998acc'. FileSystem: 'raw-container'. ErrorCode: 'AuthorizationPermissionMismatch'. Message: 'This request is not authorized to perform this operation using this permission.'. RequestId: '04dc03d9-a01f-009f-1c85-6c9ead000000'. TimeStamp: 'Tue, 11 Apr 2023 14:52:57 GMT'.. Operation returned an invalid status code 'Forbidden' Activity ID: 500eff90-9056-4084-a7cb-428c38af0523</code></p>
<p><a href=""https://i.stack.imgur.com/eOTEl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eOTEl.jpg"" alt=""enter image description here"" /></a></p>
<p>can you please help me i have checked access on IAM , public network access is also enabled</p>
","<azure><azure-data-factory><azure-data-lake-gen2><azure-storage-account>","2023-04-11 15:42:32","54","1","1","75992452","<p>I reproduce the same in my environment. I got the same error.</p>
<p><img src=""https://i.imgur.com/c2OosCU.png"" alt=""enter image description here"" /></p>
<p>To resolve this issue. You need to <code>enable and allow all networks</code> in the storage account and also grant the <code>Storage blob contributor</code> role to the storage account.</p>
<p>If you already have enough permission still you getting the same error. Try to use a different authentication method. If you are using a service principal to authenticate with your storage account, Try to use a managed identity or Account key or SAS. This will eliminate your issues with the permissions. In my scenario, I am using the Account key as the Authentication type.</p>
<p><img src=""https://i.imgur.com/IX9ckdX.png"" alt=""enter image description here"" /></p>
"
"75986958","Regular expression in ADF","<p>How to split the string based on white spaces in DataFlow expression builder in ADF pipeline.
A string can have 1 or more white spaces.</p>
<pre><code>Eg: 

I used split(name,’ ‘) : Thiswill split the word based on single space.Some times it can have more than 1 space. How to handle that?

Joe Smith(1 white space)
Joel  Smith(2 white space)
Joplin   Smith(3 white space)
</code></pre>
","<regex><azure><azure-data-factory>","2023-04-11 14:19:08","83","1","2","75992619","<p>This is my sample data with spaces.</p>
<p><img src=""https://i.imgur.com/jq1Z25M.png"" alt=""enter image description here"" /></p>
<pre><code>column
Steve Smith C      S  K
Rakesh Govindula     Laddu
Chinna R     C      B
</code></pre>
<p>Use the below expression to get the words array without empty strings.</p>
<p><code>filter(split(column,' '),#item!='')</code></p>
<p><img src=""https://i.imgur.com/4KCsPty.png"" alt=""enter image description here"" /></p>
<p>As there is no join function in dataflow expressions, to get the string of words with one space, use below expression from this <a href=""https://stackoverflow.com/a/75089327"">SO Answer</a> by <strong>@Jarred Jobe</strong>.</p>
<pre><code>dropLeft(toString(reduce(filter(split(column,' '),#item!=''), '', #acc +  ' '  + #item, #result)), 0)
</code></pre>
<p><img src=""https://i.imgur.com/w8k2VD5.png"" alt=""enter image description here"" /></p>
"
"75986958","Regular expression in ADF","<p>How to split the string based on white spaces in DataFlow expression builder in ADF pipeline.
A string can have 1 or more white spaces.</p>
<pre><code>Eg: 

I used split(name,’ ‘) : Thiswill split the word based on single space.Some times it can have more than 1 space. How to handle that?

Joe Smith(1 white space)
Joel  Smith(2 white space)
Joplin   Smith(3 white space)
</code></pre>
","<regex><azure><azure-data-factory>","2023-04-11 14:19:08","83","1","2","75996435","<p>I notice a derivative of this problem from <a href=""https://stackoverflow.com/questions/2455750/replace-duplicate-spaces-with-a-single-space-in-t-sql"">other programming langauges</a>.
If you want to remove multiple extra spaces then you can use 3 Replace() functions in a row to replace them with a single space.</p>
<p>If we set a parameter &quot;newparam&quot; to your example <code>&quot;Joplin   Smith&quot;</code>, we can use 3 replaces like so:</p>
<p><code>@replace(replace(replace(pipeline().parameters.newparam, ' ', ' %'), '% ', ''), '%', '')</code></p>
<p>Output after each replace:</p>
<pre><code> 1 - &quot;value&quot;: &quot;Joplin % % %Smith&quot;
 2 - &quot;value&quot;: &quot;Joplin %Smith&quot;
 3 - &quot;value&quot;: &quot;Joplin Smith&quot;
</code></pre>
<p>At this point you can wrap the command with the split() function and it will only intercept single spaces:</p>
<pre><code>@split(replace(replace(replace(pipeline().parameters.newparam, ' ', ' %'), '% ', ''), '%', '')
, ' ')
</code></pre>
<p>Produces the array:</p>
<pre><code>&quot;value&quot;: [
        &quot;Joplin&quot;,
        &quot;Smith&quot;
    ]
</code></pre>
"
"75985506","how to get selected columns from OData source Table using azure data factory?","<p>I'm trying to get the selected column values from the OData source table using azure data factory with copy activity, but i got all the columns from the source table. i want only selected columns.</p>
","<dataset><azure-pipelines><azure-data-factory><odata><azure-data-lake-gen2>","2023-04-11 11:42:49","61","0","1","75991742","<ul>
<li>Give the entity name in the path of the OData dataset.</li>
</ul>
<p><img src=""https://i.imgur.com/ypLoMmW.png"" alt=""enter image description here"" /></p>
<ul>
<li>The example dataset has 3 columns. From these three columns, query is written to select only <code>name</code> and <code>lcaocode</code> fields.</li>
<li>In source tab of copy activity, select Query option and enter <code>$select=Name,IcaoCode</code> in the query textbox.</li>
</ul>
<p><img src=""https://i.imgur.com/y2bbjea.png"" alt=""enter image description here"" /></p>
<ul>
<li>By this way, only required columns from OData source can be selected.</li>
</ul>
"
"75985242","What is the best way to input an xml file into sql Server database","<p>I have a xml file that looks like this below</p>
<p>The format of the Xml file is the same but there are many more</p>
<pre><code>&lt;id&gt;220&lt;/id&gt;
&lt;direction&gt;1&lt;/direction&gt;
&lt;count&gt;1&lt;/count&gt;
&lt;date&gt;2017-10-05T00:00:00&lt;/date&gt; in the actual Xml file
</code></pre>
<p>Here is the Xml file</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;counts&gt;
&lt;count&gt;
&lt;id&gt;220&lt;/id&gt;
&lt;direction&gt;1&lt;/direction&gt;
&lt;count&gt;1&lt;/count&gt;
&lt;date&gt;2017-10-05T00:00:00&lt;/date&gt;
&lt;/count&gt;
&lt;count&gt;
&lt;id&gt;220&lt;/id&gt;
&lt;direction&gt;2&lt;/direction&gt;
&lt;count&gt;0&lt;/count&gt;
&lt;date&gt;2017-10-05T00:00:00&lt;/date&gt;
&lt;/count&gt;
&lt;counts&gt;

I first tried to copy this Xml file into Json format and this work fine creating this json file
{&quot;counts&quot;:{&quot;count&quot;:[
{&quot;id&quot;:220,&quot;direction&quot;:1,&quot;count&quot;:1,&quot;date&quot;:&quot;2017-10-05T00:00:00&quot;},
{&quot;id&quot;:220,&quot;direction&quot;:2,&quot;count&quot;:0,&quot;date&quot;:&quot;2017-10-05T00:00:00&quot;},
]}}
</code></pre>
<p>The problem start when I try to map because the map in data factory looks like this</p>
<p><a href=""https://i.stack.imgur.com/fabLD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fabLD.png"" alt=""enter image description here"" /></a></p>
<p>So only one record from json is added to the database table.
So I wonder what is the best way to go.</p>
<ol>
<li>One solution could be if it is possible to change the json file that is looks like this
{&quot;id&quot;:220,&quot;direction&quot;:1,&quot;count&quot;:1,&quot;date&quot;:&quot;2017-10-05T00:00:00&quot;}
{&quot;id&quot;:220,&quot;direction&quot;:2,&quot;count&quot;:0,&quot;date&quot;:&quot;2017-10-05T00:00:00&quot;}</li>
</ol>
<p>Both the xml file and the json file is a blob in different container in azure.
So If it's possible to change the blob json file how could I do that.</p>
<p>Many thanks in advance</p>
","<azure><azure-data-factory>","2023-04-11 11:15:45","64","0","1","75992728","<ul>
<li>You can use the advance editor in the mapping after importing the schema to achieve the requirement. I have the following data in my xml file:</li>
</ul>
<p><img src=""https://i.imgur.com/mgAK28N.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I chose the file pattern for my JSON sink as Array of objects and then use import schema. There you can see advance editor option. Enable it and select <code>count</code> array from drop down which will result in the following mapping (give sink names as required):</li>
</ul>
<p><img src=""https://i.imgur.com/jQlPAEG.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, when you run the pipeline, you will get the desired result. The following is the image of final data:</li>
</ul>
<p><img src=""https://i.imgur.com/oaQvkJS.png"" alt=""enter image description here"" /></p>
"
"75984270","parameterize the count of input file rows in azure data flow","<ol>
<li><p>I have input file as csv now i want to generate valid and invalid records as csv with same input file name as output file in azure data flow,</p>
</li>
<li><p>Now i want to get the count of valid and invalid records as parameter value by using azure data factory data flow.</p>
</li>
</ol>
<p>Please suggest the way for both requirements.</p>
","<azure><azure-data-factory>","2023-04-11 09:19:58","42","0","1","75995504","<p>I have csv file in my blob storage named &quot;dbo.Customer.csv&quot;</p>
<p><img src=""https://i.imgur.com/4yaNIyB.png"" alt=""enter image description here"" /></p>
<p>Created dataset with above data in data flow and added that as source in dataflow, added derived column activity to the source and created two new columns named &quot;Validation&quot; with value <code>iif(Id % 2 == 0, 'InValid', 'Valid')</code> and filename with &quot;dbo.Customer.csv&quot;</p>
<p><img src=""https://i.imgur.com/PKDnnNS.png"" alt=""enter image description here"" /></p>
<p>Added window activity to count the validation count and selected <code>Validation</code> column in <code>over</code> tab and created windows column named <code>validation_count</code> with value <code>count(Valdation)</code></p>
<p><img src=""https://i.imgur.com/BOI0pfa.png"" alt=""enter image description here"" /></p>
<p>Added sink to the windows activity selected same dataset which selected to the source and selected file Name option as &quot;Name file as column data&quot; and selected &quot;filename&quot; column:</p>
<p><img src=""https://i.imgur.com/RBJVZ3G.png"" alt=""enter image description here"" /></p>
<p>Created pipeline and performed dataflow activity by selecting created data flow and debug it</p>
<p><img src=""https://i.imgur.com/ON6eLys.png"" alt=""enter image description here"" /></p>
<p>My output after debugging dataflow saved in the blob storage account with file name as inut file name</p>
<p><img src=""https://i.imgur.com/8juPDFv.png"" alt=""enter image description here"" /></p>
"
"75982726","tsql to create extract key and value from JSON data in a serverless sql pool","<p>How to create external table using Serverless SQL pool using tsql in Azure Synapse Analytics for this scenario:</p>
<p>I have a 'employeeInfo' column in a table called as 'companyDetail'.
This is the query to create 'companyDetail' table:</p>
<pre><code>CREATE EXTERNAL TABLE companyDetail
(
  companyName varchar(100),
  employeeInfo varchar(2048) 
) 
WITH
(
    LOCATION = '/all_parquet_files/*.parquet',
    DATA_SOURCE = parquet_datasource,
    FILE_FORMAT = parquet
)
 
</code></pre>
<p>companyDetail table data:</p>
<pre><code>companyName| employeeInfo
----------------------------
ABC        | {name: Ramesh, age:32 years}
ABC        | {name: Mohan, experience:2 years}
DEF        | {name: Dinesh, age:39, experience:5 years}
HIJ        |
DEF        | {name: Mohit}

</code></pre>
<p>I have to create an external table 'employee' from this data which should have this result:</p>
<pre><code>companyName| employeeKey | employeeValue
------------------------------------------
ABC        | name        | Ramesh
ABC        | age         | 32 years
ABC        | name        | Mohan
ABC        | experience  | 2 years
ABC        | name        | Dinesh
ABC        | age         | 39
ABC        | experience  | 2 years
HIJ        |             |
DEF        | name        | Mohit

</code></pre>
","<json><tsql><azure-data-factory><serverless><azure-synapse>","2023-04-11 05:48:21","36","0","1","75982998","<p>One approach would be to import the data from the external table into a temporary table or a table variable, and then use the OPENJSON function on the imported data to flip the data into columns. e.g:</p>
<pre><code>SELECT companyName, employeeInfo
INTO #tempTable
FROM companyDetail
-- WHERE ...

SELECT companyName, [Key] as employeeKey, [Value] as employeeValue
FROM #tempTable
CROSS APPLY OPENJSON(employeeInfo)
</code></pre>
<p>see: <a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/openjson-transact-sql?view=sql-server-ver16"" rel=""nofollow noreferrer"">OPENJSON</a></p>
"
"75982462","ADF Event trigger based on sharepoint","<p>I have a requirement to trigger ADF pipeline when a file is uploaded into sharepoint. The pipeline copies the file from sharepoint to ADLS. As per my understanding, It is not possible to create storage event trigger based on Sharepoint (as per Microsoft documentation <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory</a>)</p>
<p>So the alternate approach I am planning is to use a tumbling trigger which will trigger the pipeline every 30 minutes and check if the file is available in the sharepoint location and if it is available it will copy files to ADLS location and another pipeline will be triggered which is event based on ADLS location and does the necessary processing</p>
<p>Can anyone please let me know if this is a good approach or any better approach is recommended. I am pretty new to Azure world.
Thanks in advance</p>
","<azure><sharepoint><azure-data-factory>","2023-04-11 04:42:23","70","0","1","75983373","<p>Best way would be to leverage a logic app/power automate
<a href=""https://i.stack.imgur.com/jMFsY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jMFsY.png"" alt=""enter image description here"" /></a></p>
<p>which would trigger once the file appears in sharepoint.</p>
<p>Then via logic app, you can trigger the ADF pipeline via its REST API or in built logic app connector</p>
<p><a href=""https://i.stack.imgur.com/BGIFW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BGIFW.png"" alt=""enter image description here"" /></a></p>
"
"75979543","How to modify source column in Copy Activity of Azure Data Factory","<p>I am trying to copy data from a csv file into a database table. I created a pipeline in Azure Data Factory and added a <code>Copy Activity</code>. I set the source and the destination datasets.</p>
<p>During the copy operation, I want to prefix the values of the column <code>Col1</code> in the source csv file with a fixed string. At the bottom of the <code>Mapping</code> section in the UI, there is an option such as &quot;<code>Add dynamic content</code>&quot;. I guess I would be able to add an expression here for such a mapping but I could not find any information on how to do that.</p>
<p>What is the correct way of accomplishing this?</p>
","<azure><azure-data-factory>","2023-04-10 18:05:03","61","0","2","75983075","<p>Adding a dynamic column value in copy activity supports only the ADF parameter /function values which would be common for all rows.</p>
<p>As of now there is no direct support to modify a column at row by row level in copy activity.</p>
<p><a href=""https://i.stack.imgur.com/bP1W0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bP1W0.png"" alt=""enter image description here"" /></a></p>
<p>You can do it via 2 ways:</p>
<ol>
<li>use copy activity to copy into staging table and then use an SP activity to modify from staging table to final tables</li>
<li>use dataflow derived column transformation</li>
</ol>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column</a></p>
"
"75979543","How to modify source column in Copy Activity of Azure Data Factory","<p>I am trying to copy data from a csv file into a database table. I created a pipeline in Azure Data Factory and added a <code>Copy Activity</code>. I set the source and the destination datasets.</p>
<p>During the copy operation, I want to prefix the values of the column <code>Col1</code> in the source csv file with a fixed string. At the bottom of the <code>Mapping</code> section in the UI, there is an option such as &quot;<code>Add dynamic content</code>&quot;. I guess I would be able to add an expression here for such a mapping but I could not find any information on how to do that.</p>
<p>What is the correct way of accomplishing this?</p>
","<azure><azure-data-factory>","2023-04-10 18:05:03","61","0","2","75983234","<ul>
<li>As directed by <strong>@Zorkolot</strong> and <strong>@Nandan</strong>, copy data activity in general is to deal with moving files/folders from different sources to different sinks.</li>
<li>Since the requirement is to add a fixed string prefix to an existing column, you need to use dataflows. Dataflows can help with manipulating/converting data as required.</li>
<li>You can use derived column transformation as shown in the below. I have added <code>pre</code> string as a prefix to id column.</li>
</ul>
<p><img src=""https://i.imgur.com/cumBEcF.png"" alt=""enter image description here"" /></p>
<ul>
<li>Another way is to use one copy data activity and a script activity to copy to the database and write an update query with <code>concat</code> function on the required column with prefix with a query like this:</li>
</ul>
<pre><code>update t1 set &lt;your_col&gt;=concat('pre',&lt;your_col&gt;)
</code></pre>
<p><img src=""https://i.imgur.com/otwTEsD.png"" alt=""enter image description here"" /></p>
<ul>
<li>Another way would be to use Python notebook to add the prefix to required column and then move it to database.</li>
</ul>
"
"75979474","Why is Azure Data Factory SharePoint Online List Linked Service Connecting without Error, but not Showing Preview Data","<p>I am attempting to use a SharePoint Online list as a data source in an Azure Data Factory pipeline.  I can get it to connect, but it shows no data.</p>
<p>Background:
I have established an Azure Data Factory Copy Data Activity and Linked Service to do so per the Microsoft documentation:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list?tabs=data-factory#linked-service-properties"" rel=""nofollow noreferrer"">Microsoft Docs</a></p>
<p>As it instructs, I established an App, set up its secret in the Azure Vault, have an integrated runtime, and established the Apps permissions on the SharePoint site. It now connects without error and returns the list of assets in Azure Data Factory when clicking the &quot;List name&quot; drop down list in the Linked Service Connection tab.</p>
<p>What is not working is the &quot;Preview data&quot; link.  After clicking, it attempts to get the data, but returns an empty object with the screen message of &quot;No Data to Preview&quot;.  (There are a couple of hundred rows in the list.)</p>
<p><a href=""https://i.stack.imgur.com/Oy3iq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oy3iq.png"" alt=""enter image description here"" /></a></p>
<p>I explored using a query and confirmed through its syntax that it is connecting and even validating all column names of the List - it just is not returning data.</p>
<p>What am I missing?</p>
","<azure-data-factory><sharepoint-online>","2023-04-10 17:54:41","48","0","1","75983304","<p>I tried to reproduce the issue in my environment and able to get the data from <code>SharePoint Online List</code>  and able to get the data to get the data from <code>SharePoint Online List</code> follow below steps:</p>
<ol>
<li>Register a application with the Microsoft Identity platform (Azure Active Directory). Take note of the following values, which you will use to define the associated service:
<ul>
<li>Application ID</li>
<li>Application key</li>
<li>Tenant ID</li>
</ul>
</li>
<li>Permit your registered application to access SharePoint Online. follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list?tabs=data-factory#prerequisites"" rel=""nofollow noreferrer"">Allow app to access SharePoint Online</a>.</li>
</ol>
<p><strong>My lists and the no item in each list:</strong></p>
<p><img src=""https://i.imgur.com/TWSrBwI.png"" alt=""enter image description here"" /></p>
<p><strong>In ADF I am using this list only which have 2 items:</strong></p>
<p><img src=""https://i.imgur.com/vMppZ6Q.png"" alt=""enter image description here"" /></p>
<p><strong>I am able to preview the data correctly.</strong></p>
<p><img src=""https://i.imgur.com/PN1vj09.png"" alt=""enter image description here"" /></p>
<hr />
<p><strong>If still issue persist :</strong></p>
<ul>
<li>check your data if it is available in list, or in proper format.</li>
<li>Or raise a <a href=""https://azure.microsoft.com/en-us/support/create-ticket"" rel=""nofollow noreferrer"">support ticket</a> for deeper investigation.</li>
</ul>
"
"75974715","Is there any way to upload files to S3 bucket using Azure Data Factory?","<p>I am trying to setup an ETL pipeline where</p>
<ol>
<li>Source is a SQL Server table's column in binary stream form</li>
<li>Destination (sink) is s3 bucket</li>
</ol>
<p>My requirements are:</p>
<ol>
<li>To read binary stream column from SQL Server table</li>
<li>Process the binary stream data row by row</li>
<li>Upload file to a S3 bucket for each binary stream data</li>
</ol>
<p>I have tried DataFlow, Copy, AWS Connectors on Azure Data Factory, but there is no option to set s3 bucket as destination (sink)</p>
<p>Is there any other approach available in Azure Data Factory to match these requirements?</p>
","<amazon-web-services><amazon-s3><.net-core><azure-functions><azure-data-factory>","2023-04-10 06:19:07","103","0","1","76110198","<p>One workaround is to use SFTP as sink using copy data activity. Below is something I have tried.</p>
<p>Here is the pipeline where I have used Lookup to get the SQL Table rows.</p>
<p><img src=""https://i.imgur.com/M64dhSB.png"" alt=""enter image description here"" /></p>
<p>In my foreach loop I have set the filename and file data into 2 variables.</p>
<p><img src=""https://i.imgur.com/cZvmaHn.png"" alt=""enter image description here"" /></p>
<hr />
<p><img src=""https://i.imgur.com/z7swJsL.png"" alt=""enter image description here"" /></p>
<p>Take a sample csv file (1 row, 1 column which will be ignore anyway) and add an additional column with dynamic content value as processed binary stream data (keep only the required additional column and delete the rest in mapping).</p>
<p><img src=""https://i.imgur.com/7cJ5TNO.png"" alt=""enter image description here"" /></p>
<hr />
<p><img src=""https://i.imgur.com/mOiwyCf.png"" alt=""enter image description here"" /></p>
<p>Create a sink dataset to destination blob container where you'll store each row data as separate blob.</p>
<p><img src=""https://i.imgur.com/o6AMMJJ.png"" alt=""enter image description here"" /></p>
<p>Now you can add another copy activity making the sink of copy activity 1 (previous copy activity) as source and then make sink as SFTP which allows you to connect to amazon s3.</p>
<p><img src=""https://i.imgur.com/dY5BpWS.png"" alt=""enter image description here"" /></p>
<hr />
<p><img src=""https://i.imgur.com/kVVellX.png"" alt=""enter image description here"" /></p>
<p>For more information, you can refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp?tabs=data-factory"" rel=""nofollow noreferrer"">Copy and transform data in SFTP server using Azure Data Factory</a></p>
"
"75973860","How to upload bindary stream data to S3 bucket in file format using Azure Data Factory?","<p>I am trying to create an AZURE PIPELINE to</p>
<ol>
<li>READ BINARY STREAM DATA from SQL SERVER</li>
<li>and UPLOAD this BINARY STREAM DATA as a FILE on S3 BUCKET</li>
</ol>
<p>I have tried COPY/DATAFLOW feature but there is no option to SINK data to S3 buckcet</p>
<p><strong>Is there any process on AZURE DATA FACTORY which is able to do that?</strong></p>
","<azure><amazon-s3><azure-data-factory>","2023-04-10 02:10:17","41","0","1","75977365","<blockquote>
<p><strong>Is there any process on AZURE DATA FACTORY which is able to do that?</strong></p>
</blockquote>
<hr />
<p><strong>AFAIK, we can't set Amazon S3 as sink in data factory we have to try alternate to copy file to S3.</strong></p>
<hr />
<p>To active this I will suggest you to first copy the file from SQL server to blob storage and then use databricks notebook to copy file from blob storage to Amazon S3</p>
<ul>
<li><p>Copy data to Azure blob Storage.
Source:
<img src=""https://i.imgur.com/cmNovNZ.png"" alt=""enter image description here"" />
Destination:
<img src=""https://i.imgur.com/Nz5pojP.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Create notebook in databricks to copy file from Azure blob storage to Amazon S3.</p>
</li>
</ul>
<p><strong>Code Example:</strong></p>
<pre class=""lang-py prettyprint-override""><code>from pyspark.sql import SparkSession
# Get file name from ADF
filename = dbutils.widgets.get(&quot;file&quot;)
# Define the Azure Blob Storage account credentials
spark.conf.set(&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.blob.core.windows.net&quot;, &quot;&lt;your-storage-account-access-key&gt;&quot;)
# Define the S3 bucket credentials
spark.conf.set(&quot;spark.hadoop.fs.s3a.access.key&quot;, &quot;&lt;your-s3-access-key&gt;&quot;)
spark.conf.set(&quot;spark.hadoop.fs.s3a.secret.key&quot;, &quot;&lt;your-s3-secret-key&gt;&quot;)
# Define the source and destination paths
source_path = &quot;wasbs://&lt;container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net/&quot;+filename
Destination_path = &quot;s3a://&lt;your-s3-bucket-name&gt;/&lt;s3-destination-path&gt;&quot;+filename
# Copy the blob file from Azure to S3
dbutils.fs.cp(source_path, destination_path)
</code></pre>
<p><strong>Notebook:</strong>
<img src=""https://i.imgur.com/UyqJMXu.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, call that databricks notebook in your pipeline to copy files and create   Base parameters <strong>file</strong> with <code>filename value</code>.
<img src=""https://i.imgur.com/CN8Xr8A.png"" alt=""enter image description here"" /></li>
</ul>
<hr />
<p><strong>You can raise the feature request for this sink connector <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"" title=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"">here</a>.</strong></p>
"
"75973653","Azure data factory Pipeline is failing : spark.rpc.message.maxSize","<p>&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to reason: at Source 'source1': Job aborted due to stage failure: Serialized task 10:0 was 135562862 bytes, which exceeds max allowed: spark.rpc.message.maxSize (134217728 bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values.&quot;</p>
<p>I have simple adf pipeline which was working fine but started failing from few days.</p>
<p>The source is a REST API call.
Can you please help in fixing this?, where can I change the suggested setting.</p>
<p><a href=""https://i.stack.imgur.com/upbLw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/upbLw.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ObPyY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ObPyY.png"" alt=""enter image description here"" /></a></p>
","<azure><google-cloud-dataflow><azure-data-factory>","2023-04-10 01:07:04","50","0","1","75975092","<blockquote>
<p>StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to reason: at Source 'source1': Job aborted due to stage failure: Serialized task 10:0 was 135562862 bytes, which exceeds max allowed: spark.rpc.message.maxSize (134217728 bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values.</p>
</blockquote>
<p>According to the error instruction, the problem appears to be caused by overloaded spark resources. Please experiment with an Integration runtime and increase the <strong>compute size</strong> of the <strong>Data Flow Integration</strong>. compute size is size of compute used in spark cluster.</p>
<ul>
<li>Go to linked service and click on pencil icon beside integration runtime.
<img src=""https://i.imgur.com/kJmwK7D.png"" alt=""enter image description here"" /></li>
<li>Data Flow Runtime &gt;&gt; Compute size
<img src=""https://i.imgur.com/byURo7i.png"" alt=""enter image description here"" /></li>
</ul>
"
"75971879","How to Feed Output of Azure Function to For-Each Activity of Data Factory?","<p>I am using Azure Function using Python code to fetch the list of all collections in a Cosmos Db and feed the Output to For-Each Activity in Data factory. Ultimate goal is to Copy All Collections Dynamically to another DB.</p>
<p>Pseudo script</p>
<pre><code>
List1=[&quot;col1&quot;,&quot;col2&quot;,&quot;col3&quot;]

Json=json.dumps(List1)

return func.HttpsResponse(List1)

</code></pre>
<p>The python script is working and returns a list of all collections successfully.</p>
<p>However, when I am calling the function through data factory, the output is coming as a String rather than a Array. For -Each activity is failing as it expects Array.</p>
<p>Do I need to modify the python script or Need to modify something in Data factory side?</p>
<p>This stackoverflow thread has something similar, but the exact answer about how to write the script is Not provided
<a href=""https://stackoverflow.com"">text</a>
<a href=""https://stackoverflow.com/questions/67243634/is-there-any-azure-data-factory-activity-to-get-cosmos-collection-list"">Is there any Azure Data factory activity to get cosmos collection list?</a></p>
","<python><azure-functions><azure-cosmosdb><azure-data-factory>","2023-04-09 17:13:57","67","1","1","75974441","<blockquote>
<p>However, when I am calling the function through data factory, the output is coming as a String rather than a Array. For -Each activity is failing as it expects Array.</p>
</blockquote>
<p>I tried with below code in my environment and I got the same output in String type.</p>
<pre><code>List1=[&quot;col1&quot;,&quot;col2&quot;,&quot;col3&quot;]
Json=json.dumps(List1)
return func.HttpResponse(Json)
</code></pre>
<p><img src=""https://i.imgur.com/ztq93QV.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>Do I need to modify the python script or Need to modify something in Data factory side?</p>
</blockquote>
<p>To use the above response in a ForEach, you need to convert it to array. For that use <code>json()</code> function.</p>
<p>Here, for sample I have converted the string response from Azure function activity to an array variable using the below expression. You can use it in ForEach expression.</p>
<pre><code>@json(activity('Azure Function1').output.Response)
</code></pre>
<p><img src=""https://i.imgur.com/bHyfeVB.png"" alt=""enter image description here"" /></p>
<p><strong>Output as array:</strong></p>
<p><img src=""https://i.imgur.com/P7kKMR7.png"" alt=""enter image description here"" /></p>
"
"75971638","How to change the column data types of a data flow in Azure Data Factory","<p>I'm running a dataflow activity in ADF which uses a REST API response as its source. The response has integer values for some key values like the following example.</p>
<p><code>&quot;ValuatedBy&quot;: 0,</code></p>
<p>These values are interpreted as Booleans by ADF and displays as follows in the inspect tab for the source.</p>
<ol>
<li>How to change these values to have the correct data type?</li>
<li>Why does this happen?</li>
</ol>
<p><a href=""https://i.stack.imgur.com/OX2eQ.png"" rel=""nofollow noreferrer"">Wrong interpretation of the data types</a></p>
<p>I tried Casting but casting activity won't help as Boolean casted to String displayed as &quot;true&quot; or &quot;false&quot;.</p>
","<azure-data-factory>","2023-04-09 16:24:36","76","0","1","75974150","<ul>
<li>You can change the type of the column manually in source transformation.</li>
<li>Click the <strong>Projection</strong> tab in the source transformation of data flow.</li>
<li>In the column name which contains <strong>ValuatedBy</strong> field, select <em>Define Complex Type</em>.</li>
</ul>
<p><img src=""https://i.imgur.com/0A0tIbr.png"" alt=""img1"" /></p>
<ul>
<li>In dataflow expression builder, change the type of <code>ValuatedBy</code> field from <code>boolean</code> to <code>Integer</code> or any other required type. Then click Save and Finish.</li>
</ul>
<p><img src=""https://user-images.githubusercontent.com/113445679/230813084-7ecbd55c-8562-424e-ac7a-2758e3711301.gif"" alt=""gif1"" /></p>
<ul>
<li>Once changed, ValuatedBy is reflected as Integer type in <em>Inspect</em> tab.
<img src=""https://i.imgur.com/opOVziS.png"" alt=""img2"" /></li>
</ul>
<blockquote>
<ol start=""2"">
<li>Why does this happen?</li>
</ol>
</blockquote>
<p>This happens because ADF automatically infers the data types of the columns in the source based on the first few rows of data. If the first few rows of data contain only 0s and 1s, ADF may infer that the column as a Boolean column.</p>
"
"75969733","Azure Functions Apps combined with a Self-Hosted Integration Runtime","<p>We developed an Azure Function generating some files. We would like to copy these files to a on-premise SMB-share. This SMB-share has been made available by an on-premise self-hosted integration runtime.</p>
<p>Is it possible to copy files from an Azure Function to an on-premise SMB-share through an on-premise self-hosted integration runtime? If so, how?</p>
","<azure-functions><azure-data-factory>","2023-04-09 09:37:26","51","0","1","76061005","<p>Yes, it is possible to copy files from an Azure Function to an on-premise SMB-share through an on-premise self-hosted integration runtime. You can follow the below steps:</p>
<ol>
<li>First, Configure the on-premise self-hosted integration runtime to connect to the on-premise SMB-share. You can do this by installing the integration runtime on a machine that has access to the SMB-share, and then configuring the SMB-share as a linked service in Azure Data Factory.</li>
<li>In your Azure Function, use the Azure File Storage API to write the files to an Azure File Share. You can create a new file share using the Azure Storage Explorer or the Azure Portal.</li>
<li>Create an Azure Data Factory pipeline that copies the files from the  Azure File Share to the on-premise SMB-share. You can use the &quot;Copy Data&quot; activity in Azure Data Factory to do this. In the linked service for the source, select the Azure File Share you created in step 2. In the linked service for the destination, select the SMB-share you configured in step 1.</li>
<li>Trigger the Azure Data Factory pipeline from your Azure Function. You can use the Azure Data Factory REST API to trigger the pipeline from your Azure Function.</li>
</ol>
"
"75954131","Unable to delete a data flow in Azure Data factory in git mode","<p>I have an azure data factory data flow that I have to delete. I have to because in order to publish my latest data factory changes, I need to clear the validations on this data flow. However, I can't delete it nor can I update it to another name or fix the validations. When I save any of those changes, it says it failed to save, or gives the below error:</p>
<pre><code>Failed to save All resources.
Error: {&quot;$id&quot;:&quot;1&quot;,&quot;innerException&quot;:null,&quot;message&quot;:&quot;The path '//dataflow/currentplan.json' does not exist at commit 'b04d8869c860afc6ac801503d7a76cd438a8f91f'\r\nParameter name:
</code></pre>
<p>Yet when I refresh, I see that data flow there and I can click into it and see the flow objects.</p>
<p>Any tips on how I can get around this? I'd try an earlier commit but I can't find a way to check out an earlier one like you would with normal git. I also would just copy my good data flows to another data factory and start there, but I have no idea how to do that. Has anyone encountered this issue with data factory?</p>
<p>Thank you!</p>
<p>Tried deleting as normal, tried editing the json file for the data flow.</p>
","<azure><azure-data-factory><etl>","2023-04-06 22:08:01","28","0","1","75961780","<p>Go the git branch---&gt; dataflow ---&gt; yourfile.json ( delete directly from there )</p>
<p>Now if you refresh datafactory ,you wont see the dataflow anymore and if you continue to work on same branch ,adf_publish also gets updated if you are in collab branch .</p>
"
"75942653","Is it possible to use ""The same git hub enterprise repo for different data factories ""?","<p>I have a dev datafactory named &quot;adf-dev&quot;
And another prod datafactory named &quot;adf-prod&quot;</p>
<p>And I have a single Github Enterprise account which has One repo named &quot;adf-repo&quot; which is having main,dev,uat branches .</p>
<p>can I connect adf-repo to the both datafactories ? and publish if I maintain different publish branches ?</p>
","<azure><azure-data-factory>","2023-04-05 17:56:57","55","0","1","75946041","<blockquote>
<p>Can I connect adf-repo to the both datafactories ? and publish if I maintain different publish branches ?</p>
</blockquote>
<p>Yes, you can connect 2 different data factories to the same repo in GitHub. And you can also have different publish branches for each of the data factories for the same repo. The following is the demonstration of the same. I have 2 data factories (one's factory environment is production and the other's is development) connected to same repo but different branches.</p>
<ul>
<li>For df060423
<img src=""https://i.imgur.com/JSArVpl.png"" alt=""enter image description here"" /></li>
</ul>
<br>
<ul>
<li>For df050423
<img src=""https://i.imgur.com/D3ZkNnb.png"" alt=""enter image description here"" /></li>
</ul>
<p>When I publish a sample pipeline to each of them, it has been successfully published as shown in the below images.</p>
<ul>
<li>For 060423
<img src=""https://i.imgur.com/PNhUkT0.png"" alt=""enter image description here"" /></li>
</ul>
<br>
<ul>
<li>For df050423
<img src=""https://i.imgur.com/yOTcCyJ.png"" alt=""enter image description here"" /></li>
</ul>
"
"75942096","Block access to a specific pipeline in ADF","<p>I have a pipeline named &quot;Privacy Pipeline&quot;.</p>
<p>I would like to only one person to be able to access this pipeline &quot;user@data.com&quot;</p>
<p>Is it possible to apply RBAC IAM controls at this level?</p>
<p>I've tried creating a custom role in Access control in the data factory and in PowerShell but I've not been able to get this working. Is it even possible?</p>
<p><a href=""https://i.stack.imgur.com/EHFsA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EHFsA.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-rbac>","2023-04-05 16:50:42","40","0","1","75993865","<p>From the above image, the error message is self explanotry to solve the problem:</p>
<p><strong>Problem:</strong> You are experiencing this issue due to you're currently signed in with a user that <strong>doesn't have permission to assign roles</strong> at the selected scope shown in the image.</p>
<p><strong>Solution:</strong> You must be signed in as a user that is assigned a role such as <code>owner</code> or <code>user access administrator</code> at this scope to resolve the issue.</p>
<p>Below gif shows steps to see all available permissions and actions for Azure Data factory while creating custom role. Kindly try the same and go through permissions and actions there and take call which one to consider in your custom role:</p>
<p><a href=""https://i.stack.imgur.com/cV8yH.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cV8yH.gif"" alt=""enter image description here"" /></a></p>
<p>For more details, refer to <a href=""https://techcommunity.microsoft.com/t5/fasttrack-for-azure/custom-role-to-restrict-azure-data-factory-pipeline-developers/ba-p/3299161"" rel=""nofollow noreferrer"">Custom role to restrict Azure Data Factory pipeline developers to create/delete linked services</a>.</p>
"
"75941784","Parameter value in table","<p>I have hard coded values in one of the table</p>
<p><img src=""https://i.stack.imgur.com/imV2M.png"" alt=""enter image description here"" /></p>
<p>And I am passing this column as a parameter and try to delete some values .The below is mentioned querie:</p>
<pre class=""lang-sql prettyprint-override""><code>DELETE FROM [S4].[@{pipeline().parameters.DESTINATION_TABLE_NAME}] WHERE ERDAT &gt;='@{item().LastLoadDate}' OR @{pipeline().parameters.WHERE_SQL} &gt;='@{item().LastLoadDate}
</code></pre>
<p>I am getting error :</p>
<blockquote>
<p>ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Incorrect syntax near '&gt;'.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Incorrect syntax near '&gt;'.,Source=.Net SqlClient Data Provider,SqlErrorNumber=102,Class=15,ErrorCode=-2146232060,State=1,Errors=[{Class=15,Number=102,State=1,Message=Incorrect syntax near '&gt;'.,},],'</p>
</blockquote>
<p>Please advise.</p>
<p>Regards
Rohit</p>
","<azure-data-factory>","2023-04-05 16:13:13","61","0","1","75975360","<ul>
<li>As shown in the image, the <code>where_sql</code> column also has null values. When you try to use one of these values in the query, the error occurs.</li>
<li>The following is a demonstration where I have a table with <code>where_sql</code> column. I used a look up activity to get these values.</li>
</ul>
<p><img src=""https://i.imgur.com/kfndY0V.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I used for each to iterate through this result. Inside for each I have used a script activity with following script:</li>
</ul>
<pre><code>delete  from demo where idt&gt;='2023-04-01'  or @{item().where_sql}&gt;='2023-04-01'
</code></pre>
<ul>
<li>When I execute this script, it gives the required result. But when there is null value, the query gives the same error.</li>
</ul>
<p><img src=""https://i.imgur.com/3GBXN6r.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can see what query is getting executed in this scenario. The following is an image for reference.</li>
</ul>
<p><img src=""https://i.imgur.com/rM47Mzg.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, try filtering out the rows where <code>where_sql</code> column value is null thus leaving you with the required column names to query with.</li>
</ul>
"
"75941650","Can we use ADF to copy data into Google Cloud stoarge","<p>Is it possible to utilize ADF for transferring data into Google Cloud storage from a blob folder or export data from a database to Google Cloud storage.</p>
<p><a href=""https://i.stack.imgur.com/wuNHN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wuNHN.png"" alt=""enter image description here"" /></a></p>
<p>Error message : The linked service in sink dataset does not support sink.</p>
","<azure><azure-data-factory>","2023-04-05 16:00:17","57","0","1","76074443","<p>As per the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview#supported-data-stores"" rel=""nofollow noreferrer"">Official Documentation</a> of copy activity connectors,</p>
<blockquote>
<p>Google Cloud Storage only supports as Source of the copy the activity. Currently it is not supported as the sink of the copy activity.</p>
</blockquote>
<p>You can see here when I am trying to create GCS sink dataset.</p>
<p><img src=""https://i.imgur.com/eBcNI7c.png"" alt=""enter image description here"" /></p>
<p>You can raise the feature request for that <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">here</a>.</p>
<p>So, as a workaround try to copy the data to GCS using code via databricks or functions as per your requirement by making connections to both source and target.</p>
"
"75940557","Azure data factory - pass multiple items to foreach to use as parameters?","<p>I have a table that looks like the below stored in SQL.. it contains a LOT of rows which I need to loop through.</p>
<pre><code>ContactID       Fromdate           Todate
1               01-Jan-2022      01-Feb-2022
2               01-Jan-2022      01-Feb-2022
</code></pre>
<p>I have a lookup activity to read from this table, and I need to pass the output of this lookup activity to my foreach so I can amend the linked service to use a dynamic API link..</p>
<p><a href=""https://i.stack.imgur.com/kKT1g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kKT1g.png"" alt=""enter image description here"" /></a></p>
<p>As of now I am able to pass the ContactID into the foreach and everything works fine.. however I now need to pass in the fromdate and todate also but I am not sure how to do this.. can someone explain the best way of achieving this?</p>
","<azure-data-factory>","2023-04-05 14:17:16","30","0","1","75941007","<p>There are a few things to understand that will inform your answer.</p>
<p>First: you do not &quot;pass parameters&quot; to a ForEach. Instead, ForEach accesses variables and parameters just like any other activity. All variables and parameters are global in the context of the pipeline run.</p>
<p>Second: in the case of a ForEach, you do specify the item list to iterate over. By specifying the output value of the Lookup, it should be one row per iteration. Inside a ForEach, you can reference the current iteration by using <code>@item()</code>, so you should be able to reference properties like <code>@item().Fromdate</code>.</p>
<p>Third: because of the global nature of variables, you may seriously want to consider checking &quot;Sequential&quot;. Running iterations in parallel that reference variables can lead to unexpected results.</p>
<p>Fourth: you say there are &quot;a LOT of rows&quot;. I believe Lookup has a hard limit of 5,000 rows, so you should be aware of that.</p>
"
"75933884","ADF Storage account trigger pipeline delete after use but data keeps coming","<p>Azure Data factory pipeline gets triggered when files gets uploaded in Azure Blob Storage. Data factory waits 5 minutes and starts running. These are the tasks</p>
<ol>
<li>Copies to SQL server</li>
<li>Deletes (soft delete enabled)</li>
</ol>
<p>But between #1 and #2, more files where added to blob storage. How to prevent the second step from deleting these new data?</p>
","<azure-data-factory>","2023-04-04 21:11:16","56","0","1","75947875","<blockquote>
<p>How to prevent the second step (deleting all files) from deleting new data?</p>
</blockquote>
<p>As we discussed in comment you can adjust your delete so that it only deletes the file, processed in particular pipeline not all files,</p>
<ul>
<li><p>Create file name and folder path parameter in pipeline as string
<img src=""https://i.imgur.com/p3Zf6gI.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then go to edit trigger and add details captured by trigger such as the folder path and file name of the blob into the properties <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code> to the respective pipeline parameters.
<img src=""https://i.imgur.com/fH93wQL.png"" alt=""enter image description here"" /></p>
</li>
<li><p>To delete the currently processed file you need to take delete activity and then create a dataset parameter to source.
<img src=""https://i.imgur.com/oq7ykDG.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Add this parameter in connection.
<img src=""https://i.imgur.com/TFAq3DI.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Add respective dynamic expression to each of the dataset parameter in a delete activity source respectively .
<strong>you can use the following dynamic expressions to fetch file path from trigger respectively:</strong></p>
</li>
</ul>
<pre class=""lang-json prettyprint-override""><code>To get Container name from folder path of trigger: @split(pipeline().parameters.folderpath,'/')[0]
To get folders details from folder path of trigger:@join(skip(split(pipeline().parameters.folderpath,'/'),1),'/')
To get file name from trigger : @pipeline().parameters.filename
</code></pre>
<p><img src=""https://i.imgur.com/x7VbZ2d.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>It will delete the current file processed by pipeline trigger.</strong></p>
</blockquote>
"
"75933592","How to set up authentication on Purview to ADF?","<p>I just connected Azure Data Factory to my Purview account for the first time, the connection is working but seems like it's different from other data sources.</p>
<p><a href=""https://i.stack.imgur.com/zYK9h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zYK9h.png"" alt=""enter image description here"" /></a></p>
<p>Also I can see is connected from my ADF:</p>
<p><a href=""https://i.stack.imgur.com/4lH24.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4lH24.png"" alt=""enter image description here"" /></a></p>
<p>How can I setup the authentication and start the scan? I read on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connect-data-factory-to-azure-purview#set-up-authentication"" rel=""nofollow noreferrer"">documentation</a> that I might need to grant the data factory's managed identity Data Curator role on your Microsoft Purview root collection, but I didn't understand how.</p>
<p>Can someone do a step-by-step?</p>
","<azure-data-factory><azure-purview>","2023-04-04 20:29:24","71","0","1","75984349","<p>Connect the Azure account to thedata factoy by clicking on Connect to Purview account</p>
<p><img src=""https://i.imgur.com/qvj12tw.png"" alt=""enter image description here"" /></p>
<p>Select the Purview account</p>
<p><img src=""https://i.imgur.com/e4iCv35.png"" alt=""enter image description here"" /></p>
<p>It will connect automatically without any authentication to the data factory</p>
<p><img src=""https://i.imgur.com/3eKHqnJ.png"" alt=""enter image description here"" /></p>
<p>You can scan all the data flows in ADF</p>
<p><img src=""https://i.imgur.com/0b4fswc.png"" alt=""enter image description here"" /></p>
<p>Go to purview account and click on browse asset</p>
<p><img src=""https://i.imgur.com/0Rc22zc.png"" alt=""enter image description here"" /></p>
<p>Select data factory as source type:</p>
<p><img src=""https://i.imgur.com/e6Yh9Tn.png"" alt=""enter image description here"" /></p>
<p>You can find all activities of data factory there.
example:</p>
<p><img src=""https://i.imgur.com/bNdc8rz.png"" alt=""enter image description here"" /></p>
"
"75931254","Azure Data Factory - loop through parametrized linked service?","<p>I have a pipeline job that pulls data from an API linked service and inserts it into a SQL table via the copy data activity.</p>
<p>The base URL for the API is below</p>
<p><a href=""https://api.test.com/v3/contacts/14299157/contactStats?startDate=2022-10-01&amp;endDate=2022-11-03"" rel=""nofollow noreferrer"">https://api.test.com/v3/contacts/14299157/contactStats?startDate=2022-10-01&amp;endDate=2022-11-03</a></p>
<p>I have a SQL table that looks like the below..</p>
<pre><code> ContactID              StartDate              EndDate
     1                  01-jan-2020           01-jan-2021
     1                  01-jan-2022           01-jan-2023
</code></pre>
<p>Is it possible to run a loop on this? So I want it to reference my SQL table and run the API for the first row which would look like this..
<a href=""https://api.test.com/v3/contacts/1/contactStats?startDate=2020-01-01&amp;endDate=2021-01-01"" rel=""nofollow noreferrer"">https://api.test.com/v3/contacts/1/contactStats?startDate=2020-01-01&amp;endDate=2021-01-01</a>
and output the data into a table. It would then run again for the next row..
<a href=""https://api.test.com/v3/contacts/1/contactStats?startDate=2022-01-01&amp;endDate=2023-01-01"" rel=""nofollow noreferrer"">https://api.test.com/v3/contacts/1/contactStats?startDate=2022-01-01&amp;endDate=2023-01-01</a></p>
<p>and also output this data to my table, until it has finished all of the rows in my SQL table.</p>
<p>How would I go about setting this up? Can someone please explain if this is doable?</p>
","<azure-data-factory>","2023-04-04 15:35:21","51","0","1","75936732","<blockquote>
<p>How would I go about setting this up? Can someone please explain if this is doable?</p>
</blockquote>
<p><strong>UPDATED ANSWER:</strong></p>
<p>To loop on parameterized linked service, we need to pass the list of API's or modify the res Api dynamically as per your requirement and to for each loop.</p>
<ul>
<li>First take Lookup activity and get the table data from SQL.
<img src=""https://i.imgur.com/jmEJCxU.png"" alt=""enter image description here"" /></li>
<li>Then pass that lookup Output to Foreach activity <code>@activity('Lookup1').output.value</code>. <img src=""https://i.imgur.com/xCKukXg.png"" alt=""enter image description here"" /></li>
<li>Create linked service for Rest API And divide your URL to base URL and Relative URL as blow enter your <code>https://api.test.com/v3/contacts/</code> as base URL.
<img src=""https://i.imgur.com/xL1zsAJ.png"" alt=""enter image description here"" /></li>
<li>Then go to that dataset and create dataset Parameter
<img src=""https://i.imgur.com/JeBmLSt.png"" alt=""enter image description here"" />- Enter relative URL as <code>@concat(dataset().contactId,'/contactStats?startDate=',dataset().Startdate,'&amp;endDate=',dataset().Enddate)</code>  With dynamic parameters
<img src=""https://i.imgur.com/dAP3cLK.png"" alt=""enter image description here"" /></li>
<li>Under ForEach loop take a copy activity and select the source as Rest API and pass dynamic value foreach activity item as <code>@item().ContactId</code>,<code>@item().StartDate</code>,<code>@item().EndDate</code> respectively for each dataset parameter.
<img src=""https://i.imgur.com/GGBI2Ok.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Looping API:</strong>
<img src=""https://i.imgur.com/kkG6R9Q.png"" alt=""enter image description here"" /></p>
"
"75930811","Azure Data factory pipeline - dynamically get tenant ID","<p>So, I need to dynamically get tenant id inside the data factory pipeline.  Can anyone help in how to construct dynamic expression ?
So, for instance, am running a POST request to get SharePoint authentication token and for that I have the URL:</p>
<pre><code>https://accounts.accesscontrol.windows.net/&lt;TenantID&gt;/tokens/OAuth/2
</code></pre>
<p>So, the part</p>
<pre><code>https://accounts.accesscontrol.windows.net/
</code></pre>
<p>will be stored as a global parameter.
And my goal is to construct a dynamic expression inside the activity that would look like this</p>
<pre><code>@concat(pipeline().globalParameters.MSFTAccessControl_BaseURL,**&lt;getTenantId&gt;**, '/tokens/OAuth/2')
</code></pre>
<p>I need help with the function to construct the part <strong>**&lt;getTenantId&gt;**</strong></p>
","<azure-functions><azure-data-factory>","2023-04-04 14:48:44","61","0","1","75938136","<p>You can use the below Powershell script to get the Tenant ID.</p>
<pre><code>Connect-AzureAD
$tenant=Get-AzureADTenantDetail
$tenant.ObjectId
</code></pre>
<p><img src=""https://i.imgur.com/8qsKalu.png"" alt=""enter image description here"" /></p>
<p>Execute this <strong>Powershell script from ADF using custom activity</strong>.</p>
<p>You can go through this <a href=""https://azurelib.com/custom-activity-powershell-script-azure-data-factory/"" rel=""nofollow noreferrer"">tutorial</a> by <strong>@<strong>Deepak Goyal</strong></strong> to know about how to execute a Powershell script using custom activity.</p>
<p>After this custom activity, <strong>get the above Tenant Id</strong> from its output using the below dynamic content.
<code>@activity('&lt;MyCustomActivity&gt;').output.customOutput</code></p>
<p>Store it in a variable and use it in building your Sharepoint link using concat or String interpolation like this.</p>
<pre><code>@concat(pipeline().globalParameters.MSFTAccessControl_BaseURL,variables('tenant id'), '/tokens/OAuth/2')
</code></pre>
"
"75928540","Can we achieve multiple files(placed inside multiple container) copy to different target folder(each for one source container) using Azure datafactory","<p>I am having files in multiple Azure storage containers. And need to copy those files to respective destination folders. Can we achieve this using copy data activity in Azure datafactory ? If yes could you please let me know the steps.</p>
<p>Also needs to implement event based trigger on top it. Can we achieve the whole set up using Azure data factory ?</p>
","<azure><azure-data-factory>","2023-04-04 10:59:40","41","0","1","75931545","<blockquote>
<p>I don't have any specific copy condition. Simply I need a copy data pipeline. I do have files inside multiple containers which are present inside one storage account in source side. And on destination side I do have a parent folder inside a onprem server and I need to copy all files present inside the storage account to this parent folder with respective hierarchy.</p>
</blockquote>
<p>You can use <strong><code>Preserve heirarchy</code></strong> in copy activity sink.</p>
<p>These are my souce containers with some files in a storage account.</p>
<p><img src=""https://i.imgur.com/lhk5SFr.png"" alt=""enter image description here"" /></p>
<p>Create two binary datasets one for source and another for sink.</p>
<p>Don't give any file path in the source dataset.</p>
<p><img src=""https://i.imgur.com/9xCfpcO.png"" alt=""enter image description here"" /></p>
<p>In sink dataset, give the file path till your parent folder.</p>
<p><img src=""https://i.imgur.com/tMENaRP.png"" alt=""enter image description here"" /></p>
<p>Give the above to copy activity source and sink and in sink, select <code>Preserve heirarchy</code> as <strong>copyBehavior</strong>.</p>
<p><img src=""https://i.imgur.com/xvtmK7x.png"" alt=""enter image description here"" /></p>
<p>Containers will be copied with same hierarchy to the target folder like below.</p>
<p><img src=""https://i.imgur.com/85HYwTP.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>Also needs to implement event based trigger on top it. Can we achieve the whole set up using Azure data factory ?</p>
</blockquote>
<p>Yes, this can be done. If you want to copy whole hierarchy every time pipeline triggered, you can add the trigger to the above pipeline.</p>
<p>If you want to copy only the triggered file with same hierarchy in the target you need to use another pipeline for the trigger.</p>
<p>In the storage event trigger, select All containers for the source storage account.</p>
<p><img src=""https://i.imgur.com/lASqLWH.png"" alt=""enter image description here"" /></p>
<p>Go through <a href=""https://stackoverflow.com/a/75182900/18836744"">this answer</a> to understand about how to copy the triggered file from source to sink using the dataset parameters to preserve the hierarchy of the file.</p>
<p>But, in this it requires the <strong>Blob path ends with</strong>. So, in this case you need to create trigger to the same pipeline for every type of file ends with <code>.csv</code>,<code>.json</code>,.. etc to copy all types of files.</p>
"
"75928334","Azure Copy Activity to copy Hierarchical JSON from a REST API to Datalake Gen 2 with out explicit mapping","<p>Hi i am trying to copy a JSON object content using a GET request to a file with in data Lake gen 2 and when using Copy Activity i wanted to read all the columns and use the default mapping to load it as a parquet file into data lake gen 2. Could any one show me on what variables to set with in copy activity source as i have multiple REST API calls and I wanted to created separate files for each of them and still use only 1 copy activity to do that? Thank you very much</p>
<p>Some one told me that copy activity is not possible and to use data flow and flatten hierarchy but still in that we need to specify the contents on the JSON to extract all the columns</p>
","<azure-data-factory>","2023-04-04 10:34:29","52","1","1","76003894","<p>To copy data from rest API JSON to ADLS gen2 in parquet format follow below procedure:</p>
<p>Create Web Activity to retrieve the details of web activity with API URL and <code>GET</code> method</p>
<p><img src=""https://i.imgur.com/FraeE6i.png"" alt=""enter image description here"" /></p>
<p>Create Linked service for Rest API with Base URL:</p>
<p><img src=""https://i.imgur.com/WMq4zDj.png"" alt=""enter image description here"" /></p>
<p>Create Linked Service for ADLS Gen2 :</p>
<p><img src=""https://i.imgur.com/qSA1gdr.png"" alt=""enter image description here"" /></p>
<p>connect for each activity on success of web activity enter range function for items expression as <code>@range(1,activity('restAPI').output.total_pages)</code>add copy activity in for each activity, create Rest Api dataset by using created linked service for source of copy activity and set get activity as request method created parameter url and added value as <code>?page=@{item()}</code> and entered relative url as <code>@dataset().url</code></p>
<p>Source:</p>
<p><img src=""https://i.imgur.com/kZJTKvQ.png"" alt=""enter image description here"" /></p>
<p>Created ADLS parquet dataset by using created linked service for sink created filename parameter and added <code>Apipage@{item()}</code> as dynamic content and set it to filename in dataset and AFAIK flatten if the JSON is having one array, then it can be flattened in copy activity otherwise need to use data flow.</p>
<p><img src=""https://i.imgur.com/l3ym0aU.png"" alt=""enter image description here"" /></p>
<p>debug the pipeline, it debugged successfully.</p>
<p><img src=""https://i.imgur.com/jDlz2uK.png"" alt=""enter image description here"" /></p>
<p>The data is copied successfully to ADLS Gen 2 account.</p>
<p><img src=""https://i.imgur.com/Nul0wT1.png"" alt=""enter image description here"" /></p>
"
"75927232","Azure Data Factory Trigger json file: Add all pipelines in pipelines property","<p>Is there a value that can be entered that will apply the trigger to all pipelines in the branch? Currently I have to apply the trigger to each pipeline manually and in the future there will be more pipelines, if there is a value that applies to all then this would automate the process. I have not been able to find anything in Microsoft documents about it.</p>
<p>I have added the code of the current configuration with no pipelines added yet.</p>
<pre><code>{

    &quot;name&quot;: &quot;DailyTrigger&quot;,

    &quot;properties&quot;: {

        &quot;description&quot;: &quot;This trigger will execute once a day at 8am.&quot;,

        &quot;annotations&quot;: [ ],

        &quot;runtimeState&quot;: &quot;Started&quot;,

        &quot;pipelines&quot;: [ ],

        &quot;type&quot;: &quot;ScheduleTrigger&quot;,

        &quot;typeProperties&quot;: {

            &quot;recurrence&quot;: {

                &quot;frequency&quot;: &quot;Day&quot;,

                &quot;interval&quot;: 1,

                &quot;startTime&quot;: &quot;2023-04-04T07:55:00&quot;,

                &quot;timeZone&quot;: &quot;GMT Standard Time&quot;,

                &quot;schedule&quot;: {

                    &quot;minutes&quot;: [

                        0

                    ],

                    &quot;hours&quot;: [

                        8

                    ]

                }

            }

        }

    }

}
</code></pre>
<p>Nothing I have tried so far has worked. I couldn't find a character or anything to use from looking.</p>
","<json><azure><azure-data-factory>","2023-04-04 08:40:14","87","0","1","75938782","<ul>
<li>The following is a trigger that I have which has no pipelines attached.</li>
</ul>
<p><img src=""https://i.imgur.com/XbxpQLU.png"" alt=""enter image description here"" /></p>
<p>You can achieve this requirement using Rest API. I have the run the following python code in Azure Databricks notebook.</p>
<pre><code>#install msal using !pip install msal for getting bearer token
import msal
import requests
import json

tenant_id = '&lt;tenant_id&gt;'
client_id = '&lt;client_id&gt;'
client_secret = '&lt;client_secret&gt;'


app = msal.ConfidentialClientApplication(client_id=client_id, authority=f&quot;https://login.microsoftonline.com/{tenant_id}&quot;, client_credential=client_secret)

# Get the token
scopes = [&quot;https://management.azure.com/.default&quot;]
result = app.acquire_token_for_client(scopes=scopes)
print(result)

# Set the endpoint URL for the trigger you want to update
headers = {'Authorization': 'Bearer ' + result['access_token'], 'Content-Type': 'application/json'}

# Get the current trigger definition
trigger = requests.get(trigger_url, headers=headers).json()

trigger['properties']['pipelines'] = [{
                        &quot;parameters&quot;: {},
                        &quot;pipelineReference&quot;: {&quot;referenceName&quot;: &quot;pipeline1&quot;, &quot;type&quot;: &quot;PipelineReference&quot;},
                    },{
                        &quot;parameters&quot;: {},
                        &quot;pipelineReference&quot;: {&quot;referenceName&quot;: &quot;pipeline2&quot;, &quot;type&quot;: &quot;PipelineReference&quot;},
                    }]
# Send a PUT request to update the trigger
response = requests.put('https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/triggers/{triggerName}?api-version=2018-06-01', headers=headers, data=json.dumps(updated_trigger)).json()
print(response)
</code></pre>
<p><img src=""https://i.imgur.com/WDh8zXZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>After executing the above code, I was able to attach the trigger to required pipelines.</li>
</ul>
<p><img src=""https://i.imgur.com/ncn2MvY.png"" alt=""enter image description here"" /></p>
<ul>
<li>If you want to further automate this process, use another Rest API <code>https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelines?api-version=2018-06-01</code> which returns the details of all pipelines (GET method).</li>
<li>If you want to start the trigger, you can use the API <code>https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/triggers/{triggerName}/start?api-version=2018-06-01</code> (POST method)</li>
</ul>
<p><strong>NOTE:</strong> Create a service principle first and add role assignment to this app in your azure data factory. I have assigned it Contributor role.</p>
"
"75926223","Triggering a Databricks Delta Live Table from Azure Data Factory resets the whole tables. How do I disable that?","<p>I have created a pipeline in Azure Data Factory that triggers a Delta Live Table in Azure Databricks through a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web activity</a> mentioned <a href=""https://learn.microsoft.com/en-us/azure/databricks/delta-live-tables/workflows"" rel=""nofollow noreferrer"">here</a> in the Microsoft documentation.</p>
<p>My problem is that when I trigger my DLT from ADF, it resets the whole tables, meaning that my data becomes unavailable during the pipeline execution. To be more clear, it has this additional step in the screenshot below:</p>
<p><a href=""https://i.stack.imgur.com/hD9CS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hD9CS.png"" alt=""enter image description here"" /></a></p>
<p>However, when I run it directly from the Databricks UI, the tables will not get reset and the data is available during the execution of my pipeline. Here's how it looks like:</p>
<p><a href=""https://i.stack.imgur.com/jWFsZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jWFsZ.png"" alt=""enter image description here"" /></a></p>
<p>I would like to have the same behavior in ADF, as I have when trigger the pipeline directly from the Databricks UI. I don't want to have this additional &quot;resetting tables&quot; step in my DLT pipeline when I trigger it from ADF.</p>
<p>Anyone has any solution for this?</p>
","<azure-data-factory><azure-databricks><delta-live-tables>","2023-04-04 06:43:38","58","1","1","75926567","<p>It looks like you have <code>{&quot;full_refresh&quot;: &quot;true&quot;}</code> added to your web activity parameters - with this it will always do a full refresh. To avoid it, just pass the empty object (as <code>{}</code>) instead.</p>
"
"75925921","how to copy multiple tables from azure sql to gen2 using databricks notebook in adf","<p>Is it possible to capy multiple tables from azure sql to gen2 using databricks notebook in adf</p>
<p>to copy multiple tables from azuresql to gen2 is it possible? And How?
I tried many times but isnt working, can anyone share the answer</p>
","<azure><azure-sql-database><azure-data-factory>","2023-04-04 05:56:44","50","0","1","75926962","<p>You can copy multiple tables from Azure SQl database to ADLS Gen 2 account by using copy activity in azure data factory without using Databricks notebook. You can below procedure:</p>
<p>Perform look up activity to get the tables list in database. Select sql data set and select query option and enter below query</p>
<pre><code>SELECT TABLE_SCHEMA,TABLE_NAME FROM information_schema.TABLES
WHERE TABLE_TYPE = 'BASE TABLE' and TABLE_SCHEMA = 'dbo'
</code></pre>
<p>after successful of lookup connect for each item and add <code>@activity('Lookup1').output.value</code>  as dynamic content in item tab. go to activities of foreach and select copy activity,Create two sql linked services one for lookup activity and one for source and adls gen2 linked service and create sql data set with linked service select the sql dataset as source and create two parameters named Schema and tableName and given below values:</p>
<pre><code>schema: @item().TABLE_SCHEMA
tableName: @item().TABLE_NAME
</code></pre>
<p>I passed above parameters as directory of table in copy activity source dataset as mentioned below:</p>
<pre><code>Table: @dataset.schema.@dataset.table
</code></pre>
<p>Create ADLS delimited dataset with created linked service and select the ADLS data set as data source.</p>
<p><img src=""https://i.imgur.com/2w4idwO.png"" alt=""enter image description here"" /></p>
<p>Debug the pipeline it run successfully.</p>
<p><img src=""https://i.imgur.com/mftu7lu.png"" alt=""enter image description here"" /></p>
<p>Those table save in ADLS successfully.</p>
<p><img src=""https://i.imgur.com/np5qCSp.png"" alt=""enter image description here"" /></p>
<p>Pipeline Json:</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;AzureSqlSource&quot;,
                        &quot;sqlReaderQuery&quot;: &quot;SELECT TABLE_SCHEMA,TABLE_NAME FROM information_schema.TABLES\nWHERE TABLE_TYPE = 'BASE TABLE' and TABLE_SCHEMA = 'dbo'&quot;,
                        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                        &quot;partitionOption&quot;: &quot;None&quot;
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;AzureSqlTable2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Lookup1').output.value\n&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy data1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                                    &quot;partitionOption&quot;: &quot;None&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                                        &quot;quoteAllText&quot;: true,
                                        &quot;fileExtension&quot;: &quot;.csv&quot;
                                    }
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;translator&quot;: {
                                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                                    &quot;typeConversion&quot;: true,
                                    &quot;typeConversionSettings&quot;: {
                                        &quot;allowDataTruncation&quot;: true,
                                        &quot;treatBooleanAsNumber&quot;: false
                                    }
                                }
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;Schema&quot;: {
                                            &quot;value&quot;: &quot;@item().TABLE_SCHEMA&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;tableName&quot;: {
                                            &quot;value&quot;: &quot;@item().TABLE_NAME&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
"
"75917859","How to create json object from set of items","<p>In Azure Data Factory, I have a Pipeline (made of flowlets, but that's a technicality) that is more or less the following flow:</p>
<ol>
<li><p><strong>Get a set of items</strong> from a Data Set (let's say : I get 5 cars, each car has its &quot;columns&quot; -- id, color, model, ...)</p>
</li>
<li><p>turn that set into an <strong>array</strong> : I do it with an <strong>Aggregate</strong> block which contains a <strong>&quot;collect&quot;</strong> script function.</p>
</li>
</ol>
<p>What I want :</p>
<p>I would like step 2 to create an <strong>object</strong>, not an <strong>array</strong>.</p>
<p>If this was json, this is what I would like:</p>
<pre><code>//NOT THIS
[
 { &quot;id&quot;:&quot;1&quot;, &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;red&quot; }, 
 { &quot;id&quot;:&quot;2&quot;, &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;blue&quot; }, 
]


//THIS
{
 &quot;1&quot;: { &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;red&quot; }, 
 &quot;2&quot;: { &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;blue&quot; }, 
}
</code></pre>
<p>I've tried working with the objects <strong>as strings</strong> and then using a bunch of <strong>&quot;replace&quot;</strong> to turn [ ] into { } ... but that's just too much grinding -- and more importantly there's too high a risk that I make a mistake with character escape.</p>
<p><strong>How would you turn a set of items into one object instead of an array?</strong></p>
<p><em>Note: the end goal is to later be able to work with the cars as a <strong>dictionary</strong> rather than a <strong>collection,</strong> in programming terms. I just added this for anyone who might be googling this without knowing exactly what they're looking for.</em></p>
","<azure-data-factory>","2023-04-03 09:21:55","78","0","2","75930727","<p>The question is nonsensical.
this answer is meant for anyone else to understand why.</p>
<p>Data Factory works with <strong>schemas.</strong> Each <strong>column</strong> is <strong>strongly typed.</strong></p>
<p>For example, if we imagine that I started with a Data Set based off a json file :</p>
<pre><code>[
 { &quot;id&quot;:&quot;1&quot;, &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;red&quot; }, 
 { &quot;id&quot;:&quot;2&quot;, &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;blue&quot; }, 
]
</code></pre>
<p>Then those values are immediately <strong>named</strong> and converted to <strong>known data types</strong> :  There's a column called &quot;id&quot;, another column called &quot;model&quot;, and a third column called &quot;color&quot;. They have names. There's a pattern. Each row has the same pattern.</p>
<p>The only thing that's allowed not to have a name is an <strong>array</strong> : because we know that it's a set of items that all have the same schema (i.e. column names).</p>
<p>But if you turn it into a dictionary, <strong>you break the pattern :</strong>  you now have an object whose fields (columns) have unpredictable names.</p>
<pre><code>{
 &quot;1&quot;: { &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;red&quot; }, 
 &quot;2&quot;: { &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;blue&quot; }, 
}
</code></pre>
<p>for example in the dictionary just above there's now a field (column) named &quot;1&quot;, another one named &quot;2&quot;, another one named &quot;3&quot;... There's no pattern to that object. We don't know how many fields it can have. Remember: That's not an array. That's meant to be an object. You're supposed to know how many fields an object has if you plan on typing it. It would be even worse if the Ids were Guids. Completely scrambled, unpredictable fields/column names.</p>
<p>In other words, <strong>you cannot infer a type</strong> for that json data. and therefore you cannot parse it to an object that has a <strong>schema,</strong> to feed it to your flow in Data Factory.</p>
<p>=================</p>
<p>That being said, you can still create the Dictionary <strong>as a string</strong> that you might want to <strong>parse later, outside of Data Factory</strong> (e.g. in C#, in your backend that reacts on insertions into the sink Dataset)</p>
<p>Then here is how you get it :</p>
<ol>
<li>Create a dummy column to store the new so-called json string. To do so, use the usual Data Factory technique : Create a <strong>&quot;derived column&quot;</strong> block in your flow, give a name to the new column (e.g. &quot;asStringDictionary&quot;), then click on &quot;Expression builder&quot; just under the &quot;Expression&quot; field of your dummy column.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/TRfnv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TRfnv.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li><p>In the expression builder, use an expression like this. In essence, use the &quot;reducer&quot; pattern :</p>
<p>reduce(
arrayItems,<br />
&quot;&quot; ,
#acc + toString(#item) + &quot;,&quot;,
concat( char(123), #result ,  char(125) )
)</p>
</li>
</ol>
<p>Of course, replace &quot;arrayItems&quot; with the name of your own set of items that you got from the flow brick just before the derived Column brick you just added.</p>
<p>Please note that in my case, the following transformation had already been performed in a previous brick :</p>
<pre><code>from
{ &quot;id&quot; : &quot;1&quot;, ... ALL OTHER FIELDS ... }
to
{  &quot;1&quot;: { ... ALL OTHER FIELDS ... }  }
</code></pre>
<p>I did it in yet another derived column, this time with an expression like :</p>
<pre><code>associate(id, @())
</code></pre>
"
"75917859","How to create json object from set of items","<p>In Azure Data Factory, I have a Pipeline (made of flowlets, but that's a technicality) that is more or less the following flow:</p>
<ol>
<li><p><strong>Get a set of items</strong> from a Data Set (let's say : I get 5 cars, each car has its &quot;columns&quot; -- id, color, model, ...)</p>
</li>
<li><p>turn that set into an <strong>array</strong> : I do it with an <strong>Aggregate</strong> block which contains a <strong>&quot;collect&quot;</strong> script function.</p>
</li>
</ol>
<p>What I want :</p>
<p>I would like step 2 to create an <strong>object</strong>, not an <strong>array</strong>.</p>
<p>If this was json, this is what I would like:</p>
<pre><code>//NOT THIS
[
 { &quot;id&quot;:&quot;1&quot;, &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;red&quot; }, 
 { &quot;id&quot;:&quot;2&quot;, &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;blue&quot; }, 
]


//THIS
{
 &quot;1&quot;: { &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;red&quot; }, 
 &quot;2&quot;: { &quot;model&quot;:&quot;&quot;, &quot;color&quot;:&quot;blue&quot; }, 
}
</code></pre>
<p>I've tried working with the objects <strong>as strings</strong> and then using a bunch of <strong>&quot;replace&quot;</strong> to turn [ ] into { } ... but that's just too much grinding -- and more importantly there's too high a risk that I make a mistake with character escape.</p>
<p><strong>How would you turn a set of items into one object instead of an array?</strong></p>
<p><em>Note: the end goal is to later be able to work with the cars as a <strong>dictionary</strong> rather than a <strong>collection,</strong> in programming terms. I just added this for anyone who might be googling this without knowing exactly what they're looking for.</em></p>
","<azure-data-factory>","2023-04-03 09:21:55","78","0","2","75983431","<p>The other answer says that the question is nonsensical. But it's not entirely true.</p>
<p>If your goal is <em>really</em> to create a dictionary, which is to say that you really want to have <strong>one column per Id, i.e. one output column per input row</strong>  then you can investigate the following Azure Data Factory field : <strong>&quot;rows to columns&quot;, aka pivot.</strong> for example : <a href=""https://stackoverflow.com/questions/64390248/rows-to-columns-in-adf"">Rows to Columns in ADF</a></p>
<p><strong>Pay close attention to the explanations on &quot;Allow schema drift&quot;</strong> to manage the evolution of the schema as the columns get transformed.</p>
<p>Note : of course you want to be sure that there will be a reasonably low number of input Ids -- you don't want to create one giant dictionary. I don't know how ADF would react to that, performance-wise.</p>
<p>Note: In this answer I'm discussing only the <strong>Ids</strong> transformation, but of course there's still the matter of turning the other fields/columns into the dictionary object meant to match the key. That would be done the same way as in the <em>other</em> answer (look at the part dealing with &quot;ALL OTHER FIELDS&quot;).</p>
"
"75916093","Azure Synapse Performance optimization","<p>I have a couple of tables with clustered column store index in synapse with hash partition and a view was written on those tables with multiple cte. Since we dont have indexes like clustered index and non clustered index as we used to have in sql which will operate on row level , how can we improve the performance of this view.</p>
<p>Count of this view is 7 million after handling all the filters perfectly and is taking longer time to display the query result.</p>
<p>Any Suggestions on performance optimization techniques in synapse where we dont have pk and fk constraints and row level indexes to work ?</p>
","<sql><database><azure-data-factory><query-optimization><azure-synapse>","2023-04-03 04:42:58","31","0","1","75916134","<p>Since you are using Synapse with clustered column store index, try:</p>
<ul>
<li>Data Compression -- reduce the size of the data on disk</li>
<li>Columnstore Index Organized Tables -- can be used to physically cluster data on disk</li>
<li>Partitioning -- you are already using hash partitioning, try to optimize the partitioning scheme to align with the query patterns of your view</li>
<li>Materialized Views -- precompute the results of your view and store them in a table</li>
<li>Query Tuning -- analyze the query execution plan and identify areas for improvement(indexing, query rewriting, using hints to influence the optimizer)</li>
</ul>
<p>ps. you may need to try multiple techniques to find the optimal solution for your specific use case</p>
"
"75914389","How do I combine static values with csv source data in Azure Data Factory","<p>Up until recently, I've been using Jitterbit to upsert data into Salesforce. I generate an external ID through the upsert by mapping a column from the CSV source called <em>license_no</em> to a field in Salesforce called <em>License_External_ID__c</em>. From there I'm able to easily append whatever static strings I want to the source value. I'm not sure how to make this happen in ADF.</p>
<p><a href=""https://i.stack.imgur.com/rbdHG.png"" rel=""nofollow noreferrer"">Jitterbit Concatenation</a></p>
<p>Attached are screenshots of what I have set up in ADF so far and the equivalent mapping in Jitterbit. To avoid confusion, I'll note that <em>license_no</em> is also being directly mapped to a field in SF called <em>License_Number__c</em>, but again, I want to take the same value and transform it into an external ID. In other words, taking a license number that equals <em>123456</em> and transforming it into <em>Dentist_MD_123456</em> as shown in the first screenshot.</p>
<p><a href=""https://i.stack.imgur.com/DkJy8.png"" rel=""nofollow noreferrer"">Source</a></p>
<p><a href=""https://i.stack.imgur.com/ANlGX.png"" rel=""nofollow noreferrer"">Sink</a></p>
<p><a href=""https://i.stack.imgur.com/cMALT.png"" rel=""nofollow noreferrer"">Mapping</a></p>
<p><a href=""https://i.stack.imgur.com/xRNMV.png"" rel=""nofollow noreferrer"">Jitterbit Mapping</a></p>
","<salesforce><azure-data-factory><jitterbit>","2023-04-02 20:13:19","56","0","1","75927064","<ul>
<li>To manipulate data in azure data factory, you can use dataflows. The following is the demonstration about the same.</li>
<li>Let's say I have the following data in my source csv:</li>
</ul>
<p><img src=""https://i.imgur.com/PxvX10w.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now to add a new column (or change value of an existing column), you can use derived column transformation. I have a parameter called <code>license_no</code> (static value) and created a new column as shown below using the expression <code>concat('Dentist_MD_',$license_no)</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/pJorwhb.png"" alt=""enter image description here"" /></p>
<ul>
<li>The result would be as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/CH4BRfF.png"" alt=""enter image description here"" /></p>
<ul>
<li>For any data manipulation activities, you can use the transformations provided by azure dataflows. You can configure your mapping in the sink section same as in copy data activity.</li>
</ul>
"
"75911643","Converting azure data factory source code to terraform script","<p>I have the below azure sourcedataset and sinkdataset source code, I am trying to write a terraform script to create the datasets. I am able to create a basic dataset, but if I try to achieve the same with parameters, I am getting errors to run the terraform script.</p>
<p>I am using azurerm version = &quot;3.50.0&quot;</p>
<p><strong>SinkDataset</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;SinkAllParquetDataset_withTableName&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AzureStorageLinkedService&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;parameters&quot;: {
            &quot;TableName&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;FileName&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;SchemaName&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;Parquet&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobStorageLocation&quot;,
                &quot;fileName&quot;: {
                    &quot;value&quot;: &quot;@concat(dataset().FileName,formatDatetime(utcnow(),'_ddMMyyyy_hhmmss'),'.parquet')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;folderPath&quot;: {
                    &quot;value&quot;: &quot;@concat(dataset().SchemaName,'/',dataset().FileName,'/',formatDatetime(utcnow(),'yyyy'),'/',formatDatetime(utcnow(),'MM'),'/',formatDatetime(utcnow(),'dd'),'/')\n&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;container&quot;: &quot;latest-parquet-container&quot;
            },
            &quot;compressionCodec&quot;: &quot;snappy&quot;
        },
        &quot;schema&quot;: []
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p><strong>SourceDatset</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;All_Tables_SourceDataset&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AzureSqlDatabaseLinkedService&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureSqlTable&quot;,
        &quot;schema&quot;: []
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p><strong>Terrafrom Script</strong></p>
<pre><code>resource &quot;azurerm_data_factory_dataset_sql_server_table&quot; &quot;source&quot; {
  name       = &quot;All_Tables_SourceDataset&quot;
  folder = &quot;datasets&quot;
  
  definition = jsonencode({
    name = &quot;All_Tables_SourceDataset&quot;
    properties = {
      linkedServiceName = {
        referenceName = &quot;AzureSqlDatabaseLinkedService&quot;
        type = &quot;LinkedServiceReference&quot;
      }
      annotations = []
      type = &quot;AzureSqlTable&quot;
      schema = []
    }
    type = &quot;Microsoft.DataFactory/factories/datasets&quot;
  })
}

resource &quot;azurerm_data_factory_dataset_azure_blob&quot; &quot;sink&quot; {
  name       = &quot;SinkAllParquetDataset_withTableName&quot;
  folder = &quot;datasets&quot;
  definition = jsonencode({
    name = &quot;SinkAllParquetDataset_withTableName&quot;
    properties = {
      linkedServiceName = {
        referenceName = &quot;AzureStorageLinkedService&quot;
        type = &quot;LinkedServiceReference&quot;
      }
      parameters = {
        TableName = {
          type = &quot;string&quot;
        }
        FileName = {
          type = &quot;string&quot;
        }
        SchemaName = {
          type = &quot;string&quot;
        }
      }
      annotations = []
      type = &quot;Parquet&quot;
      typeProperties = {
        location = {
          type = &quot;AzureBlobStorageLocation&quot;
          fileName = {
            value = &quot;@concat(dataset().FileName,formatDatetime(utcnow(),'_ddMMyyyy_hhmmss'),'.parquet')&quot;
            type = &quot;Expression&quot;
          }
          folderPath = {
            value = &quot;@concat(dataset().SchemaName,'/',dataset().FileName,'/',formatDatetime(utcnow(),'yyyy'),'/',formatDatetime(utcnow(),'MM'),'/',formatDatetime(utcnow(),'dd'),'/')&quot;
            type = &quot;Expression&quot;
          }
          container = azurerm_storage_container.example.name
        }
        compressionCodec = &quot;snappy&quot;
      }
      schema = []
    }
    type = &quot;Microsoft.DataFactory/factories/datasets&quot;
  })
}
</code></pre>
","<terraform><azure-data-factory><terraform-provider-azure><azure-rm>","2023-04-02 11:09:22","108","0","1","75949804","<p>Check the following:</p>
<p><em><strong>In this case create source and sink linked services to get the connection strings to refer in ARM template.</strong></em></p>
<p><em>ex: &quot;${azurerm_data_factory_linked_service_azure_sql_database.example.connection_string}</em>&quot;
,
&quot;<em>${azurerm_data_factory_linked_service_azure_sql_database.example.name}</em>&quot;</p>
<p>source here in this case is blob and sink is sql database</p>
<pre><code>resource &quot;azurerm_storage_account&quot; &quot;stdg&quot; {
  name                     = &quot;storageaccountfdg&quot;
  location            = data.azurerm_resource_group.example.location
  resource_group_name = data.azurerm_resource_group.example.name
  account_tier             = &quot;Standard&quot;
  account_replication_type = &quot;LRS&quot;
}


resource &quot;azurerm_data_factory&quot; &quot;example&quot; {
  name                = &quot;karepexample&quot;
  location            = data.azurerm_resource_group.example.location
  resource_group_name = data.azurerm_resource_group.example.name
}


resource &quot;azurerm_data_factory_linked_service_azure_blob_storage&quot; &quot;example&quot; {
  name              = &quot;neexamplels&quot;
  data_factory_id   = azurerm_data_factory.example.id
  connection_string = azurerm_storage_account.stdg.primary_connection_string
}
</code></pre>
<p><a href=""https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/data_factory_linked_service_azure_blob_storage"" rel=""nofollow noreferrer"">data_factory_linked_service_azure_blob_storage | terraform registry</a></p>
<pre><code>resource &quot;azurerm_data_factory_dataset_azure_blob&quot; &quot;example&quot; {
  name                = &quot;exampledsb&quot;
  data_factory_id     = azurerm_data_factory.example.id
  linked_service_name = azurerm_data_factory_linked_service_azure_blob_storage.example.name

  path     = &quot;foo&quot;
  filename = &quot;bar.png&quot;
}
resource &quot;azurerm_data_factory_linked_service_azure_sql_database&quot; &quot;example&quot; {
  name              = &quot;example&quot;
  data_factory_id   = azurerm_data_factory.example.id
  connection_string = &quot;data source=serverhostname;initial catalog=master;user id=testUser;Password=test;integrated security=False;encrypt=True;connection timeout=30&quot;
}
</code></pre>
<p>See <a href=""https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/data_factory_linked_service_azure_sql_database"" rel=""nofollow noreferrer"">data_factory_linked_service_azure_sql_database | terraform registry</a></p>
<p>Then these have to be refered in the below code:
Below code can be checked where Arm template can be deployed using terraform script .</p>
<p>It also includes parameters.</p>
<p><strong>Code:</strong></p>
<pre><code>resource &quot;azurerm_resource_group_template_deployment&quot; &quot;example&quot; {
  name                = &quot;kaadfdeployment&quot;
  resource_group_name = data.azurerm_resource_group.example.name
  deployment_mode     = &quot;Incremental&quot;
    
  template_content =  &lt;&lt;TEMPLATE
{ 

    &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;,
    &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
    &quot;parameters&quot;: {
        &quot;dataFactoryName&quot;: {
            &quot;type&quot;: &quot;String&quot;,
              &quot;defaultValue&quot;: &quot;newdfforrepo&quot;,
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;Name of the data factory. Must be globally unique.&quot;
            }
        },
        &quot;dataFactoryLocation&quot;: {
            &quot;defaultValue&quot;: &quot;East US&quot;,
            &quot;allowedValues&quot;: [
                &quot;East US&quot;,
                &quot;East US 2&quot;,
                &quot;West Europe&quot;,
                &quot;Southeast Asia&quot;
            ],
            &quot;type&quot;: &quot;String&quot;,
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;Location of the data factory.&quot;
            }
        },
        &quot;AzureStorage_connectionString&quot;: {
            &quot;type&quot;: &quot;SecureString&quot;,
              &quot;defaultValue&quot;: &quot;${azurerm_storage_account.stdg.primary_connection_string}&quot;,
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;Connection string for the Azure Storage account.&quot;
            }
        },
        &quot;blobContainer&quot;: {
            &quot;type&quot;: &quot;String&quot;,
              &quot;defaultValue&quot;: &quot;newbbcontr&quot;,
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;Name of the blob container in the Azure Storage account.&quot;
            }
        },
        &quot;inputBlobFolder&quot;: {
            &quot;type&quot;: &quot;String&quot;,
              &quot;defaultValue&quot;: &quot;newbbfldr&quot;,
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;The folder in the blob container that has the input file.&quot;
            }
        },
        &quot;inputBlobName&quot;: {
            &quot;type&quot;: &quot;String&quot;,
              &quot;defaultValue&quot;: &quot;newinpblbr&quot;,
              
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;Name of the input file/blob.&quot;
            }
        },
        &quot;AzureSqlDatabase_connectionString&quot;: {
            &quot;type&quot;: &quot;String&quot;,
              &quot;defaultValue&quot;: &quot;&lt;give sql db connection string here&gt;&quot;,
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;Connnection string for the Azure SQL database.&quot;
            }
        },
        &quot;sqlTableName&quot;: {
            &quot;type&quot;: &quot;String&quot;,
              &quot;defaultValue&quot;: &quot;mynewinptble&quot;,
            &quot;metadata&quot;: {
                &quot;description&quot;: &quot;Name of the target table in the Azure SQL database.&quot;
            }
        }
    },
    &quot;variables&quot;: {
        &quot;azureStorageLinkedServiceName&quot;: &quot;azstrls&quot;,
        &quot;azureSqlDatabaseLinkedServiceName&quot;: &quot;AzSqlDbLinkedService&quot;,
        &quot;inputDatasetName&quot;: &quot;rInputBlobDataset&quot;,
        &quot;outputDatasetName&quot;: &quot;rOutputSqlDataset&quot;,
        &quot;pipelineName&quot;: &quot;rCopyFromBlobToSqlPipeline&quot;
    },
    &quot;resources&quot;: [
        {
            &quot;type&quot;: &quot;Microsoft.DataFactory/factories&quot;,
            &quot;apiVersion&quot;: &quot;2017-09-01-preview&quot;,
            &quot;name&quot;: &quot;[parameters('dataFactoryName')]&quot;,
            &quot;location&quot;: &quot;[parameters('dataFactoryLocation')]&quot;,
            &quot;properties&quot;: {},
            &quot;resources&quot;: [
                {
                    &quot;type&quot;: &quot;linkedservices&quot;,
                    &quot;apiVersion&quot;: &quot;2017-09-01-preview&quot;,
                    &quot;name&quot;: &quot;[variables('azureStorageLinkedServiceName')]&quot;,
                    &quot;dependsOn&quot;: [
                        &quot;[parameters('dataFactoryName')]&quot;
                    ],
                    &quot;properties&quot;: {
                        &quot;type&quot;: &quot;AzureStorage&quot;,
                        &quot;description&quot;: &quot;Azure Storage linked service&quot;,
                        &quot;typeProperties&quot;: {
                            &quot;connectionString&quot;: {
                                &quot;value&quot;: &quot;[parameters('AzureStorage_connectionString')]&quot;,
                                &quot;type&quot;: &quot;SecureString&quot;
                            }
                        }
                    }
                },
                {
                    &quot;type&quot;: &quot;linkedservices&quot;,
                    &quot;apiVersion&quot;: &quot;2017-09-01-preview&quot;,
                    &quot;name&quot;: &quot;[variables('azureSqlDatabaseLinkedServiceName')]&quot;,
                    &quot;dependsOn&quot;: [
                        &quot;[parameters('dataFactoryName')]&quot;
                    ],
                    &quot;properties&quot;: {
                        &quot;type&quot;: &quot;AzureSqlDatabase&quot;,
                        &quot;description&quot;: &quot;Azure SQL Database linked service&quot;,
                        &quot;typeProperties&quot;: {
                            &quot;connectionString&quot;: {
                                &quot;value&quot;: &quot;[parameters('AzureSqlDatabase_connectionString')]&quot;,
                                &quot;type&quot;: &quot;SecureString&quot;
                            }
                        }
                    }
                },
                {
                    &quot;type&quot;: &quot;datasets&quot;,
                    &quot;apiVersion&quot;: &quot;2017-09-01-preview&quot;,
                    &quot;name&quot;: &quot;[variables('inputDatasetName')]&quot;,
                    &quot;dependsOn&quot;: [
                        &quot;[parameters('dataFactoryName')]&quot;,
                        &quot;[variables('azureStorageLinkedServiceName')]&quot;
                    ],
                    &quot;properties&quot;: {
                        &quot;type&quot;: &quot;AzureBlob&quot;,
                        &quot;structure&quot;: [
                            {
                                &quot;name&quot;: &quot;Prop_0&quot;,
                                &quot;type&quot;: &quot;string&quot;
                            },
                            {
                                &quot;name&quot;: &quot;Prop_1&quot;,
                                &quot;type&quot;: &quot;string&quot;
                            }
                        ],
                        &quot;typeProperties&quot;: {
                            &quot;format&quot;: {
                                &quot;type&quot;: &quot;TextFormat&quot;,
                                &quot;columnDelimiter&quot;: &quot;,&quot;,
                                &quot;nullValue&quot;: &quot;\\N&quot;,
                                &quot;treatEmptyAsNull&quot;: false,
                                &quot;firstRowAsHeader&quot;: false
                            },
                            &quot;folderPath&quot;: &quot;[concat(parameters('blobContainer'), '/', parameters('inputBlobFolder'), '/')]&quot;,
                            &quot;fileName&quot;: &quot;[parameters('inputBlobName')]&quot;
                        },
                        &quot;linkedServiceName&quot;: {
                            &quot;referenceName&quot;: &quot;[variables('azureStorageLinkedServiceName')]&quot;,
                            &quot;type&quot;: &quot;LinkedServiceReference&quot;
                        }
                    }
                },
                {
                    &quot;type&quot;: &quot;datasets&quot;,
                    &quot;apiVersion&quot;: &quot;2017-09-01-preview&quot;,
                    &quot;name&quot;: &quot;[variables('outputDatasetName')]&quot;,
                    &quot;dependsOn&quot;: [
                        &quot;[parameters('dataFactoryName')]&quot;,
                        &quot;[variables('azureSqlDatabaseLinkedServiceName')]&quot;
                    ],
                    &quot;properties&quot;: {
                        &quot;type&quot;: &quot;AzureSqlTable&quot;,
                        &quot;structure&quot;: [
                            {
                                &quot;name&quot;: &quot;FirstName&quot;,
                                &quot;type&quot;: &quot;string&quot;
                            },
                            {
                                &quot;name&quot;: &quot;LastName&quot;,
                                &quot;type&quot;: &quot;string&quot;
                            }
                        ],
                        &quot;typeProperties&quot;: {
                            &quot;tableName&quot;: &quot;[parameters('sqlTableName')]&quot;
                        },
                        &quot;linkedServiceName&quot;: {
                            &quot;referenceName&quot;: &quot;[variables('azureSqlDatabaseLinkedServiceName')]&quot;,
                            &quot;type&quot;: &quot;LinkedServiceReference&quot;
                        }
                    }
                },
                {
                    &quot;type&quot;: &quot;pipelines&quot;,
                    &quot;apiVersion&quot;: &quot;2017-09-01-preview&quot;,
                    &quot;name&quot;: &quot;[variables('pipelineName')]&quot;,
                    &quot;dependsOn&quot;: [
                        &quot;[parameters('dataFactoryName')]&quot;,
                        &quot;[variables('inputDatasetName')]&quot;,
                        &quot;[variables('outputDatasetName')]&quot;
                    ],
                    &quot;properties&quot;: {
                        &quot;activities&quot;: [
                            {
                                &quot;type&quot;: &quot;Copy&quot;,
                                &quot;typeProperties&quot;: {
                                    &quot;source&quot;: {
                                        &quot;type&quot;: &quot;BlobSource&quot;,
                                        &quot;recursive&quot;: true
                                    },
                                    &quot;sink&quot;: {
                                        &quot;type&quot;: &quot;SqlSink&quot;,
                                        &quot;writeBatchSize&quot;: 10000
                                    }
                                },
                                &quot;name&quot;: &quot;MyCopyActivity&quot;,
                                &quot;inputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;[variables('inputDatasetName')]&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;
                                    }
                                ],
                                &quot;outputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;[variables('outputDatasetName')]&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;
                                    }
                                ]
                            }
                        ]
                    }
                }
            ]
        }
    ]
}
 
TEMPLATE

}
</code></pre>
<p><a href=""https://i.stack.imgur.com/YR778.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YR778.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/j65NF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j65NF.png"" alt=""enter image description here"" /></a></p>
<p>Source</p>
<p><a href=""https://i.stack.imgur.com/XCcxc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XCcxc.png"" alt=""enter image description here"" /></a></p>
<p>Sink</p>
<p><a href=""https://i.stack.imgur.com/fYSXL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fYSXL.png"" alt=""enter image description here"" /></a>
<strong>Reference :</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-resource-manager-template"" rel=""nofollow noreferrer"">Create an Azure Data Factory using ARM template | Microsoft learn</a></p>
"
"75908433","how to delete folders in Azure Gen 2 data lake using ADF","<p>I have hard time to delete the folders in the data lake from adf pipeline. I tred similar method as I did for files by using <code>get metadata</code> activity and <code>delete</code> activity inside <code>ForEach</code> activity, could not work it out.</p>
<p>So eveyday I write a snapshot data and file path looks like this -&gt; snapshot/tablename/ date when the file was offloaded -which is basiclayy  GETUTCDATE with this format <code>yyyyMMddHHmmss</code>.</p>
<p>so path = <code>snapshot/tablename/yyyyMMddHHmmss/*.parquet</code></p>
<p>My goal is to delete all the folders in the <code>tablename</code> folder which is older than 5 days.</p>
<p>any tips appreciated.</p>
","<azure><azure-data-factory>","2023-04-01 19:34:21","90","0","2","75918063","<p>You can try following approach:</p>
<ol>
<li>Add a pipeline parameter 'Iteration' with int datatype having value as '-5'</li>
<li>Add foreach activity with item expression as <code>@createArray(1,2,3,4,5)</code></li>
<li>Inside foreach, add delete activity with wildcard file path <code>@concat(addDays(utcNow(),add(pipeline().parameters.Iteration,item()),'yyyyMMdd*'),'*.parquet')</code></li>
</ol>
<p>You can tweak the expression according to the filepath if needed.</p>
<p><a href=""https://i.stack.imgur.com/khEgA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/khEgA.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/gOnVj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gOnVj.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/A8zAM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A8zAM.png"" alt=""enter image description here"" /></a></p>
"
"75908433","how to delete folders in Azure Gen 2 data lake using ADF","<p>I have hard time to delete the folders in the data lake from adf pipeline. I tred similar method as I did for files by using <code>get metadata</code> activity and <code>delete</code> activity inside <code>ForEach</code> activity, could not work it out.</p>
<p>So eveyday I write a snapshot data and file path looks like this -&gt; snapshot/tablename/ date when the file was offloaded -which is basiclayy  GETUTCDATE with this format <code>yyyyMMddHHmmss</code>.</p>
<p>so path = <code>snapshot/tablename/yyyyMMddHHmmss/*.parquet</code></p>
<p>My goal is to delete all the folders in the <code>tablename</code> folder which is older than 5 days.</p>
<p>any tips appreciated.</p>
","<azure><azure-data-factory>","2023-04-01 19:34:21","90","0","2","75920687","<blockquote>
<p>My goal is to delete all the folders in the <code>tablename</code> folder which is older than 5 days.</p>
</blockquote>
<p>You can achieve your requirement by using Filter activity like below.</p>
<p>First create an integer parameter with <code>-(n-1)</code> value where n is the number of days.
Then create a last nth day in <code>yyyyMMdd</code> format in a set variable using the below expression.</p>
<p><code>@addDays(utcnow(),pipeline().parameters.ndays,'yyyyMMdd')</code></p>
<p><img src=""https://i.imgur.com/Orr8IPy.png"" alt=""enter image description here"" /></p>
<p>Then use the Get Meta data activity to get the folders list.</p>
<p><strong>These are my folders:</strong></p>
<p><img src=""https://i.imgur.com/i4g3vGa.png"" alt=""enter image description here"" /></p>
<p>Now, give this Child Items array to Filter activity. Filter activity filters the folder names which are greater than 5 days old.</p>
<p><strong>Filter items:</strong> <code>@activity('Get Metadata1').output.childItems</code></p>
<p><strong>Filter condition:</strong> <code>@greater(int(variables('last_5thday')), int(substring(item().name,0,8)))</code></p>
<p>Filter will give the output array like this.</p>
<p><img src=""https://i.imgur.com/0ybckWd.png"" alt=""enter image description here"" /></p>
<p>Give this array to ForEach as <code>@activity('Filter1').output.Value</code> and inside ForEach use the delete activity.</p>
<p>For delete activity, use the dataset parameter for the folder name like below.</p>
<p><img src=""https://i.imgur.com/iPTf7Vx.png"" alt=""enter image description here"" /></p>
<p>Give <code>@item().name</code> as value to the parameter in the delete activity inside ForEach.</p>
<p><img src=""https://i.imgur.com/w1qNzbW.png"" alt=""enter image description here"" /></p>
<p>You can see folders which are greater than 5 days were deleted after Pipeline Execution.</p>
<p><img src=""https://i.imgur.com/IOUicp7.png"" alt=""enter image description here"" /></p>
<p><strong>This is my Pipeline JSON for your reference:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;Pipeline 1&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Get Metadata1&quot;,
            &quot;type&quot;: &quot;GetMetadata&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;source_folder&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;fieldList&quot;: [
                    &quot;childItems&quot;
                ],
                &quot;storeSettings&quot;: {
                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                    &quot;enablePartitionDiscovery&quot;: false
                },
                &quot;formatSettings&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Set variable1&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;last_5thday&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@addDays(utcnow(),pipeline().parameters.ndays,'yyyyMMdd')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Filter1&quot;,
            &quot;type&quot;: &quot;Filter&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Get Metadata1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Get Metadata1').output.childItems&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;condition&quot;: {
                    &quot;value&quot;: &quot;@greater(int(variables('last_5thday')), int(substring(item().name,0,8)))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Filter1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Filter1').output.Value&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Delete1&quot;,
                        &quot;type&quot;: &quot;Delete&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;dataset&quot;: {
                                &quot;referenceName&quot;: &quot;Binary1&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                &quot;parameters&quot;: {
                                    &quot;folder&quot;: {
                                        &quot;value&quot;: &quot;@item().name&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            },
                            &quot;enableLogging&quot;: false,
                            &quot;storeSettings&quot;: {
                                &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                &quot;recursive&quot;: true,
                                &quot;enablePartitionDiscovery&quot;: false
                            }
                        }
                    }
                ]
            }
        }
    ],
    &quot;parameters&quot;: {
        &quot;ndays&quot;: {
            &quot;type&quot;: &quot;int&quot;,
            &quot;defaultValue&quot;: -4
        }
    },
    &quot;variables&quot;: {
        &quot;last_5thday&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;test&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
"
"75897358","Customer Managed Key in Azure Data Factory","<p>I am creating a Azure Data Factory with customer managed key using Terraform as below:</p>
<pre><code>resource &quot;azurerm_resource_group&quot; &quot;example&quot; {
  name     = &quot;example-resources&quot;
  location = &quot;West Europe&quot;
}

resource &quot;azurerm_data_factory&quot; &quot;example&quot; {
  name                = &quot;example&quot;
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name
  customer_managed_key_id = var.cmkID 
  customer_managed_key_identity_id = var.IdentityID
  
}
</code></pre>
<p>I have already created a PrimaryEncryptionKey and added to Key vault Keys. And passed those values in tfvars file.
Terraform plan looks fine, When Terraform plan is applied it throws error</p>
<p><strong>Operation failed. Data factory Managed Identity does not have access to customer managed Key vault</strong></p>
<p>Since the data factory is not created yet, I dont have a identity of data factory to be added to Key vault access policy. So I removed the customer managed key variables from terraform code and created a simple data factory.</p>
<pre><code>resource &quot;azurerm_resource_group&quot; &quot;example&quot; {
  name     = &quot;example-resources&quot;
  location = &quot;West Europe&quot;
}

resource &quot;azurerm_data_factory&quot; &quot;example&quot; {
  name                = &quot;example&quot;
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name
}
</code></pre>
<p>This went fine and I was able to add the object ID of data factory along with Identity App id in key vault access policies.
After this I again ran the first code with customer managed key information. This time I got a new error as below:</p>
<p><strong>Updatefactory failed.You cannot add CMK settings for factories with existing entities.</strong></p>
<p>I have tried removing the integration run time that is created by default( which got created along with sample data factory) but in vain.</p>
<p>This looks like a deadlock situation and I am not sure if I am missing any important information here.</p>
","<azure><terraform><azure-data-factory><terraform-provider-azure>","2023-03-31 10:15:02","80","0","1","75917820","<p>I tried to create Azure data factory with CMK assigned:
But received error:</p>
<pre><code>│ Error: creating/updating Data Factory: (Factory Name &quot;kaaexample&quot; / Resource Group &quot;xxx&quot;): datafactory.FactoriesClient#CreateOrUpdate: Failure responding to 
request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=&quot;CMKAccessDeniedByCallerNotAuthorized&quot; Message=&quot;Operation failed. Data Factory Managed Identity doesn't have access to customer managed key vault.&quot;
│
</code></pre>
<p><a href=""https://i.stack.imgur.com/o8l1s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o8l1s.png"" alt=""enter image description here"" /></a></p>
<p><em><code>Make sure  to Enable Soft Delete and Do Not Purge on Azure Key Vault</code></em></p>
<p>Code:</p>
<pre><code>resource &quot;azurerm_user_assigned_identity&quot; &quot;this&quot; {
  name = &quot;example-user-id&quot;
resource_group_name = data.azurerm_resource_group.example.name
location = data.azurerm_resource_group.example.location
}

resource &quot;azurerm_data_factory&quot; &quot;example&quot; {
  name                = &quot;kaaexample&quot;
  location            = data.azurerm_resource_group.example.location
  resource_group_name = data.azurerm_resource_group.example.name

  identity {
    type         = &quot;UserAssigned&quot;
    identity_ids = [azurerm_user_assigned_identity.this.id]
  } 
}


resource &quot;azurerm_key_vault&quot; &quot;example&quot; {
  name                = &quot;cmkkaakeyvault&quot;
  location            = data.azurerm_resource_group.example.location
  resource_group_name = data.azurerm_resource_group.example.name
 tenant_id                   = data.azurerm_client_config.current.tenant_id
// tenant_id           = data.azuread_client_config.current.tenant_id
 
   purge_protection_enabled    = true
   soft_delete_retention_days = 7

  sku_name = &quot;standard&quot;
}


Note:
Dedicated  access policy is needed for the client if no  role assignmentis present .GetRotationPolicy is mandatory whether you actively use it or not. 

The client should have RBAC roles like Key Vault Crypto Officer or Key Vault Administrator or an assigned Key Vault Access Policy with permissions Create,Delete,Get,Purge,Recover,Update and GetRotationPolicy for keys without Rotation Policy.
</code></pre>
<p><a href=""https://i.stack.imgur.com/9xevM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9xevM.png"" alt=""enter image description here"" /></a></p>
<pre><code>resource &quot;azurerm_key_vault_access_policy&quot; &quot;example&quot; {
  key_vault_id = azurerm_key_vault.example.id

  tenant_id = data.azurerm_client_config.current.tenant_id
object_id = data.azurerm_client_config.current.object_id
//    object_id = data.azurerm_client_config.current.object_id  

  key_permissions = [
 &quot;Backup&quot;, &quot;Decrypt&quot;, 
  &quot;Encrypt&quot;,  &quot;Import&quot;, &quot;List&quot;, &quot;Purge&quot;, &quot;Recover&quot;, &quot;Restore&quot;, &quot;Sign&quot;, &quot;UnwrapKey&quot;, &quot;Update&quot;, 
  &quot;Verify&quot;, &quot;WrapKey&quot;, &quot;Release&quot;, &quot;Rotate&quot;, &quot;GetRotationPolicy&quot;, &quot;SetRotationPolicy&quot;,
    &quot;Create&quot;, &quot;Delete&quot;, &quot;Get&quot;

  ]
}
</code></pre>
<blockquote>
<p><strong>Note:</strong> <strong>Create ADF without any entities i.e;  Data flow or linked services initially and assigned the user assigned identity.</strong></p>
</blockquote>
<p>After the above code is executed and ADF is created without CMK .</p>
<p><strong>Then create ADF with Custom managed key:</strong></p>
<ul>
<li>Make sure the ADf managed identity has proper role to access keyvault keys or access policies like &quot;unwrapKey&quot;,   &quot;wrapKey&quot;, &quot;Rotate&quot;, &quot;GetRotationPolicy&quot;, &quot;SetRotationPolicy&quot;,   &quot;Create&quot;, &quot;Delete&quot;, &quot;Get&quot;</li>
</ul>
<p><strong>Code:</strong></p>
<pre><code>resource &quot;azurerm_key_vault_key&quot; &quot;example&quot; {
  name         = &quot;cmkexamplekey&quot;
  key_vault_id = azurerm_key_vault.example.id
  key_type     = &quot;RSA&quot;
  key_size     = 4096
  

  key_opts = [
    &quot;decrypt&quot;,
    &quot;encrypt&quot;,
    &quot;sign&quot;,
    &quot;unwrapKey&quot;,
    &quot;verify&quot;,
    &quot;wrapKey&quot;,
  ]

   depends_on = [ 
    azurerm_key_vault_access_policy.example
   ]
  
}


output &quot;key&quot; {
  value = azurerm_key_vault_key.example.version
  
}


resource &quot;azurerm_data_factory&quot; &quot;example&quot; {
  name                = &quot;kaaexample&quot;
  location            = data.azurerm_resource_group.example.location
  resource_group_name = data.azurerm_resource_group.example.name
    customer_managed_key_id  = azurerm_key_vault_key.example.id
    customer_managed_key_identity_id = azurerm_user_assigned_identity.this.id
    

  identity {
    type         = &quot;UserAssigned&quot;
    identity_ids = [azurerm_user_assigned_identity.this.id]
  
}
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/ex60o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ex60o.png"" alt=""enter image description here"" /></a></p>
<p>ADF :</p>
<p><a href=""https://i.stack.imgur.com/h7Pnz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h7Pnz.png"" alt=""enter image description here"" /></a></p>
<p><strong>Reference :</strong> <a href=""https://medium.com/twodigits/azure-terraform-add-customer-managed-key-to-git-managed-data-factory-via-terraform-4f7c56250b98"" rel=""nofollow noreferrer"">Add Customer-managed Key to Git-managed Data Factory via Terraform | by Gerrit Stapper </a></p>
"
"75896640","How to test Azure Data Factory Managed Airflow from postman","<p>I have an Airflow instance in Azure Data Factory.
I can test the APIs from swagger.
But when I try to test from postman, it's showing authentication error.
<a href=""https://i.stack.imgur.com/tgP2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tgP2Z.png"" alt=""enter image description here"" /></a></p>
<p>Tried basic-auth with my azure credentials, but did not work.
How do I authenticate Azure Data Factory Managed Airflow from postman?</p>
","<azure><airflow><azure-data-factory>","2023-03-31 08:59:22","105","0","2","75906736","<p>As, Azure ADF Airflow feature is still in preview, There’s no direct Rest API to call managed airflow service in ADF refer below:-<br />
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-does-managed-airflow-work"" rel=""nofollow noreferrer"">How does Managed Airflow work? - Azure Data Factory | Microsoft Learn</a> <em><strong>In order for API to work you need to select Basic auth by entering username and password while creating Airflow with ADF.</strong></em></p>
<p>When I used AAD as an authentication, Even I received the <strong>same error code</strong> as yours <strong>and got Sign in Page in the Postman output</strong> like below:-</p>
<p>Created Airflow1 with AAD as auth type:-</p>
<p><img src=""https://i.imgur.com/pfiVqgT.png"" alt=""enter image description here"" /></p>
<p>Rest API output With Basic Auth with AAD credentials:-</p>
<p><img src=""https://i.imgur.com/OUB9sZG.png"" alt=""enter image description here"" /></p>
<p>Generated access token with client credentials flow using service principal client ID and secret and called the API but only got sign in page in response refer below:-</p>
<pre class=""lang-http prettyprint-override""><code>GET https://login.microsoftonline.com/&lt;Tenant-ID&gt;/oauth2/v2.0/token

client id:&lt;Client-Id&gt;
tenant id:&lt;tenant-Id&gt;
client secret:&lt;client-secret&gt;
scope:https:management.azure.com/.default
grant_type:client_credentials

</code></pre>
<p>Generated access token:-</p>
<p><img src=""https://i.imgur.com/GzvuoBY.png"" alt=""enter image description here"" /></p>
<p>Called the airflow API with the access token above but received the same error:-</p>
<p><img src=""https://i.imgur.com/e1hxyr1.png"" alt=""enter image description here"" /></p>
<p>Tried calling the airflow with curl request but it gave the same error:-</p>
<p>Curl:-</p>
<pre class=""lang-http prettyprint-override""><code>curl -X GET 'https://xxxxxxxxxxxxxx.uksouth.airflow.svc.datafactory.azure.com/api/v1/connections?limit=100' \ -H 'Content-Type: application/json' \ --user &quot;username:password&quot; \ -d '{ &quot;is_paused&quot;: true }'

</code></pre>
<p><img src=""https://i.imgur.com/IP9LPwt.png"" alt=""enter image description here"" /></p>
<p>If you want to call the airflow rest API with AAD auth, You can directly call the HTTP API in your Browser and get the desires response like below:-</p>
<p>Called the API in Swagger UI:-</p>
<p><img src=""https://i.imgur.com/lONaQ1D.png"" alt=""enter image description here"" /></p>
<p>Copy the Request URL and pass it in your Browser &gt; Log in with your AAD credentials and receive the response like below:-</p>
<p><img src=""https://i.imgur.com/4IPjG2m.png"" alt=""enter image description here"" /></p>
<p>Get the same API request in redoc and call it in your browser like below:-</p>
<p><img src=""https://i.imgur.com/3dmFSrs.png"" alt=""enter image description here"" /></p>
<p><em><strong>Now, I created a new Airflow managed service in ADF with Basic auth and called the API in Postman with Basic Auth like below:-</strong></em></p>
<p><img src=""https://i.imgur.com/lmaF2Cz.png"" alt=""enter image description here"" /></p>
<p><em><strong>Called Airflow API with basic auth credentials in Postman and received the desired response like below:-</strong></em></p>
<p><img src=""https://i.imgur.com/VkKbk9E.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/EoL3qsQ.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/eAYrofL.png"" alt=""enter image description here"" /></p>
<p>With curl:-</p>
<pre class=""lang-http prettyprint-override""><code>curl -X PATCH 'https://xxxxxxxxxxxxx.uksouth.airflow.svc.datafactory.azure.com/api/v1/connections?limit=100' \ -H 'Content-Type: application/json' \ --user &quot;username:password&quot; \ -d '{ &quot;is_paused&quot;: true }'

</code></pre>
<p><img src=""https://i.imgur.com/B9NfR9W.png"" alt=""enter image description here"" /></p>
"
"75896640","How to test Azure Data Factory Managed Airflow from postman","<p>I have an Airflow instance in Azure Data Factory.
I can test the APIs from swagger.
But when I try to test from postman, it's showing authentication error.
<a href=""https://i.stack.imgur.com/tgP2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tgP2Z.png"" alt=""enter image description here"" /></a></p>
<p>Tried basic-auth with my azure credentials, but did not work.
How do I authenticate Azure Data Factory Managed Airflow from postman?</p>
","<azure><airflow><azure-data-factory>","2023-03-31 08:59:22","105","0","2","75991155","<p>At the moment, REST APIs for ADF Managed Airflow only work if your Airflow instance was created with Basic Auth. This means when you create your ADF Managed Airflow instance, select &quot;Basic Auth&quot; and enter your own username/password and used that when calling REST APIs for your Airflow instance</p>
<p>ADF Managed Airflow REST APIs with AAD Auth is currently not supported</p>
<p><a href=""https://i.stack.imgur.com/EmzZs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EmzZs.png"" alt=""enter image description here"" /></a></p>
"
"75893129","How to use Kusto query with parameters on Expression builder in DataFlow activity?","<p>Ì am trying to pass some parameters to the Kusto query that are inside a DataFlow activity which are inside a ForEach activity as well, but it's always complaining on the Expression Builder in the source of the DataFlow.</p>
<p><a href=""https://i.stack.imgur.com/LlnWr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LlnWr.png"" alt=""Kusto example"" /></a></p>
<p>My pipeline:
<a href=""https://i.stack.imgur.com/F7HUG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F7HUG.png"" alt=""enter image description here"" /></a></p>
<p>My dataflow:
<a href=""https://i.stack.imgur.com/S20Zo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S20Zo.png"" alt=""enter image description here"" /></a></p>
<p>How can I use those parameters on expression builder in DataFlow activity to my Kusto query?</p>
","<azure-data-factory>","2023-03-30 21:26:49","61","0","1","75894716","<p>When query is given in query text box by clicking <strong>Add dynamic content</strong>, there is error message in dataflow expression builder. Instead, you can use the below workaround to achieve the same requirement of filtering the table.</p>
<ul>
<li>Enter the query without where option in the query text box of source transformation. <em>Do not write the query in the expression builder by clicking add dynamic content. Write it directly on the text box or by clicking on the edit icon.</em></li>
</ul>
<pre><code>let conversion= datatable (Decimal: int, hexcode: string, binary: string)
[
0,'0','0000',
15,'f','1111'
];
conversion
</code></pre>
<p><img src=""https://i.imgur.com/POQiaO8.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Then take the filter transformation and give the filter on condition as <code>hexcode==$hexcode</code>.</p>
</li>
<li><p>For data preview, $hexcode parameter is assigned with value &quot;f&quot;. Only one row is displayed in the preview.
<img src=""https://i.imgur.com/7edrina.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p>Use the filter transformation and pass the parameter in that transformation instead of giving in the source query itself.</p>
"
"75890693","How can I use string interpolation to output a query on ADF?","<p>Following two guides, one from <a href=""https://techcommunity.microsoft.com/t5/azure-data-factory-blog/evaluate-data-flow-expressions-inline-with-strings-in-adf/ba-p/1053589"" rel=""nofollow noreferrer"">Evaluate Data Flow Expressions Inline with Strings in ADF</a> and one from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-expression-builder#string-interpolation"" rel=""nofollow noreferrer"">Microsoft String Interpolation</a> I couldn't be able to understand how can I output a query that I am using on a Data Flow activity to debug easier? It's possible to see a query input from a pipeline but easily done on a Data Flow.</p>
<p>Basically high level this is my pipeline:</p>
<p><a href=""https://i.stack.imgur.com/pg3zb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pg3zb.png"" alt=""Pipeline"" /></a></p>
<p>And this is my data flow that is inside my ForEach activity:</p>
<p><a href=""https://i.stack.imgur.com/qlxaQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qlxaQ.png"" alt=""Data Flow"" /></a></p>
<p>In the data flow, I have a source that is executing a Kusto (Azure Data Explorer) query with parameters, something like this:</p>
<pre><code>set notruncation;
let HexCode = &quot;$HexCode$SubHexCode&quot;;
Table_1
| distinct ContainerId, VmId
| where tolower(ContainerId) startswith HexCode
| join kind=inner (
    Table_2
    | extend ContainerId = VmId
    | where startofday(TIMESTAMP) == startofday(datetime($Dates))
    | where tolower(ContainerId) startswith HexCode
    | where CounterName == &quot;Percentage CPU&quot;
    )
    on ContainerId
| project
    Column1,
    Column2
</code></pre>
<p>Multiline query:</p>
<pre><code>let Conversion = datatable(Decimal: int, HexCode:string, Binary: string)
 [
  0, '0', '0000',
 1, '1', '0001',
 2, '2', '0010',
 3, '3', '0011',
 4, '4', '0100',
 5, '5', '0101',
 6, '6', '0110',
 7, '7', '0111',
 8, '8', '1000',
 9, '9', '1001',
 10,'a', '1010',
 11,'b', '1011',
 12,'c', '1100',
 13,'d', '1101',
 14,'e', '1110',
 15,'f', '1111',
 ];
Conversion
| where HexCode == {$HexCode}
</code></pre>
<p>How can I see the expected query output with values instead of variables when I run my pipeline?</p>
","<azure-data-factory>","2023-03-30 16:21:27","79","0","1","75898027","<p>I am able to achieve your requirement using String interpolation like below.</p>
<p>This is a sample data in ADX table named Table1:</p>
<p><a href=""https://i.stack.imgur.com/AWLl4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AWLl4.png"" alt=""enter image description here"" /></a></p>
<p>For this sample demonstration, I have taken only one iteration of your lookup <strong>i.e,</strong> I have passed <code>Dates</code>,<code>HexCode</code>,<code>SubHexCode</code> from Pipeline variables to dataflow parameters directly like below.</p>
<p>The values for the pipeline variables, I have given as <code>2023-03-08</code>,<code>1</code>,<code>1</code> which are of string type.</p>
<p><img src=""https://i.imgur.com/SJQq0zx.png"" alt=""enter image description here"" /></p>
<p>In your case, you need to give these dataflow parameters values as <code>@item().Dates</code>, <code>@item().HexCode</code>, <code>@item().SubHexCode</code> inside ForEach.</p>
<p><strong>This is a sample query which involves the dataflow parameters and String interpolation</strong>. Open the dynamic content of the query and given the query like below.</p>
<pre><code>&quot;set notruncation;let HexCode = '{concat($HexCode,$SubHexCode)}';Table1| where tolower(ContainerId) startswith HexCode| where startofday(PreciseTimeStamp) == startofday(datetime({$Dates}))&quot;
</code></pre>
<p>Click on Open expression builder and give the above expression.</p>
<p><img src=""https://i.imgur.com/OnQxr8X.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/ar0OrqF.png"" alt=""enter image description here"" /></p>
<p>I have made the above expression for sample, you need to add your KQL query as per your requirement.</p>
<p>I have given a blob file in sink.</p>
<p><strong>Result after Pipeline Execution:</strong></p>
<p>You can see I got the records which satisfies the above query expression.</p>
<p><img src=""https://i.imgur.com/sOc7giG.png"" alt=""enter image description here"" /></p>
<p>Also <strong>Without String interpolation</strong>, you can give the query expression using <code>concat()</code> function like below. But, I suggest to use String interpolation.</p>
<pre><code>concat('set notruncation; let HexCode = &quot;',concat($HexCode,$SubHexCode),'&quot;;Table1| where tolower(ContainerId) startswith HexCode| where startofday(PreciseTimeStamp)==startofday(datetime(',$Dates,'))')
</code></pre>
"
"75889868","Azure Data Factory data flow not reading JSON object due to null value","<p>I'm working on an Azure Data Factory (ADF) data flow that reads JSON data from a file. However, I'm encountering an issue where the data flow is failing to read JSON objects due to a null value.</p>
<p>Here's a sample JSON data with the null value:</p>
<pre><code>[
    null,
    {
        &quot;Column2&quot;: &quot;KM120&quot;,
        &quot;Mon&quot;: 8,
        &quot;Tue&quot;: 8,
        &quot;Wed&quot;: 8,
        &quot;Thu&quot;: 8,
        &quot;Fri&quot;: 8
    },
    {
        &quot;Column2&quot;: &quot;KM121&quot;,
        &quot;Mon&quot;: 8,
        &quot;Tue&quot;: 8,
        &quot;Wed&quot;: 8,
        &quot;Thu&quot;: 8,
        &quot;Fri&quot;: 8
    }
]
</code></pre>
<p>The error message I'm getting is:</p>
<pre><code>Error occurred when deserializing source JSON file. Check if the data is in valid JSON object format. Error reading JObject from JsonReader. Current JsonReader item is not an object: Null. Path '[0]', line 2, position 5.
</code></pre>
<p>I have checked the JSON file and it is in the correct format. I suspect the issue is with the null value in the first element of the array.</p>
<p>Can anyone suggest how to resolve this issue and make the data flow read the JSON objects even with null values?</p>
<p>Thanks in advance.</p>
<p>I removed the null value from the JSON file and tested the data flow, which resolved the issue. However, I'm looking for a way to automate this process since manually editing each file is time-consuming. I'm exploring the option of using a custom JSON schema in the data flow that can handle null values, but I haven't been able to get it to work so far. I'm still investigating this solution and open to other suggestions.</p>
","<azure-data-lake-gen2><azure-data-factory>","2023-03-30 15:01:54","109","0","1","75896127","<blockquote>
<p>Can anyone suggest how to resolve this issue and make the data flow read the JSON objects even with null values?</p>
</blockquote>
<p>If your Json file is in <code>Azure Blob Storage</code> or <code>ADLS gen 2</code> you can access that file using the rest API ang read the Json object with Null values.</p>
<p>To achieve this follow below procedure:</p>
<ul>
<li>First Go to your storage account and <code>change access level</code> of container to <strong>container</strong> then copy the file path of your Json file.
<img src=""https://user-images.githubusercontent.com/98518545/229049786-1a49c68d-4a1b-43bc-b18b-0176a004d7e4.gif"" alt=""31-1 1"" /></li>
<li>Then create a linked service as rest api and access this file from there <strong>paste the copied url from blob storage as base url and select authentication as Anonymous.</strong><img src=""https://i.imgur.com/WwAwSjS.png"" alt=""enter image description here"" /></li>
<li>Then create data flow and select rest api linked service as source and import the schema.
<img src=""https://user-images.githubusercontent.com/98518545/229053666-de92d7e9-d7af-4d18-9197-a06b9b830d85.gif"" alt=""31-1 2"" /></li>
<li>take select transformation and do mapping ass below:
<img src=""https://i.imgur.com/sHRYKY7.png"" alt=""enter image description here"" /></li>
</ul>
<p><em><strong>Output:</strong></em></p>
<p><img src=""https://i.imgur.com/HoM6Tv7.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Note: It is possible only if your file is in <code>Azure Blob Storage</code> or <code>ADLS gen 2</code></strong></p>
</blockquote>
"
"75889567","ADF - Run an activity if any one of a set of activities fail and capture failed activity details in that activity","<p>Currently I am running one generic activity when any of the activities in the pipeline fail based on the answer provided in
<a href=""https://learn.microsoft.com/en-us/answers/questions/208812/adf-run-an-activity-if-any-one-of-a-set-of-activit"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/208812/adf-run-an-activity-if-any-one-of-a-set-of-activit</a></p>
<p>But I want to identify which activity is failed in pipeline across 30 activities in generic activity.</p>
<p>We can write 30 if conditions like below in pipeline expression builder but I am looking for more optimized solution.</p>
<p><a href=""https://i.stack.imgur.com/ZKHvz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZKHvz.png"" alt=""enter image description here"" /></a></p>
<p>Do we have any solution to identify which activity is failed if we have large number of activities..?</p>
","<azure><azure-data-factory>","2023-03-30 14:35:25","60","0","1","75895199","<blockquote>
<p>Do we have any solution to identify which activity is failed if we have large number of activities..?</p>
</blockquote>
<p>You can use an <strong>Execute Pipeline</strong> activity for this. Give all your activities with certain names in a pipeline. Take another pipeline with Execute pipeline activity and give your pipeline to it. On the Faiilure of this activity add your Generic activity.</p>
<p>In this approach,</p>
<ul>
<li><strong>The child pipeline(your pipeline) flow will stop on the first failure of any activity and Execute pipeline activity also fails</strong>.</li>
<li>Now you can execute your Generic activity. By using this approach, it does not matter whatever the pipeline flow whether simple or complicated because it identifies first failed activity.</li>
</ul>
<p>But one thing is you need to make sure that all activities should execute only on the success of previous activities.</p>
<p>This is the sample demonstration for it.</p>
<p>My sample child pipeline with activities <code>Mylookup</code>, <code>activity1</code>, <code>activity2</code>. Here <code>Mylookup</code> designed to be failed.</p>
<p><img src=""https://i.imgur.com/rDxJCkc.png"" alt=""enter image description here"" /></p>
<p>And this is the failure message for it.</p>
<p><img src=""https://i.imgur.com/zcf6rdJ.png"" alt=""enter image description here"" /></p>
<p>We can get the name and error details of the above failed activity after Execute pipeline activity in Parent pipeline using below approach.</p>
<p>Here Execute pipeline activity will give the error details like this for any failed activity of the child pipeline.</p>
<p><img src=""https://i.imgur.com/ImPJZUz.png"" alt=""enter image description here"" /></p>
<p>So, extract the activity name from the above using below expression.</p>
<pre><code>@substring(activity('Execute Pipeline1').error.message, add(indexOf(activity('Execute Pipeline1').error.message,'target'),7), sub(sub(indexOf(activity('Execute Pipeline1').error.message,'failed'),1),add(indexOf(activity('Execute Pipeline1').error.message,'target'),7)))
</code></pre>
<p><strong>Failed activity name:</strong>
For sample, I have stored this in a set variable</p>
<p><img src=""https://i.imgur.com/GGGDRuI.png"" alt=""enter image description here"" /></p>
<p>Extract the error details JSON of activity using <code>@activity('Execute Pipeline1').error</code> expression and for error message use this <code>@activity('Execute Pipeline1').error.message</code>.</p>
<p>You can combine the activity name and error details as per your requirement in your generic activity which should execute on the failure of Execute pipeline activity.</p>
"
"75887704","Pass parameters from excel into script activity","<p>Please help me with this: I was trying to read data from excel which has two columns by using lookup activity then i need to pass these value as the input of another script task by using for each loop but i am getting error-
The variable 'source_feed_id' of type 'String' cannot be initialized or updated with value of type 'Array'. The variable 'source_feed_id' only supports values of types 'String'.</p>
<p>One column is of integer type and other is string</p>
<p>I read data of excel file through lookup activity then in foreach loop i use the output of lookup to run 17 times as the row count is 17-@activity('Lookup1').output.value</p>
<p>Then i use set variable to set the variable as the output of lookup activity with --@activity('Lookup1').output.source_feed_id</p>
<p>then the same i pass in script task as variable to update the table in sql</p>
","<azure-data-factory>","2023-03-30 11:33:29","54","0","1","75888310","<p>When the value passed is not of string, then you will get this error.</p>
<p><img src=""https://i.imgur.com/HVbjsDa.png"" alt=""enter image description here"" /></p>
<p>In order to resolve this error,</p>
<ul>
<li>When connecting for-each activity with lookup activity, give the items expression as <code>@activity('Lookup1).output.value</code></li>
</ul>
<p><img src=""https://i.imgur.com/qyNb6MB.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then inside for-each activity, in the set variable activity, give the value as  <code>@item().&lt;column_name&gt;</code>. Replace <code>&lt;column-name&gt;</code> with the required column name from lookup table. This column is not to be of array type. Otherwise, you will get the same error.</li>
</ul>
<p><img src=""https://i.imgur.com/4yV0JA8.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then you can use this variable <code>@variables('source_feed_id')</code> in the script activity to update the table.</li>
</ul>
"
"75886487","Azure Data Factory - Scale elasticpool with web activity","<p>I'm trying to scale my Azure elasticpool in datafactory with a web activity task passing the following api call:</p>
<pre><code>{
    &quot;url&quot;: &quot;https://management.azure.com/subscriptions/subID/resourceGroups/RGNAME/providers/Microsoft.Sql/servers/sqlservername/elasticPools/elasticpoolname?api-version=2020-08-01-preview&quot;,
    &quot;method&quot;: &quot;PUT&quot;,
    &quot;headers&quot;: {
        &quot;Content-Type&quot;: &quot;application/json&quot;
    },
    &quot;body&quot;: {
        &quot;sku&quot;: {
            &quot;name&quot;: &quot;StandardPool&quot;,
            &quot;tier&quot;: &quot;Standard&quot;,
            &quot;capacity&quot;: &quot;50&quot;
        },
        &quot;location&quot;: &quot;West Europe&quot;
    },
    &quot;authentication&quot;: {
        &quot;type&quot;: &quot;MSI&quot;,
        &quot;resource&quot;: &quot;https://management.azure.com/&quot;
    }
}
</code></pre>
<p>The activity fails, but the strange thing is that the scale actually works. Even though the scaling works I obviously don't want the activity to fail.</p>
<p>ADF has a contributor role in SQLServer</p>
<p>Any idea?</p>
<p>Here is the error:</p>
<pre><code>&quot;error&quot;: {
    &quot;code&quot;: &quot;AuthorizationFailed&quot;,
    &quot;message&quot;: &quot;The client 'ADF ID' with object id 'ADF ID' does not have authorization to perform action 'Microsoft.Sql/locations/elasticPoolOperationResults/read' over scope '/subscriptions/SUBID/resourceGroups/RGNAME/providers/Microsoft.Sql/locations/westeurope/elasticPoolOperationResults/id' or the scope is invalid. If access was recently granted, please refresh your credentials.&quot;
}
</code></pre>
","<azure><azure-data-factory><azure-elasticpool>","2023-03-30 09:36:08","54","0","1","75887791","<p>I reproduce same my environment. I got same error.</p>
<p><img src=""https://i.imgur.com/G7c2Cmc.png"" alt=""enter image description here"" /></p>
<p>To resolve this issue. Go to SQL server -&gt; Access control (IAM) + Add Contributor role -&gt; Go to members Assign access to Manage identity and select azure data factory.</p>
<p><img src=""https://i.imgur.com/LmOvVAb.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Try with below body and header:</strong></p>
</blockquote>
<p><strong>Header:</strong> <code>Content-Type application/json</code></p>
<p><strong>Body:</strong> {&quot;sku&quot;:{&quot;name&quot;:&quot;GP_Gen5_2&quot;,&quot;tier&quot;:&quot;GeneralPurpose&quot;},&quot;location&quot;:&quot;eastus&quot;}</p>
<p><strong>Resource:</strong> <a href=""https://management.azure.com/"" rel=""nofollow noreferrer"">https://management.azure.com/</a></p>
<blockquote>
<p><strong>If you want to check the configuration use below script:</strong></p>
</blockquote>
<p><strong>script :</strong></p>
<p><code>SELECT DATABASEPROPERTYEX(db_name(),'edition') AS ServiceTier , DATABASEPROPERTYEX(db_name(),'serviceobjective') AS ComputeSize</code></p>
<p><img src=""https://i.imgur.com/6x52wWF.png"" alt=""enter image description here"" /></p>
"
"75886457","Can't save the password in connection manager in SSIS package, it's replaced with *** (stars) after publishing","<p>I have a SSIS package shifted to ADF, it's connected to Azure SQL Server, but every time I publish, the password in connection manager is being replaced with ********** star symbols.  <a href=""https://i.stack.imgur.com/zwgtl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zwgtl.png"" alt=""password replaced"" /></a> - obviously its impossible to connect to DB after that and I have to replace password with actual one again to run the pipeline. Have 'Protection Level: EncryptSensitiveWithUserKey' in the SSIS project.</p>
","<ssis><azure-data-factory><ssis-connection-manager>","2023-03-30 09:31:27","61","0","1","75919524","<blockquote>
<p>I have to replace password with actual one again to run the pipeline.</p>
</blockquote>
<p>To resolve this issue you can use <code>dynamic content</code> for pipeline.</p>
<ul>
<li><p>In pipeline we can create string <strong>variable</strong> or a <strong>pipeline parameter</strong> and then use that variable or parameter in pipeline while runtime.
<img src=""https://i.imgur.com/ErCmk8F.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Using dynamic expression, you can fetch it like below:
<img src=""https://i.imgur.com/sNOohX9.png"" alt=""enter image description here"" /></p>
</li>
<li><p>It will take its default value automatically in runtime.so you don't need to put it in for every run.</p>
</li>
</ul>
"
"75884585","ADF pipeline changes required on enabling hierarchical namespaces in Azure Data Lake Gen2","<p>What are the changes that will be required in the ADF pipeline on enabling hierarchical namespaces in Azure Data Lake Gen2.
What improvements can be expected in ADF and Synapse once this upgrade is done.
The datalake is used to store raw data</p>
<p>Upgrade to a storage account with Azure Data Lake Gen2 capabilities, but need to know the impact on existing pipelines and data flows in ADF<br />
<a href=""https://i.stack.imgur.com/Q0oyF.png"" rel=""nofollow noreferrer"">Enabling Hierarchical Namespaces</a></p>
","<azure-data-factory><azure-data-lake><azure-data-lake-gen2>","2023-03-30 06:00:06","52","0","2","75884834","<p>One improvement with HNS is when you are doing parallel copies into ADLS, such as when the executions are happening at the same time, sometimes without HNS these resulted in errors in my experience. Without HNS, static folders had to be used, in the sense that parallel copies only copied into different folders and not the same one at the same time (imagine 50 copy activities running in parallel).</p>
<p>From that point of view, Gen2 with HNS is a better option than for example using a blob storage.</p>
<p>There are differences in ACL too, that might be beneficial for ADF/Synapse.</p>
<p>From documentation (Atomic Directory Manipulation):</p>
<p>&quot;Without real directories, applications must process potentially millions of individual blobs to achieve directory-level tasks. By contrast, a hierarchical namespace processes these tasks by updating a single entry (the parent directory).</p>
<p>This dramatic optimization is especially significant for many big data analytics frameworks. Tools like Hive, Spark, etc. often write output to temporary locations and then rename the location at the conclusion of the job. Without a hierarchical namespace, this rename can often take longer than the analytics process itself. Lower job latency equals lower total cost of ownership (TCO) for analytics workloads.&quot;</p>
<p>In short, this method of blob organization greatly enhances the performance of search operations. Here is simple illustration from one my books which shows how ADLS search improves upon blob storage:</p>
<p><a href=""https://i.stack.imgur.com/PJA2n.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PJA2n.jpg"" alt=""ADLS Gen2 layer on top of Blob"" /></a></p>
<p>That being said, of course not all workloads are going to benefit from HNS, but workloads for analytics processing is especially to benefit a lot from HNS. What is the workload/processing you have in mind?</p>
"
"75884585","ADF pipeline changes required on enabling hierarchical namespaces in Azure Data Lake Gen2","<p>What are the changes that will be required in the ADF pipeline on enabling hierarchical namespaces in Azure Data Lake Gen2.
What improvements can be expected in ADF and Synapse once this upgrade is done.
The datalake is used to store raw data</p>
<p>Upgrade to a storage account with Azure Data Lake Gen2 capabilities, but need to know the impact on existing pipelines and data flows in ADF<br />
<a href=""https://i.stack.imgur.com/Q0oyF.png"" rel=""nofollow noreferrer"">Enabling Hierarchical Namespaces</a></p>
","<azure-data-factory><azure-data-lake><azure-data-lake-gen2>","2023-03-30 06:00:06","52","0","2","75889218","<p>There are several implementation details that would impact existing pipelines and code assets.</p>
<ol>
<li><p>Blob Storage and ADLS (Gen2) have different Linked Service types, so you would need to create a new Linked Service and update all items that reference the old Blob Storage Linked Service (typically Datasets).</p>
</li>
<li><p>The URL paths and protocols are different:</p>
</li>
</ol>
<ul>
<li>The root blob storage URL format is
<em>{storage-account-name}.blob.core.windows.net</em>. For ADLS, the
format is <em>{storage-account-name}.dfs.core.windows.net</em>.</li>
<li>For services that require the custom system driver (such as Spark notebooks), Blob Storage uses <em>wasbs</em> for the protocol. ADLS uses <em>abfss</em>.</li>
</ul>
<ol start=""3"">
<li>ADLS typically requires better defined IAM/RBAC. You will most likely need to grant the StorageBlobDataReader or StorageBlobDataContributor role to the Managed Identity for your ADF instance. When you create an ADLS Linked Service, I believe the UI gives you a prompt to remind you to add that role.</li>
</ol>
"
"75878162","Error ""DSL compilation failed"", ""extraneous input ','""","<p>I'm having a classic issue in Data Factory where some <strong>forbidden character</strong> causes a failure when parsing the script file and/or the Json code representation.</p>
<p>However in this situation the &quot;extraneous character&quot; (according to the message: an unexpected <strong>comma</strong>) turns out to be elusive. The files I'm looking at don't seem to be the ones that DataFactory is complaining about (there's no <strong>position 47 on line 2</strong>!).</p>
<p>For clarity's sake, I'm only showing <strong>my <em>one</em> ultrabasic flowlet.</strong> Could it be that the error is somewhere else in my many flows?  I don't see how, as I click on &quot;Data preview&quot; specifically on <em>this</em> flow. Also, it passes validation.</p>
<p>The CSV file:</p>
<p><a href=""https://i.stack.imgur.com/Cz2qO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cz2qO.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/WEtcW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WEtcW.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/DYGqF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DYGqF.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>Spark job failed: { &quot;text/plain&quot;:
&quot;{&quot;runId&quot;:&quot;REDACTED&quot;,&quot;sessionId&quot;:&quot;REDACTED&quot;,&quot;status&quot;:&quot;Failed&quot;,&quot;payload&quot;:{&quot;statusCode&quot;:400,&quot;shortMessage&quot;:&quot;com.microsoft.dataflow.broker.InvalidOperationException:
DSL compilation failed: DF-DSL-001 - DSL stream has parsing
errors\nLine 2 Position 47: extraneous input ',' expecting ...</p>
</blockquote>
<p>Showing the files in question :</p>
<pre><code>source(useSchema: false,
    allowSchemaDrift: true,
    validateSchema: false,
    ignoreNoFilesFound: false,
    format: 'delimited',
    container: 'newmessagingtemp',
    folderPath: 'testinputs',
    fileName: 'conversation_top_3.csv',
    columnDelimiter: ',',
    escapeChar: '\\',
    quoteChar: '\&quot;',
    columnNamesAsHeader: true) ~&gt; conversationsTop3
conversationsTop3 output() ~&gt; conversationsTop3Output
</code></pre>
<p>and</p>
<pre><code>{
    &quot;name&quot;: &quot;conversations_csv&quot;,
    &quot;properties&quot;: {
        &quot;description&quot;: &quot;Use a CSV file in place of the source SQL Table for Conversations &quot;,
        &quot;folder&quot;: {
            &quot;name&quot;: &quot;Tests (can be deleted)&quot;
        },
        &quot;type&quot;: &quot;Flowlet&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;linkedService&quot;: {
                        &quot;referenceName&quot;: &quot;AzureBlobStorage1&quot;,
                        &quot;type&quot;: &quot;LinkedServiceReference&quot;
                    },
                    &quot;name&quot;: &quot;conversationsTop3&quot;,
                    &quot;description&quot;: &quot;&quot;
                }
            ],
            &quot;sinks&quot;: [],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;conversationsTop3Output&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(useSchema: false,&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false,&quot;,
                &quot;     format: 'delimited',&quot;,
                &quot;     container: 'newmessagingtemp',&quot;,
                &quot;     folderPath: 'testinputs',&quot;,
                &quot;     fileName: 'conversation_top_3.csv',&quot;,
                &quot;     columnDelimiter: ',',&quot;,
                &quot;     escapeChar: '\\\\',&quot;,
                &quot;     quoteChar: '\\\&quot;',&quot;,
                &quot;     columnNamesAsHeader: true) ~&gt; conversationsTop3&quot;,
                &quot;conversationsTop3 output() ~&gt; conversationsTop3Output&quot;
            ]
        }
    }
}
</code></pre>
","<azure-data-factory>","2023-03-29 13:52:02","66","0","1","75885124","<p>Error &quot;DSL compilation failed&quot;, &quot;extraneous input ','&quot;
Error &quot;DSL compilation failed&quot;, &quot;extraneous input ','&quot;</p>
<blockquote>
<p>Error &quot;DSL compilation failed&quot;, &quot;extraneous input ','&quot;</p>
</blockquote>
<p>I tried to reproduce the issue and got similar error.</p>
<p><img src=""https://i.imgur.com/cRZeejH.png"" alt=""enter image description here"" /></p>
<p><strong>The issue is because we are not using the schema of file.</strong></p>
<p>To resolve this, you need to import the schema for the file. you can import schema by go to <strong>Projection &gt;&gt; Import Schema &gt;&gt; import</strong></p>
<p><img src=""https://i.imgur.com/iWa6CNx.png"" alt=""enter image description here"" /></p>
<p><em><strong>Output:</strong></em></p>
<p><img src=""https://i.imgur.com/hceF5wh.png"" alt=""enter image description here"" /></p>
"
"75876571","Recreate a directory in Azure ADLS without Impacting downstream workloads","<p>We have come across a scenario in our project where in we need to switch from one directory (files current data without history) to another (files current data with history). This is a proposal floating around and the reason for this is, we intend to have a minimal disruption when history loads are on and when the history loads are done we intend to delete the old directory with no history data and rename the new directory with history data to the same old name so downstream workloads are not impacted.</p>
<p>Is this a feasible solution or are there chances the references will break in the downstream workloads due to the above approach.</p>
<p>We are working on the POC to check if this is having any impact.</p>
","<azure><azure-data-factory><azure-databricks><azure-data-lake-gen2>","2023-03-29 11:25:52","47","0","1","75878820","<p><strong>I reproduce the same thing in my environment. I got this output.</strong></p>
<p>Please follow this steps to achieve your requirement.</p>
<ul>
<li>First Mount storage account as per <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-use-databricks-spark#create-a-container-and-mount-it"" rel=""nofollow noreferrer"">Ms_Doc</a>.In my scenario this is my mount point: <code>/mnt/my_folder2/</code></li>
</ul>
<p><strong>Code:</strong></p>
<pre><code>old_dir = &quot;/mnt/my_folder2/dem&quot;
new_dir = &quot;/mnt/my_folder2/new/dem&quot;

# using `dbutils.fs.mv` move the old directory to new directory.
dbutils.fs.mv(old_dir,new_dir,recurse=True)
</code></pre>
<p><img src=""https://i.imgur.com/HI53ILg.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/0T6GS1R.png"" alt=""enter image description here"" /></p>
"
"75875201","Moving data from sql to nosql with azure data factory","<p>A simple question: I can't find any specific documentation that says that you can move data from a sql db (on-prem) to a nosql sb (mongodb) using Azure Data Factory.<br>
Is that possible? Is there any documentation about it?<br>Thanks a lot.</p>
","<sql><mongodb><azure-data-factory>","2023-03-29 09:07:26","38","0","1","75875530","<p>There is a connector in ADF for MongoDB or Atlas MongoDB that you can use as a destination.
Here is the documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb?tabs=data-factory#linked-service-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb?tabs=data-factory#linked-service-properties</a></p>
<p>Since your source is on-prem SQL server, you may need to use self hosted integration runtime to be able to connect to it (because it's inside the company firewall).
Here is the coumentation to installing it: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory</a></p>
"
"75874583","Azure Data Factory: Copy file mentioned in JSON","<p>I have a JSON where one the parts is a filename.</p>
<pre><code>{
    &quot;dial_target_file_row_count&quot;: 2440,
    &quot;dial_target_file_endpoint&quot;: &quot;consumerzone@dialconsumingp01dls&quot;,
    &quot;dial_target_file_full_path&quot;: &quot;abfss://*****/data/snapshotdate=2023-03-25&quot;,
    &quot;dial_metadata_file_location&quot;: &quot;abfss://*****/metadata/&quot;,
    &quot;dial_metadata_file_name&quot;: &quot;dsapp_goldenDataset_2023-03-25_20230326075008_metadata.json&quot;,
}
</code></pre>
<p>Now I want to copy the file 'dail_target_file_full_path' onto my blob storage.
How do I define the source and sink (the sink name should be a part of the original filename).</p>
<p>I don't see any option to have a dynamic filename</p>
","<copy><azure-data-factory><sink>","2023-03-29 07:59:36","59","0","2","75875916","<p>Add a lookup activity to your pipeline to get the content of the JSON with the file details. If this is one file, check &quot;first row only&quot;.
Add a copy data activity, and create a dataset with parameters:
<a href=""https://i.stack.imgur.com/DtTsd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DtTsd.png"" alt=""dataset"" /></a></p>
<p>In the copy data source, you can now add values for this parameters, which should be the result of the first JSON:
<a href=""https://i.stack.imgur.com/K8NKo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K8NKo.png"" alt=""source"" /></a>
Here are the values I provided in dynamic content for this parameters:</p>
<pre><code>@activity('Lookup1').output.firstRow.dial_metadata_file_location
@activity('Lookup1').output.firstRow.dial_metadata_file_name
</code></pre>
<p>You will need to get just a part of the file path for the folder (without the url part)</p>
"
"75874583","Azure Data Factory: Copy file mentioned in JSON","<p>I have a JSON where one the parts is a filename.</p>
<pre><code>{
    &quot;dial_target_file_row_count&quot;: 2440,
    &quot;dial_target_file_endpoint&quot;: &quot;consumerzone@dialconsumingp01dls&quot;,
    &quot;dial_target_file_full_path&quot;: &quot;abfss://*****/data/snapshotdate=2023-03-25&quot;,
    &quot;dial_metadata_file_location&quot;: &quot;abfss://*****/metadata/&quot;,
    &quot;dial_metadata_file_name&quot;: &quot;dsapp_goldenDataset_2023-03-25_20230326075008_metadata.json&quot;,
}
</code></pre>
<p>Now I want to copy the file 'dail_target_file_full_path' onto my blob storage.
How do I define the source and sink (the sink name should be a part of the original filename).</p>
<p>I don't see any option to have a dynamic filename</p>
","<copy><azure-data-factory><sink>","2023-03-29 07:59:36","59","0","2","75876108","<p>Adding to <strong>@Chen Hirsh</strong>, you can extract the container name, folder path and file name using variables like below from the lookup output. Here I have not checked first row only in the lookup.</p>
<p>For this, I have copied a sample file to the above path(it means I am doing for the copy activity sink and you can change this as per your requirement).</p>
<p>Use the below dynamic content for the <strong>container name</strong> from <code>dial_target_file_full_path</code> abfss path.</p>
<pre><code>@substring(activity('Lookup1').output.value[0].dial_target_file_full_path, add(indexOf(activity('Lookup1').output.value[0].dial_target_file_full_path, '//'), 2),sub(indexOf(activity('Lookup1').output.value[0].dial_target_file_full_path, '@'),add(indexOf(activity('Lookup1').output.value[0].dial_target_file_full_path, '//'), 2)))
</code></pre>
<p><img src=""https://i.imgur.com/6DY5m9W.png"" alt=""enter image description here"" /></p>
<p><strong>For folder path:</strong></p>
<pre><code>@split(activity('Lookup1').output.value[0].dial_target_file_full_path,'.net')[1]
</code></pre>
<p><img src=""https://i.imgur.com/8mOWjUZ.png"" alt=""enter image description here"" /></p>
<p><strong>For file name:</strong></p>
<p><code>@activity('Lookup1').output.value[0].dial_metadata_file_name</code></p>
<p><img src=""https://i.imgur.com/QrGDzOM.png"" alt=""enter image description here"" /></p>
<p>Create dataset parameters for container name, folder path and file name and give those in dataset path. Then use these in copy activity dataset parameters.(Here I have used it for sink).</p>
<p><img src=""https://i.imgur.com/RgooCRn.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/mBsnpXY.png"" alt=""enter image description here"" /></p>
"
"75874228","How do I process the trigger file in Azure Data Factory","<p>I want to process a file that is placed on our Blob storage using a trigger.
The trigger looks like this:
<a href=""https://i.stack.imgur.com/RgfhN.png"" rel=""nofollow noreferrer"">trigger definition:</a></p>
<p>Daily a new file is created with a data-timestamp in the name.</p>
<p>Q: I want to read the content of this JSON (which has a different name daily), but how?</p>
","<dynamic><triggers><azure-data-factory>","2023-03-29 07:17:58","55","0","1","75875393","<ul>
<li>To get the contents of the file which is triggering the pipeline, you need to use look up activity with parameters. First create a pipeline parameter as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/8FUcNIw.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now in look up, I have created a dataset. For this dataset, I have created a parameter to use as filename. I pass this value from pipeline using the above created parameter value.</li>
</ul>
<p><img src=""https://i.imgur.com/wM18EBl.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now when you create the storage event trigger for this pipeline, after filling the trigger definition and clicking continue, parameters tab will be displayed. Here give the value to <code>fileName</code> pipeline parameter as <code>@triggerBody().fileName</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/SoaLXRD.png"" alt=""enter image description here"" /></p>
<ul>
<li>When a file is uploaded/created, then the filename will be captured and stored in <code>fileName</code> parameter which will be used in look up to get the content of that file.</li>
</ul>
<p><img src=""https://i.imgur.com/5lUV6Bv.png"" alt=""enter image description here"" /></p>
"
"75873542","ADF Dynamic file naming for BLOB through Data flow and then into a copy activity to SQLDB","<p>I am using the primary answer here. (<a href=""https://stackoverflow.com/questions/74629505/how-do-i-pull-the-last-modified-file-with-data-flow-in-azure-data-factory"">How do I pull the last modified file with data flow in azure data factory?</a>) My data flow kicks back errors that look like a mapping error in data flow. :( Is there a fix for this? It's a complex json mapping flow that takes a json and creates a CSV.</p>
<p><a href=""https://i.stack.imgur.com/6hptK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6hptK.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/GDjc7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GDjc7.png"" alt=""enter image description here"" /></a></p>
<p>I was expecting it to run, but it didn't.</p>
<pre><code>    source(output(
        {$schema} as string,
        type as string,
        items as (type as string, properties as (columns as (type as string, items as (type as string)[]), rows as (type as string, items as (type as string, items as (type as string)[])[])), required as string[])[]
    ),
    allowSchemaDrift: true,
    validateSchema: false,
    ignoreNoFilesFound: false,
    purgeFiles: true,
    modifiedAfter: (toTimestamp(1679961600000L)),
    documentForm: 'arrayOfDocuments') ~&gt; source1
source1 select(mapColumn(
        columns
    ),
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~&gt; select1
source1 derive(rows = unfold(rows)) ~&gt; derivedColumn1
derivedColumn1 derive(rows = replace(replace(toString(rows),'[',''),']','')) ~&gt; derivedColumn2
select1 derive(columns = replace(replace(toString(columns),'[',''),']','')) ~&gt; derivedColumn3
derivedColumn2 rank(asc(rows, true),
    caseInsensitive: true,
    output(id as long),
    dense: true) ~&gt; rank1
rank1 select(mapColumn(
        rows,
        id
    ),
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~&gt; select2
select2, select3 union(byName: false)~&gt; union1
union1 sort(asc(id, true)) ~&gt; sort1
derivedColumn3 aggregate(groupBy(columns),
    column = first(columns)) ~&gt; aggregate1
aggregate1 select(mapColumn(
        column
    ),
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~&gt; select3
sort1 select(mapColumn(
        rows
    ),
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~&gt; select4
select4 sink(allowSchemaDrift: true,
    validateSchema: false,
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~&gt; DS2csv
</code></pre>
","<azure-data-factory>","2023-03-29 05:46:17","46","0","1","75884310","<ul>
<li>I have taken the sample data provided in this <a href=""https://stackoverflow.com/questions/75861119/complex-json-in-adf-that-has-me-scratching-my-head"">SO question</a>.</li>
</ul>
<p><img src=""https://i.imgur.com/QencLjG.png"" alt=""enter image description here"" /></p>
<ul>
<li>The error occurs because of the schema of the source data. I have got the same error when I used the transformations provided.</li>
</ul>
<p><img src=""https://i.imgur.com/KbVZql8.png"" alt=""enter image description here"" /></p>
<ul>
<li>When you look at the schema of the source, you can see that the schema does not include either <code>rows</code> or <code>columns</code> columns. And hence reference to these columns in <code>select1, derivedColumn1 and derivedColumn3</code> is throwing an error.</li>
</ul>
<p><img src=""https://i.imgur.com/KVY8lna.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I import schema, the new schema would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/qClGGcv.png"" alt=""enter image description here"" /></p>
<ul>
<li>The dataflow would work fine now since the columns <code>rows and columns</code> are included in the schema.  The sink data preview would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/jhKvj9O.png"" alt=""enter image description here"" /></p>
<ul>
<li>After running the dataflow, the file generated would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/vj0gDf4.png"" alt=""enter image description here"" /></p>
"
"75871456","How can I escape special characters?","<p>I have a pipeline with a single web activity. In the header for the web activity I have a single header, which is <code>&quot;Authorization&quot;:&quot;v4cVQr3AkUwP3LMNUCSeQg==mirwVOd7MxPBGt6x0bEeIBTp4x8cCOkB+Vsa6cMfadskMllP5PcKjgZEExmVw4bvDpmKdki1+CaGPiSgrazLnR930R2BVmpZwq9y2K7Yj7REHpkmNNqCRn/XGpoKJkKByifQKyDTAr7StpZVUGEovw==&quot;</code></p>
<p>Basically, no matter what I try, I receive the following error:</p>
<blockquote>
<p>Error calling the endpoint 'https://rest.myabsorb.com'. Response
status code: 'NA - Unknown'. More details: Exception message: 'NA -
Unknown [ClientSideException] The format of value
'v4cVQr3AkUwP3LMNUCSeQg==mirwVOd7MxPBGt6x0bEeIBTp4x8cCOkB+Vsa6cMfadskMllP5PcKjgZEExmVw4bvDpmKdki1+CaGPiSgrazLnR930R2BVmpZwq9y2K7Yj7REHpkmNNqCRn/XGpoKJkKByifQKyDTAr7StpZVUGEovw=='
is invalid. Request didn't reach the server from the client.</p>
</blockquote>
<p>However, when I remove the special characters from the <code>Authorization</code> value, my request does get sent to the server (only, the server responds with a &quot;incorrect password&quot; message) - but the point is, if I manually remove these special characters, the request has a valid format according to ADF.</p>
<p>I have tried using a nested <code>@replace(replace(replace(&lt;value&gt;, '=', '\='), '+', '\+'), '/', '\/')</code> function to replace these special characters, but it does not work. I receive the same error.</p>
<p>Is the backslash <code>\</code> not the correct escape character to use? How can I get my web activity to make this request?</p>
","<azure-data-factory>","2023-03-28 22:06:43","31","0","1","75871514","<p>Hmmm. I solved this by using <code>@concat('Bearer ', &lt;token&gt;)</code>. No <code>replace</code> function or any escape character needed.</p>
"
"75867710","Azure data factory tabular translator expressions documentation","<p>In an ADF pipeline copy action, I need to build a column renaming scheme, and I'm running into lack of documentation on the available tools.</p>
<p>Suppose I've made the following map:</p>
<pre><code>{
    &quot;type&quot;:&quot;TabularTranslator&quot;,
    &quot;mappings&quot;:
    [
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;Property1&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;Province&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;Property2&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;Division&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;Property3&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;County&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;Property4&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;District&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;Property5&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;Municipality&quot; }
        }
    ]
}
</code></pre>
<p>What tools are available to me for referencing other data from the pipeline, like parameters, variables, lookup sets? How can I match source names by expression and construct sink names from the source names? What if I wanted to rename only some columns, and have a catch-all that simply forwards the source name to sink name if none match? Should I be considering other actions besides copy? What are they?</p>
","<azure-data-factory>","2023-03-28 14:42:07","53","0","1","75874445","<ul>
<li><p>You can use the activities provided within the data factory to build the dynamic mapping JSON. The JSON you have given is the format for dynamic mapping which you will build using iterations, variables (set variable and append variable) conditionals and other activities.</p>
</li>
<li><p>The following is an example. Let's say I have a file with column names <code>p1, p2, p3, p4 and p5</code> and you want to rename these files. If you want to name the files manually, you can directly take a set variable activity to build mapping like below:</p>
</li>
</ul>
<pre><code>{
    &quot;type&quot;:&quot;TabularTranslator&quot;,
    &quot;mappings&quot;:
    [
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;p1&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;Province&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;p2&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;Division&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;p3&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;County&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;p4&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;District&quot; }
        },
        {
            &quot;source&quot;: { &quot;name&quot;: &quot;p5&quot; },
            &quot;sink&quot;: { &quot;name&quot;: &quot;Municipality&quot; }
        }
    ]
}
</code></pre>
<ul>
<li>If you have a file with a column called <code>column_names</code> with new column names like below. Look up is used to get the names of new columns.</li>
</ul>
<p><img src=""https://i.imgur.com/clbjXYg.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have used another look up with only one row and deselected first row as header to get header from data file. The file data is as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/I3xibRj.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then I have used combination of iterations and variable activities to get desired results. The following is the pipeline JSON for the same.</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;pipeline3&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;new column names&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;cols&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;old column names&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;data&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;new column names&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    },
                    {
                        &quot;activity&quot;: &quot;old column names&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@range(0,5)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Append variable1&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Set variable1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;tp&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@json(variables('map'))&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Set variable1&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;map&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;{\&quot;source\&quot;: {\&quot;name\&quot;: \&quot;@{activity('old column names').output.firstRow[concat('Prop_',item())]}\&quot;},\&quot;sink\&quot;: {\&quot;name\&quot;: \&quot;@{activity('new column names').output.value[item()]['new_columns']}\&quot;}}&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Set variable2&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;final_mapping&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;{\&quot;type\&quot;: \&quot;TabularTranslator\&quot;,\&quot;mappings\&quot;: @{variables('tp')}}&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable2&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;value&quot;: &quot;@json(variables('final_mapping'))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;variables&quot;: {
            &quot;current item&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;tp&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;map&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;final_mapping&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<ul>
<li>The following is how the final file looks like:</li>
</ul>
<p><img src=""https://i.imgur.com/vx3tslU.png"" alt=""enter image description here"" /></p>
<ul>
<li>To change the column name only based on your condition, you can simply use if conditional to check. There are also be external ways to build the dynamic mapping JSON as mentioned by <strong>@wbob</strong>.</li>
</ul>
"
"75867112","Can I get all tables dynamically from Odata source to azure SQL server using data factory?","<p>How can get multiple tables from Odata source using azure data factory?</p>
<p>Can I get multiple tables from Odata source?</p>
<p><a href=""https://i.stack.imgur.com/eWtC9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eWtC9.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/eMweo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eMweo.png"" alt=""how can i take pathname dynamically"" /></a></p>
","<azure><azure-data-factory>","2023-03-28 13:46:02","118","0","1","75887644","<p>The below method only works when you have list of tables that you want to copy from Odata source.</p>
<p>Here I have given the sample demonstration using SQL dataset as source. You can follow the same for your OData dataset.</p>
<p>First create a dataset parameter for the table name.</p>
<p><img src=""https://i.imgur.com/IOsVG9b.png"" alt=""enter image description here"" /></p>
<p>Now, go to your OData dataset and click on edit in Path. In dynamic content give parameter <code>@dataset().source_table_name</code>.</p>
<p><img src=""https://i.imgur.com/TIZsKzP.png"" alt=""enter image description here"" /></p>
<p>Do the same for sink dataset also.</p>
<p>Here, I have taken list of table names in an array parameter.</p>
<p><img src=""https://i.imgur.com/fZUCV4f.png"" alt=""enter image description here"" /></p>
<p>Give this to ForEach activity and check on Sequential in it.</p>
<p>Inside ForEach use copy activity. You can give the <code>@item()</code> value to the dataset parameter in source like below.</p>
<p><img src=""https://i.imgur.com/WZ3caFA.png"" alt=""enter image description here"" /></p>
<p>Do the same in sink of copy activity and make sure you check on <strong>Auto create table</strong> in the <strong>Table options</strong> in it.</p>
<p>By this way you can copy multiple tables from your source to SQL database.</p>
"
"75866037","incrementally copy multiple files from blob Storage to Azure SQL Database","<p>I am facing an issue while performing incremental copy of multiple files from blob storage to azure SQL Database. I just want the recent files to get copied in the database after the full load has happened. I have used the below you tube video for this load :</p>
<p><a href=""https://www.youtube.com/watch?v=36UrhwoOKUk"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=36UrhwoOKUk</a></p>
<p>when I have 3 files in the blob storage with same last modified time, ideally all three should get copied to database but only the last file in the list gets copied, For e.g. Please check the image below</p>
<p><a href=""https://i.stack.imgur.com/JCALo.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Here There are two files with same time which are marked with red, but while incremental copy only the second one gets copied. I am not sue where I am going wrong. Can someone guide</p>
<p><strong>Here are the steps in ADF in order to skip the video:</strong></p>
<ol>
<li>Get Metadata1 activity to get the list of child items and these are then passes to for each loop .</li>
<li>Inside a for each loop there is Metadata 2 acitivty which had dynamic dataset and returns the field list for Item Name and Last Modified. This Metadata 2 is then connected to a If confdition acitivity where I am using :</li>
</ol>
<p>@greater(activity('Get Metadata2').output.lastModified,variables('PreviousModifiedDate'))</p>
<p>If this expression is true then there are two set variable activities in it.</p>
<p>Note :variables('PreviousModifiedDate') comes from the variables created for the pipeline, I have crearted LatestFile and PreviousModifiedDate as variables and a defautlt value of '1753-01-01 00:00:00.000' is being set as value for PreviousModifiesDate)</p>
<ol start=""3"">
<li>The first set variable is set to PreviousModifiedDate and second set variable is set to LatestFile.</li>
<li>the For each activity s connected to copy data where source is blob storage (dynamic) and sink is Azure SQL Database.</li>
</ol>
","<azure-pipelines><azure-sql-database><azure-blob-storage><azure-data-factory><azure-storage-account>","2023-03-28 12:04:01","107","-1","1","75867487","<p>I am not sure why is it not working, but I would like to suggest another approach.
Use &quot;filter by last modified&quot; in copy activity to get the files to read.
<a href=""https://i.stack.imgur.com/cQlPd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cQlPd.png"" alt=""copy activity"" /></a>
You can hold the last modified date in a table in SQL and pass it as a variable to the copy activity.
Leave modifiedDatetimeEnd empty to get all the files older than modifiedDatetimeStart.</p>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage?tabs=data-factory#copy-activity-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage?tabs=data-factory#copy-activity-properties</a></p>
"
"75865856","Getting Error Code 22853 while connecting local system to Azure data factory using Self hosted Integration Runtime","<p>I have installed Integration runtime in my local system and tried to connect to azure datafactory. But Getting Error code 22853 and connection getting failed.</p>
<p>Thanks for help in advance.</p>
<p><a href=""https://i.stack.imgur.com/Z1xcZ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Could you please help me to find out the reason and its corresponding solution.</p>
","<azure><azure-data-factory>","2023-03-28 11:47:18","62","0","1","75873981","<p>I tried and got similar error.</p>
<p><img src=""https://i.imgur.com/QA5TDEJ.png"" alt=""enter image description here"" /></p>
<p>The problem is because you are using the latest version of integration runtime.</p>
<p><strong>To resolve this there are three ways:</strong></p>
<ol>
<li>Downgrade to version <strong>IntegrationRuntime_5.21.8277.2.msi</strong>.</li>
<li>Instead of using a locally mapped name, they also advised using a completely qualified name in the connected service that directs users to the drive address (host). To put it another way: use <strong>\servername\C$\folder</strong> instead of <strong>C:\folder</strong>. You will get server name by typing <strong><code>hostname</code></strong> in command prompt of your pc as shown in below:
<img src=""https://i.imgur.com/WcPGAUQ.png"" alt=""enter image description here"" /></li>
<li>check your username and password is correct.</li>
</ol>
<p><em><strong>Connection successful:</strong></em></p>
<p><img src=""https://i.imgur.com/yD7pP6W.png"" alt=""enter image description here"" /></p>
"
"75864494","Remove oldest record in SQL table when there is more than 1 record with the same name","<p>I've a Pipeline in Synapse which syncs data from a source SQL database to a destination SQL database. As soon as the copy data activity is done it inserts a record (with a stored procedure) in a log table.</p>
<p>This table is used to sync data incremental. As soon as I start my PipeLine again it will compare the date in this log table with the DateChanged date of the source table so that it knows which records to sync.</p>
<p>At the end of the PipeLine run I would like to delete the record with the oldest TransferTime. So that I keep my table clean and small. But I only want to do this when there are more than 1 record for the table. Just to prevent that my whole pipeline fails because it can't find a last TransferTime, so it doesn't know which records are changed synce last run.</p>
<p>In the example below I want to delete the oldest records of Table A and Table B which have the TransferTime 2023-03-28 10:00.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TableName</th>
<th>TransferTime</th>
</tr>
</thead>
<tbody>
<tr>
<td>Table A</td>
<td>2023-03-28 10:00</td>
</tr>
<tr>
<td>Table A</td>
<td>2023-03-28 10:15</td>
</tr>
<tr>
<td>Table B</td>
<td>2023-03-28 10:00</td>
</tr>
<tr>
<td>Table B</td>
<td>2023-03-28 10:15</td>
</tr>
<tr>
<td>Table C</td>
<td>2023-03-28 10:00</td>
</tr>
</tbody>
</table>
</div>","<sql><sql-server><azure-data-factory>","2023-03-28 09:29:14","52","-1","2","75864535","<p>You can use <code>EXISTS</code> to check rows before deletion:</p>
<pre class=""lang-sql prettyprint-override""><code>DELETE FROM Table1 t1
WHERE
    exists (
        select
            1
        from
            TABLE1 t2
        where
            t2.TableName = t1.TableName
            and t2.TransferTime &gt; t1.TransferTime
    )
</code></pre>
<p>This query deletes row if exists row with the same <code>TableName</code> but bigger <code>TransferTime</code>.</p>
"
"75864494","Remove oldest record in SQL table when there is more than 1 record with the same name","<p>I've a Pipeline in Synapse which syncs data from a source SQL database to a destination SQL database. As soon as the copy data activity is done it inserts a record (with a stored procedure) in a log table.</p>
<p>This table is used to sync data incremental. As soon as I start my PipeLine again it will compare the date in this log table with the DateChanged date of the source table so that it knows which records to sync.</p>
<p>At the end of the PipeLine run I would like to delete the record with the oldest TransferTime. So that I keep my table clean and small. But I only want to do this when there are more than 1 record for the table. Just to prevent that my whole pipeline fails because it can't find a last TransferTime, so it doesn't know which records are changed synce last run.</p>
<p>In the example below I want to delete the oldest records of Table A and Table B which have the TransferTime 2023-03-28 10:00.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TableName</th>
<th>TransferTime</th>
</tr>
</thead>
<tbody>
<tr>
<td>Table A</td>
<td>2023-03-28 10:00</td>
</tr>
<tr>
<td>Table A</td>
<td>2023-03-28 10:15</td>
</tr>
<tr>
<td>Table B</td>
<td>2023-03-28 10:00</td>
</tr>
<tr>
<td>Table B</td>
<td>2023-03-28 10:15</td>
</tr>
<tr>
<td>Table C</td>
<td>2023-03-28 10:00</td>
</tr>
</tbody>
</table>
</div>","<sql><sql-server><azure-data-factory>","2023-03-28 09:29:14","52","-1","2","75864561","<p>Exists provides one canonical way to do this.  Given that you are using SQL Server, you could also use a deletable CTE here:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH cte AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY TableName
                                 ORDER BY TransferTime DESC) rn
    FROM yourTable
)

DELETE
FROM cte
WHERE rn &gt; 1;
</code></pre>
"
"75862049","Easily access notebook output run in Synapse pipeline","<p>I developed a synapse notebook which does data quality checks, visualizes null values and a couple of other charts I need to monitor data quality.</p>
<p>I created a pipeline in Synapse to run the notebook automatically every day. The way I can check the notebook after the run is to go in the Pipeline Runs, click on the Pipeline, click on Activity Name and then visualize the notebook output.</p>
<p>Is there a better way to check the notebook daily? Like a single bookmark for example instead of checking every single activity run with multiple clicks?</p>
<p>Or would it be possible to send the html output of the notebook by mail?</p>
<p>Thank you for your kind help.</p>
","<azure-data-factory><azure-synapse>","2023-03-28 03:13:54","121","0","1","75866129","<blockquote>
<p>would it be possible to send the html output of the notebook by mail?</p>
</blockquote>
<p>it is possible to send the html output of the notebook by mail.</p>
<ul>
<li>To achieve that you have to store your html code into a variable and the pass that variable to <code>mssparkutils.notebook.exit()</code> as shown below.
<img src=""https://i.imgur.com/2GhmFYB.png"" alt=""enter image description here"" /></li>
<li>Then run the notebook in pipeline. it will exit the variable as exit value.
<img src=""https://i.imgur.com/GZHE97v.png"" alt=""enter image description here"" /></li>
<li>Then store this exit value in a variable using <code>@activity('Notebook 1').output.status.Output.result.exitValue</code>
<img src=""https://i.imgur.com/zMntd0i.png"" alt=""enter image description here"" /></li>
<li>Then to send an email you need to create a logic app to create it you can follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-send-email#create-the-email-workflow-in-your-logic-app"" rel=""nofollow noreferrer"">document</a>. but little change is there between HTTP request and send email you need to add 2 more things you need to initialize and set the variable.
-- Http request settings:
<img src=""https://i.imgur.com/ca3aJUF.png"" alt=""enter image description here"" />
-- Initialize variable
<img src=""https://i.imgur.com/8O2X4kJ.png"" alt=""enter image description here"" />
-- Set variable
<img src=""https://i.imgur.com/R4hYNzQ.png"" alt=""enter image description here"" />
-- Send an email v2 settings
<img src=""https://i.imgur.com/EuA1C24.png"" alt=""enter image description here"" /></li>
</ul>
<p><em><strong>Output:</strong></em></p>
<p><img src=""https://i.imgur.com/j23inFi.png"" alt=""enter image description here"" /></p>
"
"75861119","Complex JSON in ADF that has me scratching my head","<p>So, my API payload comes looking something like this</p>
<p><a href=""https://i.stack.imgur.com/jTRC0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jTRC0.png"" alt=""enter image description here"" /></a></p>
<p>The Json looks like this, sorry for the blackouts, but I don't know any of you and it has endpoints and peoples names. :)</p>
<p><a href=""https://i.stack.imgur.com/0OqS1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0OqS1.png"" alt=""enter image description here"" /></a></p>
<p>How do I get this into SQLDB through ADF? I've tried flattening down to individual rows and columns ect. Can't seem to get ADF in dataflow to put it together coherently.</p>
<p>The data is basically a Next page API with Columns and Rows for 25 records over 401 pages. :|</p>
<p>Any help would be great.</p>
<p>Tried Pivot and all the fun stuff, but it still won't transform correctly.
Code Snippet that repeats 401 times:</p>
<pre><code>`[
  {
    &quot;columns&quot;: [
      &quot;Unique ID&quot;,
      &quot;Record Organization&quot;,
      &quot;Record Location&quot;,
      &quot;Created At&quot;,
      &quot;Updated At&quot;,
      &quot;Workflow&quot;,
      &quot;Indicator Set: Incident Status&quot;,
      &quot;Indicator Set: Type of Incident&quot;,
      &quot;Indicator Set: Recordable Incidents&quot;,
      &quot;Indicator Set: Loss Work Time&quot;,
      &quot;Indicator Set: Body Part Injured by Incident&quot;,
      &quot;Indicator Set: Cause of Incident&quot;,
      &quot;Indicator Set: Classification of Incident&quot;,
      &quot;Created By&quot;,
      &quot;Link to Record&quot;
    ],
    &quot;rows&quot;: [
      [
        &quot;INC0502&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson&quot;,
        &quot;04/01/2020 01:13 PM&quot;,
        &quot;08/12/2021 05:11 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Randy&quot;,
        &quot;https://company.us.donesafe.com/module_records/502&quot;
      ],
      [
        &quot;INC0506&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson&quot;,
        &quot;04/01/2020 02:49 PM&quot;,
        &quot;10/14/2021 10:48 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Joni&quot;,
        &quot;https://company.us.donesafe.com/module_records/506&quot;
      ],
      [
        &quot;INC0507&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/01/2020 06:57 PM&quot;,
        &quot;10/09/2021 04:14 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/507&quot;
      ],
      [
        &quot;INC0508&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/01/2020 11:39 PM&quot;,
        &quot;08/11/2021 11:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        &quot;Dust particle&quot;,
        null,
        &quot;Fili&quot;,
        &quot;https://company.us.donesafe.com/module_records/508&quot;
      ],
      [
        &quot;INC0509&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson 3&quot;,
        &quot;04/02/2020 12:01 AM&quot;,
        &quot;08/12/2021 08:41 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Kyle&quot;,
        &quot;https://company.us.donesafe.com/module_records/509&quot;
      ],
      [
        &quot;INC0510&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/02/2020 08:48 PM&quot;,
        &quot;10/09/2021 04:14 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/510&quot;
      ],
      [
        &quot;INC0511&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/02/2020 11:48 PM&quot;,
        &quot;08/11/2021 11:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Jennifer&quot;,
        &quot;https://company.us.donesafe.com/module_records/511&quot;
      ],
      [
        &quot;INC0512&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Sumner&quot;,
        &quot;04/04/2020 12:21 AM&quot;,
        &quot;08/06/2021 11:17 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Laura&quot;,
        &quot;https://company.us.donesafe.com/module_records/512&quot;
      ],
      [
        &quot;INC0513&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/04/2020 12:51 AM&quot;,
        &quot;08/12/2021 08:41 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/513&quot;
      ],
      [
        &quot;INC0514&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/04/2020 03:38 PM&quot;,
        &quot;08/12/2021 08:42 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Andy&quot;,
        &quot;https://company.us.donesafe.com/module_records/514&quot;
      ],
      [
        &quot;INC0515&quot;,
        &quot;International Freight Forwarding&quot;,
        &quot;Chicago&quot;,
        &quot;04/06/2020 04:37 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Kevin&quot;,
        &quot;https://company.us.donesafe.com/module_records/515&quot;
      ],
      [
        &quot;INC0516&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/07/2020 12:26 AM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        &quot;Push / Pulling&quot;,
        null,
        &quot;Fili&quot;,
        &quot;https://company.us.donesafe.com/module_records/516&quot;
      ],
      [
        &quot;INC0519&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Sumner&quot;,
        &quot;04/07/2020 02:43 PM&quot;,
        &quot;10/14/2021 10:48 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        &quot;Injury/Illness&quot;,
        &quot;Yes&quot;,
        null,
        &quot;Head/Face/Eyes&quot;,
        &quot;Struck by object&quot;,
        &quot;Restricted Duty&quot;,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/519&quot;
      ],
      [
        &quot;INC0520&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Sumner&quot;,
        &quot;04/07/2020 06:39 PM&quot;,
        &quot;01/09/2023 10:29 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        &quot;Injury/Illness&quot;,
        &quot;Yes&quot;,
        &quot;Yes&quot;,
        &quot;Hand/Wrist&quot;,
        &quot;Other&quot;,
        &quot;Loss Work Time&quot;,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/520&quot;
      ],
      [
        &quot;INC0521&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Sumner&quot;,
        &quot;04/07/2020 07:44 PM&quot;,
        &quot;01/09/2023 11:47 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        &quot;Injury/Illness&quot;,
        &quot;Yes&quot;,
        &quot;Yes&quot;,
        null,
        &quot;Struck by PIT&quot;,
        &quot;Loss Work Time&quot;,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/521&quot;
      ],
      [
        &quot;INC0522&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/07/2020 10:29 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/522&quot;
      ],
      [
        &quot;INC0523&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/07/2020 10:47 PM&quot;,
        &quot;10/09/2021 04:15 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/523&quot;
      ],
      [
        &quot;INC0525&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/08/2020 02:34 PM&quot;,
        &quot;08/06/2021 11:16 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Abel&quot;,
        &quot;https://company.us.donesafe.com/module_records/525&quot;
      ],
      [
        &quot;INC0526&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/08/2020 11:31 PM&quot;,
        &quot;08/11/2021 11:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Jennifer&quot;,
        &quot;https://company.us.donesafe.com/module_records/526&quot;
      ],
      [
        &quot;INC0527&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson 3&quot;,
        &quot;04/09/2020 03:39 AM&quot;,
        &quot;08/12/2021 08:41 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Kyle&quot;,
        &quot;https://company.us.donesafe.com/module_records/527&quot;
      ],
      [
        &quot;INC0528&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson 3&quot;,
        &quot;04/09/2020 09:34 AM&quot;,
        &quot;08/12/2021 05:08 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;Yes&quot;,
        null,
        null,
        null,
        null,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/528&quot;
      ],
      [
        &quot;INC0529&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson 3&quot;,
        &quot;04/09/2020 10:06 AM&quot;,
        &quot;08/12/2021 05:08 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        &quot;Injury/Illness&quot;,
        &quot;Yes&quot;,
        null,
        null,
        null,
        &quot;Restricted Duty&quot;,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/529&quot;
      ],
      [
        &quot;INC0530&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson 3&quot;,
        &quot;04/09/2020 10:32 AM&quot;,
        &quot;08/12/2021 05:08 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;Yes&quot;,
        null,
        null,
        null,
        null,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/530&quot;
      ],
      [
        &quot;INC0532&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/09/2020 12:51 PM&quot;,
        &quot;10/09/2021 04:15 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;Yes&quot;,
        null,
        null,
        null,
        null,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/532&quot;
      ],
      [
        &quot;INC0533&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/09/2020 01:40 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;Yes&quot;,
        null,
        null,
        null,
        null,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/533&quot;
      ]
    ]
  },
  {
    &quot;columns&quot;: [
      &quot;Unique ID&quot;,
      &quot;Record Organization&quot;,
      &quot;Record Location&quot;,
      &quot;Created At&quot;,
      &quot;Updated At&quot;,
      &quot;Workflow&quot;,
      &quot;Indicator Set: Incident Status&quot;,
      &quot;Indicator Set: Type of Incident&quot;,
      &quot;Indicator Set: Recordable Incidents&quot;,
      &quot;Indicator Set: Loss Work Time&quot;,
      &quot;Indicator Set: Body Part Injured by Incident&quot;,
      &quot;Indicator Set: Cause of Incident&quot;,
      &quot;Indicator Set: Classification of Incident&quot;,
      &quot;Created By&quot;,
      &quot;Link to Record&quot;
    ],
    &quot;rows&quot;: [
      [
        &quot;INC0534&quot;,
        &quot;Yusen Logistics Americas&quot;,
        &quot;Carson&quot;,
        &quot;04/09/2020 04:41 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Esmeralda&quot;,
        &quot;https://company.us.donesafe.com/module_records/534&quot;
      ],
      [
        &quot;INC0536&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/09/2020 05:47 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/536&quot;
      ],
      [
        &quot;INC0537&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/10/2020 02:07 AM&quot;,
        &quot;08/12/2021 08:41 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Michael&quot;,
        &quot;https://company.us.donesafe.com/module_records/537&quot;
      ],
      [
        &quot;INC0538&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/10/2020 03:37 AM&quot;,
        &quot;10/09/2021 04:14 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Jennifer&quot;,
        &quot;https://company.us.donesafe.com/module_records/538&quot;
      ],
      [
        &quot;INC0539&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/10/2020 01:47 PM&quot;,
        &quot;08/12/2021 08:42 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Andy&quot;,
        &quot;https://company.us.donesafe.com/module_records/539&quot;
      ],
      [
        &quot;INC0541&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Rogers&quot;,
        &quot;04/13/2020 11:18 AM&quot;,
        &quot;08/11/2021 11:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Theresa&quot;,
        &quot;https://company.us.donesafe.com/module_records/541&quot;
      ],
      [
        &quot;INC0542&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/13/2020 11:19 AM&quot;,
        &quot;06/03/2021 03:15 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Chip&quot;,
        &quot;https://company.us.donesafe.com/module_records/542&quot;
      ],
      [
        &quot;INC0543&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Sumner&quot;,
        &quot;04/13/2020 01:31 PM&quot;,
        &quot;06/03/2021 03:15 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Steve&quot;,
        &quot;https://company.us.donesafe.com/module_records/543&quot;
      ],
      [
        &quot;INC0544&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/13/2020 03:05 PM&quot;,
        &quot;08/06/2021 11:17 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Joni&quot;,
        &quot;https://company.us.donesafe.com/module_records/544&quot;
      ],
      [
        &quot;INC0545&quot;,
        &quot;International Freight Forwarding&quot;,
        &quot;Los Angeles&quot;,
        &quot;04/13/2020 05:13 PM&quot;,
        &quot;08/06/2021 11:16 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        &quot;Injury/Illness&quot;,
        &quot;Yes&quot;,
        null,
        &quot;Back&quot;,
        &quot;Accident involving moving motor vehicle&quot;,
        &quot;Restricted Duty&quot;,
        &quot;Alex&quot;,
        &quot;https://company.us.donesafe.com/module_records/545&quot;
      ],
      [
        &quot;INC0546&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/13/2020 09:38 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Fili&quot;,
        &quot;https://company.us.donesafe.com/module_records/546&quot;
      ],
      [
        &quot;INC0562&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/13/2020 10:47 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/562&quot;
      ],
      [
        &quot;INC0563&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/14/2020 12:29 AM&quot;,
        &quot;10/09/2021 04:14 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/563&quot;
      ],
      [
        &quot;INC0564&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/14/2020 04:48 AM&quot;,
        &quot;08/11/2021 11:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Fili&quot;,
        &quot;https://company.us.donesafe.com/module_records/564&quot;
      ],
      [
        &quot;INC0566&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/14/2020 10:46 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/566&quot;
      ],
      [
        &quot;INC0567&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/15/2020 12:06 AM&quot;,
        &quot;10/09/2021 04:15 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/567&quot;
      ],
      [
        &quot;INC0570&quot;,
        &quot;International Freight Forwarding&quot;,
        &quot;Atlanta&quot;,
        &quot;04/15/2020 01:41 PM&quot;,
        &quot;06/03/2021 03:15 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Charles&quot;,
        &quot;https://company.us.donesafe.com/module_records/570&quot;
      ],
      [
        &quot;INC0571&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson 3&quot;,
        &quot;04/15/2020 10:20 PM&quot;,
        &quot;08/12/2021 08:41 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Kyle&quot;,
        &quot;https://company.us.donesafe.com/module_records/571&quot;
      ],
      [
        &quot;INC0572&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/15/2020 10:58 PM&quot;,
        &quot;06/03/2021 03:15 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Nicholas&quot;,
        &quot;https://company.us.donesafe.com/module_records/572&quot;
      ],
      [
        &quot;INC0573&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/15/2020 11:23 PM&quot;,
        &quot;10/09/2021 04:13 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Juan&quot;,
        &quot;https://company.us.donesafe.com/module_records/573&quot;
      ],
      [
        &quot;INC0575&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Waco&quot;,
        &quot;04/16/2020 08:37 AM&quot;,
        &quot;09/16/2021 02:41 PM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Geoffrey&quot;,
        &quot;https://company.us.donesafe.com/module_records/575&quot;
      ],
      [
        &quot;INC0576&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson 3&quot;,
        &quot;04/16/2020 10:53 AM&quot;,
        &quot;09/15/2021 08:58 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Alejandro&quot;,
        &quot;https://company.us.donesafe.com/module_records/576&quot;
      ],
      [
        &quot;INC0577&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson&quot;,
        &quot;04/16/2020 11:37 AM&quot;,
        &quot;09/15/2021 08:58 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Alejandro&quot;,
        &quot;https://company.us.donesafe.com/module_records/577&quot;
      ],
      [
        &quot;INC0578&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Carson&quot;,
        &quot;04/16/2020 12:00 PM&quot;,
        &quot;09/15/2021 08:58 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Alejandro&quot;,
        &quot;https://company.us.donesafe.com/module_records/578&quot;
      ],
      [
        &quot;INC0580&quot;,
        &quot;Contract Logistics Group&quot;,
        &quot;Dominguez&quot;,
        &quot;04/16/2020 03:00 PM&quot;,
        &quot;09/15/2021 08:58 AM&quot;,
        &quot;Closed&quot;,
        &quot;Closed&quot;,
        null,
        &quot;No&quot;,
        null,
        null,
        null,
        null,
        &quot;Jesus  (WHSE Supervisor)&quot;,
        &quot;https://company.us.donesafe.com/module_records/580&quot;
      ]
    ]
  },'
</code></pre>
","<azure-data-factory>","2023-03-27 23:11:21","55","1","1","75863668","<ul>
<li>I have taken the response as a JSON file for demonstration. Use transformations as shown in the below dataflow JSON:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Json1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;derivedColumn1&quot;
                },
                {
                    &quot;name&quot;: &quot;select1&quot;
                },
                {
                    &quot;name&quot;: &quot;derivedColumn2&quot;
                },
                {
                    &quot;name&quot;: &quot;derivedColumn3&quot;
                },
                {
                    &quot;name&quot;: &quot;rank1&quot;
                },
                {
                    &quot;name&quot;: &quot;select2&quot;
                },
                {
                    &quot;name&quot;: &quot;union1&quot;
                },
                {
                    &quot;name&quot;: &quot;sort1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          columns as string[],&quot;,
                &quot;          rows as string[][]&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false,&quot;,
                &quot;     documentForm: 'documentPerLine') ~&gt; source1&quot;,
                &quot;source1 derive(rows = unfold(rows)) ~&gt; derivedColumn1&quot;,
                &quot;source1 select(mapColumn(&quot;,
                &quot;          columns&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select1&quot;,
                &quot;derivedColumn1 derive(rows = replace(replace(toString(rows),'[',''),']','')) ~&gt; derivedColumn2&quot;,
                &quot;select1 derive(id = 0,&quot;,
                &quot;          columns = replace(replace(toString(columns),'[',''),']','')) ~&gt; derivedColumn3&quot;,
                &quot;derivedColumn2 rank(asc(rows, true),&quot;,
                &quot;     output(id as long),&quot;,
                &quot;     dense: true) ~&gt; rank1&quot;,
                &quot;rank1 select(mapColumn(&quot;,
                &quot;          rows,&quot;,
                &quot;          id&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select2&quot;,
                &quot;select2, derivedColumn3 union(byName: false)~&gt; union1&quot;,
                &quot;union1 sort(asc(id, true)) ~&gt; sort1&quot;,
                &quot;sort1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     partitionFileNames:['output.csv'],&quot;,
                &quot;     umask: 0022,&quot;,
                &quot;     preCommands: [],&quot;,
                &quot;     postCommands: [],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     mapColumn(&quot;,
                &quot;          rows&quot;,
                &quot;     ),&quot;,
                &quot;     partitionBy('hash', 1)) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
<p><img src=""https://i.imgur.com/TqpaisP.png"" alt=""enter image description here"" /></p>
<ul>
<li>I am writing the above data to a csv file using the following dataset configurations:</li>
</ul>
<p><img src=""https://i.imgur.com/HMpr7j0.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, this generates the file as shown below. Read this file with first row as header to get the data in required format.</li>
</ul>
<p><img src=""https://i.imgur.com/tC0pEcs.png"" alt=""enter image description here"" /></p>
"
"75860490","How to pass or use value from Lookup to Data Flow?","<p>I have a pipeline working on ADF already, but I would like to try something by exporting Kusto data to ADLS instead of using the Copy activity (comparing performance as well).</p>
<p>So, is it possible to use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup activity value</a> to a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data Flow mapping</a>?</p>
<p>Lookup Kusto query:</p>
<pre><code>let pickDate = datetime(2023-03-01);
let Conversion = datatable(Decimal: int, HexCode: string, Binary: string)
[
    0, '0', '0000',
    1, '1', '0001',
    2, '2', '0010',
    3, '3', '0011',
    4, '4', '0100',
    5, '5', '0101',
    6, '6', '0110',
    7, '7', '0111',
    8, '8', '1000',
    9, '9', '1001',
    10, 'a', '1010',
    11, 'b', '1011',
    12, 'c', '1100',
    13, 'd', '1101',
    14, 'e', '1110',
    15, 'f', '1111',
];
let data = materialize(range Dates from pickDate to pickDate step 1d
| extend Dates = format_datetime(Dates, 'yyyy-MM-dd')
| extend JoinColumn = 1
| join kind=inner
    (
    Conversion
    | extend JoinColumn = 1
    )
    on JoinColumn
| project Dates, HexCode);
let HexCodes = data 
| summarize make_set(HexCode);
data
| extend SubHexCode = toscalar(HexCodes)
| mv-expand SubHexCode
</code></pre>
<p>Output:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Dates</th>
<th>HexCode</th>
<th>SubHexCode</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023-03-01</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2023-03-01</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>2023-03-01</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>What I need is to use the output from the Lookup query in my data flow instead of my pipeline, so the query will be similar to this:</p>
<pre><code>set notruncation;
let HexCode = &quot;@{item().HexCode}@{item().SubHexCode}&quot;;
Test
| where tolower(ContainerId) startswith HexCode
| join kind=inner (
    Test2
    | extend ContainerId = VmId
    | where startofday(TIMESTAMP) == startofday(datetime(@{item().Dates}))
    | where tolower(ContainerId) startswith HexCode
    )
    on ContainerId
| project x, y, z
</code></pre>
<p><a href=""https://i.stack.imgur.com/6MAyj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6MAyj.png"" alt=""Current pipeline"" /></a></p>
<p>In the image above, I would change the CopyData actitivy to the DataFlow activity, and in the dataflow I would use a Kusto query with variables from Lookup value.</p>
<p><a href=""https://i.stack.imgur.com/T7eNI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T7eNI.png"" alt=""Data flow"" /></a></p>
<p><strong>--- UPDATE ---</strong></p>
<p>Following @Rakesh answer, I changed my pipeline to have parameters and enabled the option &quot;expression&quot; for each parameter, but I still having troubles.</p>
<p>Also I dind't understand how can I use string interpolation to see the query output. This is my current pipeline:</p>
<p>High level:</p>
<p><a href=""https://i.stack.imgur.com/o1YsH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o1YsH.png"" alt=""High level pipeline"" /></a></p>
<p>This is the parameters on the DataFlow activity:</p>
<p>I enabled the expressions for each parameter.</p>
<p><a href=""https://i.stack.imgur.com/oF6bH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oF6bH.png"" alt=""Data Flow activity"" /></a></p>
<p>Inside the dataflow, this is the parameters set:</p>
<p><a href=""https://i.stack.imgur.com/kqlrK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kqlrK.png"" alt=""Inside data flow"" /></a></p>
<p>This is the Kusto query with parameters:</p>
<p>I am doing <code>let HexCode = &quot;$HexCode$SubHexCode&quot;;</code> because I need &quot;00&quot; from my lookup value. And using <code>| where startofday(TIMESTAMP) == startofday(datetime($Dates))</code> for the datetime.</p>
<p>The error I am still getting from the query below:</p>
<p><a href=""https://i.stack.imgur.com/i7YbS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i7YbS.png"" alt=""Error"" /></a></p>
<p><strong>-- UPDATE 2 --</strong></p>
<p>Sample data for the Kusto:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>VmId</th>
<th>ContainerId</th>
<th>SampleCount</th>
<th>AverageCounterValue</th>
<th>CounterName</th>
<th>MinCounterValue</th>
<th>MaxCounterValue</th>
<th>PreciseTimeStamp</th>
</tr>
</thead>
<tbody>
<tr>
<td>11c1b7c1-d1b1-11fb-11a1-0c711fe111c0</td>
<td>11c1b7c1-d1b1-11fb-11a1-0c711fe111c0</td>
<td>20</td>
<td>13.57497519</td>
<td>Percentage CPU</td>
<td>3.420510085</td>
<td>26.68882703</td>
<td>2023-03-08T05:04:59.9999999Z</td>
</tr>
<tr>
<td>11c1b7c1-d1b1-11fb-11a1-0c711fe111c0</td>
<td>11c1b7c1-d1b1-11fb-11a1-0c711fe111c0</td>
<td>20</td>
<td>10.97057026</td>
<td>Percentage CPU</td>
<td>6.853699734</td>
<td>20.73203381</td>
<td>2023-03-08T05:04:59.9999999Z</td>
</tr>
<tr>
<td>11c1b7c1-d1b1-11fb-11a1-0c711fe111c0</td>
<td>11c1b7c1-d1b1-11fb-11a1-0c711fe111c0</td>
<td>40</td>
<td>4.061182971</td>
<td>Percentage CPU</td>
<td>3.075506647</td>
<td>5.650666854</td>
<td>2023-03-08T09:54:59.9999999Z</td>
</tr>
</tbody>
</table>
</div>","<azure-data-factory>","2023-03-27 21:24:04","125","0","1","75866342","<p>You can use dataflow parameters for this.</p>
<p>This is the output for lookup of your query:</p>
<p><img src=""https://i.imgur.com/0aNZJqU.png"" alt=""enter image description here"" /></p>
<p>As you want to use the above values in dataflow in each iteration, create the dataflow parameters for those like below.</p>
<p><img src=""https://i.imgur.com/Jy8FqeB.png"" alt=""enter image description here"" /></p>
<p>Pass the <code>@item().Date</code>, <code>@item().HexCode</code> and <code>@item().SubHexCode</code> to dataflow parameters inside ForEach like below.</p>
<p><img src=""https://i.imgur.com/P8W1tQS.png"" alt=""enter image description here"" /></p>
<p>You can use the above the dataflow parameters in your kustos query by using <code>$</code> before the parameter like <code>$Dates</code>,<code>$HexCode</code>,<code>$SubHexCode</code> as per your requirement.</p>
"
"75858690","ADF Copy Data Activity Creates Multiple Lines in CSV","<p>I am trying to create a data flow where i can remove the line break from a csv file and because of it I'm getting an error whenever I tried INSERTING those values in a table.</p>
<p>The problem is I have a CSV file with &quot;Comments&quot; column where the user inputs paragraph like values in a cell for example:</p>
<p>&quot;Caller received something</p>
<ol>
<li>Mail</li>
<li>Email&quot;</li>
</ol>
<p>So I need to put this in a BLOB container in Azure where I can use Data flow to remove line breaks but the problem is whenever I use COPY DATA activity to move the file from a directory to BLOB Container, it separates this type of comments into 3 rows. I think the reason here is the row delimeter setting in COPY DATA.</p>
<p><a href=""https://i.stack.imgur.com/R0tzF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R0tzF.jpg"" alt=""enter image description here"" /></a></p>
<p>In this image I set it as No delimeterr but it gives me an error like this.</p>
<p><a href=""https://i.stack.imgur.com/W7EeX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W7EeX.jpg"" alt=""enter image description here"" /></a></p>
<p>I really need to find an option to put the file using ADF to Blob as it is without changing any values or structures from the CSV. Any suggestions?</p>
<p>Here is the sample data im trying to insert:</p>
<p><a href=""https://i.stack.imgur.com/f1c8t.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f1c8t.jpg"" alt=""enter image description here"" /></a></p>
<p>In the 2nd row you can notice there's a 3 line comment in it.
The only reason I'm moving this file to BLOB is so that I can use data flows to remove the line breaks in the comments column and make each comments look like a single sentence. Are there are any approach and suggestions to do this? I've tried doing this <a href=""https://learn.microsoft.com/en-us/answers/questions/91071/how-to-remove-line-breaks-from-excel-file-in-adf"" rel=""nofollow noreferrer"">solution</a> but it doesn't work on CSV files.</p>
","<azure><csv><azure-data-factory>","2023-03-27 17:29:54","114","0","3","75862502","<p>If you want ADF to copy the files to blob storage as is, without changing them, use the binary data format in your source and sink datasets.
<a href=""https://i.stack.imgur.com/aSJRw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aSJRw.png"" alt=""enter image description here"" /></a></p>
"
"75858690","ADF Copy Data Activity Creates Multiple Lines in CSV","<p>I am trying to create a data flow where i can remove the line break from a csv file and because of it I'm getting an error whenever I tried INSERTING those values in a table.</p>
<p>The problem is I have a CSV file with &quot;Comments&quot; column where the user inputs paragraph like values in a cell for example:</p>
<p>&quot;Caller received something</p>
<ol>
<li>Mail</li>
<li>Email&quot;</li>
</ol>
<p>So I need to put this in a BLOB container in Azure where I can use Data flow to remove line breaks but the problem is whenever I use COPY DATA activity to move the file from a directory to BLOB Container, it separates this type of comments into 3 rows. I think the reason here is the row delimeter setting in COPY DATA.</p>
<p><a href=""https://i.stack.imgur.com/R0tzF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R0tzF.jpg"" alt=""enter image description here"" /></a></p>
<p>In this image I set it as No delimeterr but it gives me an error like this.</p>
<p><a href=""https://i.stack.imgur.com/W7EeX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W7EeX.jpg"" alt=""enter image description here"" /></a></p>
<p>I really need to find an option to put the file using ADF to Blob as it is without changing any values or structures from the CSV. Any suggestions?</p>
<p>Here is the sample data im trying to insert:</p>
<p><a href=""https://i.stack.imgur.com/f1c8t.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f1c8t.jpg"" alt=""enter image description here"" /></a></p>
<p>In the 2nd row you can notice there's a 3 line comment in it.
The only reason I'm moving this file to BLOB is so that I can use data flows to remove the line breaks in the comments column and make each comments look like a single sentence. Are there are any approach and suggestions to do this? I've tried doing this <a href=""https://learn.microsoft.com/en-us/answers/questions/91071/how-to-remove-line-breaks-from-excel-file-in-adf"" rel=""nofollow noreferrer"">solution</a> but it doesn't work on CSV files.</p>
","<azure><csv><azure-data-factory>","2023-03-27 17:29:54","114","0","3","75862575","<p>Since the data is wrapped with quotes, you can use <code>Double quote (&quot;)</code> as quote character and <code>default (\r\n)</code> as row delimiter.</p>
<ul>
<li>Sample data with line breaks is taken.
<img src=""https://i.imgur.com/43R5eG3.png"" alt=""enter image description here"" /></li>
</ul>
<p>When no quote character is given, data is divided into three rows.</p>
<p><img src=""https://i.imgur.com/5jX9h1W.png"" alt=""enter image description here"" /></p>
<ul>
<li>CSV dataset configuration is given as in below image.</li>
</ul>
<pre><code>&quot;columnDelimiter&quot;: &quot;,&quot;,
&quot;escapeChar&quot;: &quot;\\&quot;,
&quot;quoteChar&quot;: &quot;\&quot;&quot;
</code></pre>
<p>When preview data is clicked, data is previewed in a single line itself.</p>
<p><img src=""https://i.imgur.com/wuAnzWw.png"" alt=""enter image description here"" /></p>
"
"75858690","ADF Copy Data Activity Creates Multiple Lines in CSV","<p>I am trying to create a data flow where i can remove the line break from a csv file and because of it I'm getting an error whenever I tried INSERTING those values in a table.</p>
<p>The problem is I have a CSV file with &quot;Comments&quot; column where the user inputs paragraph like values in a cell for example:</p>
<p>&quot;Caller received something</p>
<ol>
<li>Mail</li>
<li>Email&quot;</li>
</ol>
<p>So I need to put this in a BLOB container in Azure where I can use Data flow to remove line breaks but the problem is whenever I use COPY DATA activity to move the file from a directory to BLOB Container, it separates this type of comments into 3 rows. I think the reason here is the row delimeter setting in COPY DATA.</p>
<p><a href=""https://i.stack.imgur.com/R0tzF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R0tzF.jpg"" alt=""enter image description here"" /></a></p>
<p>In this image I set it as No delimeterr but it gives me an error like this.</p>
<p><a href=""https://i.stack.imgur.com/W7EeX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W7EeX.jpg"" alt=""enter image description here"" /></a></p>
<p>I really need to find an option to put the file using ADF to Blob as it is without changing any values or structures from the CSV. Any suggestions?</p>
<p>Here is the sample data im trying to insert:</p>
<p><a href=""https://i.stack.imgur.com/f1c8t.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f1c8t.jpg"" alt=""enter image description here"" /></a></p>
<p>In the 2nd row you can notice there's a 3 line comment in it.
The only reason I'm moving this file to BLOB is so that I can use data flows to remove the line breaks in the comments column and make each comments look like a single sentence. Are there are any approach and suggestions to do this? I've tried doing this <a href=""https://learn.microsoft.com/en-us/answers/questions/91071/how-to-remove-line-breaks-from-excel-file-in-adf"" rel=""nofollow noreferrer"">solution</a> but it doesn't work on CSV files.</p>
","<azure><csv><azure-data-factory>","2023-03-27 17:29:54","114","0","3","75979643","<p>I found an answer to this. So I still used Data Flows Derived Column to manipulate the comments but I used nested replace function to make sure all the possible line breaks are gonna be handled. Including trailing spaces before and after linebreaks.</p>
<blockquote>
<p>replace(replace(replace(replace(Comments, &quot;\n&quot;, &quot;&quot;),&quot;\&quot;, &quot;&quot;),&quot; \n&quot;,
&quot;&quot;),&quot;\n &quot;, &quot;&quot;)</p>
</blockquote>
"
"75857405","How do you set a custom message or a json in the succcess output of a Azure Data Factory pipeline?","<p>There is a rest API to query the status of a pipeline and when the pipeline completes all you get is a single value in a Json that says 'Success' . How do you append additional data to this message.</p>
<p>I have not tried as I dont have an idea.</p>
<p>So there is a Failure Activity for instance . You can set the flow to a Fail Activity and it allows to set a custom message in the error status of the pipeline. Similarly I dont see anything for success.</p>
","<azure><azure-data-factory>","2023-03-27 15:09:03","56","-1","1","75863678","<p><strong>There is a rest API to query the status of a pipeline and when the pipeline completes all you get is a single value in a Json that says 'Success' . How do you append additional data to this message.</strong></p>
<p>To achieve your scenario, we have the only option is to use set variable activity.  here you can append additional data to output of web activity.</p>
<ul>
<li>Here I created <strong>response</strong> variable with <code>string</code> type and passed value as <code>@concat('Pipeline status is',activity('Web1').output.response)</code> it will give you output as <strong>Pipeline status is Success.</strong>
<img src=""https://i.imgur.com/cpHBPHo.png"" alt=""enter image description here"" /></li>
</ul>
<blockquote>
<p>Fail Activity is the activity in Azure Data Factory use to purposefully fail the pipeline along with sending the custom error code and error message with it. there is no such thig for success.</p>
</blockquote>
"
"75856747","org.apache.parquet.schema.InvalidSchemaException:Cannot write a schema with an empty group","<p>I am extracting responses from an API in ADF, I used COPY activity source as REST API, and sink as ADLS Gen2 Parquet. When I debug the copy activity it fails for below reason.</p>
<p><a href=""https://i.stack.imgur.com/kHCf5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kHCf5.png"" alt=""enter image description here"" /></a></p>
<p>Here is the response JSON(sample) that I get from API upon request.
Response JSON in the format of</p>
<pre><code>{
    &quot;Regions&quot;: [
        {
            &quot;RCode&quot;: &quot;a&quot;,
            &quot;Name&quot;: &quot;b&quot;,
            &quot;DbCode&quot;: &quot;c&quot;,
            &quot;DisplayOrder&quot;: 1,
            &quot;HasAccess&quot;: false
        },
        {
            &quot;RCode&quot;: &quot;e&quot;,
            &quot;Name&quot;: &quot;f&quot;,
            &quot;DbCode&quot;: &quot;g&quot;,
            &quot;DisplayOrder&quot;: 2,
            &quot;HasAccess&quot;: false
        },
}
</code></pre>
<p>But the WEB activity(without saving to ADLS) is completed successfully. Can Someone help me understand what the issue is?</p>
","<api><azure-data-factory><parquet>","2023-03-27 14:08:31","60","0","1","75864574","<p>You need to give the mapping setting manually in the copy activity. Otherwise, you will get that error.</p>
<ul>
<li>Got the same error when mapping is not given.</li>
</ul>
<blockquote>
<p>ErrorCode=ParquetJavaInvocationException,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=An error occurred when invoking java, message: org.apache.parquet.schema.InvalidSchemaException:Cannot write a schema with an empty group: message adms_schema</p>
</blockquote>
<p><img src=""https://i.imgur.com/rJJzMN0.png"" alt=""enter image description here"" /></p>
<ul>
<li>In order to overcome this error, click the mapping tab in copy activity. Click <code>Import schemas</code>. Select <code>Advance editors</code> and click the <code>Region</code> array in collection reference and map the input column with output.</li>
</ul>
<p><img src=""https://i.imgur.com/dwoN4VZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>When the pipeline is run, It is executed without any error.</li>
<li>Similarly for delimited(csv) sink, when mapping is given, all records are copied. Otherwise, zero rows are copied to csv sink.</li>
</ul>
<p><strong>Reference</strong>: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""nofollow noreferrer"">Schema and data type mapping in copy activity - Azure Data Factory &amp; Azure Synapse | Microsoft Learn</a></p>
"
"75856371","how to get ADF pipeline historical data","<p>I want to retrieve adf pipeline data log of the month of January. But filtering is only available up to 45 days. I am not able get data before 45 days. How to get data log from January?</p>
<p>In the image you can see jan 1st to jan 31st is trying to be filtered but no data found</p>
<p><a href=""https://i.stack.imgur.com/MWIly.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MWIly.png"" alt=""enter image description here"" /></a></p>
<p>I've heard about data monitoring, azure data factory analytics. Will these help for getting historical log or does they start collecting from future only? If any of these works explain how to do the full process please.</p>
","<azure><azure-web-app-service><azure-data-factory>","2023-03-27 13:30:33","51","0","1","75857360","<p><a href=""https://i.stack.imgur.com/Ikowq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ikowq.png"" alt=""enter image description here"" /></a></p>
<p>Keeping Azure Data Factory metrics and pipeline-run data
Data Factory stores pipeline-run data for only 45 days. Use Azure Monitor if you want to keep that data for a longer time</p>
<p>for diagnostic settings :
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-configure-diagnostics"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-configure-diagnostics</a></p>
<p><a href=""https://i.stack.imgur.com/JMWYw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JMWYw.png"" alt=""enter image description here"" /></a></p>
"
"75855795","ADF LOOKUP not updating the required table","<p>I have a lookup activity in ADF pipeline. This activity truncating then inserting a value in a single record table.</p>
<p>In some random occasions the activity will succeed but the table still hold the old value even though the output shows me impacted rows = 1</p>
<p>SQL I use in the Lookup</p>
<pre class=""lang-sql prettyprint-override""><code>TRUNCATE TABLE [CRM_Mapping].[IncrementalLoadLock] 
INSERT INTO [CRM_Mapping].[IncrementalLoadLock] 
SELECT 0

SELECT @@RowCount AS ImpactedRow 
</code></pre>
<p>the output of this activity when it fails</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;firstRow&quot;: {
        &quot;ImpactedRow&quot;: 1
    },
    &quot;effectiveIntegrationRuntime&quot;: &quot;ManagedIntegrationRuntime (Australia East)&quot;,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;ManagedVNetIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;DIUHours&quot;
            }
        ]
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 1
    }
}
</code></pre>
<p>The only difference I noticed when this activity doesn't update the table (event though it succeeded) is the IntegragionRuntimeQueue changes from 0 to 1.</p>
","<sql><azure><azure-data-factory>","2023-03-27 12:33:20","45","0","1","75856897","<p>As Chen Hirsh suggested try to use a script activity instead of the lookup activity.</p>
<p>Any of the data sources enabled by data factory and Synapse pipelines can be used by lookup activity to obtain a dataset. A configuration file or database table's information is read and returned by a lookup action. Additionally, it provides the outcome of a query or saved process. A singleton number or a collection of characteristics can be the output. it is mainly focused on returning the output.</p>
<p>You can carry out routine tasks with Data Manipulation Language (DML) and Data Definition Language using the script action. (DDL). Users can enter, change, remove, and obtain data from the database using DML statements like enter, UPDATE, remove, and SELECT. A database manager can build, change, and delete database objects like tables, indexes, and users using DDL statements like build, ALTER, and DROP.</p>
<p><strong>Its returning you similar output:</strong>
<img src=""https://i.imgur.com/O7BgpFS.png"" alt=""enter image description here"" /></p>
"
"75852950","Query data in Azure DataBricks from Azure DataFactory","<p>I have an Azure DataFactory instance and I want to query data from Azure Databricks. We have unity catlog in Databricks. What are my options here? Best practice?</p>
<p>I haven't really tried it technically, I want to get pointed into the right direction before trying.</p>
","<azure-data-factory><azure-databricks>","2023-03-27 07:08:15","63","0","2","75853077","<ul>
<li>You can use the Databricks Notebook activity in Azure Data Factory to run a Databricks notebook against the Databricks jobs cluster.</li>
<li>The notebook can contain the code to extract data from the Databricks catalog and write it to a file or database. You can then use Azure Data Factory to copy the data from the file or database to your desired destination.</li>
</ul>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook#databricks-notebook-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook#databricks-notebook-activity</a></p>
<p><a href=""https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/azure-data-factory-and-azure-databricks-best-practices/ba-p/3074262#:%7E:text=ADF%20copy%20activities%20can%20ingest%20data%20from%20various,make%20sure%20to%20check%20out%20this%20blog%20post"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/azure-data-factory-and-azure-databricks-best-practices/ba-p/3074262#:~:text=ADF%20copy%20activities%20can%20ingest%20data%20from%20various,make%20sure%20to%20check%20out%20this%20blog%20post</a>.</p>
"
"75852950","Query data in Azure DataBricks from Azure DataFactory","<p>I have an Azure DataFactory instance and I want to query data from Azure Databricks. We have unity catlog in Databricks. What are my options here? Best practice?</p>
<p>I haven't really tried it technically, I want to get pointed into the right direction before trying.</p>
","<azure-data-factory><azure-databricks>","2023-03-27 07:08:15","63","0","2","75853464","<p>I want to add to @AnnuKumari answer,
that you can also use Databricks delta tables as a source without writing to a file first.
You can create a linked service in ADF to databricks:</p>
<p><a href=""https://i.stack.imgur.com/1aUpn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1aUpn.png"" alt=""databricks linked service"" /></a></p>
<p>See this documentation:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-databricks-delta-lake?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-databricks-delta-lake?tabs=data-factory</a></p>
"
"75852417","Synapse/ ADF - How to Truncate table if dynamic config colomn is True in pre-copy script","<p>I want to truncate tables where the value of my column isTrun in the config table is True.
The scripts need to run in the Sink pre-copy script of the copy activity.</p>
<p>The dataflow is the following:</p>
<p>The look-up activity looks up the config table. Then enters the for each loop and copy's the tables that are looked up. before the copy, there needs to be a truncate for the tables that have in the config table the value True in the isTrun column.</p>
<p>The problem that I am facing is that all my tries have invalid syntax.</p>
<p>I already tried the followings scripts:</p>
<pre><code>if(@item().isTrun = 1, TRUNCATE TABLE [@{item().targetSchema}].[@{item().targetTable}], '1=1')
</code></pre>
<p>and this script:</p>
<pre><code>TRUNCATE TABLE @{if(equals(item().isTrun, 'True'),'[@{item().targetSchema.item()}].[@{targetTable}]',' ')}
</code></pre>
<p>Thanks in advance!</p>
","<sql><azure><azure-synapse><azure-data-factory>","2023-03-27 05:41:38","88","0","2","75853234","<ul>
<li>Instead of trying to use string interpolation with if condition, you can directly use if function only. The following is the output of my lookup:</li>
</ul>
<p><img src=""https://i.imgur.com/7j5IwJS.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following is how you can do it. In the <code>pre-copy script</code> that you can use to achieve the requirement.</li>
</ul>
<pre><code>@{if(equals(item().isTrun,'True'),concat('TRUNCATE TABLE ',item().table),'')}
</code></pre>
<p><img src=""https://i.imgur.com/bq3Y0bp.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, when the <code>isTrun</code> value is True, then if condition checks and executes the truncate statement and ignores if the isTrun value is false. The following is table t2 (1 row in source and sink) after the above execution (isTrun is false).</li>
</ul>
<p><img src=""https://i.imgur.com/3LYWDJI.png"" alt=""enter image description here"" /></p>
"
"75852417","Synapse/ ADF - How to Truncate table if dynamic config colomn is True in pre-copy script","<p>I want to truncate tables where the value of my column isTrun in the config table is True.
The scripts need to run in the Sink pre-copy script of the copy activity.</p>
<p>The dataflow is the following:</p>
<p>The look-up activity looks up the config table. Then enters the for each loop and copy's the tables that are looked up. before the copy, there needs to be a truncate for the tables that have in the config table the value True in the isTrun column.</p>
<p>The problem that I am facing is that all my tries have invalid syntax.</p>
<p>I already tried the followings scripts:</p>
<pre><code>if(@item().isTrun = 1, TRUNCATE TABLE [@{item().targetSchema}].[@{item().targetTable}], '1=1')
</code></pre>
<p>and this script:</p>
<pre><code>TRUNCATE TABLE @{if(equals(item().isTrun, 'True'),'[@{item().targetSchema.item()}].[@{targetTable}]',' ')}
</code></pre>
<p>Thanks in advance!</p>
","<sql><azure><azure-synapse><azure-data-factory>","2023-03-27 05:41:38","88","0","2","75853380","<p>Use this expression for the pre-copy script:</p>
<pre><code>@{if( 
equals(item().isTrun,1), 
concat( 'truncate table ', item().doel_schema, '.', item().doel_tabel ) ,'')}
</code></pre>
<p>It checks if isTrun equals 1 for this loop, if so it concatenate the truncate command with the table name. If not, the pre-script will be an empty string that does nothing.</p>
"
"75852177","Understanding managed VNet using Private Endpoint for azure data factory","<p>I'm working on writing some basic azure data factory pipelines, and i'm new to this.</p>
<p>We have a private network behind a firewall, with an on-prem sql server database I'm connecting to. I thought I could use a self-hosted runtime, but when I try to run a data flow, it instead tells me I need to use a &quot;managed VNet using Private Endpoint. &quot;</p>
<p>I read the microsoft doc on it and still can't quite wrap my head around what this is, and if I actually need it. The self-hosted runtime seems to allow me to connect to the server, just not run a data flow, so I'm confused what the difference is.</p>
<p>Does it sound like I actually need to go down this route, and if so, any tips on how to implement? TY!</p>
<p>read MS docs but couldn't understand</p>
","<sql-server><azure><azure-data-factory>","2023-03-27 04:37:32","96","1","1","75862824","<p>I created self-hosted integration runtime and connected on-premises sql server successfully.</p>
<p><img src=""https://i.imgur.com/zu0JX6y.png"" alt=""enter image description here"" /></p>
<p>I created dataset with above linked service and tried to use that in dataflow got below error:</p>
<p><img src=""https://i.imgur.com/97pQnuH.png"" alt=""enter image description here"" /></p>
<p>I found that as per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#integration-runtime-types"" rel=""nofollow noreferrer"">this</a> self-hosted integration runtime is not supported for dataflow. If we want to use on premises sql database data copy that data to azure data lake or blob storage through copy activity trough self-hosted integrated runtime and use otherwise use the Azure IR with managed vnet. To create that you can follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-managed-virtual-network-on-premise-sql-server"" rel=""nofollow noreferrer"">MS Document</a>.</p>
"
"75848687","How to create a Azure Data Factory Linked service to connect with private Azure Data Explorer using private endpoint","<p>I have to create a linked service in Azure Data Factory which connects to private Azure Data Explorer using private end point.</p>
<p>In Azure Data Explorer, I have restricted public access and created a private endpoint on it.</p>
<p>In Azure Data Factory, I have performed below steps:</p>
<ol>
<li>Created Integrated Runtime with virtual network enabled.</li>
<li>Created managed private endpoint to connect ADX (same is approved under Azure Data Explorer private endpoint section).</li>
</ol>
<p>Now, while creating a linked service, I cannot see any option to select private endpoint. But if I try to created a linked service for SQL server, then there is an option to select private endpoint.</p>
<p>So, I would like to know if ADF linked service can be connected to PRIVATE azure data explorer. If so, how can we achieve this.</p>
","<azure><azure-data-factory><azure-data-explorer>","2023-03-26 15:25:26","90","0","1","75852250","<p>According to Microsoft ADF documentation, managed private endpoint connection is not supported from ADF to data explorer.
<a href=""https://i.stack.imgur.com/6aVG8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6aVG8.png"" alt=""ADF documentation"" /></a></p>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-overview</a></p>
"
"75845853","Using parameters and dynamic content in pre-SQL script for Azure Data Factory data flow sink transformation","<p>I have a pipeline parameter called query_sink (type string) it comes from a database and the posible values for the parameter could be</p>
<p><a href=""https://i.stack.imgur.com/0zQoG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0zQoG.png"" alt=""possible values"" /></a></p>
<p>The second record is something like <code>IF EXISTS(...) DELETE FROM table1 WHERE country = 1</code></p>
<p>So, I want to use a dataflow where in the sink transformation use the parameter query_sink.
<a href=""https://i.stack.imgur.com/bSfSB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bSfSB.png"" alt="""" /></a></p>
<p>Then use it in the pre SQL script in the sink transformation, For now I just pass the parameter without changes like this</p>
<p><a href=""https://i.stack.imgur.com/NyKpK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NyKpK.png"" alt="""" /></a></p>
<p>but I have problems when the value in the database is <strong>null</strong></p>
<blockquote>
<p>'sink1': There are no batches in the input script.&quot;</p>
</blockquote>
<p>I'm not sure if the parameter in that case is null or is it as if the parameter didn't receive anything.
What I need is in the case the parameter value is a query(second record of the first image) execut it, but in the other case(first record of the first image) do nothing.</p>
<p>Edit:
the input for the pipeline parameter receive this value</p>
","<azure-data-factory>","2023-03-26 04:25:08","95","0","1","75863532","<blockquote>
<p>What I need is in the case the parameter value is a query(second record of the first image) execut it, but in the other case(first record of the first image) do nothing.</p>
</blockquote>
<p>You can pass the below dynamic content to the dataflow parameter where <code>query</code> is a pipeline parameter.</p>
<pre><code>@if(greater(length(pipeline().parameters.query),4), concat('''',pipeline().parameters.query,''''),concat('''','DECLARE @id AS INT;',''''))
</code></pre>
<p>Give this to a string variable and pass the string to the dataflow parameter like below.</p>
<p><img src=""https://i.imgur.com/LJcwZZN.png"" alt=""enter image description here"" /></p>
<p><strong>This is my source table:</strong></p>
<p><img src=""https://i.imgur.com/S6JX4q1.png"" alt=""enter image description here"" /></p>
<p><strong>target table <code>final_target2</code> before deleting:</strong></p>
<p><img src=""https://i.imgur.com/f7hpZh6.png"" alt=""enter image description here"" /></p>
<p>Target table when query parameter value is <code>delete from final_target2 where country=1;</code>.</p>
<p><img src=""https://i.imgur.com/5PrJCAn.png"" alt=""enter image description here"" /></p>
<p>You can see the records with <code>country=1</code> were deleted and new records from <code>final_source1</code> were inserted above.</p>
<p>Target table when query parameter value is <code>NULL</code>. Here in this case I am Executing a sample query <code>DECLARE @id AS INT;</code> which does nothing to target table.</p>
<p><img src=""https://i.imgur.com/ZnRuu0F.png"" alt=""enter image description here"" /></p>
<p><strong>My pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;var&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@if(greater(length(pipeline().parameters.query),4), concat('''',pipeline().parameters.query,''''),concat('''','DECLARE @id AS INT;',''''))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Data flow1&quot;,
                &quot;type&quot;: &quot;ExecuteDataFlow&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataflow&quot;: {
                        &quot;referenceName&quot;: &quot;dataflow1&quot;,
                        &quot;type&quot;: &quot;DataFlowReference&quot;,
                        &quot;parameters&quot;: {
                            &quot;query_sink&quot;: {
                                &quot;value&quot;: &quot;@variables('var')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    &quot;compute&quot;: {
                        &quot;coreCount&quot;: 8,
                        &quot;computeType&quot;: &quot;General&quot;
                    },
                    &quot;traceLevel&quot;: &quot;Fine&quot;
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;query&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;NULL&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;var&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;lastPublishTime&quot;: &quot;2023-03-27T15:35:50Z&quot;
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p><strong>My dataflow JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;source2_table&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;final_target&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [],
            &quot;scriptLines&quot;: [
                &quot;parameters{&quot;,
                &quot;     query_sink as string&quot;,
                &quot;}&quot;,
                &quot;source(output(&quot;,
                &quot;          id as integer,&quot;,
                &quot;          country as integer&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     isolationLevel: 'READ_UNCOMMITTED',&quot;,
                &quot;     format: 'table') ~&gt; source1&quot;,
                &quot;source1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     input(&quot;,
                &quot;          name as string&quot;,
                &quot;     ),&quot;,
                &quot;     deletable:false,&quot;,
                &quot;     insertable:true,&quot;,
                &quot;     updateable:false,&quot;,
                &quot;     upsertable:false,&quot;,
                &quot;     format: 'table',&quot;,
                &quot;     preSQLs:[($query_sink)],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     errorHandlingOption: 'stopOnFirstError') ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
"
"75841393","How to replicate Alteryx logic of ""MultiRow"" Transfromation in Azure Dataflow","<p>I have a tool in Alteryx(ETL Tool) where there is a transformation called &quot;Multi Row&quot; Which does the following :
Tool configuration:</p>
<p>Explaination of what its doing:
If the col has null ,its taking THE VALUE BEHIND ONE ROW .As &quot;2012&quot; is behind every null ,its filling up everything with &quot;2012&quot;.</p>
<p>How to achieve this in Azure Data Flow ?
<a href=""https://i.stack.imgur.com/RnSjw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RnSjw.png"" alt=""enter image description here"" /></a></p>
<p>input data :<a href=""https://i.stack.imgur.com/euRJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/euRJX.png"" alt=""enter image description here"" /></a></p>
<p>output data :<a href=""https://i.stack.imgur.com/12cpI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/12cpI.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-03-25 11:27:38","63","0","1","75841713","<p>You can use <strong>Window transformation</strong> in dataflow to fill the null values with the previous value. I reproduced with sample input data and below is the approach.</p>
<ul>
<li><p>Input data is taken like this.
<img src=""https://i.imgur.com/OT1pqI5.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Derived column is added with a dummy column named dummy and value is <code>1</code> for all rows.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/SyuwwPX.png"" alt=""enter image description here"" /></p>
<ul>
<li>Surrogate key is created for sort column in windows transformation. (If you have any field for sorting, use that in the Windows transformation.)</li>
</ul>
<p>Surrogate key settings:</p>
<pre><code>Key name: sk 
Start value: 1
Step value: 1
</code></pre>
<p><img src=""https://i.imgur.com/IiODgNQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Windows transformation is taken and settings is given as</li>
</ul>
<pre><code>1. over: dummy
2. sort:  sk ascending
3. Range by : unbounded
4. windows column: year = coalesce(year, last(year, true()))
</code></pre>
<p><img src=""https://user-images.githubusercontent.com/113445679/227717083-9695c1f7-382d-49fb-b681-be4249eb02d2.gif"" alt=""gifwindowstransformation"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/l82W2HM.png"" alt=""enter image description here"" /></p>
<p>You can use select transformation and select only the required fields and discard the dummy fields.</p>
"
"75840573","Get key vault data from web activity in Data factory or Synapse workspace","<p>I am working on a case on Azure Data Factory/Synapse Workspace making use of Key Vault to generate token for further use in the pipeline. I have followed an article using the principle getting clientid and secret from a web activity. But in the guideline the author is using the same setup in both activities resulting of course that the same outcome or output. Can anybody tell what to do so I can get clientid from one of the activities in the pipeline</p>
<p><a href=""https://www.hubnet.cloud/2021/05/17/part-2-reporting-on-log-analytics-data-building-the-query-and-extracting-data-using-azure-data-factory/"" rel=""nofollow noreferrer"">https://www.hubnet.cloud/2021/05/17/part-2-reporting-on-log-analytics-data-building-the-query-and-extracting-data-using-azure-data-factory/</a></p>
<p><a href=""https://i.stack.imgur.com/PeWOL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PeWOL.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-synapse>","2023-03-25 08:48:32","61","0","1","75853040","<p><strong>To get the details from key vault you have to crate two secrets one is <code>ClientId</code> and <code>ClientSecret</code>.</strong></p>
<p><img src=""https://i.imgur.com/H234Sa5.png"" alt=""enter image description here"" /></p>
<p>Then fetch the details from web activity in ADF by passing below values for each secret in web activity.</p>
<pre class=""lang-json prettyprint-override""><code>URL : [Your secret URI value]?api-version=7.0
Method : GET
Authentication : System Assigned Managed Identity
Resource : https://vault.azure.net
</code></pre>
<p><img src=""https://i.imgur.com/dHoTLxA.png"" alt=""enter image description here"" /></p>
<p>And you will be able to access all the values.</p>
<p><strong>Reference:</strong>  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-use-azure-key-vault-secrets-pipeline-activities"" rel=""nofollow noreferrer"">Use Azure Key Vault secrets in pipeline activities</a></p>
"
"75839525","Copying Data From a Container of one azure Account to Container of another azure account","<p>I am new to azure and I want to copy a data of container from one azure container to another container. (<strong>Note: The container are in two different azure accounts</strong>). I have a ready only access to the source container and the destination one is my own container so i have a full access to it. The owner of source container claims to allow only one specific ip to access the resource of container.
So which approach should i use?
a) Data Factory
b) Azure Function
c) Az copy</p>
<p>I tried using data factory but i got stuck while linking the service. I was able to link the storage of same account but could not link to storage of completely different account. I tried using SAS to link the service but it keeps giving 403 issue. I also tried writing python script but whenever i try to upload to destination container, it gives resource not found issue.</p>
","<azure><azure-functions><azure-blob-storage><azure-data-factory>","2023-03-25 03:33:32","149","0","1","75852114","<p>I have reproduced in my environment and got expected results as below and I followed <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10"" rel=""nofollow noreferrer"">Microsoft-Document</a>:</p>
<p>Firstly on Source account files:</p>
<p><img src=""https://i.imgur.com/ILKoEiS.png"" alt=""enter image description here"" /></p>
<p>Then used below command:</p>
<pre><code>azcopy copy 'https://sourcestorage.blob.core.windows.net/sourcecontiner?sastoken' 'https://destinationstorage.blob.core.windows.net/destinationcontainer?sastoken' --recursive
</code></pre>
<p>With below permissions:</p>
<p>On Source Blob:</p>
<p><img src=""https://i.imgur.com/DnmuxFI.png"" alt=""enter image description here"" /></p>
<p>On Destination Blob:</p>
<p><img src=""https://i.imgur.com/BmyTOEm.png"" alt=""enter image description here"" /></p>
<p>I received Same Error as below:</p>
<p>Output:</p>
<p><img src=""https://i.imgur.com/ndtr5PS.png"" alt=""enter image description here"" /></p>
<p>Then I gave all permissions on Source and blob as below:</p>
<p><img src=""https://i.imgur.com/0kSGQbV.png"" alt=""enter image description here"" /></p>
<p>Output:</p>
<p><img src=""https://i.imgur.com/YSDR37r.png"" alt=""enter image description here"" /></p>
<p>Then in Destination Storage Account:</p>
<p><img src=""https://i.imgur.com/ztvIZL6.png"" alt=""enter image description here"" /></p>
"
"75831307","Which Spark version is being used by Mapping Data Flows?","<p>I ran into some issues recently which lead me to believe that the Mapping Data Flows used in my Synapse Analytics Workspace are still running Spark 2.4, which has reached EOL. I tried to find out which Spark version is being used and how to change it, without any success.</p>
<p>Is there a way to migrate to a newer version or to at least verify which Spark version is being used under the covers?</p>
<p>The concrete reasons for me caring revolve around the breaking changes regarding date/time handling in Spark 3.x.</p>
","<azure><azure-data-factory><azure-synapse><azure-mapping-data-flow>","2023-03-24 08:15:51","87","0","1","75831783","<p>You could figure out the version of spark through Monitoring output.
Migration to Spark 3.x is already underway. And your subscription should be migrated to 3.x soon (if its not already migrated)</p>
<p>You can find the information in the output details of a data flow activity.
<a href=""https://i.stack.imgur.com/FKbFq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FKbFq.png"" alt=""Spark Activity Output"" /></a></p>
"
"75829496","Best way to move power BI dataflows to Azure synapse","<p>I am in the process of creating an enterprise operating model for Power BI and Azure Synapse.</p>
<p>I'm trying to see if there are any ways to move a power BI dataflow transformation to Azure Synapse. I'll give an example use-case:</p>
<ul>
<li>I build a dataflow transformation in Power BI, it is relatively complex with multiple steps involved.</li>
<li>I want to then strengthen and productionise this transformation in the form of an ETL pipeline using Azure Synapse.</li>
</ul>
<p>What is the best way to leverage as much of the effort put into the dataflow as possible?</p>
","<powerbi><azure-synapse><azure-data-factory>","2023-03-24 02:12:34","44","0","1","75841970","<p>Also interested in this. I have a Power App backed by a set of Dataverse tables. The data from this app is being distributed to end users by a Dataflow that feeds a shared Excel. My aim is for transformed data to flow to the org Azure Synapse in its transformed state rather than having all the individual Dataverse tables be linked to Synapse via the Dataverse connector. Surely this can be achieved. Seems pointless to have to create an ADF pipeline duplicating the effort in the Dataflow.</p>
<p>Edit: This might be helpful to you - <a href=""https://powerbi.microsoft.com/es-es/blog/power-bi-dataflows-and-azure-data-lake-storage-gen2-integration-preview/"" rel=""nofollow noreferrer"">https://powerbi.microsoft.com/es-es/blog/power-bi-dataflows-and-azure-data-lake-storage-gen2-integration-preview/</a></p>
<p>CDM is the answer I believe.</p>
"
"75829216","How to make a POST request using Python to Azure?","<p>I'm trying to make a POST request to Azure with the following code:</p>
<pre><code>import requests

api_url = https://management.azure.com/subscriptions/subscription_id/resourcegroups/resourcegroup_id/providers/Microsoft.DataFactory/factories/datafactory_id/airflow/sync?api-version=2018-06-01&quot;
todo = {
    &quot;IntegrationRuntimeName&quot;: &quot;Airflow&quot;,
    &quot;LinkedServiceName&quot;: &quot;LAIRFLOW&quot;,
    &quot;StorageFolderPath&quot;: &quot;airflow/dev/&quot;,
    &quot;CopyFolderStructure&quot;: &quot;true&quot;,
    &quot;Overwrite&quot;: &quot;true&quot;,
}
response = requests.post(api_url, json=todo)
print(response.json())
print(response.status_code)
</code></pre>
<p>But I'm getting the following error:</p>
<pre><code>{'error': {'code': 'AuthenticationFailed', 'message': &quot;Authentication failed. The 'Authorization' header is missing.&quot;}}
401
</code></pre>
<p>I've tried to include the client and secret id:</p>
<pre><code>response = requests.post(api_url, json=todo, auth=(AZURE_CLIENT_ID, AZURE_CLIENT_SECRET))
</code></pre>
<p>But I got the following error:</p>
<pre><code>{'error': {'code': 'AuthenticationFailedInvalidHeader', 'message': &quot;Authentication failed. The 'Authorization' header is provided in an invalid format.&quot;}}
401
</code></pre>
<p>How should I make the request?</p>
","<azure><azure-data-factory><azure-rest-api><azure-sdk-python><azure-python-sdk>","2023-03-24 00:59:11","128","-2","1","75846366","<p>First, generate an access token using the code below, and then include it in the authorization header when sending a Management API request.</p>
<pre><code>import requests

# Replace these with your Azure AD tenant ID, client ID, and client secret
tenant_id = '&lt;your_tenant_id&gt;'
client_id = '&lt;your_client_id&gt;'
client_secret = '&lt;your_client_secret&gt;'

# Construct the OAuth2 token endpoint URL
token_endpoint = f'https://login.microsoftonline.com/{tenant_id}/oauth2/token'

# Define the required parameters for the token endpoint
data = {
    'grant_type': 'client_credentials',
    'client_id': client_id,
    'client_secret': client_secret,
    'resource': 'https://management.azure.com/'
}

# Make a request to the token endpoint to obtain an access token
response = requests.post(token_endpoint, data=data)

# Extract the access token from the response
access_token = response.json()['access_token']

api_url = &quot;https://management.azure.com/subscriptions/subscription_id/resourcegroups/resourcegroup_id/providers/Microsoft.DataFactory/factories/datafactory_id/airflow/sync?api-version=2018-06-01&quot;
todo = {
    &quot;IntegrationRuntimeName&quot;: &quot;Airflow&quot;,
    &quot;LinkedServiceName&quot;: &quot;LAIRFLOW&quot;,
    &quot;StorageFolderPath&quot;: &quot;airflow/dev/&quot;,
    &quot;CopyFolderStructure&quot;: &quot;true&quot;,
    &quot;Overwrite&quot;: &quot;true&quot;,
}
access_token = access_token
headers = {'Authorization': f'Bearer {access_token}'}
response = requests.post(api_url, json=todo, headers=headers)
print(response.json())
print(response.status_code)
</code></pre>
"
"75824667","ADF: find pipelines by sink resource","<p>Is there an option to filter/find Azure DataFactory pipelines by &quot;sink&quot; resource they are using ?</p>
<p>For example I have a datasent linked as a SQL table &quot;ddd.TABLE&quot; and I want to find all pipelines that have something to do with it. How do I do that ?</p>
","<azure><azure-data-factory>","2023-03-23 15:02:45","28","0","1","75826323","<p>It may not be exactly what you are looking for, but most assets have a &quot;Related&quot; feature that will show you where they are used. For example: go to the &quot;Manage&quot; tab and select &quot;Linked Services&quot;. The &quot;Related&quot; column will show you how many references are in the system to that Linked Service:</p>
<p><a href=""https://i.stack.imgur.com/szNyF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/szNyF.png"" alt=""enter image description here"" /></a></p>
<p>Clicking on the number will open a flyout on the right side showing you which assets reference the service:</p>
<p><a href=""https://i.stack.imgur.com/JFvWT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JFvWT.png"" alt=""enter image description here"" /></a></p>
<p>The list items are clickable and will open that asset. In this example the items are all Datasets. In the Dataset, under &quot;Properties&quot;, you will find another &quot;Related&quot; tab that lists all the assets that reference the Dataset:</p>
<p><a href=""https://i.stack.imgur.com/jBYbF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jBYbF.png"" alt=""enter image description here"" /></a></p>
<p>As before, the items in the list are clickable and will open the asset.</p>
"
"75822990","I want to Extract date part from File MUNDIPHARMA_HospitalPROFITS_20230228.zip in ADF","<p>I have these Many files want to store them in a date folder format like yyyy folder then mm folder then dd folder then i want to place files in it
<a href=""https://i.stack.imgur.com/RsDrn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RsDrn.png"" alt=""enter image description here"" /></a></p>
<p>Thanks in Advance</p>
","<azure><azure-data-factory><azure-data-lake><azure-synapse>","2023-03-23 12:27:13","42","0","1","75823569","<p>If your requirement is to copy the above files to its respective folders, you can follow this approach.</p>
<p>These are my source zip files.</p>
<p><img src=""https://i.imgur.com/TohGKvY.png"" alt=""enter image description here"" /></p>
<p>I have given it to Get Meta data using a binary dataset and got child items array and passed to foreach. Inside ForEach, I have extracted the year, month and date from the filename using split and substring and copied to respective folders.
This is my pipeline flow.</p>
<p><img src=""https://i.imgur.com/3zNoUWI.png"" alt=""enter image description here"" /></p>
<p>These are my Binary datasets for the copy activity source and sink.</p>
<p><strong>Source:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;copybinarysource&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AzureDataLakeStorage1&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;parameters&quot;: {
            &quot;filename&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;Binary&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
                &quot;fileName&quot;: {
                    &quot;value&quot;: &quot;@dataset().filename&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;folderPath&quot;: &quot;input&quot;,
                &quot;fileSystem&quot;: &quot;data&quot;
            }
        }
    }
}
</code></pre>
<p><strong>Sink:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;sinkbinary&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AzureDataLakeStorage1&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;parameters&quot;: {
            &quot;folder_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            },
            &quot;file_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;Binary&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
                &quot;fileName&quot;: {
                    &quot;value&quot;: &quot;@dataset().file_name&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;folderPath&quot;: {
                    &quot;value&quot;: &quot;@dataset().folder_name&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;fileSystem&quot;: &quot;target1&quot;
            }
        }
    }
}
</code></pre>
<p><strong>This is my pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Get Metadata1&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Binary1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;fieldList&quot;: [
                        &quot;childItems&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;BinaryReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Get Metadata1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Get Metadata1').output.childItems&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;year&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;year&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@substring(split(item().name,'_')[add(length(split(item().name,'_')),-1)],0,4)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;month&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;year&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;month&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@substring(split(item().name,'_')[add(length(split(item().name,'_')),-1)],4,2)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;date&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;month&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;date&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@substring(split(item().name,'_')[add(length(split(item().name,'_')),-1)],6,2)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Copy data1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;date&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;BinarySource&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                        &quot;recursive&quot;: true
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;BinaryReadSettings&quot;
                                    }
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;BinarySink&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                    }
                                },
                                &quot;enableStaging&quot;: false
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;copybinarysource&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;filename&quot;: {
                                            &quot;value&quot;: &quot;@item().name&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;sinkbinary&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;folder_name&quot;: {
                                            &quot;value&quot;: &quot;@concat(variables('year'),'/',variables('month'),'/',variables('date'))&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;file_name&quot;: {
                                            &quot;value&quot;: &quot;@item().name&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;year&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;month&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;date&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>Files in target folders:</strong></p>
<p><img src=""https://i.imgur.com/QMWaHay.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/Ppo2nJK.png"" alt=""enter image description here"" /></p>
"
"75818205","Azure Data Factory mistakenly thinking resource is already in use","<p>I am following <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">this tutorial</a> on how to copy blobs from one container to another using the python ADF sdk. This has been working for one project, but now that I want to create another ADF from the same application instance in Azure Active Directory, I am getting very strange error, in that when I call</p>
<pre class=""lang-py prettyprint-override""><code>...
adf_client = DataFactoryManagementClient(credentials, subscription_id)
df_resource = Factory(location='westus')
df = adf_client.factories.create_or_update(rg_name, 'metadata-adf', df_resource)
</code></pre>
<p>I get</p>
<pre><code>ResourceExistsError
(DataFactoryNameInUse) The specified resource name 'metadata-adf' is already in use. Resource names must be globally unique.
</code></pre>
<p>Even if I've already deleted an adf by that name. (I can further check this by seeing no adfs are listed under my resource group.)</p>
<p>For a bit I could get around by giving my ADF a different name (not 'metadata-adf').  But now if I use another name, I get the exact same error as above (with the same resource name and all).</p>
<p>Clearly this is a bug on Azure's side, and if I'm doing something incorrectly, the error handling is extremely opaque. How can I get around this?</p>
<p>Edit: <a href=""https://gist.github.com/bfbarry/49463d8e6558a7205b3298cc491b85fe"" rel=""nofollow noreferrer"">full code here</a></p>
","<azure><azure-data-factory>","2023-03-22 23:55:13","53","0","1","75822882","<p>If I understood correctly you used to have this name and cannot create the same ADF with the same name?</p>
<p>This is not a bug, Data factory names must be unique across Microsoft Azure. Names are case-insensitive, that is, MyDF and mydf refer to the same data factory.</p>
<p>You are simply not using a name which is not already taken.</p>
"
"75814629","How to Implement a Lookup Activity in ADF to Check Rows Added After the Last Trigger Run","<p>I have an ADF pipeline that contains a scheduled trigger to run the pipeline every two minutes. The pipeline's input table is linked to an SQL database in SSMS, and it detects any DDL changes. The input table has several columns, including event, request SQL, and audit datetime. Now, I want to implement a lookup activity in ADF that checks the rows added to the input database after the last trigger run. The lookup activity can use an SQL query like 'count * from input table where audit datetime &gt; time of the last trigger run'. How can I achieve this?</p>
","<azure-data-factory><ssms-2017>","2023-03-22 16:02:03","89","0","1","75822034","<p>You can use three lookup activities for this scenario. One is to check the datetime of last trigger run time. Second one is to check the current date time. Third one is to check the count of records in the log between these two datetime. Below is the detailed approach.</p>
<ul>
<li>Input tables <code>log_tab</code> and <code>wm_tab</code> is taken as in below image. wm_tab is used to store the last trigger run datetime. Initially, value is assigned with <code>1900-01-01 00:00:00.0000000</code></li>
</ul>
<p><img src=""https://i.imgur.com/0OlZTEH.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>In ADF, Lookup1 is taken and query for source dataset is given as <code>select * from wm_tab</code>. This will take and store the value of last triggered date value.</p>
</li>
<li><p>Lookup2 is taken and query for source dataset is given as <code>select getdate() as current_datetime</code>. This will store the current date value.</p>
</li>
<li><p>Lookup3 activity is taken, and it is connected to Lookup1 and lookup2 in such a way that it gets executed only after lookup1 and lookup2 gets executed. Query is given as,</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>select count(1) as count_of_new_records from log_tab where
audit_time &gt;= '@{activity('Lookup1').output.firstRow.audit_time}' and 
audit_time &lt; '@{activity('Lookup2').output.firstRow.current_datetime}'
</code></pre>
<ul>
<li>script activity is taken to update the current date time value as new watermark value in <code>wm_tab</code></li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>update wm_tab
set
audit_time='@{activity('Lookup2').output.firstRow.current_datetime}'
</code></pre>
<p><strong>Result</strong></p>
<ul>
<li>case:1 When pipeline is run for the first time, count is shown as 5 records. Pipeline checks the count of rows from 1900-01-01 to current date time.</li>
</ul>
<p><img src=""https://i.imgur.com/vNdoiMq.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Also <code>wm_tab</code> is updated with new value.
<img src=""https://i.imgur.com/T5XWsfR.png"" alt=""enter image description here"" /></p>
</li>
<li><p>case:2 I have not inserted any records after the time <code>2023-03-23 01:57:40.36000000</code> and when pipeline is run, it shows count of new records as <code>0</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/pdmCfww.png"" alt=""enter image description here"" /></p>
"
"75811214","Build trriger set on fire when an event occurs to the input dataset in azure data factory","<p>i want to build a trigger in ADF that set on fire when an event occurs to the input dataset , i used linked service to import the input dataset from SQL server management studio.</p>
<p>I tried to create a storage event trigger, but it doesn't seem to be working. The trigger requires a connection to Azure Blob and the specification of a Blob path that ends with or starts with certain values. However, I only have an input dataset that is linked to a SQL Server. Can you help me troubleshoot this issue?</p>
","<triggers><azure-data-factory><ssms-2017>","2023-03-22 10:43:22","19","0","1","75811439","<p>Unfortunately, at the moment there is no trigger type in ADF to fire on change in source data. You will need to create a scheduling trigger to run every X minutes (or hours\days...), and in the first step of the pipeline you run to check for the condition. Then use an IF activity to continue the process if the condition is met.
Another option is to use logic app, please see this answer with an example: <a href=""https://stackoverflow.com/questions/74563667/azure-data-factory-sql-trigger-events"">Azure Data Factory SQL trigger events</a></p>
"
"75805993","Dataset created with Az PowerShell not showing in Data Factory Studio","<p>I've created a new dataset in my data factory using the command</p>
<pre><code>Set-AzDataFactoryV2Dataset
</code></pre>
<p>This command ran successfully and now when I run the command</p>
<pre><code>Get-AzDataFactoryV2Dataset
</code></pre>
<p>I can see my new dataset as well as the other datasets I had previously created in the Data Factory Studio. However, the new dataset does not show up in Azure Data Factory Studio or the Git repo for the factory.</p>
<p>I've refreshed the Data Factory Studio page many times and also published the factory using the &quot;publish&quot; button in Data Factory Studio.</p>
<p>If I try to create the same dataset from Az Powershell again, it promps to ask me if I want to override the existing dataset. So the dataset seems to exist in the Powershell environment but not in the studio environment.</p>
<p>Is there a sort of Push command that I have to run from Azure Powershell? I can't find anything about it.</p>
<p>Any help would be appreciated.</p>
","<azure><powershell><azure-data-factory><azure-powershell>","2023-03-21 20:18:09","118","0","1","75841710","<p>Create Azure DataFactory via PowerShell Follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-powershell#create-datasets"" rel=""nofollow noreferrer"">document</a> created DataFactory/Datasets in Azure Data Factory Studio .</p>
<p>Created via PowerShell with the same azure DataFactory/dataset with the same names I got the below error.
<a href=""https://i.stack.imgur.com/YOnBz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YOnBz.png"" alt=""enter image description here"" /></a></p>
<p>Suppose we create datasets with the same names in the existing Powershell environment or azure data factory studio environment. It gets a prompt error.</p>
<p><a href=""https://i.stack.imgur.com/uE1Su.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uE1Su.png"" alt=""enter image description here"" /></a> </p>
<p>Using the new name in azure DataFactory/dataset was successfully created and reflected in Azure Data Factory Studio.</p>
<p><a href=""https://i.stack.imgur.com/EL4xC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EL4xC.png"" alt=""enter image description here"" /></a></p>
<p>Azure Data Factory Studio and created dataset</p>
<p><a href=""https://i.stack.imgur.com/wvTni.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wvTni.png"" alt=""enter image description here"" /></a></p>
<p>Reflected datasets in PowerShell</p>
<p><a href=""https://i.stack.imgur.com/OOBFq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OOBFq.png"" alt=""enter image description here"" /></a></p>
<p>Now I got both datasets reflected in both Azure DataFactory Studio datasets and showing the same details in PowerShell.</p>
"
"75805817","Difference between Lookup and Join in Azure Data Factory","<p>I know that when a look up is performed in ADF, it's essentially a left outer join. Why use look up activity when we can use a join activity instead? What's the basic difference between them? I've looked everywhere but haven't found a solution that helps me understand this difference. Thanks!</p>
","<join><azure-data-factory><lookup>","2023-03-21 19:54:27","59","0","1","75806374","<p>The lookup transformation has several features built into it that are specifically there to support lookup of reference data. If you look at the configuration section of the Lookup doc (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-lookup#configuration"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-lookup#configuration</a>) you will see features like  matching options and tagging matched rows. Lookup also support caching your lookup results (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-lookup#cached-lookup"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-lookup#cached-lookup</a>).</p>
"
"75803099","Can't use Az.Datafactory in Powershell because my factory is the wrong resource type","<p>I'm trying to use Az.Datafactory module in Powershell to interact with my data factory. I've created this factory in the Azure portal. But the module can't seem to find the factory, because it's resource type is Microsoft.DataFactory/factories, but Az.Datafactory looks in Microsoft.DataFactory/dataFactories.</p>
<pre><code>Name                                 ResourceType                                       Location      
----                                 ------------                                       --------
sqlCdc1                              Microsoft.DataFactory/factories                    eastus
testFactoryBD                        Microsoft.DataFactory/factories                    canadaeast
</code></pre>
<p>Here is the error message:</p>
<pre><code>Get-AzDataFactory : HTTP Status Code: NotFound
Error Code: ResourceNotFound
Error Message: The Resource 'Microsoft.DataFactory/dataFactories/sqlCdc1' under resource group 'BD_prelimTests' was not found. For more details please go to
https://aka.ms/ARMResourceNotFoundFix
Request Id: f5407d39-57e1-4d10-a706-e55522467a99
Timestamp (Utc):03/21/2023 14:46:22
Au caractère Ligne:1 : 1
+ Get-AzDataFactory -ResourceGroupName BD_prelimTests -Name sqlCdc1
</code></pre>
<p>My Azure portal puts data factories by default in /factories. When I try to create a factory with Az.Datafactory in Powershell to see if the factory will be created in /dataFactories, it says it cannot create a factory in my region (canadaeast). When I try with a different region (eastus), it says my tenant doesn't allow data factory creation in that region, even though I can do it in the Azure portal as you can see above.</p>
","<azure><powershell><azure-data-factory><azure-powershell>","2023-03-21 15:19:38","59","0","1","75803120","<p>The Module which you are using is tied to Data Factory V1 and not V2 version.
Please leverage : Get-AzDataFactoryV2
<a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/get-azdatafactoryv2?view=azps-9.5.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/az.datafactory/get-azdatafactoryv2?view=azps-9.5.0</a></p>
"
"75800946","how to store the metadata of a filehieracrchy into sql table or csv file in adf?","<p>I have a container in that there is a folder called loaded data in that I have subfolders also.
I need to store the metadata in the following  format  containername|foldername|foldername|filename
in a SQL table or file.</p>
<p>can anyone show the steps to achieve this.</p>
","<azure-data-factory>","2023-03-21 12:02:57","16","0","1","75837167","<p>The most straight forward approach will be to use an Azure function to get the all the metadata and then use Data Factory to store that result.</p>
"
"75798132","Data Factory git - how to revert to latest published version","<p>I am using Data Factory V2 with git integration. I am developing in browser, from the Azure Portal. So far I kept making new changes, testing, publishing.</p>
<p>Today I am in the situation that I made some bad changes.
Those are not yet published to the master branch.
Is there any way to discard the most recent changes == revert to master?</p>
<p>Many thanks!!</p>
<p>Looking for an easy way to revert to master branch, from the web UI.
By that I mean less effort than dropping one pipeline + source + destination + schedule and re-creating from scratch.
Is that possible?</p>
","<git><azure-data-factory>","2023-03-21 07:00:16","39","0","1","75798628","<p>You can use git reset to revert that branch to a prior state, you can do this with the cloudshell/powershell but not with the UX.</p>
<p>Even though this doc is from azure devops, it should be valid for github as well:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/devops/repos/git/undo?view=azure-devops&amp;tabs=visual-studio-2022"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/repos/git/undo?view=azure-devops&amp;tabs=visual-studio-2022</a></p>
<p>Also getting Github for your OS and cloning your collab branch enables you to see any change that has been done and which JSON was affected.</p>
<p>Right clicking on a commit, and clicking 'revert commit' reverses the selected change. Push it back to your remote git repository, and refresh your page on ADF to see the changes reflected.</p>
<p>If you want to selectively undo, you have to change the JSON yourself in order to partially undo a commit.</p>
<p>use the line-by-line differences provided by git (in your most recent changes) in order to identify / isolate your changes, adjust them &amp; push these changes back to the collaboration branch.\</p>
"
"75797053","Using NULL in Azure ADF Dynamic content","<p>Hi I am new to Azure ADF and trying to copy additional column values to a table. While doing so sometimes I am getting NULL values from source but while adding that to destination table I get following error.</p>
<p>ErrorCode=TypeConversionFailure,Exception occurred when converting value '' for column name 'clearingHouseContactTelephoneNumber' from type 'String' (precision:, scale:) to type 'Decimal' (precision:10, scale:0). Additional info: The input wasn't in a correct format.</p>
<p>I want to make sure if there is an empty field then I need to simply add NULL value to that field. However 'NULL' keyword is not acceptable in dynamic content. Please take a look at this example screenshot. <a href=""https://i.stack.imgur.com/2BAF8.png"" rel=""nofollow noreferrer"">example image</a></p>
<p>Please help me with adding NULL value to the table.</p>
","<azure><azure-data-factory>","2023-03-21 03:18:24","126","0","2","75797157","<p>You can use coalesce function and pass your dynamic value as first parameter and  <strong>null</strong> as second parameter, this function will return the first not null value.</p>
<p><a href=""https://i.stack.imgur.com/DBVZS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DBVZS.png"" alt=""image"" /></a></p>
"
"75797053","Using NULL in Azure ADF Dynamic content","<p>Hi I am new to Azure ADF and trying to copy additional column values to a table. While doing so sometimes I am getting NULL values from source but while adding that to destination table I get following error.</p>
<p>ErrorCode=TypeConversionFailure,Exception occurred when converting value '' for column name 'clearingHouseContactTelephoneNumber' from type 'String' (precision:, scale:) to type 'Decimal' (precision:10, scale:0). Additional info: The input wasn't in a correct format.</p>
<p>I want to make sure if there is an empty field then I need to simply add NULL value to that field. However 'NULL' keyword is not acceptable in dynamic content. Please take a look at this example screenshot. <a href=""https://i.stack.imgur.com/2BAF8.png"" rel=""nofollow noreferrer"">example image</a></p>
<p>Please help me with adding NULL value to the table.</p>
","<azure><azure-data-factory>","2023-03-21 03:18:24","126","0","2","75798901","<p>Using NULL in Azure ADF Dynamic content
Using NULL in Azure ADF Dynamic content</p>
<p>I tried to reproduce the error with similar dynamic content in copy activity and got similar error.</p>
<p><img src=""https://i.imgur.com/dY9AQlx.png"" alt=""enter image description here"" /></p>
<p>The reason of this error is the conversion of the string data type to decimal datatype.</p>
<p>The workaround is to using <code>DataFlow</code>: If your data flow contains numerous data fields, you must create new columns or modify current ones using the derived column transformation.</p>
<p>For more details, refer  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived column transformation in mapping data flow</a>.</p>
<p>You can even refer to this Microsoft Q&amp;A post for more insights:  <a href=""https://learn.microsoft.com/en-us/answers/questions/401569/copy-task-failure-because-of-conversion-failure.html"" rel=""nofollow noreferrer"">Copy Task failure because of conversion failure</a></p>
<p>follow below steps:</p>
<ul>
<li>Create a data flow add your source to the dataset then create the derived column to get values from your expression.
<img src=""https://i.imgur.com/kLtqMuN.png"" alt=""enter image description here"" /></li>
<li>for derived column expression create a string parameter.
<img src=""https://i.imgur.com/pJo84cU.png"" alt=""enter image description here"" /></li>
<li>Go back to pipeline click on dataflow under parameter you can see your parameter for the value select pipeline expression for this parameter and add your dynamic expression
<img src=""https://i.imgur.com/uHQxCvz.png"" alt=""enter image description here"" /></li>
<li>And now, run the pipeline.</li>
</ul>
"
"75795818","TypeConversionFailure while inserting empty string in decimal type column","<p>I am trying to insert a null value into a column with Decimal data type. Below is the error I get.</p>
<pre><code>ErrorCode=TypeConversionFailure,Exception occurred when converting 
value '' for column name 'xyz' from type 'String' (precision:, scale:) 
to type 'Decimal' (precision:18, scale:0). Additional info: The input 
wasn't in a correct format.
</code></pre>
<p>I tried multiple solutions like this -</p>
<pre><code>@if(equals(activity('Lookup1').output.value[int(variables('i'))].eftEnrollmentUniqueIdentifier,'null'),'NULL',activity('Lookup1').output.value[int(variables('i'))]
</code></pre>
<p><img src=""https://i.stack.imgur.com/jWxfg.png"" alt=""enter image description here"" />)</p>
<p>I also tried comparing it with empty string '' but it did not help.</p>
<p>Here is the output of the lookup activity that I am inserting into other table
<a href=""https://i.stack.imgur.com/2P3WI.png"" rel=""nofollow noreferrer"">Lookup activity output</a></p>
","<azure><azure-data-factory>","2023-03-20 22:19:35","73","0","1","75799983","<ul>
<li>I gave the same expression and tried to pass <code>''</code> empty string and <code>null</code> values as input to copy activity and got the same.</li>
</ul>
<p><img src=""https://i.imgur.com/JEUWyZm.png"" alt=""enter image description here"" /></p>
<p>In Copy activity, Value in additional column can be of string only. It does not accept null value there. But the target column is of decimal(18,0). That is the reason, we get this conversion error. In order to meet this requirement, you can use dataflow.</p>
<p>Create a dataflow and create a dataflow parameter. Define the dataflow parameter in the pipeline with the same expression you gave. Refer the below image.</p>
<p><img src=""https://i.imgur.com/xQJpo9N.png"" alt=""enter image description here"" /></p>
<ul>
<li>In dataflow, add a source tansformation. Then add a derived column and give the parameter value to that column.</li>
</ul>
<p><img src=""https://i.imgur.com/oeM6oKJ.png"" alt=""enter image description here"" /></p>
<p>By this way, you can copy the null data into sink.</p>
"
"75795127","Azure Databricks job result permissions","<p>I have a group of users that should have permission to see the log of a databricks job started by ADF and I don't want to give admin to them.
I found only documentation of how ytou can set the permitions for an individual job, I want to set it to all existing and future jobs. Is that possible?</p>
<p>PS.: I already enabled the &quot;Job Visibility Control&quot;
<a href=""https://learn.microsoft.com/en-us/azure/databricks/administration-guide/access-control/jobs-acl#jobs-visibility"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/databricks/administration-guide/access-control/jobs-acl#jobs-visibility</a></p>
","<azure><azure-data-factory><azure-databricks>","2023-03-20 20:46:12","93","1","1","75805251","<p>If you have a job that is triggered as an ADF task, then it's not possible - permissions for a job run will be given only for a job defined inside the Databricks to users who has corresponding permissions on job itself. The reason for that is that ADF still uses old Jobs API 2.0 (function Submit Run) that doesn't allow to set permissions for job runs (it's possible to do with API 2.1).</p>
<p>You have following possibilities to fix that:</p>
<ul>
<li><p>if you continue to trigger jobs as ADF activities, then you can have a script that will go through the recent job runs and add view permissions to selected groups.</p>
</li>
<li><p>You can use ADF web activity to trigger jobs defined inside the Databricks. Follow the steps outlined in <a href=""https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/leverage-azure-databricks-jobs-orchestration-from-azure-data/ba-p/3123862"" rel=""nofollow noreferrer"">this article</a>.</p>
</li>
</ul>
"
"75793972","Pass array variable to IN condition in ADF","<p>I need to be able to query my Azure database (Currently in Azure Synapse Dedicated Pool), return a list of items, and then pass that list of items to query from a source database. The number of values from my initial query could be anywhere from 1 to 200. I know that I could technically pass these to a for-each and query them one at a time, but was trying to be more efficient with my query to the source.</p>
<p>I am currently doing a lookup to select the value I need:</p>
<pre><code>select ID as ID from listTable
</code></pre>
<p>I am then trying to set that to a variable for the query using</p>
<pre><code>@concat('select ID from source where ID in (',activity('Lookup1').output.value,')')
</code></pre>
<p>When I do this it works, but the query is including both the ID label and the value.</p>
<pre><code>select ID from source where ID in ([{\&quot;ID\&quot;:\&quot;ffce9c3e476a9d98f30db348436d43a4\&quot;},{\&quot;ID\&quot;:\&quot;3d838131c32de954e47c166ce0013135\&quot;},{\&quot;ID\&quot;:\&quot;561f43c8db3885543cc1892d139619c0\&quot;},{\&quot;ID\&quot;:\&quot;88e69e5e97d359143e5b37d11153af1d\&quot;},{\&quot;ID\&quot;:\&quot;e4a21fd4974c6d9ce41a78511153afb2\&quot;},{\&quot;ID\&quot;:\&quot;bc699661978da9949586fe021153af75\&quot;},{\&quot;ID\&quot;:\&quot;97aaaada47ad2d105a53bb45d36d4300\&quot;},{\&quot;ID\&quot;:\&quot;4422d05fc3c2d5506ae3963ce0013174\&quot;},{\&quot;ID\&quot;:\&quot;6a76aa3e9749a510e41a78511153af34\&quot;},{\&quot;ID\&quot;:\&quot;63f129da47192d50f30db348436d4360\&quot;}])
</code></pre>
<p>So I need to only get the 2nd portion of the array, and to somehow format it so that it will be a valid IN statement.</p>
","<azure><azure-data-factory><azure-synapse>","2023-03-20 18:22:38","70","0","1","75797424","<p>You can use <code>open_json()</code> to get the columns from the JSON array. Try below script in either lookup activity query or script activity to get the desired result.</p>
<pre><code>declare @json nvarchar(max) = N'@{activity('get IDS list lookup').output.value}';
select Id from sample2 where Id in (SELECT Id
FROM OPENJSON(@json) WITH (
   Id varchar(max)
));
</code></pre>
<p><strong>This is my first lookup output array:</strong></p>
<pre><code>[
        {
            &quot;Id&quot;: &quot;24&quot;
        },
        {
            &quot;Id&quot;: &quot;26&quot;
        },
        {
            &quot;Id&quot;: &quot;1&quot;
        },
        {
            &quot;Id&quot;: &quot;2&quot;
        },
        {
            &quot;Id&quot;: &quot;10&quot;
        },
        {
            &quot;Id&quot;: &quot;4&quot;
        },
        {
            &quot;Id&quot;: &quot;18&quot;
        },
        {
            &quot;Id&quot;: &quot;17&quot;
        },
        {
            &quot;Id&quot;: &quot;7&quot;
        }
]
</code></pre>
<p><img src=""https://i.imgur.com/eFzNh3t.png"" alt=""enter image description here"" /></p>
<p><strong>SQL table rows:</strong></p>
<p><img src=""https://i.imgur.com/qVljPJU.png"" alt=""enter image description here"" /></p>
<p>I have used above query in lookup like below.</p>
<p><img src=""https://i.imgur.com/TdgiH55.png"" alt=""enter image description here"" /></p>
<p>I have stored the result lookup object array in a set variable.</p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/2hc7oC1.png"" alt=""enter image description here"" /></p>
"
"75793322","How to Union By name for the dynamic drop of files into blob in Azure data flow","<p>My File 1 has
a,b,c
1,2,3</p>
<p>File 2 has
e
23
I am doing the Wildcard path as &quot;*.txt&quot;</p>
<p>My Expected Result should be :<br />
a,b,c,e<br />
1,2,3,NULL <br />
NULL,NULL,NULL,23</p>
<p>But it came as :<br />
a,b,c<br />
1,2,3<br />
23,NULL,NULL</p>
<p>I have Schema drift is ON. Even though Schema is not drifted.</p>
<p><strong>Scenario added :
I can get 100 files a day or 2 files a day which get dropped on a daily basis in the ADLS .
How to union by name dynamically in dataflow</strong></p>
<p><a href=""https://i.stack.imgur.com/fUE01.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fUE01.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/h3fty.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h3fty.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/OiuqL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OiuqL.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/dM9hn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dM9hn.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-03-20 17:12:30","48","0","1","75797267","<p>Unioning files using wildcard character will union all files by the column position only. Instead, you can use union transformation to achieve your requirement. In Union settings, select <strong>Union by</strong> as <strong>Name</strong>.</p>
<p>I have reproduced this with sample datasets.</p>
<p><strong>Input Dataset- source1:</strong>
<img src=""https://i.imgur.com/FV6Byi1.png"" alt=""enter image description here"" /></p>
<p><strong>Input dataset- source2</strong></p>
<p><img src=""https://i.imgur.com/sZqaZwI.png"" alt=""enter image description here"" /></p>
<ul>
<li>Union transformation settings is given as in below image. Select Union by as Name.
<img src=""https://i.imgur.com/OqSASMR.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Output of UNION transformation:</strong>
Output data is as expected
<img src=""https://i.imgur.com/nPgaMTo.png"" alt=""enter image description here"" /></p>
"
"75793062","How to auto import new DAGs in Azure Data Factory Managed Airflow?","<p>Once we imported files from a Container in Storage, it would make sense to auto import if any file is modified or excluded or if there is a new file in the Container.<a href=""https://i.stack.imgur.com/fmfbv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fmfbv.png"" alt=""enter image description here"" /></a></p>
<p>But currently this job is manually, so if I modify a DAG in the Container I have to go to Managed Airflow and do the &quot;Import files&quot; again and again.</p>
<p><a href=""https://i.stack.imgur.com/qzN5t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qzN5t.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way to set the auto refresh sync or an API to use to build an equivalent solution?</p>
","<airflow><azure-data-factory>","2023-03-20 16:46:34","101","0","2","75826466","<p>You can use the import DAG REST API for automation:</p>
<p>POST <a href=""https://management.azure.com/subscriptions/"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/</a>/resourcegroups//providers/Microsoft.DataFactory/factories//airflow/sync?api-version=2018-06-01</p>
<p>Request body
{&quot;IntegrationRuntimeName&quot;:&quot;Airflow1&quot;,&quot;LinkedServiceName&quot;:&quot;AzureBlobStorage1&quot;,&quot;StorageFolderPath&quot;:&quot;airflow/&quot;,&quot;CopyFolderStructure&quot;:true,&quot;Overwrite&quot;:true}</p>
"
"75793062","How to auto import new DAGs in Azure Data Factory Managed Airflow?","<p>Once we imported files from a Container in Storage, it would make sense to auto import if any file is modified or excluded or if there is a new file in the Container.<a href=""https://i.stack.imgur.com/fmfbv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fmfbv.png"" alt=""enter image description here"" /></a></p>
<p>But currently this job is manually, so if I modify a DAG in the Container I have to go to Managed Airflow and do the &quot;Import files&quot; again and again.</p>
<p><a href=""https://i.stack.imgur.com/qzN5t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qzN5t.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way to set the auto refresh sync or an API to use to build an equivalent solution?</p>
","<airflow><azure-data-factory>","2023-03-20 16:46:34","101","0","2","75887184","<p>To add to Abhishek Narain's correct answer, the DAG REST API follows the standard HTTP 202 pattern, so the headers of the response contains the &quot;Location&quot; URI that can be polled with a GET request for the status of the import.</p>
"
"75791375","Copy Data Task - Upsert produces duplicate entries","<p>I have a pipeline, that loads the last 7 days of data from Google Analytics 4 into a landing table.
From there, I want to copy it into my data warehouse and I dont want to get any duplicates.</p>
<p>This is my source table:</p>
<pre><code>CREATE TABLE [lnd_ga4].[dateScreenPageViews_Page](
    [sk_id] [int] IDENTITY(1,1) NOT NULL,
    [taxonomie_id] [bigint] NULL,
    [dim_date] [varchar](512) NULL,
    [dim_fullPageUrl] [varchar](512) NULL,
    [dim_pagePath] [varchar](512) NULL,
    [dim_articleId] [varchar](512) NULL,
    [dim_articleType] [varchar](512) NULL,
    [dim_pageReferrer] [varchar](512) NULL,
    [dim_pageTitle] [varchar](512) NULL,
    [dim_sessionSource] [varchar](512) NULL,
    [dim_type] [char](2) NULL,
    [engagedSessions] [bigint] NULL,
    [screenPageViews] [bigint] NULL,
    [last_updated] [datetime] NOT NULL
) ON [PRIMARY]
GO

ALTER TABLE [lnd_ga4].[dateScreenPageViews_Page] ADD  DEFAULT (getdate()) FOR [last_updated]
GO
</code></pre>
<p>This is the sink table:</p>
<pre><code>CREATE TABLE [stg0_ga4].[dateScreenPageViews_Page](
    [sk_id] [int] IDENTITY(1,1) NOT NULL,
    [taxonomie_id] [bigint] NULL,
    [dim_date] [varchar](512) NULL,
    [dim_fullPageUrl] [varchar](512) NULL,
    [dim_pagePath] [varchar](512) NULL,
    [dim_articleId] [varchar](512) NULL,
    [dim_articleType] [varchar](512) NULL,
    [dim_pageReferrer] [varchar](512) NULL,
    [dim_pageTitle] [varchar](512) NULL,
    [dim_sessionSource] [varchar](512) NULL,
    [dim_type] [char](2) NULL,
    [engagedSessions] [bigint] NULL,
    [screenPageViews] [bigint] NULL,
    [last_updated] [datetime] NOT NULL
) ON [PRIMARY]
GO

ALTER TABLE [stg0_ga4].[dateScreenPageViews_Page] ADD  DEFAULT (getdate()) FOR [last_updated]
GO
</code></pre>
<p>This is the SQL query of the copy data activity:</p>
<pre><code>SELECT 
  CAST(
    dbo.GA4_getTaxonomieID(
      CONCAT('www.xyz.com', dim_pagePath)
    ) AS bigint
  ) AS taxonomie_id, 
  TRIM(dim_date) as dim_date, 
  TRIM(
    CONCAT('www.xyz.com', dim_pagePath)
  ) AS dim_fullPageUrl, 
  TRIM(dim_pagePath) as dim_pagePath, 
  TRIM(
    dbo.GA4_getArticleID(
      CONCAT('www.xyz.com', dim_pagePath)
    )
  ) AS dim_articleId, 
  TRIM(
    dbo.GA4_getArticleType(
      CONCAT('www.xyz.com', dim_pagePath)
    )
  ) AS dim_articleType, 
  TRIM(dim_pageReferrer) AS dim_pageReferrer, 
  TRIM(dim_pageTitle) AS dim_pageTitle, 
  TRIM(dim_sessionSource) AS dim_sessionSource, 
  TRIM(
    dbo.GA4_getType(
      CONCAT('www.xyz.com', dim_pagePath)
    )
  ) AS dim_type, 
  engagedSessions, 
  screenPageViews
FROM 
  lnd_ga4.dateScreenPageViews_Page 
ORDER BY 
  sk_id DESC
</code></pre>
<p>This the source configuration of the Azure Data Factroy copy data activity:</p>
<p><a href=""https://i.stack.imgur.com/rxRdz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rxRdz.png"" alt="""" /></a></p>
<p>This is the sink side:</p>
<p><a href=""https://i.stack.imgur.com/4JGuJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4JGuJ.png"" alt="""" /></a></p>
<p>This is the expression for the key columns:</p>
<p><code>@json(replace(string(pipeline().parameters.keys),'\',''))</code></p>
<p>This is the array for the key columns:</p>
<p><code>[&quot;taxonomie_id&quot;,&quot;dim_date&quot;,&quot;dim_fullPageUrl&quot;,&quot;dim_pagePath&quot;,&quot;dim_articleId&quot;,&quot;dim_articleType&quot;,&quot;dim_pageReferrer&quot;,&quot;dim_pageTitle&quot;,&quot;dim_sessionSource&quot;,&quot;dim_type&quot;]</code></p>
<p>This is the source data output of the SQL query above:</p>
<p><a href=""https://i.stack.imgur.com/NkBbq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NkBbq.png"" alt="""" /></a></p>
<p>I have limited it to 10 records.</p>
<p>This is the content of the sink table after the first run:</p>
<p><a href=""https://i.stack.imgur.com/aOgE2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aOgE2.png"" alt="""" /></a></p>
<p>Now, if I restart the pipeline, without changing anything, I end up with twice as many rows as before:</p>
<p><a href=""https://i.stack.imgur.com/3a8Ii.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3a8Ii.png"" alt="""" /></a></p>
<p>I ve tested the column keys on a much simpler data set and it is working. I even imported the mapping scheme and entered all keys manually. The behaviour is the same.</p>
<p>I must be doing something wrong, but I cant figure it out.</p>
","<azure-data-factory>","2023-03-20 14:14:04","46","0","1","75792205","<p>Key columns cant be NULL. I put a COALESCE() arround every key column and now it works</p>
"
"75791267","Saving pyspark dataframe from azure databricks to azure data lake in a specific folder created dynamically","<p>I am doing some ETL process in Azure.</p>
<pre><code>1. Source data is in Azure data lake
2. Processing it in Azure databricks
3. Loading the output dataframe in Azure data lake to a specific folder 
   considering Current year / Month / date and then file name in csv format.
</code></pre>
<p>I am stuck in 3rd step -</p>
<pre><code>1. Tried loading the dataframe to mnt location
   outPath = '/dbfs/mnt/abcd/&lt;outputfoldername&gt;/' + cy_year + &quot;/&quot; + 
   cy_month + &quot;/&quot; + cy_date + &quot;/&quot;

df.coalesce(1).write.mode(&quot;overwrite&quot;).format(&quot;com.databricks.spark.csv&quot;).opt 
 ion(&quot;header&quot;,&quot;true&quot;).csv(outPath)
</code></pre>
<p>This is saving data to DBFS but not to ADLS as suggested by many links over internet.</p>
<pre><code>2. Tried working like - 
   spark.conf.set(&quot;fs.azure.account.key.&lt;storage account 
   name&gt;.dfs.core.windows.net&quot;, &quot;&lt;&lt;ACCESS KEY&quot;)

  output_container_path = &quot;abfss://&lt;container- 
  name&gt;@salesdetails.dfs.core.windows.net/&lt;dir path&gt;&quot;

  df.coalesce(1).write.format(&quot;csv&quot;).mode(&quot;overwrite&quot;).option(&quot;header&quot;, 
  &quot;true&quot;).format(&quot;com.databricks.spark.csv&quot;).save(output_container_path)
</code></pre>
<p>This is saving into data into ADLS but into 4 files. 3 are supported ones. I want only one final file name example abc.csv</p>
<pre><code>3. Tried with pandas dataframe which gives us flexibility to name the 
   file name but here we will need specific folder name which is not the 
   case with me.
</code></pre>
<blockquote class=""spoiler"">
<p> Please assist at the earliest. Many thanks in advance</p>
</blockquote>
","<azure><azure-databricks><azure-data-factory><databricks-community-edition>","2023-03-20 14:05:22","139","0","1","75798976","<blockquote>
<p><strong>Follow these steps:</strong></p>
</blockquote>
<p>1.Mount your storage account with azure data lake gen 2 as per <a href=""https://learn.microsoft.com/en-us/azure/databricks/dbfs/mounts"" rel=""nofollow noreferrer"">MS_Doc</a>.</p>
<pre><code>configs = {&quot;fs.azure.account.auth.type&quot;: &quot;OAuth&quot;,
          &quot;fs.azure.account.oauth.provider.type&quot;: &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;,
          &quot;fs.azure.account.oauth2.client.id&quot;: &quot;f4dab6c8-5009-4857xxxxxxxxxxxxx&quot;,
          &quot;fs.azure.account.oauth2.client.secret&quot;:&quot;3GF8Q~3ZGkgflxxxxxxxxx&quot;,
          &quot;fs.azure.account.oauth2.client.endpoint&quot;: &quot;https://login.microsoftonline.com/72f988bfxxxxxxx/oauth2/token&quot;}

dbutils.fs.mount(
  source = &quot;abfss://demo123@vamblob.dfs.core.windows.net/&quot;,
  mount_point = &quot;/mnt/abcd11&quot;,
  extra_configs = configs)
</code></pre>
<p><img src=""https://i.imgur.com/HWzq0Rd.png"" alt=""enter image description here"" /></p>
<p>2.Configure your storage account and read the Source data is in Azure data lake.</p>
<pre><code>df11 = spark.read.format(&quot;csv&quot;).load(&quot;abfss://&lt;container&gt;@&lt;Storage_acccount&gt;.dfs.core.windows.net/&quot;)

display(df11)
</code></pre>
<p><img src=""https://i.imgur.com/uUYZZ24.png"" alt=""enter image description here"" /></p>
<p><strong>Sample_Code:</strong></p>
<pre><code>from pyspark.sql.functions import year, month, dayofmonth
from datetime import datetime

now = datetime.now()
year = now.year
month = now.month
day = now.day

folder12 = &quot;/mnt/abcd1/{}/{}/{}/output.csv&quot;.format(year, month, day)

# write the dataframe into the  folder in CSV format
df1.write.option(&quot;header&quot;, &quot;true&quot;).csv(folder12, mode=&quot;overwrite&quot;)
 
</code></pre>
<p><img src=""https://i.imgur.com/OJQTy47.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/FuPnJL5.png"" alt=""enter image description here"" /></p>
"
"75790929","Why do the file size differ in the adf run and the actual file?","<p>I just loaded 1gb of file from sftp to adls gen2. so in the pipeline run i can see the data read and data written to be 1.007GB(compressed) but when I look at the actual file in the ADLS, it is only 960mb.
This is the same case with a 2.3GB(compressed) file, in adls it is only 2.18GB.</p>
<p>The pipeline runs fine, succeeds infact. What is the reason to see this. is there any data loss?</p>
","<azure><azure-data-factory><azure-data-lake>","2023-03-20 13:34:03","25","0","1","75793399","<p>I found similar scenario in my environment in data factory run its showing 18.055 kb  but in ADLS its giving me 17.63 kb. as per below:</p>
<ul>
<li><p>Data factory run:
<img src=""https://i.imgur.com/g8v7Oi9.png"" alt=""enter image description here"" /></p>
</li>
<li><p>ADLS:
<img src=""https://i.imgur.com/CDEQUbO.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>But I found no data loss.</strong></p>
<p>The reason for this might be.</p>
<ul>
<li>The size of files can be decreased during the transmission procedure thanks to the built-in compression features of ADF. As a result, the file size indicated during the ADF process might not be the full size of the file.</li>
<li>The file may be changed to a different encoding during the upload procedure if it uses a character encoding other than the usual character encoding used by ADF or includes non-ASCII characters.</li>
</ul>
<p>It's crucial to understand that there may not always be an issue if the real file size differs from the ADF run. It's always a good idea to confirm that the file was transmitted properly and that all the required information was included, though.</p>
"
"75790744","JSON column does not appear in flatten activity in data flow in Azure Data Factory","<p>Using Azure Data Factory, I've copied data from an open-source API that requires no authorisation (<a href=""https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/q73"" rel=""nofollow noreferrer"">https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/q73</a>) and put into into an Azure blob storage account as a JSON file. Now I'm trying to flatten some columns in a data flow but I'm having issues.  In my source, I have a particular column called End (Organisation.Roles.Role.Date.End).  It exists in my source when I preview the data:</p>
<p><a href=""https://i.stack.imgur.com/U9WAN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U9WAN.png"" alt=""enter image description here"" /></a></p>
<p>One thing I've noticed is that for some reason the column isn't being recognised as a date but the main issue is that in the flatten activity, the column does not exist when trying to set it as an input column for mapping:</p>
<p><a href=""https://i.stack.imgur.com/S9Ub5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S9Ub5.png"" alt=""enter image description here"" /></a></p>
<p>I have set the <code>Unroll by</code> array to <code>Organisation.Roles.Role.Date</code>.</p>
<p>Does anyone have any idea about:</p>
<ol>
<li>Why this column is being read as a string rather than a date in my source activity?</li>
<li>Why this column does not appear in the flatten activity?</li>
</ol>
<p>Any help is appreciated! :) Apologies in advance if I've missed any information- I'm very new to this!</p>
<p>Edit:</p>
<p>If I add the below array to the input mapping:</p>
<p><a href=""https://i.stack.imgur.com/aOqVB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aOqVB.png"" alt=""enter image description here"" /></a></p>
<p>The End column appears in the data preview (but I still can't select it specifically in the input data mapping):</p>
<p><a href=""https://i.stack.imgur.com/To5xY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/To5xY.png"" alt=""enter image description here"" /></a></p>
<p>So it is clearly there!  I just can't map it as a separate column, which is what I want.</p>
","<arrays><json><azure><azure-data-factory>","2023-03-20 13:14:45","58","0","1","75791547","<p>I've figured it out!  A silly mistake of course...</p>
<p>A previous version of the API endpoint I was using did not have an End column included (it was null for that particular OdsCode).  Under the Projection section of the source activity in the data flow, I had imported the schema when only the previous version of the API endpoint was being included in my pipeline.  I didn't re-import the schema when I included the new version of the API endpoint with the new column.  Under the source activity, all I needed to do was hit this button and it auto-magically worked:</p>
<p><a href=""https://i.stack.imgur.com/bSE6Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bSE6Q.png"" alt=""enter image description here"" /></a></p>
"
"75788157","Azure Data Factory - Copy Data Task - Upsert is not updating","<p>I am using a copy data activity in Azure Data Factory. The source dataset is a Table on a Azure SQL server and the sink is aswell.
The source data looks like this:</p>
<p><a href=""https://i.stack.imgur.com/1wcLH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1wcLH.png"" alt=""Source data"" /></a></p>
<p>The sink data table has the same structure as the source, but I only want 1 record per taxonomie_id and dim_date. The value screenPageViews should be updated.</p>
<p>This is the sink data I get after the first run of the Copy Data Activity, with Upsert activated and taxonomie_id and dim_date columns as keys:</p>
<p><a href=""https://i.stack.imgur.com/D5Tpw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D5Tpw.png"" alt=""Sink after 1st run"" /></a></p>
<p>I thought I would end up with just one record per key combination.</p>
<p>Something like this:</p>
<pre><code>79540;20230316;1

79560;20230316;2

79530;20230316;3
</code></pre>
<p>It even gets worse if I run the pipeline a second time. I then get this:</p>
<p><a href=""https://i.stack.imgur.com/qy9xd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qy9xd.png"" alt=""Sink data after second run"" /></a></p>
<p>I now have twice as many records as before and at this point, the data is useless and wrong.</p>
<p>This the source tab in the Copy Data Activity:</p>
<p><a href=""https://i.stack.imgur.com/aJtIZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aJtIZ.png"" alt=""Source configuration of copy data activity"" /></a></p>
<p>This is the query:</p>
<pre><code>SELECT 
taxonomie_id,
dim_date,
screenPageViews 
FROM stg0_ga4.dateScreenPageViews_Page 
ORDER BY 2 DESC,1 DESC;
</code></pre>
<p>This is the sink configuration:</p>
<p><a href=""https://i.stack.imgur.com/AWLYC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AWLYC.png"" alt=""Sink configuration of copy data activity"" /></a></p>
<p>What am I doing wrong? Do I missunderstand the concept of the upsert functionality?</p>
","<azure-data-factory>","2023-03-20 08:50:08","90","0","2","75788331","<p>The upsert moves all rows from source to sink, and since you have multiple rows per key in your source, it would move all the rows to sink.
Try an aggregation function on your source query:</p>
<pre><code>SELECT 
taxonomie_id,
dim_date,
max(screenPageViews) as screenPageViews
FROM stg0_ga4.dateScreenPageViews_Page 
GROUP BY taxonomie_id,dim_date
</code></pre>
<p>To only move one rows per key. Just make user the max aggregate is the right function in you case.</p>
"
"75788157","Azure Data Factory - Copy Data Task - Upsert is not updating","<p>I am using a copy data activity in Azure Data Factory. The source dataset is a Table on a Azure SQL server and the sink is aswell.
The source data looks like this:</p>
<p><a href=""https://i.stack.imgur.com/1wcLH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1wcLH.png"" alt=""Source data"" /></a></p>
<p>The sink data table has the same structure as the source, but I only want 1 record per taxonomie_id and dim_date. The value screenPageViews should be updated.</p>
<p>This is the sink data I get after the first run of the Copy Data Activity, with Upsert activated and taxonomie_id and dim_date columns as keys:</p>
<p><a href=""https://i.stack.imgur.com/D5Tpw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D5Tpw.png"" alt=""Sink after 1st run"" /></a></p>
<p>I thought I would end up with just one record per key combination.</p>
<p>Something like this:</p>
<pre><code>79540;20230316;1

79560;20230316;2

79530;20230316;3
</code></pre>
<p>It even gets worse if I run the pipeline a second time. I then get this:</p>
<p><a href=""https://i.stack.imgur.com/qy9xd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qy9xd.png"" alt=""Sink data after second run"" /></a></p>
<p>I now have twice as many records as before and at this point, the data is useless and wrong.</p>
<p>This the source tab in the Copy Data Activity:</p>
<p><a href=""https://i.stack.imgur.com/aJtIZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aJtIZ.png"" alt=""Source configuration of copy data activity"" /></a></p>
<p>This is the query:</p>
<pre><code>SELECT 
taxonomie_id,
dim_date,
screenPageViews 
FROM stg0_ga4.dateScreenPageViews_Page 
ORDER BY 2 DESC,1 DESC;
</code></pre>
<p>This is the sink configuration:</p>
<p><a href=""https://i.stack.imgur.com/AWLYC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AWLYC.png"" alt=""Sink configuration of copy data activity"" /></a></p>
<p>What am I doing wrong? Do I missunderstand the concept of the upsert functionality?</p>
","<azure-data-factory>","2023-03-20 08:50:08","90","0","2","75789300","<blockquote>
<p>What am I doing wrong? Do I missunderstand the concept of the upsert functionality?</p>
</blockquote>
<p><strong>Working of <code>UPSERT</code> function in copy activity.</strong></p>
<ul>
<li>When a key column value is missing from the target database, the <code>upsert</code> command adds data and changes the values of other rows.</li>
<li>As a result, it is updating all entries without regard to data modifications. There is no method to instruct copy action to ignore the situation where a complete row already appears in the target database, unlike SQL's <code>Merge</code> function.</li>
<li>Therefore, even when the value of <code>key_column</code> fits, it will change the values for the other columns.</li>
<li><code>Upsert</code> function check all the rows of source to all the rows of sink and the based upon matching on key columns it will update or insert record in sink.</li>
</ul>
<blockquote>
<p>I only want 1 record per taxonomie_id and dim_date.</p>
</blockquote>
<p>Agreed with Chen Hirsh <strong>as per your assumed Output</strong> and you want only 1 record per <code>taxonomie_id</code> and <code>dim_dateyou</code> so you have to group them and for <code>screenPageViews</code> you have to either group by function or aggregate function to select <code>max</code> (aggregate function) <strong>screenPageViews</strong> by grouping other two columns.</p>
<p>With <code>Group By</code> function:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT taxonomie_id, dim_date, screenPageViews
FROM [dbo].[sql1demo]
GROUP  BY taxonomie_id,dim_date,screenPageViews
Order  by  2  desc,  1  desc
</code></pre>
<p>This will give you only 1 record with per <strong>taxonomie_id</strong> and <strong>dim_date</strong></p>
<p>Without group by function or aggregate function for <strong>screenPageViews</strong> it will throw you an error as:</p>
<p><img src=""https://i.imgur.com/jVwW6en.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>I dont want the max value, I want the data to be updated in that order.</p>
</blockquote>
<ul>
<li>If sink also has multiple entries it will update that it will not give you single entry for all similar entries.</li>
<li>If sink doesn't have any data the <code>upsert</code> function will insert all the data that is not matching with key columns.</li>
</ul>
"
"75782059","How to Access Web Activity Output Elements from Data Array in Next Activity in ADF","<p>I am using a Web activity and output is below after GET:</p>
<p>{
&quot;status&quot;: &quot;success&quot;,
&quot;data&quot;: [
{
&quot;id&quot;: 1,
&quot;employee_name&quot;: &quot;Tiger Nixon&quot;,
&quot;employee_salary&quot;: 320800,
&quot;employee_age&quot;: 61,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 2,
&quot;employee_name&quot;: &quot;Garrett Winters&quot;,
&quot;employee_salary&quot;: 170750,
&quot;employee_age&quot;: 63,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 3,
&quot;employee_name&quot;: &quot;Ashton Cox&quot;,
&quot;employee_salary&quot;: 86000,
&quot;employee_age&quot;: 66,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 4,
&quot;employee_name&quot;: &quot;Cedric Kelly&quot;,
&quot;employee_salary&quot;: 433060,
&quot;employee_age&quot;: 22,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 5,
&quot;employee_name&quot;: &quot;Airi Satou&quot;,
&quot;employee_salary&quot;: 162700,
&quot;employee_age&quot;: 33,
&quot;profile_image&quot;: &quot;&quot;
}......................</p>
<p>Now in <strong>If Condition Activity</strong>, <em>I want to write a check on &quot;employee_age&quot; &lt;30 from all the elements.</em></p>
<p>(<a href=""https://i.stack.imgur.com/yaZTK.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/yaZTK.png</a>)</p>
<p>Using expression &quot;@less( activity('Web').output.data.employee_age,30 )&quot; fails.</p>
<p>If using &quot;@less( activity('Web').output.data[0].employee_age,30 )&quot; it only applies the rule to first array index.</p>
<p>How to apply rule on all array indexes?</p>
<p>Expect the result to access all array indexes and apply condition check from Web activity output.</p>
","<arrays><if-statement><azure-data-factory>","2023-03-19 12:43:35","55","0","2","75782295","<p>Since the result of the web activity is an array of objects, you need to use a foreach activity to loop on all the objects, and put the if activity inside the foreach , to test each object.
<a href=""https://i.stack.imgur.com/7xnef.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7xnef.png"" alt=""enter image description here"" /></a></p>
"
"75782059","How to Access Web Activity Output Elements from Data Array in Next Activity in ADF","<p>I am using a Web activity and output is below after GET:</p>
<p>{
&quot;status&quot;: &quot;success&quot;,
&quot;data&quot;: [
{
&quot;id&quot;: 1,
&quot;employee_name&quot;: &quot;Tiger Nixon&quot;,
&quot;employee_salary&quot;: 320800,
&quot;employee_age&quot;: 61,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 2,
&quot;employee_name&quot;: &quot;Garrett Winters&quot;,
&quot;employee_salary&quot;: 170750,
&quot;employee_age&quot;: 63,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 3,
&quot;employee_name&quot;: &quot;Ashton Cox&quot;,
&quot;employee_salary&quot;: 86000,
&quot;employee_age&quot;: 66,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 4,
&quot;employee_name&quot;: &quot;Cedric Kelly&quot;,
&quot;employee_salary&quot;: 433060,
&quot;employee_age&quot;: 22,
&quot;profile_image&quot;: &quot;&quot;
},
{
&quot;id&quot;: 5,
&quot;employee_name&quot;: &quot;Airi Satou&quot;,
&quot;employee_salary&quot;: 162700,
&quot;employee_age&quot;: 33,
&quot;profile_image&quot;: &quot;&quot;
}......................</p>
<p>Now in <strong>If Condition Activity</strong>, <em>I want to write a check on &quot;employee_age&quot; &lt;30 from all the elements.</em></p>
<p>(<a href=""https://i.stack.imgur.com/yaZTK.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/yaZTK.png</a>)</p>
<p>Using expression &quot;@less( activity('Web').output.data.employee_age,30 )&quot; fails.</p>
<p>If using &quot;@less( activity('Web').output.data[0].employee_age,30 )&quot; it only applies the rule to first array index.</p>
<p>How to apply rule on all array indexes?</p>
<p>Expect the result to access all array indexes and apply condition check from Web activity output.</p>
","<arrays><if-statement><azure-data-factory>","2023-03-19 12:43:35","55","0","2","75797773","<ul>
<li>You can use <code>filter</code> activity on the result of your web activity (on data array). I have taken the sample web activity response in a json file and used lookup to read it.</li>
</ul>
<p><img src=""https://i.imgur.com/ei76XP2.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now in filter activity, use the following dynamic content for items and condition respectively to get the objects where employee age is less than 30.</li>
</ul>
<pre><code>#since I used look up, I am using the following dynamic content. You can use webactivity response to point to the data array


items : @activity('Lookup1').output.value[0].data

condition : @greater(30,item().employee_age)
</code></pre>
<ul>
<li>The result would be as shown below for the above case:</li>
</ul>
<p><img src=""https://i.imgur.com/qnjdJQp.png"" alt=""enter image description here"" /></p>
"
"75772915","Mapping JSON columns Azure Data Factory","<p>I'm trying to map the following request to an SQL database
<a href=""https://api.exchangerate.host/timeseries?start_date=2023-01-01&amp;end_date=2023-01-10&amp;base=USD&amp;symbols=MXN&amp;format=json"" rel=""nofollow noreferrer"">https://api.exchangerate.host/timeseries?start_date=2023-01-01&amp;end_date=2023-01-10&amp;base=USD&amp;symbols=MXN&amp;format=json</a></p>
<p>The issue is that dates are constantly changing and I need somehow to map them with some sort of wildcard. I cannot use dataflows for this particular case.</p>
<p><a href=""https://i.stack.imgur.com/mjI1w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mjI1w.png"" alt=""enter image description here"" /></a></p>
<p>At the end I need two custom columns with pre-defined integers, the date and the rate value</p>
","<azure><http><azure-data-factory>","2023-03-17 23:17:04","98","0","1","75800676","<blockquote>
<p>At the end I need two custom columns with pre-defined integers, the date and the rate value</p>
</blockquote>
<p>By using your request, I am able to get your requirement done using split and for loops and <code>openjson()</code>.</p>
<blockquote>
<p>The issue is that dates are constantly changing and I need somehow to map them with some sort of wildcard</p>
</blockquote>
<p>As the dates are changing, you cannot access the values dynamically. To get it dynamically, first I have taken your request in a web activity and used a split on the string of <code>rates</code> object with <code>':{&quot;MXN&quot;:'</code> with the below dynamic content.</p>
<p><code>@split(string(activity('Web1').output.rates),':{&quot;MXN&quot;:')</code></p>
<p>I have stored this array in an array variable.</p>
<p><img src=""https://i.imgur.com/su3umOu.png"" alt=""enter image description here"" /></p>
<p>Then I have passed this array this ForEach by skipping the last value in it with below dynamic content.</p>
<p><code>@take(variables('split_arr'),sub(length(variables('split_arr')),1))</code></p>
<p>I am using this ForEach to build a JSON array with date and rate as objects in every index. For that I have used an append variable activity inside forEach to <code>json_arr</code> array variable with following dynamic content.</p>
<pre><code>@json(concat('{&quot;date&quot;:&quot;',substring(item(),sub(length(item()),11), 10),'&quot;,&quot;rate&quot;:',activity('Web1').output.rates[substring(item(),sub(length(item()),11), 10)].MXN,'}'))
</code></pre>
<p><img src=""https://i.imgur.com/hjRLr5L.png"" alt=""enter image description here"" /></p>
<p>It will give the JSON array like below.</p>
<p><img src=""https://i.imgur.com/DTmRmO0.png"" alt=""enter image description here"" /></p>
<p>Use this JSON array with SQL as source to copy activity and use the <code>openjson()</code> in query like below.</p>
<pre><code>declare @json nvarchar(max) = N'@{variables('json_arr')}';
SELECT date,rate FROM OPENJSON(@json) WITH (
date varchar(max),
rate decimal(8,6)
);
</code></pre>
<p><img src=""https://i.imgur.com/BIzkzVS.png"" alt=""enter image description here"" /></p>
<p>In sink of copy activity, give your sink as per your requirement. Here I have used an SQL table.</p>
<p><strong>This is my pipeline JSON for your reference:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline2&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Web1&quot;,
            &quot;type&quot;: &quot;WebActivity&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;url&quot;: &quot;https://api.exchangerate.host/timeseries?start_date=2023-01-01&amp;end_date=2023-01-10&amp;base=USD&amp;symbols=MXN&amp;format=json&quot;,
                &quot;method&quot;: &quot;GET&quot;
            }
        },
        {
            &quot;name&quot;: &quot;Set variable1&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Web1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;split_arr&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@split(string(activity('Web1').output.rates),':{\&quot;MXN\&quot;:')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@take(variables('split_arr'),sub(length(variables('split_arr')),1))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;json_arr&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@json(concat('{\&quot;date\&quot;:\&quot;',substring(item(),sub(length(item()),11), 10),'\&quot;,\&quot;rate\&quot;:',activity('Web1').output.rates[substring(item(),sub(length(item()),11), 10)].MXN,'}'))&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Set variable2&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;ForEach1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;json_arr1&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@variables('json_arr')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Copy data1&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable2&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                    &quot;sqlReaderQuery&quot;: {
                        &quot;value&quot;: &quot;declare @json nvarchar(max) = N'@{variables('json_arr')}';\n\nSELECT date,rate FROM OPENJSON(@json) WITH (\n   date varchar(max),\n\trate decimal(8,6)\n);&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                    &quot;partitionOption&quot;: &quot;None&quot;
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSink&quot;,
                    &quot;writeBehavior&quot;: &quot;insert&quot;,
                    &quot;sqlWriterUseTableLock&quot;: false,
                    &quot;tableOption&quot;: &quot;autoCreate&quot;,
                    &quot;disableMetricsCollection&quot;: false
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;targetsql&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        }
    ],
    &quot;variables&quot;: {
        &quot;split_arr&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;json_arr&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;json_arr1&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: [],
    &quot;lastPublishTime&quot;: &quot;2023-03-20T16:55:11Z&quot;
},
&quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/Q7IPrYi.png"" alt=""enter image description here"" /></p>
"
"75772583","Data flow mapping custom join getting error DF-Executor-ColumnNotFound but column exists","<p>I have an azure mapping data flow that is called from an azure-synapse pipeline.  I need to change one of the joins in the mapping data flow from inner to custom because the correct date isn't being returned and I need to use between, equal, and, or functions for the join.</p>
<p>Here is the current join <a href=""https://i.stack.imgur.com/soPnA.png"" rel=""nofollow noreferrer"">current inner join</a>, I underlined the join between the mgr id's on the inner join.</p>
<p>I started out simple on the custom join, because this is the first time I have used the custom join.  This is first part of my logic for the custom join expression,
<a href=""https://i.stack.imgur.com/cdX1d.png"" rel=""nofollow noreferrer"">custom join expression</a>, I underlined the search the expression values and I see the value listed.  I have tried typing it in and I have tried clicking on it and it doesn't like it either way.</p>
<p>This is the part of the data flow mapping, where the problem is happening. <a href=""https://i.stack.imgur.com/7VeKN.png"" rel=""nofollow noreferrer"">join5 managers for results of join4</a></p>
<p>When I get pass this simple part of the join error, my final expression will be
and(equals(Asgnmt_PS_MANAGER_POSITION_ID,mgr_PS_Manager_position_id),or(between(selectemployees@EFF_DT, Mgr_comp_eff_dt,Mgr_comp_eff_end_dt),between(selectemployees@END_EFF_DT,Mgr_comp_eff_dt,Mgr_comp_eff_end_dt)))</p>
<p>Any idea's on what I am doing wrong?</p>
<p>Thanks!</p>
<p>I tried clicking to add the expression value in and I tried typing them in.  I am starting with a simple version of the join.</p>
","<azure-functions><azure-synapse><azure-data-factory>","2023-03-17 22:06:10","63","0","1","75788750","<p>I reproduced this error for sample datasets. Initially I had column called Time in my <code>source2</code>. Again when running the dataflow, the column is removed and it gave error message like <code>at source 'source1'....unavailable or invalid</code>.</p>
<p><img src=""https://i.imgur.com/5inu0wA.png"" alt=""enter image description here"" /></p>
<p>Below are some potential fixes for this issue:</p>
<ul>
<li><p>Verify that the input dataset used in the join transformation has the column <strong>mgr_PS_Manager_position_id</strong>. You may achieve this by taking a look at the input dataset's schema in the data flow.</p>
</li>
<li><p>Check that the join transformation and the custom expression both use the right spelling for the column name.</p>
</li>
<li><p>Make sure the input dataset used in the join transformation has the correct definition and availability of the column <code>mgr_PS_Manager_position_id</code> if it was created through a prior transformation.</p>
</li>
<li><p>You can give expression like
<code>equals(join5@Asgnmt_PS_MANAGER_POSITION_ID,managers@mgr_PS_Manager_position_id)</code>, so that, <code>mgr_PS_Manager_position_id</code> will</p>
</li>
<li><p>Make sure the file schema of <strong>managers</strong> matches the schema if the input dataset is a file-based dataset.</p>
</li>
</ul>
"
"75771327","How to store 10000 character string in a Column ,Does ADF reads it or truncates it in Dataflow in the sink","<p>I have a column called Notes Where user will enter their long notes which can go greater than 10000 characters ,they store it in col, in csv .</p>
<p>If I read that file in Dataflow and sink to blob as csv as Sql doesnot support more than 8000 characters .</p>
<p>Will it sink or truncate the column to its limit because i want to know the limit .</p>
","<azure><azure-data-factory>","2023-03-17 19:06:47","38","0","1","75771846","<p>Dataflow is converted to Spark at runtime. Spark implements a StringType, which has no limit defined. So as long as both the underlying source and sink dataset 's linked services support sizes greater than 8000, Dataflow should be able to handle the full size in both Source and Sink.</p>
<p>Likewise, Copy activity uses the same Datasets. Schemas are optional in Datasets, and even if you do define one, it also uses a generic String type with no size limit.</p>
<p>In other words, if you are reading a text file and writing to a text file, there is effectively no limit and there should be no truncation.</p>
<p>NOTE: if you later try to query this file in a SQL-based system, like Synapse Serverless SQL or SQL Polybase, it will fail on the first row that contains column data over 8000.</p>
"
"75770281","Azure DataBricks ImportError: cannot import name dataclass_transform","<p>I have a python notebook running the following imports on a DataBricks cluster</p>
<pre><code>%pip install presidio_analyzer
%pip install presidio_anonymizer
import spacy.cli
spacy.cli.download(&quot;en_core_web_lg&quot;)
nlp = spacy.load(&quot;en_core_web_lg&quot;)
import csv
import pprint
import collections
from typing import List, Iterable, Optional, Union, Dict
import pandas as pd
from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import EngineResult
</code></pre>
<p>To install and run the Microsoft Presidio library to anonymise data.</p>
<p>The code works fine and runs when called through the Databricks notebooks UI, but when attempting to call this notebook as a step in Azure Data Factory pipelines, it gives the following error:</p>
<pre><code>&quot;runError&quot;: &quot;ImportError: cannot import name dataclass_transform&quot;
</code></pre>
<p>From trial and error in the Databricks UI, I can determine that this error was generated due to missing certain parts of the imported libraries but the commands given at the beginning of the code resolved this in DataBricks notebooks.</p>
<p>I cannot reason why this step will not work when called as an ADF step.</p>
","<python><azure><azure-data-factory><databricks><presidio>","2023-03-17 17:00:05","250","0","2","75782225","<p>I had a similar issue in my environment these days. It looks like this is caused by spaCy version 3.5.0. I downgraded  (explicitly specified) to use version 3.3.0 (3.4.0 maybe also works) and it was working again.</p>
"
"75770281","Azure DataBricks ImportError: cannot import name dataclass_transform","<p>I have a python notebook running the following imports on a DataBricks cluster</p>
<pre><code>%pip install presidio_analyzer
%pip install presidio_anonymizer
import spacy.cli
spacy.cli.download(&quot;en_core_web_lg&quot;)
nlp = spacy.load(&quot;en_core_web_lg&quot;)
import csv
import pprint
import collections
from typing import List, Iterable, Optional, Union, Dict
import pandas as pd
from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import EngineResult
</code></pre>
<p>To install and run the Microsoft Presidio library to anonymise data.</p>
<p>The code works fine and runs when called through the Databricks notebooks UI, but when attempting to call this notebook as a step in Azure Data Factory pipelines, it gives the following error:</p>
<pre><code>&quot;runError&quot;: &quot;ImportError: cannot import name dataclass_transform&quot;
</code></pre>
<p>From trial and error in the Databricks UI, I can determine that this error was generated due to missing certain parts of the imported libraries but the commands given at the beginning of the code resolved this in DataBricks notebooks.</p>
<p>I cannot reason why this step will not work when called as an ADF step.</p>
","<python><azure><azure-data-factory><databricks><presidio>","2023-03-17 17:00:05","250","0","2","75789159","<p>The solution was that that libraries needed to be installed directly on the cluster via the compute tab in the DataBricks UI. I am unclear why the install commands failed to run when called from an Azure DF pipeline. If anyone has a clear answer as to why, please expand on my answer.</p>
"
"75769197","Direct copying data from Azure Databricks Delta Lake is only supported when sink is a folder, please enable staging or fix File path","<p>Currently, I am trying to copy data from Azure Databricks Delta Lake to my Azure Data Lake through Azure Data Factory.
I want to copy to a dynamic directory and a dynamic file name, but I keep receiving this error &quot;Direct copying data from Azure Databricks Delta Lake is only supported when sink is a folder, please enable staging or fix File path&quot; however, when I chose a fix file path and a fix container the pipeline works, but I am not able to copying in different directories. Does anyone face something like this? Who can advise.</p>
<p>However, I tried to enable staging and the error is fixed, but I get another error Databricks is not authorized to perform this operation using this permission xxx 403...</p>
","<azure-data-factory><azure-databricks><delta-lake>","2023-03-17 15:07:27","133","0","1","75790296","<blockquote>
<p>I want to copy to a dynamic directory and a dynamic file name</p>
</blockquote>
<p>To achieve this, you can use dataset parameters like below. Create dataset parameters for the folder name and file name and give them in the file path.</p>
<p><img src=""https://i.imgur.com/kNTmYdW.png"" alt=""enter image description here"" /></p>
<p>In copy activity sink, you can give the folder names and file names dynamically as per your requirement.</p>
<p><img src=""https://i.imgur.com/FNG9RM9.png"" alt=""enter image description here"" /></p>
<p>If you want to copy the delta table to only a dynamic folder, you can use only one parameter for the folder name.</p>
<p>If you want copy to file name as well, then in this case you need to enable the staging and give the staging location.</p>
<p>In above both cases make sure you give the following in the spark config of the databricks cluster(as you mentioned you are copying to ADLS).</p>
<pre><code>spark.databricks.delta.autoCompact.enabled true
spark.databricks.delta.optimizeWrite.enabled true
spark.databricks.delta.preview.enabled true
spark.hadoop.fs.azure.account.key.rakeshgen2.dfs.core.windows.net &lt;Access key&gt;
</code></pre>
<p><img src=""https://i.imgur.com/t7fhqNp.png"" alt=""enter image description here"" /></p>
<p><strong>My Result when copied to folder:</strong></p>
<p><img src=""https://i.imgur.com/s4krAuI.png"" alt=""enter image description here"" /></p>
<p><strong>My Result when copied to single file:</strong></p>
<p><img src=""https://i.imgur.com/V9t0bfo.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>but I get another error Databricks is not authorized to perform this operation using this permission xxx 403..</p>
</blockquote>
<p>This error occurs when you don't have permission to read or write the data to your Storage account. Make sure you have the required permissions.</p>
"
"75768891","Azure Data Factory: How to check if a folder exists","<p>I've setup a Cost Management export that is trigged weekly to a Storage Account.</p>
<p>In that storage account the CSV is exported inside a container, a folder, another folder, and then inside a folder with <code>FROM-TO</code> dates:</p>
<p><a href=""https://i.stack.imgur.com/jiqQk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jiqQk.png"" alt=""enter image description here"" /></a></p>
<p>I would like Data Factory to check:</p>
<ul>
<li>If a folder exists --&gt; do something</li>
<li>If a folder exists but has the suffix &quot;*-imported&quot; --&gt; do nothing</li>
</ul>
<p>So I need a boolean.</p>
<p>So I created a Validation but the only thing I can can put is a binary:</p>
<p><a href=""https://i.stack.imgur.com/L2rxM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L2rxM.png"" alt=""enter image description here"" /></a></p>
<p>I don't need a binary or a CSV or whatever other file format.</p>
<p>How to check if a folder exists and if it has or not a suffix?</p>
<p>This <a href=""https://stackoverflow.com/questions/69002270/check-if-folder-exists-and-delete-it-in-azure-data-factory-adf"">link </a>is not helping.</p>
","<azure><ssis><azure-data-factory>","2023-03-17 14:38:51","133","0","2","75770804","<p>You do need a Dataset, and Binary makes the most sense for this scenario.</p>
<p>Create a Binary Dataset with a folder path parameter:</p>
<p><a href=""https://i.stack.imgur.com/2HMPl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2HMPl.png"" alt=""enter image description here"" /></a></p>
<p>Reference the parameters in the Connection tab:</p>
<p><a href=""https://i.stack.imgur.com/Kvn08.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kvn08.png"" alt=""enter image description here"" /></a></p>
<p>In the Pipeline, use GetMetadata. Point to this Dataset and select &quot;Exists&quot; under &quot;Field list&quot;:</p>
<p><a href=""https://i.stack.imgur.com/MYqn5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MYqn5.png"" alt=""enter image description here"" /></a></p>
<p>If you do not include &quot;-imported&quot; in the folder path, those will be ignored.</p>
<p>Here is a link to an article I wrote about using <a href=""https://causewaysolutions.com/blog/introduction-to-get-metadata-in-azure-data-factory/"" rel=""nofollow noreferrer"">GetMetadata</a> and another about <a href=""https://causewaysolutions.com/blog/creating-reusable-datasets-in-azure-data-factory-with-parameters/"" rel=""nofollow noreferrer"">Parameterized Datasets</a>, you may find them useful.</p>
"
"75768891","Azure Data Factory: How to check if a folder exists","<p>I've setup a Cost Management export that is trigged weekly to a Storage Account.</p>
<p>In that storage account the CSV is exported inside a container, a folder, another folder, and then inside a folder with <code>FROM-TO</code> dates:</p>
<p><a href=""https://i.stack.imgur.com/jiqQk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jiqQk.png"" alt=""enter image description here"" /></a></p>
<p>I would like Data Factory to check:</p>
<ul>
<li>If a folder exists --&gt; do something</li>
<li>If a folder exists but has the suffix &quot;*-imported&quot; --&gt; do nothing</li>
</ul>
<p>So I need a boolean.</p>
<p>So I created a Validation but the only thing I can can put is a binary:</p>
<p><a href=""https://i.stack.imgur.com/L2rxM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L2rxM.png"" alt=""enter image description here"" /></a></p>
<p>I don't need a binary or a CSV or whatever other file format.</p>
<p>How to check if a folder exists and if it has or not a suffix?</p>
<p>This <a href=""https://stackoverflow.com/questions/69002270/check-if-folder-exists-and-delete-it-in-azure-data-factory-adf"">link </a>is not helping.</p>
","<azure><ssis><azure-data-factory>","2023-03-17 14:38:51","133","0","2","75774259","<p>You can use two validation activities. First activity is to check if the folder <code>fromdate-todate-imported</code> exists. If it exists, do nothing. If first validation activity gets <strong>failed</strong>, add another validation activity to check if folder <code>fromdate-todate</code> exists. If exists, Do the required activities.</p>
<p><img src=""https://i.imgur.com/Efb78Xw.png"" alt=""enter image description here"" /></p>
<ul>
<li>In validation1, Binary dataset is taken, and file path is given as <code>containername/folder1/folder2/fromdate-todate-imported</code>. Filename is left blank in the file path.</li>
<li>In validation2, Binary dataset is taken, and file path is given as <code>containername/folder1/folder2/fromdate-todate</code>. Filename is left blank in the file path.</li>
</ul>
<p>I run pipeline for three different cases.</p>
<p><strong>case:1 Folder exists and folder name contains -imported</strong></p>
<p><img src=""https://i.imgur.com/WbUx55E.png"" alt=""enter image description here"" /></p>
<ul>
<li>When pipeline is executed, it is run succesfully. Only validation1 has run.
<img src=""https://i.imgur.com/RwPGo2d.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Case:2 When folder exists and folder name does not contain -imported</strong></p>
<p><img src=""https://i.imgur.com/odLJtWo.png"" alt=""enter image description here"" /></p>
<ul>
<li>When pipeline is run, all 3 activities are executed.
<img src=""https://i.imgur.com/hy9Jlxc.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Case:3 When folder doesn't exist</strong>
Results when pipeline is run,
<img src=""https://i.imgur.com/lL0NaQa.png"" alt=""enter image description here"" /></p>
"
"75764937","Azure Data factory, Data flow output to be uploaded to FTP","<p>Below is my dataflow. My requirement is
1)To merge 2 files.
2)Create a merged tab file
3) Upload the file to FTP</p>
<p><a href=""https://i.stack.imgur.com/TXLwQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TXLwQ.png"" alt=""enter image description here"" /></a></p>
<p>I was able to achieve first 2 steps. Now gettibg error while trying to upload to FTP.Showing as FTP is not supported as sink Dataset.</p>
<p>Is there any way I can achieve this.</p>
<p>My ultimate aim is to upload this merged file to FTP.</p>
<p>Getting error as below while running the pipeline for above dataflow.</p>
<p><a href=""https://i.stack.imgur.com/VJ4G4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VJ4G4.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-03-17 07:48:05","72","0","1","75767158","<p>FTP sink will not support Dataflow. As per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp?tabs=data-factory#supported-capabilities"" rel=""nofollow noreferrer"">MS Doc</a>,</p>
<p><img src=""https://i.imgur.com/GqrDiBe.png"" alt=""enter image description here"" /></p>
<p>You can raise the feature request for that <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">here</a>.</p>
<p><strong>Or</strong></p>
<p>You can try this alternative approach with custom activity or use logic app FTP operations.</p>
<p><img src=""https://i.imgur.com/jhLa7eu.png"" alt=""enter image description here"" /></p>
"
"75760405","Can we install ADF SHIR and Logic Apps on-prem Data Gateway on the same machine?","<p>We understand that a VM on-prem hosts the SHIR and this then is registered to the ADF.</p>
<p>Can we use same VM for installing the on-prem Data Gateway for Logic apps or both need to be on separate machines?</p>
","<azure-logic-apps><azure-logic-app-standard><azure-data-factory>","2023-03-16 18:38:47","44","0","1","75763640","<p>You can install Power BI SHIR and On Premises data gateway on the same server but it is ideally not recommended.</p>
<p>because both gateways communicate via same ports.</p>
<p><a href=""https://i.stack.imgur.com/BcKzH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BcKzH.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory</a></p>
"
"75759443","Can we cap throughput in ADF copy activity?","<p>Is there any way to cap throughput in ADF copy activity like we have in AzCopy ?</p>
<p>This is needed to ensure our Ingestion ( parallel Ingestion of all tables of a given source) doesn't consume all the available bandwidth</p>
","<azure-data-factory><azcopy>","2023-03-16 17:07:35","39","0","1","75767538","<p>No we cannot. As the docs suggest, the best throughput is given.</p>
<p><a href=""https://i.stack.imgur.com/8x8jq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8x8jq.png"" alt=""enter image description here"" /></a></p>
<p>When you run copy's in parallel, this shouldnt be a problem, as long as your source do not have any other task that is occupying the bandwidth (aside from these copy's).</p>
<p>If you reach throttling in the data store, reduce the concurrent workload. Maybe reducing the best throughput will be supported in future, but I doubt it as you will be overpaying for the resources you will not be using, because in the end you will still be occupying the same DIUs.</p>
"
"75759069","Include the integration runtime name as a parameter in the ARM Template of Data Factory","<p>I developed a CI/CD to deploy the content (pipelines, Global parameters, LinkedServices...) of a DEV Data Factory in a PROD Data Factory.</p>
<p>I use 2 shirs for the DEV and PROD environments where each environment uses its own shir.</p>
<p>When I publish content from the DEV Data Factory which is connected to an Azure DevOps repository, I generate the <code>ARMTemplateForFactory.json</code> and <code>ARMTemplateParametersForFactory.json</code> files</p>
<p>The problem is that the name of the shir, by default is not a parameter of the <code>ARMTemplateParametersForFactory.json</code> file</p>
<p>I would like to include the name of the integration runtime (shir) used in each environment as a parameter of the generated ARM Template when publishing.</p>
<p>After some research I found that you could modify the structure of the <code>arm-template-parameters-definition.json</code> file to customize the settings of the ARM Template. However, I don't know what Syntax used to do so: /</p>
<p>So my question is: how to include the name of the integration runtime as a parameter of the ARM Template.</p>
<p>Thanks a lot !</p>
<p><a href=""https://i.stack.imgur.com/lK2JV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lK2JV.png"" alt=""enter image description here"" /></a></p>
<p>This is the json description of my integration runtime :</p>
<pre><code>{
    &quot;name&quot;: &quot;TestRuntimeSam&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;Managed&quot;,
        &quot;typeProperties&quot;: {
            &quot;computeProperties&quot;: {
                &quot;location&quot;: &quot;AutoResolve&quot;,
                &quot;dataFlowProperties&quot;: {
                    &quot;computeType&quot;: &quot;General&quot;,
                    &quot;coreCount&quot;: 8,
                    &quot;timeToLive&quot;: 10,
                    &quot;cleanup&quot;: false
                }
            }
        }
    }
}
</code></pre>
<p>and this is the basic content of the <code>arm-template-parameters-definition.json</code> file :</p>
<pre><code>{
    &quot;Microsoft.DataFactory/factories&quot;: {
        &quot;properties&quot;: {
            &quot;globalParameters&quot;: {
                &quot;*&quot;: {
                    &quot;value&quot;: &quot;=&quot;
                }
            },
            &quot;globalConfigurations&quot;: {
                &quot;*&quot;: &quot;=&quot;
            },
            &quot;encryption&quot;: {
                &quot;*&quot;: &quot;=&quot;,
                &quot;identity&quot;: {
                    &quot;*&quot;: &quot;=&quot;
                }
            }
        },
        &quot;location&quot;: &quot;=&quot;,
        &quot;identity&quot;: {
            &quot;type&quot;: &quot;=&quot;,
            &quot;userAssignedIdentities&quot;: &quot;=&quot;
        }
    },
    &quot;Microsoft.DataFactory/factories/pipelines&quot;: {},
    &quot;Microsoft.DataFactory/factories/integrationRuntimes&quot;: {
        &quot;properties&quot;: {
            &quot;typeProperties&quot;: {
                &quot;ssisProperties&quot;: {
                    &quot;catalogInfo&quot;: {
                        &quot;catalogServerEndpoint&quot;: &quot;=&quot;,
                        &quot;catalogAdminUserName&quot;: &quot;=&quot;,
                        &quot;catalogAdminPassword&quot;: {
                            &quot;value&quot;: &quot;-::secureString&quot;
                        }
                    },
                    &quot;customSetupScriptProperties&quot;: {
                        &quot;sasToken&quot;: {
                            &quot;value&quot;: &quot;-::secureString&quot;
                        }
                    }
                },
                &quot;linkedInfo&quot;: {
                    &quot;key&quot;: {
                        &quot;value&quot;: &quot;-::secureString&quot;
                    },
                    &quot;resourceId&quot;: &quot;=&quot;
                },
                &quot;computeProperties&quot;: {
                    &quot;dataFlowProperties&quot;: {
                        &quot;externalComputeInfo&quot;: [
                            {
                                &quot;accessToken&quot;: &quot;-::secureString&quot;
                            }
                        ]
                    }
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/triggers&quot;: {
        &quot;properties&quot;: {
            &quot;pipelines&quot;: [
                {
                    &quot;parameters&quot;: {
                        &quot;*&quot;: &quot;=&quot;
                    }
                },
                &quot;pipelineReference.referenceName&quot;
            ],
            &quot;pipeline&quot;: {
                &quot;parameters&quot;: {
                    &quot;*&quot;: &quot;=&quot;
                }
            },
            &quot;typeProperties&quot;: {
                &quot;scope&quot;: &quot;=&quot;
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/linkedServices&quot;: {
        &quot;*&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;accountName&quot;: &quot;=&quot;,
                    &quot;accountEndpoint&quot;: &quot;=&quot;,
                    &quot;username&quot;: &quot;=&quot;,
                    &quot;userName&quot;: &quot;=&quot;,
                    &quot;accessKeyId&quot;: &quot;=&quot;,
                    &quot;endpoint&quot;: &quot;=&quot;,
                    &quot;servicePrincipalId&quot;: &quot;=&quot;,
                    &quot;userId&quot;: &quot;=&quot;,
                    &quot;host&quot;: &quot;=&quot;,
                    &quot;clientId&quot;: &quot;=&quot;,
                    &quot;existingClusterId&quot;: &quot;=&quot;,
                    &quot;clusterUserName&quot;: &quot;=&quot;,
                    &quot;clusterSshUserName&quot;: &quot;=&quot;,
                    &quot;hostSubscriptionId&quot;: &quot;=&quot;,
                    &quot;clusterResourceGroup&quot;: &quot;=&quot;,
                    &quot;subscriptionId&quot;: &quot;=&quot;,
                    &quot;resourceGroupName&quot;: &quot;=&quot;,
                    &quot;tenant&quot;: &quot;=&quot;,
                    &quot;dataLakeStoreUri&quot;: &quot;=&quot;,
                    &quot;baseUrl&quot;: &quot;=&quot;,
                    &quot;database&quot;: &quot;=&quot;,
                    &quot;serviceEndpoint&quot;: &quot;=&quot;,
                    &quot;batchUri&quot;: &quot;=&quot;,
                    &quot;poolName&quot;: &quot;=&quot;,
                    &quot;databaseName&quot;: &quot;=&quot;,
                    &quot;systemNumber&quot;: &quot;=&quot;,
                    &quot;server&quot;: &quot;=&quot;,
                    &quot;url&quot;: &quot;=&quot;,
                    &quot;functionAppUrl&quot;: &quot;=&quot;,
                    &quot;environmentUrl&quot;: &quot;=&quot;,
                    &quot;aadResourceId&quot;: &quot;=&quot;,
                    &quot;sasUri&quot;: &quot;|:-sasUri:secureString&quot;,
                    &quot;sasToken&quot;: &quot;|&quot;,
                    &quot;connectionString&quot;: &quot;|:-connectionString:secureString&quot;,
                    &quot;hostKeyFingerprint&quot;: &quot;=&quot;,
                    &quot;mlWorkspaceName&quot;: &quot;=&quot;
                }
            }
        },
        &quot;Odbc&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;userName&quot;: &quot;=&quot;,
                    &quot;connectionString&quot;: {
                        &quot;secretName&quot;: &quot;=&quot;
                    }
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/datasets&quot;: {
        &quot;*&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;folderPath&quot;: &quot;=&quot;,
                    &quot;fileName&quot;: &quot;=&quot;
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/credentials&quot;: {
        &quot;*&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;token&quot;: &quot;=&quot;
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/managedVirtualNetworks/managedPrivateEndpoints&quot;: {
        &quot;properties&quot;: {
            &quot;privateLinkResourceId&quot;: &quot;=&quot;,
            &quot;groupId&quot;: &quot;=&quot;,
            &quot;fqdns&quot;: &quot;=&quot;
        }
    },
    &quot;Microsoft.DataFactory/factories/globalparameters&quot;: {
        &quot;properties&quot;: {
            &quot;*&quot;: {
                &quot;value&quot;: &quot;=&quot;
            }
        }
    }
}
</code></pre>
<p>What shall I add ?</p>
","<azure><azure-data-factory>","2023-03-16 16:34:53","106","0","2","75759534","<p>Please open the {} symbol in from of the IR name and look through the properties
and then add them in the template under a section as shown in the screenshot below:
<a href=""https://i.stack.imgur.com/TuOTF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TuOTF.png"" alt=""enter image description here"" /></a></p>
<p>Please use this link : <a href=""https://patrick-picard.medium.com/azure-data-factory-modifying-arm-template-parameters-53b11f38bced"" rel=""nofollow noreferrer"">https://patrick-picard.medium.com/azure-data-factory-modifying-arm-template-parameters-53b11f38bced</a></p>
"
"75759069","Include the integration runtime name as a parameter in the ARM Template of Data Factory","<p>I developed a CI/CD to deploy the content (pipelines, Global parameters, LinkedServices...) of a DEV Data Factory in a PROD Data Factory.</p>
<p>I use 2 shirs for the DEV and PROD environments where each environment uses its own shir.</p>
<p>When I publish content from the DEV Data Factory which is connected to an Azure DevOps repository, I generate the <code>ARMTemplateForFactory.json</code> and <code>ARMTemplateParametersForFactory.json</code> files</p>
<p>The problem is that the name of the shir, by default is not a parameter of the <code>ARMTemplateParametersForFactory.json</code> file</p>
<p>I would like to include the name of the integration runtime (shir) used in each environment as a parameter of the generated ARM Template when publishing.</p>
<p>After some research I found that you could modify the structure of the <code>arm-template-parameters-definition.json</code> file to customize the settings of the ARM Template. However, I don't know what Syntax used to do so: /</p>
<p>So my question is: how to include the name of the integration runtime as a parameter of the ARM Template.</p>
<p>Thanks a lot !</p>
<p><a href=""https://i.stack.imgur.com/lK2JV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lK2JV.png"" alt=""enter image description here"" /></a></p>
<p>This is the json description of my integration runtime :</p>
<pre><code>{
    &quot;name&quot;: &quot;TestRuntimeSam&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;Managed&quot;,
        &quot;typeProperties&quot;: {
            &quot;computeProperties&quot;: {
                &quot;location&quot;: &quot;AutoResolve&quot;,
                &quot;dataFlowProperties&quot;: {
                    &quot;computeType&quot;: &quot;General&quot;,
                    &quot;coreCount&quot;: 8,
                    &quot;timeToLive&quot;: 10,
                    &quot;cleanup&quot;: false
                }
            }
        }
    }
}
</code></pre>
<p>and this is the basic content of the <code>arm-template-parameters-definition.json</code> file :</p>
<pre><code>{
    &quot;Microsoft.DataFactory/factories&quot;: {
        &quot;properties&quot;: {
            &quot;globalParameters&quot;: {
                &quot;*&quot;: {
                    &quot;value&quot;: &quot;=&quot;
                }
            },
            &quot;globalConfigurations&quot;: {
                &quot;*&quot;: &quot;=&quot;
            },
            &quot;encryption&quot;: {
                &quot;*&quot;: &quot;=&quot;,
                &quot;identity&quot;: {
                    &quot;*&quot;: &quot;=&quot;
                }
            }
        },
        &quot;location&quot;: &quot;=&quot;,
        &quot;identity&quot;: {
            &quot;type&quot;: &quot;=&quot;,
            &quot;userAssignedIdentities&quot;: &quot;=&quot;
        }
    },
    &quot;Microsoft.DataFactory/factories/pipelines&quot;: {},
    &quot;Microsoft.DataFactory/factories/integrationRuntimes&quot;: {
        &quot;properties&quot;: {
            &quot;typeProperties&quot;: {
                &quot;ssisProperties&quot;: {
                    &quot;catalogInfo&quot;: {
                        &quot;catalogServerEndpoint&quot;: &quot;=&quot;,
                        &quot;catalogAdminUserName&quot;: &quot;=&quot;,
                        &quot;catalogAdminPassword&quot;: {
                            &quot;value&quot;: &quot;-::secureString&quot;
                        }
                    },
                    &quot;customSetupScriptProperties&quot;: {
                        &quot;sasToken&quot;: {
                            &quot;value&quot;: &quot;-::secureString&quot;
                        }
                    }
                },
                &quot;linkedInfo&quot;: {
                    &quot;key&quot;: {
                        &quot;value&quot;: &quot;-::secureString&quot;
                    },
                    &quot;resourceId&quot;: &quot;=&quot;
                },
                &quot;computeProperties&quot;: {
                    &quot;dataFlowProperties&quot;: {
                        &quot;externalComputeInfo&quot;: [
                            {
                                &quot;accessToken&quot;: &quot;-::secureString&quot;
                            }
                        ]
                    }
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/triggers&quot;: {
        &quot;properties&quot;: {
            &quot;pipelines&quot;: [
                {
                    &quot;parameters&quot;: {
                        &quot;*&quot;: &quot;=&quot;
                    }
                },
                &quot;pipelineReference.referenceName&quot;
            ],
            &quot;pipeline&quot;: {
                &quot;parameters&quot;: {
                    &quot;*&quot;: &quot;=&quot;
                }
            },
            &quot;typeProperties&quot;: {
                &quot;scope&quot;: &quot;=&quot;
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/linkedServices&quot;: {
        &quot;*&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;accountName&quot;: &quot;=&quot;,
                    &quot;accountEndpoint&quot;: &quot;=&quot;,
                    &quot;username&quot;: &quot;=&quot;,
                    &quot;userName&quot;: &quot;=&quot;,
                    &quot;accessKeyId&quot;: &quot;=&quot;,
                    &quot;endpoint&quot;: &quot;=&quot;,
                    &quot;servicePrincipalId&quot;: &quot;=&quot;,
                    &quot;userId&quot;: &quot;=&quot;,
                    &quot;host&quot;: &quot;=&quot;,
                    &quot;clientId&quot;: &quot;=&quot;,
                    &quot;existingClusterId&quot;: &quot;=&quot;,
                    &quot;clusterUserName&quot;: &quot;=&quot;,
                    &quot;clusterSshUserName&quot;: &quot;=&quot;,
                    &quot;hostSubscriptionId&quot;: &quot;=&quot;,
                    &quot;clusterResourceGroup&quot;: &quot;=&quot;,
                    &quot;subscriptionId&quot;: &quot;=&quot;,
                    &quot;resourceGroupName&quot;: &quot;=&quot;,
                    &quot;tenant&quot;: &quot;=&quot;,
                    &quot;dataLakeStoreUri&quot;: &quot;=&quot;,
                    &quot;baseUrl&quot;: &quot;=&quot;,
                    &quot;database&quot;: &quot;=&quot;,
                    &quot;serviceEndpoint&quot;: &quot;=&quot;,
                    &quot;batchUri&quot;: &quot;=&quot;,
                    &quot;poolName&quot;: &quot;=&quot;,
                    &quot;databaseName&quot;: &quot;=&quot;,
                    &quot;systemNumber&quot;: &quot;=&quot;,
                    &quot;server&quot;: &quot;=&quot;,
                    &quot;url&quot;: &quot;=&quot;,
                    &quot;functionAppUrl&quot;: &quot;=&quot;,
                    &quot;environmentUrl&quot;: &quot;=&quot;,
                    &quot;aadResourceId&quot;: &quot;=&quot;,
                    &quot;sasUri&quot;: &quot;|:-sasUri:secureString&quot;,
                    &quot;sasToken&quot;: &quot;|&quot;,
                    &quot;connectionString&quot;: &quot;|:-connectionString:secureString&quot;,
                    &quot;hostKeyFingerprint&quot;: &quot;=&quot;,
                    &quot;mlWorkspaceName&quot;: &quot;=&quot;
                }
            }
        },
        &quot;Odbc&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;userName&quot;: &quot;=&quot;,
                    &quot;connectionString&quot;: {
                        &quot;secretName&quot;: &quot;=&quot;
                    }
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/datasets&quot;: {
        &quot;*&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;folderPath&quot;: &quot;=&quot;,
                    &quot;fileName&quot;: &quot;=&quot;
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/credentials&quot;: {
        &quot;*&quot;: {
            &quot;properties&quot;: {
                &quot;typeProperties&quot;: {
                    &quot;token&quot;: &quot;=&quot;
                }
            }
        }
    },
    &quot;Microsoft.DataFactory/factories/managedVirtualNetworks/managedPrivateEndpoints&quot;: {
        &quot;properties&quot;: {
            &quot;privateLinkResourceId&quot;: &quot;=&quot;,
            &quot;groupId&quot;: &quot;=&quot;,
            &quot;fqdns&quot;: &quot;=&quot;
        }
    },
    &quot;Microsoft.DataFactory/factories/globalparameters&quot;: {
        &quot;properties&quot;: {
            &quot;*&quot;: {
                &quot;value&quot;: &quot;=&quot;
            }
        }
    }
}
</code></pre>
<p>What shall I add ?</p>
","<azure><azure-data-factory>","2023-03-16 16:34:53","106","0","2","76129692","<p>I appreciate this isn't precisely the answer you were looking for (you wanted to know what to put in the parameters file).  I couldn't find that answer when I looked, I don't think it's possible, unless something has changed recently.  What I did find, is that you can add a Powershell Script, at the start of your Deployment pipeline, that does a find and replace in your Template File, and replaces the name.  It doesn't seem ideal, but it works.  See the answer given here:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/776353/how-can-we-parametrize-two-different-integration-r"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/776353/how-can-we-parametrize-two-different-integration-r</a></p>
"
"75758672","Getting ""Malformed records are detected in schema inference"" when trying to flatten JSON using a data flow in ADF","<p>Using Azure Data Factory, I've copied data from an open-source API that requires no authorisation (<a href=""https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/rxl"" rel=""nofollow noreferrer"">https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/rxl</a>) and put into into an Azure blob storage account as a JSON file.  Now I'm trying to flatten some columns in a data flow but I'm having issues with my source activity.  When I try to preview the data, I'm getting this error:</p>
<blockquote>
<p>Malformed records are detected in schema inference. Parse Mode: FAILFAST. It could be because of a wrong selection in document form to parse json file(s). Please try a different 'Document form' (Single document/Document per line/Array of documents) on the json source.</p>
</blockquote>
<p>I have tried changing the 'Document form' option to every option and I'm still getting the same error.</p>
<p>I can preview the data ok when looking at the dataset.  But I can't look at it in the data flow... Any help appreciated!</p>
<p>Thanks.</p>
<p>Here is a sample of the data:</p>
<pre><code>[
    {
        &quot;Organisation&quot;: {
            &quot;Rels&quot;: {
                &quot;Rel&quot;: [
                    {
                        &quot;Date&quot;: [
                            {
                                &quot;Start&quot;: &quot;2020-04-01&quot;,
                                &quot;Type&quot;: &quot;Operational&quot;
                            }
                        ],
                        &quot;id&quot;: &quot;RE5&quot;,
                        &quot;Status&quot;: &quot;Active&quot;,
                        &quot;Target&quot;: {
                            &quot;OrgId&quot;: {
                                &quot;assigningAuthorityName&quot;: &quot;HSCIC&quot;,
                                &quot;extension&quot;: &quot;QE1&quot;,
                                &quot;root&quot;: &quot;2.16.840.1.113883.2.1.3.2.4.18.48&quot;
                            },
                            &quot;PrimaryRoleId&quot;: {
                                &quot;id&quot;: &quot;RO261&quot;,
                                &quot;uniqueRoleId&quot;: 300734
                            }
                        },
                        &quot;uniqueRelId&quot;: 666658
                    },
                    {
                        &quot;Date&quot;: [
                            {
                                &quot;End&quot;: &quot;2020-03-31&quot;,
                                &quot;Start&quot;: &quot;2016-04-01&quot;,
                                &quot;Type&quot;: &quot;Operational&quot;
                            }
                        ],
                        &quot;id&quot;: &quot;RE5&quot;,
                        &quot;Status&quot;: &quot;Inactive&quot;,
                        &quot;Target&quot;: {
                            &quot;OrgId&quot;: {
                                &quot;assigningAuthorityName&quot;: &quot;HSCIC&quot;,
                                &quot;extension&quot;: &quot;Q84&quot;,
                                &quot;root&quot;: &quot;2.16.840.1.113883.2.1.3.2.4.18.48&quot;
                            },
                            &quot;PrimaryRoleId&quot;: {
                                &quot;id&quot;: &quot;RO210&quot;,
                                &quot;uniqueRoleId&quot;: 278955
                            }
                        },
                        &quot;uniqueRelId&quot;: 464825
                    }
...
</code></pre>
","<arrays><json><azure-data-factory>","2023-03-16 15:58:25","91","0","1","75763713","<ul>
<li>I got a similar error when I try to copy the file to blob storage as JSON file and then try to read it in dataflow (while importing projection)</li>
</ul>
<p><img src=""https://i.imgur.com/1W7QHpt.png"" alt=""enter image description here"" /></p>
<ul>
<li>In copy data activity sink settings, using file pattern as <code>Array of objects</code> is causing this error to occur (dataset preview is working fine same as in your case).  Instead, set this to file pattern as <code>Set of Objects</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/OKvB48E.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now reading this file after importing projection would give the following schema using JSON setting as <code>Document per line</code>:</li>
</ul>
<p><img src=""https://i.imgur.com/okWT7U9.png"" alt=""enter image description here"" /></p>
<ul>
<li>The Dataflow's data preview would look as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/FgnvFms.png"" alt=""enter image description here"" /></p>
<p>NOTE: Instead of copying the response to a JSON file, use REST linked service to access this open-source API to directly read the data. However the data preview would be slightly different from what you see above.</p>
<p><img src=""https://i.imgur.com/QTQXS99.png"" alt=""enter image description here"" /></p>
"
"75758552","I Need to Add few columns Based on Condition in pyspark","<p><a href=""https://i.stack.imgur.com/L4YlH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L4YlH.png"" alt=""enter image description here"" /></a></p>
<p>One File has Status_MP_c This column need to Split in same file and need to make sent,open,click columns if it is true will you provide logic in pyspark</p>
","<azure><azure-databricks><azure-data-factory><azure-data-lake-gen2>","2023-03-16 15:49:50","58","0","1","75767042","<blockquote>
<p>One File has Status_MP_c This column need to Split in same file and need to make sent,open,click columns if it is true will you provide logic in pyspark</p>
</blockquote>
<p>You can use pivot here.</p>
<p>I took the same data as dataframe.</p>
<p><img src=""https://i.imgur.com/lSBkWVT.png"" alt=""enter image description here"" /></p>
<p>As you want to make columns of <code>sent</code>, <code>Open</code> like that, use pivot on that column like below.</p>
<pre><code>from pyspark.sql.functions import *
res_df=df.groupBy(&quot;MP&quot;).pivot(&quot;Status&quot;).agg(first(&quot;res&quot;))
display(res_df)
</code></pre>
<p><img src=""https://i.imgur.com/rkxiWG7.png"" alt=""enter image description here"" /></p>
<p>Whatever the values in the 3rd column of your data whether it is True or False, those will be converted like above.</p>
<p>If you don't want a column with False you can drop that column if the value in the column is False (Or) you can delete the rows which have False in that column before Pivot as well.</p>
"
"75757175","dynamic content in filepath","<p>I am currently trying to ingest from a SFTP server and within the SFTP server the directory updates on it's own and I am needing to obtain the lates updated folder. The folder name is the date in format YYYY_MM_DD - 2 days so say today is 16/03/2023 I need the dynamic content to be 2023-03-14 but I have been unable to achieve this using inbuild functions such as utcnow() I am trying to get to the directory of: dailies/domain_names_dropped_whois/2023_03_14/</p>
<p>Thank you, hope this is enough info :)</p>
<p><a href=""https://i.stack.imgur.com/SKEMc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SKEMc.png"" alt=""enter image description here"" /></a></p>
","<variables><dynamic><azure-data-factory>","2023-03-16 13:47:29","41","0","1","75758903","<p>You can use the below dynamic content in your folder name to achieve your requirement.</p>
<p><code>@formatDateTime(adddays(utcNow(),-2),'yyyy_MM_dd')</code></p>
<p>Give this expression in your dataset folder name like below (Here for sample I have taken ADLS dataset and folder structure will be <code>data/2023_03_14/</code>).</p>
<p><img src=""https://i.imgur.com/cH8VQg9.png"" alt=""enter image description here"" /></p>
<p>For sample output I have given it to a string set variable activity and you can see that I got <strong>present date-2</strong> as output.</p>
<p><img src=""https://i.imgur.com/EzRkLaD.png"" alt=""enter image description here"" /></p>
"
"75755561","Delete sink records when records are deleted in source with ADF pipeline copy data activity","<p>In ADF I've created a pipeline which contains multiple copy data activities. All of those activities have SQL as a source and SQL as a sink.</p>
<p>I'm making use of the upsert function as much as I can to perform an insert or update action. This function doesn't support delete actions, so that's why I've found out that I'm having a mismatch of records in some of the the source and sink tables.</p>
<p>So in my case I've the following data issue, on the left the source table and on the right the sink table (destination). Where I'm expecting to have in the sink table the same data as in the source data table:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>CustomerId</th>
<th>CustomerId</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td></td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<p>What is the best way to solve this issue?</p>
<p>I could truncate the sink table with every pipeline run and create another destination table. So that when the new data in the sync tables has arrived I'm executing a stored procedure which merges the data into a destination table. This way I'm sure that the user of the sync tables (which is mostly a business intelligence tool) has data all the time. But this feels a bit old school... I've the feeling that there should be another (better) solution to solve this.</p>
<p>Am I supposed to solve those kind of issues in a data flow? I've quite a lot of tables where in the source application it's allowed to remove records. This would mean that for every table I've to create a data flow, this can be quite data/time consuming.</p>
","<azure><stored-procedures><azure-data-factory><azure-synapse>","2023-03-16 11:21:21","118","0","1","75801545","<ul>
<li>Using the truncate table on sink and creating a new table would achieve the job for you.</li>
</ul>
<p><img src=""https://i.imgur.com/ktwJPkj.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Dataflow's alter row transformation only allows you to only alter the data from transformed source data. Since sink has to be checked, dataflows would complicate the requirement.</p>
</li>
<li><p>If majority of the records are to be deleted from sink to match the source data, then truncating might be the better option.</p>
</li>
<li><p>As an alternative in case there are very few records that need to be deleted, you can stage the source table into the destination database.</p>
</li>
<li><p>Using this temporary table, delete records which are not present in source table from sink table using a query similar to below:</p>
</li>
</ul>
<pre><code>delete from t2 where ID not in (select ID from t1)
</code></pre>
<ul>
<li>To copy the source table from source database to destination database, you can refer to this <a href=""https://stackoverflow.com/questions/75511705/dynamically-update-filed-value-in-adf-copy-activity/75516941#75516941"">SO answer</a>.</li>
</ul>
"
"75753936","How can I use an already configured integration runtime (IR) with an Azure data factory that is recreated?","<p>One of the data factories that we are running got deleted. The VM running the IR (integration runtime) is still working - and as soon as the data factory was recreated it connected to this:</p>
<p><a href=""https://i.stack.imgur.com/lGzQC.png"" rel=""nofollow noreferrer"">Self hosted IR connected</a></p>
<p>But what are the steps I need to take in order to get this working from the Azure side?</p>
<p><a href=""https://i.stack.imgur.com/D8OVw.png"" rel=""nofollow noreferrer"">Azure portal view</a></p>
<p>Regards,
Andy</p>
<ul>
<li>recreated the data factory</li>
</ul>
","<azure><azure-data-factory>","2023-03-16 08:59:43","23","0","1","75755778","<p>answering my own question here. I have done this using the following solution:</p>
<ol>
<li>Log into the VM</li>
<li>Remove the existing IR</li>
<li>Launch the Azure Portal from the VM</li>
<li>Select the relevant ADF</li>
<li>Delete any (not linked/shared) IRs that this ADF has defined</li>
<li>Create a new IR, and use the express setup on this computer</li>
<li>Wait for it to install and configure.</li>
</ol>
<p>Voila!</p>
<p>Regards,
Andy</p>
"
"75752312","Can we push Azure blob data to Tableau hyper file in Azure data factory","<p>I have to push my blob csvs,parquets to Tableau .Generally Tableau hyperfiles we would be creating in Tableau.</p>
<p>I dont find any connector for Tableau.</p>
<p>Is there any other way to do it via python code or something ?</p>
","<azure><azure-data-factory>","2023-03-16 04:58:05","43","0","1","75754756","<p>This is not supported, but you can take these files directly from your Data Lake gen2 or other sources supported by Tableaus own ODBC connectors. From my Tableau I can see these options:</p>
<p><a href=""https://i.stack.imgur.com/x8fC3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x8fC3.png"" alt=""enter image description here"" /></a></p>
<p>Regarding the hyper data format, I have found a few sources, maybe you can run some python code inside an azure function to convert them to hyper files. Links mention Hyper API</p>
<p><a href=""https://medium.com/@elliottstam/hey-kaustubh-yes-you-can-convert-a-csv-file-into-a-hyper-extract-e2da11a41fc7"" rel=""nofollow noreferrer"">https://medium.com/@elliottstam/hey-kaustubh-yes-you-can-convert-a-csv-file-into-a-hyper-extract-e2da11a41fc7</a></p>
<p><a href=""https://gist.github.com/divinorum-webb/3bdafdeb5348e143676d3b4f26545b02"" rel=""nofollow noreferrer"">https://gist.github.com/divinorum-webb/3bdafdeb5348e143676d3b4f26545b02</a></p>
<p>Converting to .hyper files inside Azure should not be a problem. But I do not know how to transfer a .hyper file directly from Azure into Tableau, you might have to use at least one intermediary such as Data Lake.</p>
"
"75750555","Azure Data Factory Lookup Previously Working Now Failing","<p>I have a Lookup Shape pointed to a snowflake data set that is defined from a snowflake linked service.</p>
<p>Both the Linked Service and Dataset pass their connection tests.</p>
<p>However when invoking a lookup shape using this dataset I am now getting the following error:</p>
<p><strong>No active warehouse selected in the current session. Select an active warehouse with the 'use warehouse' command.</strong></p>
<p>On adding a <em>use warehouse whname</em> command before the SQL query I get an error. So merely adding this statement is probably not the resolution to this issue.</p>
<p>This was a previously working lookup shape, there have been backend configuration changes.</p>
<p>What is the most likely cause of this syntactical change?</p>
","<snowflake-cloud-data-platform><azure-data-factory>","2023-03-15 22:14:27","45","0","1","75756900","<p>This could be because you have not granted access to the Warehouse to the role:</p>
<pre><code>GRANT USAGE ON WAREHOUSE my_warehouse TO ROLE my_role;
</code></pre>
<p>Please try if this helps.</p>
"
"75749908","How to get the first day of the month in Azure data flow","<p>I have &quot;Date&quot; column with yyyy-MM-dd format .I want to fetch the 1st day i.e,
if date is 2023-05-28 ----&gt; 2023-05-01
I have tried method below
and apply it in</p>
<p>iif(toString(Date)&gt;=substring(toString(currentDate()),1,8)+'01',substring(toString(Date),1,8)+'01',toString(null()))</p>
<p>Its working but not for all dates .I dont know wh.Is there any alternative ?</p>
","<azure><azure-data-factory>","2023-03-15 20:44:30","90","0","1","75752917","<ul>
<li>According to the condition written, you would get null values in every case where the date value belongs to the previous months. The required result would be generated only for dates that belong to current or future dates.</li>
<li>Look at the following example. I have the following data in my date column.</li>
</ul>
<p><img src=""https://i.imgur.com/L8hxY2d.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I use the expression similar to yours, then it gives the result as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/jzn879J.png"" alt=""enter image description here"" /></p>
<ul>
<li>If you want to convert any date to get <code>yyyy-MM-01</code>, then you can directly use the dynamic content <code>substring(toString(date),1,8)+'01'</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/2faFJFD.png"" alt=""enter image description here"" /></p>
"
"75748909","I have created a DataFow in Azure Data Factory. In the sink , I need to add a time stamp to my filename","<p>In the sink  filename, I need to add filename</p>
<p><strong>SampleFileName_dev_8Mar2023.tab</strong></p>
<p>In the Dataflow - Sink - Settings - Filename , I need to add the Dataflow expression builder.</p>
<p>What expression do I need to use to achieve above filename?</p>
<p>I tried to add the filename as below.
<a href=""https://i.stack.imgur.com/BGVsk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BGVsk.png"" alt=""enter image description here"" /></a></p>
<p>Under expressions tried this expression.
concat('SampleFileName ',toString(currentUTC('dd-MM-yyy')),'.tab').</p>
<p>But its not working</p>
","<azure-data-factory>","2023-03-15 18:46:40","54","1","1","75752672","<ul>
<li>Select <strong>Output to single file</strong> in Sink settings' File Name option.</li>
</ul>
<p><img src=""https://i.imgur.com/33VJ3mD.png"" alt=""enter image description here"" /></p>
<ul>
<li>In the file Name, Click <strong>Open expression builder</strong>.</li>
</ul>
<p><img src=""https://i.imgur.com/hofohK6.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Enter the expression as  <code>concat('SampleFileName_dev_',toString(dayOfMonth(currentUTC())),'-',toString(month(currentUTC())),'-',toString(year(currentUTC())),'.tab')</code></p>
</li>
<li><p>In optimize, Select <strong>Single partition</strong> as partition type.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/zXoA5N8.png"" alt=""enter image description here"" /></p>
<ul>
<li>When the pipeline containing this  dataflow is executed, the file is created as expected.</li>
</ul>
<p><img src=""https://i.imgur.com/eAoRJQ2.png"" alt=""enter image description here"" /></p>
"
"75748269","Expression to change format of a date in azure data factory (SQL Server source)?","<p>I currently use the following SSIS expression to format a date in a certain way needed for processing. A common one is using the T-SQL function <code>GETDATE</code> like this:</p>
<pre><code>((DT_STR,4,1252)DATEPART(&quot;yyyy&quot;, GETDATE())) + (RIGHT(&quot;00&quot; + (DT_STR,2,1252)DATEPART(&quot;mm&quot;, GETDATE()), 2))
</code></pre>
<p>That would turn today's date into an int like this: 202303</p>
<p>However, in azure data factory, I am trying to re-create this derived column data flow task, but this expression isn't supported. I'm not too familiar with adf's expression language. Anyone able to help me figure out how this <em>should</em> be done in adf? TY!</p>
<p>I googled, looked at the expression white papers, but can't see how this would be done in adf's expression language.</p>
","<sql-server><azure><ssis><azure-data-factory>","2023-03-15 17:37:20","60","0","1","75752934","<blockquote>
<p>That would turn today's date into an int like this: 202303</p>
</blockquote>
<p>You can use the below expression in the derived column of dataflow to get the above result.</p>
<pre><code>toInteger(concat(toString(year(currentDate('UTC'))),split(toString(currentDate('UTC')),'-')[2]))
</code></pre>
<p><strong>My Sample:</strong></p>
<p><img src=""https://i.imgur.com/95EIONH.png"" alt=""enter image description here"" /></p>
<p><strong>(OR)</strong></p>
<p>You can try like this also.</p>
<pre><code>toInteger(concat(split(toString(currentDate('UTC')),'-')[1],split(toString(currentDate('UTC')),'-')[2]))
</code></pre>
<p><img src=""https://i.imgur.com/LzB4oJP.png"" alt=""enter image description here"" /></p>
"
"75743479","How to pass runid of a pipeline dynamically while using web activity with GET metho","<p>I want to get the running status of a particular pipeline in ADF from a different pipeline in same ADF instance.</p>
<p>I am using web activity with GET method, but issue with it is that every time I have to put the RunId of that pipeline manually in below URL. I am not able to pass it dynamically.
Like it takes pipeline RunId by itself.</p>
<p>GET <a href=""https://management.azure.com/subscriptions/%7BsubscriptionId%7D/resourceGroups/%7BresourceGroupName%7D/providers/Microsoft.DataFactory/factories/%7BfactoryName%7D/pipelineruns/%7BrunId%7D?api-version=2018-06-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelineruns/{runId}?api-version=2018-06-01</a></p>
<p>I tried using @pipeline().RunId</p>
","<azure><azure-data-factory>","2023-03-15 10:31:49","71","0","1","75758387","<blockquote>
<p>I have to put the RunId of that pipeline manually in below URL. I am not able to pass it dynamically.</p>
</blockquote>
<p><strong>There is no direct way to pass the parameter from one pipeline to another pipeline.</strong></p>
<p>To achieve this, we have to use execute pipeline to execute <strong>Pipeline B</strong> and the fetch the <strong>run id</strong> of that pipeline.</p>
<p><img src=""https://i.imgur.com/bapIQDw.png"" alt=""enter image description here"" /></p>
<p>To get pipeline run id of executed <strong>Pipeline B</strong> in <strong>Pipeline A</strong> <code>@activity('Execute Pipeline1').output.pipelinerunid</code></p>
<p><img src=""https://i.imgur.com/X1IIhDk.png"" alt=""enter image description here"" /></p>
<p><em><strong>Output:</strong></em></p>
<p><img src=""https://i.imgur.com/3v2ouZM.png"" alt=""enter image description here"" /></p>
"
"75741536","Copy data from SFTP to ADLS Gen 2 via ADF","<p>I have around 200GB of data in SFTP server in .gz format. I need to copy that into ADLS via ADF.
Can it be done.? what would be dataset file format that needs to be taken. The file format should also be the same .gz format in adls. There is no need for any transformation here.</p>
<p>Aslo, would the region of the data matter here?</p>
<p>Is there any other quicker way other than ADF to achieve this?</p>
","<azure><azure-data-factory><sftp><azure-databricks><azure-data-lake>","2023-03-15 07:17:47","132","-2","1","75743025","<p>You can use copy activity with <strong>SFTP Connector</strong> and use that connector for source dataset.</p>
<ul>
<li>Enter the compression type as <code>gzip(.gz)</code> in source dataset.</li>
</ul>
<p><img src=""https://i.imgur.com/YphrjQq.png"" alt=""enter image description here"" /></p>
<p><strong>Source Dataset configuration</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
&quot;name&quot;: &quot;DelimitedText66&quot;,
&quot;properties&quot;: {
&quot;linkedServiceName&quot;: {
&quot;referenceName&quot;: &quot;LS-SFTP&quot;,
&quot;type&quot;: &quot;LinkedServiceReference&quot;
},
&quot;annotations&quot;: [],
&quot;type&quot;: &quot;DelimitedText&quot;,
&quot;typeProperties&quot;: {
&quot;location&quot;: {
&quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
&quot;fileName&quot;: &quot;Book1.xlsx.gz&quot;,
&quot;fileSystem&quot;: &quot;con1&quot;
},
&quot;columnDelimiter&quot;: &quot;,&quot;,
&quot;compressionCodec&quot;: &quot;gzip&quot;,
&quot;escapeChar&quot;: &quot;\\&quot;,
&quot;quoteChar&quot;: &quot;\&quot;&quot;
},
&quot;schema&quot;: [
{
&quot;type&quot;: &quot;String&quot;
},
{
&quot;type&quot;: &quot;String&quot;
}
]
}
}
</code></pre>
<ul>
<li>Similarly, create a linked service for ADLS and create a sink dataset.</li>
<li>Use these datasets in copy activity and execute the pipeline to copy from SFTP to ADLS.</li>
</ul>
<blockquote>
<p>would the region of the data matter here?</p>
</blockquote>
<p>It depends on the location of your SFTP server and the ADLS account. If the SFTP server and the ADLS account are in the same region, the data transfer may be faster.</p>
<p>Reference:</p>
<ol>
<li><a href=""https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/data-factory/connector-sftp.md"" rel=""nofollow noreferrer"">azure-docs/connector-sftp.md at main · MicrosoftDocs/azure-docs (github.com)</a></li>
<li><a href=""https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/data-factory/connector-azure-data-lake-storage.md"" rel=""nofollow noreferrer"">azure-docs/connector-azure-data-lake-storage.md at main · MicrosoftDocs/azure-docs (github.com)</a></li>
</ol>
"
"75740322","How to dynamically handle a file with missing column names in ADF pipeline?","<p>I am copying data from ADLS csv file to Azure SQL DB.
File contains 9 columns and mapping is done according to that, but sometimes the source file is only having 6 or 7 columns. In that case the pipeline is failing with error</p>
<blockquote>
<p>The name of column index 'x' is empty. Make sure column name is properly specified in the header row</p>
</blockquote>
<p>Even some columns are missing I want to copy other columns data which is available and ignore the missing columns.
How can I dynamically handle this case and do mapping in case some columns are missing? As the files from source will have different set of columns each time.</p>
","<azure><ssis><azure-data-factory>","2023-03-15 03:40:04","136","0","2","75740632","<p>You can remove the mapping from the copy activity so it will try to map automatically.</p>
<p><strong>Note:</strong> the column order in csv file should match the column order in table</p>
"
"75740322","How to dynamically handle a file with missing column names in ADF pipeline?","<p>I am copying data from ADLS csv file to Azure SQL DB.
File contains 9 columns and mapping is done according to that, but sometimes the source file is only having 6 or 7 columns. In that case the pipeline is failing with error</p>
<blockquote>
<p>The name of column index 'x' is empty. Make sure column name is properly specified in the header row</p>
</blockquote>
<p>Even some columns are missing I want to copy other columns data which is available and ignore the missing columns.
How can I dynamically handle this case and do mapping in case some columns are missing? As the files from source will have different set of columns each time.</p>
","<azure><ssis><azure-data-factory>","2023-03-15 03:40:04","136","0","2","75743017","<p>I created pipeline and performed get metadata activity with  <code>Column count</code> field to get the number of columns of my csv file which is in ADLS account by selecting the ADLS dataset. After successful execution of metadata activity connected with lookup activity. selected sql database dataset for lookup and executed below code to find the number of columns of my sql table.</p>
<pre><code>SELECT  COUNT(COLUMN_NAME) as c
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_CATALOG =  '&lt;database&gt;'  AND TABLE_SCHEMA =  '&lt;schema&gt;'
AND TABLE_NAME =  '&lt;table&gt;' 
</code></pre>
<p>After successful execution of look up connected to if condition and given below expression to compare the columns of meta data activity and lookup activity</p>
<pre><code>@equals(activity('Get Metadata1').output.columnCount,activity('Lookup1').output.firstRow.c) 
</code></pre>
<p>If it is true I implemented copy activity to copy data from adls to sql by following below procedure</p>
<p>Selected delimited text dataset by selecting csv file from adls linked service as source enable first row as header in dataset</p>
<p><img src=""https://i.imgur.com/HmDq4N1.png"" alt=""enter image description here"" /></p>
<p>and select sql database dataset as sink and imported the schema in mapping of copy activity.
If it is false I implemented dynamic mapping following below procedure:
I created table in sql database with below columns</p>
<pre><code>create  table table_mappings(

    sourceFile nvarchar(30),
    sinkTableSchema nvarchar(30),
    sinkTableName nvarchar(30),
    jsonMapping nvarchar(max)
)
</code></pre>
<p>Inserted below values</p>
<pre><code>insert  into table_mappings values('&lt;sourcefilename&gt;','&lt;schema&gt;','&lt;tablename&gt;','{&quot;type&quot;: &quot;TabularTranslator&quot;,

&quot;mappings&quot;: [
             {
                &quot;source&quot;: {
                    &quot;name&quot;: &quot;EMPLOYEE_ID&quot;,
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;physicalType&quot;: &quot;String&quot;
                 },
                &quot;sink&quot;: {
                    &quot;name&quot;: &quot;EMPLOYEE_ID&quot;,
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;physicalType&quot;: &quot;String&quot;
                }
            },
            {
                &quot;source&quot;: {
                    &quot;name&quot;: &quot;FIRST_NAME&quot;,
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;physicalType&quot;: &quot;String&quot;
                 },
                 &quot;sink&quot;: {
                    &quot;name&quot;: &quot;FIRST_NAME&quot;,
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;physicalType&quot;: &quot;String&quot;
                }
           }
       ]}')    
</code></pre>
<p>In jsonMapping column inserted columns of source and columns of table where the values need to store with above format.</p>
<p><img src=""https://i.imgur.com/zMr61xN.png"" alt=""enter image description here"" /></p>
<p>I implemented lookup activity to by selecting sql database dataset and entered below query</p>
<pre><code>select * from table_mappings where sourceFile = '&lt;filename&gt;'
</code></pre>
<p><img src=""https://i.imgur.com/h4M3HNf.png"" alt=""enter image description here"" /></p>
<p>After successful execution of lookup created connected to copy activity. selected adls delimited text as source and sql database dataset as sink and created two parameters in sink named schema with <code>@activity('Lookup1').output.firstRow.sinkTableSchema</code>  value and table with <code>@activity('Lookup1').output.firstRow.sinkTableName</code>
and used those parameters for selecting the table in dataset as <code>@dataset().schema</code>.<code>@dataset().table</code> I added mapping dynamically using <code>@json(activity('Lookup1').output.firstRow.jsonMapping)</code>
executed the pipeline, it executed successfully.</p>
<p><img src=""https://i.imgur.com/nEljRRC.png"" alt=""enter image description here"" /></p>
<p>Here is my Json of pipeline:</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline3&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Get Metadata1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;AzureSqlSource&quot;,
                        &quot;sqlReaderQuery&quot;: &quot;SELECT COUNT(COLUMN_NAME) as c\nFROM INFORMATION_SCHEMA.COLUMNS \nWHERE TABLE_CATALOG = 'db' AND TABLE_SCHEMA = 'dbo'\nAND TABLE_NAME = 'employees'  &quot;,
                        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                        &quot;partitionOption&quot;: &quot;None&quot;
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Get Metadata1&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;fieldList&quot;: [
                        &quot;columnCount&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;If Condition1&quot;,
                &quot;type&quot;: &quot;IfCondition&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;expression&quot;: {
                        &quot;value&quot;: &quot;@equals(activity('Get Metadata1').output.columnCount,activity('Lookup1').output.firstRow.c)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;ifFalseActivities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy data1_copy1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Lookup1_copy1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                        &quot;recursive&quot;: true,
                                        &quot;enablePartitionDiscovery&quot;: false
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                    }
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;AzureSqlSink&quot;,
                                    &quot;writeBehavior&quot;: &quot;insert&quot;,
                                    &quot;sqlWriterUseTableLock&quot;: false
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;translator&quot;: {
                                    &quot;value&quot;: &quot;@json(activity('Lookup1').output.firstRow.jsonMapping)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;AzureSqlTable2&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;schema&quot;: {
                                            &quot;value&quot;: &quot;@activity('Lookup1').output.firstRow.sinkTableSchema&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;table&quot;: {
                                            &quot;value&quot;: &quot;@activity('Lookup1').output.firstRow.sinkTableName&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            ]
                        },
                        {
                            &quot;name&quot;: &quot;Lookup1_copy1&quot;,
                            &quot;type&quot;: &quot;Lookup&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                                    &quot;sqlReaderQuery&quot;: &quot;select * from table_mappings where sourceFile = 'employees.csv'&quot;,
                                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                                    &quot;partitionOption&quot;: &quot;None&quot;
                                },
                                &quot;dataset&quot;: {
                                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            }
                        }
                    ],
                    &quot;ifTrueActivities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy data1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                        &quot;recursive&quot;: true,
                                        &quot;enablePartitionDiscovery&quot;: false
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                    }
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;AzureSqlSink&quot;,
                                    &quot;writeBehavior&quot;: &quot;insert&quot;,
                                    &quot;sqlWriterUseTableLock&quot;: false,
                                    &quot;disableMetricsCollection&quot;: false
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;translator&quot;: {
                                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                                    &quot;mappings&quot;: [
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;EMPLOYEE_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;EMPLOYEE_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;FIRST_NAME&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;FIRST_NAME&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;LAST_NAME&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;LAST_NAME&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;EMAIL&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;EMAIL&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;PHONE_NUMBER&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;PHONE_NUMBER&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;HIRE_DATE&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;HIRE_DATE&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;JOB_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;JOB_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;SALARY&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;SALARY&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;COMMISSION_PCT&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;COMMISSION_PCT&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;MANAGER_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;MANAGER_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        },
                                        {
                                            &quot;source&quot;: {
                                                &quot;name&quot;: &quot;DEPARTMENT_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            },
                                            &quot;sink&quot;: {
                                                &quot;name&quot;: &quot;DEPARTMENT_ID&quot;,
                                                &quot;type&quot;: &quot;String&quot;,
                                                &quot;physicalType&quot;: &quot;String&quot;
                                            }
                                        }
                                    ],
                                    &quot;typeConversion&quot;: true,
                                    &quot;typeConversionSettings&quot;: {
                                        &quot;allowDataTruncation&quot;: true,
                                        &quot;treatBooleanAsNumber&quot;: false
                                    }
                                }
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;hello&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>If there are multiple files then perform above activities in foreach loop.</p>
"
"75739706","Rename columns based on first line of the dataset","<p>I am working on a ADF pipeline and I am on the last step of the DataFlow transformation but I am dealing with a dynamic column rename.</p>
<p>Basically I have a task that is creating a surrogated key in order to identify the rownumber with the column names that I need to use:</p>
<p><a href=""https://i.stack.imgur.com/wEA5R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wEA5R.png"" alt=""enter image description here"" /></a></p>
<p>The rownum = 1 (first row in the image) has the names that I need to replace, let's say Column_1 should be renamed to &quot;Org_Ind&quot;, Column_3 to &quot;Indicador&quot; and so on, but I have used a Derived column and I do not know how to create the expression, for now I have this:</p>
<p><a href=""https://i.stack.imgur.com/I0y5x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I0y5x.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-03-15 01:11:21","52","0","1","75743755","<p>If your source dataset is File from Blob storage or ADLS you have the option called first row as header. You can select it and it will convert it as header.</p>
<p><img src=""https://i.imgur.com/vubuLHz.png"" alt=""enter image description here"" /></p>
<p><strong>In Other conditions</strong></p>
<ul>
<li>we need to create dataflow parameter array of first row values
<code>array('Org_lnd','Indicador','1/1/2022','2022-02 01','3/1/2022','202244-01','1')</code>
<img src=""https://i.imgur.com/xW0oztu.png"" alt=""enter image description here"" /></li>
<li>After this take Filter transformation after Surrogate key and filter the rows ehere key is not equal to 1 <code>Surrogate_key!=1</code>
<img src=""https://i.imgur.com/QDYW8ie.png"" alt=""enter image description here"" /></li>
<li>Then in derived column mapp the accordingly with parameter array index
<img src=""https://i.imgur.com/EHJQyC9.png"" alt=""enter image description here"" /></li>
</ul>
<p><em><strong>Output</strong></em></p>
<p><img src=""https://i.imgur.com/F0314VP.png"" alt=""enter image description here"" /></p>
"
"75739657","How to unzip a blob programmatically using Azure Data Factory","<p>I am following the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">quickstart tutorial</a> on using the python azure sdk to copy a blob from one container to the next. This works in transferring the blob, but I'd like the blob in the receiving/sink container to be unzipped (basically what <a href=""https://www.youtube.com/watch?v=TEtpvdnULZ8"" rel=""nofollow noreferrer"">this tutorial on ADF is performing</a>, except I want to use the SDK, not the UI).</p>
<p>The block of code from the tutorial that performs the copy activity is as follows</p>
<pre class=""lang-py prettyprint-override""><code># here ds_name_* variables refers to Azure datasets for source and sink, respectively
    act_name = 'copyBlobtoBlob'
    blob_source = BlobSource()
    blob_sink = BlobSink()
    dsin_ref = DatasetReference(reference_name=ds_name_in, type='DatasetReference')
    dsout_ref = DatasetReference(reference_name=ds_name_out, type='DatasetReference')
    copy_activity = CopyActivity(name=act_name, inputs=[dsin_ref], outputs=[dsout_ref], source=blob_source, sink=blob_sink)
        # make pipeline w/ copy activity
    p_name = 'copyPipeline'
    params_for_pipeline = {}
    p_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline)
    p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)

    #create pipeline run
    run_response = adf_client.pipelines.create_run(rg_name, df_name, p_name, parameters={})
</code></pre>
<p>The SDK documentation for the <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.copyactivity?view=azure-python"" rel=""nofollow noreferrer""><code>CopyActivity</code> class</a> mentions an <code>additional_properties</code> parameter that may be relevant, but does not exemplify possible values.  There is also a <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.zipdeflatereadsettings?view=azure-python"" rel=""nofollow noreferrer""><code>ZipDeflateReadSettings</code> class that may be relevant</a>, but I cannot find a use case for it anywhere online or in the docs.</p>
<p>Edit:</p>
<p>The part of my script, pasted below, where I create a dataset may also be salient. Again the <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.datasetresource?view=azure-python"" rel=""nofollow noreferrer"">sdk doc for the <code>DatasetResource</code> class doesn't indicate any compression/decompression parameters</a>.</p>
<pre class=""lang-py prettyprint-override""><code>    ds_name_out = 'ds_out'
    ds_ls_out = LinkedServiceReference(reference_name=ls_name_out, type='LinkedServiceReference')
    blob_path_out = 'container_name/folder_name' # container name
    ds_azure_blob_out = DatasetResource(properties=AzureBlobDataset(
        linked_service_name=ds_ls_out, folder_path=blob_path_out))
    ds = adf_client.datasets.create_or_update(
        rg_name, df_name, ds_name_out, ds_azure_blob_out)
</code></pre>
","<azure><azure-blob-storage><azure-data-factory><azure-sdk><azure-sdk-python>","2023-03-15 01:00:00","58","0","1","75749104","<p>The object to parametrize with compression information in this case was <code>AzureBlobDataset</code>.  By passing a <code>compression</code> argument I was able to unzip the zip file in the target container:</p>
<pre class=""lang-py prettyprint-override""><code>ds_azure_blob_in = DatasetResource(properties=AzureBlobDataset(
        linked_service_name=ds_ls_in, folder_path=blob_path, file_name=blob_filename, compression=DatasetCompression(type='ZipDeflate'))) 
</code></pre>
<p>I think confusion arose from the abuse of abstraction in the Azure SDK here. I may be wrong, but something tells me that nesting three class instances in the above solution is not a pretty design pattern for a seemingly trivial task.</p>
"
"75737634","Azure Data Factory Remove Empty Child Directory","<p>In the following file structure</p>
<p>container/Directory/Sub-Directory</p>
<p>I wish to delete Sub-Directory --because at this point in my process it is empty.</p>
<p>Note: I am using the term 'Directory' as opposed to folder as this structure was added using 'Add Directory' -- I'm not sure if this is just a semantic difference directory/folder or is significant.</p>
<p>On invoking a delete shape using a dataset where the</p>
<p>Container, Directory and Sub-Directory are passed as parameters.</p>
<p>Within the delete shape setting</p>
<p>I am substituting within the boxes as follows.</p>
<p>|$Container/@concat( $Directory,'/', $Sub-Directory ) /     -- Filename is left blank.</p>
<p>On invoking the shape the following error occurs.</p>
<p>Failed to execute delete activity with data source
'AzureBlobStorage' and error
'The required Blob is missing.</p>
<p>Folder path: Container/Directory/Sub-Directory/.'.
For details, please reference log file here:</p>
<p>It appears to be looking for a file to delete?</p>
<p>Any suggestions?</p>
","<azure-blob-storage><azure-data-factory>","2023-03-14 19:35:42","66","0","1","75743498","<p>We cannot create an empty directory in Azure Blob Storage account. It's possible only in ADLS GEN2. You mentioned that the requirement is to delete the already existing empty folder in the storage account. So, your storage account must be ADLS GEN2.</p>
<p>In ADF, we can create two linked service for an ADLS storage account i.e, <strong>Azure Blob Storage</strong> and <strong>Azure Data Lake Storage</strong>.</p>
<p>I tried with <strong>Azure Blob Storage</strong> linked service and got same error.</p>
<p><img src=""https://i.imgur.com/nRTDX6t.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>It appears to be looking for a file to delete?</p>
</blockquote>
<p>No. <strong>AFAIK</strong>, Using Blob storage linked service might be the reason for this error as you can see, for the empty folder(directory) <code>subfolder3</code> it gave me same error when I browse through the files.</p>
<p><img src=""https://i.imgur.com/Gdp0ioK.png"" alt=""enter image description here"" /></p>
<p>When I use the same Delete activity with <strong>Azure Data Lake Storage</strong> linked service, I am able to delete the empty folder.</p>
<p><img src=""https://i.imgur.com/z2U7gQR.png"" alt=""enter image description here"" /></p>
<p>So, you can try the Azure Data Lake Storage as a workaround for it.</p>
"
"75734008","Are there differences in how bindings are handled when using the bulk insert feature in ADF and SQL?","<p>When using a copy activity and SQL Server connector in ADF pipeline, there are observed differences between how ADF handles a &quot;bulk insert&quot; operation and how SQL Server handles &quot;bulk insert&quot; when there are bindings on the table being copied into.</p>
<p>For example:</p>
<ul>
<li>On-prem SQL Server bulk insert command automatically populates an empty string for not null field if null in source, ADF cannot</li>
<li>On-prem SQL Server bulk insert command automatically populates null in nullable datetime field even though source has string with spaces</li>
<li>On-prem SQL Server automatically enforces bindings, however Azure SQL does not.</li>
</ul>
<p>We tried the examples above in SQL Server using bulk insert and observed the expected behavior.</p>
<p>In ADF using bulk insert, the expected behavior was not seen</p>
","<azure-data-factory>","2023-03-14 13:47:06","37","0","1","75758034","<p>Please help clarify what 'bulk insert' you are using in SQL server. Bulk insert in ADF is <a href=""https://learn.microsoft.com/en-us/sql/tools/bcp-utility?view=sql-server-ver16"" rel=""nofollow noreferrer"">BCP</a> rather than an insert cluase. If the 'bulk insert' you mentioned is not BCP, then they shoud be by design differences.</p>
"
"75733994","Azure synpase get output data from lookup activity to data flow source","<p>I have a problem. I got the data from oracle on premise by query in LOOKUP ACTIVITY now I want to get this data into the DATA FLOW source so I can append this data into the my master CSV file in BLOb storage.</p>
<p>please help out this</p>
<p>Azure pipeline get output value data from lookup activity to data flow source</p>
","<azure><azure-devops><azure-functions><azure-data-factory><azure-synapse>","2023-03-14 13:46:07","79","0","2","75734772","<p>You can parameterize based on</p>
<p>@activity('LookupActivityName').output.value</p>
<p>Check this out:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/1136282/pass-lookup-activity-output-to-dataflow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/1136282/pass-lookup-activity-output-to-dataflow</a></p>
"
"75733994","Azure synpase get output data from lookup activity to data flow source","<p>I have a problem. I got the data from oracle on premise by query in LOOKUP ACTIVITY now I want to get this data into the DATA FLOW source so I can append this data into the my master CSV file in BLOb storage.</p>
<p>please help out this</p>
<p>Azure pipeline get output value data from lookup activity to data flow source</p>
","<azure><azure-devops><azure-functions><azure-data-factory><azure-synapse>","2023-03-14 13:46:07","79","0","2","75747142","<blockquote>
<p>Azure pipeline get output value data from lookup activity to data flow source.</p>
</blockquote>
<p>If you want to do it with only lookup, first get the column values array using a ForEach. then pass that array as an array parameter to dataflow as suggested by <strong>@Ziya Mert Karakas</strong>.</p>
<p>In Dataflow use a dummy source and add derived column and unfold the parameter array to get it as a column. You can go through this <a href=""https://stackoverflow.com/questions/75621518/azure-data-flow-source-from-output-in-pipeline/75625419#75625419"">SO answer</a> by <strong>@RithwikBojja</strong> to learn more about it.<br>
But this method only works if there is only a single column in your data. If there are multiple columns in it, unfolding multiple column arrays at a time might not be possible in dataflow.</p>
<p>As an alternative you can use the below method which involves a temporary file. <strong>First copy the Oracle data to a temporary Blob file using copy activity</strong>. Now use dataflow after it.</p>
<blockquote>
<p>DATA FLOW source so I can append this data into the my master CSV file in BLOb storage</p>
</blockquote>
<p>But, dataflow will overwrite the sink blob csv file. If you want append to your master csv file, then use two sources first is your master csv file and second one is the temporary blob file. <strong>Do a union transformation on these two by <code>Name</code></strong> like below sample.</p>
<p><strong>My master csv file with some data:</strong></p>
<p><img src=""https://i.imgur.com/6vZc27m.png"" alt=""enter image description here"" /></p>
<p><strong>Temporary file with new data which we will get from copy activity:</strong></p>
<p><img src=""https://i.imgur.com/VvXGFA8.png"" alt=""enter image description here"" /></p>
<p><strong>Union transformation by Name:</strong></p>
<p><img src=""https://i.imgur.com/wbUiwg3.png"" alt=""enter image description here"" /></p>
<p>Then give the same master csv file dataset as sink. Check on output to single file and give the master csv file name here.</p>
<p><img src=""https://i.imgur.com/FQCb8Dm.png"" alt=""enter image description here"" /></p>
<p><strong>Sink result with appended data:</strong></p>
<p><img src=""https://i.imgur.com/4K8ewp3.png"" alt=""enter image description here"" /></p>
<p>After copy activity to temporary file, use the dataflow activity and execute the pipeline to get the data in the master csv file.</p>
"
"75733779","How to do not in ( ) concept in Azure data flow","<p>I have given an expression like</p>
<pre><code>not(in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null())],upper(CUSTOMER_PO_NUMBER))in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,upper(CUSTOMER_PO_NUMBER)))
</code></pre>
<p>in my filter Stream.</p>
<p>Here iam getting wrong no of rows filtered because nulls also been taken out.
My other idea was to do a &quot;Conditional Split&quot; ,in conditional split i mentioned</p>
<pre><code>in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null())],upper(CUSTOMER_PO_NUMBER))in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,upper(CUSTOMER_PO_NUMBER))
</code></pre>
<p>Now when i see no of rows in where the condition is false :Am getting the right no of rows using Conditional Split .</p>
<p>So I thought of including toString(null()) as :</p>
<pre><code>not(in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null())],upper(CUSTOMER_PO_NUMBER))in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null()),upper(CUSTOMER_PO_NUMBER)))
</code></pre>
<p>in filter .But that giving me 0 rows .</p>
<p>How to get the same output as conditional split False condition in filter ? not(in()) is not working in this scenario.</p>
","<azure><azure-data-factory>","2023-03-14 13:28:15","59","0","1","75741565","<p>I tried to replicate the scenario in my local.</p>
<p><strong>Input used</strong></p>
<p><img src=""https://i.imgur.com/Y2Sug8E.png"" alt=""enter image description here"" /></p>
<p><strong>Dataflow</strong></p>
<p><img src=""https://i.imgur.com/EvAZz5Z.png"" alt=""enter image description here"" /></p>
<p><strong>Expression for filter1</strong></p>
<p><img src=""https://i.imgur.com/IgioVCS.png"" alt=""enter image description here"" /></p>
<p><strong>Code used in filter expression</strong>   <code>in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;],upper(CUSTOMER_PO_NUMBER))||isNull(CUSTOMER_PO_NUMBER)</code></p>
<p><strong>Explanation</strong></p>
<p>Using <code>toString(null())</code> into <code>in()</code> was not checking for null values and returning only matching records except null/empty value. I have removed <code>toString(null())</code> from <code>in()</code> and used  <code>||</code> with <code>isNull()</code> and it gave below result including null records.</p>
<p><strong>Result/Data preview</strong></p>
<p><img src=""https://i.imgur.com/TAC3YEp.png"" alt=""enter image description here"" /></p>
<p>To get not matching records you can use <code>not()</code> as mentioned below.    <code>not(in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;],upper(CUSTOMER_PO_NUMBER))||isNull(CUSTOMER_PO_NUMBER))</code></p>
<p><strong>Result/Data preview</strong></p>
<p><img src=""https://i.imgur.com/bmQDPhv.png"" alt=""enter image description here"" /></p>
"
"75731989","Error in Azure Data Pipeline to get user details from Microsoft 365 need to opt in to Microsoft Data Connect","<p>I need to retrieve all Microsoft 365 users within our tenant and extract these info to Blob Storage using Azure Data Factory</p>
<p>Here's my linked service as source
<a href=""https://i.stack.imgur.com/k46Un.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k46Un.png"" alt=""enter image description here"" /></a></p>
<p>I encountered an error that I have to opt in to Microsoft Data Connect</p>
<p><a href=""https://i.stack.imgur.com/nTHjs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nTHjs.png"" alt=""enter image description here"" /></a></p>
<p>does it have cost even the data set I'm going to use is BasicDataSetv0.User_v1 because in the microsoft page it says it's free only for BasicDataSetv0.User_v0</p>
<p><a href=""https://i.stack.imgur.com/K5mSa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K5mSa.png"" alt=""enter image description here"" /></a></p>
<p>Also upon checking there's an approval stage after running the pipeline is there a way to auto approve it? Because I want to have this copied daily at a specific time.</p>
","<azure><azure-data-factory><microsoft365>","2023-03-14 10:32:40","40","0","1","75767788","<p>I used the rest api linked service to call graph api instead and it's free</p>
"
"75731706","How to pass a pipeline variable into a Dataset Parameter?","<p>I'm currently working on a pipeline which requires me to dynamically create a @FileName(FileName_YYYY_MM_DD.csv) variable which has the date appended to it.</p>
<p>The aim is to export data from an Azure SQL Database and create a CSV file with a dynamic @FileName in my data lake.</p>
<p><a href=""https://i.stack.imgur.com/IkFbN.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<ol>
<li>I have a lookup activity which triggers a SQL script which produces 1 line (FileName_YYYY_MM_DD.csv)</li>
<li>That value is returned as an Array so I store it in a variable @FileNames</li>
<li>I then have another Set Variable Activity which extracts the file name as a string and stores it into another variable called @FileName</li>
<li>I'm then trying to pass the pipeline variable @FileName to a Dataset Parameter in a CopyData Activity Sink which links to my Data Lake.</li>
</ol>
<p>However, when I try to run my pipeline I get an error which is not very descriptive:</p>
<pre class=""lang-json prettyprint-override""><code>{
&quot;code&quot;: &quot;BadRequest&quot;,
&quot;message&quot;: null,
&quot;target&quot;: &quot;pipeline//runid/*******&quot;,
&quot;details&quot;: null,
&quot;error&quot;: null
}
</code></pre>
<p>I've looked at some similar issues but everyone suggests that I have a white space somewhere. I've double-checked my strings and I don't have any  white spaces.</p>
<p>Could you please help me pass the variable to my dataset parameter?</p>
<p>However, when I try to run my pipeline I get an error which is not very descriptive:</p>
<pre class=""lang-json prettyprint-override""><code>{
&quot;code&quot;: &quot;BadRequest&quot;,
&quot;message&quot;: null,
&quot;target&quot;: &quot;pipeline//runid/*******&quot;,
&quot;details&quot;: null,
&quot;error&quot;: null
}
</code></pre>
<p>I've looked at some similar issues but everyone suggests that I have a white space somewhere. I've double-checked my strings and I don't have any  white spaces.</p>
","<azure><azure-sql-database><azure-data-factory><azure-data-lake>","2023-03-14 10:12:25","100","0","1","75853272","<blockquote>
<p>{
&quot;code&quot;: &quot;BadRequest&quot;,
&quot;message&quot;: null,
&quot;target&quot;: &quot;pipeline//runid/*******&quot;,
&quot;details&quot;: null,
&quot;error&quot;: null
}</p>
</blockquote>
<p>In ADF, this error occurs only when we gave wrong dynamic content or any special characters like (<code>!@#..</code>) in dynamic content.</p>
<p>You can see I have same error when I gave the below dynamic content.</p>
<p><code>@activity('Lookup1').output.value[0].name# </code></p>
<p><img src=""https://i.imgur.com/y8AUaC1.png"" alt=""enter image description here"" /></p>
<p>I have reproduced your scenario and able to get the requirement done and this is my pipeline JSON.</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline2&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;AzureSqlSource&quot;,
                        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                        &quot;partitionOption&quot;: &quot;None&quot;
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;filename&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@activity('Lookup1').output.value[0].name&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;AzureSqlSource&quot;,
                        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                        &quot;partitionOption&quot;: &quot;None&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;typeConversion&quot;: true,
                        &quot;typeConversionSettings&quot;: {
                            &quot;allowDataTruncation&quot;: true,
                            &quot;treatBooleanAsNumber&quot;: false
                        }
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;sinkfile&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;,
                        &quot;parameters&quot;: {
                            &quot;FileName&quot;: {
                                &quot;value&quot;: &quot;@variables('filename')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        ],
        &quot;variables&quot;: {
            &quot;filename&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><img src=""https://i.imgur.com/4bwXU1S.png"" alt=""enter image description here"" /></p>
"
"75727723","ADF Trigger from Storage Event not working","<p>I created a trigger in ADF that would look for Storage Events. I am able to see the blob/files in the trigger but when the files get generated, the ADF pipeline does not start.</p>
<p>I did link the trigger to a pipeline.</p>
<p>Is there a firewall setting or access of some sort that I am missing?</p>
<p>Azure/Microsoft docs dont specify anything in this regard.</p>
","<azure-data-factory><azure-storage><eventtrigger>","2023-03-13 23:12:34","85","0","1","75737684","<p>I was able to resolve the issue.</p>
<p>The setup for done right with the appropriate roles as mentioned in the documentation (<a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-output?tabs=in-process&amp;pivots=programming-language-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-output?tabs=in-process&amp;pivots=programming-language-python</a>)</p>
<p>The issue was with ADF. We have a new ADF setup that is connected to a ADO REPO. The change I was making was in a branch and were not being deployed to the LIVE ADF instance.</p>
<p>I disconnected the REPO from the ADF and made the triggers in the LIVE ADF and it worked.</p>
<p>There are 2 ways to trigger ADF from dbt jobs.</p>
<ol>
<li>dbt Webhook --&gt; Azure Function --&gt; Blob --&gt; ADF</li>
<li>dbt webhook --&gt; Azure Function --&gt; EventGrid --ADF</li>
</ol>
<p>The 2nd option seem to be a better alternative as you save on managing and cost for the blob storage. BLOB triggers are also sent to EventGrid. So using EventGrid directly seems better.</p>
<p>Thanks for your help.</p>
"
"75726498","How to deal with error 403 in Azure data factory with creating datasets?","<p>I have enountered several error but non of them appears to be the problem. I have already granted fully access and roles to my account but I keep having these errors.</p>
<p><a href=""https://i.stack.imgur.com/T7XXX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T7XXX.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-03-13 20:12:38","78","0","1","75729091","<p>I got same error when Networking is <strong>disabled</strong> in Storage account.</p>
<p><img src=""https://i.imgur.com/r9ZDrFR.png"" alt=""enter image description here"" /></p>
<p>To resolve this, go to <strong>Storage account -&gt; Networking -&gt; Enabled from all Networks</strong> like below and save it.</p>
<p><img src=""https://i.imgur.com/HN6FOW0.png"" alt=""enter image description here"" /></p>
<p>You can see, now I am able to access the files when creating dataset.</p>
<p><img src=""https://i.imgur.com/qwOQvUS.png"" alt=""enter image description here"" /></p>
"
"75725670","Is it possible to create self hosted integration runtime using VMSS?","<p>I need this kind of setup as it will save money as many window virtual machines are needed, such as one for Datagateway, SHIR for Datafactory, and SHIR for Purview, and which will only start up when there is a call, for example very similar to the azure devops agent on VMSS which used to only start instances under VMSS when there is job requirement.</p>
","<azure><azure-data-factory>","2023-03-13 18:30:55","58","1","1","75725728","<p>The answer for this is no. See here for example:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/1010423/can-we-install-shir-on-virtual-mahine-scale-set-is?orderby=oldest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/1010423/can-we-install-shir-on-virtual-mahine-scale-set-is?orderby=oldest</a></p>
<p>However, you can think about basing your SHIR in containers or even app service:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-run-self-hosted-integration-runtime-in-windows-container"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-run-self-hosted-integration-runtime-in-windows-container</a></p>
"
"75725204","Missing jvm.dll in self-hosted integration runtime Windows Container","<p>I'm spinning up a Windows Container image (provided my MS <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container"" rel=""nofollow noreferrer"">here</a>) containing the Self-Hosted Integration Runtime to be able to use ADF in a on-premise situation. It ran smoothly until I needed to use Parquet files.</p>
<p>When I pointed the output to a .parquet I got a Data Factory task failure pointing the absence of Java in the Integration Runtime container.</p>
<p><code>ErrorCode=JreNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Java Runtime Environment cannot be found on the Self-hosted Integration Runtime machine. It is required for parsing or writing to Parquet/ORC files. Make sure Java Runtime Environment has been installed on the Self-hosted Integration Runtime machine.,Source=Microsoft.DataTransfer.Common,''Type=System.DllNotFoundException,Message=Unable to load DLL 'jvm.dll': The specified module could not be found.</code></p>
<p>I took the path of modifying the <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container/blob/main/SHIR/setup.ps1"" rel=""nofollow noreferrer"">build.ps1</a> file to install and configure the dependencies during container image creation. These are the steps taken:</p>
<ul>
<li><p>Install Microsoft Visual C++ 2010 Service Pack 1 (<a href=""https://www.microsoft.com/en-us/download/details.aspx?id=26999"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Instal JDK provided from Microsoft OpenJDK 17.0.6 LTS - 64bits MSI (<a href=""https://learn.microsoft.com/pt-br/java/openjdk/download"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Manually set JAVA_HOME environment variable: <code>setx -m JAVA_HOME &quot;C:\Program Files\Microsoft\jdk-17.0.6.10-hotspot&quot;</code> (As for as I got SHIR will look in the registry for JRE location and in case it is not found it will look JAVA_HOME env var).</p>
</li>
</ul>
<p>Java seems to be working fine, since when I run <code>java -version</code> it returns me the following output.</p>
<pre><code>openjdk version &quot;17.0.6&quot; 2023-01-17 LTS
OpenJDK Runtime Environment Microsoft-7209853 (build 17.0.6+10-LTS)
OpenJDK 64-Bit Server VM Microsoft-7209853 (build 17.0.6+10-LTS, mixed mode, sharing)
</code></pre>
<p>Everything seems to be Ok but I keep getting the error I mentioned above. I tried to install JRE7, JRE8, configure registry keys, but nothing seems to work.</p>
","<java><windows><azure-data-factory><windows-container>","2023-03-13 17:41:30","618","1","5","75726229","<p>I have found some links which might help or answer your question.</p>
<p><a href=""https://learn.microsoft.com/en-us/troubleshoot/azure/general/error-run-copy-activity-azure"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/troubleshoot/azure/general/error-run-copy-activity-azure</a></p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/532391/azure-pipeline-issue"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/532391/azure-pipeline-issue</a></p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/582143/pipeline-fail-during-the-sink-of-a-copy-data-to-pa"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/582143/pipeline-fail-during-the-sink-of-a-copy-data-to-pa</a></p>
<p>Could this be something about registry?</p>
<p>or could this be ?</p>
<p>64-bit JRE for 64-bit ADF Integration Runtime should be installed in the folder: C:\Program Files\Java\</p>
<p>The folder is not C:\Program Files (x86)\Java\</p>
"
"75725204","Missing jvm.dll in self-hosted integration runtime Windows Container","<p>I'm spinning up a Windows Container image (provided my MS <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container"" rel=""nofollow noreferrer"">here</a>) containing the Self-Hosted Integration Runtime to be able to use ADF in a on-premise situation. It ran smoothly until I needed to use Parquet files.</p>
<p>When I pointed the output to a .parquet I got a Data Factory task failure pointing the absence of Java in the Integration Runtime container.</p>
<p><code>ErrorCode=JreNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Java Runtime Environment cannot be found on the Self-hosted Integration Runtime machine. It is required for parsing or writing to Parquet/ORC files. Make sure Java Runtime Environment has been installed on the Self-hosted Integration Runtime machine.,Source=Microsoft.DataTransfer.Common,''Type=System.DllNotFoundException,Message=Unable to load DLL 'jvm.dll': The specified module could not be found.</code></p>
<p>I took the path of modifying the <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container/blob/main/SHIR/setup.ps1"" rel=""nofollow noreferrer"">build.ps1</a> file to install and configure the dependencies during container image creation. These are the steps taken:</p>
<ul>
<li><p>Install Microsoft Visual C++ 2010 Service Pack 1 (<a href=""https://www.microsoft.com/en-us/download/details.aspx?id=26999"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Instal JDK provided from Microsoft OpenJDK 17.0.6 LTS - 64bits MSI (<a href=""https://learn.microsoft.com/pt-br/java/openjdk/download"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Manually set JAVA_HOME environment variable: <code>setx -m JAVA_HOME &quot;C:\Program Files\Microsoft\jdk-17.0.6.10-hotspot&quot;</code> (As for as I got SHIR will look in the registry for JRE location and in case it is not found it will look JAVA_HOME env var).</p>
</li>
</ul>
<p>Java seems to be working fine, since when I run <code>java -version</code> it returns me the following output.</p>
<pre><code>openjdk version &quot;17.0.6&quot; 2023-01-17 LTS
OpenJDK Runtime Environment Microsoft-7209853 (build 17.0.6+10-LTS)
OpenJDK 64-Bit Server VM Microsoft-7209853 (build 17.0.6+10-LTS, mixed mode, sharing)
</code></pre>
<p>Everything seems to be Ok but I keep getting the error I mentioned above. I tried to install JRE7, JRE8, configure registry keys, but nothing seems to work.</p>
","<java><windows><azure-data-factory><windows-container>","2023-03-13 17:41:30","618","1","5","75863063","<p>We are experiencing the exact same issue since our production environments have been upgraded from 5.24x to 5.26x. Looking at the release notes for the Integration Runtime upgrades found here:
<a href=""https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fdownload.microsoft.com%2Fdownload%2FE%2F4%2F7%2FE4771905-1079-445B-8BF9-8A1A075D8A10%2FRelease%2520Notes.doc&amp;wdOrigin=BROWSELINK"" rel=""nofollow noreferrer"">release notes</a></p>
<p>You can see that there are known issues regarding &quot;finding&quot; dll files. They dont specify which files have issues though.</p>
<p>We attempted upgrading to the newest version 5.28x, but that still gives us the same issues.</p>
<p>We are currently downgrading to 5.27.8441.3 <a href=""https://download.microsoft.com/download/e/4/7/e4771905-1079-445b-8bf9-8a1a075d8a10/integrationruntime_5.27.8441.3.msi"" rel=""nofollow noreferrer"">download link</a>. I will update my response if that solved the issue.</p>
<p><strong>UPDATE:</strong>
We have attempted the current version: 5.28.8473.1
a previous version: 5.27.8441.3
another previous version: 5.26.8431.1.</p>
<p>all not fixing the issue. I have raised a ticket with Support,
and am currently downgrading to our known working version: 5.24.8369.1.</p>
<p>I will update again if this version is actually still working.</p>
<p><strong>UPDATE2:</strong>
@FlodenH our release pipeline is still running (+- 10 more minutes) but someone told me it works for him again using this older version.</p>
<p>You can download any version by changing this url:</p>
<p><a href=""https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_VERSION.msi?update=1"" rel=""nofollow noreferrer"">https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_VERSION.msi?update=1</a></p>
<p>so for the working version it should be:
<a href=""https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_5.24.8369.1.msi?update=1"" rel=""nofollow noreferrer"">link</a></p>
<p><strong>UPDATE3:</strong>
I can confirm that version: 5.24.8369.1 works. Download link can be found above. A ticket was raised with Microsoft and they are investigating it on the newer version, but for now this version is the only one that works for parquet convertion.</p>
"
"75725204","Missing jvm.dll in self-hosted integration runtime Windows Container","<p>I'm spinning up a Windows Container image (provided my MS <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container"" rel=""nofollow noreferrer"">here</a>) containing the Self-Hosted Integration Runtime to be able to use ADF in a on-premise situation. It ran smoothly until I needed to use Parquet files.</p>
<p>When I pointed the output to a .parquet I got a Data Factory task failure pointing the absence of Java in the Integration Runtime container.</p>
<p><code>ErrorCode=JreNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Java Runtime Environment cannot be found on the Self-hosted Integration Runtime machine. It is required for parsing or writing to Parquet/ORC files. Make sure Java Runtime Environment has been installed on the Self-hosted Integration Runtime machine.,Source=Microsoft.DataTransfer.Common,''Type=System.DllNotFoundException,Message=Unable to load DLL 'jvm.dll': The specified module could not be found.</code></p>
<p>I took the path of modifying the <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container/blob/main/SHIR/setup.ps1"" rel=""nofollow noreferrer"">build.ps1</a> file to install and configure the dependencies during container image creation. These are the steps taken:</p>
<ul>
<li><p>Install Microsoft Visual C++ 2010 Service Pack 1 (<a href=""https://www.microsoft.com/en-us/download/details.aspx?id=26999"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Instal JDK provided from Microsoft OpenJDK 17.0.6 LTS - 64bits MSI (<a href=""https://learn.microsoft.com/pt-br/java/openjdk/download"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Manually set JAVA_HOME environment variable: <code>setx -m JAVA_HOME &quot;C:\Program Files\Microsoft\jdk-17.0.6.10-hotspot&quot;</code> (As for as I got SHIR will look in the registry for JRE location and in case it is not found it will look JAVA_HOME env var).</p>
</li>
</ul>
<p>Java seems to be working fine, since when I run <code>java -version</code> it returns me the following output.</p>
<pre><code>openjdk version &quot;17.0.6&quot; 2023-01-17 LTS
OpenJDK Runtime Environment Microsoft-7209853 (build 17.0.6+10-LTS)
OpenJDK 64-Bit Server VM Microsoft-7209853 (build 17.0.6+10-LTS, mixed mode, sharing)
</code></pre>
<p>Everything seems to be Ok but I keep getting the error I mentioned above. I tried to install JRE7, JRE8, configure registry keys, but nothing seems to work.</p>
","<java><windows><azure-data-factory><windows-container>","2023-03-13 17:41:30","618","1","5","75876778","<p>We had the same issue, and in our case, we managed to work around it with a registry change. Basically, we added REG_SZ values for &quot;CurrentVersion&quot; under <em>HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit</em> and <em>HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment</em>.</p>
<p>At the end the JavaSoft.reg file we applied looked like this:</p>
<pre><code>Windows Registry Editor Version 5.00

[HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft]

[HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit]
&quot;CurrentVersion&quot;=&quot;1.8&quot;

[HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit\1.8]
&quot;JavaHome&quot;=&quot;C:\\Program Files\\Temurin\\&quot;

[HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit\8.0.362.9]
&quot;JavaHome&quot;=&quot;C:\\Program Files\\Temurin\\&quot;

[HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment]
&quot;CurrentVersion&quot;=&quot;1.8&quot;

[HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment\1.8]
&quot;JavaHome&quot;=&quot;C:\\Program Files\\Temurin\\jre&quot;

[HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment\8.0.352.8]
&quot;JavaHome&quot;=&quot;C:\\Program Files\\Temurin\\jre&quot;
</code></pre>
<p>Before merging this .reg file, we did not have the &quot;CurrentVersion&quot; values under the two keys.</p>
<p>That said, please note that the current documentation from Microsoft is actually requesting Temurin 11. Verbatim from the Prerequisites section at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory</a>:</p>
<blockquote>
<p>Java Runtime (JRE) version 11 from a JRE provider such as Eclipse Temurin. Ensure that the JAVA_HOME environment variable is set to the JDK folder (and not just the JRE folder) you may also need to add the bin folder to your system's PATH environment variable.</p>
</blockquote>
<p>Please also note while troubleshooting via the Sysinternals Procmon64.exe tool, we noticed the Integration Runtime process was unable to find 2 files, <em>jvm.dll</em>, which was addressed from the registry change described above, and <em>msvcr100.dll</em>. We tried to work around or fix also the finding of the latter, but we gave up as we noticed that didn't had any impact in our scenarios.
Just manually copying <em>msvcr100.dll</em> from <em>%SystemRoot%/System32</em> into <em>C:\Program Files\Temurin\bin</em> would work around the issue, if that impacts you, but I wouldn't recommend that solution (if you use Temurin 8, I would suggest evaluating to upgrade to Temurin 11).</p>
"
"75725204","Missing jvm.dll in self-hosted integration runtime Windows Container","<p>I'm spinning up a Windows Container image (provided my MS <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container"" rel=""nofollow noreferrer"">here</a>) containing the Self-Hosted Integration Runtime to be able to use ADF in a on-premise situation. It ran smoothly until I needed to use Parquet files.</p>
<p>When I pointed the output to a .parquet I got a Data Factory task failure pointing the absence of Java in the Integration Runtime container.</p>
<p><code>ErrorCode=JreNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Java Runtime Environment cannot be found on the Self-hosted Integration Runtime machine. It is required for parsing or writing to Parquet/ORC files. Make sure Java Runtime Environment has been installed on the Self-hosted Integration Runtime machine.,Source=Microsoft.DataTransfer.Common,''Type=System.DllNotFoundException,Message=Unable to load DLL 'jvm.dll': The specified module could not be found.</code></p>
<p>I took the path of modifying the <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container/blob/main/SHIR/setup.ps1"" rel=""nofollow noreferrer"">build.ps1</a> file to install and configure the dependencies during container image creation. These are the steps taken:</p>
<ul>
<li><p>Install Microsoft Visual C++ 2010 Service Pack 1 (<a href=""https://www.microsoft.com/en-us/download/details.aspx?id=26999"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Instal JDK provided from Microsoft OpenJDK 17.0.6 LTS - 64bits MSI (<a href=""https://learn.microsoft.com/pt-br/java/openjdk/download"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Manually set JAVA_HOME environment variable: <code>setx -m JAVA_HOME &quot;C:\Program Files\Microsoft\jdk-17.0.6.10-hotspot&quot;</code> (As for as I got SHIR will look in the registry for JRE location and in case it is not found it will look JAVA_HOME env var).</p>
</li>
</ul>
<p>Java seems to be working fine, since when I run <code>java -version</code> it returns me the following output.</p>
<pre><code>openjdk version &quot;17.0.6&quot; 2023-01-17 LTS
OpenJDK Runtime Environment Microsoft-7209853 (build 17.0.6+10-LTS)
OpenJDK 64-Bit Server VM Microsoft-7209853 (build 17.0.6+10-LTS, mixed mode, sharing)
</code></pre>
<p>Everything seems to be Ok but I keep getting the error I mentioned above. I tried to install JRE7, JRE8, configure registry keys, but nothing seems to work.</p>
","<java><windows><azure-data-factory><windows-container>","2023-03-13 17:41:30","618","1","5","75879275","<p>Until now we were using Java 17 (Microsoft OpenJDK) and unfortunately the newest auto-update broke the loading of the jvm.dll from that distro.</p>
<p>For us the following combination works:</p>
<p>Integration Runtime: <a href=""https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_5.26.8431.1.msi?update=1"" rel=""nofollow noreferrer"">5.26.8431.1</a> + Java (Adoptium): <a href=""https://adoptium.net/temurin/releases/?version=8"" rel=""nofollow noreferrer"">jdk8u362-b09</a></p>
<p>Make sure that you select the &quot;Set JAVA_HOME variable&quot; and/or the &quot;JavaSoft Registry keys&quot; feature when installing. Also don't forget to restart the IR afterwards:</p>
<pre><code>C:\Program Files\Microsoft Integration Runtime\5.0\Shared\diacmd.exe -r
</code></pre>
<p><em><strong>Microsoft statement:</strong></em></p>
<p><strong>Workaround:</strong> Customers impacted by this issue could use one of following alternatives:</p>
<p><strong>Option 1:</strong> Downgrade to SHIR version 5.25.8410.1, where Microsoft build of OpenJDK is applicable.
<strong>Option 2:</strong> If you'd prefer to stay at newer version of SHIR, switch to use Oracle JDK (e.g. jdk-8u341-windows-x64) or OpenJDK implementations of other providers like Eclipse Temurin.</p>
"
"75725204","Missing jvm.dll in self-hosted integration runtime Windows Container","<p>I'm spinning up a Windows Container image (provided my MS <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container"" rel=""nofollow noreferrer"">here</a>) containing the Self-Hosted Integration Runtime to be able to use ADF in a on-premise situation. It ran smoothly until I needed to use Parquet files.</p>
<p>When I pointed the output to a .parquet I got a Data Factory task failure pointing the absence of Java in the Integration Runtime container.</p>
<p><code>ErrorCode=JreNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Java Runtime Environment cannot be found on the Self-hosted Integration Runtime machine. It is required for parsing or writing to Parquet/ORC files. Make sure Java Runtime Environment has been installed on the Self-hosted Integration Runtime machine.,Source=Microsoft.DataTransfer.Common,''Type=System.DllNotFoundException,Message=Unable to load DLL 'jvm.dll': The specified module could not be found.</code></p>
<p>I took the path of modifying the <a href=""https://github.com/Azure/Azure-Data-Factory-Integration-Runtime-in-Windows-Container/blob/main/SHIR/setup.ps1"" rel=""nofollow noreferrer"">build.ps1</a> file to install and configure the dependencies during container image creation. These are the steps taken:</p>
<ul>
<li><p>Install Microsoft Visual C++ 2010 Service Pack 1 (<a href=""https://www.microsoft.com/en-us/download/details.aspx?id=26999"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Instal JDK provided from Microsoft OpenJDK 17.0.6 LTS - 64bits MSI (<a href=""https://learn.microsoft.com/pt-br/java/openjdk/download"" rel=""nofollow noreferrer"">here</a>)</p>
</li>
<li><p>Manually set JAVA_HOME environment variable: <code>setx -m JAVA_HOME &quot;C:\Program Files\Microsoft\jdk-17.0.6.10-hotspot&quot;</code> (As for as I got SHIR will look in the registry for JRE location and in case it is not found it will look JAVA_HOME env var).</p>
</li>
</ul>
<p>Java seems to be working fine, since when I run <code>java -version</code> it returns me the following output.</p>
<pre><code>openjdk version &quot;17.0.6&quot; 2023-01-17 LTS
OpenJDK Runtime Environment Microsoft-7209853 (build 17.0.6+10-LTS)
OpenJDK 64-Bit Server VM Microsoft-7209853 (build 17.0.6+10-LTS, mixed mode, sharing)
</code></pre>
<p>Everything seems to be Ok but I keep getting the error I mentioned above. I tried to install JRE7, JRE8, configure registry keys, but nothing seems to work.</p>
","<java><windows><azure-data-factory><windows-container>","2023-03-13 17:41:30","618","1","5","75890623","<p>For us the following combination works:</p>
<ul>
<li>Integration runtime: 5.26.8431.1</li>
<li>Java OpenJDK11U-jdk_x64_windows_hotspot_11.0.18_10</li>
<li>PATH environement variable: C:\Program Files\Eclipse Adoptium\jdk-11.0.18.10-hotspot\bin;C:\Program Files\Eclipse Adoptium\jdk-11.0.18.10-hotspot\bin\server;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH\</li>
<li>Registry: Computer\HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\JDK\11 and Computer\HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\JDK\11.0.18.10 (no other key under JavaSoft)</li>
</ul>
"
"75725184","Generate .tab file Azure Data Factory with specific static text up to 4 rows","<p>In my .tab file, I need to have first 4 line some static text. 5 th row should be kept empty.The sql data(data to copy)  should be added from line 6 onwards. Any leads for this?</p>
","<azure><azure-data-factory>","2023-03-13 17:38:50","68","0","1","75733246","<p>As the requirement you mentioned is not directly possible in ADF to pass 4 lines static text then skip one line and then copy sql dataset.</p>
<ul>
<li>To achieve your scenario, you have to first create a text file with 4 lines of static text and 1 empty line as below.
<img src=""https://i.imgur.com/FZmgfgj.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Get a SQL table and save results as TAB file:</strong></p>
<ul>
<li><p>Create a SQL Dataset with table table:
<img src=""https://i.imgur.com/poL2C2e.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Create another Dataset for the file with static text. I'm using ADLS in this example, but the concept is the same for other storage types:
<img src=""https://i.imgur.com/JZJ8v98.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Create another Dataset for the TAB file. I'm using ADLS in this example, but the concept is the same for other storage types:
<img src=""https://i.imgur.com/v1yarTm.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>Type 1 (all the columns with tab separated)</strong></p>
<ul>
<li>In your pipeline, add a DataFlow activity. create new dataflow, select source1 as, Dataset created for the file with static text.
<img src=""https://i.imgur.com/8Cs87dw.png"" alt=""enter image description here"" /></li>
<li>Then in your dataflow activity create another source as SQL db dataset which you created earlier
<img src=""https://i.imgur.com/wlehO9n.png"" alt=""enter image description here"" /></li>
<li>Then after source 1 take union transformation to combine both the dataset.
<img src=""https://i.imgur.com/VtNZVAi.png"" alt=""enter image description here"" /></li>
<li>Then store the output to destination tabfile sink settings as below.
<img src=""https://i.imgur.com/Xpg4EsW.png"" alt=""enter image description here"" /></li>
</ul>
<p><em><strong>Output:</strong></em></p>
<p><img src=""https://i.imgur.com/UYir6YF.png"" alt=""enter image description here"" /></p>
<p><strong>Type 2 (combine all the columns as tab separated string)</strong></p>
<ul>
<li>In your pipeline, add a DataFlow activity. create new dataflow, select source1 as, Dataset created for the file with static text.
<img src=""https://i.imgur.com/F9AR4Fq.png"" alt=""enter image description here"" /></li>
<li>Then in your dataflow activity create another source as SQL db dataset which you created earlier
<img src=""https://i.imgur.com/A5b55ig.png"" alt=""enter image description here"" /></li>
<li>Then create derived column for source 2 and concat all columns with tab separation <code>concatWS(' ',col1,col2,col3,col4)</code>
<img src=""https://i.imgur.com/iSYhQL0.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/Gwn5zrW.png"" alt=""enter image description here"" /></li>
<li>After this take select transformation and delete columns which are not required
<img src=""https://i.imgur.com/KaeIJI2.png"" alt=""enter image description here"" /></li>
<li>Then after source 1 take union transformation to combine source1 data with select column data.
<img src=""https://i.imgur.com/5hu57si.png"" alt=""enter image description here"" /></li>
<li>Then store the output to destination tabfile sink settings as below.
<img src=""https://i.imgur.com/6mSugCX.png"" alt=""enter image description here"" /></li>
</ul>
<p><em><strong>Output:</strong></em></p>
<p><img src=""https://i.imgur.com/snm8P7x.png"" alt=""enter image description here"" /></p>
"
"75724828","Dynamic PowerQuery using ADF","<p>Using ADF Pipelines, is there a way to pass parameters (or the entire M query) into the PowerQuery itself?</p>
<p>Here's a description of my challenge:
• I'm dealing with a dynamic data model from a form builder type application so the tables and column names are stored as data<br />
• I'm generating PowerQuery code dynamically using Python as an initial activity in an ADF pipeline. There will be dozens of these PowerQuery scripts.</p>
<p>• I want to pass the PowerQuery code directly into the PowerQuery activity</p>
<p>• I don't see any way to make the PowerQuery dynamic --- it only seems to allow me to manually enter the PowerQuery</p>
<p>I've looked at PowerQuery parameters but I don't see a way to populate these outside the PowerQuery.</p>
<p>** The select answer (1) was very helpful. A few notes since this was done in an ADF pipeline, not Excel:<br />
• A new activity was created that wrote the &quot;parameters&quot; for the PowerQuery into an Azure SQL table (with just &quot;key&quot; and &quot;value&quot; columns)</p>
<p>• A new query was created in PowerQuery that loaded the parameters from the table</p>
<p>•The parameters were assigned to variables in the PowerQuery -- e.g. groupFields = PowerQuery{[Key = &quot;groupFields&quot;]}[Value]</p>
<p>With this approach, the same PowerQuery code to generate multiple tables, (some with 100+ columns) with no hard-coding of the columns names.</p>
","<azure-data-factory><powerquery><m>","2023-03-13 17:00:25","41","0","1","75724984","<p>In powerquery you can</p>
<p>(1) Pull text from the main excel sheet by creating a range name, and pulling that through. The contents of the range name could be populated from a VBA or a form, or whatever</p>
<p>For example, create range name <strong>aaa</strong> in the excel sheet and populate it using AFG, VBA, or some other method. Pull the final contents on that range into powerquery per 2nd row in this example, subsequently used in 3rd row in place of hardcoded filepath</p>
<pre><code>let
NameValue= Excel.CurrentWorkbook(){[Name=&quot;aaa&quot;]}[Content]{0}[Column1],
Source = Table.FromColumns({Lines.FromBinary(File.Contents(NameValue), null, null, 1252)})
in Source
</code></pre>
<p>(2) generate the entire powerquery iself using VBA</p>
<p>(3) load and execute powerquery code contained in an external text file that you create</p>
<pre><code>let
//Load M code from text file
// https://blog.crossjoin.co.uk/2014/02/04/loading-power-query-m-code-from-text-files/
Source = Text.FromBinary(File.Contents(&quot;C:\directory\MyMCode.txt&quot;)),
//Evaluate the code from the file as an M expression
EvaluatedExpression = Expression.Evaluate(Source, #shared)    
in
EvaluatedExpression 
</code></pre>
"
"75723335","How to copy data from SQL table to create sub folders for each table data using azure data factory","<p>Lets say I have 3 tables Table1,Table2,Table3. I want these to be in CSV files in the azure data lake storage. This I have already completed but they are together in a folder. How to copy them in 3 different folders like Table1,Table2,Table3</p>
","<azure><azure-data-factory>","2023-03-13 14:39:13","61","0","1","75733104","<blockquote>
<p>How to copy them in 3 different folders like Table1,Table2,Table3</p>
</blockquote>
<p>Use the below approach to achieve your requirement.</p>
<p>Create the datasets of SQL database and ADLS like below.</p>
<p><strong>SQL database dataset:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;AzureSqlTable1&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;AzureSqlDatabase1&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;parameters&quot;: {
            &quot;table_name&quot;: {
                &quot;type&quot;: &quot;string&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureSqlTable&quot;,
        &quot;schema&quot;: [],
        &quot;typeProperties&quot;: {
            &quot;schema&quot;: &quot;dbo&quot;,
            &quot;table&quot;: {
                &quot;value&quot;: &quot;@dataset().table_name&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            }
        }
    }
}
</code></pre>
<p><strong>ADLS dataset with parameter:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;target_csv_files&quot;,
&quot;properties&quot;: {
    &quot;linkedServiceName&quot;: {
        &quot;referenceName&quot;: &quot;AzureDataLakeStorage1&quot;,
        &quot;type&quot;: &quot;LinkedServiceReference&quot;
    },
    &quot;parameters&quot;: {
        &quot;folder_name&quot;: {
            &quot;type&quot;: &quot;string&quot;
        },
        &quot;table_name&quot;: {
            &quot;type&quot;: &quot;string&quot;
        }
    },
    &quot;annotations&quot;: [],
    &quot;type&quot;: &quot;DelimitedText&quot;,
    &quot;typeProperties&quot;: {
        &quot;location&quot;: {
            &quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
            &quot;fileName&quot;: {
                &quot;value&quot;: &quot;@dataset().table_name&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            },
            &quot;folderPath&quot;: {
                &quot;value&quot;: &quot;@dataset().folder_name&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            },
            &quot;fileSystem&quot;: &quot;data&quot;
        },
        &quot;columnDelimiter&quot;: &quot;,&quot;,
        &quot;escapeChar&quot;: &quot;\\&quot;,
        &quot;firstRowAsHeader&quot;: true,
        &quot;quoteChar&quot;: &quot;\&quot;&quot;
    },
    &quot;schema&quot;: []
}
}
</code></pre>
<p>Then create a lookup activity with <code>SELECT name FROM sys.Tables;</code> script to take list of tables.</p>
<p><img src=""https://i.imgur.com/6p1WCwt.png"" alt=""enter image description here"" /></p>
<p>Lookup will give output like below:</p>
<p><img src=""https://i.imgur.com/zj20vd0.png"" alt=""enter image description here"" /></p>
<p>Give this array to a ForEach and inside ForEach use the copy activity. Give the source and sink of the copy activities like below with dynamic contents  <code>@item().name</code> and <code>@concat(item().name,'.csv')</code>.</p>
<p><strong>Source:</strong></p>
<p><img src=""https://i.imgur.com/YGpxdeC.png"" alt=""enter image description here"" /></p>
<p><strong>Sink:</strong></p>
<p><img src=""https://i.imgur.com/RuZ9Ipn.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/krXl2gY.png"" alt=""enter image description here"" /></p>
"
"75721820","How do I identify and remove a special character from a string in ADF or SQL?","<p>I have a file being extracted in an unusual format, it comes through as a text file, however it appears there are spaces between each of the characters. I have some SQL to clean this up, but when I try to import it using ADF it identifies these additional spaces as an unusual character I have never seen before.</p>
<p>Example of what I see in ADF :</p>
<p>Above is a snapshot I have taken of how it appears in ADF when I preview the inputs. I would like to just replace the symbol that appears to read 'bul' but I have no idea what these are and I am unable to copy and paste the symbol as everything reads it either as a space or not at all.</p>
<p>So far I have tried to enter the codes for bullet points in a replace function within ADF, when attempting to just run the file into SQL I get only blank rows currently too, so I believe this character is breaking my import too.</p>
<p>Any help you can offer is appreciated, I'm really at my wits end on this one.</p>
<p>edit:
After a prompt from Dump Eldor here is an image of the file from Notepad++. It looks like it says nul between each character. Is this a utf-16 issue?</p>
<p><a href=""https://i.stack.imgur.com/GXNzj.png"" rel=""nofollow noreferrer"">Notepad Screenshot</a></p>
","<tsql><unicode><azure-data-factory>","2023-03-13 12:23:21","70","0","1","75723641","<p>Problem solved. The issue was my original file is sent as UTF-16BE encoding instead of UTF-8.</p>
<p>I was able to get Data Factory to read it in the correct format and all the issues are gone!</p>
"
"75718939","Running multiple databricks notebooks concurrently","<p>I have an Azure Data Factory pipeline that triggers a Databricks notebook.
Inside this notebook, I have the following code to unmount / mount storage,</p>
<pre><code># Unmount and mount storage
mnt_point = &quot;/mnt&quot;
out_mnt_point = &quot;/out_mnt&quot;

# Unmount storage, if any
for mount in dbutils.fs.mounts():
    if (mount.mountPoint == mnt_point):
        dbutils.fs.unmount(mnt_point)
    elif (mount.mountPoint == out_mnt_point):
        dbutils.fs.unmount(out_mnt_point)

# Mount storage for input
dbutils.fs.mount(
    source = f&quot;wasbs://&quot; + input_folder + &quot;@xxx.blob.core.windows.net&quot;,
    mount_point = mnt_point,
    extra_configs = {f&quot;fs.azure.account.key.xxx.blob.core.windows.net&quot;: azure_account_key }
)

# Mount storage for output
dbutils.fs.mount(
    source = f&quot;wasbs://&quot; + output_folder + &quot;@xxx.blob.core.windows.net&quot;,
    mount_point = out_mnt_point,
    extra_configs = {f&quot;fs.azure.account.key.xxx.blob.core.windows.net&quot;: azure_account_key }
)
</code></pre>
<p>My question is that, if there are multiple instances of the pipeline are running concurrently, will this affect each other (e.g. one notebook is mounting and the other is unmounting and make the other process fail)? Or does each instance has it's own specific isolated resource?</p>
","<azure-data-factory><azure-databricks>","2023-03-13 07:15:23","85","0","1","75722394","<ul>
<li>Running the same notebook simultaneously to unmount and mount containers in azure data factory, they are considered as separate notebook runs.</li>
<li>However, since the code is accessing the same resources (blob storage containers), notebooks running simultaneously is causing error.</li>
<li>I have used a for each activity with batch execution to run databricks notebook activity to execute notebook activity simultaneously for demonstration.</li>
</ul>
<p><img src=""https://i.imgur.com/HXn9Djb.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, when this is executed, the activities fail apart from one in this case:</li>
</ul>
<p><img src=""https://i.imgur.com/2SSEvGb.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I open the runs to check for error, the message says that <code>Directory already mounted: /mnt/ip</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/aIjDQFH.png"" alt=""enter image description here"" /></p>
<ul>
<li>This is because the simultaneous notebook execution is causing some of the mount operations to occur before others in respective notebook runs. So, there is great chance for notebook run to fail in case of simultaneous execution.</li>
</ul>
<p><strong>UPDATE:</strong></p>
<p>Try using the following code:</p>
<pre><code># Unmount and mount storage
azure_account_key = 'SRzAYuN2/aRJuSdHkwSXxXIE3qpBl0ekvtVSQ4BKqFAi+z2SM86qrUM3rt5tD3s68m450n/aledC+AStTrzdBw=='
mnt_point = &quot;/mnt/ip&quot;
out_mnt_point = &quot;/mnt/op&quot;

ip_mount = 0
op_mount = 0


for mount in dbutils.fs.mounts():
    if(ip_mount ==0 or op_mount ==0):
        if (mount.mountPoint == mnt_point):
            ip_mount+=1
        elif (mount.mountPoint == out_mnt_point):
            op_mount+=1
    
else:
    if(ip_mount==0):
        dbutils.fs.mount(
    source = f&quot;wasbs://&quot; + input_folder + &quot;@xxx.blob.core.windows.net&quot;,
    mount_point = mnt_point,
    extra_configs = {f&quot;fs.azure.account.key.xxx.blob.core.windows.net&quot;: azure_account_key }
)
        
    if(op_mount==0):
        dbutils.fs.mount(
    source = f&quot;wasbs://&quot; + output_folder + &quot;@xxx.blob.core.windows.net&quot;,
    mount_point = out_mnt_point,
    extra_configs = {f&quot;fs.azure.account.key.xxx.blob.core.windows.net&quot;: azure_account_key }
)
</code></pre>
<ul>
<li>Running the notebook with above code simultaneously did not throw any error:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/pdvNy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pdvNy.png"" alt=""enter image description here"" /></a></p>
<p>NOTE: The case is same for even simultaneous pipeline runs as well instead of simultaneous activity runs as demonstrated above.</p>
"
"75716409","Azure Data Factory import from SAP HANA - Performance Issues","<p>We are importing SAP HANA Objects via the adf SAP HANA connector and write to an ADLS gen2. The Throughput is &lt; 2mbit/s. The issue is (our findings), that adf requests many small packages. We tested and ruled out that it's an infrastructure error.</p>
<p>The only options from the adf side to fiddle with are 'Partition option' and 'Packet size (KB)'. We tested both and they dont have any impact on the speed importing the big view in question.</p>
<p>screen_1: adf copy data source settings
<a href=""https://i.stack.imgur.com/BM3Qr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BM3Qr.png"" alt=""adf copy data source settings"" /></a></p>
<p>screen_2: adf copy file details
<a href=""https://i.stack.imgur.com/cmMP1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cmMP1.png"" alt=""adf copy file details"" /></a></p>
<p>Does anyone had a similar experience and some wise words to share here?</p>
","<azure-data-factory><hana>","2023-03-12 21:13:14","93","0","1","75716487","<p>Maybe you can try increasing the parallelism. According to doc,</p>
<ul>
<li>When copying data from partition-option-enabled data store (including Azure SQL Database, Azure SQL Managed Instance, Azure Synapse Analytics, Oracle, Amazon RDS for Oracle, Netezza, SAP HANA, SAP Open Hub, SAP Table, SQL Server, Amazon RDS for SQL Server and Teradata), default parallel copy is 4.</li>
</ul>
<p>try increasing the node count on your SHIR, increase your parallelism or size. (Scale up or out)</p>
<p>If you copy data from partition-option-enabled data stores (like SAP HANA), consider to gradually tune the parallel copies, note that too many parallel copies may even hurt the performance.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-troubleshooting#troubleshoot-copy-activity-on-self-hosted-ir"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-troubleshooting#troubleshoot-copy-activity-on-self-hosted-ir</a></p>
<p>Have you been into this documentation?</p>
<p>If the suggestions in the docs dont fix your issue, and you tried a lot of things already. Think whether the SAP HANA is the bottleneck here. Also note that, it takes some time for the copy speed to eventually increase and reach peak levels.</p>
<p>For me to help you, you should try all these steps in the docs, and then list what has not worked for you so far.</p>
<hr />
<p>Aaand.. most importantly:</p>
<p>Here are more details how to scale up or scale out the Self-hosted IR:</p>
<p>If the CPU and available memory on the Self-hosted IR node are not fully utilized, but the execution of concurrent jobs is reaching the limit, you should scale up by increasing the number of concurrent jobs that can run on a node. See here for instructions.
If on the other hand, the CPU is high on the Self-hosted IR node or available memory is low, you can add a new node to help scale out the load across the multiple nodes. See here for instructions.
Note in the following scenarios, single copy activity execution can leverage multiple Self-hosted IR nodes:</p>
<p>Copy data from file-based stores, depending on the number and size of the files.
Copy data from partition-option-enabled data store (including Azure SQL Database, Azure SQL Managed Instance, Azure Synapse Analytics, Oracle, Netezza, SAP HANA, SAP Open Hub, SAP Table, SQL Server, and Teradata), depending on the number of data partitions.</p>
"
"75711651","Unexpected Error in Synapse Datflow- XX column is not of the same unpivot datatype","<p>I wanna unpivot column in Synapse dataflow, but the debug was not worked due to below error.
<a href=""https://i.stack.imgur.com/EHpi0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EHpi0.png"" alt=""the error message"" /></a></p>
<p>The error says &quot;XX column is not of the same unpivot datatype&quot;, but I select the correct datatype.
Before unpivot, datatype of columns that I wanna unpivot is all decimal.
<a href=""https://i.stack.imgur.com/OwoQW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OwoQW.png"" alt=""Unpivot Setting"" /></a></p>
<p>As you can see the column datatype is all Decimal, so I'm confused...
<a href=""https://i.stack.imgur.com/u8IOf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u8IOf.png"" alt=""The data before unpivot"" /></a></p>
<p>The deadline of this task is imminent and I am in a hurry.
Any answer would be helped.</p>
<p>Thank you.</p>
","<azure><azure-synapse><azure-data-factory>","2023-03-12 07:28:43","44","0","1","75718949","<p>I tried to reproduce the same thing with all decimal column which we want to unpivot and got similar error.</p>
<p><img src=""https://i.imgur.com/jKawBwI.png"" alt=""enter image description here"" /></p>
<p>sample data:</p>
<p><img src=""https://i.imgur.com/W5Y0IsT.png"" alt=""enter image description here"" /></p>
<p>To resolve this, <strong>you have to mention all columns other than decimal data type has to be mentioned in <code>Ungroup by</code></strong> .</p>
<p><img src=""https://i.imgur.com/OnBbNqN.png"" alt=""enter image description here"" /></p>
<p>Output:</p>
<p><img src=""https://i.imgur.com/w0Ib6qV.png"" alt=""enter image description here"" /></p>
"
"75711420","Beat methods to validate/test the output data with original data","<p>I have built dataflows from customer input data in Asure data factory. We designed the Workflows which are built on Alteryx(another Etl tool) in Azure data factory .</p>
<p>Now testing the sample data is ok on both sides.</p>
<p>But how to validate the entire output of Alteryx and Azure to be matched/Validated. Is there any tool.</p>
<p>My output file format is CSV.</p>
<p>Is there any automation proceaa to validate all the rows of Alteryx outout and Azure Output so that i can be sure that i have built the right Dataflow logics.</p>
","<azure><azure-data-factory>","2023-03-12 06:28:30","55","0","1","75712541","<p>You should be able to do verify the whole dataset through validate activity if thats what you want (that the dataset meets a certain criteria etc.):</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-validation-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-validation-activity</a></p>
<p>Dataset validation directly on Alteryx output is not supported, in which case you need to store the dataset somewhere that you just transformed in Alteryx like a blob store, then you can validate there. Validation activity usually supports file storage systems like datalake, (S)FTP, or a relational database.</p>
<p>You can also do this through dataflows but you need to store it somewhere, as ADF cannot store any files or data by itself. Once you have both datasets stored, you can for example do this through dataflows by self-joining the datasets, and creating hash values and comparing those values, there is a video like this:</p>
<p><a href=""https://www.youtube.com/watch?v=GACpvMjOJgE"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=GACpvMjOJgE</a></p>
<p>And here is another post similar to yours:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/856074/how-can-i-compare-source-data-(sql-server)-with-de"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/856074/how-can-i-compare-source-data-(sql-server)-with-de</a></p>
<p>Later if you wanna do this going forward, you can use CDC.</p>
"
"75705941","Zero Padding in a variable","<p>Below is my scenario :
Input value=1234
Input Range= 2</p>
<p>Expected output =1200</p>
<p>Input value=12345
Input Range= 3</p>
<p>Expected output =12000</p>
<p>The ask is replace the number of characters/digits from right with zero equivalent to the Input range</p>
<p>I have followed the below logic:</p>
<pre><code>{
    &quot;name&quot;: &quot;RoundPad&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@range(1,pipeline().parameters.Range)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Set variable1&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;TempPower&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@string(mul(int(variables('Power')),10))&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;Set variable2&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Set variable1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;Power&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@variables('TempPower')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Set variable3&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;Output&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@string(mul(div(int(pipeline().parameters.Input),int(variables('Power') )),int(variables('Power'))))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;Input&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;1234&quot;
            },
            &quot;Range&quot;: {
                &quot;type&quot;: &quot;int&quot;,
                &quot;defaultValue&quot;: 2
            }
        },
        &quot;variables&quot;: {
            &quot;TempPower&quot;: {
                &quot;type&quot;: &quot;String&quot;,
                &quot;defaultValue&quot;: &quot;1&quot;
            },
            &quot;Output&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;Power&quot;: {
                &quot;type&quot;: &quot;String&quot;,
                &quot;defaultValue&quot;: &quot;1&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/ttoeP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ttoeP.png"" alt=""enter image description here"" /></a></p>
<p>Is there any better way to do this rather than leveraging For each iteration and multiple variables?
I might be missing something here</p>
","<azure-data-factory>","2023-03-11 15:25:40","102","0","1","75717880","<ul>
<li>You can take the substring from input value i.e., last n number of characters/digits, where n is the Range.</li>
<li>Then subtract that number from the input value to get the desired result. The following is the complete pipeline JSON:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;replace_range&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;sample1&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@substring(pipeline().parameters.input,sub(length(pipeline().parameters.input),pipeline().parameters.Range),pipeline().parameters.Range)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Set variable2&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;res&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@string(sub(int(pipeline().parameters.input),int(variables('sample1'))))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;input&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;12345&quot;
            },
            &quot;Range&quot;: {
                &quot;type&quot;: &quot;int&quot;,
                &quot;defaultValue&quot;: 3
            }
        },
        &quot;variables&quot;: {
            &quot;sample1&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;res&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;prac&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;lastPublishTime&quot;: &quot;2023-03-11T17:42:32Z&quot;
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p>Result when input value is 12345 and range is 3:</p>
<p><img src=""https://i.imgur.com/UqIikrz.png"" alt=""enter image description here"" /></p>
"
"75699243","Dynamically fetch a ""since"" Timestamp in the body of my web activity and dynamically update it after successful pipeline runs","<p>I have been stuck with this issue - I have a pipeline - see attached a screenshot, that would would download tables from a REST API asynchronously. The pipeline has the below workflow:</p>
<p>Step 1: web activity to get access_token using a POST method. Here, API KEY was passed for the activity to return an acess_token</p>
<p>Step 2: Another web activity that would accept that access_token from step one using a POST method of API calls. Here, the body of the web activity contains this json - {&quot;format&quot;:&quot;jsonl&quot;,&quot;since&quot;:&quot;2023-03-07T20:01:39.135Z&quot;}</p>
<p>Step 3: An Until activity which contains another 'web activity' and a 'wait' condition. The web activity in step 2, would get a update query and the one inside the Until activity, would return the status (either waiting or complete). The 'Wait condition' tells the Until asctivity to either continue to the next task when the status is complete or wait until it is complete.</p>
<p>Step 4: Another web activity which will get url downloads that would be passed to a copy activity</p>
<p>Step 5: ForEach activity which contain a copy activity. This copy activity uses the web activity from step 4 to download a files from the api into a datalake.</p>
<p>Following the above workflow, I was able to get the pipeline working and some files were downloaded in my gen2 datalake.</p>
<p>Here is my challenge:</p>
<p>Because I would want to query this data almost on a daily, I want to be able to DYNAMICALLY reference the json in the body of the web activity in STEP 2.</p>
<p>For clarity, what I mean is, say I run this pipeline today, according to the body of the web acivity in STEP 2, I want to fetch file updates &quot;since&quot;:&quot;2023-03-07T20:01:39.135Z&quot; to &quot;2023-03-10T20:01:39.135Z&quot;. That would mean that, if I run the pipeline tomorrow, I would want to fetch files &quot;since&quot;:&quot;2023-03-11T20:01:39.135Z&quot;. What I want to achieve is the ability to dynamically fetch this without having to do manually update the json in the body of web activity in step 2. I want the body in STEP 2 to dynamically update to the new &quot;since&quot;:&quot;Timestamp&quot; everytime the pipeline is run.</p>
<p>I would appreciate if any  one has any idea on how to execute this.</p>
<p><a href=""https://i.stack.imgur.com/XjpOp.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory>","2023-03-10 17:26:17","70","0","1","75703832","<p>AFAIK, documents state that there is no such case. The output of earlier activities would serve as the current activity's input. The same can be used to specifically put in a variable for future use.</p>
<p>You can only view or copy inputs of an activity from pipeline run output or errors.</p>
<p><img src=""https://i.imgur.com/ryjGiSf.png"" alt=""enter image description here"" /></p>
<p><strong>for your requirement the workaround would be as follow:</strong></p>
<ul>
<li>you have to create a blob storage and upload a file in it with since value.
<img src=""https://i.imgur.com/DylfZPU.png"" alt=""enter image description here"" /></li>
<li>Then in pipeline take a lookup activity to get since value from file as below
<img src=""https://i.imgur.com/JfKaM72.png"" alt=""enter image description here"" /></li>
<li>Then pass the output of the lookup to web activity body like <code>{&quot;format&quot;:&quot;jsonl&quot;,&quot;since&quot;:&quot;@{activity('Lookup1').output.firstRow.since}&quot;}</code>
<img src=""https://i.imgur.com/bCKZv7u.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/biwzH0I.png"" alt=""enter image description here"" /></li>
<li>After this take a set variable to update the timestamp with expression <code>@adddays(activity('Lookup1').output.firstRow.since,1)</code> <strong>this expression will add 1 day to timestamp</strong>
<img src=""https://i.imgur.com/12qxaNl.png"" alt=""enter image description here"" /></li>
<li>Then update this value in blob storage using copy activity in source of the copy activity add the same file as dataset and create additional column as <strong>since1</strong> with value <code>@variable('output')</code> for sink also take same file.
<img src=""https://i.imgur.com/WccnV68.png"" alt=""enter image description here"" />
go to mapping &gt;&gt; import mapping &gt;&gt; map since1 to since and delete other mappings.
<img src=""https://i.imgur.com/R2uCGFS.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>This will dynamically take updated value from blob file.</strong></p>
"
"75698992","Azure Data Factory equivalent to SSIS 'data conversion transformation' task?","<p>I'm wondering if anyone familiar with Azure data factory can help me figure this out. In SSIS I use the &quot;data conversion transformation&quot; task. It might be staring me right in the face, but I can't find the equivalent task in azure data factory. Anyone know? TY</p>
<p>I've looked through all of the ADF tasks available and none seem to cover data conversion.</p>
","<azure><ssis><azure-data-factory>","2023-03-10 16:58:42","44","0","1","75715440","<p>That is either the Mapping dataflow activity or PowerQuery activity depending on what you want to do. If your goal is ETL/ELT then its dataflow either in ADF or Synapse pipelines (they are the same thing essentially).  The activity you talked about in SSIS essentially changes the datatype of the column and copies to another column, for example int64 into float.</p>
<p>In ADF mapping dataflow, this transformation is called Cast under Schema modifiers, influenced by the spark logic:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-cast"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-cast</a></p>
<p>Here is a nice video i found:</p>
<p><a href=""https://www.youtube.com/watch?v=OUX4ALdBbvA"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=OUX4ALdBbvA</a></p>
"
"75698601","Move Data from Oracle Server to Azure Blob Storage with Incremental Changes or delta load","<p>I have a problem with the incremental load I want to create an delta load (incremental load) pipeline from the Oracle on-premise server to Azure data lake(blob storage) I don't have Azure SQL. I just want to push in blob storage as a CSV file. in my case, I am confused about how can I do this?. someone told me in your case you have to use parquet delta. please help me with this I am stuck for many days.</p>
<p>First I loaded with full load (copy activity) now I want load only new data from oracle server with triger every week on sunday.</p>
<p>PLease help in this issue!
Thanks in advance
Waris</p>
<p>I need help in pipeline to build increment load only new rows from oracle server to azure data lake</p>
","<azure><azure-synapse><azure-data-factory>","2023-03-10 16:23:38","77","0","1","75702868","<p>In the copy activity, instead of selecting the table option in the source, you can select Query and there you can write your select statement with where clause on any timestamp column (ex: <code>loadtimestamp &gt;= getdate()-7</code>) for incremental data load.</p>
<p><a href=""https://i.stack.imgur.com/IAWMG.png"" rel=""nofollow noreferrer"">copy activity</a></p>
"
"75698132","Concatenate with a "","" the responses from Rest DataSource in a Copy Activity of Azure Synpase/Data Factory","<p>I have a copy activity that is using a REST data source to query an API with pagination the response format is JSON and a Azure Data Lake Gen 2 JSON file as the Synk, my issue is that the output of the copy activity is added each rest pagination response JSON object in each line of the output JSON file, making it a not valid JSON file.</p>
<p>in example if the source task of the copy activity query 3 pages from the API the output Json file will look like:</p>
<pre><code>{items:[content of page 1]}
{items:[content of page 2]}
{items:[content of page 3]}
</code></pre>
<p>Sample REST output:</p>
<pre><code>{
   items:[
      &quot;property1: &quot;value1&quot;,
      &quot;property2: &quot;value2&quot;,
      ...
   ]
}

</code></pre>
<p>I have tried doing this with a loop activity and writing a JSON file for each page/response from the API, but I feel that will incur on many I/O operations and therefore increase the cost.</p>
<p>I would like to format the output (JSON file) to a valid JSON format so it can be consumed for other services, in a single copy activity call:
in example:</p>
<pre><code>{items:[
{items:[content of page 1]},
{items:[content of page 2]},
{items:[content of page 3]},
]}
</code></pre>
<p><strong>EDIT Screenshots of current configuration</strong></p>
<p>Copy Activity:
<a href=""https://i.stack.imgur.com/Z3Q7C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z3Q7C.png"" alt=""screen shot of copy activity"" /></a></p>
<p>Output JSON File
<a href=""https://i.stack.imgur.com/LAkUG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LAkUG.png"" alt=""screen shot of output json file"" /></a></p>
","<azure-synapse><azure-data-factory>","2023-03-10 15:41:56","62","1","1","75702824","<p>In order to concatenate all responses with <code>,</code> comma, you need to change the file pattern to <code>Array of Objects</code>. Below are the steps to change the file pattern.</p>
<ul>
<li>Click the <strong>Sink</strong> settings tab of copy activity.</li>
<li>In File pattern, Select <strong>Array of objects</strong>.  If this value is left blank or Set as <code>Set of objects</code>, it stores each Json response in a separate line.</li>
</ul>
<p><img src=""https://i.imgur.com/fYJoxMf.png"" alt=""enter image description here"" /></p>
<p><strong>Output data when File Pattern is Set of Objects:</strong>
All Json object is stored in a separate line.
<img src=""https://i.imgur.com/LnMGUGd.png"" alt=""enter image description here"" /></p>
<p><strong>Output data when File Pattern is Array of Objects:</strong>
All Json objects are concatenated with commas.
<img src=""https://i.imgur.com/7i5MzAr.png"" alt=""enter image description here"" /></p>
"
"75697218","Error 2001 in Azure Data Factory executing a Stored Procedure","<p>We are trying to execute and stored procedure from ADF, however we are getting this error message</p>
<pre><code>The length of execution output is over limit (around 4MB currently)
</code></pre>
<p>We have checked on internet a solution for this, but the official docs make reference to this error only during the execution of a Web activity:</p>
<p><a href=""https://i.stack.imgur.com/7SIBj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7SIBj.png"" alt=""enter image description here"" /></a></p>
<pre><code>https://learn.microsoft.com/en-us/answers/questions/700102/how-to-scale-out-the-size-of-data-moving-using-azu
</code></pre>
<p>However, here we are not moving data throught ADF, we are just executing a SP that populates a table based on another one in the same Data Base.</p>
<p>What could be a possible explanation for this error?</p>
","<azure><stored-procedures><azure-data-factory>","2023-03-10 14:15:24","72","0","1","75710243","<p>As with this kind of limit related errors, there should be a workaround in diversifying the load.</p>
<p>See this recent post about this, the responder suggests a workaround with ForEach activity:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/700102/how-to-scale-out-the-size-of-data-moving-using-azu"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/700102/how-to-scale-out-the-size-of-data-moving-using-azu</a></p>
<p>I do remember similar problems with APIs when you dont use pagination in the activity, then it would go easily over the limit, then it would throw The HTTP 429 Too Many Requests error (I know yours isnt the same, but they are similar). Instead, implementing an iterative process to go through pagination have resolved this error, for example. An iterative process should provide the solution</p>
"
"75696567","Getting Error while sending parameters from child Pipeline to Inner Parent Pipeline in ADF","<p>I referred <strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-return-value"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-return-value</a></strong></p>
<p>I created one Parent Pipeline -&gt;2 nd Parent Pipeline -&gt;Child Pipeline
I declared one variable inside the  child  Pipeline(Which is return variable).
retval</p>
<p>Then  i validate<br />
@activity(&quot;Pname&quot;).output.retval
And i am cancelling this child pipeline  using rest API on a particular case
.Then control flow to 2 nd Parent
After that  i tried to get value  from child in 2nd Parent Pipeline
@activity(&quot;Pname&quot;).output.pipelineReturnValue.retval</p>
<p>But here Im getting error @activity(&quot;Pname&quot;).output.pipelineReturnValue.retval
cannot be evaluated because  pipelineReturnValue doesnt exits available properties are pipelinename,Pipeline Run Id</p>
<p>Some body help me</p>
","<azure-data-factory>","2023-03-10 13:11:22","95","0","1","75697077","<ul>
<li><p>Since the child pipeline is cancelled and the parent pipeline2 fails, you cannot get the return value from child pipeline.</p>
</li>
<li><p>I have used a set variable to return value and a <code>fail activity</code> after that such that pipeline fails (instead of cancelling).</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/H0XaCez.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I execute the child pipeline is parent pipeline2, the parent pipeline2 fails.</li>
</ul>
<p><img src=""https://i.imgur.com/1zv0nb7.png"" alt=""enter image description here"" /></p>
<ul>
<li>Since the return value is configured before cancelling API call, and cancelling is leading to parent pipeline2 failing, you can give the <code>on fail</code> flow to another set variable to get the return value even when the child pipeline fails:</li>
</ul>
<p><img src=""https://i.imgur.com/t50JCPt.png"" alt=""enter image description here"" /></p>
<ul>
<li>Or instead of using the pipeline return value feature, you can write your value to a file so that it can be accessed  anytime as well.</li>
</ul>
<p>UPDATE:</p>
<p>To store variable output in a csv file use the following steps:</p>
<ol>
<li><p>Take a sample file with header and just one row and one column (this will be ignored anyway).
<img src=""https://i.imgur.com/gwA3V9k.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Take this file as source for copy data activity, create an additional column with any name and value as dynamic content (set variable output):</p>
</li>
</ol>
<p><img src=""https://i.imgur.com/VuHEnsw.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>Configure sink as shown below:</li>
</ol>
<p><img src=""https://i.imgur.com/gjs1mjm.png"" alt=""enter image description here"" /></p>
<ol start=""4"">
<li>Import mapping and delete all the other columns, except for the one created above.</li>
</ol>
<p><img src=""https://i.imgur.com/Y7g1aUI.png"" alt=""enter image description here"" /></p>
<ol start=""5"">
<li>After running the pipeline, it would give the desired results, set variable value in a csv file. You can use lookup any time to retrieve this value:</li>
</ol>
<p><img src=""https://i.imgur.com/A1kbdbI.png"" alt=""enter image description here"" /></p>
"
"75693298","Can I add an Azure data factory service principal inside Azure Databricks to grant it access to run notebooks rather than giving it ""Contributor"" role","<p>I followed <a href=""https://www.kimanimbugua.com/post/set-up-azure-databricks-managed-identity-in-data-factory"" rel=""nofollow noreferrer"">this link</a> to use MSI authentication for Axure Data factory to configure and run my Azure Databricks notebooks.</p>
<p>This requires ADF managed identity to have &quot;Contributor&quot; permissions.</p>
<p>Question1:
Is it possible to use &quot;Reader&quot; permissions to create the linked services and run the notebooks ?</p>
<p>Question2:
In case we decide to assign permissions to ADF MSI inside the &quot;Users and Groups&quot; section in Admin console in the Databricks - can we do so ?</p>
<p>Please suggest.</p>
","<azure-databricks><azure-data-factory>","2023-03-10 07:30:32","119","-1","1","75693593","<p>You need to have contributor permissions, reader does not grant you any kind of edits or creates, including linked services etc. Reader is a very restricted BuiltInRole type as defined by IAM &quot;View all resources but does not allow you to make any changes.&quot;</p>
<p>Question2: You still need to configure everything according to documentation with MSI, there is no other way to do it.</p>
"
"75691573","Pipeline Logging in Azure Data Factory","<p>I have been developing ADF pipelines and using SQL Server tables to log at each stage of the pipeline run.</p>
<p>Now that the organisation has decided to move away from SQL Server and rely only on the ADF (Data being outputted into excel / csv files - so we dont need SQL Server).</p>
<p>I am wondering how can i log each step and the output including all the errors in the ADF itself ?</p>
<p>and can we customise the errors logged for messaging purposes ?</p>
<p>Also My pipelines were previously using the SQL Tables to check if the pipeline is already running (by checking values in tables) and do not start the pipeline if that is the case, if we move away from SQL server, is there a way we can achieve these in ADF ?</p>
<p>Is it something configurable in ADF ?</p>
<p>I have tried to google it, but couldn't find a satisfactory solution.
Any help please?</p>
","<azure-data-factory>","2023-03-10 01:40:40","67","-1","3","75692114","<p>What are your source and sink in case of ADF ?
Is it blob location?
You can use a file to read and write your logs based on custom logics, below blog can help :
<a href=""https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory/</a></p>
<p>Else you can configure diagnostic settings in ADF :<a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-configure-diagnostics"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-configure-diagnostics</a></p>
<p>for your logs to get into Log Analytics or blob location automatically</p>
"
"75691573","Pipeline Logging in Azure Data Factory","<p>I have been developing ADF pipelines and using SQL Server tables to log at each stage of the pipeline run.</p>
<p>Now that the organisation has decided to move away from SQL Server and rely only on the ADF (Data being outputted into excel / csv files - so we dont need SQL Server).</p>
<p>I am wondering how can i log each step and the output including all the errors in the ADF itself ?</p>
<p>and can we customise the errors logged for messaging purposes ?</p>
<p>Also My pipelines were previously using the SQL Tables to check if the pipeline is already running (by checking values in tables) and do not start the pipeline if that is the case, if we move away from SQL server, is there a way we can achieve these in ADF ?</p>
<p>Is it something configurable in ADF ?</p>
<p>I have tried to google it, but couldn't find a satisfactory solution.
Any help please?</p>
","<azure-data-factory>","2023-03-10 01:40:40","67","-1","3","75693955","<p>Easiest way is just to use log analytics, verbose logs there should be enough for anything that you might need. It is indeed a good idea to move away from legacy applications.</p>
<p>You should be able to customize the logging level.</p>
<p>To check for values you normally use Lookup inside an if statement, and you can connect it to an ExecutePipeline activity which starts based on a True or False condition.</p>
"
"75691573","Pipeline Logging in Azure Data Factory","<p>I have been developing ADF pipelines and using SQL Server tables to log at each stage of the pipeline run.</p>
<p>Now that the organisation has decided to move away from SQL Server and rely only on the ADF (Data being outputted into excel / csv files - so we dont need SQL Server).</p>
<p>I am wondering how can i log each step and the output including all the errors in the ADF itself ?</p>
<p>and can we customise the errors logged for messaging purposes ?</p>
<p>Also My pipelines were previously using the SQL Tables to check if the pipeline is already running (by checking values in tables) and do not start the pipeline if that is the case, if we move away from SQL server, is there a way we can achieve these in ADF ?</p>
<p>Is it something configurable in ADF ?</p>
<p>I have tried to google it, but couldn't find a satisfactory solution.
Any help please?</p>
","<azure-data-factory>","2023-03-10 01:40:40","67","-1","3","75837526","<p>From that list of requirements it sounds like you need a different low or no-cost database.  Yes there is probably a technical workaround using multiple various Azure tools but it's arguable if it's worth doing.</p>
<p>Azure Cosmos DB (NoSQL) is free to a certain point- 1,000 RU/s and 25 GB of storage for free for the lifetime of the account.</p>
<p>If you want a heavily discounted relational SQL Server db, you can plan your pipeline runs accordingly and use Azure SQL Server Database Server-less; which you only pay storage costs when it's deactivated.  I think the best part of this one is it abstracts the annual SQL Server licensing fees and you don't pay for a VM on top.  Can easily have this one for $20-$50 month with low usage and it's still Enterprise.</p>
"
"75690837","How to add dynamic content while creating a dataset in ADF to pull files from a FTP server which has yesterday's date attached to it","<p>I have the following files in the FTP server:
<a href=""https://i.stack.imgur.com/Mjp1T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mjp1T.png"" alt=""Files"" /></a></p>
<p>Each file in the ftp server has the following format: EFS_339751_110_{yesterday_date}</p>
<p>The number before yesterday's date remains the same  always. I need to be able to create a dataset in ADF by picking up the yesterday's file:<a href=""https://i.stack.imgur.com/dOVFb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dOVFb.png"" alt=""ADF Pipeline"" /></a></p>
<p>I need to be able to add something in the highlighted part of the picture to be able to do that.
Any help is appreciated, thank you.</p>
","<date><dataset><azure-data-factory>","2023-03-09 23:02:24","39","0","1","75692338","<ul>
<li>Click on <code>Add dynamic content</code> in the file name.</li>
</ul>
<p><img src=""https://i.imgur.com/uXQXndM.png"" alt=""enter image description here"" /></p>
<ul>
<li>Type the below expression in the pipeline expression builder to concat the <code>EFS_339751_110_</code> with yesterday's date.</li>
</ul>
<pre><code>@concat('EFS_339751_110_',formatDateTime(getPastTime(1,'Day'),'yyyyMMdd'))
</code></pre>
<p><strong>getPastTime()</strong> function returns the current Timestamp (in UTC) minus the specified unit. <strong>formatDateTime()</strong> function returns the value in the specified date format.</p>
<ul>
<li>Click Ok.</li>
</ul>
<p><img src=""https://i.imgur.com/kU7hlG4.png"" alt=""enter image description here"" /></p>
<ul>
<li>When Preview data is clicked on the dataset, data is shown without any error.
<img src=""https://i.imgur.com/H8vpGnJ.png"" alt=""enter image description here"" /></li>
</ul>
"
"75690225","How to run multiple queries from append variable?","<p>Pipeline setup:
Lookup (JSON from link that generates ID for each user) &gt; Foreach with Append Variable inside it ( <a href=""https://microsoft.com/createdby=@%7Bitem().user.value%7D"" rel=""nofollow noreferrer"">https://microsoft.com/createdby=@{item().user.value}</a>)</p>
<p>Now I've many rows of Append Variable with link+ID as value.
For example: <a href=""https://microsoft.com/createdby=rfegr23rfgfde3"" rel=""nofollow noreferrer"">https://microsoft.com/createdby=rfegr23rfgfde3</a></p>
<p>Every row has its unique value and they all lead to a JSON file for that user.
I want to run them all (multiple links) and import to SQLDB (Copy data activity or something like that)</p>
<p>How can I accomplish this?</p>
","<azure-data-factory>","2023-03-09 21:29:57","64","0","1","75694414","<p>You can use <strong>http</strong> connector in copy activity to copy the data from that link to SQL database.</p>
<p>In your approach, you use lookup activity which lists the user ids and concat each user id with the string <code>https://microsoft.com/createdby=</code> before to it using for-each activity. You store this list of URLs in an array variable using Append Variable activity.
Since variable cannot be passed dynamically as Base URL in HTTP connector, instead of using append variable activity which stores URL in array variable, follow the below approach.</p>
<ul>
<li><p>Inside for-each activity, Use the <strong>Copy activity</strong></p>
</li>
<li><p>In Source, Create a source dataset.</p>
<ol>
<li>Click <strong>+New</strong></li>
<li>Search for HTTP and click <strong>HTTP</strong></li>
<li>Click <strong>Continue</strong>
<img src=""https://i.imgur.com/yv9K7hl.png"" alt=""enter image description here"" /></li>
<li>Select JSON format. Click Continue.
<img src=""https://i.imgur.com/j1BXjvh.png"" alt=""enter image description here"" /></li>
<li>Create a new linked service and give the Base URL as <code>https://microsoft.com/api</code></li>
<li>If URL is private and has credentials, give the appropriate authentication type and values. Otherwise choose Anonymous.</li>
<li>Click create and click OK.
<img src=""https://i.imgur.com/5CRkbBH.png"" alt=""enter image description here"" /></li>
</ol>
</li>
<li><p>Once Source dataset is created, Click on the <strong>edit</strong> icon.</p>
</li>
<li><p>Create a dataset parameter for Relative URL named <code>relativeURL</code> and pass the value <code>@concat('thread_createdby=',item().user.value)</code> for that parameter.
<img src=""https://user-images.githubusercontent.com/113445679/224272569-8a429b9d-b974-40e1-9459-bd71a7d73684.gif"" alt=""gif108"" /></p>
</li>
<li><p>Similarly, create sink dataset for SQL database.</p>
</li>
<li><p>I used <strong>Auto create table</strong> option in the sink dataset and table name as <code>@{item().user.value}</code>. So that table will be created with name of user id itself.</p>
</li>
</ul>
<p><img src=""https://user-images.githubusercontent.com/113445679/224275255-1e8e1f3a-6738-440e-a5e2-7ea2b6eb25d2.gif"" alt=""gif108"" /></p>
"
"75689062","ADF error "" Expression of type: 'String' does not match the field: 'mappingV2'"" while mapping a @pipeline().parameters.myParameter","<p>While retrieving API response from an API using CopyData in ADF. I would like to map <code>@pipeline().parameters.myParameter</code> so that it can get sinked with the data in the same row but a different Table.</p>
<p>When I go to dynamic mapping to select the <code>@pipeline().parameters.myParameter</code>, all other mappings get removed and I get an error &quot;<code>Expression of type: 'String' does not match the field: 'mappingV2'</code>&quot;</p>
<p>Now, my data type on the mapping also shows String. but I am not sure how to solve this..</p>
","<azure><azure-data-factory>","2023-03-09 19:11:31","141","0","1","75692578","<ul>
<li>The mapping has to be a map object but not string. When I gave a string value to parameter <code>map</code> (value 'hello'), I get a similar warning.</li>
</ul>
<p><img src=""https://i.imgur.com/BPxNZFV.png"" alt=""enter image description here"" /></p>
<ul>
<li>Let's say I have the following as my source:</li>
</ul>
<p><img src=""https://i.imgur.com/S0vXlVh.png"" alt=""enter image description here"" /></p>
<ul>
<li>Using the following parameter (<strong>object type</strong>) value, I was able to give a dynamic mapping to get only <code>id and first_name</code></li>
</ul>
<pre><code>{
   &quot;type&quot;:&quot;TabularTranslator&quot;,
   &quot;mappings&quot;:[
      {
         &quot;source&quot;:{
            &quot;name&quot;:&quot;id&quot;
         },
         &quot;sink&quot;:{
            &quot;name&quot;:&quot;id&quot;
         }
      },
      {
         &quot;source&quot;:{
            &quot;name&quot;:&quot;first_name&quot;
         },
         &quot;sink&quot;:{
            &quot;name&quot;:&quot;firstName&quot;
         }
      }
   ]
}
</code></pre>
<ul>
<li>The result of the copy activity would be as shown in the below image (only id and firstname are mapped).</li>
</ul>
<p><a href=""https://i.stack.imgur.com/uh5zl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uh5zl.png"" alt=""enter image description here"" /></a></p>
"
"75688434","Generate a .tab file by creating a pipeline in Azure Data factory and upload to FTP","<p>I need to create a pipeline where I have to exceute some SQL query, the output of which I have to write to a .tab file(template already have) . After creating this.tab file, I need to upload this file to FTP.</p>
<p>As I am new to this anyone please help me , how I can implement this scenario.</p>
<p><strong>EDIT [not in original scope]</strong></p>
<p>In my .tab file, I need to have  first 4 line some static text. 5 th row should be kept empty.The sql data should be added from line 6 onwards. Any leads for this?</p>
","<azure-data-factory>","2023-03-09 18:02:59","60","0","2","75688984","<p>If I get your question correctly, you are trying to move a .tab file from Azure ADLS to some SFTP server.
In order to achieve this,</p>
<ol>
<li>You can create a linked service which can connect to your SFTP server and then create a dataset.</li>
<li>Create a copy activity and use a dataset which points to the .tab file as source and the above SFTP dataset as Sink
This should do the job of copying files from ADLS/Blob to SFTP folders</li>
</ol>
"
"75688434","Generate a .tab file by creating a pipeline in Azure Data factory and upload to FTP","<p>I need to create a pipeline where I have to exceute some SQL query, the output of which I have to write to a .tab file(template already have) . After creating this.tab file, I need to upload this file to FTP.</p>
<p>As I am new to this anyone please help me , how I can implement this scenario.</p>
<p><strong>EDIT [not in original scope]</strong></p>
<p>In my .tab file, I need to have  first 4 line some static text. 5 th row should be kept empty.The sql data should be added from line 6 onwards. Any leads for this?</p>
","<azure-data-factory>","2023-03-09 18:02:59","60","0","2","75689747","<p>Let's break this down into its component parts:</p>
<p><strong>Execute a SQL query and save results as TAB file:</strong></p>
<ul>
<li>Create a SQL Dataset with no table:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/Fp6Tm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fp6Tm.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>In your pipeline, add a Copy activity. Under the Source tab, to this SQL dataset. For &quot;Use Query&quot;, select the &quot;Query&quot; option and insert your SELECT statement:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/crxYD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/crxYD.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Create another Dataset for the TAB file. I'm using ADLS in this example, but the concept is the same for other storage types:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/PRup4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PRup4.png"" alt=""enter image description here"" /></a></p>
<p>This example is built to be as generic as possible, so it uses Dataset parameters to configure the options at runtime:</p>
<p><a href=""https://i.stack.imgur.com/qF9Wf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qF9Wf.png"" alt=""enter image description here"" /></a></p>
<p>NOTE:
Feel free to hard code any of these values if you prefer. It is easier to select the Tab delimiter in the Dataset: supplying it in this manner requires you to copy and paste a Tab (like from Notepad) which you cannot see in the UI.</p>
<p>The most important step is to NOT define a Schema:</p>
<p><a href=""https://i.stack.imgur.com/sb2HT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sb2HT.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>In the Copy activity, under the Sink tab, select this Dataset and supply the appropriate values:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/hooeO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hooeO.png"" alt=""enter image description here"" /></a></p>
<p>Now when this activity runs, it will copy the results of the query to the text file in storage.</p>
<p><strong>Upload TAB file to FTP site:</strong></p>
<p>Use Copy activity for this as well. For pure data transfer, you should use Binary Datasets.</p>
<ul>
<li>Create a Binary Dataset for your storage account:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/bA2H1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bA2H1.png"" alt=""enter image description here"" /></a></p>
<p>Again, this Dataset uses parameters to make it reusable across a wide array of situations.</p>
<ul>
<li>Create a Binary Dataset for your FTP account:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/M1EHw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M1EHw.png"" alt=""enter image description here"" /></a></p>
<p>This example also uses parameters.</p>
<ul>
<li>Add a Copy activity in your pipeline using the storage Dataset as your Source and the FTP Dataset as your Sink. Fill in the appropriate parameter values in each and you should have a finished pipeline something like this:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/7NTsa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7NTsa.png"" alt=""enter image description here"" /></a></p>
<p><strong>NOTES:</strong></p>
<ul>
<li>As you can tell, parameterized Datasets are very powerful. Here is <a href=""https://causewaysolutions.com/blog/creating-reusable-datasets-in-azure-data-factory-with-parameters/"" rel=""nofollow noreferrer"">an article I wrote</a> that goes into more detail.</li>
<li>If you don't need to retain the TAB file, you can try to use SQL as the source and FTP as the Sink, but the FTP Dataset could no longer be Binary which may hinder performance. I think the approach outlined here is better. If you want, you can add a Delete activity after the FTP upload to remove the TAB file from storage.</li>
<li>If you want to ZIP the file in flight to the FTP, select the ZipDeflate compression type in the FTP Dataset. This will automatically zip the file (but you will need to add the &quot;.zip&quot; extension to the filename yourself).</li>
</ul>
"
"75687980","How can I load several CSVs with the same columns but in different column orders into a single table using Azure Data Factory?","<p>I have a set of 200 CSV files in blob storage in a single folder that each contain the exact same set of 90 columns with the exact same set of column headers, but in some of the files, these columns appear in a different order. For example, in some of the files, the first column is named &quot;transaction_data&quot; while in others, &quot;transaction_data&quot; may be the 8th or 73rd column. I receive several of these files every day via an automated process, and I am not able to manually review them or bucket them into logical groups based on their column orders.</p>
<p>There is no way to know from the filename or any file metadata which of the files have the columns in the &quot;right order&quot; (the order in which the columns appear in my SQL Database table I'm using as the sink), it's more-or-less random. I know this problem is solvable by using some preprocessing like a Python script to force the files to all have columns in the correct order, but that feels like a lot of extra work, time, and potential cost when this is something it seems ADF should handle natively and I'm just doing something wrong.</p>
<p>What I have right now is a data flow where:</p>
<ul>
<li>Source settings &quot;Allow Schema Drift&quot; is checked</li>
<li>Source options are using wildcard paths to find all *.csv files</li>
<li>Sink is an Azure SQL Database table</li>
<li>Sink options &quot;Allow schema drift&quot; is checked</li>
<li>Sink mapping &quot;Auto mapping&quot; is checked</li>
</ul>
<p>But what's happening is that whatever file it picks up first, it's using that file's column order and applying it to all the rest of the files that are processed. As a result, the Sink table will have some rows where, e.g., the data for &quot;transaction_data&quot; is in the wrong column. There must be some way to force it to evaluate each file on its own? As far as I can tell from ADF's documentation and the info popups, it should be using the field names from my CSVs to do the mapping, in fact when I check the &quot;Auto mapping&quot; option on the Mapping tab of my sink, the text reads &quot;All inputs mapped by name including drifted columns&quot;. This is exactly what I want, but isn't the behavior I'm getting.</p>
","<azure><azure-sql-database><azure-data-factory>","2023-03-09 17:19:22","44","0","1","75690558","<p>I was never able to make this work with a data flow, so I ended up using a foreach loop in my pipeline instead, which worked just fine. Bummer because it's much more of a pain to set up but at least it works.</p>
"
"75686328","How to use ADF map() function","<p>I need to perform a data transformation in ADF and I'm having difficulty understanding how to use the map() function in DataFlow.</p>
<p>In the input container, I have items with a format similar to the example below:</p>
<pre><code>{
    &quot;companyName&quot;: &quot;ExampleName&quot;,
    &quot;departmentManager&quot;: [
        {
            &quot;supervisor&quot;: &quot;Jhon&quot;,
            &quot;department&quot;: &quot;Marketing&quot;,
            &quot;salary&quot;: &quot;$135000&quot;
        },
        {
            &quot;supervisor&quot;: &quot;Emily&quot;,
            &quot;department&quot;: &quot;Human Resources&quot;,
            &quot;salary&quot;: &quot;$135000&quot;
        }
    ],
    &quot;id&quot;: &quot;123456798&quot;
}
</code></pre>
<p>I have a container in Cosmos that I retrieve several items and apply derived column and map() to transform the departmentManager column before sending it to another container. The output format will be as follows:</p>
<pre><code>{
    &quot;companyName&quot;: &quot;ExampleName&quot;,
    &quot;departmentManager&quot;: [
        {
            &quot;supervisor&quot;: &quot;Jhon&quot;,
            &quot;department&quot;: &quot;Marketing&quot;,
            &quot;salary&quot;: 135000
        },
        {
            &quot;supervisor&quot;: &quot;Emily&quot;,
            &quot;department&quot;: &quot;Human Resources&quot;,
            &quot;salary&quot;: 135000
        }
    ],
    &quot;id&quot;: &quot;123456798&quot;
}
</code></pre>
<p>As you can see, I am changing the type from string to long. How can I achieve this goal using the map() function?</p>
<p>I was trying to use the following approach:</p>
<pre><code>map(item -&gt; 
    {
        &quot;supervisor&quot;: item.supervisor,
        &quot;department&quot;: item.department,
        &quot;salary&quot;: toLong(replace(item.salary, &quot;$&quot;, &quot;&quot;))
    })
</code></pre>
<p>But I am getting this error:</p>
<p><a href=""https://i.stack.imgur.com/9XHoT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9XHoT.png"" alt=""enter image description here"" /></a></p>
<p>I was looking at the documentation, but the example is not very helpful.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#map"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#map</a></p>
","<azure-cosmosdb><azure-data-factory>","2023-03-09 14:59:04","80","0","1","75694894","<p>There are two different approaches I have tried converting the <em>salary</em> value to long.</p>
<p><em><strong>Approach1</strong></em> using Copy Data/Lookup Activity of ADF and Cosmos DB query.</p>
<p><strong>Configuration for Copy Data/Lookup Activity</strong>
<img src=""https://i.imgur.com/yl596q4.png"" alt=""enter image description here"" /></p>
<p><strong>Cosmos DB Query</strong></p>
<pre><code>SELECT f.companyName,ARRAY(
select c.supervisor,
c.department,
udf.convertToInt(replace(c.salary,&quot;$&quot;,&quot;&quot;)) as salary FROM c
in f.departmentManager)as departmentManager, f.id from f
</code></pre>
<p><strong>Code for udf.convertToInt</strong></p>
<pre><code>`function convertToInt(strNum) {return parseInt(strNum);}`;
</code></pre>
<p><strong>Result</strong></p>
<p><img src=""https://i.imgur.com/0sJKAiy.png"" alt=""enter image description here"" /></p>
<p><em><strong>Approach2</strong></em> using dataflow
Here is the flow I created
<img src=""https://i.imgur.com/UTyxFGc.png"" alt=""enter image description here"" /></p>
<ol>
<li><strong>Source1</strong> is creating source connection with Cosmos DB account.</li>
<li><strong>flatten1</strong> using this activity I have flatten the <em>departmentManager</em> structure to tabular format using configuration for <em>Unroll by</em> and <em>Unroll root</em>.</li>
</ol>
<p><strong>Configuration</strong>
<img src=""https://i.imgur.com/g5vfXRt.png"" alt=""enter image description here"" />
<strong>Result-/Preview</strong>
<img src=""https://i.imgur.com/nT68IwQ.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li><strong>derivedColumn1</strong> added a column <em>departmentManager</em> with expression. Here I have written the expression to convert string to long conversion.</li>
</ol>
<p><strong>Code for expression</strong>
<code>@(supervisor=departmentManager.supervisor, department=departmentManager.department, salary=toLong(replace(departmentManager.salary,&quot;$&quot;,&quot;&quot;)))</code></p>
<p><strong>Configuration</strong><img src=""https://i.imgur.com/Hqj5nif.png"" alt=""enter image description here"" /></p>
<p><strong>Result-/Preview</strong>
Here you can see datatype of <em>salary</em> is showing as <em>long</em>.<img src=""https://i.imgur.com/SgfZZm9.png"" alt=""enter image description here"" /></p>
<ol start=""4"">
<li><strong>aggregate1</strong> used this to convert flatten structure again to Json structure. Used <em>companyName</em> in for <em>Group by</em> and under <em>Aggregates</em> tab used <code>collect(departmentManager)</code> expression for <em>departmentManager</em> column.</li>
</ol>
<p><strong>Configuration</strong></p>
<p><img src=""https://i.imgur.com/BwbeZBn.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/tpDgzxd.png"" alt=""enter image description here"" />
<strong>Result-/Preview</strong>
<img src=""https://i.imgur.com/ftWLrz9.png"" alt=""enter image description here"" /></p>
"
"75685028","Wrong columns while importing schema from Excell sheet in Azure Data flow","<p>My input file is Excel which is on DatalakeGen2 blob.Am using Excel connector in Azure data flow and specified all the filename,path,cotainer .But its showing me the wrong Columns When i import schema ,But when I preview the data in Dataset level ,the columns are coming fine.</p>
<p>Below are the images which explain better.My original Excel file has some formatting done like shown below.
<a href=""https://i.stack.imgur.com/O0HTz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O0HTz.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/G8bre.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G8bre.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/aHN3A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aHN3A.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-03-09 13:05:31","52","0","1","75689580","<p>Did you try checking the header checkbox while creating the dataset for this excel file. If already created, please check the header option in the dataset.</p>
"
"75683399","Trigger API the end of ADF - CSV to Mongo migration","<p>I have a setup where Azure Data Factory converts a CSV into a MongoDB collection. I mean the entries of the CSV file are saved as records in Mongo DB. At the end of migration, I want to trigger an API (either microservice of Azure function) at the end of migration. Is it feasible to detect the end of migration to trigger the API? If yes, how to set up that in ADF?</p>
","<azure><azure-data-factory>","2023-03-09 10:34:39","33","0","1","75684911","<blockquote>
<p>Is it feasible to detect the end of migration to trigger the API? If yes, how to set up that in ADF?</p>
</blockquote>
<p>Yes, you can trigger the API After the successful end of migration with the help of Execute pipeline activity. you can call API or azure function from web activity or Azure function activity.</p>
<ul>
<li>First create pipeline which can migrate the entries of the CSV file are saved as records in Mongo DB.
<img src=""https://i.imgur.com/gO489jy.png"" alt=""enter image description here"" /></li>
<li>Then create another pipeline with execute pipeline activity and web activity or Azure function activity. To ensure that the perform pipeline action runs in order rather than in parallel, make sure to select the <code>Wait on Completion</code> option.
<img src=""https://i.imgur.com/t1eYY3s.png"" alt=""enter image description here"" /></li>
<li>Now run this pipeline it will run first pipeline after successful completion of it will trigger web activity or azure function activity.</li>
</ul>
"
"75678595","Upsert from ADF to Salesforce writes a record when no changes","<p>I am creating a copy activity that pulls information from ADF and I want it to upsert into an object in Salesforce.  The activity is working, I saw the record get created, and it changes if I modify my query.  IE:  add a date, date is in Salesforce, remove it, date is removed.</p>
<p>However, when I check the activity in ADF I see a row was written when there are no changes.  Is this correct?  I apologize I am new to the ADF Upsert and want to verify it is running correctly.</p>
<p><a href=""https://i.stack.imgur.com/ra6ud.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ra6ud.png"" alt=""enter image description here"" /></a></p>
<p>I created a copy activity.  I am expecting it to read a row but not write one when there are no changes.  Unless I am mistaken as to how upsert works with Salesforce.</p>
","<azure><azure-data-factory><upsert>","2023-03-08 21:55:59","33","0","1","75678984","<p>So basically an upsert is a combination of update or else insert. When a row is pulled from azure sql into salesforce, update or insert operation does not matter for the activity summary as for the categorization, and either action will be recorded as &quot;written&quot;. So this is by design.</p>
<p>Here is a post in MSFT forums regarding a similar question, the responder suggests creating temporal tables to see whats happening in sink:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/961348/how-to-tell-if-insert-or-update-was-done-using-cop"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/961348/how-to-tell-if-insert-or-update-was-done-using-cop</a></p>
<p>Hope my answer helped, sorry for the confusion.</p>
"
"75676776","ADF - Pick up values from JSON array to use it as variable","<p>What I want to do?</p>
<p>Step one: I want to pick up all values of users (ID) from JSON file and put them together in a list.</p>
<p>Step two: Use ID's in a variable like <a href=""https://microsoft.com/API/threads_createdby=%22&amp;ID&amp;%22"" rel=""nofollow noreferrer"">https://microsoft.com/API/threads_createdby=&quot;&amp;ID&amp;&quot;</a> to retrieve all threads created by User1, User2, User3 and so on.</p>
<p>Now I'm importing JSON file with Lookup (Dataset) see below:</p>
<p><code>{ &quot;count&quot;: 1, &quot;value&quot;: [ { &quot;result&quot;: [ { &quot;user&quot;: { &quot;display_value&quot;: &quot;User1&quot;, &quot;link&quot;: &quot;https://microsoft.com&quot;, &quot;value&quot;: &quot;4356yrtge43t5gre&quot; } }, { &quot;user&quot;: { &quot;display_value&quot;: &quot;User2&quot;, &quot;link&quot;: &quot;https://microsoft.com&quot;, &quot;value&quot;: &quot;fdgtrh5t4rewfr54&quot; } }, { &quot;user&quot;: { &quot;display_value&quot;: &quot;User3&quot;, &quot;link&quot;: &quot;https://microsoft.com&quot;, &quot;value&quot;: &quot;rgethyj7u6y5terf&quot; } } ] } ], &quot;effectiveIntegrationRuntime&quot;: &quot;server343&quot;, &quot;billingReference&quot;: { &quot;activityType&quot;: &quot;PipelineActivity&quot;, &quot;billableDuration&quot;: [ { &quot;meterType&quot;: &quot;SelfhostedIR&quot;, &quot;duration&quot;: 0.016666666666666666, &quot;unit&quot;: &quot;Hours&quot; } ] }, &quot;durationInQueue&quot;: { &quot;integrationRuntimeQueue&quot;: 1 } }</code></p>
<p>Next step is Foreach with Append Variable (@activity('JSONImport').output.value &amp; @item().result and I can see all ID's together with this error message <code>of type 'Array' cannot be appended to the variable 'Value' of type 'Array'. The action type 'AppendToArrayVariable' only supports values of types 'Float, Integer, String, Boolean, Object'.</code></p>
<p>With @item().result[0].user.value I see this: <code>{ &quot;variableName&quot;: &quot;Value&quot;, &quot;value&quot;: &quot;4356yrtge43t5gre&quot; }</code></p>
<p>What I'am doing wrong?</p>
<p>Best regards</p>
","<azure-data-factory>","2023-03-08 18:16:18","235","1","1","75680264","<p>When I tried the same expression that you have given in Foreach and append I ended up with same error.</p>
<p><img src=""https://i.imgur.com/fuI5HBY.png"" alt=""enter image description here"" /></p>
<p>The above error occured because when you look at the <strong>lookup activity output array value is an array</strong> and <strong><code>result</code> is also an array</strong>.  The error states that appending array to an array is not supported. So, the error came because you tried to append an array(<code>result</code>) to another.</p>
<blockquote>
<p>Step two: Use ID's in a variable like <a href=""https://microsoft.com/API/threads_createdby=%22&amp;ID&amp;%22"" rel=""nofollow noreferrer"">https://microsoft.com/API/threads_createdby=&quot;&amp;ID&amp;&quot;</a> to retrieve all threads created by User1, User2, User3 and so on.</p>
</blockquote>
<p>The <code>result</code> array is at the <code>0th</code> index of the lookup output array. So, to achieve your requirement give the result array to Foreach and use append activity inside it.</p>
<p><code>@activity('Lookup1').output.value[0].result</code></p>
<p>In append, for sample I have given the below dynamic content(To domonstrate accessing <code>value</code> key).</p>
<p><code>https://microsoft.com/API/thread_createdby=@{item().user.value}</code></p>
<p><img src=""https://i.imgur.com/d0hSuI5.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/WFTaNii.png"" alt=""enter image description here"" /></p>
<p>If you want to store the result array in an array variable, you can directly use the same <code>@activity('Lookup1').output.value[0].result</code> in a set variable activity and use as per your requirement.</p>
"
"75674566","How to obtain the array element as string","<p>I have a JSON result in my copy job which like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;data&quot;: [{
            &quot;id&quot;: 1,
            &quot;name&quot;: &quot;Company 1&quot;
        },
        {
            &quot;id&quot;: 2,
            &quot;name&quot;: &quot;Company 2&quot;
        },
        {
            &quot;id&quot;: 3,
            &quot;name&quot;: &quot;Company 3&quot;
        }
    ]
}
</code></pre>
<p>I want to store the the result in a table with one row per company, and it works if I have a table containing all the fields, I have &quot;Collection reference&quot; set to <code>$['data']</code> and I can then map <code>['id']</code> to &quot;id&quot; and so forth.</p>
<p>But I'd like to store the whole:</p>
<pre class=""lang-json prettyprint-override""><code>{
            &quot;id&quot;: 1,
            &quot;name&quot;: &quot;Company 1&quot;
}
</code></pre>
<p>in a table field, because there are lots of custom fields I will not be able to map in ADF copy activity. Is there a way to reference the JSON array element? I'd be surprised if it's not possible, because I can already reference the fields within the element as <code>$['data']['id']</code> .</p>
<p>Any ideas?</p>
","<azure-data-factory>","2023-03-08 14:48:30","125","0","1","75684093","<ul>
<li>Use web activity to make a call to your REST API and iterate through the response returned (data property).</li>
<li>I have stored your response in a look up and iterated through <code>data</code> property for demonstration.</li>
<li>Inside for each, I have used an <code>append variable</code> activity with the following dynamic content:</li>
</ul>
<pre><code>'@{string(item())}'
</code></pre>
<p><img src=""https://i.imgur.com/zhw5wKb.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I have used a copy data activity to store the data in a csv file as additional column. I have taken a file as shown below as my source (header and a row):</li>
</ul>
<p><img src=""https://i.imgur.com/k3Gmo0w.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, add an additional column called <code>data</code> with value as shown below:</li>
</ul>
<pre><code>@join(variables('demo'),'
')
</code></pre>
<ul>
<li>Configure your sink (delimited text) as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/HzXQCou.png"" alt=""enter image description here"" /></p>
<ul>
<li>In mapping, keep only the column that we created above and delete the rest (delete <code>id</code> in the below).</li>
</ul>
<p><img src=""https://i.imgur.com/VWHySqK.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I write this file, I get the resulting csv file as shown in the below image (array element as string):</li>
</ul>
<p><img src=""https://i.imgur.com/tfjRfx6.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can read the above file with quote character as single quote and then configure this data to your sink accordingly. Each of the element would be written as string.</li>
</ul>
"
"75672552","In ADF We are using Source with REST API which has multiple Object ,how can i fetch filename of multiple object with Same dataset(Sink)","<p>ADF filename with Multiple Object with REST API</p>
<p>For Each object Created a different dataSet(Sink) with Filename</p>
","<azure-data-factory>","2023-03-08 11:38:44","62","0","1","75801746","<blockquote>
<p>In ADF We are using Source with REST API which has multiple Object ,how can i fetch filename of multiple object with Same dataset(Sink)</p>
</blockquote>
<ul>
<li>To achieve this first you have to fetch the rest Api objects from web activity(here I used blob storage Api).
<img src=""https://i.imgur.com/Hia1zKO.png"" alt=""enter image description here"" /></li>
<li>Then pass this value to set variable to convert it into an array format <code>@array(json(activity('Web1').output.Response))</code>.
<img src=""https://i.imgur.com/CzkdtUH.png"" alt=""enter image description here"" /></li>
<li>Then pass the variable to for each loop
<img src=""https://i.imgur.com/DK571xy.png"" alt=""enter image description here"" /></li>
<li>and assess filename from multiple objects in dataset parameter value.
<img src=""https://i.imgur.com/ubCrLPE.png"" alt=""enter image description here"" /></li>
</ul>
"
"75672134","How to retrieve objects out of Json in Azure Data Factory","<p>I have a Json file that when run by Data Factory only contains one nested object &quot;Properties&quot; and it is fine but Value4 looks like another nested object and is being visible as one long string,  altough it contains multiple columns, same thing with Text1, is this a proper json format ? I cannot figure it out how to flatten this the only value that is detected by ADF is Properties as an object, Value4 and Text1 is not recognized by ADF as an object or array, just one long string of values</p>
<p>Any idea how to fix this problem or the Json is really badly created?</p>
<p>{&quot;RoleInstance&quot;:&quot;HIDDEN&quot;,&quot;City&quot;:&quot;Boston&quot;,&quot;Country&quot;:&quot;UnitedStates&quot;,]&quot;ClientIP&quot;:&quot;0.0.0.0&quot;,&quot;ClientStateOrProvince&quot;:&quot;MA&quot;
<strong>&quot;Properties&quot;</strong>:{&quot;Value1&quot;:&quot;Test&quot;,&quot;Value2&quot;:&quot;12345&quot;,&quot;Value3&quot;:&quot;User&quot;,
<strong>&quot;Value4&quot;:&quot;</strong>{&quot;HostSpecType&quot;:&quot;User&quot;,&quot;ListSpecType&quot;:&quot;User&quot;,&quot;RoleID&quot;:&quot;156&quot;,&quot;OwnerIDs&quot;:[159],&quot;OwnerID&quot;:3,&quot;ColumnsToLoad&quot;:[&quot;UsrID&quot;,&quot;SpecID&quot;,&quot;SpecName&quot;,&quot;SpecTyp&quot;,&quot;SpecOpt&quot;,&quot;ParentSpecID&quot;,&quot;ParentSpecTyp&quot;,&quot;InternalVersion&quot;,&quot;Email&quot;,&quot;Office&quot;,&quot;DispName&quot;,&quot;FirstName&quot;,&quot;LastName&quot;,&quot;Phone&quot;,&quot;OfcID&quot;,&quot;UserPrin&quot;,&quot;EmplID&quot;]}&quot;,</p>
<p>&quot;UserID&quot;:&quot;498&quot;,&quot;MachineName&quot;:&quot;QG6T&quot;,
<strong>&quot;Text1&quot;</strong>:&quot;{&quot;ID&quot;:&quot;1596&quot;,&quot;Name&quot;:&quot;LaunchSchedule&quot;,&quot;Type&quot;:&quot;Initializa Mailbox Monitor&quot;,&quot;AppID&quot;:&quot;1002&quot;}&quot;,
&quot;TaskID&quot;:&quot;16415&quot;,
&quot;JobID&quot;:&quot;167&quot;},</p>
<p>&quot;ResourceGUID&quot;:&quot;c94e4&quot;,&quot;SDKVersion&quot;:&quot;dotnet:2-21496&quot;}</p>
","<arrays><json><azure-data-factory>","2023-03-08 10:58:00","105","0","1","75680882","<p>As per your Json, I tried to reproduce the same  in my environment I got the same string values.</p>
<pre><code>{  
&quot;RoleInstance&quot;:&quot;HIDDEN&quot;,  
&quot;City&quot;:&quot;Boston&quot;,  
&quot;Country&quot;:&quot;UnitedStates&quot;,  
&quot;ClientIP&quot;:&quot;0.0.0.0&quot;,  
&quot;ClientStateOrProvince&quot;:&quot;MA&quot;,  
&quot;Properties&quot;:  
{  
&quot;Value1&quot;:&quot;Test&quot;,  
&quot;Value2&quot;:&quot;12345&quot;,  
&quot;Value3&quot;:&quot;User&quot;,  
&quot;Value4&quot;:'{&quot;HostSpecType&quot;:&quot;User&quot;,&quot;ListSpecType&quot;:&quot;User&quot;,&quot;RoleID&quot;:&quot;156&quot;,&quot;OwnerIDs&quot;:[159],&quot;OwnerID&quot;:3,&quot;ColumnsToLoad&quot;:[&quot;UsrID&quot;,&quot;SpecID&quot;,&quot;SpecName&quot;,&quot;SpecTyp&quot;,&quot;SpecOpt&quot;,&quot;ParentSpecID&quot;,&quot;ParentSpecTyp&quot;,&quot;InternalVersion&quot;,&quot;Email&quot;,&quot;Office&quot;,&quot;DispName&quot;,&quot;FirstName&quot;,&quot;LastName&quot;,&quot;Phone&quot;,&quot;OfcID&quot;,&quot;UserPrin&quot;,&quot;EmplID&quot;]}',  
&quot;UserID&quot;:&quot;498&quot;,  
&quot;MachineName&quot;:&quot;QG6T&quot;,  
&quot;Text1&quot;:'{&quot;ID&quot;:&quot;1596&quot;,&quot;Name&quot;:&quot;LaunchSchedule&quot;,&quot;Type&quot;:&quot;Initializa Mailbox Monitor&quot;,&quot;AppID&quot;:&quot;1002&quot;}',  
&quot;TaskID&quot;:&quot;16415&quot;,  
&quot;JobID&quot;:&quot;167&quot;  
},  
&quot;ResourceGUID&quot;:&quot;c94e4&quot;,  
&quot;SDKVersion&quot;:&quot;dotnet:2-21496&quot;  
}
</code></pre>
<p><img src=""https://i.imgur.com/3WkGR6c.png"" alt=""enter image description here"" /></p>
<p>If you modified the Json like this, It will fix your problem:</p>
<pre><code>{
    &quot;RoleInstance&quot;:&quot;HIDDEN&quot;,
    &quot;City&quot;:&quot;Boston&quot;,
    &quot;Country&quot;:&quot;UnitedStates&quot;,
    &quot;ClientIP&quot;:&quot;0.0.0.0&quot;,
    &quot;ClientStateOrProvince&quot;:&quot;MA&quot;,
    &quot;Properties&quot;:
    {
        &quot;Value1&quot;:&quot;Test&quot;,
        &quot;Value2&quot;:&quot;12345&quot;,
        &quot;Value3&quot;:&quot;User&quot;,
        &quot;Value4&quot;:
        {
            &quot;HostSpecType&quot;:&quot;User&quot;,
            &quot;ListSpecType&quot;:&quot;User&quot;,
            &quot;RoleID&quot;:&quot;156&quot;,
            &quot;OwnerIDs&quot;:[159,144],
            &quot;OwnerID&quot;:3,
            &quot;ColumnsToLoad&quot;:
            [
                &quot;UsrID&quot;,&quot;SpecID&quot;,&quot;SpecName&quot;,&quot;SpecTyp&quot;,&quot;SpecOpt&quot;,&quot;ParentSpecID&quot;,&quot;ParentSpecTyp&quot;,&quot;InternalVersion&quot;,&quot;Email&quot;,&quot;Office&quot;,&quot;DispName&quot;,&quot;FirstName&quot;,&quot;LastName&quot;,&quot;Phone&quot;,&quot;OfcID&quot;,&quot;UserPrin&quot;,&quot;EmplID&quot;
            ]
        },
        &quot;UserID&quot;:&quot;498&quot;,
        &quot;MachineName&quot;:&quot;QG6T&quot;,
        &quot;Text1&quot;:
        {
            &quot;ID&quot;:&quot;1596&quot;,
            &quot;Name&quot;:&quot;LaunchSchedule&quot;,
            &quot;Type&quot;:&quot;Initializa Mailbox Monitor&quot;,
            &quot;AppID&quot;:&quot;1002&quot;
        }, 
        &quot;TaskID&quot;:&quot;16415&quot;,
        &quot;JobID&quot;:&quot;167&quot;
    },
        &quot;ResourceGUID&quot;:&quot;c94e4&quot;,
        &quot;SDKVersion&quot;:&quot;dotnet:2-21496&quot;
}
</code></pre>
<p><img src=""https://i.imgur.com/JnftoX3.png"" alt=""enter image description here"" /></p>
"
"75669485","Azure Data Factory only returns the first 100 pipeline runs","<p>I have an Azure Data Factory instance and I am trying to read all pipeline runs from the last day or so. It's working, but I can't seem to get more than 100 results back from the api. I believe from what I've googled that I am reading the <code>AsyncPageable</code> correctly, but it seems like I am only getting one page back from the api. Here's the code:</p>
<pre><code>ResourceIdentifier dataFactoryResourceId = DataFactoryResource.CreateResourceIdentifier(subscriptionId, resourceGroupName, factoryName);
DataFactoryResource dataFactory = _armClient.GetDataFactoryResource(dataFactoryResourceId);     

RunFilterContent content = new RunFilterContent(DateTimeOffset.UtcNow.AddDays(-1).AddHours(-1), DateTimeOffset.MaxValue);
content.OrderBy.Add(new RunQueryOrderBy(RunQueryOrderByField.RunStart, RunQueryOrder.Desc));

List&lt;FactoryPipelineRunInfo&gt; runs = new List&lt;FactoryPipelineRunInfo&gt;();
await foreach (FactoryPipelineRunInfo item in dataFactory.GetPipelineRunsAsync(content))
{
    runs.Add(item);
}
</code></pre>
","<azure-data-factory><azure-resource-manager>","2023-03-08 04:37:14","76","0","1","75687743","<p>I found a workaround, so sort of an answer. I guess the SDK is still prerelease(1.0.0-beta2) at the time I'm writing this, so it's probably something that will be addressed in the future.</p>
<p>I ended up using an <code>HttpClient</code> and making the calls directly to the API. Each response contains a continuation token that when passed back in subsequent calls gets the next page of data, until you get to the last page and the continuation token comes back null. I'm guessing the SDK doesn't handle the token correctly.</p>
<p>In case anyone is interested, here's the code I wrote to do that:</p>
<pre><code> string subscriptionId = _config.GetValue&lt;string&gt;(&quot;Azure:SubscriptionId&quot;);
 string resourceGroupName = _config.GetValue&lt;string&gt;(&quot;Azure:DataFactoryResourceGroupName&quot;);
 string factoryName = _config.GetValue&lt;string&gt;(&quot;Azure:DataFactoryName&quot;);
 var http = _httpFactory.CreateClient(&quot;AzureManagement&quot;);
 string url = $&quot;https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/queryPipelineRuns?api-version=2018-06-01&quot;;
 JsonObject body = new JsonObject
 {
   { &quot;lastUpdatedAfter&quot;, DateTimeOffset.UtcNow.AddDays(-1).AddHours(-1).ToString() },
   { &quot;lastUpdatedBefore&quot;, DateTimeOffset.MaxValue }
 };
 PipelineRunResponseDTO? runInfoResponse = null;
 List&lt;PipelineRunDTO&gt; runs = new List&lt;PipelineRunDTO&gt;();
 do
 {
   if (!string.IsNullOrWhiteSpace(runInfoResponse?.ContinuationToken))
   {
     body[&quot;continuationToken&quot;] = runInfoResponse.ContinuationToken;
   }
   var postBody = new StringContent(body.ToString(), Encoding.UTF8, &quot;application/json&quot;);
   var response = await http.PostAsync(url, postBody);
   if (response.IsSuccessStatusCode)
   {
     string responseStr = await response.Content.ReadAsStringAsync();
     runInfoResponse = JsonSerializer.Deserialize&lt;PipelineRunResponseDTO&gt;(responseStr);
     if (runInfoResponse != null)
     {
       runs.AddRange(runInfoResponse.Value);
     }
   }
 } while (!string.IsNullOrWhiteSpace(runInfoResponse?.ContinuationToken));
 return runs;
</code></pre>
<p><code>PipelineRunResponseDTO</code> is a class I wrote that has the same properties as <code>FactoryPipelineRunInfo</code>, but I couldn't use <code>FactoryPipelineRunInfo</code> because all it's constructors are marked as internal.</p>
"
"75664293","Trigger Azure Data Factory Pipeline using Postman","<p>I'm kinda lost, I'm trying to trigger a pipeline manually using the following:</p>
<p>POST <a href=""https://management.azure.com/subscriptions/%7BsubscriptionId%7D/resourceGroups/%7BresourceGroupName%7D/providers/Microsoft.DataFactory/factories/%7BfactoryName%7D/pipelines/%7BpipelineName%7D/createRun?api-version=2018-06-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelines/{pipelineName}/createRun?api-version=2018-06-01</a></p>
<p>I use to generate an access token using the following:
<a href=""https://i.stack.imgur.com/H4Cow.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H4Cow.png"" alt=""enter image description here"" /></a></p>
<p>But I encountered the following error:</p>
<p><a href=""https://i.stack.imgur.com/LTrkT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LTrkT.png"" alt=""enter image description here"" /></a></p>
<p>I'm not sure if it's related but I am assigned as a Data Factory Contributor Role, I can make pipelines but I could not publish them. But token does not says it is account related so.</p>
<p>I'm not sure if my client id and client secret are correct, it is the same as the one used for the database same subscription but different resource group.</p>
<p>How can I obtain the correct access token or is this even possible?</p>
","<azure><postman><azure-data-factory><azure-rbac>","2023-03-07 15:56:54","165","0","1","75669336","<p><em><strong>I tried to reproduce the same in my environment and got below results:</strong></em></p>
<p>I registered one Azure AD application and added <code>API permission</code> like this:</p>
<p><img src=""https://i.imgur.com/4Vur6OA.png"" alt=""enter image description here"" /></p>
<p>Now, I generated <strong>access token</strong> via Postman same as you with below parameters:</p>
<pre class=""lang-http prettyprint-override""><code>GET https://login.microsoftonline.com/&lt;tenantID&gt;/oauth2/token

grant_type:client_credentials
client_id: &lt;appID&gt;
client_secret: &lt;secret&gt;
resource: https://management.azure.com/
</code></pre>
<p><strong>Response:</strong></p>
<p><img src=""https://i.imgur.com/7irqhR1.png"" alt=""enter image description here"" /></p>
<p>When I ran the same query to trigger Azure Data Factory pipeline by including above access token via Postman, I got <strong>same error</strong> as below:</p>
<pre><code>POST https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelines/{pipelineName}/createRun?api-version=2018-06-01
</code></pre>
<p><strong>Response:</strong></p>
<p><img src=""https://i.imgur.com/JbyGzDw.png"" alt=""enter image description here"" /></p>
<p>To <strong>resolve</strong> the error, you need to assign <code>Data Factory Contributor</code> Role to service principal/application with client ID in error response, before generating access token.</p>
<p><em>Go to Azure Portal -&gt; Data factories -&gt; Your Data Factory -&gt; Access control (IAM) -&gt; Add role assignment</em></p>
<p><img src=""https://i.imgur.com/BDblIPn.png"" alt=""enter image description here"" /></p>
<p>After assigning role to service principal, generate access token again and use it to run query.</p>
<p>When I ran query with new access token, I got <strong>response</strong> successfully like below:</p>
<pre><code>POST https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelines/{pipelineName}/createRun?api-version=2018-06-01
</code></pre>
<p><strong>Response:</strong></p>
<p><img src=""https://i.imgur.com/s8YtORJ.png"" alt=""enter image description here"" /></p>
<p>In your case, make sure to assign <code>Data Factory Contributor</code> role to <strong>service principal</strong>(Azure AD application) to resolve the error.</p>
"
"75662391","Import CSV file into multiple tables in D365","<p>I'm searching for a no-code solution to import data from a single CSV file into multiple tables in D365, in a single data import process.
OOB Import Data in D365 can only import data into a single table, not in multiple table, in one shot.
Do you think Azure Data Factory can be used for this task?</p>
<p>What other alternatives can I take, please?</p>
<p>Thanks in advance for your time and knowledge!</p>
","<dynamics-crm><azure-data-factory><dynamics-365><import-csv>","2023-03-07 13:08:17","104","0","2","75680255","<p>You can use ADF pipelines (Dataflow) with multiple sources and load data into Dynamics. <a href=""https://medium.com/capgemini-microsoft-team/using-azure-data-factory-v2-to-load-microsoft-dynamics-365-crm-from-text-file-771ce7df2dd6"" rel=""nofollow noreferrer"">Read more</a> and <a href=""https://stackoverflow.com/questions/74761195/how-azure-data-factory-dataflow-can-create-crm-related-entities-records-in-sing"">refer this</a></p>
<p>Alternatives are <a href=""https://learn.microsoft.com/en-us/power-platform/admin/manage-configuration-data?WT.mc_id=DX-MVP-5004422"" rel=""nofollow noreferrer"">configuration migration utility</a> and <a href=""https://support.cobalt.net/hc/en-us/articles/4410824948763-Import-Data-Into-CRM#importing-multiple-records"" rel=""nofollow noreferrer"">Data import using zip file</a></p>
<p>All these are not full fledged ETL tools, so hiccups are possible, even roadblocks considering your no code requirement.</p>
<p>You can try SSIS+Kingswaysoft or Scribe as an option considering <a href=""https://learn.microsoft.com/en-us/power-apps/maker/data-platform/data-platform-import-export?WT.mc_id=DX-MVP-5004422"" rel=""nofollow noreferrer"">Import wizard</a> is not getting much attention in the Dataverse product.</p>
"
"75662391","Import CSV file into multiple tables in D365","<p>I'm searching for a no-code solution to import data from a single CSV file into multiple tables in D365, in a single data import process.
OOB Import Data in D365 can only import data into a single table, not in multiple table, in one shot.
Do you think Azure Data Factory can be used for this task?</p>
<p>What other alternatives can I take, please?</p>
<p>Thanks in advance for your time and knowledge!</p>
","<dynamics-crm><azure-data-factory><dynamics-365><import-csv>","2023-03-07 13:08:17","104","0","2","75954154","<p>XrmToolbox Field Creator tool is a life savior. It's free and does the job.</p>
"
"75661494","How to authenticate to Azure Active Directory Protected API from a scheduled Azure Data Factory pipeline","<p>We are using Microsoft Identity Platform for our Authentication.
We have registered our .NET API (e.g. name of the api = example-api) in Azure Active Directory App registrations.</p>
<p>We are planning to use Azure Data Factory for some data imports/exports.</p>
<p>In this process we need to call this API.</p>
<p><strong>Problem</strong><br />
How to call the API from Azure Data Factory pipeline without a user i.e., a scheduled job</p>
<p>Microsoft suggested to use System assigned Managed Identity or User Assigned Managed Identity.</p>
<p>How can I give permissions to the System assigned Managed Identity or User Assigned Managed Identity to access <strong>example-api</strong> which is registered in <strong>AAD app registrations</strong>.</p>
<p>I couldn't find much documentation related apps registered in AAD with managed identity, there is some doc for Users/Groups.</p>
<p>Could someone please help me to solve this issue.</p>
<p>Thank you,<br />
Vamshi</p>
","<azure-active-directory><azure-data-factory>","2023-03-07 11:38:12","79","0","1","75699449","<p>So first thing to be very careful about, and some people run into this problem is the line of sight between tenants and accounts, make sure your registered app or sharepoint or whatever is in the same tenant as the adf. Otherwise adf wont be able to see or authenticate. There might be a way to connect multiple tenants but AAD guys know this better. In your scenario, UAMI is the choice because you can associate multiple resources into UAMI as here:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-view-associated-resources-for-an-identity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-view-associated-resources-for-an-identity</a></p>
<p>While SAMI is easier to use, they are tied to the resources individually. See here:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azp"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azp</a></p>
<p>First link says SAMI cannot be shared across resources. Under 2nd link you can also see how to assign roles and manage access to UAMI.</p>
"
"75657491","For each to extract files from different subfolders","<p>I need to load files from different subfolders in Azure Blob Storage to Azure SQL using For each activity but it does not working. Is There a way to read from different subfolders?</p>
<p><a href=""https://i.stack.imgur.com/CWYA2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CWYA2.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Kb5Qx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kb5Qx.png"" alt=""enter image description here"" /></a></p>
","<azure-blob-storage><azure-data-factory>","2023-03-07 02:15:01","72","0","1","75658226","<ul>
<li>You can use <code>Get metadata</code> activity to get the names of all the folders and then iterate through these folder names (build path inside for each), considering all the sub folders are at same hierarchy level.</li>
<li>The following is the output of my <code>Get metadata</code> activity (child items of the folder containing sub-directories):</li>
</ul>
<p><img src=""https://i.imgur.com/Qwpn3HI.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now inside for each loop, you can use the current item value in your path and copy the data as required. I have used set variable activity for demonstration using the dynamic content <code>data/ip/@{item().name}</code>. @item().name would contain the name of sub directory for each iteration.</li>
</ul>
<p><img src=""https://i.imgur.com/wy1sRBL.png"" alt=""enter image description here"" /></p>
<ul>
<li>Here, <code>data</code> is container name, <code>ip</code> is parent directory and <code>01</code> is sub-directory name.</li>
</ul>
"
"75656924","Delimiter issue - Copying Data from Azure SQL to Snowflake","<p>I am trying to load data from Azure SQL to Snowflake. I am under the constraint to use the copy activity only.
I am using a lookup table to reference a metadata table with the source schema, source tables, target schema, and target tables.
I am then using a For Each Activity to iterate through the metadata table.
I’m using two copy activities inside of that - one to move the data from SQL to Blob and the other to move the data from Blob to Snowflake. I am getting an error « « Found character C instead of delimiter ‘,’ «. I have tried using different delimiters but my data consists of those characters.
Is there a way to resolve this, in a way to use a specific file format on the snowflake end before the copy activity takes place( cannot write a script as the storage account is in West US2 and Snowflake account is in East US2).</p>
<p>Any help is greatly appreciated!!!!</p>
<p>Thank you.</p>
<p>Tried using different delimiters.</p>
","<snowflake-cloud-data-platform><azure-data-factory><etl>","2023-03-07 00:06:40","81","0","1","75742766","<p>Copy data to/from Snowflake takes advantage of Snowflake's COPY into command.</p>
<p>If you check in the the Copy Activity (Blob to Snowflake)</p>
<p>In Sink Tab: You would find :</p>
<ul>
<li>Additional Snowflake Copy Options</li>
<li>Additional Snowflake format options</li>
</ul>
<p><a href=""https://i.stack.imgur.com/g03tu.png"" rel=""nofollow noreferrer"">addtional options in ADF</a></p>
<p>You could click on the + Symbol to add these options.</p>
<p>Additional copy options, provided as a dictionary of key-value pairs. Examples: ON_ERROR, FORCE, LOAD_UNCERTAIN_FILES.</p>
<p>Additional file format options provided to the COPY command, provided as a dictionary of key-value pairs. Examples: DATE_FORMAT, TIME_FORMAT, TIMESTAMP_FORMAT.</p>
<p>For more information, check below links.</p>
<p><a href=""https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#copy-options-copyoptions"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#copy-options-copyoptions</a>
<a href=""https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#format-type-options-formattypeoptions"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#format-type-options-formattypeoptions</a></p>
<p>In your case you could add below in your Additional Snowflake format options:</p>
<p>FIELD_DELIMITER = ''</p>
<p>Also check out this option</p>
<p>FIELD_OPTIONALLY_ENCLOSED_BY = 'character'  ,if needed for your use-case.</p>
<p>For detailed documentation of Snowflake Connector in ADF, refer to this :
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-snowflake?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-snowflake?tabs=data-factory</a></p>
"
"75654563","ADF connectivity with SAP HANA - SSL","<p>I'm working on a project to get data from SAP HANA tables using Azure Data Factory, and I don't know if SSL certificates are supported by the SAP HANA connector, I would like to know if SAP HANA supports SSL certificates.
Note: according to the Azure documentation there's no info on that.</p>
<p>This is my reference info: <a href=""https://raw.githubusercontent.com/Azure/Azure-DataFactory/main/whitepaper/SAP%20Data%20Integration%20using%20Azure%20Data%20Factory.pdf"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/Azure/Azure-DataFactory/main/whitepaper/SAP%20Data%20Integration%20using%20Azure%20Data%20Factory.pdf</a></p>
<p>but from 2020</p>
","<ssl><azure-data-factory><hana>","2023-03-06 18:32:25","76","0","1","75693288","<p>SAP HANA supports network encryption with SSL.
Details vary if you connect to HANA Cloud (managed service) or HANA running in a vm or an appliance.
In the first case, SSL is mandatory, in the second, it is optional <a href=""https://help.sap.com/docs/SAP_HANA_PLATFORM/b3ee5778bc2e4a089d3299b82ec762a7/dbd3d887bb571014bf05ca887f897b99.html"" rel=""nofollow noreferrer"">unless configured otherwise</a>.</p>
<p>The ADF SAP HANA Connector relies on the HANA ODBC driver. SSL and other connection settings are set in the ODBC data source.</p>
<p>FYI, if you intend to connect at the database level, you should make  sure there is a valid license to do so, even if it's technically possible.</p>
"
"75650275","Based on Boolean value, setup a conditional activity - without using the IF ACTIVITY","<p>I am trying to setup my pipeline such that it uses a condition based on the boolean value and if it is true to continue in a direction of the pipeline, if it is false to continue in another direction.</p>
<p>if it true I want it to continue the top of the pipeline... Set Folder Path etc and if is false to just end the pipeline with the failed slack notification. I want to achieve this without using the IF condition activity as it will mean I will have to integrate the whole continuation of the pipeline within the condition so if this can be achieved in another means, that would be perfect.</p>
<p><a href=""https://i.stack.imgur.com/Okkfn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Okkfn.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/8usf8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8usf8.png"" alt=""enter image description here"" /></a></p>
","<conditional-statements><boolean><azure-data-factory>","2023-03-06 11:25:51","46","0","1","75650652","<blockquote>
<p>I want to achieve this without using the IF condition activity.</p>
</blockquote>
<p>First, I took a simple set variable to get Boolean response.</p>
<p><img src=""https://i.imgur.com/E0pC8hh.png"" alt=""enter image description here"" /></p>
<p>To achieve this scenario, you have to take one set variable activity with <strong>demo</strong> variable with <code>string</code> type and <code>@if(variables('bool'),'true',0)</code> so if value of variable is <strong>true</strong> then it will assign <strong>string value true to variable</strong> and if the value of variable is false it will assign <code>0</code>  integer value to it and activity will fail.</p>
<p><img src=""https://i.imgur.com/OrIqqpf.png"" alt=""enter image description here"" /></p>
<p>On Succeed you can call activities to run which on <strong>true</strong> response.</p>
<p><img src=""https://i.imgur.com/1nHAuVX.png"" alt=""enter image description here"" /></p>
<p>On Failure you can call activities to run which on <strong>false</strong> response.</p>
<p><img src=""https://i.imgur.com/EP4uEo9.png"" alt=""enter image description here"" /></p>
"
"75649885","ADF: running an R script on Azure Data Factory","<p>Do you know a way to run an R script on adf and schedule an automatic email with blob attachments? thanks!</p>
<p>I tried using pools it didn't work
I've tried this solution from 2018 but didn't work with the current version: <a href=""https://stackoverflow.com/questions/52160348/how-to-execute-r-script-along-with-azure-data-factory"">solution</a></p>
","<r><azure><azure-data-factory>","2023-03-06 10:49:16","79","0","1","75661221","<p>You can use Databricks R Notebook here. Call the Notebook activity from ADF. If you have R code to send the email, then you can directly call it from ADF and send the mail.</p>
<p>If you don't have any R code to send the mail, use ADF and logic app to send the mail as a workaround.</p>
<p>Sample Notebook R code which returns JSON for the attachment.</p>
<pre><code>query &lt;- '{&quot;start_relative&quot;: {
    &quot;value&quot;: &quot;4&quot;,
    &quot;unit&quot;: &quot;years&quot;
  },
  &quot;metrics&quot;: [
    {
      &quot;name&quot;: &quot;test&quot;,
      &quot;limit&quot;: 10000
    }
  ]}'
dbutils.notebook.exit(query)
</code></pre>
<p>Call this Notebook from ADF and it will execute like below.</p>
<p><img src=""https://i.imgur.com/5MlTGiw.png"" alt=""enter image description here"" /></p>
<p>Then use a web activity to call the logic app. Create a logic app with HTTP request and give the HTTP POST url to web activity. Give the below dynamic content as body.</p>
<p><code>@activity('Notebook1').output.runOutput</code></p>
<p><img src=""https://i.imgur.com/yYdpjFK.png"" alt=""enter image description here"" /></p>
<p><strong>Logic app:</strong></p>
<p><img src=""https://i.imgur.com/R26WMmw.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/HpKjDFK.png"" alt=""enter image description here"" /></p>
"
"75645120","Pass parameters to HTTP linked service in Azure Data Factory","<p>my query is realted to HTTP linked service (LS) in Azure Data Factory. I have created a HTTP LS and have defined three int type parameters; date, month, and day.
I have using 'add dynamic content' to fetch current day -1 day,month, date and trying to pass these to the HTTP link.
Dynamic content formula for year: <code>@int(substring(getPastTime(1,'Day'),0,4))</code>
Dynamic content formula for month: <code>@int(substring(getPastTime(1,'Day'),5,2))</code>
Dynamic content formula for day: <code>@int(substring(getPastTime(1,'Day'),8,2))</code></p>
<p>However, this is dynamic content is not working but if I manually pass on year, month and day, it seems to be working fine.</p>
<p>Please can someone idenity the problem with my dynamic content formula.</p>
","<azure><azure-data-factory><pipeline>","2023-03-05 20:24:23","93","0","1","75647137","<ul>
<li>You cannot directly use dynamic content to assign values to parameters within the linked service. The static values would only work.</li>
<li>In order to use dynamic content, you need to pass the dynamic content from dataset to linked service or from pipeline activity to dataset and then to linked service.</li>
<li>The following is a demonstration where I have created 3 parameters similar to yours:</li>
</ul>
<p><img src=""https://i.imgur.com/sQT5BIb.png"" alt=""enter image description here"" /></p>
<ul>
<li>The URL I have used is as shown below (Using this would throw an error but the error would indicate that I will be successfully able to use these values):</li>
</ul>
<pre><code>https://jsonplaceholder.typicode.com/posts/@{linkedService().year}/@{linkedService().month}/@{linkedService().day}
</code></pre>
<ul>
<li>I have passed the following values from dataset as shown below:</li>
</ul>
<pre><code>year : @substring(getPastTime(1,'Day'),0,4)
month : @substring(getPastTime(1,'Day'),5,2)
day : @substring(getPastTime(1,'Day'),8,2)
</code></pre>
<ul>
<li>When I use this dataset as source in copy data activity, I get an error since URL is not valid. But you can see the URL is built as per dynamic content.</li>
</ul>
<p><img src=""https://i.imgur.com/w24ZlUb.png"" alt=""enter image description here"" /></p>
"
"75633734","Is it possible to create date-specific triggers in Azure Synapse Analytics?","<p>Experts.
I want to create date-specific trigger in Azure Synapse Analytics.
But I don't know if I can make that.
The trigger requirements I would like to make are as follows.</p>
<p>Execution Date(Do not Execute on January,April,July and October.)
Feb 4/Mar 4/May 6/Jun 3/Aug 5/Sep 2/Nov 4/Dec 2</p>
<p>Execution Time
・10 am on all execution dates</p>
<p>Any answer would be helped.
Thank you.</p>
","<azure><azure-data-factory><azure-synapse>","2023-03-04 04:58:48","66","0","1","75638540","<p>Not sure about this but, in schedule trigger excluding certain months is not supported as of now. You can of course try having one schedule trigger for each of these, which would be the standard approach</p>
<p>One thing to try however is maybe you could exclude the months in the json in the schedule property see if it works (I dont think its going to work). Anyways, I think just having separate triggers for each of this would be enough, and its unnecessary to push for all-in-one schedule trigger approach (you dont lose anything from having separate triggers for these dates) So first thing you should ask yourself is what is the use case for having all the dates in one trigger while you can easily set up a separate one for each of these?</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory#schedule-property"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory#schedule-property</a></p>
<p>Also see this:</p>
<p><a href=""https://www.mutazag.com/blog/code/tutorial/ADF-custom-paramaters/#changing-trigger-settings"" rel=""nofollow noreferrer"">https://www.mutazag.com/blog/code/tutorial/ADF-custom-paramaters/#changing-trigger-settings</a></p>
"
"75629543","Copy response body which has content type as octet-stream using ADF","<p>I am working with an API that sends the response as octet-stream. I am calling the API using Web activity in ADF. Now this response has to be saved in the ADLS file, could be either in csv or json format. Is there any possible way to do this.</p>
<p>I am not at all familiar with this octet-stream types, so any help regarding this would be appreciated.</p>
","<azure><azure-data-factory><octet-stream>","2023-03-03 16:22:41","77","0","1","75651531","<p>I solved this myself, I figured out that the response was octet-stream because it was sending a zip file. So I saved the response using copy activity with HTTP linked service to ADLS as .zip output file and later used a copy activity again to extract that zip file into the ADLS store again using the compression type as zipdeflate, like how its usually done in ADF to extract.</p>
<p><a href=""https://i.stack.imgur.com/Aqf7J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aqf7J.png"" alt=""Source Dataset Settings"" /></a></p>
"
"75624562","Azure DataFactory Merge Filter Array output to inside ForEach Loop LookUp output values","<p>I am trying to merge the filter activity output which is outside of the ForEach Loop generated result. based on that, I have using item() value to filter again lookup activity output from azure database table. finally i have two kind of out with my hand. one is outside ForEach Loop acivity we have Filter activity provided one JSON array output. similarly we have inside forEach Loop activity to call lookup generated JSON array output as below like.</p>
<p>Filter Activity output:</p>
<pre><code>{
    &quot;ItemsCount&quot;: 9,
    &quot;FilteredItemsCount&quot;: 7,
    &quot;Value&quot;: [
        {
            &quot;no_day&quot;: 60,
            &quot;server&quot;: &quot;naserxxxxx&quot;,
            &quot;database&quot;: &quot;db12&quot;,
            &quot;table&quot;: &quot;tablename12&quot;,
            &quot;asset_id&quot;: &quot;1cdsdsadsadsadasdsad&quot;,
            &quot;indicator&quot;: [
                &quot;value&quot;
            ],
            &quot;sub_indicator&quot;: [
                &quot;va&quot;
            ]
        },
        {
            &quot;no_day&quot;: 999,
            &quot;server&quot;: &quot;sssss&quot;,
            &quot;database&quot;: &quot;db11&quot;,
            &quot;table&quot;: &quot;Entity&quot;,
            &quot;asset_id&quot;: &quot;b3ddd-b56b-4756-b5a5-ffffff&quot;,
            &quot;indicator&quot;: [
                &quot;val&quot;
            ],
            &quot;sub_indicator&quot;: [
                &quot;ef_dt&quot;
            ]
        },
        {
            &quot;no_day&quot;: 30,
            &quot;server&quot;: &quot;sssssss&quot;,
            &quot;database&quot;: &quot;db15&quot;,
            &quot;table&quot;: &quot;tablename11&quot;,
            &quot;asset_id&quot;: &quot;xxx-yyyyy-ddddddddd&quot;,
            &quot;indicator&quot;: [
                &quot;val&quot;
            ],
            &quot;sub_indicator&quot;: [
                &quot;p_dt&quot;
            ]
        },
        {
            &quot;no_day&quot;: 75,
            &quot;server&quot;: &quot;servername6&quot;,
            &quot;database&quot;: &quot;db10&quot;,
            &quot;table&quot;: &quot;tablenamet10&quot;,
            &quot;asset_id&quot;: &quot;d8xxx-88d3-4a38-rrr61-xxxxcec&quot;,
            &quot;indicator&quot;: [
                &quot;val4&quot;,
                &quot;val3&quot;
            ],
            &quot;sub_indicator&quot;: [
                &quot;time&quot;,
                &quot;dt&quot;
            ]
        },
        {
            &quot;no_day&quot;: 50,
            &quot;server&quot;: &quot;server4&quot;,
            &quot;database&quot;: &quot;db4&quot;,
            &quot;table&quot;: &quot;tablename4&quot;,
            &quot;asset_id&quot;: &quot;ca31cxxxxxx-25099dddddedf&quot;,
            &quot;indicator&quot;: [
                &quot;valueadsad&quot;
            ],
            &quot;sub_indicator&quot;: [
                &quot;val2&quot;,
                &quot;val3&quot;
            ]
        },
        {
            &quot;no_day&quot;: 100,
            &quot;server&quot;: &quot;serverName2&quot;,
            &quot;database&quot;: &quot;db2&quot;,
            &quot;table&quot;: &quot;tablename2&quot;,
            &quot;asset_id&quot;: &quot;dsdssdsdsd-7a9xxxxxx&quot;,
            &quot;indicator&quot;: [
                &quot;val1&quot;,
                &quot;val2&quot;
            ],
            &quot;sub_indicator&quot;: [
                &quot;tx_date&quot;
            ]
        },
        {
            &quot;no_day&quot;: 60,
            &quot;server&quot;: &quot;serverName&quot;,
            &quot;database&quot;: &quot;dbname&quot;,
            &quot;table&quot;: &quot;tablename&quot;,
            &quot;asset_id&quot;: &quot;32232323232323323232&quot;,
            &quot;indicator&quot;: [
                &quot;key&quot;
            ],
            &quot;sub_indicator&quot;: [
                &quot;datedt&quot;
            ]
        }
    ]
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/WFHM1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WFHM1.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/BnIV3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BnIV3.png"" alt=""enter image description here"" /></a></p>
<p>LookUp acivity each iteration will generate output as below like:</p>
<pre><code>{
    &quot;count&quot;: 1,
    &quot;value&quot;: [
        {
            &quot;id&quot;: 1,
            &quot;name&quot;: &quot;tablename&quot;,
            &quot;path&quot;: &quot;folder/test/&lt;Timestamp&gt;&quot;
        }
    ],
    &quot;effectiveIntegrationRuntime&quot;: &quot;AutoResolveIntegrationRuntime&quot;,
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzurexxIR&quot;,
                &quot;duration&quot;: sdsds,
                &quot;unit&quot;: &quot;xxx&quot;
            }
        ]
    },
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    }
} 
</code></pre>
<p><a href=""https://i.stack.imgur.com/kvn1u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kvn1u.png"" alt=""enter image description here"" /></a></p>
<p>I would like to merge the Filter activity output and Lookup activity output in single manager to pass one of the notebook activity. please help us</p>
","<azure-data-factory>","2023-03-03 08:04:58","75","0","1","75627928","<p>To get the desired result, follow the below approach.</p>
<p>Here, I have taken your filter array of JSON in a blob file and used a lookup activity. I have given it to ForEach activity. In your case, give the Filter array to ForEach.</p>
<p>Inside ForEach, for this demo I have used the <strong>same lookup activity(with same single record output) in every iteration</strong>. In your case change the lookup query as per your requirement with <code>@item()</code> in every iteration.</p>
<p>These are my variables for the pipeline. <code>res</code> to store the result and <code>res_show</code> is to show the result array(optional).</p>
<p><img src=""https://i.imgur.com/f18486O.png"" alt=""enter image description here"" /></p>
<p>Inside ForEach, take an append variable activity and give the below expression to it.</p>
<pre><code>@json(concat('{',substring(string(item()),1,add(length(string(item())),-2)),',',substring(string(activity('My lookup').output.value[0]),1,add(length(string(activity('My lookup').output.value[0])),-2)),'}'))
</code></pre>
<p><img src=""https://i.imgur.com/WsHSPF5.png"" alt=""enter image description here"" /></p>
<p>After Foreach, I have stored the <code>res</code> array into <code>res_show</code> and this is the required array of JSON.</p>
<p><img src=""https://i.imgur.com/MiTa95O.png"" alt=""enter image description here"" /></p>
<p>You can use this array of JSON as per your requirement.</p>
"
"75621518","Azure Data Flow source from output in pipeline","<p>Is it possible to create a Data Flow that gets data from the output of an activity in a Pipeline?</p>
<p>Something like this:
<a href=""https://i.stack.imgur.com/L0oAg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L0oAg.png"" alt=""enter image description here"" /></a></p>
<p>I want to create a pipeline that has an Azure function activity to process some data and then pass that processed data to a Data flow.</p>
<p>But apparently the Data flow only accepts a Dataset as a source.</p>
","<azure><azure-data-factory>","2023-03-02 22:42:36","153","0","2","75623218","<p>I dont think this is possible as the data needs to be stored somewhere. My advice would be to put the data somewhere from the function and take it from there to the dataflow. ADF doesnt have the capacity on its own to store data.</p>
"
"75621518","Azure Data Flow source from output in pipeline","<p>Is it possible to create a Data Flow that gets data from the output of an activity in a Pipeline?</p>
<p>Something like this:
<a href=""https://i.stack.imgur.com/L0oAg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L0oAg.png"" alt=""enter image description here"" /></a></p>
<p>I want to create a pipeline that has an Azure function activity to process some data and then pass that processed data to a Data flow.</p>
<p>But apparently the Data flow only accepts a Dataset as a source.</p>
","<azure><azure-data-factory>","2023-03-02 22:42:36","153","0","2","75625419","<p>If your Azure Function Activity gives a array of Json, Then you can follow below process:</p>
<p>Here I have used Lookup Instead of Azure Function which gives Following output:</p>
<p><img src=""https://i.imgur.com/LFieVZ3.png%5C"" alt=""enter image description here"" /></p>
<p>Use this Output in ForEach Activity and generate array for name object like below with append variable activivty inside ForEach:</p>
<pre><code> [ &quot;Rakesh&quot;, &quot;Bhavan&quot;, &quot;sai&quot;, &quot;kiran&quot;, &quot;kowsik&quot;, &quot;Rakesh&quot;, &quot;kowsik&quot;, &quot;arun&quot;, &quot;sai&quot;, &quot;virat&quot; ]
</code></pre>
<p>Now create a dataflow and with an array parameter. Pass the above array to the Dataflow Parameter from Pipeline:</p>
<p><img src=""https://i.imgur.com/qK3jhVt.png"" alt=""enter image description here"" /></p>
<p>Inside DataFlow take a source with dummy dataset and dummy column,
Generate the column from the array parameter using derived column expression <code>unfold($arr) </code> like below:</p>
<p><img src=""https://i.imgur.com/yqTTev3.png"" alt=""enter image description here"" /></p>
<p>Derived Column Result with column:</p>
<p><img src=""https://i.imgur.com/ASx7AF3.png"" alt=""enter image description here"" /></p>
<p>But the above procedure only works for a single array. So, if you have multiple arrays you have to store the Azure function Output to a blob or ADLS and then Get it back to Dataflow as suggested by @Ziya Mert Karakas.</p>
"
"75621400","ADF dataflow external library use case","<p>I'm using ADF dataflow to process my data. I have an use case where I need to convert a column X to Y based on custom logic in a library. When handling in code, we could import the library into the project to achieve this. Does ADF dataflow provides such capability ? I understand doing an external call to an endpoint is possible here. But doing millions of call for a simple conversion doesn't seem ideal.</p>
","<azure-data-factory>","2023-03-02 22:25:18","85","0","1","75638628","<p>Because ADF uses a &quot;managed Spark&quot; environment for data flows, via the Integration Runtime, we do not allow custom libraries for execution of code in data flows. The best solution is to augment your data flows by adding custom logic through a pipeline activity using a Notebook, SQL script, or Azure Function.</p>
"
"75619065","check for null values on Data Flow Azure Data factory","<p>Imagine I have following four rows on a SQL database:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>county</th>
<th>city</th>
<th>adress</th>
<th>price</th>
<th>datefrom</th>
<th>dateto</th>
<th>iscurrent</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>PT</td>
<td>Lisbon</td>
<td>jack</td>
<td>10000</td>
<td>2012-1-1</td>
<td>2022-1-10</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>PT</td>
<td>Lisbon</td>
<td>jack</td>
<td>10000</td>
<td>2012-8-1</td>
<td>null</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>ES</td>
<td>Madrid</td>
<td>ola str</td>
<td>23000</td>
<td>2022-3-1</td>
<td>2022-3-10</td>
<td>0</td>
</tr>
<tr>
<td>4</td>
<td>ES</td>
<td>Madrid</td>
<td>ola str</td>
<td>23000</td>
<td>2022-10-1</td>
<td>2023-1-01</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>I want to create a date flow on azure data factory that checks if the inactive rows (iscurrent =0) have a duplicate row (of the first 5 columns from id to price) that is active (iscurrent =1).</p>
<p>To do that I create an Aggregate transformation, I group by ID,COUNTRY,ADRESS,PRICE. My problem is which kind of expression should I use under aggregation to create a new column (havenulls) that shows 1 if there is null values on dateto, or 0 if there is no values. The output should be:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>county</th>
<th>city</th>
<th>adress</th>
<th>price</th>
<th>datefrom</th>
<th>havevulls</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>PT</td>
<td>Lisbon</td>
<td>jack</td>
<td>10000</td>
<td>2012-1-1</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>ES</td>
<td>Madrid</td>
<td>ola str</td>
<td>23000</td>
<td>2022-10-1</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>If I manage to get this or similar output I can thereafter split between &quot;duplicates&quot; that are active or not.</p>
<p>If you guys know another way to get the same result I am eager to hear from you.</p>
<p>thanks in advance</p>
<p><a href=""https://i.stack.imgur.com/krUZ0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/krUZ0.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-03-02 17:45:17","146","0","1","75623073","<p>It is the assert activity that you are looking for:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-assert"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-assert</a></p>
<p>You can check for duplicate or null as well.</p>
"
"75617269","How to Dynamic Pivot multiple parquet files (same schema) in Azure Synapse","<p>I have a dataset that needs to be divided and partitioned by a key (template name), I was able to do this. I now have new parquet files that need to be pivoted by AttName. This is a dynamic set of files as the source system creates new templates, there will now be a new file. The files are Container/Template/Year/Month.</p>
<p>I've tried a lot of things within the same dataflow and it always fails. I've attempted to use metadata and a foreach but I keep getting lost.</p>
<p>Any and all help would be nice.</p>
","<pivot><azure-data-factory><azure-synapse>","2023-03-02 15:03:08","106","0","1","75653523","<p>You can use a Get Meta data wild card placeholder(dataset parameter) with values like <code>*/*/*/*.csv</code> to get all files list and pass it to a ForEach with dataflow activity as discussed in comments.</p>
<p>Also, you can try the below alternative if you are not sure about dataflow wild card path.</p>
<p>This are my variable and pipeline flow.</p>
<p><img src=""https://i.imgur.com/UWJ7685.png"" alt=""enter image description here"" /></p>
<p>First use a copy activity with wild card path and an additional column <code>$FILEPATH</code>. Use merge option in sink and copy this single file to a temporary location.</p>
<p><img src=""https://i.imgur.com/HC6CvWW.png"" alt=""enter image description here"" /></p>
<p>This will add a column for all file records paths and merges into a single file.
Then use lookup for that temporary merged file and give that to a ForEach(with <code>@item().filepath</code> to append activity to <code>patharray</code> variable) to get array of all file paths from the column.</p>
<p>Then to get all unique file paths array take another set variable activity with dynamic content <code>@union(variables('pathsarray'),variables('pathsarray'))</code> to <code>respaths</code> variable which will give the below result.</p>
<p><img src=""https://i.imgur.com/DNU3YH8.png"" alt=""enter image description here"" /></p>
<p>Give this to another ForEach activity and inside ForEach use dataflow with below source.</p>
<p><img src=""https://i.imgur.com/XtA9p6P.png"" alt=""enter image description here"" /></p>
<p>Create a dataflow parameter <code>sinkfilename</code> and give that to sink in dataflow.</p>
<p><img src=""https://i.imgur.com/m27TXes.png"" alt=""enter image description here"" /></p>
<p>Now, inside dataflow pass <code>@item()</code> to source dataset parameter and the below dynamic content to dataflow parameter from pipeline.</p>
<p><code>@concat('targetfolder/',split(item(),'/')[sub(length(split(item(),'/')),1)])</code></p>
<p><img src=""https://i.imgur.com/nOylO3j.png"" alt=""enter image description here"" /></p>
<p>Inside dataflow, you can use pivot as transformation as per your requirement.</p>
<p><strong>This is my Pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline3&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Copy data for merge&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;additionalColumns&quot;: [
                        {
                            &quot;name&quot;: &quot;filepath&quot;,
                            &quot;value&quot;: &quot;$$FILEPATH&quot;
                        }
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;wildcardFolderPath&quot;: &quot;*/*/*&quot;,
                        &quot;wildcardFileName&quot;: &quot;*.csv&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;,
                        &quot;copyBehavior&quot;: &quot;MergeFiles&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                        &quot;quoteAllText&quot;: true,
                        &quot;fileExtension&quot;: &quot;.csv&quot;
                    }
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;sourcefiles&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;Tempcsvfiles&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        },
        {
            &quot;name&quot;: &quot;Lookup to get filepath&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Copy data for merge&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Tempcsvfiles&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;Iterate for filepath array&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup to get filepath&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Lookup to get filepath').output.value&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;pathsarray&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@item().filepath&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Union array&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Iterate for filepath array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;respaths&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@union(variables('pathsarray'),variables('pathsarray'))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach2&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Union array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('respaths')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Data flow1&quot;,
                        &quot;type&quot;: &quot;ExecuteDataFlow&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;dataflow&quot;: {
                                &quot;referenceName&quot;: &quot;dataflow1&quot;,
                                &quot;type&quot;: &quot;DataFlowReference&quot;,
                                &quot;parameters&quot;: {
                                    &quot;sinkfilename&quot;: {
                                        &quot;value&quot;: &quot;'@{concat('targetfolder/',split(item(),'/')[sub(length(split(item(),'/')),1)])}'&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                },
                                &quot;datasetParameters&quot;: {
                                    &quot;source1&quot;: {
                                        &quot;filename&quot;: {
                                            &quot;value&quot;: &quot;@item()&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            },
                            &quot;compute&quot;: {
                                &quot;coreCount&quot;: 8,
                                &quot;computeType&quot;: &quot;General&quot;
                            },
                            &quot;traceLevel&quot;: &quot;Fine&quot;
                        }
                    }
                ]
            }
        }
    ],
    &quot;variables&quot;: {
        &quot;pathsarray&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;respaths&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>This is my Result:</strong></p>
<p><img src=""https://i.imgur.com/Wo2pNG9.png"" alt=""enter image description here"" /></p>
"
"75617115","How to pass variable from pipeline into dataflow destination dataset","<p>I have created variable in a pipeline, next I pass it to copy activity and then I used it to build path in a dataflow so I am able to pass that variable into dataflow source dataset, but for some reason Iam not able to pass it into dataflow destination dataset
I have created same parameters in destination as in the source but destinations dataset is not picking it,not sure why when source dataset is having it</p>
","<google-cloud-dataflow><azure-data-factory>","2023-03-02 14:49:19","100","1","1","75624040","<blockquote>
<p>How to pass variable from pipeline into dataflow destination dataset.</p>
</blockquote>
<p>To pass pipeline variable to the destination dataset in dataflow create destination dataset with parameters and pass that parameter to path values.</p>
<p>Here I have this destination dataset. In that dataset under <code>Parameters</code> created 3 parameters.</p>
<p><img src=""https://i.imgur.com/TbVB7R9.png"" alt=""enter image description here"" /></p>
<p>Then pass that dataset parameters to the respective path under <code>Connection</code> from dynamic expressions.</p>
<p><img src=""https://i.imgur.com/EOisOeu.png"" alt=""enter image description here"" /></p>
<p>After this go to your pipeline under Data flow <code>settings</code> you will see Source and sink parameters par your pipeline variable as value to the respective parameter.</p>
<p><img src=""https://i.imgur.com/wdsJCe9.png"" alt=""enter image description here"" /></p>
<p>If still not working unselect the data flow and select it again it will refresh the dataflow to take all values.</p>
"
"75616433","automate deploy datafactory ARM template","<p>I am trying to automate the deploiement of azure datafactory arm template <br>
I am following this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-improvements"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-improvements</a> <br></p>
<pre><code> Failed to export ARM template. Error:  No resource found in specified input path: C:\\azp\\agent\\_work\\26/self/ADF. Please set correct path and try again.
</code></pre>
<p>Is there some prequistes files to put in the root folder before running the command?</p>
","<azure><npm><azure-devops><azure-data-factory>","2023-03-02 13:57:44","35","0","1","75617778","<p>As I understand, you are trying to run:</p>
<pre><code>npm run build export C:\DataFactories\DevDataFactory /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/testResourceGroup/providers/Microsoft.DataFactory/factories/DevDataFactory ArmTemplateOutput
</code></pre>
<p>In this case, your ADF folder has to contain ADF sources. Do you use source control for it? <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/source-control</a></p>
<p>You may disable publishing from the adf_publish branch if your trying to use deployment from sources: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control#editing-repo-settings"" rel=""nofollow noreferrer"">Editing repo settings</a></p>
"
"75612263","Excel file Processing using Azure Data factory","<p>There are 10 excel files in on premise machine and each excel file contains different number of sheets and the sheets will vary dynamically(mostly increasing the sheets).For example Excel-1 contains two different sheets in the initial days and the later third sheet included in the file. Need to process accordingly and pick only the few required column from each sheet .Once after picking the columns need to join those 10 excel files and get the columns from each excel and load into final Sql table(Assume Sql table contains 100 columns and columns are coming from all the excels).</p>
<p>Best approach to design the Azure data factory pipeline?</p>
","<azure><azure-data-factory>","2023-03-02 07:09:00","60","0","1","75639370","<p>Of course this can be done with the mapping dataflow. You can take those excel files as seperate sources, join them, if you need to select certain columns, use select transformation etc. In source you need to specify where the file will be coming from. You can add new source by adding a new dataset, and you can add many sources as you like and include them in joins. All join logic is based on merge logic from spark.</p>
<p>In the linked service to .xls or .xlsx files you can specify worksheet mode(name or index) and sheet name. you can also parameterize these values.</p>
<p><a href=""https://i.stack.imgur.com/Ko81a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ko81a.png"" alt=""Excel Linked Service"" /></a></p>
<p><a href=""https://i.stack.imgur.com/MDnwn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MDnwn.png"" alt=""Multiple joins"" /></a></p>
"
"75609278","Exporting data factory pipelines","<p>I am trying to export my data factory pipelines as a backup but the problem is that i have to export each individual pipeline JSON file from the code.
Is there an easier way of exporting all at once or atleast export the whole folder with all the underlying pipelines in it along with the sub folder structures.</p>
","<azure-data-factory>","2023-03-01 21:38:55","34","0","1","75612085","<p>You can export the entire data factory as an ARM template.
Go to manage -&gt; ARM template, and click on Export ARM template.
<a href=""https://i.stack.imgur.com/7WGzj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7WGzj.png"" alt=""export ARM template"" /></a></p>
<p>If you want to extract the pipelines from the ARM template, you can use code to write each pipeline to it's own file.
Here is an example in Python:</p>
<pre><code>import json
with open(&quot;ARMTemplateForFactory.json&quot;,&quot;r&quot;) as ARM:
    data = json.load(ARM)
for resource in data[&quot;resources&quot;]:
    if resource[&quot;type&quot;]==&quot;Microsoft.DataFactory/factories/pipelines&quot;:
        #print(data[&quot;resources&quot;][0][&quot;name&quot;][37:-3])
        resource_name = str(resource[&quot;name&quot;])
        file_name = resource_name[str(resource_name).find(&quot;,&quot;)+4:-3]
        print(file_name)
        pl_file = open(file_name+&quot;.json&quot;,&quot;x&quot;)
        #pl_file.write(str(resource))
        json.dump(resource,pl_file)
        pl_file.close()
</code></pre>
<p>ARMTemplateForFactory.json is the file extracted from the ARM template zip file.</p>
"
"75608999","nested json sql","<p>I have a json column in a sql table called &quot;report_json &quot;. The strucute of json is as below</p>
<pre><code>{
    &quot;drugs&quot;: {
        &quot;Codeine&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Codeine&quot;,
            &quot;icons&quot;: [
                93,
                100,
                103
            ],
            &quot;drug_id&quot;: 36,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;prodrug&quot;,
                &quot;text&quot;: &quot;is **inactive**, its metabolites are active.&quot;
            },
            &quot;targets&quot;: [
                &quot;OPRM1&quot;
            ],
            &quot;rxnorm_id&quot;: &quot;2670&quot;,
            &quot;priclasses&quot;: [
                &quot;Analgesic/Anesthesiology&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                9,
                17,
                29,
                30,
                159,
                171
            ],
            &quot;subclasses&quot;: [
                &quot;Analgesic agent&quot;,
                &quot;Antitussive agent&quot;,
                &quot;Opioid agonist&quot;,
                &quot;Phenanthrene &quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;OPRM1&quot;: &quot;78&quot;,
                &quot;metabolic&quot;: &quot;6&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2D6&quot;,
                &quot;OPRM1&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Normal to reduced morphine formation. Use label recommended age- or weight-specific dosing. If no response, may need to consider alternative analgesics such as morphine or a non-opioid.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 103
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Predicted codeine metabolism is reduced.&quot;,
                    &quot;icons_id&quot;: 58,
                    &quot;sort_key&quot;: 58,
       
      
</code></pre>
<p>my question is how can i get the bin and name data for all the values as the json contains an array for name.</p>
<p>for exampel my resulted output i need is</p>
<p>bin name</p>
<p>r.   Codeine</p>
<p>y.   Digoxin</p>
","<sql><json><sql-server><json.net><azure-data-factory>","2023-03-01 21:04:57","54","0","1","75609293","<p>The JSON you posted is incomplete but presumably you need something like</p>
<pre><code>SELECT bin = JSON_VALUE(d.value, '$.bin'),
      name = JSON_VALUE(d.value, '$.name')
FROM OPENJSON(@JSON, '$.drugs') d
</code></pre>
<p>(On the assumption that the relevant parts of the JSON would be like the below)</p>
<pre><code>DECLARE @JSON NVARCHAR(MAX) = N'{
  &quot;drugs&quot;: {
    &quot;Codeine&quot;: {
      &quot;bin&quot;: &quot;r&quot;,
      &quot;name&quot;: &quot;Codeine&quot;,
      &quot;icons&quot;: [
        93,
        100,
        103
      ]
    },
    &quot;Digoxin&quot;: {
      &quot;bin&quot;: &quot;y&quot;,
      &quot;name&quot;: &quot;Digoxin&quot;,
      &quot;icons&quot;: []
    }
  }
}'
</code></pre>
"
"75606875","Azure Data Factory Filter JSON Object Data From Notebook Activity","<p>I am trying to call the one of the API in our databricks notebook and it will give the appropriate output. finally we will get JSON object NoteBook activity in ADF. I would like to Filter the empty array object, which is one of the array object ( nested json) needs to skip and where ever coming proper without empty value, I have filter out. please find below my sample input json from NoteBook Activity.</p>
<pre><code>[

 {

  'day': 60.0,

  'server': 'xxxx',

  'database': 'ddddd',

  'table': 'tablename',

  'asset_id': '23232323',

  'indicate': ['value1'],

  'sub_indicator': ['sub']

  },

 {'day': 999.0,

  'server': 'sadsadsad',

  'database': 'dbbb',

  'table': 'tablename2',

  'asset_id': 'xxxxxx1',

  'indicate': ['value2'],

  'sub_indicator': ['sub2']

  },

 {'day': 30.0,

  'server': 'server3',

  'database': 'db3',

  'table': 'tablename3',

  'asset_id': 'xxxxxxx',

  'indicate': ['value3'],

  'sub_indicator': ['sub3']},

 {'day': 75.0,

  'server': 'ser',

  'database': 'db',

  'table': 'tablename',

  'asset_id': 'asdasd-adasdsa',

  'indicate': ['val1', 'val2'],

  'sub_indicator': ['sub1', 'sub2']},

 {'day': 50.0,

  'server': 'serrr',

  'database': 'dbb',

  'table': 'tablename4',

  'asset_id': 'yyyyyyyy',

  'indicate': ['value4'],

  'sub_indicator': ['sub1', 'sub2']},

 {'day': 100.0,

  'server': 'ser',

  'database': 'IRF_Everest',

  'table': 'tablename5',

  'asset_id': 'adsadasdadasdasdasd',

  'indicate': ['sub1', 'sub2'],

  'sub_indicator': ['val1']},

 {'day': 60.0,

  'server': 'server3',

  'database': 'db1',

  'table': 'tablename7',

  'asset_id': '3312312321fsdasfasf',

  'indicate': ['val1'],

  'sub_indicator': []},

 {'day': 50.0,

  'server': 'serrrrr',

  'database': 'db11',

  'table': 'tablename8',

  'asset_id': '6ac9aea1-sdsdsdsadasdsadsad',

  'indicate': ['val'],

  'sub_indicator': []},

 {'day': 60.0,

  'server': 'serrr',

  'database': 'db22',

  'table': 'tablename10',

  'asset_id': '98e3dff0-adsadsadasd',

  'indicate': ['key'],

  'sub_indicator': ['sub_key']

  }

  ]
</code></pre>
<p>below object sub_indicator filed you can see the empty value, so that we have to skip entire object while filter activity. please help us, how do we filter each object wise.</p>
<pre><code>{'day': 60.0,

  'server': 'server3',

  'database': 'db1',

  'table': 'tablename7',

  'asset_id': '3312312321fsdasfasf',

  'indicate': ['val1'],

  **'sub_indicator': []**
},
</code></pre>
<p>After Filter, we are experiencing as below JSON like.</p>
<pre><code>[
 {
  'day': 60.0,

  'server': 'xxxx',

  'database': 'ddddd',

  'table': 'tablename',

  'asset_id': '23232323',

  'indicate': ['value1'],

  'sub_indicator': ['sub']

  },

 {'day': 999.0,

  'server': 'sadsadsad',

  'database': 'dbbb',

  'table': 'tablename2',

  'asset_id': 'xxxxxx1',

  'indicate': ['value2'],

  'sub_indicator': ['sub2']

  },

 {'day': 30.0,

  'server': 'server3',

  'database': 'db3',

  'table': 'tablename3',

  'asset_id': 'xxxxxxx',

  'indicate': ['value3'],

  'sub_indicator': ['sub3']},

 {'day': 75.0,

  'server': 'ser',

  'database': 'db',

  'table': 'tablename',

  'asset_id': 'asdasd-adasdsa',

  'indicate': ['val1', 'val2'],

  'sub_indicator': ['sub1', 'sub2']},

 {'day': 50.0,

  'server': 'serrr',

  'database': 'dbb',

  'table': 'tablename4',

  'asset_id': 'yyyyyyyy',

  'indicate': ['value4'],

  'sub_indicator': ['sub1', 'sub2']},

 {'day': 100.0,

  'server': 'ser',

  'database': 'IRF_Everest',

  'table': 'tablename5',

  'asset_id': 'adsadasdadasdasdasd',

  'indicate': ['sub1', 'sub2'],

  'sub_indicator': ['val1']
},
{'day': 60.0,

  'server': 'serrr',

  'database': 'db22',

  'table': 'tablename10',

  'asset_id': '98e3dff0-adsadsadasd',

  'indicate': ['key'],

  'sub_indicator': ['sub_key']

  }
 ]
</code></pre>
","<azure-data-factory>","2023-03-01 17:08:09","113","0","1","75623366","<p>Considering you already know what the array items are inside the notebook output array of object, you can directly use the filter condition as shown below:</p>
<pre><code>items: 
@activity('Notebook1').output.runOutput

condition:
@and(not(equals(length(item().sub_indicator),0)),not(equals(length(item().indicate),0)))
</code></pre>
<p><img src=""https://i.imgur.com/Q6STEnt.png"" alt=""enter image description here"" /></p>
<ul>
<li>If these key names for array items in notebook run output are unknown, then you can add the following additional code to databricks notebook to return these key names.</li>
</ul>
<pre><code>#data is the variable I used to store the given notebook output json
keys = [k for k in data[0].keys()]
#print(keys)
req_keys  = [k for k in keys if isinstance(data[0][k],list)]
#print(req_keys)
key_obj = [{'req_keys':req_keys}]
#print(key_obj)
key_obj.extend(data)
print(key_obj)

import json
json_str = json.dumps(key_obj)
dbutils.notebook.exit(json_str)
</code></pre>
<p><img src=""https://i.imgur.com/zuUyuQ6.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now in data factory do the following steps.</li>
</ul>
<ol>
<li>Assign the notebook run output to an array variable (data).</li>
<li>Use for each activity to loop through the keys using the dynamic content <code>@variables('data')[0]['req_keys']</code> where data is the array variable.</li>
<li>Assign the current item of for each iterator to a variable (tp).</li>
<li>Use filter activity with items and condition as shown below (here item is filter iterator but not for each iterator):</li>
</ol>
<pre><code>items:
@skip(activity('Notebook1').output.runOutput,1)

condition:
@not(equals(length(item()[variables('tp')]),0))
</code></pre>
<ol start=""5"">
<li>Re-assign the value of this filter output to data variable (array).</li>
</ol>
<ul>
<li>The following is the entire pipeline JSON for above procedure:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Notebook1&quot;,
                &quot;type&quot;: &quot;DatabricksNotebook&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;notebookPath&quot;: &quot;/nb1&quot;
                },
                &quot;linkedServiceName&quot;: {
                    &quot;referenceName&quot;: &quot;AzureDatabricks1&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                }
            },
            {
                &quot;name&quot;: &quot;loop through keys&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;activity output&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@variables('data')[0]['req_keys']&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Filter1&quot;,
                            &quot;type&quot;: &quot;Filter&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;for each current item&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;items&quot;: {
                                    &quot;value&quot;: &quot;@skip(activity('Notebook1').output.runOutput,1)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                },
                                &quot;condition&quot;: {
                                    &quot;value&quot;: &quot;@not(equals(length(item()[variables('tp')]),0))&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;for each current item&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;tp&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@item()&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        },
                        {
                            &quot;name&quot;: &quot;reassign&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Filter1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;data&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@activity('Filter1').output.Value&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;activity output&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Notebook1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;data&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@activity('Notebook1').output.runOutput&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;data&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;tp&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<ul>
<li>This would give you the desired output even when you don't know the names of the keys whose item value is an array and empty.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/8CS5i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8CS5i.png"" alt=""enter image description here"" /></a></p>
"
"75606191","Azure Data Factory can not get multiselect option list from dynamics 365","<p>Azure Data Factory Link Service has a Dataverse connector that allows you to extract data from Dynamics 365.</p>
<p>After extracting data we noticed that none of the multi-select fields were populating our Data Warehouse.</p>
<p>After doing some research we came across the <a href=""https://learn.microsoft.com/en-us/answers/questions/358967/azure-data-factory-can-not-get-multiselect-option"" rel=""nofollow noreferrer"">link</a>, which states that Azure Data Factory can not get multiselect option list from dynamics 365.</p>
<p>We noticed that the article was created in 2021.</p>
<p>Can someone let me know if it is still the case that Azure Data Factory can not get multiselect option list from dynamics 365.</p>
","<azure-data-factory>","2023-03-01 16:06:55","75","0","1","75644883","<p>Please see here:  <a href=""https://stackoverflow.com/questions/61098163/unable-to-read-multi-select-option-set-in-azure-data-factory"">Unable to read Multi-select option set in Azure Data Factory</a></p>
<p>And then go to docs quoted in that question, you will see that even though most attributes are supported, your required one is not:</p>
<p>The Dynamics data types AttributeType.CalendarRules, AttributeType.MultiSelectPicklist, and AttributeType.PartyList aren't supported.</p>
<p>What you can do is find a feature request most similar to your situation and upvote it and hope it will be implemented. You can also create your own request.</p>
<p><a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c?query=hubspot"" rel=""nofollow noreferrer"">https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c?query=hubspot</a></p>
<p>Edit: I have also heard of using FetchXML for these attributes you can see if its works by making more research about this</p>
"
"75602662","Azure Data Factory (ADF): Failed to run Pipeline Bad Request","<p>I have a simple master pipeline that executes multiple other pipelines in parallel. When I trigger the individual child pipelines manually, I have no issue. But when I try to run the master pipeline it returns a BadRequest, with an empty message.</p>
<pre><code>&quot;code&quot;: &quot;BadRequest&quot;,
&quot;message&quot;: null,
&quot;target&quot;: &quot;pipeline//runid/a15ff459-a32e-4127-8b57-20695d2b1979&quot;,
&quot;details&quot;: null,
&quot;error&quot;: null
}```
</code></pre>
","<bad-request><azure-data-factory>","2023-03-01 10:48:47","222","0","1","75602663","<p>I figured out that the name of one of the Pipeline execution tasks/blocks has a trailing whitespace, once this was removed the master pipeline was able to run successfully.</p>
"
"75601017","ADF Expression function ""IF"" with more than 2 conditions","<p>I Have question about Azure Data Factory Expression for function IF</p>
<p>in SQL we can create using</p>
<pre><code>CASE WHEN 'A' Then 1
     when  'B' then 2
     when 'C' then 3
else 100
</code></pre>
<p>is this correct expression to IF?</p>
<pre><code>@if(equals(variables('varInput'), 'a'), 'Ax', 
if(equals(variables('varInput'), 'b'), 'Bx', 'C'))
</code></pre>
<p>how to us &quot;IF&quot; function for more than 2 or we can use another Function like &quot;Or&quot;?</p>
<p>this is the expression that i built</p>
<pre><code>@if(equals(formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'MMdd'),'0101'), concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(getPastTime(1,'Year'),'SE Asia Standard Time'),'yyyy'),'/','12'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')), if(equals(formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'dd'),'01'), concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'yyyy'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Month'),'SE Asia Standard Time'),'MM'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')),concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'yyyy'),'/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'MM'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')))
</code></pre>
<p>I want to create folder base on this case</p>
<ol>
<li>If today's date is 2023-01-01 then  folder name path will be PRODUCT/2022/12/20221231</li>
<li>if today's date is 2023-02-01  then folder name path will be PRODUCT/2023/01/20230131</li>
<li>if todays date is 2023-03-02 then folder name path will be PRODUCT/2023/03/20230301</li>
</ol>
<p>but expression IF doest support more than 2 arguments</p>
","<azure><azure-devops><azure-functions><azure-data-factory>","2023-03-01 08:12:22","146","0","2","75602020","<p>Check the case function as well, this should be the one you are looking for, not if function.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#case"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#case</a></p>
<p>please also confirm if this answer helped you</p>
"
"75601017","ADF Expression function ""IF"" with more than 2 conditions","<p>I Have question about Azure Data Factory Expression for function IF</p>
<p>in SQL we can create using</p>
<pre><code>CASE WHEN 'A' Then 1
     when  'B' then 2
     when 'C' then 3
else 100
</code></pre>
<p>is this correct expression to IF?</p>
<pre><code>@if(equals(variables('varInput'), 'a'), 'Ax', 
if(equals(variables('varInput'), 'b'), 'Bx', 'C'))
</code></pre>
<p>how to us &quot;IF&quot; function for more than 2 or we can use another Function like &quot;Or&quot;?</p>
<p>this is the expression that i built</p>
<pre><code>@if(equals(formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'MMdd'),'0101'), concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(getPastTime(1,'Year'),'SE Asia Standard Time'),'yyyy'),'/','12'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')), if(equals(formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'dd'),'01'), concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'yyyy'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Month'),'SE Asia Standard Time'),'MM'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')),concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'yyyy'),'/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'MM'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')))
</code></pre>
<p>I want to create folder base on this case</p>
<ol>
<li>If today's date is 2023-01-01 then  folder name path will be PRODUCT/2022/12/20221231</li>
<li>if today's date is 2023-02-01  then folder name path will be PRODUCT/2023/01/20230131</li>
<li>if todays date is 2023-03-02 then folder name path will be PRODUCT/2023/03/20230301</li>
</ol>
<p>but expression IF doest support more than 2 arguments</p>
","<azure><azure-devops><azure-functions><azure-data-factory>","2023-03-01 08:12:22","146","0","2","75626989","<blockquote>
<p>IF doesn't support more than 2 arguments.</p>
</blockquote>
<p>In order to give multiple conditions, you can use the nested if expression i.e.  if expression inside another if .</p>
<p>I used the same expression what you have tried and got error.</p>
<p>This error is because, closing paranthesis <code>)</code> are misplaced in that expression. I have highlighted that in the below image.</p>
<p><img src=""https://i.imgur.com/M1QEMDP.png"" alt=""enter image description here"" /></p>
<p><strong>Corrected expression</strong> :</p>
<p>You can use the below expression for your requirement.</p>
<pre><code>@if(equals(formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'MMdd'),'0101'), concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(getPastTime(1,'Year'),'SE Asia Standard Time'),'yyyy'),'/','12','/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')), if(equals(formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'dd'),'01'), concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'yyyy'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Month'),'SE Asia Standard Time'),'MM'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd')),concat('PRODUCT/','daily/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'yyyy'),'/',formatDateTime(convertFromUtc(utcNow(),'SE Asia Standard Time'),'MM'),'/',formatDateTime(convertFromUtc(getPastTime(1,'Day'),'SE Asia Standard Time'),'yyyyMMdd'))))
</code></pre>
"
"75596560","Azure Lookup Activity Query Option to Parameterized Select Statement","<p>I am trying to do the select query option in my lookup activity to connect azure sql database but if I am passing multiple parameter with condition. I am getting failed activity. kindly help me.</p>
<p>ForEach loop @item will give the dynamic table values to pass our lookup activity.</p>
<pre><code>select * from tablename where columnname ='@{item().table} IS NOT NULL' AND columnname='@{item().table}!=&quot;&quot; '
</code></pre>
<p>I want to apply the column should not give the NULL value and EMPTY value. so that I am using</p>
<blockquote>
<p>NOT NULL</p>
</blockquote>
<p>keyword and checking <code>column!=''</code> also</p>
<p><a href=""https://i.stack.imgur.com/NbVfV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NbVfV.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-02-28 19:38:51","24","0","1","75597315","<p>To get better answers, you should provide more detail. You say you get a failed activity but don't provide the specific failure or the error message.</p>
<p>Also, you should be able to look at the run/debug detail and see the constructed query, which should give some clues as to the issue.</p>
<p>Without that information, I'm just guessing, but the following query does not make sense to me because the ' marks encapsulate IS NOT NULL and != &quot;&quot;:</p>
<pre><code>select * from tablename where columnname ='@{item().table} IS NOT NULL' AND columnname='@{item().table}!=&quot;&quot; '
</code></pre>
<p>I assume you mean something like this:</p>
<pre><code>SELECT *
FROM tablename
WHERE columnname {@item().table} IS NOT NULL
AND columnname {@item().table} != &quot;&quot;
</code></pre>
<p>Do you see the difference between these two? I'm also assuming here that &quot;tablename&quot; and &quot;columnname&quot; are placeholders for known values, not that they are also dynamic.</p>
"
"75596448","How to exclude ADF auto generated properties from Web activity","<p>I have a web activity in ADF pipeline. This web activity calls an API and get data from that. The output Web activity is in below format,</p>
<pre><code>{
  &quot;fdgdhgfh&quot;: {
    &quot;so2_production&quot;: 7hjhgj953,
    &quot;battery_charge&quot;: jkjlkj,
    &quot;battery_discharge&quot;: kjlklj,
    &quot;critical_load_energy&quot;: 4ljljh4
},
  &quot;9fsdsfb&quot;: {   
    &quot;so2_production&quot;: asdasd,
    &quot;battery_charge&quot;: sdaasf,
    &quot;battery_discharge&quot;: ewewrwer,
    &quot;critical_load_energy&quot;: bmvkbjk
  },
&quot;ADFWebActivityResponseHeaders&quot;:{
&quot;Content-Length&quot;:14268,
&quot;Content-Type&quot;:&quot;application/json&quot;
},
&quot;effectiveIntegrationRunTime&quot;:&quot;Azure IR&quot;,
&quot;billingReference&quot;:{
&quot;activityType&quot;:&quot;ExternalActivity&quot;
},
}
</code></pre>
<p>From above output I only want API response without ADF auto generated properties like,</p>
<pre><code>{
  &quot;fdgdhgfh&quot;: {
    &quot;so2_production&quot;: 7hjhgj953,
    &quot;battery_charge&quot;: jkjlkj,
    &quot;battery_discharge&quot;: kjlklj,
    &quot;critical_load_energy&quot;: 4ljljh4
  },
  &quot;9fsdsfb&quot;: {
   
    &quot;so2_production&quot;: asdasd,
    &quot;battery_charge&quot;: sdaasf,
    &quot;battery_discharge&quot;: ewewrwer,
    &quot;critical_load_energy&quot;: bmvkbjk
  }
}
</code></pre>
<p>Let me know the process or expression to achieve in ADF pipeline.</p>
","<azure><azure-data-factory>","2023-02-28 19:23:35","85","0","1","75623500","<ul>
<li>Since you are not getting the <code>response</code> attribute in the set variable activity, you can use the following procedure instead.</li>
<li>Store the output of <code>web activity</code> output as a string in a variable using set variable activity. The output would look like something shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/ilUi2vZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, use another set variable activity to split the above string using <code>ADFWebActivityResponseHeaders</code> as delimiter.</li>
</ul>
<pre><code>@split(variables('tp2'),'ADFWebActivityResponseHeaders')[0]
</code></pre>
<p><img src=""https://i.imgur.com/ovlpy03.png"" alt=""enter image description here"" /></p>
<ul>
<li>Finally, using the following dynamic content in another set variable activity, I was able to extract required part:</li>
</ul>
<pre><code>@array(json(substring(variables('tp1'),0,sub(length(variables('tp1')),3))))
</code></pre>
<p><img src=""https://i.imgur.com/0XRZu03.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have taken it as an array for demonstration. You can access the required part from this using the dynamic content <code>@variables('tp3')[0]</code> where tp3 is the variable with value in the above image.</li>
</ul>
"
"75595579","How to concatinate string with date in Dataflow expression builder","<p>Why this does not work in dataflow expression builder in azure data factory:
concat(subDays(currentUTC(),-1),'.parquet')</p>
<p>I dont understand why is it so difficult???</p>
","<azure><azure-data-factory>","2023-02-28 17:44:43","89","0","1","75599492","<p><strong>Concat</strong> function expects its arguments to be of 'string' type. But subDays(currentUTC(),1) will give the data in <strong>timestamp</strong> / <strong>date</strong> type. This is the reason we get the error as <code>Type mismatch</code></p>
<p><img src=""https://i.imgur.com/qoL4vYo.png"" alt=""enter image description here"" /></p>
<p>In order to remove this error message, convert the timestamp data to string type using <code>toString()</code>. The expression will be,</p>
<p><code>concat(toString(subDays(currentUTC(),-1)),'.parquet')</code></p>
<p><img src=""https://i.imgur.com/vT8eBoq.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong>
This expression gives result without any error. This expression subtract <code>-1</code> from <strong>currentUTC()</strong> day. Subtracting -1 is nothing but adding one to the day.</p>
<p><img src=""https://i.imgur.com/vS7Nyjy.png"" alt=""enter image description here"" /></p>
"
"75595081","Convert ADF variable from string to Integer then increment by 1","<p>I am struggling to convert a variable I assign to a integer than to add +1</p>
<p>I am reading in the size of a file in bytes in ADF and diving it to get the first number of the byte (e.g. 3234123 bytes and the output is 3) however, how it's setup is that it's being read in as a string, and I also want to add 1 to this value but I cannot figure out how to get that activity working. It is being assigned to the String 'cnt'. So wondering the process of if I am able to cast this 'cnt' string to an Integer and add 1 to this value and be able to then reference this 'cnt' further in the pipeline.</p>
<p>Thanks, hope there is enough here for you to understand my problem.</p>
<p><a href=""https://i.stack.imgur.com/KW63S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KW63S.png"" alt=""enter image description here"" /></a></p>
","<variables><casting><azure-data-factory>","2023-02-28 16:55:16","60","0","1","75599468","<p>In order to add 1 to the value, before converting the value to string, you can use add() function and add 1 to the divided value.</p>
<p>Expression:
<code>@string(add(div(activity('Get Cisco size').output.size,10000000),1))</code></p>
<p><img src=""https://i.imgur.com/HmTNwzw.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Value stored in variable <strong>before</strong> using add() function to add 1:
<img src=""https://i.imgur.com/qnxv6Zr.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Value stored in variable <strong>after</strong> using add() function:
<img src=""https://i.imgur.com/Dr3srpY.png"" alt=""enter image description here"" /></p>
</li>
</ul>
"
"75593009","Copy Files from yesterday date ans update that value in SQL Database","<p>Need help.</p>
<ol>
<li>There was a input folder with 5 files. when files exist that should reflect column verified to 1 in SQL DB table.</li>
</ol>
<p>Sol: i have tried getmetadata- exist and if condition* to throw TRUE when file exist. BUT how we send TRUE value to DB Table.*</p>
<ol start=""2"">
<li>When files not exist have to copy file folder of last sys date.</li>
</ol>
<p>Get meta to check file exist and if condtio to take decision used. How to send TRUE to DB when files exist ??</p>
","<azure><azure-data-factory>","2023-02-28 13:53:40","52","0","1","75603744","<blockquote>
<p>how we send TRUE value to DB Table.</p>
</blockquote>
<p>To resolve your issue, you need update the value using script activity based upon get metadata result.</p>
<p>I assumed from information you have provided you have file table in SQL, and you are checking foreach file if it is existing in input folder or not if not copying file.</p>
<p>first get all files from sql table to check. with lookup activity I am gelling below output.
<img src=""https://i.imgur.com/PEcNTmm.png"" alt=""enter image description here"" /></p>
<p>After this I am passing this output value to for each loop as <code>@activity('Lookup1').output.value</code></p>
<p><img src=""https://i.imgur.com/34koiqV.png"" alt=""enter image description here"" /></p>
<p>Under foreach activity I took get metadata activity to check file exist or not in particular folder here I created <code>filename</code> <strong>string</strong> parameter. and passing <code>@item().filename</code> to dataset parameter.
<img src=""https://i.imgur.com/qAhpKSH.png"" alt=""enter image description here"" /></p>
<p>and passing this output to if condition with <code>@activity('Get Metadata1').output.exists</code> if result of get meta data is true then it will execute true condition.</p>
<p><img src=""https://i.imgur.com/7AzgFVq.png"" alt=""enter image description here"" /></p>
<p>under if I took script activity to update value in SQL table.</p>
<pre><code>@concat('update filetb set verified = 1 where filename = ''',item().filename,'''')
</code></pre>
<p><img src=""https://i.imgur.com/LUAkh5N.png"" alt=""enter image description here"" /></p>
<p>this query will update verified as 1 if filename exist.</p>
<p>Otherwise, control go to false condition and there you can put copy activity.
<img src=""https://i.imgur.com/H7KJhTt.png"" alt=""enter image description here"" /></p>
<p>when get metadata is false control is going to false condition.</p>
"
"75591201","How to pass variable content from a Pipeline to a dataset path inside of a Data Flow","<p>I have a pipeline that sets variable for the purpose of building dynamic data path that gets passed into a dataset within a pipeline, that works fine thanks to help on this site, but there is another issue when I tried to pass that variable into a data flow and use it as source for building a dataset within a dataflow it does not work, pipeline expression builder does not see it, it sees only parameters created locally from the dataset</p>
","<azure-data-factory>","2023-02-28 11:09:13","110","0","1","75591778","<ul>
<li>When you create a parameter for dataset and use that dataset in dataflow, it is not showing within the dataflow itself. But when you finish building the dataflow and use this dataflow inside the pipeline in <code>dataflow</code> activity.</li>
<li>Lets say I have the following dataset, where I have created a parameter called <code>req_path</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/5UpDa1I.png"" alt=""enter image description here"" /></p>
<ul>
<li>This parameter does not show up when I use it in either source or sink of the dataflow:</li>
</ul>
<p><img src=""https://i.imgur.com/NtFNW69.png"" alt=""enter image description here"" /></p>
<ul>
<li>However, when I complete building the dataflow and use it in pipeline, you will be prompted to enter the value for <code>req_path</code> parameter.</li>
</ul>
<p><img src=""https://i.imgur.com/wQdZ9pL.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, there is no need to pass the path value to dataflow and instead pass it directly to dataset (when dataset is used in dataflow transformation).</li>
</ul>
"
"75590526","SQL dynamic query in ADF","<p>I have SQL dynamic query with a &quot;?&quot; that needs to be replaced with pipeline parameters. The fetched query has a question mark &quot;?&quot; how to replace that?</p>
<p>I tried passing the parameters but ADF doesn't recognize the ? symbol and the correct parameters are not passed</p>
","<sql><azure><ssms><azure-data-factory><pipeline>","2023-02-28 10:10:22","63","0","1","75591064","<ul>
<li>You can either use <code>concat</code> function to concatenate the query with your dynamic content i.e., your parameters or you can use string interpolation (<code>@{...}</code>).</li>
<li>Either way, you can build a dynamic query where you will be using parameters (with values as you wish).</li>
<li>Consider, you want to build a query as specified in the comment, and have 2 parameters as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/qElFyff.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can use the following dynamic content to build the query. Enclose the parameter within <code>@{...}</code> (string interpolation) wherever required:</li>
</ul>
<pre><code>Declare @SourceID int, @datafileid int;
Set @SourceID=@{pipeline().parameters.source_id};
Set @datafileid=@{pipeline().parameters.datafield}
</code></pre>
<ul>
<li>The above would generate the query as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/tdFpDwu.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can either directly use similar dynamic content wherever you are using the query or store the value in a <code>set variable</code> activity and use that variable in place of query.</li>
</ul>
"
"75590385","How to get data using the until activity","<p>I would like to use the until activity and take data for a specified date.
Can you please tell me how to do this?</p>
<p>The data is stored in GEN2 as AAA/year/mm/day by day.</p>
<p><img src=""https://i.stack.imgur.com/VaIWx.png"" alt=""pipeline image"" /></p>
","<azure><azure-data-factory>","2023-02-28 09:53:51","80","0","1","75602052","<blockquote>
<p>I want to get all yyyy/mm/dd/xxx.parquet data from a specific date to a specific date.</p>
</blockquote>
<p>I could able to achieve your requirement by <strong>incrementing dates inside until activity</strong>.</p>
<p>This is my folder structure and sample csv files.</p>
<pre><code>data
    2023
        02
            26
                csv26.csv
            27
                csv27.csv
            28
                csv28.csv
        03
            01
                csv01.csv
            02
                csv02.csv
               
</code></pre>
<p>Here my date range is from <code>26-02-2023</code> to <code>02-03-2023</code>. For <code>counter variable</code> I have given it as <code>startdate-1</code> <code>(2023/02/25)</code>.</p>
<p><img src=""https://i.imgur.com/uJGKItR.png"" alt=""enter image description here"" /></p>
<p><strong>Until condition:</strong> <code>@equals(string(pipeline().parameters.end_date),variables('counter'))</code></p>
<p>Inside until, we have to increment the day by 1. As self-referencing the variables is not supported in ADF, I have taken a <code>temp</code> string variable and stored the incremented date in it with below expression.</p>
<p><code>@formatdatetime(adddays(formatdatetime(variables('counter'),'yyyy/MM/dd'),1),'yyyy/MM/dd')</code></p>
<p>After this, I have reassigned the <code>temp</code> variable to <code>counter</code> variable.</p>
<p>Now, take a copy activity and for source use the wild card path and give the <code>counter</code> variable like below. I have used <code>csv</code> files, for you give it as <code>*.parquet</code>.</p>
<p><img src=""https://i.imgur.com/0PbnEY2.png"" alt=""enter image description here"" /></p>
<p><strong>This is my Pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline2&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Until1&quot;,
            &quot;type&quot;: &quot;Until&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable3&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;expression&quot;: {
                    &quot;value&quot;: &quot;@equals(string(pipeline().parameters.end_date),variables('counter'))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Temp&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;temp&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@formatdatetime(adddays(formatdatetime(variables('counter'),'yyyy/MM/dd'),1),'yyyy/MM/dd')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Incrementing Counter&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Temp&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;counter&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@variables('temp')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Copy data1&quot;,
                        &quot;type&quot;: &quot;Copy&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Incrementing Counter&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;source&quot;: {
                                &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                    &quot;recursive&quot;: true,
                                    &quot;wildcardFolderPath&quot;: {
                                        &quot;value&quot;: &quot;@variables('counter')&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    },
                                    &quot;wildcardFileName&quot;: &quot;*.csv&quot;,
                                    &quot;enablePartitionDiscovery&quot;: false
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                }
                            },
                            &quot;sink&quot;: {
                                &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                                    &quot;quoteAllText&quot;: true,
                                    &quot;fileExtension&quot;: &quot;.txt&quot;
                                }
                            },
                            &quot;enableStaging&quot;: false,
                            &quot;translator&quot;: {
                                &quot;type&quot;: &quot;TabularTranslator&quot;,
                                &quot;typeConversion&quot;: true,
                                &quot;typeConversionSettings&quot;: {
                                    &quot;allowDataTruncation&quot;: true,
                                    &quot;treatBooleanAsNumber&quot;: false
                                }
                            }
                        },
                        &quot;inputs&quot;: [
                            {
                                &quot;referenceName&quot;: &quot;Sourcefiles&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;
                            }
                        ],
                        &quot;outputs&quot;: [
                            {
                                &quot;referenceName&quot;: &quot;target&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;
                            }
                        ]
                    }
                ],
                &quot;timeout&quot;: &quot;0.12:00:00&quot;
            }
        },
        {
            &quot;name&quot;: &quot;Set variable3&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;counter&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@pipeline().parameters.st_date&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        }
    ],
    &quot;parameters&quot;: {
        &quot;st_date&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;defaultValue&quot;: &quot;2023/02/25&quot;
        },
        &quot;end_date&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;defaultValue&quot;: &quot;2023/03/02&quot;
        }
    },
    &quot;variables&quot;: {
        &quot;counter&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;temp&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p>Execute this and you can see the files are copied to the target.</p>
<p><img src=""https://i.imgur.com/i8xLbo6.png"" alt=""enter image description here"" /></p>
"
"75589911","ODBC connection issue to load data. Copy Activity source not working","<p>I am using odbc connector to connect to AS400 DB2 system. I am fetching metadata from copy activity and it is throwing the following error.</p>
<p>Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [IBM][System i Access ODBC Driver][DB2 for i5/OS] - Error message text unavailable. Message can not be translated successfully.,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [IBM][System i Access ODBC Driver][DB2 for i5/OS] - Error message text unavailable. Message can not be translated successfully.,Source=CWBODBC.DLL,'</p>
<p>We have read access to the database. Is this access error for the given user? Or network connectivity issue because some copy activity runs while some fail. We are trying to load the data in data lake and everything is parameterized. Let me know if this is a library for Windows issue/Network issue/access issue.?</p>
","<azure><db2><azure-data-factory><ibm-midrange><azure-data-lake>","2023-02-28 09:07:08","101","0","2","75590080","<p>If it is as you say, a transient issue in copy activity, it is highly likely that it is network related. Some network errors are caused by the underlying driver (in fact it is common in Azure-SSIS scenarios where the driver in the SSISDB is old).</p>
<p>If you could write about how you set up the connector, it might be useful. Try changing the driver, first try with IBM db2 ODBC driver:</p>
<p>Are you using SHIR or Azure IR ? if it is SHIR, then you need further instructions on how to install the driver to the machine.</p>
<p><a href=""https://www.ibm.com/docs/en/db2/11.1?topic=dsd-installing-data-server-driver-odbc-cli-software-windows-operating-systems"" rel=""nofollow noreferrer"">https://www.ibm.com/docs/en/db2/11.1?topic=dsd-installing-data-server-driver-odbc-cli-software-windows-operating-systems</a></p>
<p>For Azure IR, unfortunately you cannot choose which driver you want. If you have this problem with Azure IR, then you can consider using the ODBC connector to connect and see if it works (I assume you are on DB2 connector):</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-odbc?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-odbc?tabs=data-factory</a></p>
"
"75589911","ODBC connection issue to load data. Copy Activity source not working","<p>I am using odbc connector to connect to AS400 DB2 system. I am fetching metadata from copy activity and it is throwing the following error.</p>
<p>Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [IBM][System i Access ODBC Driver][DB2 for i5/OS] - Error message text unavailable. Message can not be translated successfully.,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [IBM][System i Access ODBC Driver][DB2 for i5/OS] - Error message text unavailable. Message can not be translated successfully.,Source=CWBODBC.DLL,'</p>
<p>We have read access to the database. Is this access error for the given user? Or network connectivity issue because some copy activity runs while some fail. We are trying to load the data in data lake and everything is parameterized. Let me know if this is a library for Windows issue/Network issue/access issue.?</p>
","<azure><db2><azure-data-factory><ibm-midrange><azure-data-lake>","2023-02-28 09:07:08","101","0","2","75601430","<blockquote>
<p>It looks like transient issue related to network or driver may be. Its
not looking like amy permission related issue. Are you using Self
hosted IR here? If yes could you please try to reinstall it or upgrade
it and see if that helps.</p>
</blockquote>
"
"75585431","json column with nested values","<p>I have a JSON type SQL column in SQL table as below example. I want the below code to be converted into separate columns such as drugs as table name and other attribute as column name, how can I use adf or any other means please guide. The below code is a example of table called report where I need to convert this into separate columns .</p>
<pre><code>{
    &quot;drugs&quot;: {
        &quot;Codeine&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Codeine&quot;,
            &quot;icons&quot;: [
                93,
                100,
                103
            ],
            &quot;drug_id&quot;: 36,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;prodrug&quot;,
                &quot;text&quot;: &quot;is **inactive**, its metabolites are active.&quot;
            },
            &quot;targets&quot;: [
                &quot;OPRM1&quot;
            ],
            &quot;rxnorm_id&quot;: &quot;2670&quot;,
            &quot;priclasses&quot;: [
                &quot;Analgesic/Anesthesiology&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                9,
                17,
                29,
                30,
                159,
                171
            ],
            &quot;subclasses&quot;: [
                &quot;Analgesic agent&quot;,
                &quot;Antitussive agent&quot;,
                &quot;Opioid agonist&quot;,
                &quot;Phenanthrene &quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;OPRM1&quot;: &quot;78&quot;,
                &quot;metabolic&quot;: &quot;6&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2D6&quot;,
                &quot;OPRM1&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Normal to reduced morphine formation. Use label recommended age- or weight-specific dosing. If no response, may need to consider alternative analgesics such as morphine or a non-opioid.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 103
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Predicted codeine metabolism is reduced.&quot;,
                    &quot;icons_id&quot;: 58,
                    &quot;sort_key&quot;: 58,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible decrease in exposure to the active metabolite(s) of codeine.&quot;,
                    &quot;icons_id&quot;: 93,
                    &quot;sort_key&quot;: 56,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Decreased analgesic effects due to OPRM1 genotype.&quot;,
                    &quot;icons_id&quot;: 100,
                    &quot;sort_key&quot;: 52,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of codeine in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Dapsone&quot;: {
            &quot;bin&quot;: &quot;X&quot;,
            &quot;name&quot;: &quot;Dapsone&quot;,
            &quot;icons&quot;: [
                99
            ],
            &quot;drug_id&quot;: 514,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3108&quot;,
            &quot;priclasses&quot;: [
                &quot;Infectious disease&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Miscellaneous antibiotic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Hemolysis and Heinz body formation may be exaggerated in individuals with a glucose-6-phosphate dehydrogenase (G6PD) deficiency, or methemoglobin reductase deficiency, or hemoglobin M. This reaction is frequently dose-related. Dapsone should be given with caution to these patients or if the patient is exposed to other agents or conditions such as infection or diabetic ketosis capable of producing hemolysis. Drugs or chemicals which have produced significant hemolysis in G6PD or methemoglobin reductase deficient patients include dapsone, sulfanilamide, nitrite, aniline, phenylhydrazine, napthalene, niridazole, nitro-furantoin and 8-amino-antimalarials such as primaquine. Toxic hepatitis and cholestatic jaundice have been reported early in therapy. Hyperbilirubinemia may occur more often in G6PD deficient patients. When feasible, baseline and subsequent monitoring of liver function is recommended; if abnormal, dapsone should be discontinued until the source of the abnormality is established.&quot;,
                    &quot;source&quot;: &quot;FDA - Additional testing&quot;,
                    &quot;guidelines_id&quot;: 453
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;According to FDA labeling, additional laboratory testing may be indicated.&quot;,
                    &quot;icons_id&quot;: 99,
                    &quot;sort_key&quot;: 51,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Digoxin&quot;: {
            &quot;bin&quot;: &quot;B&quot;,
            &quot;name&quot;: &quot;Digoxin&quot;,
            &quot;icons&quot;: [],
            &quot;drug_id&quot;: 47,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;nometab&quot;,
                &quot;text&quot;: &quot;is not significantly metabolized, or not absorbed.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3407&quot;,
            &quot;priclasses&quot;: [
                &quot;Cardiovascular&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Antiarrhythmic agent&quot;,
                &quot;Cardiac glycoside&quot;,
                &quot;Miscellaneous antiarrhythmic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;All of digoxin's actions are mediated through its effects on Na-K ATPase. Up to 70% of the dose is excreted unchanged in the urine and only a small percentage (13%) of a dose of digoxin is metabolized in healthy volunteers. The metabolism of digoxin is not dependent upon the cytochrome P-450 system, and digoxin is not known to induce or inhibit the cytochrome P-450 system. Digoxin is a substrate for P-glycoprotein (P-gp). As an efflux protein on the apical membrane of enterocytes and of renal tubular cells, P-glycoprotein may limit the absorption and enhance the excretion of digoxin, respectively. Studies have suggested that variants of the ABCB1 gene (encoding P-gp) may influence digoxin serum levels; however, changes in digoxin exposure related to ABCB1 genotype are generally small (10-30%), account only for approximately 10% of the variability seen in digoxin pharmacokinetics, and are unlikely to be clinically significant.&quot;,
                    &quot;icons_id&quot;: 60,
                    &quot;sort_key&quot;: 1,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Doxepin&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Doxepin&quot;,
            &quot;icons&quot;: [
                92,
                103
            ],
            &quot;drug_id&quot;: 452,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3638&quot;,
            &quot;priclasses&quot;: [
                &quot;Psychiatry&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                50
            ],
            &quot;subclasses&quot;: [
                &quot;Antidepressant&quot;,
                &quot;Tricyclic antidepressant&quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;metabolic&quot;: &quot;5&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Certain drugs inhibit the activity of CYP2D6 and make normal metabolizers resemble poor metabolizers. An individual who is stable on a given dose of TCA may become abruptly toxic when given one of these inhibiting drugs as concomitant therapy. Concomitant use of tricyclic antidepressants with drugs that can inhibit cytochrome P450 2D6 may require lower doses than usually prescribed for either the tricyclic antidepressant or the other drug. Furthermore, whenever one of these other drugs is withdrawn from co-therapy, an increased dose of tricyclic antidepressant may be required. It is desirable to monitor TCA plasma levels whenever a TCA is going to be coadministered with another drug known to be an inhibitor of cytochrome P450 2D6.&quot;,
                    &quot;source&quot;: &quot;FDA&quot;,
                    &quot;guidelines_id&quot;: 349
                },
                {
                    &quot;text&quot;: &quot;A 25% reduction of recommended starting dose may need to be considered. Utilize therapeutic drug monitoring to guide dose adjustments.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 66
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible increase in exposure to doxepin.&quot;,
                    &quot;icons_id&quot;: 92,
                    &quot;sort_key&quot;: 57,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of doxepin in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]

</code></pre>
<p>The expected output i need is</p>
<p>Table name : drugs</p>
<p>bin
name
n so on for other values</p>
","<sql><json><sql-server><azure-data-factory>","2023-02-27 20:49:36","71","0","3","75585568","<p>You can use openjson to shred the document into smaller pieces:</p>
<pre class=""lang-sql prettyprint-override""><code>DECLARE @json NVARCHAR(MAX) = 'your json'

select json_value(x.value,'$.name') AS name
  , json_value(x.value,'$.bin') AS bin
  , json_value(x.value,'$.drug_id') AS drugId
  , json_value(x.value,'$.pathway.code') AS pathcode
  , json_value(x.value,'$.icons[0]') icon1
from openjson(@json, '$.drugs') x

</code></pre>
<p>Some of your properties are arrays etc, and those are a bit harder to put into single columns, so you got some work for you. You might wanna read about JSON_VALUE, JSON_QUERY to figure out what exactly you might need</p>
"
"75585431","json column with nested values","<p>I have a JSON type SQL column in SQL table as below example. I want the below code to be converted into separate columns such as drugs as table name and other attribute as column name, how can I use adf or any other means please guide. The below code is a example of table called report where I need to convert this into separate columns .</p>
<pre><code>{
    &quot;drugs&quot;: {
        &quot;Codeine&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Codeine&quot;,
            &quot;icons&quot;: [
                93,
                100,
                103
            ],
            &quot;drug_id&quot;: 36,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;prodrug&quot;,
                &quot;text&quot;: &quot;is **inactive**, its metabolites are active.&quot;
            },
            &quot;targets&quot;: [
                &quot;OPRM1&quot;
            ],
            &quot;rxnorm_id&quot;: &quot;2670&quot;,
            &quot;priclasses&quot;: [
                &quot;Analgesic/Anesthesiology&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                9,
                17,
                29,
                30,
                159,
                171
            ],
            &quot;subclasses&quot;: [
                &quot;Analgesic agent&quot;,
                &quot;Antitussive agent&quot;,
                &quot;Opioid agonist&quot;,
                &quot;Phenanthrene &quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;OPRM1&quot;: &quot;78&quot;,
                &quot;metabolic&quot;: &quot;6&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2D6&quot;,
                &quot;OPRM1&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Normal to reduced morphine formation. Use label recommended age- or weight-specific dosing. If no response, may need to consider alternative analgesics such as morphine or a non-opioid.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 103
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Predicted codeine metabolism is reduced.&quot;,
                    &quot;icons_id&quot;: 58,
                    &quot;sort_key&quot;: 58,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible decrease in exposure to the active metabolite(s) of codeine.&quot;,
                    &quot;icons_id&quot;: 93,
                    &quot;sort_key&quot;: 56,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Decreased analgesic effects due to OPRM1 genotype.&quot;,
                    &quot;icons_id&quot;: 100,
                    &quot;sort_key&quot;: 52,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of codeine in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Dapsone&quot;: {
            &quot;bin&quot;: &quot;X&quot;,
            &quot;name&quot;: &quot;Dapsone&quot;,
            &quot;icons&quot;: [
                99
            ],
            &quot;drug_id&quot;: 514,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3108&quot;,
            &quot;priclasses&quot;: [
                &quot;Infectious disease&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Miscellaneous antibiotic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Hemolysis and Heinz body formation may be exaggerated in individuals with a glucose-6-phosphate dehydrogenase (G6PD) deficiency, or methemoglobin reductase deficiency, or hemoglobin M. This reaction is frequently dose-related. Dapsone should be given with caution to these patients or if the patient is exposed to other agents or conditions such as infection or diabetic ketosis capable of producing hemolysis. Drugs or chemicals which have produced significant hemolysis in G6PD or methemoglobin reductase deficient patients include dapsone, sulfanilamide, nitrite, aniline, phenylhydrazine, napthalene, niridazole, nitro-furantoin and 8-amino-antimalarials such as primaquine. Toxic hepatitis and cholestatic jaundice have been reported early in therapy. Hyperbilirubinemia may occur more often in G6PD deficient patients. When feasible, baseline and subsequent monitoring of liver function is recommended; if abnormal, dapsone should be discontinued until the source of the abnormality is established.&quot;,
                    &quot;source&quot;: &quot;FDA - Additional testing&quot;,
                    &quot;guidelines_id&quot;: 453
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;According to FDA labeling, additional laboratory testing may be indicated.&quot;,
                    &quot;icons_id&quot;: 99,
                    &quot;sort_key&quot;: 51,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Digoxin&quot;: {
            &quot;bin&quot;: &quot;B&quot;,
            &quot;name&quot;: &quot;Digoxin&quot;,
            &quot;icons&quot;: [],
            &quot;drug_id&quot;: 47,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;nometab&quot;,
                &quot;text&quot;: &quot;is not significantly metabolized, or not absorbed.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3407&quot;,
            &quot;priclasses&quot;: [
                &quot;Cardiovascular&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Antiarrhythmic agent&quot;,
                &quot;Cardiac glycoside&quot;,
                &quot;Miscellaneous antiarrhythmic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;All of digoxin's actions are mediated through its effects on Na-K ATPase. Up to 70% of the dose is excreted unchanged in the urine and only a small percentage (13%) of a dose of digoxin is metabolized in healthy volunteers. The metabolism of digoxin is not dependent upon the cytochrome P-450 system, and digoxin is not known to induce or inhibit the cytochrome P-450 system. Digoxin is a substrate for P-glycoprotein (P-gp). As an efflux protein on the apical membrane of enterocytes and of renal tubular cells, P-glycoprotein may limit the absorption and enhance the excretion of digoxin, respectively. Studies have suggested that variants of the ABCB1 gene (encoding P-gp) may influence digoxin serum levels; however, changes in digoxin exposure related to ABCB1 genotype are generally small (10-30%), account only for approximately 10% of the variability seen in digoxin pharmacokinetics, and are unlikely to be clinically significant.&quot;,
                    &quot;icons_id&quot;: 60,
                    &quot;sort_key&quot;: 1,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Doxepin&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Doxepin&quot;,
            &quot;icons&quot;: [
                92,
                103
            ],
            &quot;drug_id&quot;: 452,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3638&quot;,
            &quot;priclasses&quot;: [
                &quot;Psychiatry&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                50
            ],
            &quot;subclasses&quot;: [
                &quot;Antidepressant&quot;,
                &quot;Tricyclic antidepressant&quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;metabolic&quot;: &quot;5&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Certain drugs inhibit the activity of CYP2D6 and make normal metabolizers resemble poor metabolizers. An individual who is stable on a given dose of TCA may become abruptly toxic when given one of these inhibiting drugs as concomitant therapy. Concomitant use of tricyclic antidepressants with drugs that can inhibit cytochrome P450 2D6 may require lower doses than usually prescribed for either the tricyclic antidepressant or the other drug. Furthermore, whenever one of these other drugs is withdrawn from co-therapy, an increased dose of tricyclic antidepressant may be required. It is desirable to monitor TCA plasma levels whenever a TCA is going to be coadministered with another drug known to be an inhibitor of cytochrome P450 2D6.&quot;,
                    &quot;source&quot;: &quot;FDA&quot;,
                    &quot;guidelines_id&quot;: 349
                },
                {
                    &quot;text&quot;: &quot;A 25% reduction of recommended starting dose may need to be considered. Utilize therapeutic drug monitoring to guide dose adjustments.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 66
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible increase in exposure to doxepin.&quot;,
                    &quot;icons_id&quot;: 92,
                    &quot;sort_key&quot;: 57,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of doxepin in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]

</code></pre>
<p>The expected output i need is</p>
<p>Table name : drugs</p>
<p>bin
name
n so on for other values</p>
","<sql><json><sql-server><azure-data-factory>","2023-02-27 20:49:36","71","0","3","75586017","<p>This is a quick example of how to parse json and nested arrays. You just have to OUTER APPLY OPENJSON() on the nested array and define the columns with WITH</p>
<pre><code>    SELECT 
        main.ID
        , Owner.email
    FROM 
        tblWithJson aj
        OUTER APPLY OPENJSON(aj.JsonColumnName)
    WITH
        (
            ID INT 'strict $.ID'
            , Owner NVARCHAR(MAX) '$.ActualNameOfElementFromNestedArray' AS JSON  --nested array
        ) AS main
    OUTER APPLY OPENJSON(Owner)
    WITH
        (
             Email VARCHAR(255) ' $.Email'
        ) AS Owner
    ;
</code></pre>
"
"75585431","json column with nested values","<p>I have a JSON type SQL column in SQL table as below example. I want the below code to be converted into separate columns such as drugs as table name and other attribute as column name, how can I use adf or any other means please guide. The below code is a example of table called report where I need to convert this into separate columns .</p>
<pre><code>{
    &quot;drugs&quot;: {
        &quot;Codeine&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Codeine&quot;,
            &quot;icons&quot;: [
                93,
                100,
                103
            ],
            &quot;drug_id&quot;: 36,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;prodrug&quot;,
                &quot;text&quot;: &quot;is **inactive**, its metabolites are active.&quot;
            },
            &quot;targets&quot;: [
                &quot;OPRM1&quot;
            ],
            &quot;rxnorm_id&quot;: &quot;2670&quot;,
            &quot;priclasses&quot;: [
                &quot;Analgesic/Anesthesiology&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                9,
                17,
                29,
                30,
                159,
                171
            ],
            &quot;subclasses&quot;: [
                &quot;Analgesic agent&quot;,
                &quot;Antitussive agent&quot;,
                &quot;Opioid agonist&quot;,
                &quot;Phenanthrene &quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;OPRM1&quot;: &quot;78&quot;,
                &quot;metabolic&quot;: &quot;6&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2D6&quot;,
                &quot;OPRM1&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Normal to reduced morphine formation. Use label recommended age- or weight-specific dosing. If no response, may need to consider alternative analgesics such as morphine or a non-opioid.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 103
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Predicted codeine metabolism is reduced.&quot;,
                    &quot;icons_id&quot;: 58,
                    &quot;sort_key&quot;: 58,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible decrease in exposure to the active metabolite(s) of codeine.&quot;,
                    &quot;icons_id&quot;: 93,
                    &quot;sort_key&quot;: 56,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Decreased analgesic effects due to OPRM1 genotype.&quot;,
                    &quot;icons_id&quot;: 100,
                    &quot;sort_key&quot;: 52,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of codeine in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Dapsone&quot;: {
            &quot;bin&quot;: &quot;X&quot;,
            &quot;name&quot;: &quot;Dapsone&quot;,
            &quot;icons&quot;: [
                99
            ],
            &quot;drug_id&quot;: 514,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3108&quot;,
            &quot;priclasses&quot;: [
                &quot;Infectious disease&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Miscellaneous antibiotic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Hemolysis and Heinz body formation may be exaggerated in individuals with a glucose-6-phosphate dehydrogenase (G6PD) deficiency, or methemoglobin reductase deficiency, or hemoglobin M. This reaction is frequently dose-related. Dapsone should be given with caution to these patients or if the patient is exposed to other agents or conditions such as infection or diabetic ketosis capable of producing hemolysis. Drugs or chemicals which have produced significant hemolysis in G6PD or methemoglobin reductase deficient patients include dapsone, sulfanilamide, nitrite, aniline, phenylhydrazine, napthalene, niridazole, nitro-furantoin and 8-amino-antimalarials such as primaquine. Toxic hepatitis and cholestatic jaundice have been reported early in therapy. Hyperbilirubinemia may occur more often in G6PD deficient patients. When feasible, baseline and subsequent monitoring of liver function is recommended; if abnormal, dapsone should be discontinued until the source of the abnormality is established.&quot;,
                    &quot;source&quot;: &quot;FDA - Additional testing&quot;,
                    &quot;guidelines_id&quot;: 453
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;According to FDA labeling, additional laboratory testing may be indicated.&quot;,
                    &quot;icons_id&quot;: 99,
                    &quot;sort_key&quot;: 51,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Digoxin&quot;: {
            &quot;bin&quot;: &quot;B&quot;,
            &quot;name&quot;: &quot;Digoxin&quot;,
            &quot;icons&quot;: [],
            &quot;drug_id&quot;: 47,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;nometab&quot;,
                &quot;text&quot;: &quot;is not significantly metabolized, or not absorbed.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3407&quot;,
            &quot;priclasses&quot;: [
                &quot;Cardiovascular&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Antiarrhythmic agent&quot;,
                &quot;Cardiac glycoside&quot;,
                &quot;Miscellaneous antiarrhythmic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;All of digoxin's actions are mediated through its effects on Na-K ATPase. Up to 70% of the dose is excreted unchanged in the urine and only a small percentage (13%) of a dose of digoxin is metabolized in healthy volunteers. The metabolism of digoxin is not dependent upon the cytochrome P-450 system, and digoxin is not known to induce or inhibit the cytochrome P-450 system. Digoxin is a substrate for P-glycoprotein (P-gp). As an efflux protein on the apical membrane of enterocytes and of renal tubular cells, P-glycoprotein may limit the absorption and enhance the excretion of digoxin, respectively. Studies have suggested that variants of the ABCB1 gene (encoding P-gp) may influence digoxin serum levels; however, changes in digoxin exposure related to ABCB1 genotype are generally small (10-30%), account only for approximately 10% of the variability seen in digoxin pharmacokinetics, and are unlikely to be clinically significant.&quot;,
                    &quot;icons_id&quot;: 60,
                    &quot;sort_key&quot;: 1,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Doxepin&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Doxepin&quot;,
            &quot;icons&quot;: [
                92,
                103
            ],
            &quot;drug_id&quot;: 452,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3638&quot;,
            &quot;priclasses&quot;: [
                &quot;Psychiatry&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                50
            ],
            &quot;subclasses&quot;: [
                &quot;Antidepressant&quot;,
                &quot;Tricyclic antidepressant&quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;metabolic&quot;: &quot;5&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Certain drugs inhibit the activity of CYP2D6 and make normal metabolizers resemble poor metabolizers. An individual who is stable on a given dose of TCA may become abruptly toxic when given one of these inhibiting drugs as concomitant therapy. Concomitant use of tricyclic antidepressants with drugs that can inhibit cytochrome P450 2D6 may require lower doses than usually prescribed for either the tricyclic antidepressant or the other drug. Furthermore, whenever one of these other drugs is withdrawn from co-therapy, an increased dose of tricyclic antidepressant may be required. It is desirable to monitor TCA plasma levels whenever a TCA is going to be coadministered with another drug known to be an inhibitor of cytochrome P450 2D6.&quot;,
                    &quot;source&quot;: &quot;FDA&quot;,
                    &quot;guidelines_id&quot;: 349
                },
                {
                    &quot;text&quot;: &quot;A 25% reduction of recommended starting dose may need to be considered. Utilize therapeutic drug monitoring to guide dose adjustments.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 66
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible increase in exposure to doxepin.&quot;,
                    &quot;icons_id&quot;: 92,
                    &quot;sort_key&quot;: 57,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of doxepin in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]

</code></pre>
<p>The expected output i need is</p>
<p>Table name : drugs</p>
<p>bin
name
n so on for other values</p>
","<sql><json><sql-server><azure-data-factory>","2023-02-27 20:49:36","71","0","3","75586587","<p>You can use <a href=""https://zed.brimdata.io/docs/install"" rel=""nofollow noreferrer"">zq</a> and the <a href=""https://zed.brimdata.io/docs/language"" rel=""nofollow noreferrer"">Zed language</a> to transform your data into a table format. Once it's in a neat table you can easily insert it into a schema-defined table in a database. You will have to figure out what to do with the arrays though. Some dbs support arrays but others do not.</p>
<pre><code>over drugs
| {name: key[0], ...value} 
| over flatten(this) =&gt; ( {key: join(key, &quot;.&quot;), value} 
    | collect(this) 
    | yield collect
  ) 
| unflatten(this) 
| fuse
</code></pre>
<p>That will output data that looks like so:</p>
<p><a href=""https://i.stack.imgur.com/giBBu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/giBBu.png"" alt=""zui"" /></a></p>
<p>You can either download the <a href=""https://github.com/brimdata/zui-insiders"" rel=""nofollow noreferrer"">Zui Desktop App</a> and use it to transform your data into a csv file. Or you can use the zq command line tool. The command for zq would be:</p>
<pre><code>zq -f csv 'over drugs
| {name: key[0], ...value}
| over flatten(this) =&gt; ( {key: join(key, &quot;.&quot;), value}
    | collect(this)
    | yield collect
  )
| unflatten(this)
| fuse' json-column-with-nested-values.json
</code></pre>
"
"75584988","Dynamic Lookup Activity and split function ADF","<p>I am trying to run a select * query for table having where clause as tablename.
The table name is coming from pipelinename, so want to use split function.
<a href=""https://i.stack.imgur.com/3mihC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3mihC.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-02-27 19:53:50","50","0","1","75588720","<p>Based on the above picture, you are giving the query in SQL. Your expression is correct but give the query in <strong>lookup activity query</strong> to get the desired result.</p>
<p>My Example:</p>
<p><img src=""https://i.imgur.com/CItHPds.png"" alt=""enter image description here"" /></p>
<p>Here, <code>sample1</code> is my SQL table name and <code>pipeline_sample1</code> is my pipeline name. I have given the below sample expression.</p>
<p><code>select * from @{split(pipeline().Pipeline,'_')[1]}</code></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/ohvYG2f.png"" alt=""enter image description here"" /></p>
"
"75583423","How to use ForEach Loop for DataBricks Notebook Json object output","<p>I am trying to fetch the notebook value(json value) to loop ForEach activity. how do we use this approach. also my goal is ForEach activity needs to all lookup activity to fetch some details from azure SQL DB to concatenate the values to send another notebook.</p>
<p>Kindly advice and possibility way to integrate.</p>
<p>sample JSON out from notebook</p>
<pre><code>{
    &quot;runPageUrl&quot;: &quot;https://adb-url&quot;,
    &quot;runOutput&quot;: {
        &quot;value&quot;: {
            &quot;xxxx&quot;: 60,
            &quot;mmmm&quot;: &quot;value&quot;,
            &quot;db&quot;: &quot;dbname&quot;,
            &quot;table&quot;: &quot;tablename&quot;,
            &quot;asset_id&quot;: &quot;1cda102e-dddddxxxxx9c775&quot;,
            &quot;aggvalue&quot;: [
                &quot;value&quot;
            ],
            &quot;date_val&quot;: [
                &quot;date_val&quot;
            ]
        },
        &quot;vlue2&quot;: &quot;value&quot;: {
            &quot;xxxx&quot;: 60,
            &quot;mmmm&quot;: &quot;value&quot;,
            &quot;db&quot;: &quot;dbname&quot;,
            &quot;table&quot;: &quot;tablename&quot;,
            &quot;asset_id&quot;: &quot;1cda102e-dddddxxxxx9c775&quot;,
            &quot;aggvalue&quot;: [
                &quot;value&quot;
            ],
            &quot;date_val&quot;: [
                &quot;date_val&quot;
            ]
        },
        
    &quot;effectiveIntegrationRuntime&quot;: &quot;AutoResolveIntegrationRuntime&quot;,
    &quot;executionDuration&quot;: 297,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;ExternalActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.3343434,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    } }
</code></pre>
","<azure-data-factory><azure-databricks>","2023-02-27 17:01:49","141","0","1","75592116","<p>I tried your JSON in my environment and found that your JSON is not valid with below error in databricks.</p>
<p><img src=""https://i.imgur.com/bAqEUAe.png"" alt=""enter image description here"" /></p>
<p>Then I used below JSON.</p>
<pre><code>{
    &quot;value&quot;: {
        &quot;xxxx&quot;: 60,
        &quot;mmmm&quot;: &quot;value&quot;,
        &quot;db&quot;: &quot;dbname&quot;,
        &quot;table&quot;: &quot;tablename&quot;,
        &quot;asset_id&quot;: &quot;1cda102e-dddddxxxxx9c775&quot;,
        &quot;aggvalue&quot;: [
            &quot;value&quot;
        ],
        &quot;date_val&quot;: [
            &quot;date_val&quot;
        ]
    },
    &quot;value&quot;: {
        &quot;xxxx&quot;: 60,
        &quot;mmmm&quot;: &quot;value&quot;,
        &quot;db&quot;: &quot;dbname&quot;,
        &quot;table&quot;: &quot;tablename&quot;,
        &quot;asset_id&quot;: &quot;1cda102e-dddddxxxxx9c775&quot;,
        &quot;aggvalue&quot;: [
            &quot;value&quot;
        ],
        &quot;date_val&quot;: [
            &quot;date_val&quot;
        ]
    }}
</code></pre>
<p>When I execute the above JSON, you can see the same key <code>value</code> is updated.</p>
<p><img src=""https://i.imgur.com/uv21i47.png"" alt=""enter image description here"" /></p>
<p>When I pass the same JSON to ADF pipeline using <code>dbutils.notebook.exit(data)</code> and I have used the <code>@activity('Notebook1').output.runOutput</code> dynamic content in the ForEach. You can see I got same error as yours.</p>
<p><img src=""https://i.imgur.com/28CHqwq.png"" alt=""enter image description here"" /></p>
<p>As the error suggests, ForEach only supports array or string to iterate.</p>
<p>So, change your JSON like below (array of objects) and pass it to the ForEach which is good for iterating.</p>
<pre><code>[
{
    &quot;xxxx&quot;: 60,
    &quot;mmmm&quot;: &quot;value&quot;,
    &quot;db&quot;: &quot;dbname&quot;,
    &quot;table&quot;: &quot;tablename&quot;,
    &quot;asset_id&quot;: &quot;1cda102e-dddddxxxxx9c775&quot;,
    &quot;aggvalue&quot;: [&quot;value&quot;],
    &quot;date_val&quot;: [&quot;date_val&quot;]
},
{
    &quot;xxxx&quot;: 60,
    &quot;mmmm&quot;: &quot;value&quot;,
    &quot;db&quot;: &quot;dbname&quot;,
    &quot;table&quot;: &quot;tablename&quot;,
    &quot;asset_id&quot;: &quot;1cda102e-dddddxxxxx9c775&quot;,
    &quot;aggvalue&quot;: [&quot;value&quot;],
    &quot;date_val&quot;: [&quot;date_val&quot;]
}
]
</code></pre>
<p><img src=""https://i.imgur.com/E8Dul7n.png"" alt=""enter image description here"" /></p>
<p>Coming to using lookup values, <strong>ForEach only supports iterating one array(Notebook output)</strong>. So, if you want to concatenate the lookup values also inside this ForEach, both Notebook output array length and lookup output array length should be same.</p>
<p>Then you can iterate two arrays at a time by using an index array with ForEach activity.
To understand about iterating about 2 arrays at same time go through this <a href=""https://stackoverflow.com/questions/74701965/adf-passing-more-than-one-array-paramater-to-logicapps/74713105#74713105"">SO answer</a>.</p>
"
"75581667","Parquet File from Data Lakes to SQL Pool - Arithmetic overflow converting tinyint to TINYINT","<p>We're trying to import a parquet File from Data Lakes to the Azure dedicated SQL Pool but getting an error randomly.</p>
<p>The error is:</p>
<p>HadoopSqlException: Arithmetic overflow error converting tinyint to data type TINYINT</p>
<p>In Synapse Studio, it's a simple copy task with the source as the parquet file and the sink as the SQL Pool table. We have several TINYINT columns. In the Parquet file, it is represented as INT32 BUT all of the values fit in TINYINT. So why is this error happening?</p>
","<azure-data-factory><azure-synapse><azure-synapse-analytics>","2023-02-27 14:22:10","87","0","1","75588748","<p>I tried to reproduce the same error.</p>
<p><img src=""https://i.imgur.com/XoiUO4A.png"" alt=""enter image description here"" /></p>
<p>You're presumably attempting to enter data into a table when its 'tinyint' column has hit its data type's maximum if you receive the error message, &quot;<strong><em>Arithmetic overflow error converting tinyint to data type TINYINT</em></strong>.&quot;</p>
<ul>
<li>In this instance, the <code>tinyint</code> data type, <strong>which has a range of 0 to 255</strong> and has numbers above this maximum in your file, is used by my <code>tinyint</code> column.</li>
<li>In data column mapping of file, it is taking as <code>int32</code>  even it has values greater than 255</li>
</ul>
<p>To resolve this convert column datatype of column to <code>bigint</code> or <code>int</code> using <strong>pre-copy script</strong>.</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE table_name
ALTER COLUMN column_name INT;
</code></pre>
<p><img src=""https://i.imgur.com/eRTySbE.png"" alt=""enter image description here"" /></p>
<p>Now the pipeline is running successfully.</p>
<p><img src=""https://i.imgur.com/ycX71yI.png"" alt=""enter image description here"" /></p>
"
"75579876","Azure Python SDK for DataFactory to point to a specific GIT branch","<p>I'm trying to update the Datasets programatically using <code>azure-mgmt-datafactory</code> SDK. I'm able to connect to the DataFactory, list and update datasets to the adf_publish/live mode. However, I'm interested to connect to my GIT branch first and update. Below is the sample code</p>
<pre><code>from azure.identity import ClientSecretCredential
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *
import time

subscription_id = 'xxxxx'

credentials = ClientSecretCredential(
    tenant_id='xxxx',
    client_id='xxxx',
    client_secret='xxxx'
)

adf_client = DataFactoryManagementClient(credentials, subscription_id)

# How to connect to my branch?
response = adf_client.datasets.get('xxxxxx',
                        'adf-xxx',
                        'DS_DEV_OP_XXX')

# This is giving the dataset not from my branch
print(response.as_dict())
</code></pre>
<p>Could you please let me know as to how to connect to my GIT branch?</p>
<p>Thanks in advance.</p>
","<python-3.x><azure><azure-data-factory>","2023-02-27 11:28:16","115","0","1","75605246","<p><strong>Option1:</strong></p>
<p>There is no direct method to connect to a specific Git branch in the Azure-mgmt-datafactory SDK. However, you can obtain the dataset from the Data Factory using the get method of the datasets class. The resource group name, the Data Factory name, and the dataset name are the three parameters required by the get method.
You are retrieving the dataset DS DEV OP XXX from the Data Factory adf-xxx using the get method in your code. The dataset's most recent version, which might not be from your Git branch, is obtained using the get method. You must first check out the branch in your local repository in order to retrieve the dataset from a specific Git branch, and then you must access the Data Factory.</p>
<p><strong>Option:2</strong></p>
<p>We're going to employ Azure DevOps. The procedures will be the same if you use GitHub.</p>
<p><a href=""https://i.stack.imgur.com/96KHR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/96KHR.png"" alt=""enter image description here"" /></a></p>
<p>Go to Azure Data factory Studio-&gt;Manage-&gt;Git configuration-&gt;configure</p>
<p><a href=""https://i.stack.imgur.com/Hb1yy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hb1yy.png"" alt=""enter image description here"" /></a></p>
<p>Select Repository type: Azure DevOps Git, Azure Active Directory: “AAD Name”</p>
<p><a href=""https://i.stack.imgur.com/EB0oL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EB0oL.png"" alt=""enter image description here"" /></a></p>
<p>Give all the required details like Project name, Repo name, Branch etc.</p>
<p><a href=""https://i.stack.imgur.com/odJ58.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/odJ58.jpg"" alt=""enter image description here"" /></a></p>
<p>Now we can check all the data like Dataset, dataflow, linkedService etc.
Go to Azure Data Factory Studio. Select 'Author'.</p>
<ol>
<li>Now we see 'dev' branch is selected by default.</li>
<li>If we do new developments in a feature branch. Click &quot;New branch&quot;. We can save our incomplete development in this way. It is crucial to keep in mind that in order to return the code to the &quot;dev&quot; branch after testing the pipeline, we must &quot;Create pull request.&quot;</li>
<li>Our modifications can all be &quot;Published.&quot; With this, the new modifications will become &quot;live&quot; and the pipelines in the &quot;dev&quot; branch will be pushed into the &quot;adf publish dev&quot; branch.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/06YMK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/06YMK.jpg"" alt=""enter image description here"" /></a></p>
"
"75579533","ADF Industries Best Practices","<p>ADF Industries Best Practices  with data size wise flow time with number of records
I need an table with Industries Best Practices for each data load
for eg :
data size : 1 GB
number of records : 1 millions
what is the flow time of an pipeline</p>
<p>Source : SAP
Destination : SQL Synapse</p>
","<azure-data-factory><azure-data-lake-gen2>","2023-02-27 10:54:38","59","0","1","75580223","<p>If you are talking about optimizing dataflow performance, here is your documentation:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance</a></p>
<p>I dont think there is a table with datasize and how many rows and an estimated speed and time to completion because each environment and data is structured differently.</p>
"
"75576527","Issue in Connecting ADF to snowflake","<p>I am trying to connect to snowflake using Azure Data Factory using Self-hosted integration run time. I am getting the below error. Does anyone know how to resolve this?</p>
<blockquote>
<p>ERROR [HY000] [Microsoft][Snowflake] (4) REST request for URL  failed:
CURLerror (curl_easy_perform() failed) - code=35 msg='SSL connect
error' osCode=9 osMsg='Bad file descriptor'.</p>
</blockquote>
","<snowflake-cloud-data-platform><odbc><azure-data-factory>","2023-02-27 03:24:24","140","0","1","75593321","<p>ERROR [ HY000 ] [Microsoft][Snowflake] (4) REST request for URL failed: CURLerror (curl_easy_perform() failed) - code=35 msg='SSL connect error' osCode=9 osMsg='Bad file descriptor'.</p>
<p>Make sure to add IP address of Self hosted Integration runtime server to the allowed list in Snowflake.   Below are the recommendations to avoid this error from Snowflake.</p>
<blockquote>
<ul>
<li>Make sure to bypass the endpoints found from <code>SYSTEM$ALLOWLIST </code> [0] and <code>SYSTEM$ALLOWLIST_PRIVATELINK</code> [1].</li>
<li>Bypass the S3 bucket URL [the ones that are seen in the error stack of the error thrown ].</li>
</ul>
</blockquote>
<p>Refer <a href=""https://community.snowflake.com/s/article/CURLerror-SSL-peer-certificate-or-SSH-remote-key-was-not-OK-during"" rel=""nofollow noreferrer"">CURLerror 'SSL peer certificate or SSH remote key was not OK' using ODBC driver connection to Snowflake- Snowflake documention</a> for more details.</p>
<p>[0] <a href=""https://docs.snowflake.com/en/sql-reference/functions/system_allowlist"" rel=""nofollow noreferrer"">SYSTEM$ALLOWLIST | Snowflake Documentation</a></p>
<p>[1] <a href=""https://docs.snowflake.com/en/sql-reference/functions/system_allowlist_privatelink"" rel=""nofollow noreferrer"">SYSTEM$ALLOWLIST_PRIVATELINK | Snowflake Documentation</a></p>
"
"75573923","ADF Pipeline Copy data ""BadRequest"" error - SQL to Parquet using expression builder for Mapping","<p>I get a very non-explanatory error running this pipeline:
{
&quot;code&quot;: &quot;BadRequest&quot;,
&quot;message&quot;: null,
&quot;target&quot;: &quot;pipeline//runid/xxx&quot;,
&quot;details&quot;: null,
&quot;error&quot;: null
}</p>
<p>Environment: Azure Synapse Analytics (ADF v2)</p>
<p>Activity: Copy data</p>
<p>Source: SQL Server and table with approx. 95 columns - with Parquet incompatible characters like white space and ()</p>
<p>Sink: Datalake gen2 Parquet</p>
<p>Mapping: Dynamic with attached (reduced columns to fulfill body limit in post) expression to &quot;rename&quot; columns to Parquet compatible naming.</p>
<p>Any good ideas? I am giving up finding an answer on the different fora :(</p>
<p>Working expression with 24 columns:</p>
<pre><code>@json('{
            &quot;type&quot;: &quot;TabularTranslator&quot;,
            &quot;mappings&quot;: [
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Posting Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Posting_Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Document No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Document_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Location Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Location_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Inventory Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Inventory_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Valued Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Valued_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Invoiced Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Invoiced_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Cost per Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Cost_per_Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Sales Amount (Actual)&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Sales_Amount__Actual_&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch_ Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch__Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Discount Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Discount_Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;User ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;User_ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Applies-to Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Applies-to_Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 1 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_1_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 2 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_2_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                }

            ]
        }')

</code></pre>
<p>Non-working expression with 25 columns:</p>
<pre><code>@json('{
            &quot;type&quot;: &quot;TabularTranslator&quot;,
            &quot;mappings&quot;: [
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Posting Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Posting_Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Document No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Document_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Location Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Location_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Inventory Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Inventory_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Valued Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Valued_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Invoiced Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Invoiced_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Cost per Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Cost_per_Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Sales Amount (Actual)&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Sales_Amount__Actual_&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch_ Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch__Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Discount Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Discount_Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;User ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;User_ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Applies-to Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Applies-to_Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 1 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_1_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 2 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_2_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                }

            ]
        }')

</code></pre>
<p>Problem-solving: tried to reduce amount of columns and adding few at the time to find the column triggering the error.</p>
<p>Conclusion: few columns works fine, at column 25 it fails, but nothing wrong with json-object, tried to move number 25 to 24 and run 24 with success and move earlier to 25 - it keeps failing when there are 25 or more columns. But I can't find any information if there are limits in the expression builder or other in terms of json-doc and &quot;manual&quot; mapping.</p>
<p>Info: the pipeline works fine if sink is csv and schema is imported.</p>
","<azure-data-factory><parquet>","2023-02-26 17:49:29","74","0","2","75573961","<p>Congrats on your first question, since the error message is pretty much empty, its hard to say the root cause, however I think it might be ADLS related.</p>
<p>In any case, you should create a support ticket with ADF, and they can see the backend logs to be able to give you a resolution.</p>
<p>ADLS Gen2 Badrequest with InvalidAccessControlList is a known issue, but your error message is empty.</p>
"
"75573923","ADF Pipeline Copy data ""BadRequest"" error - SQL to Parquet using expression builder for Mapping","<p>I get a very non-explanatory error running this pipeline:
{
&quot;code&quot;: &quot;BadRequest&quot;,
&quot;message&quot;: null,
&quot;target&quot;: &quot;pipeline//runid/xxx&quot;,
&quot;details&quot;: null,
&quot;error&quot;: null
}</p>
<p>Environment: Azure Synapse Analytics (ADF v2)</p>
<p>Activity: Copy data</p>
<p>Source: SQL Server and table with approx. 95 columns - with Parquet incompatible characters like white space and ()</p>
<p>Sink: Datalake gen2 Parquet</p>
<p>Mapping: Dynamic with attached (reduced columns to fulfill body limit in post) expression to &quot;rename&quot; columns to Parquet compatible naming.</p>
<p>Any good ideas? I am giving up finding an answer on the different fora :(</p>
<p>Working expression with 24 columns:</p>
<pre><code>@json('{
            &quot;type&quot;: &quot;TabularTranslator&quot;,
            &quot;mappings&quot;: [
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Posting Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Posting_Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Document No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Document_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Location Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Location_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Inventory Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Inventory_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Valued Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Valued_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Invoiced Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Invoiced_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Cost per Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Cost_per_Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Sales Amount (Actual)&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Sales_Amount__Actual_&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch_ Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch__Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Discount Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Discount_Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;User ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;User_ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Applies-to Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Applies-to_Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 1 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_1_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 2 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_2_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                }

            ]
        }')

</code></pre>
<p>Non-working expression with 25 columns:</p>
<pre><code>@json('{
            &quot;type&quot;: &quot;TabularTranslator&quot;,
            &quot;mappings&quot;: [
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;timestamp&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Posting Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Posting_Date&quot;,
                        &quot;type&quot;: &quot;DateTime&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Document No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Document_No_&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Description&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Location Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Location_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Inventory Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Inventory_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Posting Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Posting_Group&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_No_&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Valued Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Valued_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Item Ledger Entry Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Item_Ledger_Entry_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Invoiced Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Invoiced_Quantity&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Cost per Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Cost_per_Unit&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Sales Amount (Actual)&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Sales_Amount__Actual_&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch_ Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Salespers__Purch__Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Discount Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Discount_Amount&quot;,
                        &quot;type&quot;: &quot;Decimal&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;User ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;User_ID&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Applies-to Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Applies-to_Entry&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 1 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_1_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Global Dimension 2 Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Global_Dimension_2_Code&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Source Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Source_Type&quot;,
                        &quot;type&quot;: &quot;Int32&quot;
                    }
                }

            ]
        }')

</code></pre>
<p>Problem-solving: tried to reduce amount of columns and adding few at the time to find the column triggering the error.</p>
<p>Conclusion: few columns works fine, at column 25 it fails, but nothing wrong with json-object, tried to move number 25 to 24 and run 24 with success and move earlier to 25 - it keeps failing when there are 25 or more columns. But I can't find any information if there are limits in the expression builder or other in terms of json-doc and &quot;manual&quot; mapping.</p>
<p>Info: the pipeline works fine if sink is csv and schema is imported.</p>
","<azure-data-factory><parquet>","2023-02-26 17:49:29","74","0","2","75578464","<p>Instead of passing Json value directly in expression builder,</p>
<ul>
<li>You can use parameter or variable and define the mapping json in that variable/parameter.</li>
<li>Then pass that value as a dynamic content in expression builder of mapping.</li>
</ul>
<p>I reproduced this by creating a mapping Json for more than 25 source columns and passed as a parameter in mapping settings. When pipeline is run, it got executed successfully.
<img src=""https://i.imgur.com/pm2VvsV.png"" alt=""enter image description here"" /></p>
"
"75571709","How Parameterize Truncating a table in Azure SQL DB within an Azure Data Factory Copy Activity","<p>I following an SO question on how to truncate a table in Azure SQL DB with ADF
<a href=""https://stackoverflow.com/questions/69456500/how-to-replace-data-in-azure-sql-database-using-azure-data-factory"">how to replace data in azure sql database using azure data factory?</a></p>
<p>I am trying to emulate the sample in the question using my Copy Activity as follows:</p>
<p>My Source details are as follows:</p>
<p><a href=""https://i.stack.imgur.com/CMHgg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CMHgg.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/YXusw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YXusw.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/VZ128.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VZ128.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/SE7dL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SE7dL.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/L49mS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L49mS.png"" alt=""enter image description here"" /></a></p>
<p>With the above configuration I get the following error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Cannot find the object \&quot;targettable\&quot; because it does not exist or you do not have permissions.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Cannot find the object \&quot;targettable\&quot; because it does not exist or you do not have permissions.,Source=.Net SqlClient Data Provider,SqlErrorNumber=4701,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=4701,State=1,Message=Cannot find the object \&quot;targettable\&quot; because it does not exist or you do not have permissions.,},],'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy From CRM to SQLDB&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>However, if I were to hardcode the schema and table name as follows <code>TRUNCATE TABLE dbo.mytablename</code> everything would work fine.</p>
<p>By way of an update, I just tried with the following parameters</p>
<pre><code>TRUNCATE TableName@pipeline().parameters.TableName
</code></pre>
<p>And it failed.</p>
","<azure-data-factory>","2023-02-26 11:37:20","89","0","1","75572316","<blockquote>
<p><code>Message=A database operation failed with the following error: 'Cannot find the object \&quot;targettable\&quot; because it does not exist or you do not have permissions.</code></p>
</blockquote>
<p>This error occurred because of this query <code>TRUNCATE TABLE dbo.targettable</code>.</p>
<ul>
<li>Here <code>targettable</code> is a SQL database parameter and we cannot access its value inside pipeline and also you have given it with query.</li>
<li>It made the meaning of SQL query as to truncate the table named <code>targettable</code> which is not available in the database and that's why it gave you the above error.</li>
</ul>
<p>To resolve it give the correct parameter in the query with <strong>string interpolation</strong> like below.</p>
<p><code>TRUNCATE table @{pipeline().parameters.TableName}</code></p>
<p><img src=""https://i.imgur.com/nWzmPYk.png"" alt=""enter image description here"" /></p>
<p>I have given the pipeline parameter as <code>sample1</code> which is my table name.</p>
<p><strong>Before Truncate:</strong></p>
<p><img src=""https://i.imgur.com/A5m3Pht.png"" alt=""enter image description here"" /></p>
<p><strong>After Truncate:</strong></p>
<p><img src=""https://i.imgur.com/XxO15pZ.png"" alt=""enter image description here"" /></p>
"
"75564559","Azure data factory, need to copy data between two azure sql databases, And i need to get the identity column after inserting and update it back","<p>The problem is after each row is inserted I need to store the identity and update again it in the source. How is it possible in ADF, in ssms we can use identity_insert?</p>
<p>in first database, i have a table</p>
<pre><code>ID1, name, ID2
1,'a',null
2,'b',null
</code></pre>
<p>in 2nd databse, table is</p>
<pre><code>ID2, name
1,'a'
</code></pre>
<p>So, I need to update ID2 in first database after inserting from 1st database table to 2nd database table</p>
","<azure><azure-data-factory>","2023-02-25 09:49:47","140","0","2","75569115","<p>Showcase of the full pipeline, as well as better explanation would be useful. Are you using storedprocs here to modify the databases?</p>
<p>Also, if you want changes to be updated every single time the table in the first database is updated, you can use CDC (Change data capture).</p>
"
"75564559","Azure data factory, need to copy data between two azure sql databases, And i need to get the identity column after inserting and update it back","<p>The problem is after each row is inserted I need to store the identity and update again it in the source. How is it possible in ADF, in ssms we can use identity_insert?</p>
<p>in first database, i have a table</p>
<pre><code>ID1, name, ID2
1,'a',null
2,'b',null
</code></pre>
<p>in 2nd databse, table is</p>
<pre><code>ID2, name
1,'a'
</code></pre>
<p>So, I need to update ID2 in first database after inserting from 1st database table to 2nd database table</p>
","<azure><azure-data-factory>","2023-02-25 09:49:47","140","0","2","75582868","<p>If your source and sink databases are same, then using copy activity pre copy script, you can get the desired result.</p>
<p>But you mentioned the source and sink are two different databases.
I could able to achieve your requirement as below.</p>
<p><strong>NOTE:</strong> This approach only works for inserting records.</p>
<p><strong>This is my source table with identity column ID1:</strong></p>
<p><img src=""https://i.imgur.com/kTz7RMv.png"" alt=""enter image description here"" /></p>
<p><strong>My target table with Identity column ID2:</strong></p>
<p><img src=""https://i.imgur.com/4RJSVFp.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>First, I have taken source table ID1 column first value using lookup activity query <code>select TOP (1) ID1 from [dbo].[mysource]</code>.</p>
</li>
<li><p><strong>After the copy activity from source table to target table</strong> I have generated an array using range of values from <strong>ID1(1st row value)</strong> to <strong>number of records copied in copy activity(till last row ID1)</strong>. I have used below dynamic content for it.
<code>@range(activity('Source First ID').output.value[0].ID1,activity('Copy data1').output.rowsCopied)</code></p>
<p><img src=""https://i.imgur.com/X7UkSmf.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then I have taken another lookup query <code>select IDENT_CURRENT('mytarget2') as ID2;</code> to target table to get the <strong>current identity(last inserted ID2)</strong>. I have used <strong>copy activity rows copied</strong> and this lookup output to generate target ID2 array using below dynamic content.</p>
<pre><code>@range(int(string(add(sub(activity('get last ID2').output.value[0].ID2,activity('Copy data1').output.rowsCopied),1))), activity('Copy data1').output.rowsCopied)
</code></pre>
<p><img src=""https://i.imgur.com/ToTrkli.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Now, I am going to use the above arrays to update source table ID2 using script activity inside a ForEach. For iterating two arrays at a time inside a For Each I have generated an index array with <code>@range(0, activity('Copy data1').output.rowsCopied)</code>.</p>
</li>
<li><p>Give this index array to ForEach activity and inside the ForEach use a script activity for source database with following dynamic content.</p>
<pre><code>UPDATE mysource SET ID2=@{variables('targetID2array')[item()]} WHERE ID1 = @{variables('sourceidsarray')[item()]};
</code></pre>
<p><img src=""https://i.imgur.com/LhfR7jn.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p>This is my Pipeline flow:</p>
<p><img src=""https://i.imgur.com/EB49S0x.png"" alt=""enter image description here"" /></p>
<p><strong>My Pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline2&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Source First ID&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                    &quot;sqlReaderQuery&quot;: &quot;select TOP (1) ID1 from [dbo].[mysource]&quot;,
                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                    &quot;partitionOption&quot;: &quot;None&quot;
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;Copy data1&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Source First ID&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                    &quot;sqlReaderQuery&quot;: &quot;select name from mysource;&quot;,
                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                    &quot;partitionOption&quot;: &quot;None&quot;
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSink&quot;,
                    &quot;writeBehavior&quot;: &quot;insert&quot;,
                    &quot;sqlWriterUseTableLock&quot;: false
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;target&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        },
        {
            &quot;name&quot;: &quot;SourceIDSarray&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Copy data1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;sourceidsarray&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@range(activity('Source First ID').output.value[0].ID1,activity('Copy data1').output.rowsCopied)&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;targetID2 array&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;get last ID2&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;targetID2array&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@range(int(string(add(sub(activity('get last ID2').output.value[0].ID2,activity('Copy data1').output.rowsCopied),1))), activity('Copy data1').output.rowsCopied)&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;get last ID2&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;SourceIDSarray&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                    &quot;sqlReaderQuery&quot;: &quot;select IDENT_CURRENT('mytarget2') as ID2;&quot;,
                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                    &quot;partitionOption&quot;: &quot;None&quot;
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;target&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;Index array&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;targetID2 array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;index_array&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@range(0, activity('Copy data1').output.rowsCopied)&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Index array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('index_array')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Script1&quot;,
                        &quot;type&quot;: &quot;Script&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;linkedServiceName&quot;: {
                            &quot;referenceName&quot;: &quot;AzureSqlDatabase1&quot;,
                            &quot;type&quot;: &quot;LinkedServiceReference&quot;
                        },
                        &quot;typeProperties&quot;: {
                            &quot;scripts&quot;: [
                                {
                                    &quot;type&quot;: &quot;Query&quot;,
                                    &quot;text&quot;: {
                                        &quot;value&quot;: &quot;UPDATE mysource SET ID2=@{variables('targetID2array')[item()]} WHERE ID1 = @{variables('sourceidsarray')[item()]};&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            ],
                            &quot;scriptBlockExecutionTimeout&quot;: &quot;02:00:00&quot;
                        }
                    }
                ]
            }
        }
    ],
    &quot;variables&quot;: {
        &quot;sourceidsarray&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;targetID2array&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;index_array&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;ok&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Target table after Pipeline execution:</strong></p>
<p><img src=""https://i.imgur.com/GW6kXeA.png"" alt=""enter image description here"" /></p>
<p><strong>Source table with updated ID2 values after Pipeline execution:</strong></p>
<p><img src=""https://i.imgur.com/p52KYTB.png"" alt=""enter image description here"" /></p>
"
"75563931","Azure data factory, stored procedure activity is not updating values in the table but it runs normally in ssms its working as expected wit same input","<p>The procedure's input is the below one. its a json request to the procedure. in ssms with the same input its correctly updating the table.
the activity and the pipeline is being executed correctly but the update statement is not working in the procedure</p>
<pre><code>{
        &quot;source&quot;: {
            &quot;type&quot;: &quot;AzureSqlSource&quot;,
            &quot;sqlReaderStoredProcedureName&quot;: &quot;[dbo].[UpdateProclaimIDs_test_adf2]&quot;,
            &quot;storedProcedureParameters&quot;: {
                &quot;p_request_string&quot;: {
                    &quot;type&quot;: &quot;String&quot;,
                    &quot;value&quot;: &quot;[[{\&quot;proclaim_id\&quot;:1,\&quot;test_patient_id\&quot;:1},{\&quot;proclaim_id\&quot;:2,\&quot;test_patient_id\&quot;:2},{\&quot;proclaim_id\&quot;:3,\&quot;test_patient_id\&quot;:3},{\&quot;proclaim_id\&quot;:4,\&quot;test_patient_id\&quot;:4},{\&quot;proclaim_id\&quot;:5,\&quot;test_patient_id\&quot;:1},{\&quot;proclaim_id\&quot;:6,\&quot;test_patient_id\&quot;:2},{\&quot;proclaim_id\&quot;:7,\&quot;test_patient_id\&quot;:3},{\&quot;proclaim_id\&quot;:8,\&quot;test_patient_id\&quot;:4}]]&quot;
                }
            },
            &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
            &quot;partitionOption&quot;: &quot;None&quot;
        },
        &quot;dataset&quot;: {
            &quot;referenceName&quot;: &quot;AzureSqlTable2&quot;,
            &quot;type&quot;: &quot;DatasetReference&quot;,
            &quot;parameters&quot;: {}
        }
    }
</code></pre>
<p>procedure code is</p>
<pre><code>alter procedure [dbo].[UpdateProclaimIDs_test_adf2]                                                                                                  
       @p_request_string  nvarchar(max)                                                                                                 
AS                                                                                                  
 BEGIN    
     
  declare @var1 nvarchar(max)= JSON_query(@p_request_string,'$.source.storedProcedureParameters.p_request_string')

  declare @var2 nvarchar(max)= JSON_value(@var1,'$.value')

  --drop table #temp2
   SELECT value
   into #temp2
   FROM OPENJSON(@var2)

     create table #ProclaimIDs    
  (testPatientID int,proclaim_id int) 

  insert into #ProclaimIDs(testPatientID,proclaim_id)
   select test_patient_id,proclaim_id
   from #temp2 
   CROSS APPLY openjson(value) WITH 
   (proclaim_id int '$.proclaim_id',
   test_patient_id int  '$.test_patient_id') 
    
update pta                             
  set proclaim_id= pis.proclaim_id                  
from proclaim_test_adf2 pta    
join #ProclaimIDs pis on pis.testPatientID=pta.test_id  
</code></pre>
","<azure><azure-data-factory>","2023-02-25 07:30:58","70","0","1","75565365","<ul>
<li>The variables <code>@var1 and @var2</code> are not being assigned the proper values during stored procedure call and hence the temporary tables are not getting populated properly and the <code>proclaim_test_adf2</code> is not getting updated properly.</li>
</ul>
<br>
<ul>
<li>Also, the value passed to the stored procedure should be as shown below:</li>
</ul>
<pre><code>[{&quot;proclaim_id&quot;:1,&quot;test_patient_id&quot;:1},{&quot;proclaim_id&quot;:2,&quot;test_patient_id&quot;:2},{&quot;proclaim_id&quot;:3,&quot;test_patient_id&quot;:3},{&quot;proclaim_id&quot;:4,&quot;test_patient_id&quot;:4},{&quot;proclaim_id&quot;:5,&quot;test_patient_id&quot;:1},{&quot;proclaim_id&quot;:6,&quot;test_patient_id&quot;:2},{&quot;proclaim_id&quot;:7,&quot;test_patient_id&quot;:3},{&quot;proclaim_id&quot;:8,&quot;test_patient_id&quot;:4}]
</code></pre>
<br>
<ul>
<li>Now the stored procedure code would be:</li>
</ul>
<pre><code>create or alter procedure [dbo].[UpdateProclaimIDs_test_adf2]                                                                                                  
       @p_request_string  nvarchar(max)                                                                                                 
AS                                                                                                  
 BEGIN    
     
  declare @var1 nvarchar(max)= JSON_query(@p_request_string,'$')

  --drop table #temp2
   SELECT value
   into #temp2
   FROM OPENJSON(@var1)

     create table #ProclaimIDs    
  (testPatientID int,proclaim_id int) 

  insert into #ProclaimIDs(testPatientID,proclaim_id)
   select test_patient_id,proclaim_id
   from #temp2 
   CROSS APPLY openjson(value) WITH 
   (proclaim_id int '$.proclaim_id',
   test_patient_id int  '$.test_patient_id') 
    
update proclaim_test_adf2                            
  set proclaim_id= pis.proclaim_id                  
from proclaim_test_adf2  
join #ProclaimIDs pis on pis.testPatientID=proclaim_test_adf2.test_id

end
</code></pre>
<br>
<ul>
<li>The following is the starting data I have taken for <code> proclaim_test_adf2</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/CFrhxZC.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>The following is the pipeline JSON for stored procedure activity:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Stored procedure1&quot;,
                &quot;type&quot;: &quot;SqlServerStoredProcedure&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;storedProcedureName&quot;: &quot;[dbo].[UpdateProclaimIDs_test_adf2]&quot;,
                    &quot;storedProcedureParameters&quot;: {
                        &quot;p_request_string&quot;: {
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;[{\&quot;proclaim_id\&quot;:1,\&quot;test_patient_id\&quot;:1},{\&quot;proclaim_id\&quot;:2,\&quot;test_patient_id\&quot;:2},{\&quot;proclaim_id\&quot;:3,\&quot;test_patient_id\&quot;:3},{\&quot;proclaim_id\&quot;:4,\&quot;test_patient_id\&quot;:4},{\&quot;proclaim_id\&quot;:5,\&quot;test_patient_id\&quot;:1},{\&quot;proclaim_id\&quot;:6,\&quot;test_patient_id\&quot;:2},{\&quot;proclaim_id\&quot;:7,\&quot;test_patient_id\&quot;:3},{\&quot;proclaim_id\&quot;:8,\&quot;test_patient_id\&quot;:4}]&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            },
                            &quot;type&quot;: &quot;String&quot;
                        }
                    }
                },
                &quot;linkedServiceName&quot;: {
                    &quot;referenceName&quot;: &quot;sqldb&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                }
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/4hlnp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4hlnp.png"" alt=""enter image description here"" /></a></p>
<br>
<ul>
<li>The <code>proclaim_test_adf2</code> table also gets updated as per requirement.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/2GRbJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2GRbJ.png"" alt=""enter image description here"" /></a></p>
"
"75561351","Controlling manul cancellation for pipelines in ADF","<p>Some of the pipelines we have created start and conclude inserting/updating some register at the begining and at the end in our process table. However, recently some users got the role that allowe them to cancel the pipelines (client's policy). We wonder if there is a way to indicate in ADF the execution of a process when this event ocurrs (status = cancellation). We need to keep our table sincronized.</p>
<p><a href=""https://i.stack.imgur.com/SFzDh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SFzDh.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-02-24 21:02:46","74","0","1","75569304","<p>Cancelled pipeline run metrics under Alerts and Metrics should be useful. You can create an alert when a pipeline is cancelled. Your notifications can be in the form of emails, SMS, azure app push, or voice call.</p>
<p><a href=""https://i.stack.imgur.com/hSAI1.png"" rel=""nofollow noreferrer"">As seen here</a></p>
<p>If you know how to write Kusto (its very easy to learn), you can also write a custom alert using pipeline metrics logs in Log Analytics.</p>
<p>If you meant in the question parameterizing, not notifying cancellations, for example as in dataflow expression builder; system variables on the pipeline level in the below article does not list pipeline trigger end status as variable:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a></p>
<p>The reason is that we are parameterizing something before the fact it happens(cancels), so log analytics would be helpful in this case too I think. Query in log analytics and filter cancellations, then you can move from there. To add functionality, such as when you want to do something when a cancellation occurs, you can create an alert rule in log analytics, and add action when an action group is triggered, such as an Azure function or webhook.</p>
<p>To connect log analytics to ADF, you need to enable diagnostics setting, and send it to log analytics, there are videos on web explaining how to do that.</p>
<p>Of course as always, you can also create a support ticket with ADF to make %100 sure it is not supported as a system variable.</p>
<p>Please confirm if this answer helped you</p>
"
"75558596","How to use data flow activity and derived column transformation to do column mapping in ADF and load it in SQL Server table","<p>I have fixed length .DAT files in a ftp server and I need to use data flow activity and derived column transformation to do column mapping using ADF to be able to transform data before loading into a SQL Server table.</p>
<ol>
<li>There is no delimiter in the file.</li>
<li>The target table in SQL server has columns same as mapping photo below.</li>
<li>Need to do column mapping on dataset created from fixed length .DAT file for each line and land it in target SQL server table.</li>
</ol>
<p>I need a solution where without any delimiter specified I am able to convert .DAT file to a dataset and use derived column transformation and then take substring for every column to do column mapping in a similar way as shown below:[![Column Mapping][1]][1]</p>
<p>I would need to update the dataset daily because new file would be added everyday to the ftp server.</p>
<p>Any help/snapshots are appreciate. Thank you.</p>
","<azure><mapping><azure-data-factory>","2023-02-24 15:48:49","106","0","1","75561004","<p>Fixed width format is not natively supported as a Dataset. In order to process it, you'll need to parse the rows first. Here is a rough outline:</p>
<ol>
<li>Create a Dataset with no schema and no delimiter. This will read in each row as a single column named &quot;Prop_0&quot;.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/uBoQs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uBoQs.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Use Derived Column and substring to parse the columns out of the row. When you name the columns, name them the same as the target SQL columns.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/yRjZ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yRjZ3.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li><p>Perform any additional operations like trims (recommended) or type conversions.</p>
</li>
<li><p>Write the data to your Sink.</p>
</li>
</ol>
<p>If the data file is really large, you might want to save the data to an intermediate location, like Parquet files, and then perform the write in another Dataflow. This should make the write to SQL perform better because it will be able to parallelize. It also gives you the opportunity to validate the converted data prior to writing it to SQL.</p>
"
"75557079","ADF lookup to Foreach","<p>I have json input file and using lookup activity and passing to Foreach to get file name list.
But I am facing issue in foreach activity and get the file list.</p>
<p>Input Json:</p>
<pre><code>&gt; {
&gt;     &quot;ItemsCount&quot;: 2,
&gt;     &quot;FilteredItemsCount&quot;: 1,
&gt;     &quot;Value&quot;: [
&gt;         {
&gt;             &quot;Sourcetype&quot;: &quot;DB&quot;,
&gt;             &quot;DB&quot;: &quot;test&quot;,
&gt;             &quot;Filename&quot;: [
&gt;                 &quot;dbo.Emp&quot;,
&gt;                 &quot;dbo.Emp1&quot;
&gt;             ]
&gt;         }
&gt;     ] }
</code></pre>
<p>Foreach setting:</p>
<pre><code>@activity('Filter2').output.Filename
</code></pre>
<pre><code>Error
{
    &quot;errorCode&quot;: &quot;InvalidTemplate&quot;,
    &quot;message&quot;: &quot;The expression 'length(activity('Filter2').output.Filename)' cannot be evaluated because property 'Filename' doesn't exist, available properties are 'ItemsCount, FilteredItemsCount, Value'.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy_DB_to_DataLake&quot;,
    &quot;details&quot;: &quot;&quot;
}
</code></pre>
<p>Kindly confirm my foreach setting and let me know if anything needs to add.</p>
<p>Thanks.</p>
","<azure-data-factory>","2023-02-24 13:21:41","96","0","1","75558064","<p>Foreach requires an array. To loop over the results of a Filter, you need to reference the 'value' property:</p>
<pre><code>@activity('Filter2').output.value
</code></pre>
<p>Then inside your loop, each iteration can be referenced by using &quot;@item()&quot; as the root object:</p>
<pre><code>@item().Filename
</code></pre>
"
"75553168","DAG runs stuck in running state using Azure Data Factory Managed Airflow (version 2.4.3)","<p>I'm following Microsoft's tutorial on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-does-managed-airflow-work"" rel=""nofollow noreferrer"">how does managed airflow work</a> using the tutorial.py script referenced in the documentation (see code block below). I've set up my airflow environment in azure data factory using the same configuration in the documentation with the exception of the airflow version - I'm using version 2.4.3 as version 2.2.2 is no longer available in data factory.</p>
<p>Everything appears to be set up successfully. However, my DAG runs never succeed nor fail they just stay in the running state
<a href=""https://i.stack.imgur.com/iKSAf.png"" rel=""nofollow noreferrer"">dag monitor image</a></p>
<pre class=""lang-py prettyprint-override""><code>from datetime import datetime, timedelta
from textwrap import dedent

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
with DAG(
    &quot;tutorial&quot;,
    # These args will get passed on to each operator
    # You can override them on a per-task basis during operator initialization
    default_args={
        &quot;depends_on_past&quot;: False,
        &quot;email&quot;: [&quot;airflow@example.com&quot;],
        &quot;email_on_failure&quot;: False,
        &quot;email_on_retry&quot;: False,
        &quot;retries&quot;: 1,
        &quot;retry_delay&quot;: timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
        # 'wait_for_downstream': False,
        # 'sla': timedelta(hours=2),
        # 'execution_timeout': timedelta(seconds=300),
        # 'on_failure_callback': some_function,
        # 'on_success_callback': some_other_function,
        # 'on_retry_callback': another_function,
        # 'sla_miss_callback': yet_another_function,
        # 'trigger_rule': 'all_success'
    },
    description=&quot;A simple tutorial DAG&quot;,
    schedule=timedelta(minutes=5),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=[&quot;example&quot;],
) as dag:

    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id=&quot;print_date&quot;,
        bash_command=&quot;date&quot;,
    )

    t2 = BashOperator(
        task_id=&quot;sleep&quot;,
        depends_on_past=False,
        bash_command=&quot;sleep 5&quot;,
        retries=3,
    )
    t1.doc_md = dedent(
        &quot;&quot;&quot;\
    #### Task Documentation
    You can document your task using the attributes `doc_md` (markdown),
    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
    rendered in the UI's Task Instance Details page.
    ![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)
    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)
    &quot;&quot;&quot;
    )

    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR
    dag.doc_md = &quot;&quot;&quot;
    This is a documentation placed anywhere
    &quot;&quot;&quot;  # otherwise, type it like this
    templated_command = dedent(
        &quot;&quot;&quot;
    {% for i in range(5) %}
        echo &quot;{{ ds }}&quot;
        echo &quot;{{ macros.ds_add(ds, 7)}}&quot;
    {% endfor %}
    &quot;&quot;&quot;
    )

    t3 = BashOperator(
        task_id=&quot;templated&quot;,
        depends_on_past=False,
        bash_command=templated_command,
    )

    t1 &gt;&gt; [t2, t3]
</code></pre>
<p>I've tested this locally and the runs succeed so I'm wondering if there's any additional configuration required for azure data factory managed airflow that isn't documented in the tutorial link referenced above? Are there certain Airflow requirements or Airflow configuration overrides that need to be set when using azure data factory's managed airflow service?</p>
","<airflow><azure-data-factory>","2023-02-24 06:16:01","82","0","2","75569404","<p>Congrats on your first question. Airflow in ADF is a new feature as it is currently in preview, and as with every new feature you might sometimes run into errors.</p>
<p>For this problem I think more backend information is required why they get stuck in running state, so the best action to take is to create a support ticket with ADF and see what they can provide from the backend.</p>
"
"75553168","DAG runs stuck in running state using Azure Data Factory Managed Airflow (version 2.4.3)","<p>I'm following Microsoft's tutorial on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-does-managed-airflow-work"" rel=""nofollow noreferrer"">how does managed airflow work</a> using the tutorial.py script referenced in the documentation (see code block below). I've set up my airflow environment in azure data factory using the same configuration in the documentation with the exception of the airflow version - I'm using version 2.4.3 as version 2.2.2 is no longer available in data factory.</p>
<p>Everything appears to be set up successfully. However, my DAG runs never succeed nor fail they just stay in the running state
<a href=""https://i.stack.imgur.com/iKSAf.png"" rel=""nofollow noreferrer"">dag monitor image</a></p>
<pre class=""lang-py prettyprint-override""><code>from datetime import datetime, timedelta
from textwrap import dedent

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
with DAG(
    &quot;tutorial&quot;,
    # These args will get passed on to each operator
    # You can override them on a per-task basis during operator initialization
    default_args={
        &quot;depends_on_past&quot;: False,
        &quot;email&quot;: [&quot;airflow@example.com&quot;],
        &quot;email_on_failure&quot;: False,
        &quot;email_on_retry&quot;: False,
        &quot;retries&quot;: 1,
        &quot;retry_delay&quot;: timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
        # 'wait_for_downstream': False,
        # 'sla': timedelta(hours=2),
        # 'execution_timeout': timedelta(seconds=300),
        # 'on_failure_callback': some_function,
        # 'on_success_callback': some_other_function,
        # 'on_retry_callback': another_function,
        # 'sla_miss_callback': yet_another_function,
        # 'trigger_rule': 'all_success'
    },
    description=&quot;A simple tutorial DAG&quot;,
    schedule=timedelta(minutes=5),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=[&quot;example&quot;],
) as dag:

    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id=&quot;print_date&quot;,
        bash_command=&quot;date&quot;,
    )

    t2 = BashOperator(
        task_id=&quot;sleep&quot;,
        depends_on_past=False,
        bash_command=&quot;sleep 5&quot;,
        retries=3,
    )
    t1.doc_md = dedent(
        &quot;&quot;&quot;\
    #### Task Documentation
    You can document your task using the attributes `doc_md` (markdown),
    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
    rendered in the UI's Task Instance Details page.
    ![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)
    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)
    &quot;&quot;&quot;
    )

    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR
    dag.doc_md = &quot;&quot;&quot;
    This is a documentation placed anywhere
    &quot;&quot;&quot;  # otherwise, type it like this
    templated_command = dedent(
        &quot;&quot;&quot;
    {% for i in range(5) %}
        echo &quot;{{ ds }}&quot;
        echo &quot;{{ macros.ds_add(ds, 7)}}&quot;
    {% endfor %}
    &quot;&quot;&quot;
    )

    t3 = BashOperator(
        task_id=&quot;templated&quot;,
        depends_on_past=False,
        bash_command=templated_command,
    )

    t1 &gt;&gt; [t2, t3]
</code></pre>
<p>I've tested this locally and the runs succeed so I'm wondering if there's any additional configuration required for azure data factory managed airflow that isn't documented in the tutorial link referenced above? Are there certain Airflow requirements or Airflow configuration overrides that need to be set when using azure data factory's managed airflow service?</p>
","<airflow><azure-data-factory>","2023-02-24 06:16:01","82","0","2","75991869","<p>Looks like ADF Managed Airflow had an outage with Airflow v2.4.3 where DAGs remained stuck in a &quot;running&quot; state. Looks like this was only for a short period so it should be working now</p>
"
"75552495","How to map a json object to a column in Sql","<p>I am trying to load and transform data from an API to Azure SQL using ADF copy activity. The data from json API(source) is in below format.</p>
<pre><code>{
  &quot;fdgdhgfh&quot;: {
    &quot;so2_production&quot;: 7hjhgj953,
    &quot;battery_charge&quot;: jkjlkj,
    &quot;battery_discharge&quot;: kjlklj,
    &quot;critical_load_energy&quot;: 4ljljh4
  },
  &quot;9fsdsfb&quot;: {
   
    &quot;so2_production&quot;: asdasd,
    &quot;battery_charge&quot;: sdaasf,
    &quot;battery_discharge&quot;: ewewrwer,
    &quot;critical_load_energy&quot;: bmvkbjk
  }
}
</code></pre>
<p>I want to map &quot;fdgdhgfh&quot; or &quot;9fsdsfb&quot; to a column  in Azure SQL using copy activity. By default when I import schemas I am getting object values &quot;so2_production&quot;,&quot;battery_charge&quot;,&quot;battery_discharge&quot;, &quot;critical_load_energy&quot; to map in SQL but I want to map object not the object value.</p>
<p><a href=""https://i.stack.imgur.com/q2qxL.png"" rel=""nofollow noreferrer"">Expected SQL Result from Table</a></p>
","<json><azure><azure-sql-database><azure-data-factory>","2023-02-24 03:59:28","149","0","1","75557204","<p><strong>AFAIK</strong>, Using copy activity, it might not be possible to achieve your desired result.</p>
<p>I could able to get it done using combination of variables and script activity.</p>
<p><strong>NOTE:</strong> This approach will only work when there is no pagination as it involves usage of web activity. If there is a pagination, you need to do use web activity in every iteration. If you can use dataflow, then it be the better option with combination of derived columns and flatten transformations beacuse dataflow supports REST API and pagination.</p>
<p>These are my variables and flow of pipeline.</p>
<p><img src=""https://i.imgur.com/XOGjVnq.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Here I have used lookup activity from blob to get your JSON. In your case use web activity here.</p>
</li>
<li><p>Then I have converted the JSON to string and stored in a variable <code>jsonstring</code>.
<code>@string(activity('Lookup1').output.value[0])</code>. In your case it will be <code>@string(activity('Web1').output)</code>.</p>
</li>
<li><p>After that, I have used split on that variable with <code>},</code> and stored the result in <code>split1</code> variable.
<code>@split(substring(variables('jsonstring'),1,add(length(variables('jsonstring')),-2)), '},')</code>
which will give result as below.
<img src=""https://i.imgur.com/7kuqyzL.png"" alt=""enter image description here"" /></p>
</li>
<li><p>I have used a ForEach here, and given the above array <code>@variables('split1')</code> to it.</p>
</li>
<li><p>Inside ForEach, to store the keys of JSON into <code>cols</code> variable I have used the below expression in each iteration.
<code>@substring(split(item(),':{')[0],1,add(length(split(item(),':{')[0]),-2))</code></p>
</li>
<li><p>After that, I have used the below expression to store the <code>so2_production</code> values into <code>values</code> expression.<code>@activity('Lookup1').output.value[0][variables('cols')]['so2_production']</code>.</p>
<p>In your case it will be <code>activity('Web1').output[variables('cols')]    ['so2_production']</code></p>
</li>
<li><p>Inside same ForEach after getting the cols and object values, I have used a script activity to insert the data from variables into target table.</p>
<pre><code>insert into sample1(uid,usource) values('@{variables('cols')}','@{variables('values')}')
</code></pre>
<p><img src=""https://i.imgur.com/I9Fvjmc.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p>It will insert each key and object value for every iteration.</p>
<p><strong>My Pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline3&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Lookup1&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;JsonSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;sourcejson&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;Lookup output to string&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;jsonstring&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@string(activity('Lookup1').output.value[0])&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;split on string&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup output to string&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;split1&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@split(substring(variables('jsonstring'),1,add(length(variables('jsonstring')),-2)), '},')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;split on string&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('split1')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Cols&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;cols&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@substring(split(item(),':{')[0],1,add(length(split(item(),':{')[0]),-2))&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;values&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Cols&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;values&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@activity('Lookup1').output.value[0][variables('cols')]['so2_production']&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Script1&quot;,
                        &quot;type&quot;: &quot;Script&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;values&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;linkedServiceName&quot;: {
                            &quot;referenceName&quot;: &quot;AzureSqlDatabase1&quot;,
                            &quot;type&quot;: &quot;LinkedServiceReference&quot;
                        },
                        &quot;typeProperties&quot;: {
                            &quot;scripts&quot;: [
                                {
                                    &quot;type&quot;: &quot;Query&quot;,
                                    &quot;text&quot;: {
                                        &quot;value&quot;: &quot;insert into sample1(uid,usource) values('@{variables('cols')}','@{variables('values')}')&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            ],
                            &quot;scriptBlockExecutionTimeout&quot;: &quot;02:00:00&quot;
                        }
                    }
                ]
            }
        }
    ],
    &quot;variables&quot;: {
        &quot;jsonstring&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;split1&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;cols&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;values&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/wwAMZgh.png"" alt=""enter image description here"" /></p>
"
"75551500","The resource type '/' does not support diagnostic settings","<p>I have the following bicep to deploy Azure Data Factory with diagnostic setting:</p>
<pre><code>resource dataFactory 'Microsoft.DataFactory/factories@2018-06-01' = {
  name: name
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
    globalParameters: {
      environment: {
        type: 'String'
        value: environmentAbbreviation
      }
    }
  }
  location: location
}

resource diagnosticSettings 'Microsoft.Insights/diagnosticSettings@2021-05-01-preview' = {
  name: name
  properties: {
    logs: [
      {
        category: 'PipelineRuns'
        enabled: true
    }
    ]
    workspaceId: resourceId('microsoft.operationalinsights/workspaces','&lt;Workspace-Name&gt;')
    logAnalyticsDestinationType: null
  }
  dependsOn: [
    dataFactory
  ]
}
</code></pre>
<p>On running this, I see it failing due to error:</p>
<blockquote>
<p>The resource type '/' does not support diagnostic settings. What am I missing?</p>
</blockquote>
","<azure-data-factory><azure-resource-manager><azure-bicep><azure-diagnostics>","2023-02-24 00:02:29","142","1","1","75552960","<p>&quot;Microsoft.Insights/diagnosticSettings&quot; resource does not have dependsOn property. You need to use scope property. For template structure of diagnostic resource refer <a href=""https://learn.microsoft.com/en-us/azure/templates/microsoft.insights/diagnosticsettings?pivots=deployment-language-bicep#resource-format"" rel=""nofollow noreferrer"">this document</a>.</p>
<p>Here is sample example for reference.</p>
<pre><code>@description('Data Factory Name')
param dataFactoryName string = 'datafactory${uniqueString(resourceGroup().id)}'

@description('Location of the data factory.')
param location string = resourceGroup().location

@description('Name of the log analytics workspace')
param workspaceid string

@description('Name of the diagnostic setting')
param diagname string

resource dataFactory 'Microsoft.DataFactory/factories@2018-06-01' = {
  name: dataFactoryName
  location: location
  identity: {
    type: 'SystemAssigned'
  }
}


resource diagnosticSettings 'Microsoft.Insights/diagnosticSettings@2021-05-01-preview' = {
  name: diagname
  scope: dataFactory
  properties: {
    logs: [
      {
        category: 'PipelineRuns'
        enabled: true
    }
    ]
    workspaceId: workspaceid
    logAnalyticsDestinationType: null
  }
}
</code></pre>
"
"75549944","Azure data factory - Concurrence problem in the child pipeline","<p>i have a parent and child pipelines. Parent pipeline triggers the child pipeline inside the foreach activity. I have totally 6 items in the array so foreach triggers the child pipeline 6 times. I made foreach to trigger child pipeline parallel by unchecking the sequential check box. I have also have given 6 concurrence to the child pipeline. Currently 6 instances of the child pipeline is running at the same and ends at the same time. I want the child pipeline run parallel but i want start each instance by 30 or 40 secs delay or end by 30-40 secs difference. I have next activity which check the flag.</p>
<p>I added the delay at the end of the child pipeline but the wait exactly ends on the same time for 6 instances. I also tried adding set counter and increase the counter but two times counters are intialized with the same value as the 2 instances of the pipeline ends at the same time.</p>
","<azure-data-factory>","2023-02-23 20:22:22","42","0","1","75555144","<p>You many use <code>rand</code> function to generate random number in between range and use that number in wait activity so that you will have gap in execution of child pipelines.</p>
"
"75549306","ADF pipeline not triggered on 'appendblob' event type in ADLS Gen2","<p>I have set up an ADF pipeline to trigger whenenver a new file lands in the ADLS Gen2(source). The pipeline should copy the json file from ADlS(Gen2) and sink in Azure SQL db.</p>
<p>In ADF, I am using event based trigger: Type: BlobEventTrigger , Event : Blob Created</p>
<p><strong>Here is the problem, I am facing</strong></p>
<ul>
<li><p>The ADF pipeline works file, when I manually upload the file to ADLS, to test/debug my ADF pipeline. The trigger get activated, the pipeline copies the data from ADLS to SQL db</p>
</li>
<li><p>However, when a file programmatically gets dropped in ADLS(as append blob), the trigger is not activated and thus ADF pipeline is not activated</p>
</li>
</ul>
<p><strong>I need to know:</strong></p>
<ul>
<li>How to create an event based trigger in ADF, when a append blob event occurs in ADLS</li>
</ul>
<p>Tried both event types:</p>
<ul>
<li>Ignore empty Blobs and Dont ignore empty blobs</li>
</ul>
","<azure><azure-data-factory><event-based-programming>","2023-02-23 19:09:05","50","0","1","75555780","<blockquote>
<p>How to create an event based trigger in ADF, when a append blob event occurs in ADLS</p>
</blockquote>
<p>you are appending the blob means you are updating/modifying the blob.</p>
<p>In ADF storage event triggers It will trigger the pipeline in two scenarios whenever the new blob is getting created and when the existing blob is getting updated.</p>
<p>I created a trigger to run pipeline when blob is created or updated in specific container. settings as below:</p>
<p><img src=""https://i.imgur.com/F47R7nd.png"" alt=""enter image description here"" /></p>
<p>check the blob updated time and pipeline triggered time.</p>
<p><img src=""https://i.imgur.com/3bgVzYt.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/Fsj8Wnk.png"" alt=""enter image description here"" /></p>
"
"75548825","How can I save the content of an xml file that's in a blob container to a variable in Azure Data Factory?","<p>I'm able to get the xml content from the blob in a web activity and store it in a variable. However, the variable is set to JSON notation with the following format:</p>
<p>`{</p>
<p>&quot;name&quot;: &quot;&quot;,</p>
<p>&quot;value&quot;: &quot;&quot;</p>
<p>}`</p>
<p>I need to send the xml content through an API, but can't access the xml content from the value property of the variable.</p>
<p>I've tried using the xml() function to convert but it's not working.</p>
","<azure-data-factory>","2023-02-23 18:21:23","61","0","1","75552554","<pre><code>{
&quot;name&quot;: &quot;&quot;,
&quot;value&quot;: &quot;&quot;
}
</code></pre>
<p>This is the output of set variable activity. Name property will have name of the variable and value will have value of the variable. In this case, value will have the XML response of web activity. If you want to access the value of the variable, you can use <code>@variables('&lt;variable-name&gt;')</code> in the expression.</p>
<p>I tried to repro this with sample value.</p>
<ul>
<li><p>Web activity is taken and output of the web activity is as follows.
<img src=""https://i.imgur.com/qdvF4Yj.png"" alt=""enter image description here"" /></p>
</li>
<li><p>The response is stored in a variable of string type.
<img src=""https://i.imgur.com/oUdIBJp.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Output of Set variable activity:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/LGrorwb.png"" alt=""enter image description here"" /></p>
<ul>
<li>Value of variable v1 is assigned to variable v2 expresssion is given as <code>@variable(v1)</code></li>
</ul>
<p><img src=""https://i.imgur.com/iLUOZ2O.png"" alt=""enter image description here"" /></p>
"
"75547157","Copy Activity in Azure Datafactory","<p>How can Azure Datafactory's copy activity be used to rename folders during the process of moving data from a datalake gen1 to a datalake gen2, in the following scenarios:</p>
<p>If the original folder structure in datalake gen1 is y=2022/m=08/d=01, and the desired structure in datalake gen2 is 2022/08/01, can the copy activity be configured to rename the folders accordingly?
I want to do it for all months and days of year.</p>
","<azure><azure-data-factory>","2023-02-23 15:47:27","65","0","1","75554870","<ul>
<li>You can store the folder and file path details of source ADLS gen1 in a control table and use that as source dataset for lookup activity.</li>
</ul>
<p><img src=""https://i.imgur.com/qZB3zyJ.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Take a for-each activity, and connect it sequentially with lookup activity. In for-each activity, expression for item is given as
<code>@activity('Lookup1').output.value</code></p>
</li>
<li><p>Inside for-each activity, take a copy activity. Create a linked service for ADLS gen1 and create a dataset parameter for folder and file. In source dataset, pass <code>@item().Folder</code> as value to folder parameter and <code>@item().File</code> to file parameter.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/dMPUxvp.png"" alt=""enter image description here"" /></p>
<ul>
<li>Similarly, create a sink dataset for ADLS gen2. Also, create dataset parameters for folder and file names.</li>
<li>Pass  <code>@replace(replace(replace(item().Folder,'y=',''),'m=',''),'d=','')</code>  as a value to folder parameter. This will replace 'y=', 'm=' and 'd='
from item().folder value. Pass <code>@item().file</code> to file parameter.</li>
</ul>
<p><img src=""https://i.imgur.com/qnd51I3.png"" alt=""enter image description here"" /></p>
<p>When pipeline is run, folders are renamed as per the requirement.</p>
<p>**Source File path: **
<img src=""https://i.imgur.com/OmxroyE.png"" alt=""enter image description here"" /></p>
<p><strong>Sink File Path:</strong>
<img src=""https://i.imgur.com/3ahnRJV.png"" alt=""enter image description here"" /></p>
"
"75546505","Populate Azure Data Factory with SQL Query that has complex views","<br>
In Azure Data Factory, by using the block ""Copy Activity"",<br>you can get data from queries that include a table.<br>
Sometimes a table can be seen as a Dataset but what about views ?
<p>can I include in my Copy Activity Queries like</p>
<pre><code>SELECT * FROM l2.View_table
</code></pre>
<p>or</p>
<pre><code>select * from l2.view_table vt
INNER JOIN l2.tableA ta on ta.id = vt.id 
</code></pre>
<p>?</p>
","<sql><sql-server><azure><azure-data-factory>","2023-02-23 14:55:53","36","0","1","75547086","<p>Yes, you can use a view or a query as your source.</p>
<p>Set &quot;use query&quot; to query in source options, and paste in your query.
<a href=""https://i.stack.imgur.com/ofwhj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ofwhj.png"" alt=""copy source"" /></a></p>
"
"75545025","How to swap the elements of an array","<p>Is there any way to swap the elemnts of an array </p>
<p>input = [1,2,3,4,5,6] </p>
<p>swap 2nd index and 3rd index values </p>
<p>Output= [1,2,4,3,5,6]</p>
","<azure-data-factory><azure-synapse>","2023-02-23 12:44:34","83","0","1","75552983","<p>In ADF, for swapping array there is not a dedicated function is not supported. So, use a temporary variable and then manipulate that temporary variable value to get desire result.</p>
<p>To swap array in ADF follow the below approach.</p>
<p>I have created array variables named  <code>x</code>  with values  <code>[1,2,3,4,5,6]</code> ,<code>temp_middle</code>,<code>temp_left</code>,<code>temp_right</code>,<code>temp_combined</code>,<code>swapped_array</code> and two pipeline parameters for indexes to be swapped named <code>a</code> and <code>b</code>.</p>
<p><img src=""https://i.imgur.com/MdnlnsC.png"" alt=""enter image description here"" /><img src=""https://i.imgur.com/3FMKwnZ.png"" alt=""enter image description here"" /></p>
<p>Then I split the <code>x</code> array using indexes in 3 set variables as left side of minimum index, right side of maximum index and middle part between both index.</p>
<ul>
<li>To get Left part in variable <code>temp_left</code> I used below expression.</li>
</ul>
<pre><code>@take(variables('x'), pipeline().parameters.a)
</code></pre>
<p><img src=""https://i.imgur.com/sd9h8Wr.png"" alt=""enter image description here"" /></p>
<ul>
<li>To get Right part in variable <code>temp_right</code> I used below expression.</li>
</ul>
<pre><code>@skip(variables('x'), add(pipeline().parameters.b,1))
</code></pre>
<p><img src=""https://i.imgur.com/7YZqpCd.png"" alt=""enter image description here"" /></p>
<ul>
<li>To get middle part in variable <code>temp_middle</code> I used below expression.</li>
</ul>
<pre><code>@take(skip(variables('x'),add(pipeline().parameters.a,1)),sub(pipeline().parameters.b,add(pipeline().parameters.a,1)))
</code></pre>
<p><img src=""https://i.imgur.com/9QFxqps.png"" alt=""enter image description here"" /></p>
<ul>
<li>After this I took If activity to check if middle part of array is empty or not with expression <code>@empty(variables('temp_middle'))</code></li>
</ul>
<p><img src=""https://i.imgur.com/HPjfrMt.png"" alt=""enter image description here"" /></p>
<ul>
<li>If it is empty control will go to true condition and under true condition, we took another set variables to combine all the left, right, middle part and swap the elements. In true condition as middle part is empty, we are omitting it.</li>
</ul>
<pre><code>@createArray(variables('temp_left'),variables('x')[pipeline().parameters.b],variables('x')[pipeline().parameters.a],variables('temp_right'))
</code></pre>
<p><img src=""https://i.imgur.com/ptVKvNR.png"" alt=""enter image description here"" /></p>
<p>This will give output like below.</p>
<p><img src=""https://i.imgur.com/iJsLoiI.png"" alt=""enter image description here"" /></p>
<ul>
<li>If it is not an empty control will go to false condition and under false condition, we took another set variables to combine all the left, right, middle part and swap the elements. In false condition as middle part is not an empty, we are including it.</li>
</ul>
<pre><code>@createArray(variables('temp_left'),variables('x')[pipeline().parameters.b],variables('temp_middle'),variables('x')[pipeline().parameters.a],variables('temp_right'))
</code></pre>
<p><img src=""https://i.imgur.com/oUeOhY9.png"" alt=""enter image description here"" /></p>
<ul>
<li>To get the array in required format, use the below expression and assign the value to required variable (same for both True and false condition).</li>
</ul>
<pre><code>@json(concat('[',replace(replace(string(variables('temp')), '[', ''),']',''),']'))
</code></pre>
<p><img src=""https://i.imgur.com/7kEV5ce.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/SHlM2nA.png"" alt=""enter image description here"" /></p>
"
"75541111","Job getting aborted when triggered through ADF","<p>Databricks job gets failed when it is called from adf with the error: <code>org.apache.spark.SparkException: Job aborted.</code></p>
<p>error details:</p>
<pre><code>Caused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 65536 bytes of memory, got 0
    at org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)
    at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)
    at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.&lt;init&gt;(UnsafeInMemorySorter.java:139)
    at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.&lt;init&gt;(UnsafeExternalSorter.java:165)
    at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)
    at org.apache.spark.sql.execution.UnsafeExternalRowSorter.&lt;init&gt;(UnsafeExternalRowSorter.java:112)
    at org.apache.spark.sql.execution.UnsafeExternalRowSorter.create(UnsafeExternalRowSorter.java:97)
</code></pre>
<p>I have tried giving retry 5 after every 500 sec, it used to solve the issue as it would run fine in one of the retries, but now even after 5 runs it gets errored out. But when tried directly from notebook it runs fine.</p>
<p>I think it is a memory issue, any solution to overcome this?</p>
","<azure><azure-data-factory><databricks>","2023-02-23 06:03:12","51","0","1","75569981","<p>OOM (Out of Memory) errors are quite common when working with increasing loads, or increasing size of data. (or more shuffles?)</p>
<p>Sometimes it is common to get thrown an internal server error without OOM being specified, however it still being the cause.</p>
<p>In this case, error message states OOM, which means you should increase your memory or scale your cluster.</p>
<p>P.S: retry is not advised in OOM scenarios, as the error tends to become permanent after some time, such as when the cluster cannot take it anymore.</p>
"
"75537467","Azure ForEach Activity with GetMetadata and Set Variable Pipeline Expression Builder","<p>I am trying to fetch the directory with subfolders in azure ADF with ForEach activity. inside activity i am calling the GetMetaData activity to get Set Variable activity to do the index value. please help me, how to handle in Pipeline expression builder. please find below my sequence of the pipeline.</p>
<blockquote>
<p>setVariable --&gt;Lookup(db connect)--&gt;forEach(inside) --&gt;
Getmetadata--&gt;setVariable.</p>
</blockquote>
<p>inside Set Variable, we are trying to loop the getmetadata but always taking first index, how do we do the dynamically.</p>
<p><a href=""https://i.stack.imgur.com/GPmUP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GPmUP.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/8rX8v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8rX8v.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/1Omk3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Omk3.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/dbMA0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dbMA0.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/B4TaS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B4TaS.png"" alt=""enter image description here"" /></a></p>
<p>expression builder code:</p>
<pre><code>@activity('GetParentFolderMetadata').output.childItems[0]['name']
</code></pre>
<p>if I give above one expression, it's always taking first record, how do we get all the index.</p>
","<azure-data-factory>","2023-02-22 19:42:14","210","0","1","75544126","<p>To achieve your requirement, you need two For Each activities. one to iterate the Lookup output and another for iterating each Get meta data activity output array.</p>
<p>But ADF does not support nested ForEach. So, use Execute pipeline activity inside first ForEach and you can use another ForEach in the child pipeline.</p>
<p>I have given <code>@item()</code> which is a folder name to Get Meta data activity.</p>
<p><img src=""https://i.imgur.com/E3pOrbx.png"" alt=""enter image description here"" /></p>
<p>Now create Execute activity and inside child pipeline create an array parameter. Give the Get meta data activity output <code>@activity('Get Metadata1').output.childItems</code> to this parameter.</p>
<p><img src=""https://i.imgur.com/6mpDzpY.png"" alt=""enter image description here"" /></p>
<p>Inside child pipeline use another ForEach and pass this parameter to it.</p>
<p><img src=""https://i.imgur.com/hQn6bmH.png"" alt=""enter image description here"" /></p>
<p>Now, you can store each file name <code>@item().name</code> into a set variable inside this ForEach and then use it as per your requirement.</p>
<p><img src=""https://i.imgur.com/dcOSfWk.png"" alt=""enter image description here"" /></p>
<p><strong>My Result in one iteration of Execute pipeline:</strong></p>
<p><img src=""https://i.imgur.com/ceezhXy.png"" alt=""enter image description here"" /></p>
"
"75535465","Passing a default value in Stored procedure","<p>I have a SP which writes the watermark value after each ADF pipeline run , below is the procedure :</p>
<pre><code>ALTER PROCEDURE [dbo].[usp_write_Time_watermark] @LastModifiedtime datetime, @TableName varchar(50)
AS

BEGIN

UPDATE dbo.Store_Time_Watermarktable
SET [WatermarkValue] = @LastModifiedtime 
WHERE [TableName] = @TableName
END
</code></pre>
<p>I want to add a condition which says if the lastmodified value is null the set default value as 1900-01-01 00:00:00.000 else take the new lastmodified time. How should i tweak the above procedure</p>
<p>Thanks</p>
","<sql><if-statement><stored-procedures><azure-data-factory>","2023-02-22 16:19:43","60","0","1","75536420","<p>You can use the <code>isnull</code> function, it returns the first non-null of two values.</p>
<pre><code>SET WatermarkValue = isnull(@LastModifiedtime, '19000101')
</code></pre>
<p>If you had more than two values you would use <code>coalesce</code></p>
"
"75534077","How to automate daily copying files that are being stored in the daily folder using Azure Data Factory","<p>I have a such scenario, in a data lake Gen2 files being generated daily and stored in a folder structure like for example yyyy/mm/dd, my task is picked all all the files from previous day from daily folder merge them and move to another location in a data lake Gen2</p>
<p>What would be the right approach? can I use exclusively only ADF or has to be a combination with some sort of controlling table that would have values from last day process, like for example if there was a problem and  files need to be processed from 3 days ago instead of yesterday, how do I know? how to keep track and what would be the appraoch to automate such process, I imagine it is scenario that been developed multiple times, thanks for all your help</p>
","<azure><etl><azure-data-lake><azure-data-factory>","2023-02-22 14:25:15","120","0","1","75540984","<ul>
<li>You can use dynamic content to get the yesterday's day using which you will be able to read all the files inside the folder structure <code>yyyy/MM/dd</code>.</li>
</ul>
<pre><code>@formatDateTime(addDays(utcNow(),-1))
</code></pre>
<p><img src=""https://i.imgur.com/rupXOZg.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>You can use the dynamically generated folder structure as wildcard path to read all the files in required folder.</li>
</ul>
<pre><code>source/@{formatDateTime(variables('yesterday'),'yyyy')}/@{formatDateTime(variables('yesterday'),'MM')}/@{formatDateTime(variables('yesterday'),'dd')}
</code></pre>
<p><img src=""https://i.imgur.com/mIwVaAg.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>In the sink, you can select your destination folder, give copy behavior as <code>Merge Files</code> (give required filename in sink dataset else random name will be generated).</li>
</ul>
<p><img src=""https://i.imgur.com/vBzpwzY.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>To automate this process daily, you can use <code>schedule trigger</code> as shown below. Navigate to <code>Add trigger -&gt; New/Edit -&gt; Choose new trigger</code>. You can select the intervals at which you want to run this pipeline (one day). and create the trigger.</li>
</ul>
<p><img src=""https://i.imgur.com/8EGeK3P.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>Publish the pipeline and this will trigger the pipeline daily, merging files from yesterday's folder to create a new merged file in destination. The following is pipeline JSON for reference:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;yesterdays date&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;wildcardFolderPath&quot;: {
                                &quot;value&quot;: &quot;source/@{formatDateTime(variables('yesterday'),'yyyy')}/@{formatDateTime(variables('yesterday'),'MM')}/@{formatDateTime(variables('yesterday'),'dd')}&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            },
                            &quot;wildcardFileName&quot;: &quot;*.csv&quot;,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobStorageWriteSettings&quot;,
                            &quot;copyBehavior&quot;: &quot;MergeFiles&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;typeConversion&quot;: true,
                        &quot;typeConversionSettings&quot;: {
                            &quot;allowDataTruncation&quot;: true,
                            &quot;treatBooleanAsNumber&quot;: false
                        }
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;src&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;dest&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            },
            {
                &quot;name&quot;: &quot;yesterdays date&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;yesterday&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@formatDateTime(addDays(utcNow(),-1))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;yesterday&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
"
"75533993","How to update Nth Element of an array","<p>Is it possible to update/replace the nth element of an array</p>
<p>input =[1,2,3,4]</p>
<p>Update 3rd element from 3 to 5</p>
<p>Output=[1,2,5,4]]</p>
","<azure-data-factory><azure-synapse>","2023-02-22 14:18:08","99","0","2","75535477","<p>In ADF, self-referencing variables is not supported. So, use a temporary variable first and then reassign that temporary variable value to required one.</p>
<p>If you only want to do it in ADF and not in dataflow, follow the below approach.</p>
<p>I have created two array variables named <code>x</code> with values <code>[1,2,3,4]</code> and <code>temp</code>.</p>
<p>I have created two parameters for <strong>nth element</strong> and <strong>updated value</strong>.</p>
<p><img src=""https://i.imgur.com/4dm9t70.png"" alt=""enter image description here"" /></p>
<p><strong>X value at start:</strong></p>
<p><img src=""https://i.imgur.com/obsB6JW.png"" alt=""enter image description here"" /></p>
<p>I have used the below expression to get the updated array.</p>
<pre><code>@createArray(take(variables('x'),sub(pipeline().parameters.nth,1)),pipeline().parameters.value,skip(variables('x'),pipeline().parameters.nth))
</code></pre>
<p><img src=""https://i.imgur.com/KZ1P1mG.png"" alt=""enter image description here"" /></p>
<p>This will give output like below.</p>
<p><img src=""https://i.imgur.com/OUrr2Yw.png"" alt=""enter image description here"" /></p>
<p>To get the array in required format, use the below expression and assign the value to required variable.</p>
<pre><code>@json(concat('[',replace(replace(string(variables('temp')), '[', ''),']',''),']'))
</code></pre>
<p><img src=""https://i.imgur.com/bQsKfmn.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/y3iXaqX.png"" alt=""enter image description here"" /></p>
"
"75533993","How to update Nth Element of an array","<p>Is it possible to update/replace the nth element of an array</p>
<p>input =[1,2,3,4]</p>
<p>Update 3rd element from 3 to 5</p>
<p>Output=[1,2,5,4]]</p>
","<azure-data-factory><azure-synapse>","2023-02-22 14:18:08","99","0","2","75540811","<p>Similar thread:
<a href=""https://learn.microsoft.com/en-us/answers/questions/1183224/how-to-update-nth-element-of-an-array"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/1183224/how-to-update-nth-element-of-an-array</a></p>
<pre><code>{
&quot;name&quot;: &quot;UpdateArray&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Set variable1&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;Inter&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@take(pipeline().parameters.IA,int(pipeline().parameters.ElementNbr))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('Inter')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;Final&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@item()&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Append variable2&quot;,
            &quot;type&quot;: &quot;AppendVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;ForEach1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;Final&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@pipeline().parameters.IS&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Set variable2&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Append variable2&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;Inter&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@skip(pipeline().parameters.IA,add(int(pipeline().parameters.ElementNbr),1))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach2&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable2&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('Inter')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1_copy1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;Final&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@item()&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        }
    ],
    &quot;parameters&quot;: {
        &quot;IA&quot;: {
            &quot;type&quot;: &quot;array&quot;,
            &quot;defaultValue&quot;: [
                1,
                2,
                3,
                4,
                5,
                9
            ]
        },
        &quot;IS&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;defaultValue&quot;: &quot;6&quot;
        },
        &quot;ElementNbr&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;defaultValue&quot;: &quot;3&quot;
        }
    },
    &quot;variables&quot;: {
        &quot;Inter&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;Final&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: []
}
</code></pre>
<p>}</p>
<p>But the proposal by @rakesh would be better :) rather than having multiple iterations</p>
"
"75533594","Azure Data Factory - get input from copy data activity","<p>How do I get the input of a copy activity?</p>
<p>I am using a copy activity, that executes a stored procedure, I want to retrieve the parameters that have been passed.</p>
<p>I can see in the JSON that it clearly stores the values, but how do I go about retrieving them?</p>
<p><a href=""https://i.stack.imgur.com/iX4SW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iX4SW.png"" alt=""Pipeline Copy Data"" /></a></p>
<p>Example of parameters that are passed;</p>
<p><a href=""https://i.stack.imgur.com/83Q75.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/83Q75.png"" alt=""Copy data input activity"" /></a></p>
","<azure-data-factory>","2023-02-22 13:45:37","104","0","1","75547467","<p>Currently, in ADF, there is no dynamic content for getting the input of the activity. You can see when I tried the below dynamic expression, I got the error saying that there is no property like <code>input</code>.</p>
<p><img src=""https://i.imgur.com/ZKWxbYx.png"" alt=""enter image description here"" /></p>
<p>You can raise the feature request for that <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">here</a>.</p>
<p>As an alternative, try to get it using <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/activity-runs/query-by-pipeline-run?source=recommendations&amp;tabs=HTTP#activityrun"" rel=""nofollow noreferrer"">Activity Runs - Query By Pipeline Run</a> REST API with a web activity.</p>
<p>For that, follow this <a href=""https://stackoverflow.com/questions/70100143/how-to-use-the-rest-api-for-activity-runs-in-azure-data-factory"">approach</a> by <strong>@KarthikBhyresh-MT</strong>.</p>
"
"75532990","update data from a table using a patch REST service in Azure Data Factory","<p>I appreciate if you can help me please.
Obtain the data from a table and through its id, take it as a reference in a Patch-type Rest service, send the data in the body.
I am new to azure data factory.
Thanks</p>
<p>I have a table that contains the following columns <code>id, field1, field2, field3.</code>
The idea is for a PATCH method on the endpoint <code>https://api.localhost.com/objects/contac/{id}</code>
to be able to update the fields of the columns by the id that I have in the table, the body that the endpoint takes is</p>
<pre><code>{
   &quot;properties&quot;:
   {
     &quot;data1&quot;: &quot;field1&quot;,
     &quot;data2&quot;: &quot;field2&quot;,
     &quot;data3&quot;: &quot;field3&quot;
}
}
</code></pre>
<p>I have tried with an activity such as copy data, through a pipeline and I have not been able to, I am new to azure data factory.</p>
","<azure-data-factory>","2023-02-22 12:56:55","126","0","1","75612133","<ul>
<li>Take a <code>lookup</code> activity to get the rows of your table with id and field values.</li>
</ul>
<p><img src=""https://i.imgur.com/ZOWIgMB.png"" alt=""enter image description here"" /></p>
<ul>
<li>This returns each row as an object inside an array. So you can iterate through this data using for each loop with items value as:</li>
</ul>
<pre><code>@activity('Lookup1').output.value
</code></pre>
<ul>
<li>Inside foreach loop, you can use a <code>web activity</code>. The following would be the dynamic content that you can use to build URL and Body for <code>Patch</code> method:</li>
</ul>
<pre><code>URL: 
https://api.localhost.com/objects/contac/@{item().id}

Body: 
{
&quot;properties&quot;:
{
&quot;data1&quot;: &quot;@{item().field1}&quot;,
&quot;data2&quot;: &quot;@{item().field2}&quot;,
&quot;data3&quot;: &quot;@{item().field3}&quot;
}
}
</code></pre>
<p><img src=""https://i.imgur.com/nWgpLYU.png"" alt=""enter image description here"" /></p>
<ul>
<li>When you run this pipeline, you should be able to get what you need. Copy data does not fit your requirement and the REST api linked service does not support PATCH method as well.</li>
</ul>
<p>NOTE: My pipeline failed as I don't have a REST API for patch service.</p>
"
"75532260","can we do any kind of password protection or encryption in folder of a container in Azure data factory","<p>I'm trying to give access control based on the folder inside the container on the Azure Data Lake Storage Gen 2 . Can someone please help me out on this one by giving a quick walkthrough on it?</p>
<p>I've tried by doing it Azure key vault but it protects the whole container .I want only to protect the folders inside a container.</p>
","<azure-data-factory><azure-data-lake-gen2>","2023-02-22 11:43:31","34","0","1","75544749","<p>You can control access to specific folders inside a container on Azure Data Lake Storage Gen2 using Access Control Lists (ACLs) or Role-Based Access Control (RBAC).
You can follow below procedure:
Go to the specific folder Right click the object and select Manage ACL</p>
<p><img src=""https://i.imgur.com/6N2ND0D.png"" alt=""enter image description here"" /></p>
<p>In the Access permission tab click on Add service principal to add the specified user or group to give control of access</p>
<p><img src=""https://i.imgur.com/N0ohWhm.png"" alt=""enter image description here"" /></p>
<p>Add the service principal and give the permissions regarding to your requirement and click on save button</p>
<p><img src=""https://i.imgur.com/yJWhgv6.png"" alt=""enter image description here"" /></p>
<p>For more information you can refer <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-acl-azure-portal"" rel=""nofollow noreferrer"">this</a> .</p>
"
"75532101","Extract Data from API in ADF","<p>I need to extract data from REST API in ADF, which I am doing using WEB activity and COPY activity.</p>
<p>When used Web activity, I am getting extra <code>\</code> in response as below.</p>
<pre><code>{
    &quot;Response&quot;: &quot;[{\&quot;Name\&quot;:\&quot;Countries\&quot;,\&quot;Url\&quot;:\&quot;/api/Datab/abc\&quot;,\&quot;Trees\&quot;:[{\&quot;Name\&quot;:\&quot;Economics Locations\&quot;},{\&quot;Name\&quot;:\&quot;Economics Indicators\&quot;}]},{\&quot;Name\&quot;:\&quot;Global\&quot;,\&quot;Url\&quot;:\&quot;/api/Datab/def\&quot;,\&quot;DatabankCode\&quot;:\&quot;WDMacro\&quot;,\&quot;StartYear\&quot;:1980,\&quot;EndYear\&quot;:2050,\&quot;HasQuarterlyData\&quot;:true,\&quot;Trees\&quot;:[{\&quot;Name\&quot;:\&quot;Global Economics Locations\&quot;},{\&quot;Name\&quot;:\&quot;Global Economics Indicators\&quot;}]}]&quot;,
    &quot;ADFWebActivityResponseHeaders&quot;: {
        &quot;Pragma&quot;: &quot;no-cache&quot;,
        &quot;Rate-Limit-60&quot;: &quot;Unthrottled (0/60 requests) in 60 sec window&quot;,
        &quot;Rate-Limit-60-Status&quot;: &quot;Unthrottled&quot;,
        &quot;Rate-Limit-60-Window&quot;: &quot;60&quot;,
        &quot;Rate-Limit-60-RequestCount&quot;: &quot;0&quot;,
        &quot;Rate-Limit-60-RequestLimit&quot;: &quot;60&quot;,
        &quot;Cache-Control&quot;: &quot;no-cache&quot;,
        &quot;Date&quot;: &quot;Mon, 20 Feb 2023 11:32:52 GMT&quot;,
        &quot;Server&quot;: &quot;Microsoft-IIS/10.0&quot;,
        &quot;X-AspNet-Version&quot;: &quot;4.0.30319&quot;,
        &quot;X-Powered-By&quot;: &quot;ASP.NET&quot;,
        &quot;Content-Length&quot;: &quot;17782&quot;,
        &quot;Content-Type&quot;: &quot;application/json; charset=utf-8&quot;,
        &quot;Expires&quot;: &quot;-1&quot;
    },
    &quot;effectiveIntegrationRuntime&quot;: &quot;AutoResolveIntegrationRuntime (West Europe)&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;ExternalActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p>When I Used COPY activities(created REST DS and Linked service), I got below issue, I am assuming this is because of <code>\</code> .
<a href=""https://i.stack.imgur.com/2HBEb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2HBEb.png"" alt=""enter image description here"" /></a></p>
<p>Can someone please help me,</p>
<ol>
<li>How to get rid of <code>\</code> in response of WEB activity output?</li>
<li>How to make COPY activity WORK, So I can sink to Storage account?</li>
<li>I need to make another API call for each <code>Url</code> of Response?
As in this output, How do I parse <code>Url</code> to capture <code>/api/Datab/abc</code> and <code>/api/Datab/def</code></li>
</ol>
","<json><azure><rest><azure-data-factory>","2023-02-22 11:28:19","59","0","1","75533366","<p>I tried to reproduce the similar issue and got similar response with extra <code>slash(\)</code></p>
<p><img src=""https://i.imgur.com/MT1OLbV.png"" alt=""enter image description here"" /></p>
<p>Then I took set variable activity and created <strong>array</strong> variable with name <code>demo</code>  and converted it into Json format.</p>
<pre><code>@json(activity('Web1').output.Response)
</code></pre>
<p><img src=""https://i.imgur.com/s9cFbSA.png"" alt=""enter image description here"" /></p>
<p>this will give you correct format of array.</p>
<p><img src=""https://i.imgur.com/gn8YUte.png"" alt=""enter image description here"" /></p>
<p>Then pass this array variable to Foreach activity as below.</p>
<p><img src=""https://i.imgur.com/15elG6M.png"" alt=""enter image description here"" /></p>
<p>To access Url from this variable in foreach activity use <code>@item().Url</code></p>
<p><img src=""https://i.imgur.com/yO7jWy1.png"" alt=""enter image description here"" /></p>
<p><strong>OUTPUT</strong></p>
<p><img src=""https://i.imgur.com/9fvghdw.png"" alt=""enter image description here"" /></p>
"
"75531700","Json file to sql table in ADF","<p>Im trying to load a json file into a table in azure, the json file looks like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;Status&quot;: {
      &quot;0&quot;: &quot;Fictitious&quot;,
      &quot;1&quot;: &quot;To be checked&quot;,
      &quot;2&quot;: &quot;Created&quot;
   },
  &quot;TimePeriod&quot;: {
    &quot;Year&quot;: &quot;year&quot;,
    &quot;Quarter&quot;: &quot;quarter&quot;,
    &quot;Month&quot;: &quot;month&quot;,
    &quot;Week&quot;: &quot;week&quot;,
    &quot;Day&quot;: &quot;day&quot;
  }
}

</code></pre>
<p>And i want to load it into an azure sql database table that have this structure:</p>
<p><a href=""https://i.stack.imgur.com/iJkMx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iJkMx.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/7KfI8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7KfI8.png"" alt=""enter image description here"" /></a></p>
<p>I want to do this in ADF and i tried to use a dataflow where i unpivot the file but i can´t seem to find a way to unpivot it into rows and also get the &quot;Status&quot; as an own column.
I can´t find any examples where the json file have this structure, any idea or suggestion?</p>
","<json><azure-data-factory>","2023-02-22 10:52:25","67","0","1","75540585","<ul>
<li>You can achieve this using derived column, select and unpivot transformations. The following is how the source data looks like (sample taken as given in the question).</li>
</ul>
<p><img src=""https://i.imgur.com/LMUcqpH.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>Use derived column transformation to create 4 new columns as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/Ca6bPjV.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>Use select transformation to get only the above created columns:</li>
</ul>
<p><img src=""https://i.imgur.com/EQmvSuS.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>Now in unpivot transformation, ungroup by <code>Mode</code> column, select unpivot key as <code>Status</code> and unpivoted columns as <code>definition</code>. This would give result as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/rxRiKzy.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>You can write this to the required sink azure SQL database table as per requirement. The following is the dataflow JSON (incomplete) for the above described steps:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Json1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;derivedColumn1&quot;
                },
                {
                    &quot;name&quot;: &quot;select1&quot;
                },
                {
                    &quot;name&quot;: &quot;unpivot1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          Status as ({0} as string, {1} as string, {2} as string),&quot;,
                &quot;          TimePeriod as (Year as string, Quarter as string, Month as string, Week as string, Day as string)&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false,&quot;,
                &quot;     documentForm: 'singleDocument') ~&gt; source1&quot;,
                &quot;source1 derive({0} = Status.{0},&quot;,
                &quot;          {1} = Status.{1},&quot;,
                &quot;          {2} = Status.{2},&quot;,
                &quot;          Mode = 'Status') ~&gt; derivedColumn1&quot;,
                &quot;derivedColumn1 select(mapColumn(&quot;,
                &quot;          Mode,&quot;,
                &quot;          {0},&quot;,
                &quot;          {1},&quot;,
                &quot;          {2}&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select1&quot;,
                &quot;select1 unpivot(output(&quot;,
                &quot;          Status as string,&quot;,
                &quot;          definition as string&quot;,
                &quot;     ),&quot;,
                &quot;     ungroupBy(Mode),&quot;,
                &quot;     lateral: true,&quot;,
                &quot;     ignoreNullPivots: false) ~&gt; unpivot1&quot;
            ]
        }
    }
}
</code></pre>
<p>Updated Dataflow JSON to transform time period data as well (sinks are yet to be configured as per your requirement into SQL table):</p>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Json1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;status&quot;
                },
                {
                    &quot;name&quot;: &quot;select1&quot;
                },
                {
                    &quot;name&quot;: &quot;unpivot1&quot;
                },
                {
                    &quot;name&quot;: &quot;timeperiod&quot;
                },
                {
                    &quot;name&quot;: &quot;select2&quot;
                },
                {
                    &quot;name&quot;: &quot;unpivot2&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          Status as ({0} as string, {1} as string, {2} as string),&quot;,
                &quot;          TimePeriod as (Year as string, Quarter as string, Month as string, Week as string, Day as string)&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false,&quot;,
                &quot;     documentForm: 'singleDocument') ~&gt; source1&quot;,
                &quot;source1 derive({0} = Status.{0},&quot;,
                &quot;          {1} = Status.{1},&quot;,
                &quot;          {2} = Status.{2},&quot;,
                &quot;          Mode = 'Status') ~&gt; status&quot;,
                &quot;status select(mapColumn(&quot;,
                &quot;          Mode,&quot;,
                &quot;          {0},&quot;,
                &quot;          {1},&quot;,
                &quot;          {2}&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select1&quot;,
                &quot;select1 unpivot(output(&quot;,
                &quot;          Status as string,&quot;,
                &quot;          definition as string&quot;,
                &quot;     ),&quot;,
                &quot;     ungroupBy(Mode),&quot;,
                &quot;     lateral: true,&quot;,
                &quot;     ignoreNullPivots: false) ~&gt; unpivot1&quot;,
                &quot;source1 derive(Year = TimePeriod.Year,&quot;,
                &quot;          Quarter = TimePeriod.Quarter,&quot;,
                &quot;          Month = TimePeriod.Month,&quot;,
                &quot;          Week = TimePeriod.Week,&quot;,
                &quot;          Day = TimePeriod.Day,&quot;,
                &quot;          name = 'TimePeriod') ~&gt; timeperiod&quot;,
                &quot;timeperiod select(mapColumn(&quot;,
                &quot;          Year,&quot;,
                &quot;          Quarter,&quot;,
                &quot;          Month,&quot;,
                &quot;          Week,&quot;,
                &quot;          Day,&quot;,
                &quot;          name&quot;,
                &quot;     ),&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true) ~&gt; select2&quot;,
                &quot;select2 unpivot(output(&quot;,
                &quot;          TimePeriod as string,&quot;,
                &quot;          Value as string&quot;,
                &quot;     ),&quot;,
                &quot;     ungroupBy(name),&quot;,
                &quot;     lateral: true,&quot;,
                &quot;     ignoreNullPivots: false) ~&gt; unpivot2&quot;
            ]
        }
    }
}
</code></pre>
"
"75525027","How to construct a FETCH query to Fetch all fields in Dynamic CRM table with Azure Data Factory","<p>I would like help fetching a table in Dynamics CRM with Azure Data Factory</p>
<p>I use the following parameters to select all fields from a regular table</p>
<pre><code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
</code></pre>
<p>When contructing a query to select all fields from a table in Dynamics CRM there is are requirement to use FETCH method, see image</p>
<p><a href=""https://i.stack.imgur.com/NKMd8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NKMd8.png"" alt=""enter image description here"" /></a></p>
<p>I would like help converting the parameters</p>
<pre><code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
</code></pre>
<p>To work with FETCH as shown in the image.</p>
<p>My original thoughts were I needed to do the following:</p>
<pre><code>@concat('FETCH * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
</code></pre>
<p>But that didn't work.</p>
<p>The error I'm getting is:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=DynamicsOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Dynamics operation failed with error code: DynamicsOperationFailed, error message: The Fetch Xml query specified is invalid..,Source=Microsoft.DataTransfer.ClientLibrary.DynamicsPlugin,''Type=System.Xml.XmlException,Message=Data at the root level is invalid. Line 1, position 1.,Source=System.Xml,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy From CRM to SQLDB&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>Any thoughts?</p>
","<azure-data-factory>","2023-02-21 19:25:41","68","0","1","75528864","<ul>
<li><p>The error <code>The Fetch Xml query specified is invalid</code> indicates that the fetch query is invalid.</p>
</li>
<li><p>As given in the query (sample of fetch), the query has to be built. Hence try using the following dynamic content instead in the query section:</p>
</li>
</ul>
<pre><code>&lt;fetch&gt;
&lt;entity name=&quot;@{pipeline().parameters.domain}.@{pipeline().parameters.table}&quot;&gt;
&lt;all-attributes/&gt;
&lt;/entity&gt;
&lt;/fetch&gt;
</code></pre>
<ul>
<li>The above would generate a fetch query in required format. I have used set variable activity to demonstrate how the query would be built.</li>
</ul>
<p><img src=""https://i.imgur.com/Xb6Se8g.png"" alt=""enter image description here"" /></p>
"
"75524839","Reading JSONs in Azure Synapse Notebooks with Multiple Lines","<p>Good afternoon,</p>
<p>I'm attempting to read log data into an Azure Synapse notebook using a Pyspark dataframe. Currently the data lives in a container, and I'm attempting to access it via it's link:</p>
<pre><code>abfss://insights-logs-integrationpipelineruns@{x}.dfs.core.windows.net/resourceId=....
</code></pre>
<p>but I'm running into an issue in that the JSON file includes multiple lines</p>
<p><code>{ &quot;Level&quot;: 4, &quot;correlationId&quot;: &quot;x&quot;, &quot;time&quot;: &quot;y&quot;, &quot;runId&quot;: &quot;z&quot;, &quot;resourceId&quot;:...}</code></p>
<p><code>{ &quot;Level&quot;: 4, &quot;correlationId&quot;: &quot;a&quot;, &quot;time&quot;: &quot;b&quot;, &quot;runId&quot;: &quot;c&quot;, &quot;resourceId&quot;:...}</code></p>
<p><code>{ &quot;Level&quot;: 4, &quot;correlationId&quot;: &quot;d&quot;, &quot;time&quot;: &quot;e&quot;, &quot;runId&quot;: &quot;f&quot;, &quot;resourceId&quot;:...}</code></p>
<p>I tried using the native spark.read.json method, and also by setting the multiline option to true as follows:</p>
<pre><code>df = spark.read.option(&quot;multiLine&quot;,&quot;true&quot;).json(uri)
</code></pre>
<p>But I get the following error:</p>
<p><strong>AnalysisException: Found duplicate column(s) in the data schema: <code>level</code></strong></p>
<p>The thing is, the data I need is nested inside each line. If there's a way to just load the JSON file as a dictionary, I'd know how to parse it so that I could get the data I need.</p>
<p>Otherwise, if there's no way to do that, is there a way for me to specify pyspark to only read portions of a file?</p>
","<python><azure><pyspark><azure-data-factory><azure-synapse>","2023-02-21 19:02:53","49","0","1","75525043","<p>Adding this to my notebook fixed my issue:</p>
<pre><code>spark.conf.set(&quot;spark.sql.caseSensitive&quot;, &quot;true&quot;)
</code></pre>
"
"75519005","Can we make the ADF JSON mapping in COPY Activity Case-insensitive?","<p>So, actually I'm using the ADF Copy Activity to load the CSV files from the azure blob storage to snowflake table. And column mapping is done using the json with source and sink column name as below:</p>
<pre><code>{ &quot;type&quot;: &quot;TabularTranslator&quot;,
            &quot;mappings&quot;: [
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;first_name&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;FIRST_NAME&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;name&quot;: &quot;Address&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;ADDRESS&quot;,
                        &quot;type&quot;: &quot;String&quot;
                    }
                }
</code></pre>
<p>my csv file will come with the two column <strong>first_name</strong> and <strong>Address</strong> but the problem here is that I can receive the column name in any case like <strong>FIRST_NamE</strong> or <strong>address</strong>.</p>
<p>So is there any way to make this column mapping case-insensitive?</p>
","<snowflake-cloud-data-platform><azure-data-factory>","2023-02-21 10:12:27","144","1","1","75520389","<p>You can build the mapping dynamically and then with the help of <code>toUpper()</code> function, you can convert the column name to uppercase.</p>
<p>The mapping format would be same as given in the question which I have built dynamically using:</p>
<ul>
<li>Get Metadata activity to get the header as single value as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/FUkqFzb.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>Set variable activity to build the required map.</li>
</ul>
<p><img src=""https://i.imgur.com/7LWA3Tf.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>For each to create an array of each column map.</li>
</ul>
<p><img src=""https://i.imgur.com/4GZ2SBl.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>The last set variable activity to create the final mapping.</li>
</ul>
<p><img src=""https://i.imgur.com/NZJzoEm.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>Finally, copy data activity to with desired mapping as dynamic content.</li>
</ul>
<p><img src=""https://i.imgur.com/8cB8OOA.png"" alt=""enter image description here"" /></p>
<br>
<p>The result would be as shown below:</p>
<p><img src=""https://i.imgur.com/iwl85tG.png"" alt=""enter image description here"" /></p>
<br>
<p>The following are the pipeline JSON for:</p>
<p>Pipeline JSON:</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline3&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;get comma seperated header&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;mapping&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@split(activity('get comma seperated header').output.firstRow['Prop_0'],',')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;map&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;maps&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;{\&quot;source\&quot;: {\&quot;name\&quot;: \&quot;@{item()}\&quot;,\&quot;type\&quot;: \&quot;String\&quot;},\&quot;sink\&quot;: {\&quot;name\&quot;: \&quot;@{toUpper(item())}\&quot;,\&quot;type\&quot;: \&quot;String\&quot;}}&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;mapping&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;get comma seperated header&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;map&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;{\&quot;type\&quot;: \&quot;TabularTranslator\&quot;,&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;final&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;final&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@concat(variables('map'),'\&quot;mappings\&quot;: [',join(variables('maps'),','),']}')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;final&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;value&quot;: &quot;@json(variables('final'))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;src&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;variables&quot;: {
            &quot;map&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;maps&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;final&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<br>
<ul>
<li>Get Metadata source dataset JSON:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;csv1&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;adls&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;DelimitedText&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
                &quot;fileName&quot;: &quot;input.csv&quot;,
                &quot;fileSystem&quot;: &quot;data&quot;
            },
            &quot;columnDelimiter&quot;: &quot;|&quot;,
            &quot;escapeChar&quot;: &quot;\\&quot;,
            &quot;firstRowAsHeader&quot;: false,
            &quot;quoteChar&quot;: &quot;\&quot;&quot;
        },
        &quot;schema&quot;: [
            {
                &quot;name&quot;: &quot;FiRsT_NaME&quot;,
                &quot;type&quot;: &quot;String&quot;
            },
            {
                &quot;name&quot;: &quot;Address&quot;,
                &quot;type&quot;: &quot;String&quot;
            }
        ]
    }
}
</code></pre>
<p><strong>NOTE:</strong> I have used csv as sink for demonstration.</p>
"
"75518302","How to pass trigger parameters to notebook in Azure Data Factory","<p>In Databrick Notebook, I would like to know which file trigerred pipeline.</p>
<p>I have prepared Azure Data Factory pipeline. It got blob event based trigger and It runs databricks notebook.
In databricks notebook I would like to know which file trigerred pipeline[its path as well]</p>
<p>Trigger itself works, as it triggers when i upload a file.</p>
<p>I have prepared pipeline parameters of type string and values like
SourceFile    string  @triggerBody().fileName
SourceFolder  string  @trigger().outputs.body.folderPath</p>
<p>In task i have prepared parameters like:
SourceFile    @pipeline().parameters.SourceFile
SourceFolder  @pipeline().parameters.SourceFolder</p>
<p>In Databricks Notebook such a code:</p>
<pre><code>dbutils.widgets.text(&quot;SourceFolder&quot;, &quot;&quot;,&quot;&quot;)
y = dbutils.widgets.get('SourceFolder')
print (&quot;Param -\'SourceFolder':&quot;)
print (y)
</code></pre>
<p>prints this:</p>
<p>Param -'SourceFolder':
@trigger().outputs.body.folderPath</p>
<p>I would like to see path or file name not command.</p>
","<azure><azure-data-factory><databricks>","2023-02-21 09:07:57","121","0","1","75519911","<p>I tried the above in my environment and it is working fine for me.</p>
<p>I created two parameters <code>foldername</code> and <code>filename</code>.</p>
<p>I have created the trigger like below.</p>
<p><img src=""https://i.imgur.com/K8Nw6sy.png"" alt=""enter image description here"" /></p>
<p>Give the trigger parameters <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code> to the pipeline parameters like below.</p>
<p><img src=""https://i.imgur.com/NIht6sL.png"" alt=""enter image description here"" /></p>
<p>Then I have created a set variable to create the file path with below dynamic content.</p>
<p><code>@concat(pipeline().parameters.foldername,'/',pipeline().parameters.filename)</code></p>
<p><img src=""https://i.imgur.com/GsnWkns.png"" alt=""enter image description here"" /></p>
<p>Pass this as Notebook base parameter.</p>
<p><img src=""https://i.imgur.com/4BMuNum.png"" alt=""enter image description here"" /></p>
<p><strong>Notebook Execution:</strong></p>
<p><img src=""https://i.imgur.com/23gqFKH.png"" alt=""enter image description here"" /></p>
<p>Also, I tried with <code>@trigger().outputs.body.fileName</code> and <code>@trigger().outputs.body.folderPath</code> and it is working fine for me.</p>
<p>First try with a set variable and see if you are getting the file path from trigger parameters.</p>
<p>If still, it is giving the same, then it's better to raise a <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">Support ticket</a> for your issue.</p>
"
"75516726","Copy Data from REST API to Blob Storage","<p>Do we need some additional permissions when we are writing the data from Rest API to a container that is behind a firewall ?</p>
<p>I can copy data easily to container without firewall but the data it fails writing to firewall container with the error :</p>
<blockquote>
<p>ErrorCode=HttpFileFailedToRead,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read data from http server. Check the error from http server：The remote server returned an error: (403) Forbidden.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (403) Forbidden.,Source=System,'</p>
</blockquote>
","<azure><azure-data-factory>","2023-02-21 05:50:54","122","0","1","75529379","<blockquote>
<p>I can copy data easily to container without firewall but the data it fails writing to firewall container with the error.</p>
</blockquote>
<p>To load data in Container which is behind firewall Rules of storage account like <code>Private network access</code> are <strong>Enabled from selected virtual networks and IP addresses</strong>.</p>
<ol>
<li>You need to crate Managed private endpoints for that particular blob storage in ADF.</li>
</ol>
<p><strong>Manage &gt;&gt; Managed private endpoints &gt;&gt; New &gt;&gt;Azure Blob Storage/ Azure ADLS gen2 storage &gt;&gt; Continue.</strong></p>
<p>Provide Your subscription and Storage account name and click on <code>create</code>.</p>
<p><img src=""https://i.imgur.com/ty5VwE9.png"" alt=""enter image description here"" /></p>
<ol start=""2"">
<li>Approve this end points from your storage account by</li>
</ol>
<p><strong>Networking &gt;&gt; Private endpoint connections &gt;&gt; select endpoint requested by data factory &gt;&gt; Approve.</strong></p>
<p><img src=""https://i.imgur.com/hsaD2sw.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>Create Azure Integration runtime with <strong>Virtual network &gt;&gt; virtual network configuration as Enabled</strong></li>
</ol>
<p><img src=""https://i.imgur.com/MS167NY.png"" alt=""enter image description here"" /></p>
<ol start=""4"">
<li>Now with this Integration runtime create Linked service for particular storage account and run the pipeline.</li>
</ol>
<p><img src=""https://i.imgur.com/LjoShcN.png"" alt=""enter image description here"" /></p>
<p><strong>Successfully able to copy data from Rest API to a container that is behind a firewall.</strong></p>
<p><img src=""https://i.imgur.com/sOA0X29.png"" alt=""enter image description here"" /></p>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/managed-virtual-network-private-endpoint"" rel=""nofollow noreferrer"">Managed private endpoints</a></p>
"
"75516035","Azure Data Factory ForEach Activity to use DataBricks Notebook Functionality","<p>I have requirement to Fech the list of values from Azure SQL database table column value to give the input to FOREACH activity and inside ForEach activity, we are calling azure databricks notebook. how do we pass the value end to end. specifically, how do we get parameter value from foreach to notebook.</p>
<p>below scenario I tried but facing issues.</p>
<blockquote>
<p>Lookup activity(connected source dataset (azure sql db) --&gt; ForEach
activity(@activity('Lookup1').output.value) --- &gt; notebook (how do we
get parameter)</p>
</blockquote>
<p>Notebook input:</p>
<pre><code>{
    &quot;notebookPath&quot;: &quot;/Users/notebookpath/TestForach&quot;,
    &quot;baseParameters&quot;: {
        &quot;forachparam&quot;: {
            &quot;SourceDetailsID&quot;: 1,
            &quot;SystemName&quot;: &quot;test&quot;,
            &quot;FilePath&quot;: &quot;test/path/&quot;,
            &quot;LoadFrequencyType&quot;: &quot;xxx&quot;,
            &quot;IsActive&quot;: true,
            &quot;LastLoadDateTime&quot;: &quot;2023-02-20T17:21:44.117Z&quot;
        }
    }
}
</code></pre>
<p>error :</p>
<p>Activity failed because an inner activity failed for notebook level</p>
<blockquote>
<p>The value type
'System.Collections.Generic.Dictionary`2[[System.String, mscorlib,
Version=4.0.0.0, Culture=neutral,
PublicKeyToken=adssdasd],[System.Object, mscorlib, Version=4.0.0.0,
Culture=neutral, PublicKeyToken=sdasddasd]]', in key 'forachparam' is
not expected type 'System.String'</p>
</blockquote>
<p>We tried below parameter read in notebook level</p>
<pre><code>dbutils.widgets.text(&quot;item&quot;,&quot;&quot;)
item = dbutils.widgets.get(&quot;item&quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/vSRBe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vSRBe.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/NoI7R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NoI7R.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/10V9Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/10V9Z.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/0MuvI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0MuvI.png"" alt=""enter image description here"" /></a></p>
<p>Please advice me.</p>
","<azure-data-factory><azure-databricks>","2023-02-21 03:27:32","91","0","1","75517013","<p>I got same error, when I tried in my environment.</p>
<p><img src=""https://i.imgur.com/SJDDUEc.png"" alt=""enter image description here"" /></p>
<p>This error occurs when you directly gave <code>@item()</code>(Object value) to the Notebook parameter. Currently, Notebook activity do not support passing Object. So, pass it as a string <code>@string(item())</code> and convert back to JSON using <code>json</code> module like below.</p>
<p><img src=""https://i.imgur.com/eIQfDHx.png"" alt=""enter image description here"" /></p>
<p><strong>Notebook Execution from ADF:</strong></p>
<p><img src=""https://i.imgur.com/wnroIM4.png"" alt=""enter image description here"" /></p>
"
"75515933","Restrict Non-Owners from changing the Collaboration branch of Azure Data Factory using Azure Policy","<p>We would like to impose the following restrictions on Azure Data Factory</p>
<ol>
<li>Don't let users change the collaboration branch from develop to anything else.</li>
<li>Allow only Owners to change the collaboration branch to any other branch if required.</li>
</ol>
<p>We were able to achieve the #1 feature using the below policy</p>
<pre><code>   {
      &quot;mode&quot;: &quot;All&quot;,
      &quot;policyRule&quot;: {
      &quot;if&quot;: {
      &quot;allOf&quot;: [
        {
          &quot;field&quot;: &quot;type&quot;,
          &quot;equals&quot;: &quot;Microsoft.DataFactory/factories&quot;
        },
        {
          &quot;field&quot;: &quot;name&quot;,
          &quot;equals&quot;: &quot;adfproject-dev-df1&quot;
        },
        {
          &quot;field&quot;: &quot;Microsoft.DataFactory/factories/repoConfiguration.collaborationBranch&quot;,
          &quot;notEquals&quot;: &quot;develop&quot;
        }
      ]
    },
    &quot;then&quot;: {
      &quot;effect&quot;: &quot;deny&quot;
    }   
  } ,
  &quot;parameters&quot;: {}
}
</code></pre>
<p>However, the below rule (within the allOf condition/array for #2 is not working. any pointers would be of great help.</p>
<pre><code> &quot;field&quot;: &quot;Microsoft.Authorization/roleAssignments/principalRole&quot;,
 &quot;notEquals&quot;: &quot;Owner&quot;
</code></pre>
<p>Thanks,
Praveen</p>
","<azure-data-factory><azure-policy>","2023-02-21 03:00:12","55","0","1","75554337","<blockquote>
<p>Instead of having all users in one customer role and then apply policy
on them, I feel you can consider creating another custom role in which
you can add users who are allowed to change collaboration branch. And
don't have Azure policy on the role.</p>
</blockquote>
<p>That way this new custom role users are free to change collaboration branch as well.</p>
"
"75513463","How can I load a fixed length (no delimiter) .DAT file to a SQL Server table using Azure Data Factory?","<p>I have fixed length .DAT files in a ftp server and I need to bring them to ADLS using ADF in a .TXT format to be able to transform data in ADF before loading into a SQL Server table.</p>
<p>So far, I could copy the table as is in ADLS as a .DAT file. When I try to create a dataset in a text format in ADF it does not allow me copy the file without specifying the delimiter.</p>
<p>I need a solution where without any delimiter specified I am able to convert .DAT file to a .TXT file and store it in ADLS or be able to use the dataset without copying files to ADLS to transform dataset contents to do column mapping.</p>
<p>I would need to update the dataset daily because new file would be added everyday to the ftp server.</p>
<p>Any help is appreciated. Thank you.</p>
<p>I tried the following:
Create a dataset in ADF by specifying &quot;NO DELIMITER&quot; and was able to preview data as shown <a href=""https://i.stack.imgur.com/VKVda.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VKVda.png"" alt=""Dataset"" /></a>.This however does not allow me to copy dataset to ADLS because copy requires to define a delimiter.</p>
<p>Can I transform this dataset in the pipeline without copying to ADLS? Next step is to do mapping to load data from this dataset into a SQL Server table:<a href=""https://i.stack.imgur.com/EfJjn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EfJjn.png"" alt=""Column Mapping Example"" /></a></p>
","<azure><dataset><azure-data-factory><data-transform>","2023-02-20 19:35:15","102","0","1","75517340","<blockquote>
<p>Create a dataset in ADF by specifying &quot;NO DELIMITER&quot; and was able to preview data. But this does not allow me to copy dataset to ADLS because copy requires to define a delimiter.</p>
</blockquote>
<ul>
<li><em><strong>When the column delimiter is defined as empty string, which means no delimiter, the whole line is taken as a single column.</strong></em></li>
<li><em><strong>Currently, column delimiter as empty string is only supported for mapping data flow but not Copy activity.</strong></em></li>
</ul>
<p>So, if you are using copy activity with no delimiter you will get this error in validation as below.</p>
<p><img src=""https://i.imgur.com/KLn7qYa.png"" alt=""enter image description here"" /></p>
<p>To avoid this the work around is to use <strong><code>Data Flow activity</code></strong> to copy file with no delimiter from <strong>source</strong> to <strong>sink</strong>.</p>
<blockquote>
<p>I reproduced with <strong><code>No delimiter</code></strong> in dataflow.</p>
</blockquote>
<ul>
<li><p>Dataset:
<img src=""https://i.imgur.com/H7hHghM.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Dataflow using same file.
<img src=""https://i.imgur.com/Za4b5Pr.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Pipeline running Successfully.
<img src=""https://i.imgur.com/Ox3faA8.png"" alt=""enter image description here"" /></p>
</li>
</ul>
"
"75511705","Dynamically update filed value in ADF Copy activity","<p>I am trying to perform copy activity in ADF. Source for the pipeline is API(Json response) and Sink is Azure Sql data base.</p>
<p>I want to check, if the enddat is null then only i have take name field and map to sink column. Kindly note that i am using copy activity not data flow.</p>
<p>Response from API is in below format:</p>
<pre><code>[
{
        &quot;id&quot;: &quot;8d6c&quot;,
        &quot;customsiteid&quot;: &quot;wew&quot;,
        &quot;country&quot;: &quot;US&quot;,
        &quot;state&quot;: &quot;CA&quot;,
        &quot;city&quot;: &quot;San Jose&quot;,
        &quot;sampleIds&quot;: [
            {
                &quot;name&quot;: &quot;klio&quot;,
                &quot;acceptedat&quot;: &quot;2016-02-02T00:00:00.000Z&quot;,
                &quot;enddat&quot;: null
            },
            {
                &quot;name&quot;: &quot;jkdsjks&quot;,
                &quot;acceptedat&quot;: &quot;2011-12-23T00:00:00.000Z&quot;,
                &quot;enddat&quot;: &quot;2016-01-26T00:00:00.000Z&quot;
            }
        ]
    },
    {
        &quot;id&quot;: &quot;kjkj&quot;,
        &quot;customsiteid&quot;: &quot;iuewi&quot;,
        &quot;country&quot;: &quot;UK&quot;,
        &quot;state&quot;: &quot;Na&quot;,
        &quot;city&quot;: &quot;Mnv&quot;,
        &quot;sampleIds&quot;: [
            {
                &quot;name&quot;: &quot;asa&quot;,
                &quot;acceptedat&quot;: &quot;2017-02-02T00:00:00.000Z&quot;,
                &quot;enddat&quot;: &quot;2019-01-26T00:00:00.000Zx&quot;
            },
            {
                &quot;name&quot;: &quot;wer&quot;,
                &quot;acceptedat&quot;: &quot;2021-12-23T00:00:00.000Z&quot;,
                &quot;enddat&quot;: null
            }
        ]
    }
    ]
</code></pre>
<p>Please let me know the approach to achieve this scenario.</p>
","<azure-sql-database><azure-data-factory>","2023-02-20 16:13:43","126","0","1","75516941","<p>You can achieve this using 2 copy activities instead of one. The first one to stage the entire data in your SQL database and the second one to query only the records where <code>enddat</code> is null.</p>
<ul>
<li>In the first copy activity, I am writing to a demo table in my Azure SQL Database (sink). The following is the mapping that I have used.</li>
</ul>
<p><img src=""https://i.imgur.com/znnd9ql.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>The data will be successfully written and the result will be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/ATg7u7P.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>In the second copy data activity, use the above table (demo) as source and then use the following query to get required source data.</li>
</ul>
<pre><code>select * from demo where enddt is null
</code></pre>
<p><a href=""https://i.stack.imgur.com/jRd0Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jRd0Z.png"" alt=""enter image description here"" /></a></p>
<br>
<ul>
<li>Now, select the sink table from your Azure SQL database and use the mapping as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/0i1eTJ1.png"" alt=""enter image description here"" /></p>
<br>
<ul>
<li>This would give the desired result as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/f3iPVfE.png"" alt=""enter image description here"" /></p>
"
"75510748","Azure Function App endpoint stuck on 'running' but has already finished running","<p>I have a Durable Function App (Elastic Premium) that gets triggered via Azure Data Factory. In ADF I have an activity that checks the runtime status of the activity that was triggered. However, I have an activity whose endpoint shows 'Running':
<a href=""https://i.stack.imgur.com/u9Xlf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u9Xlf.png"" alt=""enter image description here"" /></a></p>
<p>However, if I check the status of this run on de Azure Function App portal, I can see that it has already finished successfully
<a href=""https://i.stack.imgur.com/grThj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/grThj.png"" alt=""enter image description here"" /></a></p>
<p>Any idea on what this could be?</p>
<p>For more context, all of the activities that were triggered after this one are stuck either on Running or in Pending.
<a href=""https://i.stack.imgur.com/STBag.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/STBag.png"" alt=""enter image description here"" /></a></p>
","<azure-functions><azure-data-factory><azure-functions-runtime>","2023-02-20 14:48:14","62","0","1","75577075","<p>I took the Azure durable Function API in an Azure Data Factory Instance and mapped the Function API Settings accordingly:</p>
<p><img src=""https://i.imgur.com/iRzKq59.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/2AMXKZB.png"" alt=""enter image description here"" /></p>
<p>Both the Function Status in the data studio and runtime status of the durable function API is success and completed.</p>
<p>If your activity function code is taking more time to process the request by that instance then the runtimestatus will be in Running State. Check the function timeout setting or request-response time of that function API.</p>
<p>Refer to this <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management?tabs=csharp"" rel=""nofollow noreferrer"">MS Doc</a> for more information on <strong><code>runtimeStatus</code></strong> of the Azure Durable Function APIs.</p>
"
"75508452","Challenge in data from REST API using Azure Data Factory Synapse SQL as destination","<p>I have a pipeline with a Copy activity. The pipeline is working well with Azure SQL DB as Destination but when inserting to Synapse SQL DB , the pipeline fails with error code as below. I am auto creating the table.</p>
<p>Source: REST API</p>
<p>ErrorCode=UserErrorFailToReadFromRestResource,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=An error occurred while sending the request.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.Http.HttpRequestException,Message=An error occurred while sending the request.,Source=mscorlib,''Type=System.Net.WebException,Message=The remote server returned an error:</p>
","<azure><azure-data-factory><azure-synapse>","2023-02-20 11:05:31","157","0","1","75520723","<p>As per <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview"" rel=""nofollow noreferrer"">this</a> if you are copying to serverless pool it will not support. If not, I followed below procedure to copy data from rest Api to synapse dedicated SQL pool using below procedure:</p>
<p>I created a pipeline and run web activity to retrieve the details of Api as below:</p>
<p><img src=""https://i.imgur.com/vUr7eyt.png"" alt=""enter image description here"" /></p>
<p>it executed successfully without any error, I created linked service of Rest Api for source</p>
<p><img src=""https://i.imgur.com/7vcnAe2.png"" alt=""enter image description here"" /></p>
<p>Synapse Analytics linked service for sink:</p>
<p><img src=""https://i.imgur.com/tQGwQFF.png"" alt=""enter image description here"" /></p>
<p>I drag for each activity into the pipeline on success of web activity connecting to for each I entered range function for items expression as <code>@range(1,activity('restAPI').output.total_pages)</code>added copy activity to the for each activity, I created Rest Api dataset by using created linked service for source of copy activity and set get activity as request method created parameter rurl and added value as <code>?page=@{item()}</code> and entered relative rurl as <code>@dataset().rurl</code></p>
<p>Source:</p>
<p><img src=""https://i.imgur.com/FWNLRVT.png"" alt=""enter image description here"" /></p>
<p>Created Synapse Analytics dataset by using created linked service for sink created table parameter and added <code>Apipage@{item()}</code> as dynamic content and kept the parameter as table name and enabled auto create table</p>
<p><img src=""https://i.imgur.com/LTkMacz.png"" alt=""enter image description here"" /></p>
<p>I mapped according to my requirements:</p>
<p><img src=""https://i.imgur.com/g9TOqIr.png"" alt=""enter image description here"" /></p>
<p>I debug the pipeline and it executed successfully.</p>
<p><img src=""https://i.imgur.com/oQOaqJ1.png"" alt=""enter image description here"" /></p>
<p>The data is copied successfully</p>
<p><img src=""https://i.imgur.com/SK2UR5p.png"" alt=""enter image description here"" /></p>
<p>I am having two pages that's why created 2 tables, its working fine for me check once again.</p>
"
"75507707","Is it possible to read and load the excel(xlsx) file in Azure Blob container using Snowflake SnowPark?","<p>I've multiple XLSX files in the azure blob container which I should load those
xlsx files snowflake table.</p>
<p><strong>Note:</strong> I don't want to perform the conversion operation of xlsx to csv which actually
consumes more time. Also I did used the ADF Copy Activity for loading but it is taking more time to load the data.</p>
<p>so is there any way I can use the snowflake snowpark to load the xlsx to the snowflake table?</p>
<p>Additionally, I welcome any other feasible alternatives for this issue through which I can load the xlsx data quickly into the snowflake</p>
","<azure><snowflake-cloud-data-platform><azure-data-factory><snowflake-schema>","2023-02-20 09:53:47","173","0","1","75615598","<blockquote>
<p>Additionally, I welcome any other feasible alternatives for this issue through which I can load the xlsx data quickly into the snowflake</p>
</blockquote>
<p>You can try databricks with pandas dataframe here. First Mount the Blob Storage to databricks and read the excel file using <code>openpyxl</code>. Write the pandas dataframe  to snowflake as suggested by <strong>@Sergiu</strong>.</p>
<pre><code>import pandas as pd

storage_account_name = &quot;storage account name&quot;
container_name = &quot;container name&quot;
mount_point = &quot;/mnt/mount point&quot;
dbutils.fs.mount(
  source=f&quot;wasbs://{container_name}@{storage_account_name}.blob.core.windows.net&quot;,
  mount_point=mount_point,
  extra_configs={
f&quot;fs.azure.account.key.{storage_account_name}.blob.core.windows.net&quot;: &quot;storage account access key&quot;
  }
)
</code></pre>
<p><img src=""https://i.imgur.com/FUsXzdG.png"" alt=""enter image description here"" />
install <code>openpyxl</code> if you don't have.
<code>!pip install openpyxl</code></p>
<p><strong>Reading excel file after mounting:</strong></p>
<pre><code>file_path = f&quot;/dbfs/{mount_point}/myexcel.xlsx&quot;
df = pd.read_excel(file_path,engine=&quot;openpyxl&quot;)
</code></pre>
<p><img src=""https://i.imgur.com/2SbDDec.png"" alt=""enter image description here"" /></p>
<p>After reading it as pandas dataframe, follow this <a href=""https://stephenallwright.com/write-to-snowflake-from-pandas-dataframe/"" rel=""nofollow noreferrer"">procedure</a> by <strong>@stephenallwright</strong> to write this dataframe to snowflake table.</p>
"
"75505390","Export and Import Synapse Triggers","<p>I'm setting up secondary region for my synapse workspace, is there a way I can export all the triggers from one workspace to another?</p>
","<azure-data-factory><azure-synapse>","2023-02-20 04:51:31","69","0","1","75509080","<p>You have 3 options, as far as I can see:</p>
<ol>
<li>set up Git and Devops integration between your 2 workspaces and then set up a release pipeline to copy everything from one workspace to the other. Here is a link to the <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/continuous-integration-delivery"" rel=""nofollow noreferrer"">documentation</a>. This is the best way if you have a lot to copy, and\or want to create a way to copy between environments on a regular basis.</li>
<li>Build a PowerShell script to get information about triggers from one workspace and then create them in the second workspace. Try the commands <a href=""https://learn.microsoft.com/en-us/powershell/module/az.synapse/get-azsynapsetrigger?view=azps-9.4.0"" rel=""nofollow noreferrer"">Get-AzSynapseTrigger</a> to copy from one workspace and <a href=""https://learn.microsoft.com/en-us/powershell/module/az.synapse/set-azsynapsetrigger?view=azps-9.4.0"" rel=""nofollow noreferrer"">Set-AzSynapseTrigger</a> to create on the new environment.</li>
<li>If you have few triggers, simply copying them is the simplest, thought programmatically disappointing solution.</li>
</ol>
"
"75503321","How to use secure string parameter in Add dymamic content","<p>I have one parameter created which is of type &quot;SecureString&quot; . I want to use that directly in any AddDynamiccontent box .</p>
<p>But it throws me the error like its ecpecting the type string but the passed is &quot;SecureString&quot;</p>
<p>How to pass /convert it?</p>
","<azure><azure-data-factory>","2023-02-19 20:43:50","64","-1","1","75505215","<p><a href=""https://i.stack.imgur.com/ayXXJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ayXXJ.png"" alt=""enter image description here"" /></a></p>
<p>Now to convert to string :</p>
<p><a href=""https://i.stack.imgur.com/t3ROa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t3ROa.png"" alt=""enter image description here"" /></a></p>
<p>Output :
<a href=""https://i.stack.imgur.com/4pwPV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4pwPV.png"" alt=""enter image description here"" /></a></p>
<p>So to get Specific value:
<a href=""https://i.stack.imgur.com/sRFMl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sRFMl.png"" alt=""enter image description here"" /></a></p>
<p>Output:
<a href=""https://i.stack.imgur.com/Q1JCZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q1JCZ.png"" alt=""enter image description here"" /></a></p>
"
"75501538","Azure Data Factory copy activity with json data","<p>I am using a copy activity with request method as POST to copy the JSON response from a rest api. The rest api requires a body which contains date and time details in the below format</p>
<p>[
{
&quot;start_date&quot;: &quot;2023-02-07&quot;
&quot;start_time&quot;: &quot;06:42:06&quot;
&quot;end_date&quot;: &quot;2023-02-07&quot;
&quot;end_time&quot;: &quot;08:47:06&quot;</p>
<p>}
]</p>
<p>When am using the date and time as hardcoded values like the one shown above, it works fine. When I try to make it dynamic i.e. startdate as currentdate - 1 and start time as current time - 24hrs and end date as currentdate and end time as current time its failing.
In simple words I want to make the body of the api as dynamic</p>
<p>I tried the end date as &quot;{@formatdatetime(utcnow(),'yyyy-MM-dd')}&quot;
and end time as end time as &quot;{@formatdatetime(utcnow(),'hh-MM-ss')}&quot;</p>
<p>and few other combinations of the above code but its failing. Can someone please help me with this issue.</p>
","<azure-data-factory><azure-synapse>","2023-02-19 16:14:49","95","0","3","75505286","<ul>
<li>Since I don't have access to a REST API like yours, I have taken <code>set variable</code> activities to demonstrate how to create the required body using dynamic content.</li>
<li>I have used <code>addDays()</code> function to remove 1 day from both current date (1 day) and current time (24 hours and hence 1 day).</li>
<li>Then I have formatted the result using formatDateTime in the following way:</li>
</ul>
<pre><code>[ { &quot;start_date&quot;: &quot;@{formatDateTime(addDays(utcNow(),-1),'yyyy-MM-dd')}&quot; &quot;start_time&quot;: &quot;@{formatDateTime(addDays(utcNow(),-1),'hh:mm:ss')}&quot; &quot;end_date&quot;: &quot;@{formatDateTime(utcNow(),'yyyy-MM-dd')}&quot; &quot;end_date&quot;: &quot;@{formatDateTime(utcNow(),'hh:mm:ss')}&quot;} ]
</code></pre>
<p><img src=""https://i.imgur.com/PB4UVqz.png"" alt=""enter image description here"" /></p>
<ul>
<li>The result would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/w068jMY.png"" alt=""enter image description here"" /></p>
"
"75501538","Azure Data Factory copy activity with json data","<p>I am using a copy activity with request method as POST to copy the JSON response from a rest api. The rest api requires a body which contains date and time details in the below format</p>
<p>[
{
&quot;start_date&quot;: &quot;2023-02-07&quot;
&quot;start_time&quot;: &quot;06:42:06&quot;
&quot;end_date&quot;: &quot;2023-02-07&quot;
&quot;end_time&quot;: &quot;08:47:06&quot;</p>
<p>}
]</p>
<p>When am using the date and time as hardcoded values like the one shown above, it works fine. When I try to make it dynamic i.e. startdate as currentdate - 1 and start time as current time - 24hrs and end date as currentdate and end time as current time its failing.
In simple words I want to make the body of the api as dynamic</p>
<p>I tried the end date as &quot;{@formatdatetime(utcnow(),'yyyy-MM-dd')}&quot;
and end time as end time as &quot;{@formatdatetime(utcnow(),'hh-MM-ss')}&quot;</p>
<p>and few other combinations of the above code but its failing. Can someone please help me with this issue.</p>
","<azure-data-factory><azure-synapse>","2023-02-19 16:14:49","95","0","3","75526343","<p>The JSON block which you shared is not a JSON itself , I am wondering as to how this is working when you hardcoded .</p>
<p>I see that you are using <code>@formatdatetime(utcnow(),'hh-MM-ss')</code> to me it looks like you should use <code>@formatdatetime(utcnow(),'hh-mm-ss'</code></p>
<p>** MM is for month</p>
<p>** mm is for minute</p>
"
"75501538","Azure Data Factory copy activity with json data","<p>I am using a copy activity with request method as POST to copy the JSON response from a rest api. The rest api requires a body which contains date and time details in the below format</p>
<p>[
{
&quot;start_date&quot;: &quot;2023-02-07&quot;
&quot;start_time&quot;: &quot;06:42:06&quot;
&quot;end_date&quot;: &quot;2023-02-07&quot;
&quot;end_time&quot;: &quot;08:47:06&quot;</p>
<p>}
]</p>
<p>When am using the date and time as hardcoded values like the one shown above, it works fine. When I try to make it dynamic i.e. startdate as currentdate - 1 and start time as current time - 24hrs and end date as currentdate and end time as current time its failing.
In simple words I want to make the body of the api as dynamic</p>
<p>I tried the end date as &quot;{@formatdatetime(utcnow(),'yyyy-MM-dd')}&quot;
and end time as end time as &quot;{@formatdatetime(utcnow(),'hh-MM-ss')}&quot;</p>
<p>and few other combinations of the above code but its failing. Can someone please help me with this issue.</p>
","<azure-data-factory><azure-synapse>","2023-02-19 16:14:49","95","0","3","75736290","<p>Used a set variable activity to create the dynamic date &amp; time separately and then used the output of the variable in the body of copy activity.</p>
"
"75501492","Convert to required date format with date datatype","<p>I have input col as 12/03/08 viz dd/MM/yy . So I beed it in the format of dd/MM/yyyy . SO I have use the below transformation as :
toString(toDate(col,'dd/MM/yy'),'dd/MM/yyyy')</p>
<p>This works fine.</p>
<p>But at the end i need to conver this col to date datatype with the same format as dd/MM/yyyy . When i do the CAST transformation like</p>
<p>In cast , i have converted it to date and given the format as dd/MM/yyyy .Its giving the date but in default format like dd-MM-yyyy . I need slashes instead of dashes.</p>
<p>I tried to do
toDate( col, 'dd/MM/yyyy') ,Still i get the default format dd-MM-yyyy.</p>
<p>How to fix this.</p>
","<azure><azure-data-factory>","2023-02-19 16:06:55","66","0","2","75503177","<p>When you CAST in SQL for conversion, the resulted format is determined by the default format of your database. If you want to ensure that the date is formatted in a specific way, you can use the DATE_FORMAT function in combination with the toDate function.</p>
<p>If you want to get the wanted  dd/MM/yy to the format dd/MM/yyyy, you can use the following transformation:</p>
<p>toString(toDate(col, 'dd/MM/yy'), 'dd/MM/yyyy')</p>
<p>This first converts the string to a type using toDate and then formats itt as a string with the wanted format using the toString function.</p>
<p>However, if you then want to convert the string column back to a date datatype with the format dd/MM/yyyy, you cant simply use the CAST function with the desired format, as the resulting format will still be determined by the default format of your database. Instead, you can use the DATE_FORMAT function to format the date as a string with the desired format, and then convert it back to a date datatype using the toDate function.</p>
<p>To convert the string column col back to a date datatype with the format dd/MM/yyyy, you can use the following transformation:</p>
<p>toDate(DATE_FORMAT(col, 'dd/MM/yyyy'), 'dd/MM/yyyy')</p>
<p>This first formats the date in your string column as a string with the desired format using the DATE_FORMAT function, and then converts it back to a date datatype with the same format using the toDate function.</p>
"
"75501492","Convert to required date format with date datatype","<p>I have input col as 12/03/08 viz dd/MM/yy . So I beed it in the format of dd/MM/yyyy . SO I have use the below transformation as :
toString(toDate(col,'dd/MM/yy'),'dd/MM/yyyy')</p>
<p>This works fine.</p>
<p>But at the end i need to conver this col to date datatype with the same format as dd/MM/yyyy . When i do the CAST transformation like</p>
<p>In cast , i have converted it to date and given the format as dd/MM/yyyy .Its giving the date but in default format like dd-MM-yyyy . I need slashes instead of dashes.</p>
<p>I tried to do
toDate( col, 'dd/MM/yyyy') ,Still i get the default format dd-MM-yyyy.</p>
<p>How to fix this.</p>
","<azure><azure-data-factory>","2023-02-19 16:06:55","66","0","2","75526448","<p>This should help .</p>
<pre><code>CREATE TABLE T11 
(
T1 DATETIME 
)
INSERT INTO T11 VALUES ('12/03/08')

SELECT CONVERT(VARCHAR,T1,101) AS A FROM T11
</code></pre>
<p><a href=""https://i.stack.imgur.com/vuDGU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vuDGU.png"" alt=""enter image description here"" /></a></p>
"
"75498810","Can I use Azure Data Factory for ETL Testing as well?","<p>If I have to do data migration and ETL testing for data in Azure SQL DB's then can I use Azure Data Factory?</p>
<p>If yes the please provide some links explaining how? or some tutorials or page where I can find some details?</p>
<p>Thanks in advance!
Sunil</p>
","<azure-data-factory>","2023-02-19 08:17:18","84","0","1","75509202","<p>Yes, you can do ETL testing by using data factory. I copied all tables from one database to another database by using below process:
I check all the tables by using below command:</p>
<pre><code>select TABLE_SCHEMA, TABLE_NAME FROM information_schema.

TABLES Where TABLE_TYPE =  'BASE TABLE'  and TABLE_SCHEMA =  'dbo'
</code></pre>
<p><img src=""https://i.imgur.com/kOQcNZA.png"" alt=""enter image description here"" /></p>
<p>I created pipeline and performed Lookup activity to retrieve the tables of database by entering below query.</p>
<p><img src=""https://i.imgur.com/PMWtdNg.png"" alt=""enter image description here"" /></p>
<p>Output:</p>
<p><img src=""https://i.imgur.com/Zg7ZwBn.png"" alt=""enter image description here"" /></p>
<p>I created linked service for source and sink on success of lookup activity I implemented foreach activity by enabling sequential and added copy activity to it. I created dynamic dataset by using linked service that created for source. to retrieve all the tables from database I added the data dynamically as below:</p>
<p><img src=""https://i.imgur.com/tVSv28k.png"" alt=""enter image description here"" /></p>
<p>Created schema and table parameters enter dynamic content for schema is <code>@dataset().schema</code>  and for table is <code>@dataset().table</code>  entered values for schema is <code>@item().TABLE_SCHEMA</code> for table is <code>@item().TABLE_NAME</code> .</p>
<p><img src=""https://i.imgur.com/oiaZj5Q.png"" alt=""enter image description here"" /></p>
<p>create dataset by using linked service that created for sink and created parameters and entered values same as source in the sink and enabled auto create table option.</p>
<p><img src=""https://i.imgur.com/NS4Q7oP.png"" alt=""enter image description here"" /></p>
<p>Executed the pipeline. It executed successfully.</p>
<p><img src=""https://i.imgur.com/i5Jpegu.png"" alt=""enter image description here"" /></p>
<p>All tables are copied to target database.</p>
<p><img src=""https://i.imgur.com/LcAbN8Z.png"" alt=""enter image description here"" /></p>
<p>In this way you can   copy all the tables from one database to another database.</p>
"
"75498296","Converting Multiple Fixed Width Files to CSV at a time in ADF","<p>I have three (.txt) files in a container. I need to convert all these three files at a time to CSV.</p>
<p><a href=""https://i.stack.imgur.com/cNLev.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cNLev.png"" alt=""enter image description here"" /></a></p>
<p>I have converted Fixed Width to CSV only one file at a time.
<a href=""https://i.stack.imgur.com/yt8JK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yt8JK.png"" alt=""enter image description here"" /></a>
But my requirement is to convert multiple files at a time which are available inside a container.
Could anyone please helps me with a solution.<strong>Requirement is, daily we get multiple fixed width text files and we need to convert them to CSV and load them in Tables</strong></p>
","<sql-server><azure-data-factory>","2023-02-19 06:01:48","66","0","1","75499013","<p>In a pipeline, use a metadata activity to get a list of files in the folder.
Pass that list to a foreach loop.
Inside the loop, run the dataflow you show above, and pass to it the parameters of file source and destination.</p>
"
"75495080","Is there any way to load the excel fiel from the blob storage to snowflake without using the ADF?","<p>Basically, I have  the xlsx file in the azure blob container, I tried to use the ADF but it is taking more time to load the data to the snowflake table. even for the small file it is taking 30 odd secs.</p>
<p>I tried a couple of other alternatives like;</p>
<ol>
<li>Copy into the command: it is not supporting the xlsx format</li>
<li>Snowpark: same issue as number 1</li>
</ol>
<p>It would be great for me to know if there are any alternatives for this issue.</p>
","<azure><snowflake-cloud-data-platform><azure-data-factory><snowflake-schema>","2023-02-18 17:24:17","122","0","1","75509312","<p>AS per this <a href=""https://docs.snowflake.com/en/sql-reference/sql/create-file-format"" rel=""nofollow noreferrer"">Snowflake Document</a>, It currently does not enable importing data straight from Excel files. You must change the source file's format like <code>CSV</code>.</p>
<p><strong>If you are not using ADF</strong> you need to first convert that excel files into a <strong><code>CSV</code></strong> file. since Azure Blob Storage is just object storage which treats all formats in the same way. Therefore, you must download your Excel blob to your local machine and convert it to CSV, then upload it back to Azure Blob Storage.</p>
<p>For that you can use:</p>
<ul>
<li>Azure functions that read excel file from blob storage convert it into CSV and load it in blob storage again.</li>
<li>Azure blob sdk python to convert the <code>Excel</code> to <code>CSV</code> using python pandas libraries and load it to blob storage again.</li>
<li>Python application that loads the Excel file into a Panda dataframe and then loads that dataframe to Snowflake.</li>
</ul>
<p>Also see, similar question - <a href=""https://stackoverflow.com/questions/70184641/how-to-load-excel-file-data-into-snowflake-table"">How to load Excel file data into Snowflake table</a></p>
"
"75489959","How to convert YYYYMM to YYYY-MM-01 using Azure data factory expression","<p>I need to convert the YYYYMM to YYYY-MM-01 using azure data factory pipeline expression.</p>
<p>I tried the below expression but it giving me error that date value should follow the ISO 8601 format.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@formatDateTime(concat('202301','01'),'YYYY-MM-DD')</code></pre>
</div>
</div>
</p>
<p>It should return in this format '2023-01-01'.</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2023-02-17 23:00:00","118","1","4","75490287","<p>I used multiple substring to get this as shown below, if any better answer please let me know.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@concat(substring(concat('202301','01'),0,4 ),'-',substring(concat('202301','01'),4,2),'-',substring(concat('202301','01'),6,2))</code></pre>
</div>
</div>
</p>
"
"75489959","How to convert YYYYMM to YYYY-MM-01 using Azure data factory expression","<p>I need to convert the YYYYMM to YYYY-MM-01 using azure data factory pipeline expression.</p>
<p>I tried the below expression but it giving me error that date value should follow the ISO 8601 format.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@formatDateTime(concat('202301','01'),'YYYY-MM-DD')</code></pre>
</div>
</div>
</p>
<p>It should return in this format '2023-01-01'.</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2023-02-17 23:00:00","118","1","4","75501112","<p>You can supply a format string including numbers to <code>formatDateTime</code>, for example:</p>
<pre><code>@formatDateTime(utcnow(), 'yyyy-MM-01')
</code></pre>
<p>NB You have the case of your argument wrong, it should be lower-case y for Year, upper-case M for month and lower-case d for Day.</p>
"
"75489959","How to convert YYYYMM to YYYY-MM-01 using Azure data factory expression","<p>I need to convert the YYYYMM to YYYY-MM-01 using azure data factory pipeline expression.</p>
<p>I tried the below expression but it giving me error that date value should follow the ISO 8601 format.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@formatDateTime(concat('202301','01'),'YYYY-MM-DD')</code></pre>
</div>
</div>
</p>
<p>It should return in this format '2023-01-01'.</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2023-02-17 23:00:00","118","1","4","75509538","<p>Instead of concatenating 202301 with 01 (three times) and then taking substring, you can use the below expression to achieve the same.</p>
<pre class=""lang-sql prettyprint-override""><code>@concat(substring('202301',0,4 ),'-',substring('202301',4,2),'-','01')
</code></pre>
<p><img src=""https://i.imgur.com/rfRYFSQ.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/Hi7vhJ4.png"" alt=""enter image description here"" /></p>
"
"75489959","How to convert YYYYMM to YYYY-MM-01 using Azure data factory expression","<p>I need to convert the YYYYMM to YYYY-MM-01 using azure data factory pipeline expression.</p>
<p>I tried the below expression but it giving me error that date value should follow the ISO 8601 format.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@formatDateTime(concat('202301','01'),'YYYY-MM-DD')</code></pre>
</div>
</div>
</p>
<p>It should return in this format '2023-01-01'.</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2023-02-17 23:00:00","118","1","4","75527515","<p>Just a simpler way , then what you have used.</p>
<pre><code>@concat(substring('202301',0,4),'-',substring('202301',4,2),'-01')
</code></pre>
<p><a href=""https://i.stack.imgur.com/r3eQy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r3eQy.png"" alt=""enter image description here"" /></a></p>
<p>The issue here is the</p>
<pre><code>@formatDateTime(string(concat('202301','01')),'YYYYMMDD' ) does not work 
</code></pre>
<p>is missing the &quot;-&quot; but</p>
<pre><code>@formatDateTime(string(concat('2023-01','-01')),'yyyyMMdd' )
</code></pre>
<p>Just works fine.</p>
<p>I am not sure as to what is the Source here , if it is query then you can get the yyyymm in the form of yyyy-mm and that should help .</p>
<p>Also I request to have a look on the startOfMonth function .</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#startOfMonth"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#startOfMonth</a></p>
"
"75489862","Arm template for Azure data factory with diagnostics settings in bicep","<p>I have the following bicep to create ADF resource:</p>
<pre><code>resource dataFactory 'Microsoft.DataFactory/factories@2018-06-01' = {
  name: name
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
    globalParameters: {
      environment: {
        type: 'String'
        value: environmentAbbreviation
      }
    }
  }
  location: location
}
</code></pre>
<p>I need to add a diagnostic setting to ADF resource as follows:</p>
<p><a href=""https://i.stack.imgur.com/GQCcJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GQCcJ.png"" alt=""enter image description here"" /></a></p>
<p>How do I update the bicep?</p>
","<azure-data-factory><azure-resource-manager><azure-bicep><azure-diagnostics>","2023-02-17 22:40:49","136","0","1","75494089","<p>I tried to create diagnostic settings in ADF using Bicep. Below is the code.</p>
<p><strong>Bicep code for creating data factory</strong></p>
<ul>
<li>This code is for creating data factory. It is same as the code in question.</li>
</ul>
<pre class=""lang-json prettyprint-override""><code>param settingName string='XXXXX'
param factoryName string='XXXXX'
resource datafactory 'Microsoft.DataFactory/factories@2018-06-01' = {
  name: factoryName
  location: resourceGroup().location
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
  }
}
</code></pre>
<p><strong>Bicep code for adding diagnostic setting</strong></p>
<ul>
<li>In order to add diagnostic setting to data factory, below code is added along with the code to create data factory.</li>
</ul>
<pre class=""lang-json prettyprint-override""><code>resource factoryName_microsoft_insights_settingName 'Microsoft.DataFactory/factories/providers/diagnosticSettings@2017-05-01-preview' = {
  name: '${factoryName}/microsoft.insights/${settingName}'
  location: resourceGroup().location
  properties: {
    workspaceId: 'XXXX'
    logAnalyticsDestinationType: 'Dedicated'
    logs: [
      {
        category: 'PipelineRuns'
        enabled: true
        retentionPolicy: {
          enabled: false
          days: 0
        }
      }
      {
        category: 'TriggerRuns'
        enabled: true
        retentionPolicy: {
          enabled: false
          days: 0
        }
      }
      {
        category: 'ActivityRuns'
        enabled: true
        retentionPolicy: {
          enabled: false
          days: 0
        }
      }
    ]
    metrics: [
      {
        category: 'AllMetrics'
        timeGrain: 'PT1M'
        enabled: true
        retentionPolicy: {
          enabled: false
          days: 0
        }
      }
    ]
  }
  dependsOn: [
    datafactory
  ]
}
</code></pre>
<ul>
<li><p>When the above both codes are combined and run, resources got deployed successfully.
<img src=""https://i.imgur.com/BljWq4U.png"" alt=""enter image description here"" /></p>
</li>
<li><p>The above code will enable the categories - Pipeline runs log,Trigger runs log, Pipeline activity runs log. Change the code as per the requirement.
<img src=""https://i.imgur.com/MwV3S8Z.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/templates/microsoft.insights/diagnosticsettings?pivots=deployment-language-bicep"" rel=""nofollow noreferrer"">Microsoft.Insights/diagnosticSettings - Bicep, ARM template &amp; Terraform AzAPI reference | Microsoft Learn</a></p>
"
"75489572","Diagnostic settings to log latest status of each Azure Data Factory Pipeline run","<p>I have an ADF Pipeline that runs once every day. I use Diagnostic settings to log pipeline runs and use the following kusto query:</p>
<pre><code>AzureDiagnostics 
| order by TimeGenerated desc
</code></pre>
<p>I see the following results:</p>
<p><a href=""https://i.stack.imgur.com/g5qKp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g5qKp.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way to display <strong>only</strong> the last status (<strong>Succeeded/Failed</strong>) for each pipeline run?</p>
","<azure-data-factory><azure-data-explorer><azure-log-analytics>","2023-02-17 21:57:05","60","1","1","75489700","<p>you could use the <a href=""https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/arg-max-aggfunction"" rel=""nofollow noreferrer""><code>arg_max()</code> aggregation function</a>.</p>
<p>for example:</p>
<pre><code>AzureDiagnostics 
| where TimeGenerated &gt; ago(1d)
| summarize arg_max(TimeGenerated, *) by runId
</code></pre>
"
"75483391","How to Split Country and Year part separately, Folder creation is not working properly in ADLS I can't find From Where is the Bug Raised","<p>I was Supposed to give folder structure for file is /yyyy/MM/dd But it is coming like yyyy/MM/dd and the pipeline Structure is like
<a href=""https://i.stack.imgur.com/cj8Jj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cj8Jj.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/VCBfz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VCBfz.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/HbLUf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HbLUf.png"" alt=""enter image description here"" /></a>
and the result I am getting like below</p>
<p><a href=""https://i.stack.imgur.com/BhAYx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BhAYx.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-functions><azure-data-factory><azure-databricks>","2023-02-17 10:57:23","71","0","1","75485174","<pre><code>@concat('originalData/SFMC/CampaignLevelData/',pipeline().parameters.jobName,'/',variables('YMD'),'/',variables('MM'),'/',variables('Day'),'/')
</code></pre>
<p>There is no error in the expression and I tried the same in my environment and Folder structure is <code>....AU/2023/02/17</code></p>
<p><img src=""https://i.imgur.com/jWOXve9.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/YnSOdcL.png"" alt=""enter image description here"" /></p>
<p>Error could be in value of variables('YMD'). Check the input data from your end.</p>
"
"75479291","Log the status of the ADF (Azure Data Factory) pipeline run","<p>I have an ADF Pipeline with a trigger that runs the pipeline once every day. I would like to find out the status of the pipeline run (latest pipeline run) and log it somewhere (maybe log analytics). How do I do that?</p>
<p><a href=""https://i.stack.imgur.com/OlpVk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OlpVk.png"" alt=""enter image description here"" /></a></p>
<p>UPDATE:</p>
<p>I followed the solution suggested below however I don't see any logs on running the query <em><strong>ADFPipelineRun</strong></em> in log analytics. What am I missing?</p>
<p><a href=""https://i.stack.imgur.com/2sVNH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2sVNH.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><kql><azure-log-analytics>","2023-02-17 00:44:55","205","0","1","75480252","<p>In case if you want to have a log pertaining to only the recent run, then you would have to write a custom logic within your pipeline (like a script activity that would write in a database the status of the pipeline at the end)</p>
<p>sample reference:
<a href=""https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory/</a></p>
<p>In case if you are comfortable querying a list of logs and filtering out the latest one based on some filter logics, then you can use log analytics or storage blob by enabling diagnostic settings</p>
<p><a href=""https://i.stack.imgur.com/F0Iss.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F0Iss.png"" alt=""enter image description here"" /></a></p>
"
"75478428","I cannot launch Apache Web UI after clicking the Monitor icon in ADF. 401 error","<p>I cannot launch Apache Web UI after clicking the Monitor icon in Azure Data Factory.
Within the &quot;Manage&quot; tab within ADF I have created an Airflow. Its status is &quot;Running&quot;.
When I click on the &quot;Import files&quot; icon I was able to import and select folders  okay.
But when I click on the Monitor icon next to it, I am directed to a new web page with &quot;Unauthorised 401&quot; error on it.
In the MSFT docs it states and shows I should be directed to the Apache Airflow Web UI page instead. Azure AD is integrated with that service, apparently.
In the screenshot you can see I have two Airflow instances and you can see the &quot;Import files&quot; and monitor icons. It is the monitor icon button, here, that is sending me to the error message.
<a href=""https://i.stack.imgur.com/QrrqY.png"" rel=""nofollow noreferrer"">the Monitor icon</a></p>
<p>I was expecting to be sent to the Apache Airflow Web UI page.</p>
<p><a href=""https://i.stack.imgur.com/B0cNK.png"" rel=""nofollow noreferrer"">Azure Managed Apache Airflow Web UI</a></p>
","<airflow><azure-data-factory>","2023-02-16 22:12:41","65","0","1","75484729","<p>I also tried in my environment, and it is working fine for me.</p>
<ul>
<li>Created Airflow in Azure data factory.
<img src=""https://i.imgur.com/py0p8sg.png"" alt=""enter image description here"" /></li>
<li>I am getting directed to the Apache Airflow Web UI page.
<img src=""https://i.imgur.com/y2RcwLL.png"" alt=""enter image description here"" /></li>
</ul>
<p>Try with <strong>enabling</strong> <code>Preview Experience</code> <strong>this option is to try new features. Some examples of what is currently in preview.</strong></p>
<p><img src=""https://i.imgur.com/H4A5jSO.png"" alt=""enter image description here"" /></p>
<p>As this feature is in public preview and issue still persist. You can raise support ticket to Microsoft from <a href=""https://azure.microsoft.com/en-in/support/create-ticket/"" rel=""nofollow noreferrer"">here</a> for more assistance on this.</p>
"
"75477454","Delete document from CosmosDB using Azure Data Factory","<p>My Azure Data Factory has private endpoint connection to CosmosDB and authenticates using System Assigned Managed Identity. The goal is to delete document from CosmosDB using <a href=""https://learn.microsoft.com/en-us/rest/api/cosmos-db/delete-a-document"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/cosmos-db/delete-a-document</a> called from web activity.</p>
<p>I created web activity in Data Factory and put the required headers following those documents
<a href=""https://learn.microsoft.com/en-us/rest/api/cosmos-db/common-cosmosdb-rest-request-headers"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/cosmos-db/common-cosmosdb-rest-request-headers</a>
<a href=""https://learn.microsoft.com/en-us/rest/api/cosmos-db/access-control-on-cosmosdb-resources?redirectedfrom=MSDN"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/cosmos-db/access-control-on-cosmosdb-resources?redirectedfrom=MSDN</a></p>
<p>DELETE web activity:
<img src=""https://i.stack.imgur.com/AZMN0.png"" alt=""DELETE web activity"" /></p>
<p>I am using Azure Cosmos DB RBAC so my authorization header looks like this:
type=aad&amp;ver=1.0&amp;sig=token-from-oauth</p>
<p>To get a token I was following this post
<a href=""https://medium.com/analytics-vidhya/azure-data-factory-retrieve-token-from-azure-ad-using-oauth-2-0-9a3ed3f55013"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/azure-data-factory-retrieve-token-from-azure-ad-using-oauth-2-0-9a3ed3f55013</a>
but I don't know where can I get the client_secret. I found my ADF in AAD under enterprise application so I guess client_id is application Id but there is no client secret to be found there.</p>
<p>get token web activity:
<img src=""https://i.stack.imgur.com/ndouE.png"" alt=""get token web activity"" /></p>
<p>First obvious question is where can I find this client_secret?
The second one is why is this token needed at all? Why can't it just use managed identity for authenticating the request?</p>
","<azure-active-directory><azure-cosmosdb><azure-data-factory><azure-rest-api>","2023-02-16 20:15:03","102","0","1","75481596","<blockquote>
<p><strong>Where can I find this client_secret?</strong></p>
</blockquote>
<p>Go to azure active directory -&gt; Inside mange go to app registration(if you not created a app registration create it ) -&gt; Go to registered app -&gt; Certificate &amp; Secretes.</p>
<p><img src=""https://i.imgur.com/xDaaxyB.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Why is this token needed at all? why can't it just use managed identity for authenticating the request?</strong></p>
</blockquote>
<p>Managed identities are a way to simplify the authentication process for applications running in Azure, but they do not apply to all scenarios when calling external APIs that require authentication. In these cases, you need to obtain an access token from Azure AD using a client secret.</p>
<p><strong>I reproduce same thing in my environment. Follow below approach.</strong></p>
<p><img src=""https://i.imgur.com/aU1MxMj.png"" alt=""enter image description here"" /></p>
<p><strong>URL</strong>:<code>https://login.microsoftonline.com/&lt;tenant_id&gt;/oauth2/v2.0/token</code></p>
<p><strong>Scope</strong> : <code>https://cosmos.azure.com/.default</code></p>
<p><strong>Body:</strong>  <code>grant_type=client_credentials&amp;client_id=&lt;client_id&gt;&amp;client_secret=&lt;client_secret&gt;&amp;scope=scope : https://cosmos.azure.com/.default</code></p>
<p>After execution of web1 activity you will get like this bearer token:</p>
<p><img src=""https://i.imgur.com/v3xf7N5.png"" alt=""enter image description here"" /></p>
<p>Inside Web2 activity provide valid URL as per below syntax:</p>
<pre><code>https://{databaseaccount}.documents.azure.com/dbs/{db-id}/colls/{coll-id}/docs/{doc-id}
</code></pre>
<p><strong>Add dynamic content at header part as shown in the image:</strong></p>
<p>Authorization:   <code>Bearer @{activity('Web1').output.access_token}</code></p>
<p><img src=""https://i.imgur.com/BM9c6ki.png"" alt=""enter image description here"" /></p>
"
"75476011","Azure ARM DataFactory InvalidTemplate the runAfter property not valid; scope must belong to same level","<p>Documenting here what I learned, in case it helps someone else. I had a ARM template implementing a DataFactory pipeline, which had a weird InvalidTemplate error. I am simplifying my template to a contrived bare bones template,</p>
<pre class=""lang-json prettyprint-override""><code>    resources: [
    {
        &quot;name&quot;: &quot;blah&quot;,
            &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;,
            &quot;apiVersion&quot;: &quot;2018-06-01&quot;,
            &quot;properties&quot;: {
                &quot;activities&quot;: [
                {
                    &quot;name&quot;: &quot;Foo&quot;,
                    &quot;type&quot;: &quot;SetVariable&quot;,
                    &quot;dependsOn&quot;: [],
                    &quot;userProperties&quot;: [],
                    &quot;typeProperties&quot;: {
                        &quot;variableName&quot;: &quot;hi&quot;,
                        &quot;value&quot;: {
                            &quot;value&quot;: &quot;int(1)&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                },
                {
                    &quot;name&quot;: &quot;CoolIf&quot;,
                    &quot;type&quot;: &quot;IfCondition&quot;,
                    &quot;typeProperties&quot;: {
                        &quot;expression&quot;: {
                            &quot;value&quot;: &quot;@bool(equals(variables('hi'), 1))&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        },
                        &quot;ifTrueActivities&quot;: [
                            {
                                &quot;name&quot;: &quot;Blarg&quot;,
                                &quot;type&quot;: &quot;SetVariable&quot;,
                                &quot;dependsOn&quot;: [{&quot;activity&quot;: &quot;Foo&quot;}],
                                &quot;userProperties&quot;: [],
                                &quot;typeProperties&quot;: {
                                    &quot;variableName&quot;: &quot;okay&quot;,
                                    &quot;value&quot;: {
                                        &quot;value&quot;: &quot;@string(1 + int(variables('hi')))&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            },
                        ],
                        &quot;ifFalseActivities&quot;: []
                    }
                }
                ]
            }
    }
    ]

</code></pre>
<p>produced the error message</p>
<pre><code>ErrorCode=InvalidTemplate, ErrorMessage=The template validation
 failed: 'The 'runAfter' property of template action 'BlargScope'
 is not valid: the action 'FooScope' must belong to same level as
 action 'BlargScope'. Available actions on same level: ''
</code></pre>
<p>I could not find a good answer online.</p>
","<azure><azure-data-factory>","2023-02-16 17:38:30","69","0","1","75476012","<p>After a day of trial and error I realized you can put a <code>dependsOn</code> in the <code>IfCondition</code> parent, so below, just moves the <code>dependsOn</code> from the child <code>Blarg</code> to the parent <code>CoolIf</code>. And then it started working =) .</p>
<pre class=""lang-json prettyprint-override""><code>    resources: [
    {
        &quot;name&quot;: &quot;blah&quot;,
            &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;,
            &quot;apiVersion&quot;: &quot;2018-06-01&quot;,
            &quot;properties&quot;: {
                &quot;activities&quot;: [
                {
                    &quot;name&quot;: &quot;Foo&quot;,
                    &quot;type&quot;: &quot;SetVariable&quot;,
                    &quot;dependsOn&quot;: [],
                    &quot;userProperties&quot;: [],
                    &quot;typeProperties&quot;: {
                        &quot;variableName&quot;: &quot;hi&quot;,
                        &quot;value&quot;: {
                            &quot;value&quot;: &quot;int(1)&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                },
                {
                    &quot;name&quot;: &quot;CoolIf&quot;,
                    &quot;type&quot;: &quot;IfCondition&quot;,
                    &quot;dependsOn&quot;: [{&quot;activity&quot;: &quot;Foo&quot;}],
                    &quot;typeProperties&quot;: {
                        &quot;expression&quot;: {
                            &quot;value&quot;: &quot;@bool(equals(variables('hi'), 1))&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        },
                        &quot;ifTrueActivities&quot;: [
                            {
                                &quot;name&quot;: &quot;Blarg&quot;,
                                &quot;type&quot;: &quot;SetVariable&quot;,
                                &quot;dependsOn&quot;: [],
                                &quot;userProperties&quot;: [],
                                &quot;typeProperties&quot;: {
                                    &quot;variableName&quot;: &quot;okay&quot;,
                                    &quot;value&quot;: {
                                        &quot;value&quot;: &quot;@string(1 + int(variables('hi')))&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            },
                        ],
                        &quot;ifFalseActivities&quot;: []
                    }
                }
                ]
            }
    }
    ]


</code></pre>
<p>In retrospect I feel silly how simple this was haha.</p>
"
"75476009","How to set Datatype in Additional Column in ADF","<p>I need to set datatype for Additional Column with Dynamic Content in Sink in ADF</p>
<p><strong>By default its taking nvarchar(max) from Json obj but I need bigInt</strong></p>
<p>Below is a Json Obj which create table with Additional column</p>
<pre><code>{
    &quot;source&quot;: {
        &quot;type&quot;: &quot;SqlServerSource&quot;,
         &quot;additionalColumns&quot;: [
            {
                &quot;name&quot;: &quot;ApplicationId&quot;,
                &quot;value&quot;: 3604509277250831000
            }
        ],
        &quot;sqlReaderQuery&quot;: &quot;SELECT * from Table A&quot;,
        &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
        &quot;isolationLevel&quot;: &quot;ReadUncommitted&quot;,
        &quot;partitionOption&quot;: &quot;None&quot;
    },
    &quot;sink&quot;: {
        &quot;type&quot;: &quot;AzureSqlSink&quot;,
        &quot;writeBehavior&quot;: &quot;insert&quot;,
        &quot;sqlWriterUseTableLock&quot;: false,
        &quot;tableOption&quot;: &quot;autoCreate&quot;,
        &quot;disableMetricsCollection&quot;: false
    },
    &quot;enableStaging&quot;: false,
    &quot;translator&quot;: {
        &quot;type&quot;: &quot;TabularTranslator&quot;,
        &quot;typeConversion&quot;: true,
        &quot;typeConversionSettings&quot;: {
            &quot;allowDataTruncation&quot;: true,
            &quot;treatBooleanAsNumber&quot;: false
        }
    }
}
</code></pre>
<p><strong>ADF Configuration</strong></p>
<p><a href=""https://i.stack.imgur.com/dzofE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dzofE.png"" alt=""enter image description here"" /></a></p>
<p><strong>After create table Database -  column with datatype</strong></p>
<p><a href=""https://i.stack.imgur.com/l3euE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l3euE.png"" alt=""enter image description here"" /></a></p>
<p><strong>If I convert Dynamic content into Int</strong></p>
<pre><code>@int(pipeline().parameters.application.applicationId)
</code></pre>
<p>Then getting below warning</p>
<p><a href=""https://i.stack.imgur.com/hMkUc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hMkUc.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know how can I set Datatype in ADF</p>
","<sql><sql-server><azure><azure-data-factory>","2023-02-16 17:38:24","78","0","1","75480327","<p>I also tried the same and getting same result.</p>
<p><img src=""https://i.imgur.com/C94IKJE.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>By default its taking nvarchar(max) from Json obj but I need bigInt</strong></p>
</blockquote>
<p>To resolve this when you add additional column in your source data set and in <code>Mapping</code> click on<code>import schema</code> it will import the schema of the source and also give you additional column in schema you have to change the type of the column as <code>Int64</code> as shown in below image. in below image you can see after <code>name</code> there is additional means it is an additional column.</p>
<p><img src=""https://i.imgur.com/93I81Hz.png"" alt=""enter image description here"" /></p>
<p>After this run your pipeline, It will create additional column with data type <code>bigint</code> .</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;name&quot;: &quot;pipeline2&quot;,
    &quot;properties&quot;: {
    &quot;activities&quot;: [
    {
    &quot;name&quot;: &quot;Copy data1&quot;,
    &quot;type&quot;: &quot;Copy&quot;,
    &quot;dependsOn&quot;: [],
    &quot;policy&quot;: {
    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
    &quot;retry&quot;: 0,
    &quot;retryIntervalInSeconds&quot;: 30,
    &quot;secureOutput&quot;: false,
    &quot;secureInput&quot;: false
    },
    &quot;userProperties&quot;: [],
    &quot;typeProperties&quot;: {
    &quot;source&quot;: {
    &quot;type&quot;: &quot;JsonSource&quot;,
    &quot;additionalColumns&quot;: [
    {
    &quot;name&quot;: &quot;name&quot;,
    &quot;value&quot;: {
    &quot;value&quot;: &quot;@pipeline().parameters.demo.age&quot;,
    &quot;type&quot;: &quot;Expression&quot;
    }
    }
    ],
    &quot;storeSettings&quot;: {
    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
    &quot;recursive&quot;: true,
    &quot;enablePartitionDiscovery&quot;: false
    },
    &quot;formatSettings&quot;: {
    &quot;type&quot;: &quot;JsonReadSettings&quot;
    }
    },
    &quot;sink&quot;: {
    &quot;type&quot;: &quot;AzureSqlSink&quot;,
    &quot;writeBehavior&quot;: &quot;insert&quot;,
    &quot;sqlWriterUseTableLock&quot;: false,
    &quot;tableOption&quot;: &quot;autoCreate&quot;,
    &quot;disableMetricsCollection&quot;: false
    },
    &quot;enableStaging&quot;: false,
    &quot;translator&quot;: {
    &quot;type&quot;: &quot;TabularTranslator&quot;,
    &quot;mappings&quot;: [
    {
    &quot;source&quot;: {
    &quot;path&quot;: &quot;$['taskId']&quot;
    },
    &quot;sink&quot;: {
    &quot;name&quot;: &quot;taskId&quot;,
    &quot;type&quot;: &quot;String&quot;
    }
    },
    {
    &quot;source&quot;: {
    &quot;path&quot;: &quot;$['taskObtainedScore']&quot;
    },
    &quot;sink&quot;: {
    &quot;name&quot;: &quot;taskObtainedScore&quot;,
    &quot;type&quot;: &quot;String&quot;
    }
    },
    {
    &quot;source&quot;: {
    &quot;path&quot;: &quot;$['multiInstance']&quot;
    },
    &quot;sink&quot;: {
    &quot;name&quot;: &quot;multiInstance&quot;,
    &quot;type&quot;: &quot;String&quot;
    }
    },
    {
    &quot;source&quot;: {
    &quot;path&quot;: &quot;$['name']&quot;
    },
    &quot;sink&quot;: {
    &quot;name&quot;: &quot;name&quot;,
    &quot;type&quot;: &quot;Int64&quot;
    }
    }
    ],
    &quot;collectionReference&quot;: &quot;&quot;
    }
    },
    &quot;inputs&quot;: [
    {
    &quot;referenceName&quot;: &quot;Json1&quot;,
    &quot;type&quot;: &quot;DatasetReference&quot;
    }
    ],
    &quot;outputs&quot;: [
    {
    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
    &quot;type&quot;: &quot;DatasetReference&quot;
    }
    ]
    }
    ],
    &quot;parameters&quot;: {
    &quot;demo&quot;: {
    &quot;type&quot;: &quot;object&quot;,
    &quot;defaultValue&quot;: {
    &quot;name&quot;: &quot;John&quot;,
    &quot;age&quot;: 30,
    &quot;isStudent&quot;: true
    }
    }
    },
    &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>OUTPUT:</strong></p>
<p><img src=""https://i.imgur.com/yrcGTj3.png"" alt=""enter image description here"" /></p>
"
"75475084","Connection ADF to cross-tenant Azure SQL DB","<p>I would like to connect to an Azure SQL Database using my Azure Data Factory. The Azure SQL Database is created on another tenant and has SQL authentication disabled.</p>
<p>Is there another option to connect to this Azure SQL Database than using a Service Principal?</p>
<p>I tried to connect using User Managed Instance and System Managed Instance, but this does not seem to work as the Azure SQL Database is on another tenant. SQL Authentication is also not an option.</p>
","<azure><azure-data-factory><etl>","2023-02-16 16:18:13","174","0","1","75475403","<p>In case if the Azure SQL Database is in another tenant than ADF, then SQL Auth and Service principal auth are the only ways for the connectivity .
Managed identities are only leveraged for same tenant components.</p>
"
"75472262","templateLink does not exixst","<p>So I am trying to deploy <a href=""https://github.com/Azure/azure-quickstart-templates/tree/master/quickstarts/microsoft.compute/vms-with-selfhost-integration-runtime"" rel=""nofollow noreferrer"">this</a> script from Github. What it should do is deploy a VM to my existing resource group. Then it should install an Integration Runtime on it and link it to my existing Data Factory. The part that is not working is installing the IR in the first place.</p>
<p>In the main.bicep file, there is the following parameter:</p>
<p><code>@description('The base URI where artifacts required by this template are located.')</code></p>
<p><code>param _artifactsLocation string = deployment().properties.templateLink.uri</code></p>
<p>The error that I am getting is, that the &quot;templateLink&quot; doesn't exist. I am guessing it has something to do with me deploying the script locally, however I cannot change said parameter because I don't understand what it is doing in the first place.</p>
<p>So how do I solve this? Thanks in advance!</p>
","<azure><azure-bicep><azure-data-factory>","2023-02-16 12:27:53","35","0","1","75472604","<p>you need to deploy this bicep file from github using this url: <a href=""https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/quickstarts/microsoft.compute/vms-with-selfhost-integration-runtime/main.bicep"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/quickstarts/microsoft.compute/vms-with-selfhost-integration-runtime/main.bicep</a></p>
<p>instead of cloning and deploying locally, that way it will have <code>templateLink</code> property will exist on the deployment</p>
"
"75470967","How to grab column value from ADLS gen 2 csv file and use the column value in the body of the email,also send blob data as attachment to outlook mail","<p>Here is my Scenario,
There will be a drop of csv file into blob storage every day ,that will be processed by my dataflow in ADF and generate a csv in output folder.</p>
<p>Now Using logic apps, I need to send that csv file (less than 10 mb ) as an attachement to customer via Outlook connector.
Besides ,My body of the email must have dynamic value coming from that blob csv .
For example 'AppWorks' is the column value in column 'Works/not'. Sometimes it may be &quot;AppNotWorks&quot;.So How to handle this scenario in Azure logic apps</p>
","<azure><azure-logic-apps><azure-data-factory><azure-logic-app-standard>","2023-02-16 10:31:53","80","0","1","75473351","<p>You can use the combination of both data factory and logic apps to do this. Use <code>look up</code> activity to get the first row of the file (Since the entire column value will be same, we can get the required value from one row).</p>
<p><img src=""https://i.imgur.com/Cqp5JsW.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now use web activity to trigger the logic app. Pass the logic app's HTTP request URL to web activity. In the body, pass the following dynamic content:</li>
</ul>
<pre><code>@activity('Lookup1').output.firstRow
</code></pre>
<p><img src=""https://i.imgur.com/tLnRuSQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>When you debug the pipeline, the logic app will be successfully triggered. I have given the Request Body JSON schema to get values individually. For the sample I have taken, it would look as shown below:</li>
</ul>
<pre><code>{
    &quot;properties&quot;: {
        &quot;customer&quot;: {
            &quot;type&quot;: &quot;string&quot;
        },
        &quot;id&quot;: {
            &quot;type&quot;: &quot;string&quot;
        }
    },
    &quot;type&quot;: &quot;object&quot;
}
</code></pre>
<ul>
<li>Create a connection to storage account to link the required file.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/znoQ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/znoQ2.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Now, using the Outlook connector, send the Email.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/99iuh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/99iuh.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>The following is the entire Logic app JSON:</li>
</ul>
<pre><code>{
    &quot;definition&quot;: {
        &quot;$schema&quot;: &quot;https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#&quot;,
        &quot;actions&quot;: {
            &quot;Get_blob_content_(V2)&quot;: {
                &quot;inputs&quot;: {
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['azureblob']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;get&quot;,
                    &quot;path&quot;: &quot;/v2/datasets/@{encodeURIComponent(encodeURIComponent('AccountNameFromSettings'))}/files/@{encodeURIComponent(encodeURIComponent('JTJmZGF0YSUyZnNhbXBsZTEuY3N2'))}/content&quot;,
                    &quot;queries&quot;: {
                        &quot;inferContentType&quot;: true
                    }
                },
                &quot;metadata&quot;: {
                    &quot;JTJmZGF0YSUyZnNhbXBsZTEuY3N2&quot;: &quot;/data/sample1.csv&quot;
                },
                &quot;runAfter&quot;: {},
                &quot;type&quot;: &quot;ApiConnection&quot;
            },
            &quot;Send_an_email_(V2)&quot;: {
                &quot;inputs&quot;: {
                    &quot;body&quot;: {
                        &quot;Attachments&quot;: [
                            {
                                &quot;ContentBytes&quot;: &quot;@{base64(body('Get_blob_content_(V2)'))}&quot;,
                                &quot;Name&quot;: &quot;sample1.csv&quot;
                            }
                        ],
                        &quot;Body&quot;: &quot;&lt;p&gt;Hi @{triggerBody()?['customer']},&lt;br&gt;\n&lt;br&gt;\nRandom description&lt;/p&gt;&quot;,
                        &quot;Importance&quot;: &quot;Normal&quot;,
                        &quot;Subject&quot;: &quot;sample data&quot;,
                        &quot;To&quot;: &quot;&lt;to_email&gt;&quot;
                    },
                    &quot;host&quot;: {
                        &quot;connection&quot;: {
                            &quot;name&quot;: &quot;@parameters('$connections')['office365']['connectionId']&quot;
                        }
                    },
                    &quot;method&quot;: &quot;post&quot;,
                    &quot;path&quot;: &quot;/v2/Mail&quot;
                },
                &quot;runAfter&quot;: {
                    &quot;Get_blob_content_(V2)&quot;: [
                        &quot;Succeeded&quot;
                    ]
                },
                &quot;type&quot;: &quot;ApiConnection&quot;
            }
        },
        &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
        &quot;outputs&quot;: {},
        &quot;parameters&quot;: {
            &quot;$connections&quot;: {
                &quot;defaultValue&quot;: {},
                &quot;type&quot;: &quot;Object&quot;
            }
        },
        &quot;triggers&quot;: {
            &quot;manual&quot;: {
                &quot;inputs&quot;: {
                    &quot;schema&quot;: {
                        &quot;properties&quot;: {
                            &quot;customer&quot;: {
                                &quot;type&quot;: &quot;string&quot;
                            },
                            &quot;id&quot;: {
                                &quot;type&quot;: &quot;string&quot;
                            }
                        },
                        &quot;type&quot;: &quot;object&quot;
                    }
                },
                &quot;kind&quot;: &quot;Http&quot;,
                &quot;type&quot;: &quot;Request&quot;
            }
        }
    },
    &quot;parameters&quot;: {
        &quot;$connections&quot;: {
            &quot;value&quot;: {
                &quot;azureblob&quot;: {
                    &quot;connectionId&quot;: &quot;/subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.Web/connections/azureblob&quot;,
                    &quot;connectionName&quot;: &quot;azureblob&quot;,
                    &quot;id&quot;: &quot;/subscriptions/xxx/providers/Microsoft.Web/locations/westus2/managedApis/azureblob&quot;
                },
                &quot;office365&quot;: {
                    &quot;connectionId&quot;: &quot;/subscriptions/xxx/resourceGroups/v-sarikontha-Mindtree/providers/Microsoft.Web/connections/office365&quot;,
                    &quot;connectionName&quot;: &quot;office365&quot;,
                    &quot;id&quot;: &quot;/subscriptions/xxx/providers/Microsoft.Web/locations/westus2/managedApis/office365&quot;
                }
            }
        }
    }
}
</code></pre>
<ul>
<li>The following is the resulting Mail image for reference:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/7L3mW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7L3mW.png"" alt=""enter image description here"" /></a></p>
"
"75466329","Need to Track Changes in one column","<p>I have a table called rx_claims with columns epi, drugname, gpi. The table rx_claim is have historical saved data and I need to report if any changes in the gpi which is 16 digit number. The logic which I need is if the first 10 number of gpi changes from the historical data then report as new med and if 12 digits change then report as dose change with new and old column data.</p>
<p>for example : epi is 100 and drug name is ibrofine and gpi for this drug is 0123456789012300 ( historical data ) . Now we have a incoming record of 100 and drugname is still ibrofine but dosage change so it changes the gpi code to 0123456789000000 i.e the last 4 digits has changed so i want to show the resulted out as drugname : ibrofine and oldgpi is 0123456789012300 and new gpi is0123456789000000</p>
","<sql><sql-server><azure-data-factory>","2023-02-15 23:20:14","47","-3","1","75466526","<p>you could use a query to compare the historical data with the current data</p>
<pre><code>SELECT
  epi,
  drugname,
  gpi AS current_gpi,
  CASE
    WHEN SUBSTR(gpi, 1, 10) != SUBSTR(hist.gpi, 1, 10) THEN 'New Med'
    WHEN SUBSTR(gpi, 1, 10) = SUBSTR(hist.gpi, 1, 10) AND SUBSTR(gpi, 12, 5) != SUBSTR(hist.gpi, 12, 5) THEN CONCAT('Dose Change (Old GPI: ', hist.gpi, ')')
  END AS change_description
FROM
  rx_claims
LEFT JOIN
  (
    SELECT
      epi,
      gpi
    FROM
      rx_claims
    WHERE
      -- add condition to filter for historical data
      date &lt; '2023-02-15'
  ) hist
ON
  rx_claims.epi = hist.epi
</code></pre>
<p>This query compares the current GPI value with the historical GPI value for each epi number. If the first 10 digits of the GPI value have changed, it reports the change as a &quot;New Med&quot;. If only the 12th to 16th digits have changed, it reports the change as a &quot;Dose Change&quot; with the old GPI value included in the change description.</p>
"
"75465188","Azure ForEach activity failing with: The function 'length' expects its parameter to be an array or a string. The provided value is of type 'Integer'","<p>How do I convert my value to an integer?</p>
<p>Here's added context if helpful:
My pipeline should get the column count of a blob CSV and pass that count to a ForEach activity. A switch activity is embedded in ForEach, but the pipeline is failing at ForEach with this error: 'The function 'length' expects its parameter to be an array or a string. The provided value is of type 'Integer'.</p>
<p>Metadata output:</p>
<pre><code>{
    &quot;columnCount&quot;: 52,
    &quot;effectiveIntegrationRuntime&quot;: &quot;AutoResolveIntegrationRuntime (Central US)&quot;,
    &quot;executionDuration&quot;: 1,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p>ForEach input:</p>
<pre><code>{
    &quot;items&quot;: &quot;@activity('Get Metadata1').output.columnCount&quot;,
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Switch1&quot;,
            &quot;type&quot;: &quot;Switch&quot;,
            &quot;dependsOn&quot;: [],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;on&quot;: &quot;@item()&quot;,
                &quot;cases&quot;: [
                    {
                        &quot;value&quot;: &quot;44&quot;,
                        &quot;activities&quot;: [
                            {
                                &quot;name&quot;: &quot;Copy data1_copy1&quot;,
                                &quot;type&quot;: &quot;Copy&quot;,
                                &quot;dependsOn&quot;: [],
                                &quot;policy&quot;: {
                                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                    &quot;retry&quot;: 0,
                                    &quot;retryIntervalInSeconds&quot;: 30,
                                    &quot;secureOutput&quot;: false,
                                    &quot;secureInput&quot;: false
                                },
                                &quot;userProperties&quot;: [],
                                &quot;typeProperties&quot;: {
                                    &quot;source&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                        &quot;storeSettings&quot;: {
                                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                                            &quot;recursive&quot;: false,
                                            &quot;enablePartitionDiscovery&quot;: false
                                        },
                                        &quot;formatSettings&quot;: {
                                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                        }
                                    },
                                    &quot;sink&quot;: {
                                        &quot;type&quot;: &quot;AzureSqlSink&quot;,
                                        &quot;writeBehavior&quot;: &quot;insert&quot;,
                                        &quot;sqlWriterUseTableLock&quot;: false
                                    },
                                    &quot;enableStaging&quot;: false,
                                    &quot;translator&quot;: {
                                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                                        &quot;typeConversion&quot;: true,
                                        &quot;typeConversionSettings&quot;: {
                                            &quot;allowDataTruncation&quot;: true,
                                            &quot;treatBooleanAsNumber&quot;: false
                                        }
                                    }
                                },
                                &quot;inputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_CSV&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {
                                            &quot;FileName&quot;: &quot;@pipeline().parameters.SourceFile&quot;
                                        }
                                    }
                                ],
                                &quot;outputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_10_15_SQL&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {}
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        &quot;value&quot;: &quot;52&quot;,
                        &quot;activities&quot;: [
                            {
                                &quot;name&quot;: &quot;Copy data2_copy1&quot;,
                                &quot;type&quot;: &quot;Copy&quot;,
                                &quot;dependsOn&quot;: [],
                                &quot;policy&quot;: {
                                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                    &quot;retry&quot;: 0,
                                    &quot;retryIntervalInSeconds&quot;: 30,
                                    &quot;secureOutput&quot;: false,
                                    &quot;secureInput&quot;: false
                                },
                                &quot;userProperties&quot;: [],
                                &quot;typeProperties&quot;: {
                                    &quot;source&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                        &quot;storeSettings&quot;: {
                                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                                            &quot;recursive&quot;: false,
                                            &quot;enablePartitionDiscovery&quot;: false
                                        },
                                        &quot;formatSettings&quot;: {
                                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                        }
                                    },
                                    &quot;sink&quot;: {
                                        &quot;type&quot;: &quot;AzureSqlSink&quot;,
                                        &quot;writeBehavior&quot;: &quot;insert&quot;,
                                        &quot;sqlWriterUseTableLock&quot;: false
                                    },
                                    &quot;enableStaging&quot;: false,
                                    &quot;translator&quot;: {
                                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                                        &quot;typeConversion&quot;: true,
                                        &quot;typeConversionSettings&quot;: {
                                            &quot;allowDataTruncation&quot;: true,
                                            &quot;treatBooleanAsNumber&quot;: false
                                        }
                                    }
                                },
                                &quot;inputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_CSV&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {
                                            &quot;FileName&quot;: &quot;@pipeline().parameters.SourceFile&quot;
                                        }
                                    }
                                ],
                                &quot;outputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_15_20_SQL&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {}
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        &quot;value&quot;: &quot;60&quot;,
                        &quot;activities&quot;: [
                            {
                                &quot;name&quot;: &quot;Copy data3_copy1&quot;,
                                &quot;type&quot;: &quot;Copy&quot;,
                                &quot;dependsOn&quot;: [],
                                &quot;policy&quot;: {
                                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                    &quot;retry&quot;: 0,
                                    &quot;retryIntervalInSeconds&quot;: 30,
                                    &quot;secureOutput&quot;: false,
                                    &quot;secureInput&quot;: false
                                },
                                &quot;userProperties&quot;: [],
                                &quot;typeProperties&quot;: {
                                    &quot;source&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                        &quot;storeSettings&quot;: {
                                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                                            &quot;recursive&quot;: false,
                                            &quot;enablePartitionDiscovery&quot;: false
                                        },
                                        &quot;formatSettings&quot;: {
                                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                        }
                                    },
                                    &quot;sink&quot;: {
                                        &quot;type&quot;: &quot;AzureSqlSink&quot;,
                                        &quot;writeBehavior&quot;: &quot;insert&quot;,
                                        &quot;sqlWriterUseTableLock&quot;: false
                                    },
                                    &quot;enableStaging&quot;: false,
                                    &quot;translator&quot;: {
                                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                                        &quot;typeConversion&quot;: true,
                                        &quot;typeConversionSettings&quot;: {
                                            &quot;allowDataTruncation&quot;: true,
                                            &quot;treatBooleanAsNumber&quot;: false
                                        }
                                    }
                                },
                                &quot;inputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_CSV&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {
                                            &quot;FileName&quot;: &quot;@pipeline().parameters.SourceFile&quot;
                                        }
                                    }
                                ],
                                &quot;outputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_25_30_SQL&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {}
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        &quot;value&quot;: &quot;68&quot;,
                        &quot;activities&quot;: [
                            {
                                &quot;name&quot;: &quot;Copy data4_copy1&quot;,
                                &quot;type&quot;: &quot;Copy&quot;,
                                &quot;dependsOn&quot;: [],
                                &quot;policy&quot;: {
                                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                    &quot;retry&quot;: 0,
                                    &quot;retryIntervalInSeconds&quot;: 30,
                                    &quot;secureOutput&quot;: false,
                                    &quot;secureInput&quot;: false
                                },
                                &quot;userProperties&quot;: [],
                                &quot;typeProperties&quot;: {
                                    &quot;source&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                        &quot;storeSettings&quot;: {
                                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                                            &quot;recursive&quot;: false,
                                            &quot;enablePartitionDiscovery&quot;: false
                                        },
                                        &quot;formatSettings&quot;: {
                                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                        }
                                    },
                                    &quot;sink&quot;: {
                                        &quot;type&quot;: &quot;AzureSqlSink&quot;,
                                        &quot;writeBehavior&quot;: &quot;insert&quot;,
                                        &quot;sqlWriterUseTableLock&quot;: false
                                    },
                                    &quot;enableStaging&quot;: false,
                                    &quot;translator&quot;: {
                                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                                        &quot;typeConversion&quot;: true,
                                        &quot;typeConversionSettings&quot;: {
                                            &quot;allowDataTruncation&quot;: true,
                                            &quot;treatBooleanAsNumber&quot;: false
                                        }
                                    }
                                },
                                &quot;inputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_CSV&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {
                                            &quot;FileName&quot;: &quot;@pipeline().parameters.SourceFile&quot;
                                        }
                                    }
                                ],
                                &quot;outputs&quot;: [
                                    {
                                        &quot;referenceName&quot;: &quot;ten_eighty_split_30_35_SQL&quot;,
                                        &quot;type&quot;: &quot;DatasetReference&quot;,
                                        &quot;parameters&quot;: {}
                                    }
                                ]
                            }
                        ]
                    }
                ]
            }
        }
    ]
}
</code></pre>
<p>ForEach output:
<code>{}</code></p>
<p>Not sure how to satisfy this error. Thanks!</p>
","<azure><azure-data-factory>","2023-02-15 20:34:09","324","0","1","75467843","<blockquote>
<p>Failure type: User configuration issue
Details: The function 'length' expects its parameter to be an array or a string. The provided value is of type 'Integer'.</p>
</blockquote>
<p>Since you use an integer value (<strong>columnCount</strong>) as an input to for-each activity, you are getting this error. If you have array of values and you want to iterate the activity based on each value of array, you can use for-each activity. In this case, you can use the <strong>switch case activity</strong> directly after <strong>get metadata activity</strong>.  In Switch activity, expression is given within braces <code>{...}</code> .</p>
<p><strong>Expression:</strong>
<code>@{activity('Get Metadata1').output.columnCount}</code></p>
<p>I tried this in my environment and got the same error when I give the expression <strong>without braces {..}</strong>. When {} are added, it worked. Below are the steps.</p>
<ul>
<li><strong>Get MetaData activity</strong> is taken and <strong>column count</strong> is taken as an argument.</li>
</ul>
<p><strong>Output of MetaData activity</strong>:</p>
<pre><code>{
    &quot;columnCount&quot;: 2,
    &quot;effectiveIntegrationRuntime&quot;: &quot;AutoResolveIntegrationRuntime (West US)&quot;,
    &quot;executionDuration&quot;: 1,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 0
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;AzureIR&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<ul>
<li>Then Switch activity is taken and expression and case are given.
Expression:  <code>@{activity('Get Metadata1').output.columnCount}</code></li>
</ul>
<p><img src=""https://i.imgur.com/ckLP1II.png"" alt=""enter image description here"" /></p>
<ul>
<li>When pipeline is debugged, it got executed successfully without error.</li>
</ul>
"
"75464842","ADF Web Activity to get new Refresh/Access tokens Quickbooks Online","<p>I am having troubles running a web activity to fetch the new refresh/access tokens from Quickbooks online.</p>
<p>From Postman I keep receiving an &quot;invalid_grant&quot; error, likely since the refresh token changes every 24 hrs (seems excessive to me). From ADF I am getting a &quot;bad_request&quot; error. Does anyone have an example of the Web Activity they used in ADF to capture this info?</p>
<p>If someone could provide an example of the URL, Body and Headers they are using that would be very helpful. Not sure if I am doing something wrong or if this needs to be opened up with the Intuit team. Its most likely because the RefreshToken we have stored in Key Vault is no longer valid, but I want to make sure my Web Activity is formatted correctly first.</p>
<p>I have tried hardcoding the clientId and client secrets using basic authentication to the following url <a href=""https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer"" rel=""nofollow noreferrer"">https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer</a> in order to obtain the new tokens</p>
","<azure><azure-pipelines><azure-data-factory><azure-synapse><quickbooks-online>","2023-02-15 19:55:18","73","0","1","75467954","<p>I reproduce the same in my environment using web Activity and generated a bearer Token in Azure data factory.</p>
<p>Use Your  <strong>User id</strong>  and  <strong>Password</strong>  inside Body</p>
<p><img src=""https://i.imgur.com/Zkp70xL.png"" alt=""enter image description here"" /></p>
<p>output:</p>
<p><img src=""https://i.imgur.com/ADLvdOo.png"" alt=""enter image description here"" /></p>
<p>Alternative approach refer this <a href=""https://stackoverflow.com/questions/75230149/how-to-generate-bearer-token-via-azure-data-factory/75232048#75232048"">SO</a> thread.</p>
"
"75463529","How to copy blob data from one storage acc in one subscription to other storage acc in other subscription using ADF","<p>I have data in one storage acc in dev subscription. I need to move the data to other storage acc in other  subscription.</p>
<p>Is there any way to do this by ADF by copy activity?</p>
","<azure><azure-storage><azure-data-factory>","2023-02-15 17:37:01","64","0","2","75463627","<p>You should be able to create one linked Service pointing to Storage Account in DEV Subscription and then another linked Service pointing to Storage Account in Other Subscription. Once the connections are established, you should be able to use a Copy Activity using source dataset (pointing to DEV Subscription Linked Service) and sink Dataset (pointing to Other Subscription Linked Service)</p>
"
"75463529","How to copy blob data from one storage acc in one subscription to other storage acc in other subscription using ADF","<p>I have data in one storage acc in dev subscription. I need to move the data to other storage acc in other  subscription.</p>
<p>Is there any way to do this by ADF by copy activity?</p>
","<azure><azure-storage><azure-data-factory>","2023-02-15 17:37:01","64","0","2","75467785","<p>assuming you have multiple paths that you want to copy from Dev to Prod:</p>
<ol>
<li><p>create a linked service to dev blob and prod blob</p>
</li>
<li><p>create dynamic datasets:
a) source dataset with a parameterized path (mapping to dev)
b) sink dataset with parameterized path(mapping to prod)</p>
</li>
<li><p>use a lookup activity to get the list of all paths/containers</p>
</li>
<li><p>pass it to a for each activity and within for each activity use Copy activity
<a href=""https://i.stack.imgur.com/HdosD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HdosD.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#recursive-and-copybehavior-examples"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#recursive-and-copybehavior-examples</a></p>
"
"75460383","Lookup activity output in array","<p>i have a lookup activity in ADF which retrieves data from table like below
{
&quot;count&quot;: 1,
&quot;value&quot;: [
{
&quot;Notebook_path&quot;: &quot;/Users/Name@work.com/Project/ingestion/Cust1/ADT&quot;
}
],
&quot;effectiveIntegrationRuntime&quot;: &quot;AutoResolveIntegrationRuntime (um)&quot;,
&quot;billingReference&quot;: {
&quot;activityType&quot;: &quot;PipelineActivity&quot;,
&quot;billableDuration&quot;: [
{
&quot;meterType&quot;: &quot;AzureIR&quot;,
&quot;duration&quot;: 0.016666666666666666,
&quot;unit&quot;: &quot;DIUHours&quot;
}
]
},
&quot;durationInQueue&quot;: {
&quot;integrationRuntimeQueue&quot;: 0
}
}</p>
<p>now i want to use the Notebook_path from above results and pass it to a Notebook activity to run that particular notebook as in the output, from lookup activity.</p>
<p>since its in array its creating problems . would appreciate some help here .</p>
<p>tried a lot of things but dint help.</p>
<p>Thanks in Advance</p>
","<azure-data-factory><lookup><azure-notebooks>","2023-02-15 13:19:51","136","0","1","75460845","<ul>
<li>I have a similar lookup output. The following is a reference image of the same.</li>
</ul>
<p><img src=""https://i.imgur.com/1dN1daf.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can use the following dynamic content to get the desired result.</li>
</ul>
<pre><code>@activity('Lookup1').output.value[0]['notebook_path']
</code></pre>
<p><img src=""https://i.imgur.com/QmB20iy.png"" alt=""enter image description here"" /></p>
<ul>
<li>My pipeline failed because I don't have a notebook, but you can see that the activity is trying to access the required path.</li>
</ul>
<p><img src=""https://i.imgur.com/N4xoFUg.png"" alt=""enter image description here"" /></p>
"
"75460004","Data Factory MySQL Connectors","<p>Anyone got an idea of the difference between the <strong>Azure Database for MySQL connector</strong> and the <strong>MySQL connector</strong>.   I need to create a Linked Service to extract data from a VNet Peered MySQL database.  All help gratefully received.</p>
<p>FYI, when I test the Linked Service connection, I'm getting timeout.  SSL is required.</p>
","<mysql><azure-data-factory><mysql-connector>","2023-02-15 12:45:57","34","0","1","75461373","<p>Azure MySQL connector is for MYSQL stored and managed by Microsoft as a service on Azure. The MYSQL connector is for MySQL on your on-prem environment or on a virtual machine in Azure.</p>
<p>If your MySQL server is in your own Vnet, and cannot be accessed from outside by data factory, you need to install a Self hosted integration runtime service inside your Vnet, to connect between MySQL and data factory.</p>
"
"75456974","ADF Data Flow json Flatten Unroll by","<p>I have a json result that should look like this:</p>
<pre><code>{
  &quot;TotalItemCount&quot;: 636,
  &quot;PageIndex&quot;: 0,
  &quot;PageSize&quot;: 20,
  &quot;Result&quot;: [
    {
      &quot;Id&quot;: &quot;c02dd3ca-7440-4742-9802-af2b00655fe5&quot;,
      ...
      &quot;Employment&quot;: [
        {
          &quot;EmploymentNumber&quot;: &quot;1234&quot;,
        ...
        }
      ]
    },
    {
      ...
    }
  ]
}
</code></pre>
<p>Swagger output:
<a href=""https://i.stack.imgur.com/UXAvf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UXAvf.png"" alt=""enter image description here"" /></a></p>
<p>However when it is setup with a REST Datatset source in ADF it seems like it adds a wrapping array like this:</p>
<pre><code>[
{
  &quot;TotalItemCount&quot;: 636,
  &quot;PageIndex&quot;: 0,
  &quot;PageSize&quot;: 20,
  &quot;Result&quot;: [
    {
      &quot;Id&quot;: &quot;c02dd3ca-7440-4742-9802-af2b00655fe5&quot;,
      ...
      &quot;Employment&quot;: [
        {
          &quot;EmploymentNumber&quot;: &quot;1234&quot;,
        ...
        }
      ]
    },
    {
      ...
    }
  ]
}
]
</code></pre>
<p>ADF Data flow source dataset output:
<a href=""https://i.stack.imgur.com/WNSmM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WNSmM.png"" alt=""enter image description here"" /></a></p>
<p>I want to flatten the items in the result in a data flow Flatten formatter but what do I put ion the Unroll by field when the anonymous array i added? Or do I need to perform something before the flatten to get all items in the Result array and the perform flatten on each of the items?
The data preview in the Source looks like this:
<a href=""https://i.stack.imgur.com/qNIDR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qNIDR.png"" alt=""enter image description here"" /></a></p>
<p>The result would be an array with Id and EmploymentNumber</p>
<p>Update: Setting the unroll by to body does not work
<a href=""https://i.stack.imgur.com/wwLzR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wwLzR.png"" alt=""unrollby"" /></a></p>
<p>Source projection:
<a href=""https://i.stack.imgur.com/PIdQK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PIdQK.png"" alt=""enter image description here"" /></a></p>
","<json><azure-data-factory>","2023-02-15 08:07:07","76","1","1","75458531","<ul>
<li>I have taken the data as shown below (The one you have given is not a valid).</li>
</ul>
<pre><code>{&quot;body&quot;:[
{
  &quot;TotalItemCount&quot;: 636,
  &quot;PageIndex&quot;: 0,
  &quot;PageSize&quot;: 20,
  &quot;Result&quot;: [
    {
      &quot;Id&quot;: &quot;1&quot;,
      &quot;Employment&quot;: [{
        &quot;EmploymentNumber&quot;: &quot;1234&quot;
      }]
    },
    {
        &quot;Id&quot;: &quot;2&quot;,
      &quot;Employment&quot;: [{
        &quot;EmploymentNumber&quot;: &quot;5678&quot;
      }]
    }
  ]
}
]
}
</code></pre>
<ul>
<li>You can unroll by <code>body.Result.Employment</code> using unroll root as <code>body.Result</code>. Then select both the required id and employment number columns.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/klSOV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/klSOV.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>This would give the result in the following way:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/0F0Hf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0F0Hf.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Unroll on <code>body</code> with unroll root as <code>body</code>, you can select the required columns as well:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/chOq3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/chOq3.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Using this, the result would be:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/wOgg6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wOgg6.png"" alt=""enter image description here"" /></a></p>
"
"75455543","JSON SQL column in azure data factory","<p>I have a JSON type SQL column in SQL table as below example. I want the below code to be converted into separate columns such as drugs as table name and other attribute as column name, how can I use adf or any other means please guide. The below code is a single column in a table called report where I need to convert this into separate columns .</p>
<pre><code>{
    &quot;drugs&quot;: {
        &quot;Codeine&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Codeine&quot;,
            &quot;icons&quot;: [
                93,
                100,
                103
            ],
            &quot;drug_id&quot;: 36,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;prodrug&quot;,
                &quot;text&quot;: &quot;is **inactive**, its metabolites are active.&quot;
            },
            &quot;targets&quot;: [
                &quot;OPRM1&quot;
            ],
            &quot;rxnorm_id&quot;: &quot;2670&quot;,
            &quot;priclasses&quot;: [
                &quot;Analgesic/Anesthesiology&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                9,
                17,
                29,
                30,
                159,
                171
            ],
            &quot;subclasses&quot;: [
                &quot;Analgesic agent&quot;,
                &quot;Antitussive agent&quot;,
                &quot;Opioid agonist&quot;,
                &quot;Phenanthrene &quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;OPRM1&quot;: &quot;78&quot;,
                &quot;metabolic&quot;: &quot;6&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2D6&quot;,
                &quot;OPRM1&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Normal to reduced morphine formation. Use label recommended age- or weight-specific dosing. If no response, may need to consider alternative analgesics such as morphine or a non-opioid.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 103
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Predicted codeine metabolism is reduced.&quot;,
                    &quot;icons_id&quot;: 58,
                    &quot;sort_key&quot;: 58,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible decrease in exposure to the active metabolite(s) of codeine.&quot;,
                    &quot;icons_id&quot;: 93,
                    &quot;sort_key&quot;: 56,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Decreased analgesic effects due to OPRM1 genotype.&quot;,
                    &quot;icons_id&quot;: 100,
                    &quot;sort_key&quot;: 52,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of codeine in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Dapsone&quot;: {
            &quot;bin&quot;: &quot;X&quot;,
            &quot;name&quot;: &quot;Dapsone&quot;,
            &quot;icons&quot;: [
                99
            ],
            &quot;drug_id&quot;: 514,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3108&quot;,
            &quot;priclasses&quot;: [
                &quot;Infectious disease&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Miscellaneous antibiotic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Hemolysis and Heinz body formation may be exaggerated in individuals with a glucose-6-phosphate dehydrogenase (G6PD) deficiency, or methemoglobin reductase deficiency, or hemoglobin M. This reaction is frequently dose-related. Dapsone should be given with caution to these patients or if the patient is exposed to other agents or conditions such as infection or diabetic ketosis capable of producing hemolysis. Drugs or chemicals which have produced significant hemolysis in G6PD or methemoglobin reductase deficient patients include dapsone, sulfanilamide, nitrite, aniline, phenylhydrazine, napthalene, niridazole, nitro-furantoin and 8-amino-antimalarials such as primaquine. Toxic hepatitis and cholestatic jaundice have been reported early in therapy. Hyperbilirubinemia may occur more often in G6PD deficient patients. When feasible, baseline and subsequent monitoring of liver function is recommended; if abnormal, dapsone should be discontinued until the source of the abnormality is established.&quot;,
                    &quot;source&quot;: &quot;FDA - Additional testing&quot;,
                    &quot;guidelines_id&quot;: 453
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;According to FDA labeling, additional laboratory testing may be indicated.&quot;,
                    &quot;icons_id&quot;: 99,
                    &quot;sort_key&quot;: 51,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Digoxin&quot;: {
            &quot;bin&quot;: &quot;B&quot;,
            &quot;name&quot;: &quot;Digoxin&quot;,
            &quot;icons&quot;: [],
            &quot;drug_id&quot;: 47,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;nometab&quot;,
                &quot;text&quot;: &quot;is not significantly metabolized, or not absorbed.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3407&quot;,
            &quot;priclasses&quot;: [
                &quot;Cardiovascular&quot;
            ],
            &quot;references&quot;: [
                1
            ],
            &quot;subclasses&quot;: [
                &quot;Antiarrhythmic agent&quot;,
                &quot;Cardiac glycoside&quot;,
                &quot;Miscellaneous antiarrhythmic agent&quot;
            ],
            &quot;metabolizers&quot;: [],
            &quot;phenotype_ids&quot;: {},
            &quot;relevant_genes&quot;: [],
            &quot;dosing_guidelines&quot;: [],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;All of digoxin's actions are mediated through its effects on Na-K ATPase. Up to 70% of the dose is excreted unchanged in the urine and only a small percentage (13%) of a dose of digoxin is metabolized in healthy volunteers. The metabolism of digoxin is not dependent upon the cytochrome P-450 system, and digoxin is not known to induce or inhibit the cytochrome P-450 system. Digoxin is a substrate for P-glycoprotein (P-gp). As an efflux protein on the apical membrane of enterocytes and of renal tubular cells, P-glycoprotein may limit the absorption and enhance the excretion of digoxin, respectively. Studies have suggested that variants of the ABCB1 gene (encoding P-gp) may influence digoxin serum levels; however, changes in digoxin exposure related to ABCB1 genotype are generally small (10-30%), account only for approximately 10% of the variability seen in digoxin pharmacokinetics, and are unlikely to be clinically significant.&quot;,
                    &quot;icons_id&quot;: 60,
                    &quot;sort_key&quot;: 1,
                    &quot;references_id&quot;: null
                }
            ]
        },
        &quot;Doxepin&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Doxepin&quot;,
            &quot;icons&quot;: [
                92,
                103
            ],
            &quot;drug_id&quot;: 452,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;dualactive&quot;,
                &quot;text&quot;: &quot;and its metabolites are **active**.&quot;
            },
            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;3638&quot;,
            &quot;priclasses&quot;: [
                &quot;Psychiatry&quot;
            ],
            &quot;references&quot;: [
                1,
                2,
                50
            ],
            &quot;subclasses&quot;: [
                &quot;Antidepressant&quot;,
                &quot;Tricyclic antidepressant&quot;
            ],
            &quot;metabolizers&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;phenotype_ids&quot;: {
                &quot;metabolic&quot;: &quot;5&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2C19&quot;,
                &quot;CYP2D6&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Certain drugs inhibit the activity of CYP2D6 and make normal metabolizers resemble poor metabolizers. An individual who is stable on a given dose of TCA may become abruptly toxic when given one of these inhibiting drugs as concomitant therapy. Concomitant use of tricyclic antidepressants with drugs that can inhibit cytochrome P450 2D6 may require lower doses than usually prescribed for either the tricyclic antidepressant or the other drug. Furthermore, whenever one of these other drugs is withdrawn from co-therapy, an increased dose of tricyclic antidepressant may be required. It is desirable to monitor TCA plasma levels whenever a TCA is going to be coadministered with another drug known to be an inhibitor of cytochrome P450 2D6.&quot;,
                    &quot;source&quot;: &quot;FDA&quot;,
                    &quot;guidelines_id&quot;: 349
                },
                {
                    &quot;text&quot;: &quot;A 25% reduction of recommended starting dose may need to be considered. Utilize therapeutic drug monitoring to guide dose adjustments.&quot;,
                    &quot;source&quot;: &quot;Genotype predicted&quot;,
                    &quot;guidelines_id&quot;: 66
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible increase in exposure to doxepin.&quot;,
                    &quot;icons_id&quot;: 92,
                    &quot;sort_key&quot;: 57,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of doxepin in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]

</code></pre>
","<sql><sql-server><azure><azure-data-factory>","2023-02-15 04:35:13","86","-1","1","75459567","<p>Since this json is already in a SQL column, you don't need ADF to break it down to parts. You can use JSON functions in SQL server to do that.</p>
<p>example of few first columns:</p>
<pre><code>    declare @json varchar(max) = '{
    &quot;drugs&quot;: {
        &quot;Codeine&quot;: {
            &quot;bin&quot;: &quot;Y&quot;,
            &quot;name&quot;: &quot;Codeine&quot;,
            &quot;icons&quot;: [
                93,
                103
            ],

            &quot;drug_id&quot;: 36,
            &quot;pathway&quot;: {
                &quot;code&quot;: &quot;prodrug&quot;,
                &quot;text&quot;: &quot;is **inactive**, its metabolites are active.&quot;
            },

            &quot;targets&quot;: [],
            &quot;rxnorm_id&quot;: &quot;2670&quot;,
            &quot;priclasses&quot;: [
                &quot;Analgesic/Anesthesiology&quot;
            ],
            &quot;references&quot;: [
                1,
                16,
                17,
                100
            ],

            &quot;subclasses&quot;: [
                &quot;Analgesic agent&quot;,
                &quot;Antitussive agent&quot;,
                &quot;Opioid agonist&quot;,
                &quot;Phenanthrene &quot;
            ],

            &quot;metabolizers&quot;: [
                &quot;CYP2D6&quot;
            ],

            &quot;phenotype_ids&quot;: {
                &quot;metabolic&quot;: &quot;5&quot;
            },
            &quot;relevant_genes&quot;: [
                &quot;CYP2D6&quot;
            ],
            &quot;dosing_guidelines&quot;: [
                {
                    &quot;text&quot;: &quot;Reduced morphine formation. Use label recommended age- or weight-specific dosing. If no response, consider alternative analgesics such as morphine or a non-opioid.&quot;,
                    &quot;source&quot;: &quot;CPIC&quot;,
                    &quot;guidelines_id&quot;: 1
                },
                {
                    &quot;text&quot;: &quot;Analgesia: select alternative drug (e.g., acetaminophen, NSAID, morphine-not tramadol or oxycodone) or be alert to symptoms of insufficient pain relief.&quot;,
                    &quot;source&quot;: &quot;DPWG&quot;,
                    &quot;guidelines_id&quot;: 362
                }
            ],
            &quot;drug_report_notes&quot;: [
                {
                    &quot;text&quot;: &quot;Predicted codeine metabolism is reduced.&quot;,
                    &quot;icons_id&quot;: 58,
                    &quot;sort_key&quot;: 58,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Genotype suggests a possible decrease in exposure to the active metabolite(s) of codeine.&quot;,
                    &quot;icons_id&quot;: 93,
                    &quot;sort_key&quot;: 56,
                    &quot;references_id&quot;: null
                },
                {
                    &quot;text&quot;: &quot;Professional guidelines exist for the use of codeine in patients with this genotype and/or phenotype.&quot;,
                    &quot;icons_id&quot;: 103,
                    &quot;sort_key&quot;: 50,
                    &quot;references_id&quot;: null
                }
            ]
        }
    }
}


select JSON_VALUE(JSON_QUERY(@json,'$.drugs.Codeine'),'$.bin') as bin,
       JSON_VALUE(JSON_QUERY(@json,'$.drugs.Codeine'),'$.name') as name,
       JSON_VALUE(JSON_QUERY(@json,'$.drugs.Codeine'),'$.drug_id') as drug_id,
       JSON_VALUE(JSON_QUERY(@json,'$.drugs.Codeine'),'$.icons[0]') as icon_1
'
</code></pre>
<p>You need to decide how to handle arrays, such as icons, where there are multiple values inside the same element.</p>
<p>References:</p>
<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/json-query-transact-sql?f1url=%3FappId%3DDev15IDEF1%26l%3DEN-US%26k%3Dk(JSON_QUERY_TSQL)%3Bk(sql13.swb.tsqlresults.f1)%3Bk(sql13.swb.tsqlquery.f1)%3Bk(MiscellaneousFilesProject)%3Bk(DevLang-TSQL)%26rd%3Dtrue&amp;view=sql-server-ver16"" rel=""nofollow noreferrer"">JSON_QUERY function</a></p>
<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/json-value-transact-sql?view=sql-server-ver16"" rel=""nofollow noreferrer"">JSON_VALUE function</a></p>
"
"75451025","How to Add ""2021-12-01T00:00:00Z"" This date to set variable in ADF","<p>I need to Maintain Folder Structure to store files in yyyy/MM/DD format and I am getting date like this &quot;2021-12-01T00:00:00Z&quot; I need to Extract year from the date and to store in one variable and need to extract Month from date and set to another variable and for Date as well so that I will Concat these variable result under Copy activity Sink section</p>
","<azure><azure-data-factory><azure-databricks><azure-data-lake>","2023-02-14 17:00:48","121","0","2","75456157","<p>Yes , you can Maintain Folder Structure by using  <strong>Split</strong> .</p>
<ul>
<li>First create parameter name and add value.</li>
<li>Create 3 set variable with year , month and date.</li>
</ul>
<p><img src=""https://i.imgur.com/IWcqfZQ.png"" alt=""enter image description here"" /></p>
<p>Inside <strong>Year,Month,Date</strong>. Add this dynamic content value :</p>
<p><strong>Year</strong>:<code>@split(pipeline().parameters.fileName,'-')[0]</code>
<img src=""https://i.imgur.com/XEh1GI9.png"" alt=""enter image description here"" /></p>
<p><strong>Month:</strong>  <code>@split(pipeline().parameters.fileName,'-')[1]</code></p>
<p><strong>Data:</strong></p>
<pre><code>@split(split(pipeline().parameters.fileName,'-')[2],'T')[0]
</code></pre>
<p>For more information refer this <strong>JSON Code representation.</strong></p>
<pre><code>{

&quot;name&quot;: &quot;pipeline1&quot;,

&quot;properties&quot;: {

&quot;activities&quot;: [

{

&quot;name&quot;: &quot;year&quot;,

&quot;type&quot;: &quot;SetVariable&quot;,

&quot;dependsOn&quot;: [],

&quot;userProperties&quot;: [],

&quot;typeProperties&quot;: {

&quot;variableName&quot;: &quot;year&quot;,

&quot;value&quot;: {

&quot;value&quot;: &quot;@split(pipeline().parameters.fileName,'-')[0]&quot;,

&quot;type&quot;: &quot;Expression&quot;

}

}

},

{

&quot;name&quot;: &quot;month&quot;,

&quot;type&quot;: &quot;SetVariable&quot;,

&quot;dependsOn&quot;: [

{

&quot;activity&quot;: &quot;year&quot;,

&quot;dependencyConditions&quot;: [

&quot;Succeeded&quot;

]

}

],

&quot;userProperties&quot;: [],

&quot;typeProperties&quot;: {

&quot;variableName&quot;: &quot;month&quot;,

&quot;value&quot;: {

&quot;value&quot;: &quot;@split(pipeline().parameters.fileName,'-')[1]&quot;,

&quot;type&quot;: &quot;Expression&quot;

}

}

},

{

&quot;name&quot;: &quot;date&quot;,

&quot;type&quot;: &quot;SetVariable&quot;,

&quot;dependsOn&quot;: [

{

&quot;activity&quot;: &quot;month&quot;,

&quot;dependencyConditions&quot;: [

&quot;Succeeded&quot;

]

}

],

&quot;userProperties&quot;: [],

&quot;typeProperties&quot;: {

&quot;variableName&quot;: &quot;date&quot;,

&quot;value&quot;: {

&quot;value&quot;: &quot;@split(split(pipeline().parameters.fileName,'-')[2],'T')[0]&quot;,

&quot;type&quot;: &quot;Expression&quot;

}

}

}

],

&quot;parameters&quot;: {

&quot;fileName&quot;: {

&quot;type&quot;: &quot;string&quot;,

&quot;defaultValue&quot;: &quot;2021-12-01T00:00:00Z.csv&quot;

}

},

&quot;variables&quot;: {

&quot;year&quot;: {

&quot;type&quot;: &quot;String&quot;

},

&quot;month&quot;: {

&quot;type&quot;: &quot;String&quot;

},

&quot;date&quot;: {

&quot;type&quot;: &quot;String&quot;

}

},

&quot;annotations&quot;: [],

&quot;lastPublishTime&quot;: &quot;2023-02-15T05:42:23Z&quot;

},

&quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;

}
</code></pre>
<p><strong>Output:</strong></p>
<blockquote>
<p><strong>Year:</strong></p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/M0hMs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M0hMs.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p><strong>Month</strong></p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/pp8BT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pp8BT.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p><strong>Date</strong></p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/SrD2U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SrD2U.png"" alt=""enter image description here"" /></a></p>
"
"75451025","How to Add ""2021-12-01T00:00:00Z"" This date to set variable in ADF","<p>I need to Maintain Folder Structure to store files in yyyy/MM/DD format and I am getting date like this &quot;2021-12-01T00:00:00Z&quot; I need to Extract year from the date and to store in one variable and need to extract Month from date and set to another variable and for Date as well so that I will Concat these variable result under Copy activity Sink section</p>
","<azure><azure-data-factory><azure-databricks><azure-data-lake>","2023-02-14 17:00:48","121","0","2","75456259","<p>Use a metadata activity to get the file list, and then a foreach activity to copy each file to the appropriate folder:</p>
<p><a href=""https://i.stack.imgur.com/JXBFN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JXBFN.png"" alt=""pipline"" /></a></p>
<p>Metdata activity setting:</p>
<p><a href=""https://i.stack.imgur.com/EWU9M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EWU9M.png"" alt=""metdata activity"" /></a></p>
<p>On foreach activity, in items loop through all the child items -</p>
<pre><code>@activity('Get files Metadata').output.childItems
</code></pre>
<p>Inside the foreach look, add a copy activity to copy each file:</p>
<p>Source:</p>
<p><a href=""https://i.stack.imgur.com/rlgjc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rlgjc.png"" alt=""source copy"" /></a></p>
<p>Source dataset (with a parameter on filename, to copy one file only:</p>
<p><a href=""https://i.stack.imgur.com/HHTtV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HHTtV.png"" alt=""source dataset"" /></a></p>
<p>Sync settings:</p>
<p><a href=""https://i.stack.imgur.com/pbyO5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pbyO5.png"" alt=""sync settings"" /></a></p>
<p>Expression to pass to folder parameter:</p>
<pre><code>@concat(
   formatDateTime(item().name ,'yyyy'),
   '/',
   formatDateTime(item().name ,'MM'),
   '/',
   formatDateTime(item().name ,'dd')
)
</code></pre>
<p>Sync dataset, with a parameter on folder name to create the hierarchy:</p>
<p><a href=""https://i.stack.imgur.com/X4nsy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X4nsy.png"" alt=""sync dataset"" /></a></p>
<p>The result:</p>
<p><a href=""https://i.stack.imgur.com/7fRgR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7fRgR.png"" alt=""results"" /></a></p>
"
"75447970","How to invoke Webhook as part of ADF pipeline in Azure","<p>I am building an ADF pipeline, requirement is to invoke the Webhook service as part of Azure. Can someone please help me how to implement this?</p>
<p>Thanks in advance.</p>
","<azure><azure-data-factory><webhooks>","2023-02-14 12:39:21","55","0","1","75448251","<p>There is an activity specifically for that: Web hook activity
<a href=""https://i.stack.imgur.com/BXuiM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BXuiM.png"" alt=""enter image description here"" /></a></p>
"
"75443320","Azure linked service using python","<p>I have created a Linked service in azure data factory using azure portal. I want to connect to this Linked service in notebook activity in synapse using python. Do we have any such api?</p>
<p>Please let me know.</p>
<p>Thanks</p>
","<azure-data-factory><linked-service>","2023-02-14 03:59:18","232","0","1","75469149","<p>As per your requirements you can directly copy data from synapse notebook to your Azure blob storage using python itself without creating linked service in data factory.
I created Azure blob storage and created new container and generated SAS for the blob storage.</p>
<p><img src=""https://i.imgur.com/7JGf50F.png"" alt=""enter image description here"" /></p>
<p>I am have created data frame in synapse notebook using python using below code:</p>
<p>import pandas as pd</p>
<p>from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient</p>
<pre><code>import pandas as pd
data = {
'Name': ['ABC', 'CDE', 'BBA', 'DSA'],
'Age': [25, 30, 35, 40],
'Gender': ['F', 'M', 'M', 'M']

}
df = pd.DataFrame(data)
</code></pre>
<p>I want to save the dataframe as csv file in azure blob storage for that I converted the dataframe to csv format using below code:</p>
<pre><code>csv_data = df.to_csv(index=False)
</code></pre>
<p>I copied the connection string of blob storage account.</p>
<p><img src=""https://i.imgur.com/j15FDqU.png"" alt=""enter image description here"" /></p>
<p>I copied the data into blob storage using below code:</p>
<pre><code>from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient

connection_string = &quot;&lt;connection string&gt;&quot;
container_name = &quot;&lt;container name&gt;&quot;
blob_name = &quot;&lt;filename&gt;.csv&quot;

blob_service_client = BlobServiceClient.from_connection_string(connection_string)

container_client = blob_service_client.get_container_client(container_name)

blob_client = container_client.get_blob_client(blob_name)

blob_client.upload_blob(csv_data, overwrite=True)
</code></pre>
<p>It executed successfully.</p>
<p><img src=""https://i.imgur.com/fCXR0wh.png"" alt=""enter image description here"" /></p>
<p>My file is created in blob storage container successfully.</p>
<p><img src=""https://i.imgur.com/7c4e4hJ.png"" alt=""enter image description here"" /></p>
"
"75441660","Azure Data Factory Get Metadata activity returning ""(404) not found"" error when getting column count","<p>I am trying to implement a Get Metadata activity to return the column count of files I have in a single blob storage container.</p>
<p>Get Metadata activity is returning this error:
<a href=""https://i.stack.imgur.com/Rsp5R.png"" rel=""nofollow noreferrer"">Error</a></p>
<p>I'm fairly new to Azure Data Factory and cannot solve this. Here's what I have:</p>
<p><strong>Dataset:</strong><a href=""https://i.stack.imgur.com/IVfmr.png"" rel=""nofollow noreferrer"">Source dataset</a></p>
<p>Name- ten_eighty_split_CSV</p>
<p>Connection- Blob storage</p>
<p>Schema- imported from blob storage file</p>
<p>Parameters- &quot;FileName&quot;; string; &quot;@pipeline().parameters.SourceFile&quot;</p>
<p><strong>Pipeline:</strong></p>
<p>Name: ten eighty split</p>
<p>Parameters: &quot;SourceFile&quot;; string; &quot;@pipeline().parameters.SourceFile&quot;</p>
<p>Settings: Concurrency: 1</p>
<p>Get Metadata activity: <a href=""https://i.stack.imgur.com/CKtEV.png"" rel=""nofollow noreferrer"">Get Metadata</a></p>
<p>Only argument is &quot;Column count&quot;</p>
<p>Throws the error upon debugging. I am not sure what to do, (404) not found is so broad I could not ascertain a specific solution. Thanks!</p>
","<azure><azure-data-factory><dynamic-columns>","2023-02-13 22:06:52","97","0","1","75447163","<ul>
<li><p>The error occurs because you have given incorrect file name (or) name of a file that does not exist.</p>
</li>
<li><p>Since you are trying to use blob created event trigger to find the column count, you can use the procedure below:</p>
</li>
<li><p>After configuring the get metadata activity, create a storage event trigger. Go to <code>Add trigger -&gt; choose trigger -&gt; Create new</code>.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/Ne44hfl.png"" alt=""enter image description here"" /></p>
<ul>
<li>Click on continue. You will get a Trigger Run Parameters tab. In this, give the value as <code>@triggerBody().fileName</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/dDLS8fG.png"" alt=""enter image description here"" /></p>
<ul>
<li>Complete the trigger creation and <strong>publish the pipeline</strong>. Now whenever the file is uploaded into your container (on top of which you created storage event trigger), it will trigger the pipeline automatically (no need to debug). If the container is empty and you try to debug by giving some value for sourceFile parameter, it would give the same error.</li>
<li>Upload a sample file to your container. It will trigger the pipeline and give the desired result.</li>
</ul>
<p><img src=""https://i.imgur.com/2mKKOX8.png"" alt=""enter image description here"" /></p>
<p>The following is the trigger JSON that I created for my container:</p>
<pre><code>{
    &quot;name&quot;: &quot;trigger1&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;runtimeState&quot;: &quot;Started&quot;,
        &quot;pipelines&quot;: [
            {
                &quot;pipelineReference&quot;: {
                    &quot;referenceName&quot;: &quot;pipeline1&quot;,
                    &quot;type&quot;: &quot;PipelineReference&quot;
                },
                &quot;parameters&quot;: {
                    &quot;sourceFile&quot;: &quot;@triggerBody().fileName&quot;
                }
            }
        ],
        &quot;type&quot;: &quot;BlobEventsTrigger&quot;,
        &quot;typeProperties&quot;: {
            &quot;blobPathBeginsWith&quot;: &quot;/data/blobs/&quot;,
            &quot;blobPathEndsWith&quot;: &quot;.csv&quot;,
            &quot;ignoreEmptyBlobs&quot;: true,
            &quot;scope&quot;: &quot;/subscriptions/b83c1ed3-c5b6-44fb-b5ba-2b83a074c23f/resourceGroups/&lt;user&gt;/providers/Microsoft.Storage/storageAccounts/blb1402&quot;,
            &quot;events&quot;: [
                &quot;Microsoft.Storage.BlobCreated&quot;
            ]
        }
    }
}
</code></pre>
"
"75441648","API call from azure databricks notebook to splunk","<p>My query is that lets say I connect to a VDI and then access a clients azure environment. From there I need to make an Rest API call to other on prem or cloud hosted systems such as splunk, cmdb - service now. I am not able to directly call the API in python notebook. Is there any configuration required in Azure first to do so?</p>
<p>I tried directly calling the API using import requests but not able to do so. I think the system doesnot recognize the api call from azure as a trusted one.</p>
","<azure><azure-data-factory>","2023-02-13 22:05:44","75","0","1","75606770","<p>You can connect to your Azure Databricks workspace from on-prem or instances outside Azure by following the steps below:-</p>
<p>You can follow this Document to create your Azure Databricks inside a V-NET :-</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject"" rel=""nofollow noreferrer"">Deploy Azure Databricks in your Azure virtual network (VNet injection) - Azure Databricks | Microsoft Learn</a></p>
<p>I deployed an Azure Databricks V-NET from the template in the document above for V-NET injection with Network security group to allow inbound and outbound access to Azure Databricks workspace.</p>
<p>Create one Network Security group and copy its resource ID in the template for deployment, Like below:-</p>
<p><img src=""https://i.imgur.com/8jU0OxI.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/wwLegfl.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/0kJpDOc.png"" alt=""enter image description here"" /></p>
<p>Azure Databricks V-NET got created successfully like below:-</p>
<p><img src=""https://i.imgur.com/0rTZSTM.png"" alt=""enter image description here"" /></p>
<p>Now Deploy your Azure Databricks Workspace inside the V-NET like below:-</p>
<p><img src=""https://i.imgur.com/4Y4swFH.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/PuGZo5F.png"" alt=""enter image description here"" /></p>
<p>Databricks Workspace deployed like below:-
You can enable Public Network Access for your azure Databricks to be accessible to your On-prem or Splunk or other cloud instances with NSG Rules:-</p>
<p><img src=""https://i.imgur.com/8WVppRs.png"" alt=""enter image description here"" /></p>
<p>In NSG Rules, Make sure you allow your VDI's or other cloud's, SPLUNK API's IP as an inbound and outbound rule:-</p>
<p>Inbound connection from your VDI, on-prem or Splunk hosted  API's to Azure services or Databricks.</p>
<p><img src=""https://i.imgur.com/PuPN4MZ.png"" alt=""enter image description here"" /></p>
<p>Outbound rule to allow Azure Databricks to reach the destination systems, Make sure you allow Outbound connections to your VDI and other Cloud hosted API's from azure.</p>
<p><img src=""https://i.imgur.com/YohHpxq.png"" alt=""enter image description here"" /></p>
<p>Create a VPN connection from your On-prem or other Cloud Site to Azure and then configure virtual network gateway transit to Azure Databricks Workspace like below:-</p>
<p>Create a VPN and use the same Virtual network gateway as your VPN connection from your On-prem, cloud instances to Azure Site:-</p>
<p><img src=""https://i.imgur.com/so9rw54.png"" alt=""enter image description here"" /></p>
<p>Associated Azure Virtual Network gateway to the same V-NET as the Azure Databricks workspace, If the V-NET gateway is in different V-NET than Azure Databricks V-NET, you can configure V-NET peering and configure user defined routes.</p>
<p>Additionally, Allow your on-prem, cloud hosted firewall to access Azure Databricks URL below:-</p>
<blockquote>
<p>*.azuredatabricks.net for Azure Public</p>
<p>*.databricks.azure.us for Azure Government</p>
<p><a href=""https://adb1666506161514800.0.azuredatabricks.net"" rel=""nofollow noreferrer"">https://adb1666506161514800.0.azuredatabricks.net</a> for your azure
databricks workspace only</p>
</blockquote>
<p><strong>References:-</strong></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/on-prem-network"" rel=""nofollow noreferrer"">Connect your Azure Databricks workspace to your on-premises network - Azure Databricks | Microsoft Learn</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/databricks/security/network/firewall-rules?source=recommendations"" rel=""nofollow noreferrer"">Configure domain name firewall rules - Azure Databricks | Microsoft Learn</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/vpn-gateway/tutorial-site-to-site-portal"" rel=""nofollow noreferrer"">Tutorial - Connect an on-premises network and a virtual network: S2S VPN: Azure portal - Azure VPN Gateway | Microsoft Learn</a></p>
"
"75440562","ADF doesnt create BlobCreation event with dataflow","<p>I have pipeline and a dataflow activity inside which copies the data to blob storage. I have trigger activated.</p>
<p>Problem is, the trigger works If I place the file manually on storage. But it doesn't get triggered when the dataflow puts file on the blob storage with copy activity.</p>
<p>Here is the trigger info:
<a href=""https://i.stack.imgur.com/ReQd8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ReQd8.png"" alt=""enter image description here"" /></a></p>
","<eventtrigger><azure-data-factory>","2023-02-13 19:48:51","56","0","3","75444553","<p>I tried and it's working fine for me. it is detecting blob is getting added in particular container.</p>
<p>It appears that the trigger is set up to only react when new files are added to the blob storage and not when old files change. It's possible that the dataflow activity updates an existing file rather than producing a new one when it moves data to the blob storage.</p>
<p>Agreed with @Joel Cochran in <strong>Blob path ends with</strong> you are passing <code>train_data.parquet</code> if trigger did not find any particular file with name contain similar pattern it will not trigger the pipeline.</p>
<p>You may tweak the trigger to look for changes in both new and current files to fix this. This may be achieved by including the only <code>.parquet</code> in the <strong>Blob path ends with</strong> section of the trigger setup, which will make the trigger react to any changes to files in the supplied path.</p>
<p><img src=""https://i.imgur.com/utSBC6N.png"" alt=""enter image description here"" /></p>
<p>Specify the correct details.</p>
<p>It's possible that the trigger is not configured to detect changes made by the dataflow activity. Check the trigger's settings to ensure that it is monitoring the correct blob container and that it is set up to detect the appropriate types of changes, such as new or modified blobs.</p>
"
"75440562","ADF doesnt create BlobCreation event with dataflow","<p>I have pipeline and a dataflow activity inside which copies the data to blob storage. I have trigger activated.</p>
<p>Problem is, the trigger works If I place the file manually on storage. But it doesn't get triggered when the dataflow puts file on the blob storage with copy activity.</p>
<p>Here is the trigger info:
<a href=""https://i.stack.imgur.com/ReQd8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ReQd8.png"" alt=""enter image description here"" /></a></p>
","<eventtrigger><azure-data-factory>","2023-02-13 19:48:51","56","0","3","75444905","<p>The problem is that a sink in dataflow when using parquet format generates a BlobRenamed event instead of BlobCreation. Therefore, the trigger doesn't get the right event.</p>
"
"75440562","ADF doesnt create BlobCreation event with dataflow","<p>I have pipeline and a dataflow activity inside which copies the data to blob storage. I have trigger activated.</p>
<p>Problem is, the trigger works If I place the file manually on storage. But it doesn't get triggered when the dataflow puts file on the blob storage with copy activity.</p>
<p>Here is the trigger info:
<a href=""https://i.stack.imgur.com/ReQd8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ReQd8.png"" alt=""enter image description here"" /></a></p>
","<eventtrigger><azure-data-factory>","2023-02-13 19:48:51","56","0","3","75474769","<p>I ended up using web activity to send custom blob events to custom events and using custom triggers on the receiving pipeline.</p>
"
"75438061","runOutput isn't appearing even after using dbutils.notebook.exit in ADF","<p>I am using the below code to get some information in the Azure Databricks notebook, but runOutput isn't appearing even after the successful completion of the notebook activity.</p>
<p>Code that I used.</p>
<pre><code>import json
dbutils.notebook.exit(json.dumps({
    &quot;num_records&quot; : dest_count,
    &quot;source_table_name&quot; : table_name
}))
</code></pre>
<p>Databricks notebook exited properly, but Notebook activity isn't showing runOutput.</p>
<p>Can someone please help me what is wrong here?</p>
<p><a href=""https://i.stack.imgur.com/TXVUU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TXVUU.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Ev5ch.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ev5ch.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><azure-databricks>","2023-02-13 15:37:47","71","0","2","75445743","<p>When I tried the above in my environment, it is working fine for me.</p>
<p>These are my Linked service Configurations.</p>
<p><img src=""https://i.imgur.com/I1iZtuG.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/tVD7KR5.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/NCJHUcF.png"" alt=""enter image description here"" /></p>
<p>I suggest you try the troubleshooting steps like, changing Notebook and changing the Databricks workspace with new one or using Existing cluster in linked service.</p>
<p>If still, it is giving the same, then it's better to raise a <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">Support ticket</a> for your issue.</p>
"
"75438061","runOutput isn't appearing even after using dbutils.notebook.exit in ADF","<p>I am using the below code to get some information in the Azure Databricks notebook, but runOutput isn't appearing even after the successful completion of the notebook activity.</p>
<p>Code that I used.</p>
<pre><code>import json
dbutils.notebook.exit(json.dumps({
    &quot;num_records&quot; : dest_count,
    &quot;source_table_name&quot; : table_name
}))
</code></pre>
<p>Databricks notebook exited properly, but Notebook activity isn't showing runOutput.</p>
<p>Can someone please help me what is wrong here?</p>
<p><a href=""https://i.stack.imgur.com/TXVUU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TXVUU.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Ev5ch.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ev5ch.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><azure-databricks>","2023-02-13 15:37:47","71","0","2","75529586","<p>I am not sure why it is not populating runOuput. Basically, I need to get number of records it is pushing to Azure Synapse, So I have used Lookup Activity to get the Number of records in that table.</p>
"
"75433962","I am getting an error while connecting Azure Devops with ADF Verify that your Azure DevOps account is connected to the AAD account,","<p>I am not able to connect Azure Devops with ADF. i am getting an error as mentioned in screenshot
<a href=""https://i.stack.imgur.com/IpP4Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IpP4Y.png"" alt=""enter image description here"" /></a></p>
<p>there is no repository coming when i clicked on select repo.</p>
","<azure><azure-devops><azure-data-factory>","2023-02-13 09:19:06","115","1","1","75434885","<p>Make sure your Azure DevOps account is connected to your Azure AD tenant where the user from which you’re accessing ADF. By default, the Azure DevOps account gets created in Microsoft Tenant, Switch your Azure DevOps tenant to your Azure AD default directory and add the user account to that organization.</p>
<p><em><strong>Log in to your Azure DevOps organization &gt; right side &gt; Click on your profile &gt; My profile&gt; Switch your Directory from Microsoft account to Default directory like below:-</strong></em></p>
<p><img src=""https://i.imgur.com/WglBKGi.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/Wz01Y88.png"" alt=""enter image description here"" /></p>
<p>Now, Create one organization and Project to import your Data-Factory repository. In the same Azure DevOps organization check if it’s connected to your Azure AD directory successfully like below:-</p>
<p><img src=""https://i.imgur.com/Xc0Sjsb.png"" alt=""enter image description here"" /></p>
<p>Now, connect to Azure DevOps git repo with Azure Data factory like below:-</p>
<p><img src=""https://i.imgur.com/GzYhL7e.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/3Fj1K3p.png"" alt=""enter image description here"" /></p>
<p>Azure Data factory got connected to the git repository successfully like below:-</p>
<p><img src=""https://i.imgur.com/xYmrZEU.png"" alt=""enter image description here"" /></p>
<p>Connecting with azure devops repository link [Use repository link]</p>
<p><img src=""https://i.imgur.com/0NyOrOo.png"" alt=""enter image description here"" /></p>
<p>Got connected to the repository successfully like below:-</p>
<p><img src=""https://i.imgur.com/C02fqOy.png"" alt=""enter image description here"" /></p>
<p>In your Azure DevOps organization make sure the user with whom you’re connecting git in ADF has required roles assigned on the repositories.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/devops/repos/git/set-git-repository-permissions?view=azure-devops"" rel=""nofollow noreferrer"">Set Git repository permissions - Azure Repos | Microsoft Learn</a></p>
"
"75421213","CSV file source is SFTP and in UTF-16. How to save it in UTF_8 form in ADF or ADB","<p>The data in file is coming in UTF-16 and need to save it in UTF-8 how to handle it using ADF and ADB?</p>
<p><a href=""https://i.stack.imgur.com/sEhN9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sEhN9.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><azure-databricks><azure-data-lake>","2023-02-11 15:27:21","78","0","1","75447769","<p>Use the &quot;Copy Data&quot; activity in <strong>AZURE DATA FACTORY</strong> and follow to these instructions to convert a UTF-16 encoded file from an SFTP server to UTF-8 in Azure Data Factory.</p>
<ul>
<li>Select the SFTP dataset that corresponds to the folder containing the UTF-16 encoded file in the Source section.</li>
<li>Select the encoding property to &quot;UTF-16&quot; in the Source section of the &quot;Settings&quot; tab.</li>
<li>Choose the file dataset that corresponds to the location where the UTF-8 file will be stored in the Sink section.</li>
<li>Set the encoding property to &quot;UTF-8&quot; under the &quot;Settings&quot; tab in the Sink section.</li>
</ul>
<p><img src=""https://i.imgur.com/hYHoMod.png"" alt=""enter image description here"" /></p>
<p>The &quot;Copy Data&quot; activity will read the UTF-16 file from the SFTP server, convert it to UTF-8, and write it to the target when the pipeline is activated.</p>
"
"75421061","ADF Copy Activity problem with wildcard path","<p>I have a seemingly simple task to integrate multiple json files that are residing in a data lake gen2
The problem is files that need to be integrated are located in multiple folders, for example this is a typical structure that I am dealing with:
Folder1\Folder2\Folder3\Folder4\Folder5\2022\Month\Day\Hour\Minute\ &lt;---1 file in Minute Folder
Than same structure for 20223 year, so in order for me to collect all the files I have to go to bottom of the structure which is Minute folder, if I use wildcard path it looks like this:
Wildcard paths 'source from dataset&quot;/ *.json, it copies everything including all folders, and I just want files, I tried to narrow it down and copies only first for 2022 but whatever I do is not working  in terms of wildcard paths, help is much appreciated</p>
<p>trying different wildcard combinations did not help, obviously I am doing something wrong</p>
","<azure><azure-data-factory><wildcard-mapping>","2023-02-11 15:04:28","183","1","1","75505458","<p>There is no option to copy files from multiple sub- folders to single destination folder. Flatten hierarchy as a copy behavior also will have autogenerated file names in target.</p>
<p><img src=""https://i.imgur.com/7CdYXs4.png"" alt=""enter image description here"" />
<strong>image reference</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#file-system-as-sink"" rel=""nofollow noreferrer"">MS document on copy behaviour </a></p>
<p>Instead, you can follow the below approach.</p>
<ul>
<li>In order to list the file path in the container, take the <strong>Lookup activity</strong> and connect to xml dataset with HTTP linked service.</li>
</ul>
<p>Give the <strong>Base URL in HTTP connector</strong> as,
<code>https://&lt;storage_account_name&gt;.blob.core.windows.net/&lt;container&gt;?restype=directory&amp;comp=list</code>.</p>
<p>[Replace <code>&lt;storage account name&gt;</code> and <code>&lt;container&gt;</code> with the appropriate name in the above URL]</p>
<p><img src=""https://i.imgur.com/0fkcCPy.png"" alt=""enter image description here"" /></p>
<ul>
<li>Lookup activity gives the list of folders and files as separate line items as in following image.</li>
</ul>
<p><img src=""https://i.imgur.com/8ncQQmT.png"" alt=""enter image description here"" /></p>
<ul>
<li>Take the <strong>Filter activity</strong> and filter the URLs that end with <code>.json</code>  from the lookup activity output.</li>
</ul>
<p>Settings of filter activity:</p>
<p><strong>items:</strong>
<code>@activity('Lookup1').output.value[0].EnumerationResults.Blobs.Blob</code></p>
<p><strong>condition:</strong>
<code>@endswith(item().URL,'.json')</code></p>
<p><strong>Output of filter activity</strong>
<img src=""https://i.imgur.com/HTSYnU1.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Take the for-each activity next to filter activity and give the item of for-each as  <code>@activity('Filter1').output.value</code></p>
</li>
<li><p>Inside for-each activity, take the copy activity.</p>
</li>
<li><p>Take http connector and json dataset as source, give the base url as
<code>https://&lt;account-name&gt;.blob.core.windows.net/&lt;container-name&gt;/</code></p>
</li>
<li><p>Create the parameter for relative URL and value for that parameter as <code>@item().name</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/TsrVkIQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>In sink, give the container name and folder name.</li>
<li>Give the file name as dynamic content.
<code>@split(item().name,'/')[sub(length(split(item().name,'/')),1)]</code></li>
</ul>
<p>This expression will take the filename from relative URL value.
<img src=""https://i.imgur.com/ncn3sjv.png"" alt=""enter image description here"" /></p>
<ul>
<li>When the pipeline is run, all files from multiple folders got copied to single folder.
<img src=""https://i.imgur.com/1FlQ5n6.png"" alt=""enter image description here"" /></li>
</ul>
"
"75416554","ADF Stored Procedure call takes too long","<p>I have an Azure data factory loop activity that executes stored procedure on every iteration, passing 2 JSON objects as arguments.
Stored procedure reads json objects with openJson function using cross apply few times .. and then constructs #temp table and after some sql massaging inserts data from #temp table into a static sql table.</p>
<p>If I imitate this tasks via sqlserver management studio it takes no time at all, so I am able to hit execute button 20=50  times and it would show total time always as 0.</p>
<p>However, in a data factory every execution of an activity calling stored procedure takes a long time .. can take 5-6 seconds, that in a loop amounts to a very long time.
Is there any ways to tune these in ? So that every execution takes less time ?</p>
","<azure><azure-data-factory><azure-performancecounters>","2023-02-10 22:04:16","81","0","1","75435882","<blockquote>
<p>Is there any ways to tune these in ? So that every execution takes less time?</p>
</blockquote>
<p>I also tried with sample stored procedure which is executing in 0 seconds in SSMS and taking 4-6 seconds in ADF. If your stored procedure is taking 5-6 seconds, then it's a normal time of execution in Azure data factory stored procedure activity.</p>
<p><strong>You can break up the method into a series of sub-procedures and call the master procedure and call that master procedure from data factory as a workaround to reduce the time it takes.</strong></p>
<p>The above approach will help you to reduce the take taken by procedure in one go by doing the task in sub procedures. If it's still taking more than expected time. Please check link which explains how to raise <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">support ticket</a>.</p>
"
"75414652","Mapping a particular value to column in parametrised copy activity in Azure Data Factory","<p><strong>I am using Azure Data Factory to migrate data from source systems onto Azure SQL Database.</strong></p>
<ol>
<li>Different source systems - &gt; 2. Azure Database (Data Warehouse) - &gt; 3. Azure Database (Data Mart)</li>
</ol>
<p>I'll refer to each layers by their number.</p>
<p>Between layers 1 &amp; 2, a Run_id is generated and is a new column added to each table loaded via a Copy activity in Azure Data Factory. The new column is simply added to the end of table to match against each run of the pipelines.</p>
<p>At this current stage, between layer 2 &amp; 3 is a simple one-to-one mapping, however, I also generate a new run_id for each run in layer 3, and so I do not want to keep the data in the Run_id column from layer 2, I want to exclude mapping that column and instead map the newly generated Run_id in layer 3 to each of the tables in layer 3, similar to how I've done in layer 2.</p>
<p><a href=""https://i.stack.imgur.com/2AbVz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2AbVz.png"" alt=""How I've populated Run_id in layer 2 via additional column in Source tab"" /></a></p>
<p>The issue stems from the fact that the column names Run_id  are identical and ADF automatically assumes I want to carry over that existing data in layer 2 to layer 3.</p>
<p>I have parametrised my data migration to look for a list of tables to import in a config table which ADF cycles through via a ForEach activity and hence I cannot simply edit the mappings on the fly as my single copy activity cycles through 50+ tables:</p>
<p><a href=""https://i.stack.imgur.com/zTieZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zTieZ.png"" alt=""Ingestion pipeline"" /></a></p>
<p><a href=""https://i.stack.imgur.com/tAfVO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tAfVO.png"" alt=""Inside my ForEach activity, how I've parametrised my copy activity"" /></a></p>
<p>I have tried to add a specific mapping like in the image attached:</p>
<p><a href=""https://i.stack.imgur.com/v89gB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v89gB.png"" alt=""New mapping for specific Run_id value"" /></a></p>
<p>However when I do so, the actual data I want copied over doesn't copy and only the correct Run_id copies, as you can see in the image attached:</p>
<p><a href=""https://i.stack.imgur.com/BSlZV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BSlZV.png"" alt=""Result of the above mapping"" /></a></p>
<p>I have thought out swapping my copy activity for a parametrised stored procedure instead which could give me more flexibility</p>
<p>I have also thought about changing the run_id column name for layer 2 so there's a difference in column names but I am keen on preserving column name Run_id in both layer 2 and layer 3.</p>
<p>Essentially, between layer 2 &amp; 3, I need to copy over the data in every column EXCEPT the Run_id column where I need to assign a different value to the value which is in layer 2.
Suppose layer 2:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Data</th>
<th>Run_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>B</td>
<td>100</td>
</tr>
<tr>
<td>A</td>
<td>100</td>
</tr>
</tbody>
</table>
</div>
<p>What I need in Layer 3:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Data</th>
<th>Run_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>B</td>
<td>[NEW Run_id]</td>
</tr>
<tr>
<td>A</td>
<td>[NEW Run_id]</td>
</tr>
</tbody>
</table>
</div>
<p>Thanks and please let me know if you need any clarification.</p>
","<azure><parameters><azure-sql-database><azure-data-factory>","2023-02-10 18:03:43","202","0","1","75418416","<p>In mapping settings of copy activity, you need to select either auto-mapping of all columns or manual mapping. There is no option to manually map a single column and auto map for all other columns. Since you mapped the run-id column with input and there is no input for other columns, all other columns don't have data.</p>
<p>If schemas of all tables are same, you can give map all columns manually by importing schemas. If tables have different structure, you can copy the data from layer2 to layer3 as it is and then update the run-id with script activity. I tried this with sample data. Below is the approach.</p>
<ul>
<li>Three lookup activities are taken. One for getting the list of tables for copying from layer2 to layer3. Second one for getting layer 3 run-id and third lookup activity is for layer-2 run-id.</li>
</ul>
<p><img src=""https://i.imgur.com/0Dmn4NZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>For each activity iterates the list of items from lookup activity (<em>lookup_table_list</em>) and tables are copied from layer2 to layer3 without changing the run-id to new run-id using copy activity.</li>
<li>Script activity is taken and script is written as,</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>update @{item().sch_name}.@{item().tab_name}
set
Run_id= '@{activity('Look up layer_3 Run_id').output.firstRow.Run_id}'
where Run_id= '@{activity('Look up layer_2 Run_id').output.firstRow.Run_id}'
</code></pre>
<p>This script will assign the Run_Id column with the value from Lookup layer_3 Run_id activity.</p>
<p>This repro is done by considering run-id is same for all tables in layer-2. If it is table specific, change the logic by adding the lookup activity for getting layer2 run-id inside for-each activity.</p>
"
"75414096","ADF CopyData Is it possible to have a dynamic Additional Columns that can be nullable?","<p>I have a configuration table with file names, destination table name (and other configs) to copy data into a SQL table. Sometimes I want the filename in a new column, but not for every files.</p>
<p>Is it possible to have a default value to not generate additional column for some files?</p>
<p>I tried</p>
<pre><code>@json(
    if(
        equals(item().AdditionalColumns, null), 
        '{}', 
        item().AdditionalColumns
    )
)
</code></pre>
<p>But I get this error: The value of property 'additionalColumns' is in unexpected type 'IList`1'.</p>
<p>And</p>
<pre><code>@json(
    if(
        equals(item().AdditionalColumns, null), 
        '{[]}', 
        item().AdditionalColumns
    )
)
</code></pre>
<p>But I get this error: The function 'json' parameter is not valid. The provided value '{[]}' cannot be parsed: 'Invalid property identifier character: [. Path '', line 1, position 1</p>
<p>Thank you</p>
","<azure-data-factory>","2023-02-10 17:04:52","69","0","1","75415314","<p>I figured out.</p>
<pre><code>@json(
    if(
        equals(item()?.AdditionalColumns, null), 
        '[]', 
        item()?.AdditionalColumns
    )
)
</code></pre>
"
"75407178","Deleting a data factory component not working using CI CD pipelines","<p>I have implemented CI CD for Azure Data Factory. When I modify a pipeline and publish my changes and deploy it to UAT, the changes reflect properly. The problem arises when I delete any component in dev and publish it and deploy it to the UAT data factory. The changes do not reflect.
How to handle the delete scenario in CI CD pipelines?</p>
<p>As of now I am creating separate releases. Everytime they are deployed, the deleted changes are not reflecting.</p>
","<azure><azure-devops><azure-data-factory>","2023-02-10 05:07:54","115","1","1","75426846","<p>CI\CD only merge changes, so it doesn't delete objects.</p>
<p>If you are using the Azure DevOps release pipeline, there is a Microsoft PowerShell sample script that has this ability that you can add to your release pipeline.
Explanation on how to use the script in a release pipeline - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-sample-script"" rel=""nofollow noreferrer"">here</a></p>
<p><a href=""https://github.com/Azure/Azure-DataFactory/tree/main/SamplesV2/ContinuousIntegrationAndDelivery"" rel=""nofollow noreferrer"">Link to the GitHub project containing the actual script</a>.</p>
<p>Set the parameter deleteDeployment to true (like this: -deleteDeployment $true).</p>
"
"75407026","Additional column throwing validation issue with Azure SQL data sink in Azure Data Factory","<p><a href=""https://i.stack.imgur.com/wq4iQ.png"" rel=""nofollow noreferrer"">Validation Error</a></p>
<p>I've got this weird issue where validation fails on 'additional columns' for my data sink to Azure SQL coming from a blob storage source in the Azure Data Factory GUI. No matter how many times we recreate the dataset (or specify another dataset, new) we can't get past this validation issue.</p>
<p>The irony of this is we deploy these pipelines from code and when we run them, we get no errors at all. This issue we have had just made life really difficult developing pipelines further as we have to do everything by code. We cant use the pipepline publish option.</p>
<p>Here are some screen grabs for you of the pipeline so you can see the flow.</p>
<p><a href=""https://i.stack.imgur.com/7uCqi.png"" rel=""nofollow noreferrer"">Pipeline</a></p>
<p>Inside copyCustomer.</p>
<p><a href=""https://i.stack.imgur.com/bVZMP.png"" rel=""nofollow noreferrer"">Source</a></p>
<p><a href=""https://i.stack.imgur.com/BN9Ue.png"" rel=""nofollow noreferrer"">Mapping</a></p>
<p><a href=""https://i.stack.imgur.com/SQtTK.png"" rel=""nofollow noreferrer"">Sink</a></p>
<p>Any ideas on how to fix this validation would be greatly appreciated.</p>
<p>For what it's worth, we have recreated the dataset multiple times (clone and new) to avoid any issue with the dataset model not being the latest as per what's documented here <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy</a></p>
<p>Sometimes by setting the table in sink to autocreate has shown the validation to be 'fixed' but then when we go to publish it errors out again.</p>
","<azure><azure-data-factory>","2023-02-10 04:40:06","84","0","2","75408896","<p>When your Azure SQL dataset was created long time before and is still utilizing an outdated dataset model that Additional Columns do not support, this is expected behavior.</p>
<p>As per official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy"" rel=""nofollow noreferrer"">Microsoft documentation</a></p>
<blockquote>
<p>To resolve this issue, you can just follow the error message to create a new Azure SQL dataset and use this as copy sink.
<img src=""https://i.imgur.com/OdXBIt2.png"" alt=""enter image description here"" /></p>
</blockquote>
<p>I followed error message and created new data set and it is working fine for me.</p>
<p><strong>Source:</strong>
<img src=""https://i.imgur.com/N73nKKl.png"" alt=""enter image description here"" /></p>
<p><strong>Mapping:</strong>
<img src=""https://i.imgur.com/N9b4jUO.png"" alt=""enter image description here"" /></p>
<p><strong>Sink:</strong>
<img src=""https://i.imgur.com/kQhjnMT.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong>
<img src=""https://i.imgur.com/uu0fLns.png"" alt=""enter image description here"" /></p>
"
"75407026","Additional column throwing validation issue with Azure SQL data sink in Azure Data Factory","<p><a href=""https://i.stack.imgur.com/wq4iQ.png"" rel=""nofollow noreferrer"">Validation Error</a></p>
<p>I've got this weird issue where validation fails on 'additional columns' for my data sink to Azure SQL coming from a blob storage source in the Azure Data Factory GUI. No matter how many times we recreate the dataset (or specify another dataset, new) we can't get past this validation issue.</p>
<p>The irony of this is we deploy these pipelines from code and when we run them, we get no errors at all. This issue we have had just made life really difficult developing pipelines further as we have to do everything by code. We cant use the pipepline publish option.</p>
<p>Here are some screen grabs for you of the pipeline so you can see the flow.</p>
<p><a href=""https://i.stack.imgur.com/7uCqi.png"" rel=""nofollow noreferrer"">Pipeline</a></p>
<p>Inside copyCustomer.</p>
<p><a href=""https://i.stack.imgur.com/bVZMP.png"" rel=""nofollow noreferrer"">Source</a></p>
<p><a href=""https://i.stack.imgur.com/BN9Ue.png"" rel=""nofollow noreferrer"">Mapping</a></p>
<p><a href=""https://i.stack.imgur.com/SQtTK.png"" rel=""nofollow noreferrer"">Sink</a></p>
<p>Any ideas on how to fix this validation would be greatly appreciated.</p>
<p>For what it's worth, we have recreated the dataset multiple times (clone and new) to avoid any issue with the dataset model not being the latest as per what's documented here <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy</a></p>
<p>Sometimes by setting the table in sink to autocreate has shown the validation to be 'fixed' but then when we go to publish it errors out again.</p>
","<azure><azure-data-factory>","2023-02-10 04:40:06","84","0","2","75434656","<blockquote>
<p>I suspect here, your dataset of Sink type is incorrect. I reproduced,
same at my end. Its working fine. Kindly make sure you create a sink dataset type with Azure SQL database type connector only.
Please check below screenshots from my implementation.
<a href=""https://i.stack.imgur.com/ZoRkt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZoRkt.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/ub26X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ub26X.png"" alt=""enter image description here"" /></a>
If still it helps, feel free to share your sink dataset type connector details along with screenshots.</p>
</blockquote>
"
"75406850","How to modify dynamic complex data type fields in azure data factory data flow","<p>I have a complex data type (fraudData) that undesirably has hyphen characters in the field names I need to remove or change the hypens to some other character.</p>
<p>The input schema of the complex object looks like:
<a href=""https://i.stack.imgur.com/Zoz2r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zoz2r.png"" alt=""enter image description here"" /></a></p>
<p>I have tried using the &quot;<strong>Select</strong>&quot; and &quot;<strong>Derive Column</strong>&quot; data flow functions and adding a custom mapping. It seems both functions have the same mapping interface. My current attempt with <strong>Select</strong> is:
<a href=""https://i.stack.imgur.com/eb96B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eb96B.png"" alt=""enter image description here"" /></a></p>
<p>This gets me close to the desired results. I can use the <code>replace</code> expression to convert hypens to underscores.</p>
<p>The problem here is that this mapping creates new root level columns outside of the <code>fraudData</code> structure. I would like to preserve the hierarchy of the <code>fraudData</code> structure and modify the column names in place.</p>
<p>If I am unable to modify the <code>fraudData</code> in place. Is there any way I can take the new columns and merge them into another complex data type?</p>
<p><strong>Update:</strong>. I do not know the fields of the complex data type in advance. This is a schema drift problem. This is why I have tried using the pattern matching solution. I will not be able to hardcode out kown sub-column names.</p>
","<azure><azure-data-factory>","2023-02-10 04:08:23","412","0","1","75408034","<p>You can rename the sub-columns of complex data type using derived column transformation and convert them as a complex data type again. I tried this with sample data and below is the approach.</p>
<ul>
<li><p>Sample complex data type column with two sub fields are taken as in below image.
<img src=""https://i.imgur.com/VEFkMOk.png"" alt=""enter image description here"" />
img:1 source data preview</p>
</li>
<li><p>In Derived column transformation, For the column <code>fraudData</code>, expression is given as</p>
</li>
</ul>
<pre><code>@(fraudData_1_chn=fraudData.{fraudData-1-chn},
fraudData_2_chn=fraudData.{fraudData-2-chn})
</code></pre>
<p><img src=""https://i.imgur.com/ZvSxod2.png"" alt=""enter image description here"" />
img:2 Derived column settings</p>
<ul>
<li>This expression renames the subfields and nests them under the parent column <code>fraudData</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/tYMX629.png"" alt=""enter image description here"" />
img:3 Transformed data- Fields are renamed.</p>
<p><strong>Update: To rename sub columns dynamically</strong></p>
<ul>
<li>You can use below expression to rename all the fields under the root column <code>fraudData</code>.</li>
</ul>
<pre><code>@(each(fraudData, match(true()), replace($$,'-','_') =  $$))
</code></pre>
<p><img src=""https://i.imgur.com/vz34WOU.png"" alt=""enter image description here"" /></p>
<p>This will replace fields which has <code>-</code> with <code>_</code>.</p>
<p><img src=""https://user-images.githubusercontent.com/113445679/219585616-9050ca8b-0dbe-4645-aac3-a97fae81f086.gif"" alt=""gif112"" /></p>
<p>You can also use pattern match in the expression.</p>
<pre><code>@(each(fraudData, patternMatch(`fraudData-.+` ), replace($$,'-','_') = $$))
</code></pre>
<p>This expression will take fields with pattern <code>fraudData-.+</code> and replace <code>-</code> with <code>_</code> in those fields only.</p>
<p><em><strong>Reference:</strong></em></p>
<ol>
<li><strong>Microsoft document</strong> on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-json#sample-manual-script-for-complete-hierarchical-definition"" rel=""nofollow noreferrer"">script for hierarchical definition in data flow</a>.</li>
<li><strong>Microsoft document</strong> on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column#building-schemas-using-the-expression-builder"" rel=""nofollow noreferrer"">building schemas using derived column transformation</a> .</li>
</ol>
"
"75406702","ADF-pipeline onfailure not working for throtling expections","<p><a href=""https://i.stack.imgur.com/nHSm1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nHSm1.png"" alt=""enter image description here"" /></a></p>
<p>Please check above image where highlighted with yellow that get processed activity on failure connected with drop table  but its not working  and its not even triggering when expection like throtling in get processed activity</p>
<p>Help to trigger the drop table for get proccesed activity on failure</p>
","<azure><azure-data-factory>","2023-02-10 03:39:05","52","0","1","75406950","<p>Azure data explorer command <strong>Drop table</strong> activity will be executed when Azure Data explorer command <strong>Get Alerts from Processed Table</strong> activity is failed <strong>and</strong> Lookup activity is failed <strong>and</strong> For-each activity is succeeded.</p>
<p><img src=""https://i.stack.imgur.com/nHSm1.png"" alt=""enter image description here"" /></p>
<p>The solution is to have DROP TABLE activity thrice. One upon failure of  <strong>Get Alerts from Processed table</strong> Azure data explorer command activity and second one on failure of lookup activity and the other one upon success of For-each activity.</p>
<p><img src=""https://i.imgur.com/Vy5LzyI.png"" alt=""enter image description here"" /></p>
"
"75405245","Failed to register Azure Integration Runtime (self-hosted). The Authentication Key is invalid or empty","<p>I am creating a self hosted integration runtime. There is error in registration after installed the IR on Window.The authentication key is just copied from portal and paste on it. But, the warning says the key is invalid or empty. How can I solve it?
Both Azure Data Factory and on-prem data server are on same time zone so no time delay.
Any suggestion would be great as struggled most of the day.
I have tried by using express route exe and integration runtime msi both but same result no luck :(
<a href=""https://i.stack.imgur.com/QZTNX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QZTNX.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-02-09 22:48:31","243","1","1","75449069","<p>It was networking issue as same key working on public endpoint network setting. Now it will fix private endpoint.</p>
"
"75403984","How to connect to an on-premises SQL Server from an Azure ML notebook?","<p>I typically use pyodbc when running jupyter notebooks from my machine, but this does not work on Azure ML. My assumption is that this is being caused by Azure ML not knowing if I'm on my company's network as I typically need a VPN to the server if I'm not in office. The only solutions I can find online involve copying the data over on Azure Data Factory however I need to avoid this if possible as there are many tables I will need to experiment with, but nothing is intended to be long term and I'm unsure what I will even end up using.</p>
<p>Ideally there is a way to make pyodbc work but any other suggestions are welcome. I have researched integration runtimes but was unsure if that would solve my problem here.</p>
","<sql-server><azure><odbc><azure-machine-learning-service><azure-data-factory>","2023-02-09 20:16:21","91","0","1","75412456","<blockquote>
<p>The only solutions I can find online involve copying the data over on<br />
Azure Data Factory however I need to avoid this if possible as there<br />
are many tables I will need to experiment with, but nothing is<br />
intended to be long term and I’m unsure what I will even end up using.<br />
Ideally there is a way to make pyodbc work but any other suggestions</p>
</blockquote>
<p>Unfortunately, the on-Prem SQL Server is not supported as a Data Source in Azure ML.<br />
Only the Data sources available below are supported:-</p>
<p><img src=""https://i.imgur.com/Vr25wTJ.png"" alt=""enter image description here"" /></p>
<p><em><strong>Approach1)</strong></em><br />
You can copy your data from the on-premises SQL database to Azure SQL via copy tool in Azure Data factory and connect to Azure SQL via Azure Machine learning by directly connecting to it via Datasource like below:-</p>
<p><img src=""https://i.imgur.com/oasUK5n.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/0D29vxD.png"" alt=""enter image description here"" /></p>
<p>You can also use Self-hosted integration run time to connect to your SQL server on-prem in your data factory:-</p>
<p><img src=""https://i.imgur.com/5kX0raY.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/z0F2D3L.png"" alt=""enter image description here"" /></p>
<p>Click on Option 2 to download the Integration runtime and set it in your local machine with the Registration keys mentioned above:-</p>
<p><img src=""https://i.imgur.com/inWRXG4.png"" alt=""enter image description here"" /></p>
<p><em><strong>Approach2)</strong></em><br />
If there’s a large data You can automate your entire copy process from the on-prem SQL server to Azure SQL by using the Azure DevOps pipeline.</p>
<p><strong>References:-</strong></p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/775844/unable-to-connect-sql-server-to-azure-ml-pipeline"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/775844/unable-to-connect-sql-server-to-azure-ml-pipeline</a> By Ramr-msft</p>
<p><a href=""https://servian.dev/how-to-azure-data-factory-ci-cd-with-azure-devops-pipelines-the-yaml-way-e035c20af275"" rel=""nofollow noreferrer"">How To: Azure Data Factory CI/CD with Azure DevOps pipelines — The YAML WAY! | by Raghavendra Bharadwaj | Servian</a></p>
"
"75401557","Getting access_token for Azure Managed Identity in Azure DataFactory as text","<p>Our use case is to connect Azure Datafactory (ADF) to AWS S3, but use the Managed Identity (MSI) of ADF for authentication and authorization.</p>
<h3>TL;DR version</h3>
<p>The problem we run into is that we require the access_token for the MSI in ADF, so we can exchange that for temporary credentials in AWS IAM service. We need this access_token in text, such that we can provide it in the right way to the IAM service.</p>
<h3>Situation (longer version)</h3>
<p>High over, the solution should work like this:</p>
<ol>
<li>ADF will get an access token for a specific resource using MSI</li>
<li>Using the access token, ADF will then get temporary credentials with AWS</li>
<li>Using the temporary credentials, ADF will get data from S3.</li>
</ol>
<p>In order to do this, we needed a couple of things (heavily inspired by <a href=""https://blog.identitydigest.com/azuread-access-aws/"" rel=""nofollow noreferrer"">this blog</a>):</p>
<p><strong>Azure side:</strong></p>
<ul>
<li>We created an App Registration, and set an Application ID URI (which will be the 'scope' claim in the AzureAD access_token <strong>request</strong>).</li>
<li>We created a custom role in that App Registration.</li>
<li>In the Enterprise Application object of this App Registration (at this point, I feel like I should appologize for Microsofts terminology..), we have ensured that User Assignment is required.</li>
<li>We have assigned the custom role to our ADF MSI.</li>
</ul>
<p><strong>AWS side:</strong></p>
<ul>
<li>Added our AzureAD as an Identity Provider</li>
<li>Set the audience to the same value as Application ID URI.</li>
<li>Added a new role with a trusted entity of type Web Entity, and added proper S3 permissions to it.</li>
</ul>
<p>Then to test this all out, we created an Azure Function (http triggered) which returns the <strong>request</strong> headers as body. We then created a Web Activity in ADF to this Azure Function endpoint, and set the authentication to &quot;System Assigned Managed Identity&quot;, with a resource the same as the aforementioned Application ID URI. The result is that we get the <code>Authorization</code> header value, which we then manually put into a request to the AWS IAM service to exchange for the temporary credentials. The request to the AWS IAM service has the format of <code> https://sts.amazonaws.com/?Action=AssumeRoleWithWebIdentity&amp;RoleSessionName=app1&amp;RoleArn=&lt;arn&gt;&amp;WebIdentityToken=&lt;access token&gt;</code>. This provides us with credentials, which can be used in a Linked Service in ADF (we tested this).</p>
<h3>Problem statement</h3>
<p>We now use Azure Function, in order to have ADF automatically get an access_token for the requested (AWS) resource (Application ID URI), and add that access_token to the request to the Function, which solely returns it to us. We want to do this without an additional component. I can think of two ways:</p>
<ul>
<li>(option 1) - A web activity to some Microsoft endpoint that returns the access_token immediately.</li>
<li>(option 2) - Have AWS take an <code>Authorization</code> header rather than a <code>WebIdentityToken</code> query parameter.</li>
</ul>
<p>I spent some time on option 2, but that seems like a no go; the access_token really needs to be part of the URL parameters when trying to exchange them for temporary AWS credentials.</p>
<p>Option 1 however, I had an idea; there is the IMDS on virtual machines in Azure. This can be used to get access_tokens when you are on a VM rather than a PaaS service. I tried making a call to <code>http://169.254.169.254/metadata/identity/oauth2/token?api-version=2021-12-13&amp;resource=&lt;Application ID URI&gt;</code> using Web Activity (both with a AutoResolveIR and a SelfHosted IR!), but I got the error <code>[ClientSideException] Value does not fall within the expected range</code>. I did set the header <code>Metadata</code> to value <code>true</code> as described in the docs.</p>
<p>Is there another way? Apologies if this is an abundance of information, but it does provide you with all the required details of what has been tried and how the setup should (and can) work.</p>
","<oauth-2.0><azure-data-factory><amazon-iam><azure-managed-identity>","2023-02-09 16:27:09","205","2","2","75490416","<p>It sounds like you're using Azure AD as an identity provider in AWS. If possible, you can create a AWS user with a permanent access key/secret key. The AWS user can have access to your S3 buckets, and you won't need to deal with STS in ADF.</p>
<p>Another idea is to use Azure KeyVault. When you create your S3 linked service in ADF, you can parameterize the access key and secret key. Your AWS access key and secret key will be stored in Azure KeyVault. Then you can have a Azure function that updates the KeyVault on a schedule or at the start of your ADF pipeline.</p>
"
"75401557","Getting access_token for Azure Managed Identity in Azure DataFactory as text","<p>Our use case is to connect Azure Datafactory (ADF) to AWS S3, but use the Managed Identity (MSI) of ADF for authentication and authorization.</p>
<h3>TL;DR version</h3>
<p>The problem we run into is that we require the access_token for the MSI in ADF, so we can exchange that for temporary credentials in AWS IAM service. We need this access_token in text, such that we can provide it in the right way to the IAM service.</p>
<h3>Situation (longer version)</h3>
<p>High over, the solution should work like this:</p>
<ol>
<li>ADF will get an access token for a specific resource using MSI</li>
<li>Using the access token, ADF will then get temporary credentials with AWS</li>
<li>Using the temporary credentials, ADF will get data from S3.</li>
</ol>
<p>In order to do this, we needed a couple of things (heavily inspired by <a href=""https://blog.identitydigest.com/azuread-access-aws/"" rel=""nofollow noreferrer"">this blog</a>):</p>
<p><strong>Azure side:</strong></p>
<ul>
<li>We created an App Registration, and set an Application ID URI (which will be the 'scope' claim in the AzureAD access_token <strong>request</strong>).</li>
<li>We created a custom role in that App Registration.</li>
<li>In the Enterprise Application object of this App Registration (at this point, I feel like I should appologize for Microsofts terminology..), we have ensured that User Assignment is required.</li>
<li>We have assigned the custom role to our ADF MSI.</li>
</ul>
<p><strong>AWS side:</strong></p>
<ul>
<li>Added our AzureAD as an Identity Provider</li>
<li>Set the audience to the same value as Application ID URI.</li>
<li>Added a new role with a trusted entity of type Web Entity, and added proper S3 permissions to it.</li>
</ul>
<p>Then to test this all out, we created an Azure Function (http triggered) which returns the <strong>request</strong> headers as body. We then created a Web Activity in ADF to this Azure Function endpoint, and set the authentication to &quot;System Assigned Managed Identity&quot;, with a resource the same as the aforementioned Application ID URI. The result is that we get the <code>Authorization</code> header value, which we then manually put into a request to the AWS IAM service to exchange for the temporary credentials. The request to the AWS IAM service has the format of <code> https://sts.amazonaws.com/?Action=AssumeRoleWithWebIdentity&amp;RoleSessionName=app1&amp;RoleArn=&lt;arn&gt;&amp;WebIdentityToken=&lt;access token&gt;</code>. This provides us with credentials, which can be used in a Linked Service in ADF (we tested this).</p>
<h3>Problem statement</h3>
<p>We now use Azure Function, in order to have ADF automatically get an access_token for the requested (AWS) resource (Application ID URI), and add that access_token to the request to the Function, which solely returns it to us. We want to do this without an additional component. I can think of two ways:</p>
<ul>
<li>(option 1) - A web activity to some Microsoft endpoint that returns the access_token immediately.</li>
<li>(option 2) - Have AWS take an <code>Authorization</code> header rather than a <code>WebIdentityToken</code> query parameter.</li>
</ul>
<p>I spent some time on option 2, but that seems like a no go; the access_token really needs to be part of the URL parameters when trying to exchange them for temporary AWS credentials.</p>
<p>Option 1 however, I had an idea; there is the IMDS on virtual machines in Azure. This can be used to get access_tokens when you are on a VM rather than a PaaS service. I tried making a call to <code>http://169.254.169.254/metadata/identity/oauth2/token?api-version=2021-12-13&amp;resource=&lt;Application ID URI&gt;</code> using Web Activity (both with a AutoResolveIR and a SelfHosted IR!), but I got the error <code>[ClientSideException] Value does not fall within the expected range</code>. I did set the header <code>Metadata</code> to value <code>true</code> as described in the docs.</p>
<p>Is there another way? Apologies if this is an abundance of information, but it does provide you with all the required details of what has been tried and how the setup should (and can) work.</p>
","<oauth-2.0><azure-data-factory><amazon-iam><azure-managed-identity>","2023-02-09 16:27:09","205","2","2","76013515","<p>After consultation with Microsoft, we have reached the conclusion that this cannot be done. We have opted to create an Azure Function, that will get an access token based on its own MI, which can only be called with the MI of the ADF instance. The Function will then store the temporary key in a KeyVault.</p>
<p>The call to the Azure Function will be synchronous. When it completes (should take only seconds), then we have guaranteed a valid key in KeyVault. We can use the standard copy activity with an S3 source, and point it to the KeyVault to retrieve the STS token. So the entire flow is (ADF=Azure Data Factory, AF=Azure Function, AAD=Azure Active Directory, KV=KeyVault, STS=AWS Security Token Service, MI=Managed Identity):</p>
<ol>
<li>ADF calls AF using the ADF MI.</li>
<li>AF (Python, using <code>msal</code> package) retrieves access token from AAD for resource <code>S3</code> (which is the <code>application ID URI</code> of the app registration that represents the S3 resource).</li>
<li>AF (Python, using <code>requests</code>) retrieves temporary tokens from STS using <code>AssumeRoleWithWebIdentity</code> endpoint.</li>
<li>AF stores <code>AccessKeyId</code>, <code>SecretAccessKey</code> and <code>SessionToken</code> in KV and returns status <code>200</code>.</li>
<li>ADF continues and calls copy activity, with a linked service to Amazon S3, using <code>Temporary security credentials</code> as <code>Authentication Type</code> and references the values of the KV.</li>
</ol>
"
"75400322","By Passing Wildcard filename to get all the files unioned in Azure dataflow dataset?","<p>I have created a new dataflow ,at the source i have given Wildcard file name as
input/*.csv</p>
<p>Does this do the union of all csv files under input folder and out put it ?</p>
<p>Because When I do an aggregate on bothe source1 and source 2 ,source 1 has higher row count than source 2.
Does that meant ,we can union like this without union transformation?<a href=""https://i.stack.imgur.com/Jz91C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jz91C.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/2BY2z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2BY2z.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-02-09 14:49:43","76","0","1","75407090","<blockquote>
<p>By Passing Wildcard filename to get all the files unioned in Azure dataflow dataset?</p>
</blockquote>
<p>Yes, it provides output similar to union transformation irrespective of schema of files are similar or not. if any column is not present any of the file it will return null values for that column in other files similar to union transformation.</p>
<p><img src=""https://i.imgur.com/Vu34YmT.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>Does that mean, we can union like this without union transformation?</p>
</blockquote>
<p>As Union transformation can union only 2 datasets as per blew image you can use wildcards to union all files with file name you are passing to match.</p>
<p><img src=""https://i.imgur.com/U6ANhF7.png"" alt=""enter image description here"" /></p>
"
"75399768","Connect to VM Behind Network Security Group","<p>I would like to use Azure Data Factory on MySQL Database that is being run on a VM behind NSG. I'm able to connect to it via Public IP when NSG is allowing public access on port 3306, which shouldn't be used in production. Yet, I'm not able to connect to via Private IP despite having set up a private endpoint.
Why is it not working?
<a href=""https://i.stack.imgur.com/7Crtv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Crtv.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/DdT11.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DdT11.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/Ml3Wi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ml3Wi.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-02-09 14:06:17","89","4","1","75406724","<p>Adding a private endpoint to ADF just changes the networking for connecting to the front door of ADF. It has no impact on how ADF connects to other services such as MySQL.</p>
<p>I would suggest you should create a new <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory"" rel=""nofollow noreferrer"">Self-Hosted Integration Runtime (SHIR)</a> on the same VNET (or a peered VNET) which is allowed to connect to MySQL. Then change the ADF linked service for MySQL to use the new SHIR.</p>
"
"75398724","Azure Data Factory - How to transform object with dynamic keys to array in a data flow?","<p>After spending many hours of reading the documentation, following some tutorials and trial &amp; error, i just can't figure it out; how can I transform the following complex object with key objects to an array using a data flow in Azure Data Factory?</p>
<p><strong>Input</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;headers&quot;: {
    &quot;Content-Length&quot;: 1234
  },
  &quot;body&quot;: {
    &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;: {
      &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
      &quot;name&quot;: &quot;Example 1&quot;,
      &quot;date&quot;: &quot;2023-02-09&quot;
    },
    &quot;0c206312-2348-391b-99f0-261323a94d95&quot;: {
      &quot;id&quot;: &quot;0c206312-2348-391b-99f0-261323a94d95&quot;,
      &quot;name&quot;: &quot;Example 2&quot;,
      &quot;date&quot;: &quot;2023-02-09&quot;
    },
    &quot;0c82d1e4-a897-32f2-88db-6830a21b0a43&quot;: {
      &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
      &quot;name&quot;: &quot;Example 3&quot;,
      &quot;date&quot;: &quot;2023-02-09&quot;
    },
  }
}
</code></pre>
<p><strong>Expected output</strong></p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
    &quot;name&quot;: &quot;Example 1&quot;,
    &quot;date&quot;: &quot;2023-02-09&quot;
  },
  {
    &quot;id&quot;: &quot;0c206312-2348-391b-99f0-261323a94d95&quot;,
    &quot;name&quot;: &quot;Example 2&quot;,
    &quot;date&quot;: &quot;2023-02-09&quot;
  },
  {
    &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
    &quot;name&quot;: &quot;Example 3&quot;,
    &quot;date&quot;: &quot;2023-02-09&quot;
  }
]
</code></pre>
","<arrays><json><azure><azure-data-factory>","2023-02-09 12:39:16","230","0","1","75407474","<p><strong>AFAIK</strong>, Your JSON keys are dynamic. So, getting the desired result using dataflow might not be possible.</p>
<p>In this case, you can try the below approach as a workaround. This will work only if all of your key's length is same.</p>
<p><strong>This is my Pipeline:</strong></p>
<p><img src=""https://i.imgur.com/SBwXhv7.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>First I have used a <strong>lookup activity</strong> to get the JSON file and converted the lookup output to a string and stored in a variable using below expression.
<code>@substring(string(activity('Lookup1').output.value[0].body),2,sub(length(string(activity('Lookup1').output.value[0].body)),4))</code>.</p>
</li>
<li><p>Then I have used <strong>split on that String variable</strong> with <code>'},&quot;'</code> and stored in an array variable using below expression.
<code>@split(variables('res_str'),'},&quot;')</code>
It will give the array like below.</p>
<p><img src=""https://i.imgur.com/ZzOCDfo.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Give that array to a <strong>ForEach</strong> and inside ForEach use an <strong>append variable</strong> activity to store the keys into an array with below expression.
<code>@take(item(), 36)</code></p>
</li>
<li><p>Now, I got the list of keys in an array, after the above ForEach use another ForEach activity to get the desired array of objects. Use append variable actvity inside ForEach and give the below expression for it.
<code>@activity('Lookup1').output.value[0].body[item()]</code></p>
</li>
</ul>
<p><strong>Result array after ForEach will be:</strong></p>
<p><img src=""https://i.imgur.com/P5qhwv8.png"" alt=""enter image description here"" /></p>
<p>If you want to store the above JSON into a file, you need to use <code>OPENJSON</code> from SQL. This is because copy activity additonal column only supports string type not an array type.</p>
<p>Use a SQL dataset on copy activity source and give the below SQL script in the query.</p>
<pre><code>DECLARE @json NVARCHAR(MAX)
SET @json =   
  N'@{variables('json_arr')}'  
   
SELECT * FROM  
 OPENJSON ( @json )  
WITH (   
              id varchar(200) '$.id' ,  
              name varchar(32)     '$.name',  
              date varchar(32) '$.date'
 )
</code></pre>
<p>In Sink, give a JSON dataset and select Array of Objects as File pattern.</p>
<p><img src=""https://i.imgur.com/PVGyfzC.png"" alt=""enter image description here"" /></p>
<p>Execute the pipeline and you will get the above array inside a file.</p>
<p><strong>This is my Pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline1&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Lookup1&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;JsonSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Json1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;Lookup output to Str&quot;,
            &quot;description&quot;: &quot;&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;res_str&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@substring(string(activity('Lookup1').output.value[0].body),2,sub(length(string(activity('Lookup1').output.value[0].body)),4))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Split Str to array&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup output to Str&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;split_arr&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@split(variables('res_str'),'},\&quot;')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;build keys array using split array&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Split Str to array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('split_arr')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;take first 36 chars of every item&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;keys_array&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@take(item(), 36)&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;build final array using keys array&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;build keys array using split array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('keys_array')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;description&quot;: &quot;append every object to array&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;json_arr&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@activity('Lookup1').output.value[0].body[item()]&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Just for Res show&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;build final array using keys array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;final_res_show&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@variables('json_arr')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Copy data1&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Just for Res show&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                    &quot;sqlReaderQuery&quot;: &quot;DECLARE @json NVARCHAR(MAX)\nSET @json =   \n  N'@{variables('json_arr')}'  \n   \nSELECT * FROM  \n OPENJSON ( @json )  \nWITH (   \n              id varchar(200) '$.id' ,  \n              name varchar(32)     '$.name',  \n              date varchar(32) '$.date'\n )&quot;,
                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                    &quot;partitionOption&quot;: &quot;None&quot;
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;JsonSink&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonWriteSettings&quot;,
                        &quot;filePattern&quot;: &quot;arrayOfObjects&quot;
                    }
                },
                &quot;enableStaging&quot;: false
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;Target_JSON&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        }
    ],
    &quot;variables&quot;: {
        &quot;res_str&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;split_arr&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;keys_array&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;final_res_show&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;json_arr&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Result file:</strong></p>
<p><img src=""https://i.imgur.com/BHxec9G.png"" alt=""enter image description here"" /></p>
"
"75398029","Rest call failed with client error, status code 406 NotAcceptable","<p>When I test with Advanced Rest Client(Arc) all nine API calls to REST API works fine.
I use method Get with two headers the first is Authorization Bearer
the second one is Content-type application/json. It works even if I remove
header Content-type application/json
Here is a screenshot of the response from Arc. This same REST API call give error from Azure Data Factory(ADF).
<a href=""https://i.stack.imgur.com/T3zO8.png"" rel=""nofollow noreferrer"">Sceenshot from Arc</a>
I call nine REST API from Azure Data Factory(ADF) with the same base url but different Relative URL.
Out of these nine 6 works perfect.
When I use Azure Data Factory(ADF) I use additional header
Authorization Bearer @{activity('GetToken').output.access}
What is strange is that the exact same call from Arc works fine but I get error when I call from ADF. Note also that I get the exact same error if I remove the additional header
Authorization Bearer @{activity('GetToken').output.access}
I mean that the code in REST API doesn't know if the call is comming from Arc or ADF.</p>
<p>Note also the the error is from the source side so my call to REST API with method GET
can't be handled by the REST API code for some reason.
According to the documentation for the REST API it says that
Headers Content-type application/json and
Authorization Bearer 
I tried to add a second additional header in ADF Headers Content-type application/json
but I get REST connector ignores any &quot;Content-Type&quot; header specified in additional headers when request body is empty.
I have tried to find any sensible information about my error but there no one that have had any similar. What I find very strange is that 6 Rest API calls works fine and the json that we receive when using Arc is valid.
I don't realy understand the error message when saying
Requested format \u0022application/json\u0022 is not supported
Supported MIME types are \u0022application/ld+json\u0022
Here is the complete error message I get <a href=""https://i.stack.imgur.com/NYHCD.png"" rel=""nofollow noreferrer"">Screen shot of error message for ADF</a></p>
","<json><azure><rest><azure-data-factory>","2023-02-09 11:35:46","84","0","1","75410505","<p>Your response data is JSON LD (Json linked data). Hence you are seeing this error.</p>
<blockquote>
<p>To avoid this error use <code>Content-Type</code> header value as <code>application/ld+json</code>.</p>
</blockquote>
"
"75397472","I want to copy files from File System to another FileSystem based on Country wise Tried with GetMetadata but ingesting only one file in the target","<p>Files are in the form of Country wise in the FileSystem want to copy files as is in another Source System and then I need to copy them into adls So basically I need to use copy data twise if any code Suggestions that would be great Thank you in Advance
ex:
AU==&gt;Filename1.csv
Filename2.csv
Filename3.csv
UK==&gt;Filename1.csv
Filename2.csv
EU==&gt;Filename1.csv
Filename2.csv
source path</p>
<p><a href=""https://i.stack.imgur.com/nNhSf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nNhSf.png"" alt=""[1]: https://i.stack.imgur.com/55u82.png"" /></a></p>
<p>target should be same but identically I need to copy in two locations by using single PL</p>
","<azure><azure-databricks><azure-data-factory><azure-data-lake>","2023-02-09 10:45:09","99","0","1","75408349","<p>You can use <strong>Preserve Hierarchy</strong> in the Copy behavior of copy activity to achieve your requirement.
This takes the files from source directory and preserves the same hierarchy in the target.
As you want to copy to 2 target locations, you need to use 2 copy activities.</p>
<p>I have taken my source File structure as same as yours.
First create a source delimited dataset and give the path only till Root container.</p>
<p>Give this to copy activity and use wild card path in it like below to get your all files.</p>
<p><img src=""https://i.imgur.com/ym4Uzlg.png"" alt=""enter image description here"" /></p>
<p>For sink, create another delimited dataset. Here for giving two target container names, use dataset parameter like below.</p>
<p><img src=""https://i.imgur.com/XkrEzNa.png"" alt=""enter image description here"" /></p>
<p>In copy activity sink, give your first target container name and select <strong>Preserve Hierarchy</strong>.</p>
<p><img src=""https://i.imgur.com/0aNrPgu.png"" alt=""enter image description here"" /></p>
<p>Create another copy activity like above. You can use Clone option for this.</p>
<p>In this, give second target container name.</p>
<p><img src=""https://i.imgur.com/iJ9trF4.png"" alt=""enter image description here"" /></p>
<p>Execute this and you can get the desired result.</p>
<p><strong>Target 1:</strong></p>
<p><img src=""https://i.imgur.com/f15Nngm.png"" alt=""enter image description here"" /></p>
<p><strong>Target2:</strong></p>
<p><img src=""https://i.imgur.com/F9h2BiU.png"" alt=""enter image description here"" /></p>
"
"75396074","Azure data-factory can't load data successfully through PolyBase if the source data in the last column of the first row is null","<p>I am try using Azure DataFactory to load data from Azure Blob Storage to Azure Data warehouse
The relevant data is like below:</p>
<p>source csv:</p>
<pre><code>1,james,
2,john,usa
</code></pre>
<p>sink table:</p>
<pre><code>CREATE TABLE test_null (
    id int NOT NULL,
    name nvarchar(128)  NULL,
    address nvarchar(128)  NULL
)
</code></pre>
<p>source dataset:</p>
<pre><code>{
    &quot;name&quot;: &quot;test_null_input&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;StagingBlobStorage&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;DelimitedText&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobStorageLocation&quot;,
                &quot;fileName&quot;: &quot;1.csv&quot;,
                &quot;folderPath&quot;: &quot;test_null&quot;,
                &quot;container&quot;: &quot;adf&quot;
            },
            &quot;columnDelimiter&quot;: &quot;,&quot;,
            &quot;escapeChar&quot;: &quot;&quot;,
            &quot;firstRowAsHeader&quot;: false,
            &quot;quoteChar&quot;: &quot;&quot;
        },
        &quot;schema&quot;: []
    }
}
</code></pre>
<p>sink dataset:</p>
<pre><code>{
    &quot;name&quot;: &quot;test_null_output&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;StagingAzureSqlDW&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureSqlDWTable&quot;,
        &quot;schema&quot;: [
            {
                &quot;name&quot;: &quot;id&quot;,
                &quot;type&quot;: &quot;int&quot;,
                &quot;precision&quot;: 10
            },
            {
                &quot;name&quot;: &quot;name&quot;,
                &quot;type&quot;: &quot;nvarchar&quot;
            },
            {
                &quot;name&quot;: &quot;address&quot;,
                &quot;type&quot;: &quot;nvarchar&quot;
            }
        ],
        &quot;typeProperties&quot;: {
            &quot;schema&quot;: &quot;dbo&quot;,
            &quot;table&quot;: &quot;test_null&quot;
        }
    }
}
</code></pre>
<p>pipeline</p>
<pre><code>{
    &quot;name&quot;: &quot;test_input&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;SqlDWSink&quot;,
                        &quot;allowPolyBase&quot;: true,
                        &quot;polyBaseSettings&quot;: {
                            &quot;rejectValue&quot;: 0,
                            &quot;rejectType&quot;: &quot;value&quot;,
                            &quot;useTypeDefault&quot;: false,
                            &quot;treatEmptyAsNull&quot;: true
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;mappings&quot;: [
                            {
                                &quot;source&quot;: {
                                    &quot;ordinal&quot;: 1
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;id&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;ordinal&quot;: 2
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;name&quot;
                                }
                            },
                            {
                                &quot;source&quot;: {
                                    &quot;ordinal&quot;: 3
                                },
                                &quot;sink&quot;: {
                                    &quot;name&quot;: &quot;address&quot;
                                }
                            }
                        ]
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;test_null_input&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;test_null_output&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>The last column for the first row is null so when run the pipeline it pops out the below error:</p>
<pre><code>ErrorCode=UserErrorInvalidColumnMappingColumnNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Invalid column mapping provided to copy activity: '{&quot;Prop_0&quot;:&quot;id&quot;,&quot;Prop_1&quot;:&quot;name&quot;,&quot;Prop_2&quot;:&quot;address&quot;}', Detailed message: Column 'Prop_2' defined in column mapping cannot be found in Source structure.. Check column mapping in table definition.,Source=Microsoft.DataTransfer.Common,'
</code></pre>
<p>Tried set the treatEmptyAsNull to true, still the same error. Tried set skipLineCount to 1, it can work well, seems the last column null data in the first row affects the loading of the entire file. But the weirder thing is that it  can also work well by enable staging even without setting treatEmptyAsNull and skipLineCount. In my scenario, it is unnecessary to enable it, since it is originally from blob to data warehouse. It seems unreasonable to change from blob to blob and then from blob to data warehouse after enabling, and it will bring additional data movement charges after enabling. I don't know why setting treatEmptyAsNull doesn't work, and then why enabling staging can work，this seems to make no sense？</p>
","<azure><azure-data-factory><polybase>","2023-02-09 08:38:52","122","0","1","75397374","<p>I have reproduced the above with your Pipeline JSON and got same error.</p>
<p><img src=""https://i.imgur.com/e0SRtTG.png"" alt=""enter image description here"" /></p>
<p>This error occurred because as per your JSON, this is your copy data mapping between source and sink.</p>
<p><img src=""https://i.imgur.com/8x11UW3.png"" alt=""enter image description here"" /></p>
<p>As per the above mapping you should have <code>Prop_0</code>, <code>Prop_1</code> and <code>Prop_2</code> as headers.
Here, as you didn't check the <strong>First Row as header</strong> in your source file, it is taking <code>Prop_0</code>, <code>Prop_1</code> as headers. Since there is a null value in your first Row there is no <code>Prop_2</code> column and that is the reason it is giving the error for that column.</p>
<p>To resolve it, Give a proper header your file in csv like below.</p>
<p><img src=""https://i.imgur.com/iWJhOcG.png"" alt=""enter image description here"" /></p>
<p>Then check the First Row as header in the source dataset.
It will give the mapping like below when you import.</p>
<p><img src=""https://i.imgur.com/xzsOOFy.png"" alt=""enter image description here"" /></p>
<p>Now, it will Execute successfully as mine.</p>
<p><strong>Result:</strong></p>
<p>You can see that the empty value taken as <code>NULL</code> in target table.</p>
<p><img src=""https://i.imgur.com/4f0cOmV.png"" alt=""enter image description here"" /></p>
"
"75396057","Azure data factory parsing xml in copy activity","<p>I am using azure data factory to have a soap API connection data to be transferred to snowflake. I understand that snowflake has to have the data in variant column or csv or we need to have intermediate storage in azure to finally land the data in snowflake. the problem I faced is the data from api is a string within that there is xml data. so when i put the data in blob storage, its a string. how do I avoid this and have the proper columns while putting the data ?</p>
<p><a href=""https://i.stack.imgur.com/jwmIv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jwmIv.png"" alt=""mapping in azure data factory copy activity"" /></a></p>
<p><a href=""https://i.stack.imgur.com/gnsmc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gnsmc.png"" alt=""The data in blob storage is a string"" /></a>
over here, the column is read as string. is there a way to parse it into their respective rows ? I tried to put the collection reference, it still does not recognize individual columns. Any input is highly appreciated.</p>
<p><a href=""https://i.stack.imgur.com/RMfMi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RMfMi.png"" alt=""enter image description here"" /></a></p>
","<string><azure><parsing><xml-parsing><azure-data-factory>","2023-02-09 08:36:52","156","0","1","75397929","<p>You need to change to Advanced editor in Mapping section of copy activity. I took the sample data and repro'd this. Below are the steps.</p>
<p><img src=""https://i.imgur.com/RE5wmcp.png"" alt=""enter image description here"" />
Img:1 Source dataset preview</p>
<p>In mapping section of copy activity,</p>
<ul>
<li>Click <strong>Import Schema</strong></li>
<li>Switch to <strong>Advanced editor</strong> .</li>
<li>Give the collection reference value.</li>
</ul>
<p><img src=""https://i.imgur.com/0045q6F.png"" alt=""enter image description here"" />
Img:2 Mapping settings</p>
"
"75395951","How do i get only the Json keys as a output from ADF lookup activity","<p>This is my json which read via a lookup activity</p>
<pre><code>{
 &quot;key1&quot; : { &quot;id&quot; = &quot;100&quot; },
 &quot;key2&quot; : &quot;XYZ&quot;,
 &quot;key3&quot; : [1,2,3]
}
</code></pre>
<p>I need a activity that gives me all the keys alone  from above json</p>
<pre><code>Lookup.output.firstrow.key2 gives me the string XYZ
</code></pre>
<p>What expression i can use to get all the keys alone</p>
<p>I really looking for some expression like  Lookup.output.firstrow.getKeys() which returns array of keys such as
[&quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;]</p>
","<azure><azure-data-factory>","2023-02-09 08:26:48","191","0","1","75398732","<blockquote>
<p>How do i get only the Json keys as a output from ADF lookup activity</p>
</blockquote>
<p>There is no such direct way to achieve this you have to do it by setting the variables and string manipulation.</p>
<p>Follow below procedure:</p>
<ol>
<li><p>I took json file in look up and its output is as follow:
<img src=""https://i.imgur.com/r3EJyNa.png"" alt=""enter image description here"" /></p>
</li>
<li><p>To get all keys from above Json first I took set variable activity and created a <strong>demo</strong> <code>string</code> variable with value.</p>
</li>
</ol>
<pre><code>@substring(string(activity('Lookup1').output.value),2,sub(length(string(activity('Lookup1').output.value)),4))
</code></pre>
<p>here we are converting lookup output to string and removing braces from start and end.</p>
<p><img src=""https://i.imgur.com/lVGvOph.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>Then I took another set variable activity and created a <strong>demo2</strong> <code>array</code> variable with value.</li>
</ol>
<pre><code>@split(substring(string(split(variables('demo'),':')),2,sub(length(string(split(variables('demo'),':'))),4)),',')
</code></pre>
<p>Here we are splitting the string with <code>:</code> and <code>,</code></p>
<p><img src=""https://i.imgur.com/8GP4PuB.png"" alt=""enter image description here"" /></p>
<ol start=""4"">
<li><p>Then I created an array with default range of even numbers of values 0,2,4,6 etc.
<img src=""https://i.imgur.com/7dZb5oF.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then Passed it to ForEach activity
<img src=""https://i.imgur.com/kk1J453.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then Inside For Each activity I took append variable activity and gave value as</p>
</li>
</ol>
<pre><code>@variables('demo2')[item()]
</code></pre>
<p><img src=""https://i.imgur.com/JHlgBZW.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong>
<img src=""https://i.imgur.com/F3Gunhv.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Note:</strong> if your values contain <code>:</code> or  <code>,</code> the above expression will also split those values. and if we split the values with <code>:</code> then I will split the string with <code>:</code> only and rest thing it will consider as single value. In below image the highlighted value it is taking as single value.
<img src=""https://i.imgur.com/GYXi10F.png"" alt=""enter image description here"" /></p>
</blockquote>
"
"75389847","Accessing Parameters of Data Flowlet in Master Dataflow in Azure Data Factory","<p>I have a dataflow &quot;MasterDF&quot; in which am using a DataFlowlet &quot;Flowlet1&quot; .
Iam using a source in the dataflowlet by passing parameterised dataset .</p>
<p>Now when i integrate that flowlet into my MasterDF and create a new pipeline &gt; DataFlowActivity ( pointing MasterDF)
I cannot see parameters to pass to my data set .</p>
<p>How to solve this  <a href=""https://i.stack.imgur.com/dFRe4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dFRe4.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/0krZD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0krZD.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/XPPnA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XPPnA.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-02-08 17:52:35","98","0","1","75394363","<p>You cannot see the flowlet parameter in the pipeline. You need to create a dataflow parameter and pass as an input to the flowlet parameter. In the ADF pipeline, you will get the options to pass input for dataflow parameter. I repro'd this with sample dataflow and pipeline. Below is the approach.</p>
<ul>
<li><p>Sample Flowlet is created with parameter <code>FL_par</code>. This flowlet is to add a table_name as a new column.
<img src=""https://i.imgur.com/cYl2hGS.png"" alt=""enter image description here"" /></p>
</li>
<li><p>New Dataflow is created with parameter <code>DF_par</code>.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/fOLWloa.png"" alt=""enter image description here"" /></p>
<ul>
<li>Flowlet is added in this dataflow. Dataflow parameter is passed as input to flowlet parameter.</li>
</ul>
<p><img src=""https://i.imgur.com/8QdqAtK.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then in pipeline, input is given to dataflow parameter.</li>
</ul>
<p><img src=""https://i.imgur.com/zAjyBW1.png"" alt=""enter image description here"" /></p>
<p>Thus, you can pass input to flowlet parameter through dataflow parameter.</p>
"
"75388256","Azure Data Factory DataFlow error : Temporary failure in name resolution","<p>We are getting error in Azure Data Factory Data Flow using Azure Hosted Integration runtime and Private Link linked to Private Link Service and the target is SQL Server behind load balancers.</p>
<blockquote>
<p>Job failed due to reason: java.lang.Throwable: DataFlowManagerClient.getPayload fail to getPayload request for run:90e9cb52-f2ba-48fb-ba9f-2a5de66454ab, exception:java.net.UnknownHostException: dmeus2aksig1aks.svc.datafactory.azure.com: Temporary failure in name resolution</p>
</blockquote>
<p>What could cause this?</p>
","<azure-data-factory>","2023-02-08 15:45:20","174","0","1","75673404","<p>I'm experiencing the same issue where my pipeline is failing intermittently at any step of the flow or pipeline. This happens randomly and is not related to the timeout I've set. I haven't made any changes to the flow or pipelines, and I'm seeing this issue occur in multiple pipelines daily, although sometimes it happens in just one. The only solution I've found is to re-run the pipeline, which resolves the issue temporarily. I'm looking for advice on how to troubleshoot and fix this issue permanently.</p>
"
"75386690","Azure Data Factory fails with UPSERT for every table with a TIMESTAMP column","<p>my azure data factory throws the error &quot;Cannot update a timestamp column&quot; for every table with a <code>TIMESTAMP</code> column.</p>
<blockquote>
<p>ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed. Please search error to get more details.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=Cannot update a timestamp column.,Source=.Net SqlClient Data Provider,SqlErrorNumber=272,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=272,State=1,Message=Cannot update a timestamp column.,},],'</p>
</blockquote>
<p><strong>I do not want to update the column</strong> itself. But even when I delete it from column mapping, it crashes. Here it is not yet deleted:</p>
<p><a href=""https://i.stack.imgur.com/0soCX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0soCX.png"" alt=""enter image description here"" /></a></p>
<p>I get that <code>TIMESTAMP</code> is not a simple datetime and is updated automatically whenever a another column in that row is updated.</p>
<blockquote>
<p>The timestamp data type is just an incrementing number and does not preserve a date or a time.</p>
</blockquote>
<p>But how do I solve this problem?</p>
","<azure-data-factory><upsert><sql-timestamp>","2023-02-08 13:44:35","182","0","1","75399747","<p>I tried to reproduce the issue, and on my ADF, if I remove the timestamp column from mapping the pipeline run with no errors.</p>
<p>But since this doesn't work for you, here are 2 workaround options:</p>
<p>Option 1 - on the source, use  a query and remove the timestamp column from the query.</p>
<p>Option 2 - I tried to reproduce your error, and found out that it only happens on upsert. If I use insert, it runs with no error (though it ignore the insert on the timestamp column and increment the timestamp). So you can try to insert to a staging table and then update in sql only the columns you want.</p>
"
"75386148","Azure Data Factory - connect to onprem accdb","<p>I need to retrieve data from Access onprem database by using adf. I have integration runtime installed on azure vm. Access database is on the other virtual machine. I know that on the azure virtual machine I should have a path to that Access database but how to do that?</p>
","<ms-access><azure-virtual-machine><azure-data-factory><on-premises-instances>","2023-02-08 12:57:47","91","0","1","75395688","<blockquote>
<p>I know that on the azure virtual machine I should have a path to that Access database but how to do that?</p>
</blockquote>
<ul>
<li>To connect on-premises Access database with Data factory <strong>you need to install SHIR on that particular Virtual Machine</strong>. Only for port 443 outbound traffic. This may be open by default.</li>
<li>The Integration Runtime Machine needs to have the Microsoft Access ODBC driver installed for the data storage.</li>
</ul>
<blockquote>
<p>Microsoft Access 2016 version of ODBC driver doesn't work with this connector. Use Microsoft Access 2013 or 2010 version of ODBC driver instead.</p>
</blockquote>
<ul>
<li>In linked service select that particular SHIR hosted on that particular Virtual Machine.</li>
<li>Provide correct values to the linked service properties like <code>connectionString</code>, <code>authenticationType</code>.</li>
</ul>
<p><strong>Reference</strong>- <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-microsoft-access"" rel=""nofollow noreferrer"">Copy data from and to Microsoft Access using Azure Data Factory</a> follow this official document.</p>
"
"75379282","Keep a field from updating in Data Factory","<p>I'm writing a Data Flow in which I make an upsert to a Cosmos DB NoSQL database. My goal is to use a field called <code>batch</code> to keep track of the date of insertion of a particular document, that is, I want this field to not change over an update. I see that the <code>AlterRow</code> action doesn't let me choose which fields to partially upsert or not. Is there a way to accomplish this?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2023-02-07 21:40:58","63","0","2","75382053","<p>I think if you use UPSERT , then it will update all the fields , but UPDATE should allow you to do so .</p>
<p><a href=""https://i.stack.imgur.com/S6HwY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S6HwY.png"" alt=""enter image description here"" /></a></p>
"
"75379282","Keep a field from updating in Data Factory","<p>I'm writing a Data Flow in which I make an upsert to a Cosmos DB NoSQL database. My goal is to use a field called <code>batch</code> to keep track of the date of insertion of a particular document, that is, I want this field to not change over an update. I see that the <code>AlterRow</code> action doesn't let me choose which fields to partially upsert or not. Is there a way to accomplish this?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2023-02-07 21:40:58","63","0","2","75382238","<p>Agree with HimanshuSinha-msft if you select <code>Upsert if</code> it will update all the columns Instead select <code>Update if</code> with condition</p>
<pre class=""lang-json prettyprint-override""><code>equals(source1@id,source2@id)
</code></pre>
<p><img src=""https://i.imgur.com/W3204l2.png"" alt=""enter image description here"" /></p>
<p>And in sink setting select <strong>Update method as <code>Allow update</code></strong> and pass partition key.</p>
<p><img src=""https://i.imgur.com/nHxkXV6.png"" alt=""enter image description here"" /></p>
<p>In mapping only select columns you want to update</p>
<blockquote>
<p>Mapping &gt;&gt; Uncheck Auto mapping &gt;&gt; select columns you want to update</p>
</blockquote>
<p><img src=""https://i.imgur.com/dAYuVCG.png"" alt=""enter image description here"" /></p>
"
"75378954","Azure copy data activity","<p>assume that I'm trying to copy 1000 records in a table from a database to an Azure SQL DB/Synapse using ADF Copy activity. if the Copy activity fails after copying 500 records, is it possible to re-run/restart the pipeline such that the Copy activity avoids copying already copied records( 600 records which were copied in the earlier run) and resume copy operation from the remaining 500 records?</p>
<p>Thank you.</p>
<p>n</p>
","<azure-data-factory>","2023-02-07 21:03:44","101","0","1","75381310","<p>Unfortunately based on my understanding, the copy activity would  being from the start and cannot have the scope to copy from where it had failed (in case of bulk copy)</p>
<p>There is a way (though a bad way) wherein you can can use a for each loop with the number of iterations equivalent to the rows and copy each row in an iteration and maintain a watermark feature .
In that way in case if copy failed, it would start from the watermark but this would be a bad way and a bad performance as compared to allowing a full rerun</p>
"
"75378023","facing issues while connecting to HANA","<p>I'm using a copy activity to get data from SAP HANA as source and ADLS as sink and an Azure self-IR, the test connection is ok but when I execute the pipeline I got the error below. Has someone faced a similar error?</p>
<p>I would really appreciate your help to guide me, there's no error reported in Microsoft.</p>
<blockquote>
<p>&quot;Code&quot;: 9603,
&quot;Message&quot;: &quot;ErrorCode=UserErrorFailedToConnectOdbcSource,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [SAP AG][LIBODBCHDB DLL][HDBODBC] General error;-10757 Secure store error: d:\701\w\7hhepnmjzk\src\interfaces\securestore\impl\securestore.cpp:190 - (91003) Invalid secure store key\r\nERROR [HY000] [SAP AG][LIBODBCHDB DLL][HDBODBC] General error;-10757 Secure store error: d:\701\w\7hhepnmjzk\src\interfaces\securestore\impl\securestore.cpp:190 - (91003) Invalid secure store key,Source=Microsoft.DataTransfer.Runtime.SapHanaConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [SAP AG][LIBODBCHDB DLL][HDBODBC] General error;-10757 Secure store error: d:\701\w\7hhepnmjzk\src\interfaces\securestore\impl\securestore.cpp:190 - (91003) Invalid secure store key\r\nERROR [HY000] [SAP AG][LIBODBCHDB DLL][HDBODBC] General error;-10757 Secure store error: d:\701\w\7hhepnmjzk\src\interfaces\securestore\impl\securestore.cpp:190 - (91003) Invalid secure store key,Source=,'&quot;,</p>
</blockquote>
","<azure><odbc><hana><azure-data-factory>","2023-02-07 19:18:48","144","0","1","75385801","<blockquote>
<p>ErrorCode=UserErrorFailedToConnectOdbcSource,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [SAP AG][LIBODBCHDB DLL][HDBODBC] General error;-10757 Secure store error: d:\701\w\7hhepnmjzk\src\interfaces\securestore\impl\securestore.cpp:190 - (91003) Invalid secure store key\r\nERROR [HY000] [SAP AG][LIBODBCHDB DLL][HDBODBC]</p>
</blockquote>
<p>By looking at error message and error code it seems to error is related to the <strong>SAP HANA ODBC driver</strong></p>
<p>To utilize SAP HANA Connector, you have to complete prerequisites mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sap-hana?tabs=data-factory"" rel=""nofollow noreferrer"">Microsoft documentation</a>:</p>
<ul>
<li>Set up a Self-hosted Integration Runtime.</li>
<li>On the Integration Runtime computer, install the <strong>SAP HANA ODBC driver.</strong> to use SAP HANA from computer.</li>
</ul>
<blockquote>
<p>You can download the SAP HANA ODBC driver from the <a href=""https://support.sap.com/swdc"" rel=""nofollow noreferrer"">SAP Software Download Center</a>. Search with the keyword SAP HANA CLIENT for Windows.</p>
</blockquote>
<p>Also see required linked service properties from document.</p>
<p><img src=""https://i.imgur.com/prsHRSS.png"" alt=""enter image description here"" /></p>
<p>Also see this <a href=""https://learn.microsoft.com/en-us/events/sap-on-azure-training-videos/integrating-your-sap-data-by-using-azure-data-factory-adf"" rel=""nofollow noreferrer"">video</a> of ADF and SAP HANA connection for more information.</p>
"
"75373792","Copy .csv file from Azure Blob Storage to Sharepoint site","<p>I have a CSV file stored in blob storage. The goal is to move this file into a Sharepoint site and set some metadata. What would be the best way to do this? The client does not want us to use Power Automate or Logic Apps.</p>
<p>I tried using Azure Data Factory but there seems to be an issue with writing data to SharePoint. I used the copy activity but the 'sink' to SharePoint failed. Does data factory support writing to Sharepoint?</p>
","<c#><sharepoint><azure-blob-storage><azure-data-factory>","2023-02-07 13:05:47","128","0","1","75373917","<blockquote>
<p>The client does not want us to use Power Automate or Logic Apps.</p>
</blockquote>
<p>Why not? This is the simplest way to achieve this, and is also better maintainable than for instance C# code.</p>
<blockquote>
<p>Does data factory support writing to Sharepoint?</p>
</blockquote>
<p>Yes, it does. However, using Data Factory only to copy a file to SharePoint is quite a bit of overkill.</p>
<p>If Logic Apps are not an option, have a look at an <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview"" rel=""nofollow noreferrer"">Azure Function</a> to automatically trigger when the file is created in Azure Storage, and have a look at for instance <a href=""https://www.sharepointpals.com/post/upload-file-to-sharepoint-office-365-programmatically-using-c-csom-pnp/"" rel=""nofollow noreferrer"">Upload File To SharePoint Office 365 Programmatically Using C# CSOM – PNP</a> for a C# way of uploading a file to SharePoint.</p>
"
"75371373","Azure Datafactory Pipeline query","<p>I have an Azure Data Factory requirement. There are 50 csv files and each file is named like Product, Department, Employee, Sales, etc. Each of these files has a unique number of columns. In Azure SQL Database, I have 50 tables like Product, Department, Employee, Sales, etc. The columns of each table match with its corresponding file. Every day, I receive a new set of files in an Azure Data Lake Storage Gen 2 folder at 11 PM CST. At 12:05 AM CST, each of these files should be loaded into its respective table.</p>
<p>There should be only one pipeline or there can be 2 pipelines where the parent pipeline collects the metadata of the file and supplies it to the child pipeline which does the data load. It should find the files with the timestamp of the previous day and then  it should loop through these files and load them into its respective target table, one by one. Can someone briefly explain the Activities and Transformations I need to use to fulfil this requirement.</p>
<p>I am new to ADF. I haven't tried anything so far.</p>
","<azure><azure-data-factory><etl><azure-data-lake-gen2>","2023-02-07 09:33:14","58","0","1","75382245","<blockquote>
<p>Each of these files has a unique number of columns. In Azure SQL Database, I have 50 tables like Product, Department, Employee, Sales, etc. The columns of each table match with its corresponding file.</p>
</blockquote>
<p>As you have same columns for both source and target and same names for files and tables. The below process will work for you if you have same schema for both.</p>
<p>First Use Get Meta data activity for the source folder to get the files list.</p>
<p>To get latest uploaded files, use <code>Filter by last modified</code> option in the Get meta data. This option only supports UTC time format and CST equals UTC-6. Give the <strong>Start time</strong> and <strong>End time</strong> as per your requirement by cross checking both timezones. Use appropriate <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">Date time functions</a> for it.</p>
<p>For sample I have given like below.</p>
<p><img src=""https://i.imgur.com/1wYQ95S.png"" alt=""enter image description here"" /></p>
<p>which will give the result array like this.</p>
<p><img src=""https://i.imgur.com/PDMt7ia.png"" alt=""enter image description here"" /></p>
<p>Give this ChildItems array <code>@activity('Get Metadata1').output.childItems</code> to a ForEach activity. Inside ForEach use a copy activity to copy each file iteration wise.</p>
<p>Create another Source dataset and create a dataset parameter (Here <code>sourcefiename</code>) and give it like below.</p>
<p><img src=""https://i.imgur.com/CqRRLaj.png"" alt=""enter image description here"" /></p>
<p>Give this to copy activity source and assign <code>@item().name</code> for the parameter value.</p>
<p><img src=""https://i.imgur.com/T94X9k0.png"" alt=""enter image description here"" /></p>
<p>In sink, create a Database dataset with two dataset parameters <code>schema</code> and <code>table_name</code>. Use these like below.</p>
<p><img src=""https://i.imgur.com/wj5uBGI.png"" alt=""enter image description here"" /></p>
<p>from <code>@item().name</code> extract the file name other '<code>.csv'</code> text by using split and give that to the above parameter.
<code>@split(item().name,'.c')[0]</code></p>
<p><img src=""https://i.imgur.com/Y5NFFvc.png"" alt=""enter image description here"" /></p>
<p>Now, schedule this pipeline as per your time zone using schedule trigger.</p>
<p><img src=""https://i.imgur.com/BIali4J.png"" alt=""enter image description here"" /></p>
"
"75368873","COUNTIF() equivalent and discharge 'status typing' equivalent in SSMS or ADF","<p>I am currently trying to create a table in SSMS that holds patient record information. Ideally, I can achieve my goals in SSMS alone, but I can also utilize ADF should that be a better tool.</p>
<p>The ultimate goal is outlined in part two, however, to achieve this I have had to do step one first:</p>
<ol>
<li>The number of <code>admissions prior</code> and <code>admissions ahead</code> to any given row for an individual</li>
<li>Discharge 'status typing': the type of discharge from a facility</li>
</ol>
<p>An example table is provided below. Here is a breakdown of each column/variable:</p>
<ul>
<li>Patient ID: ID attached to a patient when they admit to a facility</li>
<li>Patient Master ID: ID attached to an individual that stays with them, even if they admit to more than one facility</li>
<li>Admission: The date when a patient admits to a facility</li>
<li>Discharge: The date when a patient discharges from a facility</li>
<li>Level of Care (LOC): Term referring to the level of care required to adequately address the patient's needs; Higher numbers indicate greater levels of care and more severe symptom severity</li>
<li>Discharge Date</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Patient ID</th>
<th>Patient Master ID</th>
<th>Admission</th>
<th>Discharge</th>
<th>Level of Care (LOC)</th>
<th>Discharge Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234-5</td>
<td>aBcDD-Ecnad9-09as</td>
<td>2022-09-01</td>
<td>2022-10-01</td>
<td>3</td>
<td>2022-10-01</td>
</tr>
<tr>
<td>123-6</td>
<td>aBcDD-Ecnad9-09as</td>
<td>2022-10-01</td>
<td>2022-10-31</td>
<td>2</td>
<td>2022-10-31</td>
</tr>
</tbody>
</table>
</div>
<p>Below is an outline of what I've done in separate programs (excel for step one, and R for step two):</p>
<ol>
<li>Admissions prior and admissions ahead:
I originally did this in excel with a COUNTIF() function in excel, where each row counted the number of <code>Patient Master ID</code> before a given observation/row (<code>admissions prior</code>) and after a given observation/row (<code>admissions ahead</code>)</li>
</ol>
<p>Therefore, the example table would look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Patient ID</th>
<th>Patient Master ID</th>
<th>Admission</th>
<th>Discharge</th>
<th>(LOC)</th>
<th>Admissions Ahead</th>
<th>Admissions Prior</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234-5</td>
<td>aBcDD-Ecnad9-09as</td>
<td>2022-09-01</td>
<td>2022-10-01</td>
<td>3</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>123-6</td>
<td>aBcDD-Ecnad9-09as</td>
<td>2022-10-01</td>
<td>2022-10-31</td>
<td>2</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<ol start=""2"">
<li>I would then load this table into R and create custom functions to accomplish the 2nd goal: The type of discharge from a facility. (Please note that although I am looking to add an <code>Admission Type</code> column in my work, I will leave it out, as it involves similar logic as <code>Discharge Type</code>)</li>
</ol>
<p>A brief description of discharge types:</p>
<ul>
<li>True discharge: When a patient discharges, but does not readmit at another facility (i.e., <code>Admissions Ahead == 0</code>)</li>
<li>Stepdown: When a patient discharges, but admits to a lower level of care (LOC) (i.e., <code>Admissions Ahead == 1 &amp; LOC_new &lt; LOC_old</code></li>
<li>Lateral Transfer: When a patient discharges, but admits to the same level of care (LOC) (i.e., <code>Admissions Ahead == 1 &amp; LOC_new == LOC_old</code></li>
<li>Step-Up: When a patient discharges, but admits to a higher level of care (LOC) (i.e., <code>Admissions Ahead == 1 &amp; LOC_new &gt; LOC_old</code></li>
</ul>
<p>At the end the table would look something like this (note that I have truncated the table by removing <code>Admissions Prior</code>):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Patient ID</th>
<th>Patient Master ID</th>
<th>Admission</th>
<th>Discharge</th>
<th>(LOC)</th>
<th>Admissions Ahead</th>
<th>Discharge Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234-5</td>
<td>aBcDD-Ecnad9-09as</td>
<td>2022-09-01</td>
<td>2022-10-01</td>
<td>3</td>
<td>1</td>
<td>Stepdown</td>
</tr>
<tr>
<td>123-6</td>
<td>aBcDD-Ecnad9-09as</td>
<td>2022-10-01</td>
<td>2022-10-31</td>
<td>2</td>
<td>0</td>
<td>True Discharge</td>
</tr>
</tbody>
</table>
</div>
<p>In practice there will be many rows, and I know I will have to have the table sorted by <code>Admission</code> in order to have these functions work properly. What I'm hoping to get out of this post is to determine if I can do something similar in SSMS (or ADF if needed) to achieve this result in an efficient manner.</p>
<p>One post that's close to achieving my first step is <a href=""https://stackoverflow.com/questions/36269892/excel-countifs-equivalent-in-sql-server"">here</a>. However, if I understand the suggested queries using <code>partition by</code> wouldn't work for counting <code>Admissions Prior</code>.</p>
<p>Thank you for your time.</p>
","<sql-server><azure-data-factory>","2023-02-07 04:06:48","59","0","1","75369216","<p>To count the prior and following admission counts per patient, you can use a windowed <code>COUNT()</code> function constrained with <code>OVER( PARTITION BY ... ORDER BY ... ROWS ... )</code>, where <code>ROWS</code> limits the window to either all preceding or all following admissions.</p>
<pre><code>COUNT(*) OVER(
    PARTITION BY PatientMasterID
    ORDER BY Admission
    ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING
    ) AS AdmissionsBefore,
COUNT(*) OVER(
    PARTITION BY PatientMasterID
    ORDER BY Admission
    ROWS BETWEEN 1 FOLLOWING AND UNBOUNDED FOLLOWING
    ) AS AdmissionsAfter,
</code></pre>
<p>Note that this will count all preceding or following admissions for a patient, not just those with consecutive date ranges (transfers). Achieving the latter gets way more complicated and would involve preprocessing and assigning admission-group numbers that can later be used to limit calculations.</p>
<p>In order to calculate Discharge Type you will need to first extract the Level of Care from the next admission to compare with the current. Although I didn't see it outlined in your requirements, I expect you really only want to consider immediate following admissions, not those weeks, months, or years down the road. To determine that, you will also need to capture the next admission date for comparison with the current discharge date.</p>
<p>Both can be done using the <code>LEAD()</code> function with similar <code>OVER PARTITION BY ... ORDER BY ...)</code> constraints.</p>
<pre><code>LEAD(Admission) OVER(PARTITION BY PatientMasterID ORDER BY Admission) AS NextAdmission,
LEAD(LevelOfCare) OVER(PARTITION BY PatientMasterID ORDER BY Admission) AS NextLevelOfCare
</code></pre>
<p>If you then wrap all that up in a subselect, the outer select can reference the available information in a <code>CASE</code> expression to select the appropriate discharge type.</p>
<p>I've pulled all this together with a few more rows of sample data in <a href=""https://dbfiddle.uk/n0RfO-v4"" rel=""nofollow noreferrer"">this db&lt;&gt;fiddle</a>.</p>
<p>I'm not sure what the difference between Discharge and Discharge Date was in your sample data, so I ignored the latter.</p>
<p>And as for calculating the Admission Type, I expect you will need to access the <strong>prior</strong> LOC and perhaps Discharge date, which can be done using the <code>LAG()</code> function in a manner similar to <code>LEAD()</code>. (Left as an exercise.)</p>
"
"75367183","Is there a way to load a .DAT file in the MS SQL Sever table using ADF?","<p>I have a .DAT file in a ftp server. I have to load the data in a SQL table. I am trying to create datasets in ADF using this .DAT file using ftp as source and ADLS as sink. However it is not getting created. Please suggest how can this be done or if there are any alternatives</p>
<p>I tried loading the .DAT file to ADLS using file format as binary but I am unable to look at data.</p>
<p>If I use file format as csv then I cannot use comma, space, tab, blank space, etc. as delimiter and ADF requires me to enter a delimiter.</p>
","<azure-data-factory>","2023-02-06 22:21:34","90","1","1","75372657","<p>I have taken simple <code>.dat</code> file with month data and tried to transfer it into SQL server table followed below procedure:</p>
<p>Dataset setting:
<img src=""https://i.imgur.com/AneJnV9.png"" alt=""enter image description here"" /></p>
<p>Pipeline source settings:
<img src=""https://i.imgur.com/uRHNbAz.png"" alt=""enter image description here"" /></p>
<p>Successful pipeline run:
<img src=""https://i.imgur.com/sQjG0Eg.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>I tried loading the .DAT file to ADLS using file format as binary but I am unable to look at data.</p>
</blockquote>
<p>You cannot transfer data by using Binary as source if you took binary as source your sink also should be binary format and SQL server table is not a binary format.</p>
"
"75367178","Data factory- SQL query of historical data with periods of one minute","<p>I'm trying to make a copy activity to copy data from an on-premise SQL database with data factory. I need the to modify dynamiccaly the query, so it takes a range of dates, for example ranges of every minute:
So for example: the first copy would be using dynamical content as
...
AND DateTime &gt; '20230131 13:06'
AND DateTime &lt; '20230131 13:07'</p>
<p>and then, the next run would be:
AND DateTime &gt; '20230131 13:07'
AND DateTime &lt; '20230131 13:08'</p>
<p>I tried to use variables like:
AND DateTime &gt; @{variables('starttime')}
AND DateTime &lt; @{variables('endtime')}</p>
<p>It works when I give the times manually to variables but the actual data is for a year, so I need to read the start and end times from a file or somehow automate it. I tried to use &quot;ForEach&quot; block, but the problem is that I can only set one variable as a loop, either start or end time.
what is the best solution for this?
How can I use &quot;ForEach&quot; block on start time and enter the end time in a  way that is one minute after the start time?</p>
","<sql><azure-data-factory>","2023-02-06 22:20:30","88","0","2","75368987","<p>You can read the start time file, loop through it and add a minute to each time in for loop.</p>
<ul>
<li>The following is the output of my look up of csv file containing start_times.</li>
</ul>
<p><img src=""https://i.imgur.com/sSpzR0X.png"" alt=""enter image description here"" /></p>
<ul>
<li>Loop through this data. Inside for each I have 2 set variable activities, one for converting start time to a valid format so minute can be added using <code>addMinutes()</code> function.</li>
</ul>
<pre><code>@concat(substring(item().start_time,0,4),'-',substring(item().start_time,4,2),'-',substring(item().start_time,6,2),' ',substring(item().start_time,9,5))
</code></pre>
<p><img src=""https://i.imgur.com/TFa9igj.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, for this format, add minutes and replace the <code>-</code> value concatenated above.</li>
</ul>
<pre><code>@replace(addminutes(variables('tp'),1,'yyyy-MM-dd hh:mm'),'-','')
</code></pre>
<p><img src=""https://i.imgur.com/1Uup7TC.png"" alt=""enter image description here"" /></p>
"
"75367178","Data factory- SQL query of historical data with periods of one minute","<p>I'm trying to make a copy activity to copy data from an on-premise SQL database with data factory. I need the to modify dynamiccaly the query, so it takes a range of dates, for example ranges of every minute:
So for example: the first copy would be using dynamical content as
...
AND DateTime &gt; '20230131 13:06'
AND DateTime &lt; '20230131 13:07'</p>
<p>and then, the next run would be:
AND DateTime &gt; '20230131 13:07'
AND DateTime &lt; '20230131 13:08'</p>
<p>I tried to use variables like:
AND DateTime &gt; @{variables('starttime')}
AND DateTime &lt; @{variables('endtime')}</p>
<p>It works when I give the times manually to variables but the actual data is for a year, so I need to read the start and end times from a file or somehow automate it. I tried to use &quot;ForEach&quot; block, but the problem is that I can only set one variable as a loop, either start or end time.
what is the best solution for this?
How can I use &quot;ForEach&quot; block on start time and enter the end time in a  way that is one minute after the start time?</p>
","<sql><azure-data-factory>","2023-02-06 22:20:30","88","0","2","75370856","<p>I'm not sure how this works in Data Factory but in t-sql, you can increase your variables by one minute with the DATEADD function.    <a href=""https://dbfiddle.uk/9gTfTIdM"" rel=""nofollow noreferrer"">Fiddle</a></p>
<pre><code>DATEADD(*interval*, *number of intervals*, *start datetime*)
</code></pre>
<p>I checked Books Online and it looks like data factory uses datetime_add. It appears to work the same way though.</p>
<pre><code>DATETIME_ADD(*interval*, *number of intervals*, *start datetime*)
</code></pre>
<p>I hope this helps.</p>
<pre><code>DECLARE @startdate  datetime = '20230131 13:06'
      , @enddate    datetime
      , @finishdate datetime;
    SET @enddate    = DATEADD(minute, 1, @startdate);
    SET @finishdate = DATEADD(minute, 100, @startdate); --will loop 100 times. 
                                                        --change that to whatever you want.

  WHILE @startdate &lt; @finishdate
    BEGIN
      SELECT data 
        INTO newTable
        FROM oldTable
       WHERE oldTableDate &gt;= @startdate 
         AND oldTableDate &lt;= @enddate;

      SELECT @startdate = DATEADD(minute, 1, @startdate),
             @enddate = DATEADD(minute, 1, @enddate)
     END
</code></pre>
"
"75366991","Copy CSV file data from blob storage to Azure SQL database based on the number of columns in the file","<p>I have data files landing in a single Azure blob storage container every other day or so. These files have either 8, 16, 24, or 32 columns of data. Each column has a unique name within a file, and the names are consistent across files. I.e the column names in the 8-column file will always be the first 8 column names of the 16, 24, and 32 column files. I have the appropriate 4 tables in an Azure SQL database set up to receive the files. I need to create a pipeline(s) in Azure Data Factory that will</p>
<ol>
<li>trigger upon the landing of a new file in the blob storage container</li>
<li>check the # of columns in that file</li>
<li>use the number of columns to copy the file from the blob into the appropriate Azure SQL database table. Meaning the 8 column blob file copies to the 8 column SQL table and so on.</li>
<li>delete the file</li>
</ol>
<p>I've researched the various pieces to complete this but cannot seem to put them together. Schema drift solution got me close but parameterization of the file names lost me. Multiple pipelines to achieve this is okay, as long as the single storage container is maintained. Thanks</p>
","<eventtrigger><azure-data-factory><dynamic-columns>","2023-02-06 21:56:51","221","0","2","75368845","<ol>
<li>Use Blob trigger to trigger upon the landing of a new file in the blob storage container</li>
<li>use get meta data activity to get the # of column in file details</li>
<li>use a switch activity based on number of columns and based on that have copy activity and delete activity within the switch counterparts to copy the data and also delete the file</li>
</ol>
"
"75366991","Copy CSV file data from blob storage to Azure SQL database based on the number of columns in the file","<p>I have data files landing in a single Azure blob storage container every other day or so. These files have either 8, 16, 24, or 32 columns of data. Each column has a unique name within a file, and the names are consistent across files. I.e the column names in the 8-column file will always be the first 8 column names of the 16, 24, and 32 column files. I have the appropriate 4 tables in an Azure SQL database set up to receive the files. I need to create a pipeline(s) in Azure Data Factory that will</p>
<ol>
<li>trigger upon the landing of a new file in the blob storage container</li>
<li>check the # of columns in that file</li>
<li>use the number of columns to copy the file from the blob into the appropriate Azure SQL database table. Meaning the 8 column blob file copies to the 8 column SQL table and so on.</li>
<li>delete the file</li>
</ol>
<p>I've researched the various pieces to complete this but cannot seem to put them together. Schema drift solution got me close but parameterization of the file names lost me. Multiple pipelines to achieve this is okay, as long as the single storage container is maintained. Thanks</p>
","<eventtrigger><azure-data-factory><dynamic-columns>","2023-02-06 21:56:51","221","0","2","75372778","<p>I agree with <strong>@Nandan</strong>'s approach.  Also, you can try the below alternative using look up and filter if you want to avoid creating Switch cases.</p>
<p>For this approach, you should not have any other tables in your target database other than the above.</p>
<p>First create pipeline parameter and a Storage event trigger. Give trigger parameter<code>@triggerBody().fileName</code> to the pipeline parameter.</p>
<p><img src=""https://i.imgur.com/rw8umB1.png"" alt=""enter image description here"" /></p>
<p>Now, use lookup activity query to get the list of table schema, table name and column count as an array of objects.</p>
<pre><code>SELECT TABLE_SCHEMA
    , TABLE_NAME
    , number = COUNT(*) 
FROM INFORMATION_SCHEMA.COLUMNS 
where TABLE_NAME!='database_firewall_rules'
GROUP BY TABLE_SCHEMA, TABLE_NAME;
</code></pre>
<p><img src=""https://i.imgur.com/LZhYqC1.png"" alt=""enter image description here"" /></p>
<p>This will give the JSON array like this.</p>
<p><img src=""https://i.imgur.com/GDS5oCG.png"" alt=""enter image description here"" /></p>
<p>Next, use Get Meta activity by giving the triggered file name with dataset parameters and get the column count from it.</p>
<p>Now, use Filter activity to filter the correct SQL table which has same column count as our triggered file.</p>
<p>items: <code>@activity('Lookup1').output.value</code>
Condition: <code>@equals(activity('Get Metadata1').output.columnCount, item().number)</code></p>
<p><img src=""https://i.imgur.com/X9KEILi.png"" alt=""enter image description here"" /></p>
<p><strong>Filter output:</strong></p>
<p><img src=""https://i.imgur.com/bJ5p3Cm.png"" alt=""enter image description here"" /></p>
<p>Now, use copy activity with dataset parameters.</p>
<p><strong>Source with dataset parameters:</strong></p>
<p><img src=""https://i.imgur.com/AOPtDKu.png"" alt=""enter image description here"" /></p>
<p><strong>Sink:</strong></p>
<p><img src=""https://i.imgur.com/zNCNPRM.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/WyoYbHa.png"" alt=""enter image description here"" /></p>
<p>Then use delete activity for the triggered file.</p>
<p><strong>My pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline5_copy1&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Lookup1&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                    &quot;sqlReaderQuery&quot;: &quot;SELECT TABLE_SCHEMA\n    , TABLE_NAME\n    , number = COUNT(*) \nFROM INFORMATION_SCHEMA.COLUMNS \nwhere TABLE_NAME!='database_firewall_rules'\nGROUP BY TABLE_SCHEMA, TABLE_NAME;&quot;,
                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                    &quot;partitionOption&quot;: &quot;None&quot;
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Dataset_for_column_count&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;Filter1&quot;,
            &quot;type&quot;: &quot;Filter&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Get Metadata1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Lookup1').output.value&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;condition&quot;: {
                    &quot;value&quot;: &quot;@equals(activity('Get Metadata1').output.columnCount, item().number)&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Get Metadata1&quot;,
            &quot;type&quot;: &quot;GetMetadata&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Sourcefile&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;sourcefilename&quot;: {
                            &quot;value&quot;: &quot;@pipeline().parameters.tfilename&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                },
                &quot;fieldList&quot;: [
                    &quot;columnCount&quot;
                ],
                &quot;storeSettings&quot;: {
                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                    &quot;enablePartitionDiscovery&quot;: false
                },
                &quot;formatSettings&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Copy data1&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Filter1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSink&quot;,
                    &quot;writeBehavior&quot;: &quot;insert&quot;,
                    &quot;sqlWriterUseTableLock&quot;: false
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;Sourcefile&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;sourcefilename&quot;: {
                            &quot;value&quot;: &quot;@pipeline().parameters.tfilename&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;schema&quot;: {
                            &quot;value&quot;: &quot;@activity('Filter1').output.Value[0].TABLE_SCHEMA&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        },
                        &quot;table_name&quot;: {
                            &quot;value&quot;: &quot;@activity('Filter1').output.Value[0].TABLE_NAME&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                }
            ]
        },
        {
            &quot;name&quot;: &quot;Delete1&quot;,
            &quot;type&quot;: &quot;Delete&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Copy data1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Sourcefile&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;sourcefilename&quot;: {
                            &quot;value&quot;: &quot;@pipeline().parameters.tfilename&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                },
                &quot;enableLogging&quot;: false,
                &quot;storeSettings&quot;: {
                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                    &quot;recursive&quot;: true,
                    &quot;enablePartitionDiscovery&quot;: false
                }
            }
        }
    ],
    &quot;parameters&quot;: {
        &quot;tfilename&quot;: {
            &quot;type&quot;: &quot;string&quot;
        }
    },
    &quot;variables&quot;: {
        &quot;sample&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/Tz96xyX.png"" alt=""enter image description here"" /></p>
"
"75364584","Using managed identities for HTTP linked service","<p>I am working on creating a flow where I get JSON data from a rest api authenticating with the managed identity of my ADF instance and copy the data to a Kusto cluster. To do this, I am following the instructions here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http?tabs=data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-http?tabs=data-factory</a></p>
<p>However, when I go to create a linked service I do not see any way to authenticate against the API using my managed identity: <a href=""https://i.stack.imgur.com/C9G4R.jpg"" rel=""nofollow noreferrer"">http linked service auth options</a></p>
<p>I was expecting something like the options given in the Web task in ADF, where I am allowed to selected managed identities:<a href=""https://i.stack.imgur.com/ekGzt.jpg"" rel=""nofollow noreferrer"">web task auth options</a></p>
","<azure-data-factory><azure-data-explorer>","2023-02-06 17:23:37","62","0","1","75368954","<p>HTTP connector supports only <strong>Anonymous</strong>, <strong>Basic</strong>, <strong>Digest</strong>, <strong>Windows</strong>, and <strong>ClientCertificate</strong> as authentication types. To use Managed Identity authentication type, you can use <strong>Rest</strong> linked service.</p>
<ul>
<li><p>Search for REST in the available list of linked service and select <strong>REST</strong> connector.
<img src=""https://i.imgur.com/G2H2JsK.png"" alt=""enter image description here"" /></p>
</li>
<li><p>You can select Managed Identity as auth type.
<img src=""https://i.imgur.com/7Znr8Id.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest?tabs=data-factory#linked-service-properties"" rel=""nofollow noreferrer"">MS doc</a> on Data Fcatory - REST connector.</p>
"
"75360675","Azure - Get Log Analtycis query trough Data Factory","<p>I'm trying to create a web activity in ADF to run a query in log analytics and have this result as output in ADF (Data Factory).</p>
<p>Is it possible ?</p>
<p>I tried to generate a ADD token and passed it to web Activity:
<a href=""https://i.stack.imgur.com/UTRtN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UTRtN.png"" alt=""enter image description here"" /></a></p>
<p>and the url that I'm using is: <a href=""https://api.loganalytics.io/v1/workspaces/%5BWorkspace"" rel=""nofollow noreferrer"">https://api.loganalytics.io/v1/workspaces/[Workspace</a> ID]/query</p>
<p>but I got an error:</p>
<p><a href=""https://i.stack.imgur.com/GyECg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GyECg.png"" alt=""enter image description here"" /></a></p>
<p>I already gave access to app like reader and data access to Log Analytics.</p>
<p>Can anyone please help me in achieving this?</p>
<p>Thank you!</p>
","<azure><azure-log-analytics><azure-data-factory>","2023-02-06 11:24:49","102","0","1","75381919","<p>I think you have to get the bearer token and then pass that bearer token when you make the actual call to log analytics API , its going to be two step workflow . I use to test the thing in curl or postman and once it succeed there I try to implement in ADF .</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/logs/api/access-api#client-credentials-flow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-monitor/logs/api/access-api#client-credentials-flow</a></p>
"
"75360563","Using Azure Data Factory to import a column of type NUMBER in oracle results in a strange precision error","<p>We're going from ORACLE to SQL in azure.</p>
<p>AFAIK we have to use pipelines and data sets, with a variety of COPY operations.</p>
<p>There does not seem to be a way to import the data from Oracle and manipulate it via Data Flows without putting it into a staging database first, and even then it would be too late for this issue.</p>
<p>The issue is that a column of type NUMBER in oracle might have a value of 1.1234 or 2.23423485</p>
<p>I set the SQL data type to DECIMAL(12, 8) which should cover all the scenarios with a COPY TABLE operation.</p>
<p>I've tried doing the copy as number, and even as varchar:</p>
<pre><code>              {
                &quot;source&quot;: {
                  &quot;name&quot;: &quot;MYDECIMALVALUE&quot;,
                  &quot;type&quot;: &quot;String&quot;
                },
                &quot;sink&quot;: {
                  &quot;name&quot;: &quot;MyDecimalValue&quot;,
                  &quot;type&quot;: &quot;String&quot;,
                  &quot;physicalType&quot;: &quot;varchar&quot;
                }
              },
</code></pre>
<p>However the result for the above two numbers would be:</p>
<blockquote>
<p>2.23423485 stays as 2.23423485</p>
</blockquote>
<blockquote>
<p>1.1234 becomes 1.12340001</p>
</blockquote>
<p>Some strange precision issues pulling NUMBER out of oracle.</p>
<p>The same happens with the config above set to</p>
<pre><code>              {
                &quot;source&quot;: {
                  &quot;name&quot;: &quot;MYDECIMALVALUE&quot;,
                  &quot;type&quot;: &quot;Decimal&quot;
                },
                &quot;sink&quot;: {
                  &quot;name&quot;: &quot;MyDecimalValue&quot;,
                  &quot;type&quot;: &quot;Decimal&quot;,
                  &quot;physicalType&quot;: &quot;decimal&quot;,
                  &quot;precision&quot;: 12,
                  &quot;scale&quot;: 8,
                }
              },
</code></pre>
<p>Is there any way around this strange quirk?</p>
","<oracle><azure-sql-database><azure-data-factory>","2023-02-06 11:12:30","91","0","1","75361313","<p>I also tried to reproduce your scenario and also got similar result when we have Decimal(p,s) data type is already given in SQL table.</p>
<blockquote>
<p>DECIMAL(precision, scale)</p>
<blockquote>
<p><code>precision</code>  -- the maximum number of digits the decimal may store. Precision includes both left and right side of decimal point. It accepts values from 1 to 38. The default is 18.
<code>scale</code>  -- optional, specifies the number of digits after the decimal point. Scale must be between 0 up to the same value as the precision.</p>
</blockquote>
</blockquote>
<p>As your scenario scale his 8 and the number Geeting from Oracal has scale 4 so SQL is adding 4 zeros at the end of the number to get the required scale of data, like below image:</p>
<p><img src=""https://i.imgur.com/WY5W1KZ.png"" alt=""enter image description here"" /></p>
<p><strong>Decimal has a fixed precision while float has variable precision.</strong>
To avoid this the work around can be <strong>changing that data type to <code>float</code> in pre-copy script.</strong></p>
<pre class=""lang-sql prettyprint-override""><code>alter table MY_TABLE alter column MY_COLUMN float;
</code></pre>
<p><img src=""https://i.imgur.com/AqTacid.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong>
<img src=""https://i.imgur.com/sR26YsQ.png"" alt=""enter image description here"" /></p>
"
"75358085","How can we create Azure's Data Factory pipeline with Neo4jDB (with Graph) as data sink ? (data source being OracleDB )","<p>In the Azure environment, I have to copy data from OracleDb by using copy activity and a Neo4j Graph sink location.Using an Azure Data Factory, I need to insert/update data from the Oracledb to the GraphDb.</p>
<p>My thinking is that I need to first transform the data to json and from there insert it into the GraphDb.how to do this ?</p>
<p>In the Azure environment, I have to copy data from OracleDb by using copy activity and a Neo4j Graph sink location.Using an Azure Data Factory, I need to insert/update data from the Oracledb to the GraphDb.</p>
<p>My thinking is that I need to first transform the data to json and from there insert it into the GraphDb.how to do this ?</p>
","<oracle><azure><neo4j><azure-data-factory>","2023-02-06 06:42:43","54","0","1","75611761","<ul>
<li>There might not be any specific sink connector for <code>Neo4j GraphDB</code> in azure data factory. In general, to achieve your requirement, copy data activity should be helpful.</li>
<li>You can read your data from Oracle DB in copy data source using the required connector.</li>
</ul>
<p><img src=""https://i.imgur.com/dPxPEwT.png"" alt=""enter image description here"" /></p>
<ul>
<li>But there is no sink support for <code>Neo4j Graph DB</code>. You can refer to this official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Microsoft documentation</a> to check all the supported source and sink connectors for different activities. You can raise the feature request for this sink connector <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"" title=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"">here</a>.</li>
</ul>
<p><img src=""https://i.imgur.com/EeLhCCJ.png"" alt=""enter image description here"" /></p>
"
"75356767","Required Blob is missing when preview data of a copy data activity sink dataset","<p>I am new to Azure. I created a new ADF, pipeline, storage blob account and a copy data activity, the source is from a SQL server table and the sink output is a parquet file. But when I preview the data of my sink dataset, I got an error saying the required blob is missing.</p>
<p>I want to create a directory as well but weather I type in the folder name and file name or using parameters, I still receive the error. If I manually upload a file via the Azure Storage Explorer, the preview will have no issue.</p>
<p>Anyone what I missed?</p>
<p>Thanks for the help.</p>
<p>cheers
Albert</p>
","<azure><azure-data-factory><pipeline><parquet><copy-data>","2023-02-06 02:00:58","102","0","1","75357616","<p>I created linked service for Azure SQL database and blob storage account and created dataset of SQL database for source:</p>
<p><img src=""https://i.imgur.com/8mmCpmy.png"" alt=""enter image description here"" /></p>
<p>Dataset of blob storage for sink:</p>
<p><img src=""https://i.imgur.com/pnvuq0I.png"" alt=""enter image description here"" /></p>
<p>When I preview the data by entering file name I got below error:</p>
<p><img src=""https://i.imgur.com/w94jvQk.png"" alt=""enter image description here"" /></p>
<p>I got above error because I am not having that file in my blob storage.
In data factory the file will create automatically while debug the pipeline without entering filename.
I just gave the file path where my parquet file need save in sink dataset and debug the pipeline, it executed successfully.</p>
<p><img src=""https://i.imgur.com/QIbivMj.png"" alt=""enter image description here"" /></p>
<p>My SQL table is copied to blob storage as parquet file successfully.</p>
<p><img src=""https://i.imgur.com/Ofz7v09.png"" alt=""enter image description here"" /></p>
"
"75356262","Mapping data flow allows duplicate records when using UPSERT","<p>Using Synapse pipelines and mapping data flow to process multiple daily files residing in ADLS which represent incremental inserts and updates for any given primary key column.  Each daily physical file has ONLY one instance for any given primary key value.  Keys/rows are unique within a daily file, but the same key value can exist in multiple files for each day where attributes related to that key column changed over time.  All rows flow to the Upsert condition as shown in screen shot.</p>
<p><a href=""https://i.stack.imgur.com/OtRZD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OtRZD.png"" alt=""enter image description here"" /></a></p>
<p>Sink is a Synapse table where primary keys can only be specified with non-enforced primary key syntax which can be seen below.</p>
<p><a href=""https://i.stack.imgur.com/nPHOI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nPHOI.png"" alt=""enter image description here"" /></a></p>
<p>Best practice with mapping data flows is avoid placing mapping data flow within a foreach activity to process each file individually as this spins up a new cluster for each file which takes forever and gets expensive. Instead, I have configured the mapping data flow source to use wildcard path to process all files at once with a sort by file name to ensure they are ordered correctly within a single data flow (avoiding the foreach activity for each file).</p>
<p><a href=""https://i.stack.imgur.com/mkFZ8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mkFZ8.png"" alt=""enter image description here"" /></a></p>
<p>Under this configuration, a single data flow looking at multiple daily files can definitely expect the same key column to exist on multiple rows.  When the empty target table is first loaded from all the daily files, we get multiple rows showing up for any single key column value instead of a single INSERT for the first one and updates for the remaining ones it sees (essentially never doing any UPDATES).</p>
<p>The only way I avoid duplicate rows by the key column is to process each file individually and execute a mapping data flow for each file within a for each activity.  Does anyone have any approach that would avoid duplicates while processing all files within a single mapping data flow without a foreach activity for each file?</p>
","<azure-data-factory><azure-synapse><azure-mapping-data-flow>","2023-02-05 23:35:00","167","0","1","75359219","<blockquote>
<p>Does anyone have any approach that would avoid duplicates while processing all files within a single mapping data flow without a foreach activity for each file?</p>
</blockquote>
<p>AFAIK, there is no other way than using ForEach loop to process file one by one.</p>
<p>When we use wildcard, it takes all the matching file in the one go. like below same values from different file.</p>
<p><img src=""https://i.imgur.com/xqawsRm.png"" alt=""enter image description here"" /></p>
<p>using alter rows condition will help you to upsert rows if you have only on single file as you are using multiple files this will create duplicate records like this similar question <a href=""https://stackoverflow.com/a/60862159"">Answer</a> by Leon Yue.</p>
<p>As scenario explained you have same values in multiple files, and you want to avoid that to being getting duplicated. to avoid this, you have to iterate over each of the file and then perform dataflow operations on that file to avoid duplicates getting upsert.</p>
"
"75355619","Azure Synapse Pipeline Date Expression - Last Monday","<p>I have the following azure function that is supposed to retrieve the date of the previous Monday. It works fine except for if the current date is a monday. I need the function to still retrieve the previous monday date if it is Monday or Tuesday. This is due to the time not being updated until middle of day tuesday.</p>
<p><code>@{formatDateTime( subtractFromTime( utcNow(), sub(dayOfWeek(utcNow()),1), 'Day' ), 'yyyy-MM-dd 00:00:00' )}</code></p>
<p>I am still learning Azure synapse so I am not sure if I can write an IF statement that accomplishes this or if there is a better way to write it.</p>
","<azure-data-factory><azure-synapse>","2023-02-05 21:24:59","88","0","1","75357450","<p>I have reproduced the above and able to get the desired result by using the below dynamic content.</p>
<pre><code>@if(greater(dayOfWeek(utcnow()),1),formatDateTime(addDays(subtractFromTime(utcnow(),dayOfWeek(utcnow()),'Day'),1),'yyyy/MM/dd'),formatDateTime(addDays(subtractFromTime(utcnow(),dayOfWeek(utcnow()),'Day'),-6),'yyyy/MM/dd'))
</code></pre>
<ul>
<li>The day of week for a monday from last sunday is 1, so I am checking the current date's day of week is greater than monday or not.</li>
<li><strong>If it is greater</strong> (Today is not a Monday), then <strong>I am giving last Monday by adding 1 day to the last Sunday</strong>.</li>
<li><strong>If it is not</strong>, then <strong>I am subtracting -6 from the last Sunday which is the previous Monday</strong>.</li>
</ul>
<p>This will work for all days, but we need to change the condition and the number which we are adding and subtracting as per the day we required.</p>
<p><img src=""https://i.imgur.com/61brHNn.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/9gwinJP.png"" alt=""enter image description here"" /></p>
<p>If you want the result to be Monday even though current date is Monday or Tuesday, then give the expression for Tuesday also in the above condition using <code>or</code>.</p>
"
"75353230","How Schedule a Azure Data Factory Trigger for 30 minute intervals per day","<p>I how like a create a Azure Data Factory Triggger to run every day at 30min intervals. However, I don't seem to be able to create 30mins interval per day. The nearest I appear to get is 1 hour.</p>
<p>E.g, I would like 6:30, 7:00, 7:30, 8:00 etc.
But as you can see I appear to only schedule hourly per day</p>
<p><a href=""https://i.stack.imgur.com/T3mz3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T3mz3.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-02-05 15:05:26","75","0","2","75353298","<p>I figure it out.</p>
<p>I simply adjusted the execution times to include start minutes of 0, with interval of 30</p>
<p><a href=""https://i.stack.imgur.com/wAoBT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wAoBT.png"" alt=""enter image description here"" /></a></p>
"
"75353230","How Schedule a Azure Data Factory Trigger for 30 minute intervals per day","<p>I how like a create a Azure Data Factory Triggger to run every day at 30min intervals. However, I don't seem to be able to create 30mins interval per day. The nearest I appear to get is 1 hour.</p>
<p>E.g, I would like 6:30, 7:00, 7:30, 8:00 etc.
But as you can see I appear to only schedule hourly per day</p>
<p><a href=""https://i.stack.imgur.com/T3mz3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T3mz3.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-02-05 15:05:26","75","0","2","75381948","<p>Why not this , its more simpler , its you want to start at :00 or:30 min mark .
Then set the start time as accordingly
<a href=""https://i.stack.imgur.com/2dbz6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2dbz6.png"" alt=""enter image description here"" /></a></p>
"
"75349557","snowflake schema with number how to use it in Azure data factory to copy data from Synapse to Snowflake","<p>I am trying to copy data from Synapse and load into Snowflake, for this i am using Azure Data Factory and control table having source and target fields names</p>
<p>My problem here is the snowflake schema name starts with number for example 9289RESIST.Tablename,
but this is failing in ADF due to schema name start with number.
How to give the sink table schema name in Azure Copy activity?</p>
<p>I tried adding double cotes for schema name &quot;9289RESIST&quot; but it was returning me errors.</p>
","<snowflake-cloud-data-platform><azure-data-factory><azure-synapse><snowflake-schema>","2023-02-05 01:27:35","89","0","1","75372307","<p>I created a schema with name <code>923_rlschema</code> in snowflake and tried to call it dynamically from ADF by wrapping the schema name within double quotes and got the same error.</p>
<blockquote>
<p>Message&quot;: &quot;ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42000] SQL compilation error:\nsyntax error line 1 at position 26 unexpected '923'.\nsyntax error line 1 at position 26 unexpected '923'.\nsyntax error line 1 at position 38 unexpected '&quot;&quot;'.,</p>
</blockquote>
<ul>
<li>Then I removed the double quotes and control table looks as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/ppxtx3Y.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>This control table is taken as a dataset in lookup activity
<img src=""https://i.imgur.com/8m6AhMR.png"" alt=""enter image description here"" /></p>
</li>
<li><p>For-each activity is taken and Lookup activity array output is given as items in for-each activity.
<code>@activity('Lookup1').output.value</code></p>
</li>
<li><p>Inside for-each activity, copy activity is taken and source dataset is given</p>
</li>
<li><p>In sink dataset, schema name and table name are given as a dynamic content.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/LxrasUR.png"" alt=""enter image description here"" /></p>
<p>When pipeline is run, it got executed successfully.</p>
"
"75339214","IndexError: string index out of range ADF - Databricks - Python","<p>I'm passing a UDF value from a JSON table through Azure Data Factory as the following:</p>
<p>&quot;schema_array&quot;: &quot;[('creationdate','timestamp'),('agent_name_txt','string'),('email','string'),('agent_hire_date','date'),('days_since_hire_text','int'),('department_auto','string')]&quot;</p>
<p>But when my databricks notebooks accepts it and the indexing is interpreted as a string (which it needs to be set as an array.</p>
<p>This will work in my notebook by accepting the value when hardcoded, but not when passing from ADF. Here's the data type when pass from ADF: print(type(schema_array))
&lt;class 'str'&gt;</p>
<p>In need to know how to transform this (I'm in python) so my notebook will accept it as an array. I've tried to set it as an array in ADF many different ways, but will get the following error: &quot;The variable 'udf' of type 'String' cannot be initialized or updated with value of type 'Array'. The variable 'udf' only supports values of types 'String'&quot;.</p>
<p>I'm running it through a schema mapping function to transform my data frame:</p>
<pre><code>def string_to_datatype(datatype):
  if datatype == 'timestamp':
    final_datatype = t.TimestampType()
  elif datatype == 'integer' or datatype == 'int':
    final_datatype = t.IntegerType()
  elif datatype == 'date':
    final_datatype = t.DateType()
  else: final_datatype = t.StringType()

  return final_datatype

schema = t.StructType([])

for s in schema_array:
    schema = schema.add(t.StructField(s[0], string_to_datatype(s[1])))
</code></pre>
","<azure-data-factory><azure-databricks>","2023-02-03 17:36:18","79","0","1","75361219","<p><strong>AFAIK</strong>, Currently in ADF it is not supported to pass the arrays as parameters to Databricks Notebook.</p>
<p>This is the reason for the above error as it is doing the list operations with string.</p>
<p>So, pass it as a string from ADF to Notebook, convert it to list in Notebook and use as per your requirement.</p>
<p>Use <code>eval('variable_name')</code> for it.</p>
<p><strong>My Repro for your reference:</strong></p>
<p><img src=""https://i.imgur.com/CMxZPY8.png"" alt=""enter image description here"" /></p>
<p>Passing String variable to Notebook:</p>
<p><img src=""https://i.imgur.com/Mto8ruE.png"" alt=""enter image description here"" /></p>
<p><strong>Notebook run:</strong></p>
<p><a href=""https://i.stack.imgur.com/vx8og.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vx8og.png"" alt=""enter image description here"" /></a></p>
"
"75335382","I have to write all filenames from an ADLS folder into an csv file but after successfully pipeline run data is not reflected into destination csv file","<p>Let's suppose there are 12 folders on my Container so i have to copy the folder names to a csv file.</p>
<ol>
<li><p>In first step i used a getmetadata activity to get the folder names from the container
<a href=""https://i.stack.imgur.com/6zZ2M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6zZ2M.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In second step i used a Foreach activity and pass @activity('Get Metadata1').output.childItems as items
a) Inside foreach i used append varriable activity and append the item().name into a varriable Filename as shown in screenshot.So filename varriable is of array type and it is used to store an array of folder names in Container.
<a href=""https://i.stack.imgur.com/Msrs4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Msrs4.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In Third step i used a copy activity it will copy folder names from filename varriable in append activity and will store data into a sink(csv file).
<a href=""https://i.stack.imgur.com/X2rcU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X2rcU.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>a) The source dataset is a dummy csv file
<a href=""https://i.stack.imgur.com/H1fMl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H1fMl.png"" alt=""enter image description here"" /></a></p>
<p>b) then I check the Mapping
<a href=""https://i.stack.imgur.com/TB0xQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TB0xQ.png"" alt=""enter image description here"" /></a></p>
<p><strong>Error</strong></p>
<p>After this when i debug pipeline i am not able to see any foldername on my storage location</p>
<p><a href=""https://i.stack.imgur.com/RwxjY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RwxjY.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-02-03 11:55:34","125","1","1","75343824","<p>You have to deselect the <code>first row as header</code> option in your source dataset. Also change the quote character and escape character to none. The data will be written successfully to your sink file as shown below.</p>
<p><a href=""https://i.stack.imgur.com/iKist.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iKist.png"" alt=""enter image description here"" /></a></p>
<p>However, if you want to write all the file names to a single column, you can use the following procedure instead:</p>
<ul>
<li>I have the following folders in my source:</li>
</ul>
<p><img src=""https://i.imgur.com/VEsVWXT.png"" alt=""enter image description here"" /></p>
<ul>
<li>In the dummy source file, I have the data as following:</li>
</ul>
<p><img src=""https://i.imgur.com/oRoFbOV.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following is the source dataset JSON:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;source1&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;adls&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;DelimitedText&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
                &quot;fileSystem&quot;: &quot;data&quot;
            },
            &quot;columnDelimiter&quot;: &quot;,&quot;,
            &quot;escapeChar&quot;: &quot;\\&quot;,
            &quot;firstRowAsHeader&quot;: true,
            &quot;quoteChar&quot;: &quot;\&quot;&quot;
        },
        &quot;schema&quot;: [
            {
                &quot;type&quot;: &quot;String&quot;
            }
        ]
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<ul>
<li>Now, after using append variable activity to get all the folder names in a single array, use the following dynamic content in new column <code>folder_names</code>.</li>
</ul>
<pre><code>@join(variables('req'),decodeUriComponent('%0D%0A'))
</code></pre>
<p><img src=""https://i.imgur.com/eqRBMLe.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following is the sink dataset JSON:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;output&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;adls&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;DelimitedText&quot;,
        &quot;typeProperties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;AzureBlobFSLocation&quot;,
                &quot;fileName&quot;: &quot;op.csv&quot;,
                &quot;fileSystem&quot;: &quot;output&quot;
            },
            &quot;columnDelimiter&quot;: &quot;,&quot;,
            &quot;escapeChar&quot;: &quot;&quot;,
            &quot;firstRowAsHeader&quot;: true,
            &quot;quoteChar&quot;: &quot;&quot;
        },
        &quot;schema&quot;: []
    }
}
</code></pre>
<ul>
<li>When I run the pipeline, I would get the data as following:</li>
</ul>
<p><img src=""https://i.imgur.com/MjLO0Hh.png"" alt=""enter image description here"" /></p>
"
"75333977","Copy Files Names from a Folder as single File Name","<p>I have a requirement as ADLS container contains a folder with multiple files
Assume</p>
<ol>
<li>Product.csv</li>
<li>Market.csv
3.Sales.csv</li>
</ol>
<p>I need to consider file names Product, Market, Sales and form as Product_Market_Sales.csv in a destination path.</p>
<p>I tried multiple ways to achieve this.
Can anyone help me.</p>
<p>Thanks
Shanu</p>
","<azure-data-factory>","2023-02-03 09:47:24","83","1","1","75335697","<p><strong>If all of your files are having same schema</strong>, then the below approach will work for you.</p>
<p>These are my variables in the pipeline.</p>
<p><img src=""https://i.imgur.com/ruei5NF.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>First use Get Meta data activity to get the <code>ChildItems</code> list from the folder.</p>
<p>It will give the array like below in an alphabetical order of file names.</p>
<p><img src=""https://i.imgur.com/JReq3bZ.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then, Give this array to a ForEach activity. Inside ForEach, split each file name with <code>'.'</code> or <code>'.c'</code> and store the first element of split in an append variable activity to <code>res_filename</code> variable using below dynamic content.</p>
<p><code>@split(item().name,'.')[0]</code></p>
</li>
<li><p>Now, join this array with <code>'_'</code> and store it in a string variable.
<code>@concat(join(variables('names'),'_'),'.csv')</code></p>
<p><img src=""https://i.imgur.com/uWzMz65.png"" alt=""enter image description here"" /></p>
</li>
<li><p>After this, use copy activity and give wild card path <code>*.csv</code> in the source. Use dataset parameters for the sink dataset and give the above variable as file name and give the <strong>Copy behavior</strong> as <code>Merge files</code>.</p>
</li>
</ul>
<p><strong>This is my Pipeline JSON</strong></p>
<pre><code>{
&quot;name&quot;: &quot;filenames&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Get Metadata1&quot;,
            &quot;type&quot;: &quot;GetMetadata&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Sourcefiles&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;fieldList&quot;: [
                    &quot;childItems&quot;
                ],
                &quot;storeSettings&quot;: {
                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                    &quot;enablePartitionDiscovery&quot;: false
                },
                &quot;formatSettings&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Get Metadata1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Get Metadata1').output.childItems&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;names&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@split(item().name,'.')[0]&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Set variable1&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;ForEach1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;res_filename&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@concat(join(variables('names'),'_'),'.csv')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Copy data1&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Set variable1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;wildcardFileName&quot;: &quot;*.csv&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;,
                        &quot;copyBehavior&quot;: &quot;MergeFiles&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                        &quot;quoteAllText&quot;: true,
                        &quot;fileExtension&quot;: &quot;.txt&quot;
                    }
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;Sourcefiles&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;target&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;targetfilename&quot;: {
                            &quot;value&quot;: &quot;@variables('res_filename')&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    }
                }
            ]
        }
    ],
    &quot;variables&quot;: {
        &quot;names&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;res_filename&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Result file:</strong></p>
<p><img src=""https://i.imgur.com/ZBWJGvf.png"" alt=""enter image description here"" /></p>
<p><strong>If you want to merge the above with different source schemas</strong>, then as per my knowledge its better do that by code as suggested in comments with databricks or Azure functions.</p>
"
"75328747","Azure data factory appending to Json","<p>Wanted to pick your brains on something
So, in Azure data factory, I am running a set of activities which at the end of the run produce a json segment</p>
<pre><code>{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; }
</code></pre>
<p>This set of activities occurs in a loop - Loop Until activity.
My goal is to have a final JSON object like this:</p>
<pre><code> &quot;profiles&quot;:[
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; },
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; },
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; },
...
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; }
 ]
</code></pre>
<p>That is a concatenation of all the individual ones.<br />
To put in perspective, each individual item is a paged data from a rest api - and all them constitute the final response.  I have no control over how many are there.</p>
<p>I understand how to concatenate individual items using 2 variables</p>
<pre><code>jsonTemp = @concat(finalJson, individualResponse)
finalJson = jsonTemp
</code></pre>
<p>But, I do not know how to make it all under the single roof &quot;profiles&quot; afterwards.</p>
","<json><azure><azure-functions><azure-data-factory>","2023-02-02 20:45:37","176","0","2","75331675","<p>So this is a bit of a hacky way of doing it and happy to hear a better solution.
I'm assuming you have stored all your results in an array variable (let's call this A).</p>
<ol>
<li><p>First step is to find the number of elements in this array. You can
do this using the length(..) function.</p>
</li>
<li><p>Then you go into a loop, interating a counter variable and
concatenating each of the elements of the array making sure you add
a ',' in between each element. You have to make sure you do not add
the ',' after the last element(You will need to use an IF condition
to check if your counter has reached the length of the array. At the
end of this you should have 1 string variable like this.</p>
<p>{&quot;name&quot;:&quot;myName&quot;,&quot;email&quot;:&quot;email@somewhere.com&quot;},{&quot;name&quot;:&quot;myName&quot;,&quot;email&quot;:&quot;email@somewhere.com&quot;},{&quot;name&quot;:&quot;myName&quot;,&quot;email&quot;:&quot;email@somewhere.com&quot;},{&quot;name&quot;:&quot;myName&quot;,&quot;email&quot;:&quot;email@somewhere.com&quot;}</p>
</li>
<li><p>Now all you need to do this this expression when you are pushing the
response anywhere.</p>
<p>@json(concat('{&quot;profiles&quot;:[',&lt;your_string_variable_here&gt;,']}'))</p>
</li>
</ol>
"
"75328747","Azure data factory appending to Json","<p>Wanted to pick your brains on something
So, in Azure data factory, I am running a set of activities which at the end of the run produce a json segment</p>
<pre><code>{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; }
</code></pre>
<p>This set of activities occurs in a loop - Loop Until activity.
My goal is to have a final JSON object like this:</p>
<pre><code> &quot;profiles&quot;:[
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; },
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; },
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; },
...
{&quot;name&quot;:&quot;myName&quot;, &quot;email&quot;:&quot;email@somewhere.com&quot;, .. &lt;more elements&gt; }
 ]
</code></pre>
<p>That is a concatenation of all the individual ones.<br />
To put in perspective, each individual item is a paged data from a rest api - and all them constitute the final response.  I have no control over how many are there.</p>
<p>I understand how to concatenate individual items using 2 variables</p>
<pre><code>jsonTemp = @concat(finalJson, individualResponse)
finalJson = jsonTemp
</code></pre>
<p>But, I do not know how to make it all under the single roof &quot;profiles&quot; afterwards.</p>
","<json><azure><azure-functions><azure-data-factory>","2023-02-02 20:45:37","176","0","2","75332755","<p>I agree with <strong>@Anupam Chand</strong> and I am following the same process with a <strong>different Second step</strong>.</p>
<p>You mentioned that your object data comes from API pages and to end the until loop you have to give a condition to about the number of pages(number of iterations).</p>
<p><strong>This is my pipeline:</strong></p>
<p><img src=""https://i.imgur.com/2yAbrNz.png"" alt=""enter image description here"" /></p>
<p>First I have initilized a counter and used that counter each web page URL and in until condition to meet a certain number of pages. As ADF do not support self referencing variables, I have used another temporary variable to increment the counter.</p>
<p>To Store the objects of each iteration from web activity, I have created an array variable in pipeline.</p>
<p>Inside ForEach, use append variable activity to append each object to the array like below.</p>
<p><img src=""https://i.imgur.com/gcs2tn0.png"" alt=""enter image description here"" /></p>
<p>For my sample web activity the dynamic content will be <code>@activity('Data from REST').output.data[0]</code>. for you it will be like <code>@activity('Web1').output</code>. change it as per your requirement.</p>
<p><strong>This is my Pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline3&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Until1&quot;,
            &quot;type&quot;: &quot;Until&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Counter intialization&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;expression&quot;: {
                    &quot;value&quot;: &quot;@equals('3', variables('counter'))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Data from REST&quot;,
                        &quot;type&quot;: &quot;WebActivity&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;counter in temp variable&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;url&quot;: {
                                &quot;value&quot;: &quot;https://reqres.in/api/users?page=@{variables('counter')}&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            },
                            &quot;method&quot;: &quot;GET&quot;
                        }
                    },
                    {
                        &quot;name&quot;: &quot;counter in temp variable&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;tempCounter&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@variables('counter')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Counter increment using temp&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Data from REST&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;counter&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@string(add(int(variables('tempCounter')),1))&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Append web output to array&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Counter increment using temp&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;arr&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@activity('Data from REST').output.data[0]&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ],
                &quot;timeout&quot;: &quot;0.12:00:00&quot;
            }
        },
        {
            &quot;name&quot;: &quot;Counter intialization&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;counter&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@string('1')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;To show res array&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Until1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;res_show&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@variables('arr')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        }
    ],
    &quot;variables&quot;: {
        &quot;arr&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;counter&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;tempCounter&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;res_show&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;arr_string&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Result in an array variable:</strong></p>
<p><img src=""https://i.imgur.com/OjdTTou.png"" alt=""enter image description here"" /></p>
<p>You can access this array by the variable name. If you want the output to be like yours, you can use the below dynamic content.</p>
<p><code>@json(concat('{','&quot;profile&quot;:',string(variables('res_show')),'}')))</code></p>
<p>However, if you want to store this in a variable, you have to wrap it in <code>@string()</code> as currently, ADF variables only supports int, string and array type only.</p>
"
"75328467","How to setup an ADF pipeline that isolates every pipeline run and create its own computer resources?","<p>I have a simple pipeline in ADF that is triggered by a Logic App every time someone submits a file as response in a Microsoft forms. The pipeline creates a cluster based in a Docker and then uses a Databricks notebook to run some calculations that can take several minutes. </p>
<p>The problem is that every time the pipeline is running and someone submits a new response to the forms, it triggers another pipeline run that, for some reason, will make the previous runs to fail.</p>
<p>The last pipeline will always work fine, but earlier runs will show this error:</p>
<p> &gt; Operation on target &quot;notebook&quot; failed: Cluster <strong>0202-171614-fxvtfurn</strong> does not exist </p>
<p>However, checking the parameters of the last pipeline it uses a different cluster id, <strong>0202-171917-e616dsng</strong> for example.</p>
<p> It seems that for some reason, the computers resources for the first run are relocated in order to be used for the new pipeline run. However, the IDs of the cluster are different.</p>
<p>I have set up the concurrency up to 5 in the pipeline general settings tab, but still getting the same error. </p>
<p><a href=""https://i.stack.imgur.com/mHKEB.png"" rel=""nofollow noreferrer"">Concurrency setup screenshot</a></p>
<p>Also, in the first connector that looks up for the docker image files I have the concurrency set up to 15, but this won’t fix the issue </p>
<p><a href=""https://i.stack.imgur.com/k618z.png"" rel=""nofollow noreferrer"">look up concurrency screenshot</a></p>
<p>To me, it seems a very simple and common task when it comes to automation and data workflows, but I cannot figure it out.</p>
<p>I really appreciate any help and suggestions, thanks in advance</p>
","<azure><azure-functions><azure-data-factory>","2023-02-02 20:11:37","51","0","1","75381905","<p>The best way would be use an existing pool rather than recreating the pool everytime</p>
<p><a href=""https://i.stack.imgur.com/WnDIz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WnDIz.png"" alt=""enter image description here"" /></a></p>
"
"75323666","Get the list of names via Get Metadata activity","<p>Below is the output of Get Metadata activity which contains name and type values for child items:</p>
<p><a href=""https://i.stack.imgur.com/86SFw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/86SFw.png"" alt=""enter image description here"" /></a></p>
<p>Is it possible to just get the name values and stored within an array variable without using any iteration.</p>
<p>Output = [csv1.csv,csv2.csv,csv3.csv,csv4.csv]</p>
<p>This was achieved via Foreach and append variable, we don't want to use iterations.</p>
","<azure-data-factory><azure-synapse>","2023-02-02 13:09:09","479","1","1","75332649","<p><strong>APPROACH 1 :</strong></p>
<p>Using for each would be easier to complete the job. However, you can use string manipulation in the following way to get the desired result.</p>
<ul>
<li>Store the output of get metadata child items in a variable as a string:</li>
</ul>
<pre><code>@string(activity('Get Metadata1').output.childItems)
</code></pre>
<p><img src=""https://i.imgur.com/FblQUkk.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now replace all the unnecessary data with empty string <code>''</code> using the following dynamic content:</li>
</ul>
<pre><code>@replace(replace(replace(replace(replace(replace(replace(replace(variables('tp'),'[{',''),'}]',''),'{',''),'}',''),'&quot;type&quot;:&quot;File&quot;',''),'&quot;',''),'name:',''),',,',',')
</code></pre>
<p><img src=""https://i.imgur.com/7bB29Ye.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, ignore the last comma and split the above string with <code>,</code> as delimiter.</li>
</ul>
<pre><code>@split(substring(variables('ans'),0,sub(length(variables('ans')),1)),',')
</code></pre>
<p><img src=""https://i.imgur.com/XAG4Pzw.png"" alt=""enter image description here"" /></p>
<p><strong>APPROACH 2 :</strong></p>
<p>Let's say your source has a combination of folders and files, and you want only the names of objects whose type is File in an array, then you can use the following approach. Here there is no need of for each, but you will have to use copy data and dataflows.</p>
<ul>
<li>Create a copy data activity with a sample file with data like below:</li>
</ul>
<p><img src=""https://i.imgur.com/boI5LJy.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now create an additional column <code>my_json</code> with value as the following dynamic content:</li>
</ul>
<pre><code>@replace(string(activity('Get Metadata1').output.childItems),'&quot;',pipeline().parameters.single_quote)
</code></pre>
<ul>
<li>The following is the sink dataset configuration that I have taken:</li>
</ul>
<p><img src=""https://i.imgur.com/xX9abhA.png"" alt=""enter image description here"" /></p>
<ul>
<li>In the mapping, just select this newly created column and remove the rest (demo) column.</li>
</ul>
<p><img src=""https://i.imgur.com/Eod0Ppu.png"" alt=""enter image description here"" /></p>
<ul>
<li>Once this copy data executes, the file generated will be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/zZG6wfe.png"" alt=""enter image description here"" /></p>
<ul>
<li>In dataflow, with the above file as source with settings as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/6TGcOtV.png"" alt=""enter image description here"" /></p>
<ul>
<li>The data would be read as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/wdt19e2.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, use aggregate transformation to group by the type column and collect() on <code>name</code> column.</li>
</ul>
<p><img src=""https://i.imgur.com/WnKwVZv.png"" alt=""enter image description here"" /></p>
<ul>
<li>The result would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/lYUGB8o.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, use conditional split to separate the file type data and folder type data with the condition <code>type == 'File'</code></li>
</ul>
<p><img src=""https://i.imgur.com/FHWnFio.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now write the fileType data to sink cache. The data would look like this:</li>
</ul>
<p><img src=""https://i.imgur.com/eXTTNTL.png"" alt=""enter image description here"" /></p>
<ul>
<li>Back in the pipeline, use the following dynamic content to get the required array:</li>
</ul>
<pre><code>@activity('Data flow1').output.runStatus.output.sink1.value[0].array_of_types
</code></pre>
<p><img src=""https://i.imgur.com/E4yzd5b.png"" alt=""enter image description here"" /></p>
<p>Pipeline JSON for reference:</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline3&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Get Metadata1&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;source1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;fieldList&quot;: [
                        &quot;childItems&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Get Metadata1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;additionalColumns&quot;: [
                            {
                                &quot;name&quot;: &quot;my_json&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@replace(string(activity('Get Metadata1').output.childItems),'\&quot;',pipeline().parameters.single_quote)&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        ],
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;mappings&quot;: [
                            {
                                &quot;source&quot;: {
                                    &quot;name&quot;: &quot;my_json&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;String&quot;,
                                    &quot;physicalType&quot;: &quot;String&quot;,
                                    &quot;ordinal&quot;: 1
                                }
                            }
                        ],
                        &quot;typeConversion&quot;: true,
                        &quot;typeConversionSettings&quot;: {
                            &quot;allowDataTruncation&quot;: true,
                            &quot;treatBooleanAsNumber&quot;: false
                        }
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;sink1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            },
            {
                &quot;name&quot;: &quot;Data flow1&quot;,
                &quot;type&quot;: &quot;ExecuteDataFlow&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Copy data1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataflow&quot;: {
                        &quot;referenceName&quot;: &quot;dataflow2&quot;,
                        &quot;type&quot;: &quot;DataFlowReference&quot;
                    },
                    &quot;compute&quot;: {
                        &quot;coreCount&quot;: 8,
                        &quot;computeType&quot;: &quot;General&quot;
                    },
                    &quot;traceLevel&quot;: &quot;None&quot;
                }
            },
            {
                &quot;name&quot;: &quot;Set variable2&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Data flow1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;req&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@activity('Data flow1').output.runStatus.output.sink1.value[0].array_of_types&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;single_quote&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;'&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;req&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;tp&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;ans&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;req_array&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;lastPublishTime&quot;: &quot;2023-02-03T06:09:07Z&quot;
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p>Dataflow JSON for reference:</p>
<pre><code>{
    &quot;name&quot;: &quot;dataflow2&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Json3&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;aggregate1&quot;
                },
                {
                    &quot;name&quot;: &quot;split1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          name as string,&quot;,
                &quot;          type as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false,&quot;,
                &quot;     documentForm: 'arrayOfDocuments',&quot;,
                &quot;     singleQuoted: true) ~&gt; source1&quot;,
                &quot;source1 aggregate(groupBy(type),&quot;,
                &quot;     array_of_types = collect(name)) ~&gt; aggregate1&quot;,
                &quot;aggregate1 split(type == 'File',&quot;,
                &quot;     disjoint: false) ~&gt; split1@(fileType, folderType)&quot;,
                &quot;split1@fileType sink(validateSchema: false,&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     store: 'cache',&quot;,
                &quot;     format: 'inline',&quot;,
                &quot;     output: true,&quot;,
                &quot;     saveOrder: 1) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
"
"75314282","How to Parse Json Object Element that contains Dynamic Attribute","<p>We have a Json object with the following structure.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;results&quot;: {
    &quot;timesheets&quot;: {
      &quot;135288482&quot;: {
        &quot;id&quot;: 135288482,
        &quot;user_id&quot;: 1242515,
        &quot;jobcode_id&quot;: 17288283,        
        &quot;customfields&quot;: {
          &quot;19142&quot;: &quot;Item 1&quot;,
          &quot;19144&quot;: &quot;Item 2&quot;
        },
        &quot;attached_files&quot;: [
          50692,
          44878
        ],
        &quot;last_modified&quot;: &quot;1970-01-01T00:00:00+00:00&quot;
      },
      &quot;135288514&quot;: {
        &quot;id&quot;: 135288514,
        &quot;user_id&quot;: 1242509,
        &quot;jobcode_id&quot;: 18080900,
        &quot;customfields&quot;: {
          &quot;19142&quot;: &quot;Item 1&quot;,
          &quot;19144&quot;: &quot;Item 2&quot;
        },
        &quot;attached_files&quot;: [
          50692,
          44878
        ],
        &quot;last_modified&quot;: &quot;1970-01-01T00:00:00+00:00&quot;
      }}
</code></pre>
<p>We need to access the elements that is inside the results --&gt; timesheets --&gt; Dynamic id.</p>
<p>Example:</p>
<pre><code>{
        &quot;id&quot;: 135288482,
        &quot;user_id&quot;: 1242515,
        &quot;jobcode_id&quot;: 17288283,        
        &quot;customfields&quot;: {
          &quot;19142&quot;: &quot;Item 1&quot;,
          &quot;19144&quot;: &quot;Item 2&quot;
        },
        &quot;attached_files&quot;: [
          50692,
          44878
        ],
        &quot;last_modified&quot;: &quot;1970-01-01T00:00:00+00:00&quot;
      }
</code></pre>
<p>The problem is that &quot;135288482&quot;: { is dynamic.  How do we access what is inside of it.</p>
<p>We are trying to create data flow to parse the data. The data is dynamic, so accessing via attribute name is not possible.</p>
","<json><azure-data-factory><azure-synapse>","2023-02-01 17:56:48","106","1","1","75325211","<p><strong>AFAIK</strong>, as per your JSON structure and dynamic keys it might not be possible to get the desired result using Dataflow.
I have reproduced the above and able to get it done using set variable and ForEach like below.</p>
<p><img src=""https://i.imgur.com/NFIETD2.png"" alt=""enter image description here"" /></p>
<p>In your JSON, the <strong>keys and the values for the ids are same</strong>. So, I have used that to get the list of keys first. Then using that list of keys, I am able to access the inner JSON object.</p>
<p><strong>These are my variable in the pipeline:</strong></p>
<p><img src=""https://i.imgur.com/gmFlRyK.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>First, I have a <strong>set variable</strong> of type string and stored <code>&quot;id&quot;</code> in it.</p>
</li>
<li><p>Then I have taken <strong>lookup activity</strong> to get the above JSON file from blob.</p>
</li>
<li><p>I have stored lookup the timesheets objects as string in a set variable using the below dynamic content
<code>@string(activity('Lookup1').output.value[0].results.timesheets)</code></p>
</li>
<li><p>I have used <strong>split on that string with <code>&quot;id&quot;</code></strong> and stored the result array in an array variable.
<code>@split(variables('jsonasstring'), variables('ids'))</code>
This will give the array like below.</p>
<p><img src=""https://i.imgur.com/sKWl6dk.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Now, I took a <strong>Foreach</strong> to this array but skipped the first element. In Each iteration, I have taken first 9 indexes of the string to append variable and that is the key.
If your id values and keys are not same, then you can skip the last from split array and take the keys from the reverse side of the string as per your requirement.
This is my dynamic content for append variable activity inside ForEach <code>@take(item(), 9)</code></p>
</li>
<li><p>Then I took another ForEach, and given this keys list array to it. Inside foreach you can access the JSON with below dynamic content.
<code>@string(activity('Lookup1').output.value[0].results.timesheets[item()])</code></p>
</li>
</ul>
<p><strong>This is my pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline1&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Lookup1&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;for ids&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;JsonSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Json1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;JSON as STRING&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;jsonasstring&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@string(activity('Lookup1').output.value[0].results.timesheets)&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;for ids&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;ids&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@string('\&quot;id\&quot;:')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;after split&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;JSON as STRING&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;split_array&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@split(variables('jsonasstring'), variables('ids'))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;ForEach to append keys to array&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;after split&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@skip(variables('split_array'),1)&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;key_ids_array&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@take(item(), 9)&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;ForEach to access inner object&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;ForEach to append keys to array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('key_ids_array')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Each object&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;show_res&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@string(activity('Lookup1').output.value[0].results.timesheets[item()])&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        }
    ],
    &quot;variables&quot;: {
        &quot;jsonasstring&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;ids&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;split_array&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;key_ids_array&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;show_res&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Result:</strong></p>
<p>use <code>@json()</code> in the dynamic content to convert the below string to an object.
<img src=""https://i.imgur.com/ty2Nzzy.png"" alt=""enter image description here"" /></p>
<p>If you want to store this JSONs in a file, use a ForEach and inside Foreach use an SQL script to copy each object as a row to table in each iteration using <code>JSON_VALUE</code> . Then outside Foreach use copy activity to copy that SQL table to your destination as per your requirement.</p>
"
"75311764","Need to Connect Azure Sql db from Azure Data Factory,restricted at Schema Level using userManagedIdentity as Authentication Method","<p>I have a successful connection from Azure data Factory to my Azure Sql db .And I have set the AAD Admin as myself and also the UserManagedIdentity from the portal.</p>
<p>Now whoever use that UserManagedidentity in ADF can access the entire Sql DB.I need to restrict the access at Schema level, like X people should have access to X tables and Y people should have access to Y Tables.</p>
<p>So how can we achieve this through usermangedIdentity ,Can we set Schema level permissions via usermanagedidentity?</p>
","<azure><azure-functions><azure-web-app-service><azure-data-factory>","2023-02-01 14:33:35","39","0","2","75313038","<p>The managed identity has a corresponding user in SQL, so limit their permissions are you would any other user or group.</p>
<p>i.e.:
GRANT SELECT ON Employees TO UserManagedIdentity;</p>
"
"75311764","Need to Connect Azure Sql db from Azure Data Factory,restricted at Schema Level using userManagedIdentity as Authentication Method","<p>I have a successful connection from Azure data Factory to my Azure Sql db .And I have set the AAD Admin as myself and also the UserManagedIdentity from the portal.</p>
<p>Now whoever use that UserManagedidentity in ADF can access the entire Sql DB.I need to restrict the access at Schema level, like X people should have access to X tables and Y people should have access to Y Tables.</p>
<p>So how can we achieve this through usermangedIdentity ,Can we set Schema level permissions via usermanagedidentity?</p>
","<azure><azure-functions><azure-web-app-service><azure-data-factory>","2023-02-01 14:33:35","39","0","2","75318572","<p>Admin overrides all other restrictions. So as long as a user is part of Server admin, he/she can have the entire access.
For your use case, you would need to remove the managed identity from the admin group DL, create a new user within the database and grant the new user required access</p>
"
"75310487","Copy Activity Not able to copy any response from Rest api in Azure Data factory","<p>I am using input as rest api  url .And I am trying to save the response to a sql table.When I run the pipeline the pipeline run successfully,But it is showing zero rows copied.</p>
<p>I tested the api in postman.I am able to see the reponse data (9 mb)</p>
<p>Anybody else got this same issue,Please help me</p>
","<azure-data-factory>","2023-02-01 12:54:36","188","0","2","75332265","<p>I tried to reproduce and faced similar problem Its not inserting any records.
<img src=""https://i.imgur.com/42l6paz.png"" alt=""enter image description here"" /></p>
<p>The problem is causing due to API returns response in Json and pipeline doesn't know which object value should store in which column.</p>
<p><strong>To resolve this use <code>Mapping</code>. import the scma and amp the paricular columns as below:</strong></p>
<p><img src=""https://i.imgur.com/NeLSXqe.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong>
<img src=""https://i.imgur.com/XjBfBUw.png"" alt=""enter image description here"" /></p>
"
"75310487","Copy Activity Not able to copy any response from Rest api in Azure Data factory","<p>I am using input as rest api  url .And I am trying to save the response to a sql table.When I run the pipeline the pipeline run successfully,But it is showing zero rows copied.</p>
<p>I tested the api in postman.I am able to see the reponse data (9 mb)</p>
<p>Anybody else got this same issue,Please help me</p>
","<azure-data-factory>","2023-02-01 12:54:36","188","0","2","75382007","<p>I think the intend here is copy the response json to  SQL and if thats the case then we cannot do that with copy activity .
One way is you can use a web activity to call the API and after that you can call a Stored proc activity and pass the response as a input paramter to the SP . The SP will insert the record in the table . But 9MB of response is  too big , i doubt if the web activity can handle that .</p>
"
"75309739","Azure Data Flow: Array to columns","<p>In my data flow I have a column with an array and I need to map it to columns.
Here is an example of the data:</p>
<pre><code>[&quot;title:mr&quot;,&quot;name:jon&quot;,&quot;surname:smith&quot;]
[surname:jane&quot;]
[&quot;title:mrs&quot;,&quot;surname:peters&quot;]
[&quot;title:mr&quot;]
</code></pre>
<p>and here is an example of the desired result:</p>
<p><a href=""https://i.stack.imgur.com/6kaGG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6kaGG.png"" alt=""enter image description here"" /></a></p>
<p>what's the best approach to achieve this?</p>
","<azure-data-factory>","2023-02-01 11:48:44","337","0","1","75331342","<p>You can do this using the combination of derived column, rank and pivot transformations.</p>
<ul>
<li>Let's say I have the given sample data (array of strings) as a column <code>mycol</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/WRUItRd.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have used <code>rank</code> transformation. I have given column name <code>id</code> for rank column and used <code>mycol</code> column for sort condition (ascending order). The result would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/8Gv1jPP.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I have used derived column to create a <code>new</code> column with dynamic expression as <code>unfold(mycol)</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/nM75bPa.png"" alt=""enter image description here"" /></p>
<ul>
<li>For some reason this new column's type was not being rendered properly. So, I have used cast to make it complex type with complex type defination as <code>string[]</code>.</li>
<li>I have created 2 new columns <code>key</code> and <code>value</code>. The dynamic contents are as follows:</li>
</ul>
<pre><code>key: split(new[1],':')[1]
value: split(new[1],':')[2]
</code></pre>
<p><img src=""https://i.imgur.com/SqE4WcM.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I have used <code>pivot</code> transformation. Here I have used group by on <code>id</code>, selected pivot column as <code>key</code> and selected pivoted columns as <code>max(value)</code> (since aggregate has to be used).</li>
</ul>
<p><img src=""https://i.imgur.com/SgKMvEW.png"" alt=""enter image description here"" /></p>
<ul>
<li>The required result is obtained. The following is the entire dataflow JSON (The actual transformations start from rank as you already have the array column.)</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;dest&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [
                {
                    &quot;name&quot;: &quot;derivedColumn1&quot;
                },
                {
                    &quot;name&quot;: &quot;rank1&quot;
                },
                {
                    &quot;name&quot;: &quot;derivedColumn2&quot;
                },
                {
                    &quot;name&quot;: &quot;cast1&quot;
                },
                {
                    &quot;name&quot;: &quot;derivedColumn3&quot;
                },
                {
                    &quot;name&quot;: &quot;pivot1&quot;
                }
            ],
            &quot;scriptLines&quot;: [
                &quot;source(output(&quot;,
                &quot;          mycol as string&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     ignoreNoFilesFound: false) ~&gt; source1&quot;,
                &quot;source1 derive(mycol = split(replace(replace(replace(mycol,'[',''),']',''),'\&quot;',''),',')) ~&gt; derivedColumn1&quot;,
                &quot;derivedColumn1 rank(asc(mycol, true),&quot;,
                &quot;     output(id as long)) ~&gt; rank1&quot;,
                &quot;rank1 derive(new = unfold(mycol)) ~&gt; derivedColumn2&quot;,
                &quot;derivedColumn2 cast(output(&quot;,
                &quot;          new as string[]&quot;,
                &quot;     ),&quot;,
                &quot;     errors: true) ~&gt; cast1&quot;,
                &quot;cast1 derive(key = split(new[1],':')[1],&quot;,
                &quot;          value = split(new[1],':')[2]) ~&gt; derivedColumn3&quot;,
                &quot;derivedColumn3 pivot(groupBy(id),&quot;,
                &quot;     pivotBy(key),&quot;,
                &quot;     {} = max(value),&quot;,
                &quot;     columnNaming: '$N$V',&quot;,
                &quot;     lateral: true) ~&gt; pivot1&quot;,
                &quot;pivot1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     partitionFileNames:['op.csv'],&quot;,
                &quot;     umask: 0022,&quot;,
                &quot;     preCommands: [],&quot;,
                &quot;     postCommands: [],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     saveOrder: 1,&quot;,
                &quot;     partitionBy('hash', 1)) ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
"
"75307983","How to Set Azure Data Factory Schedule Between Intervals","<p>Is is possible to set a schedule in Azure Data Factory to execute a pipeline at intervals?</p>
<p>For example, I would like to schedule that runs every hour from Monday to Friday between 9am and 5am</p>
<p>At the moment I the following, but not sure how to enter the execution times.</p>
<p><a href=""https://i.stack.imgur.com/XJCzx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XJCzx.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-02-01 09:23:13","37","0","1","75308307","<p>It should look something like that:
<a href=""https://i.stack.imgur.com/AwqtC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AwqtC.png"" alt=""shedule trigger"" /></a></p>
"
"75307457","Azure Data Factory - Retrieve next pagination link (decoded) from response headers in a copy data activity of Azure Data Factory","<p>I have created a copy data activity in azure data factory and this data pipeline pulls the data from an API (via REST activity source) and writes the response body (json) on a file kept in the azure blob storage.
The API which I am fetching the response from, is paginated and the link to next page is sent in the response headers in response-&gt;headers-&gt;link.</p>
<p>This URL to next page is in the following general format:
&lt;https%3A%2F%2FsomeAPI.com%2Fv2%2FgetRequest%3FperPage%3D80%26sortOrder%3DDESCENDING%26nextPageToken%3DVAdjkjklfjjgkl&gt;; rel=&quot;next&quot;</p>
<p>I want to fetch the next page token present in the above URL and use it in the pagination rule.</p>
<p>I have tried using some pagination rules:</p>
<p><strong>&gt; AbsoluteURL = Headers.link</strong>
But, this did not work as the entire encoded link shown above, is getting appended directly and hence, the pipeline throws an error.</p>
<p><strong>&gt; Query Parameters</strong>
I have also tried to use the query parameters but could not get any result.</p>
<p>I have followed questions over stackoverflow and have read the documentations:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/874836/adf-copy-activity-using-parameter-for-query"" rel=""nofollow noreferrer""></a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest?tabs=data-factory#pagination-support"" rel=""nofollow noreferrer""></a></p>
<p>Please help me with how can I access this next page token or what can be the pagination rule to support the scenario.</p>
<p>Pasting postman output and ADF data pipeline, for reference.
<a href=""https://i.stack.imgur.com/mHjhJ.png"" rel=""nofollow noreferrer"">Postman Response Headers Output</a>
<a href=""https://i.stack.imgur.com/jpr2a.png"" rel=""nofollow noreferrer"">Pagination Rules, I need help on</a></p>
","<azure><pagination><azure-data-factory><data-pipeline>","2023-02-01 08:37:49","155","0","1","75613602","<p>AFAIK, there is no direct option to decode the URL in ADF paginating rule.</p>
<p>You can follow below approach:</p>
<ul>
<li><p>First take a set variable and create <code>temp</code> variable as <strong>string</strong> and value as <code>1</code>
<img src=""https://i.imgur.com/7E5uuvP.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then take until activity and give the condition as <code>@equals(variables('temp1'),10)</code> so it will iterate over it till condition matches.
<img src=""https://i.imgur.com/f4eYpgr.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Under the until loop take another set variable and create variable <code>URL1</code> as <strong>string</strong> and value as URL of rest api.
<img src=""https://i.imgur.com/HmQ6A0F.png"" alt=""enter image description here"" /></p>
</li>
<li><p>After this pass variable value to copy activity and copy the first page to destination.
<img src=""https://i.imgur.com/s2DUCNp.png"" alt=""enter image description here"" /></p>
</li>
<li><p>After this take web activity and pass that URL variable to it and get response headers.
<img src=""https://i.imgur.com/1P4ON4y.png"" alt=""enter image description here"" /></p>
</li>
<li><p>After this take set variable and update the URL to next page url usnig dynamic expression.</p>
</li>
</ul>
<pre><code>@uriComponentToString(split(split(activity('Web1').output.ADFWebActivityResponseHeaders[&quot;link&quot;],'&gt;')[0],'&lt;')[1])
</code></pre>
<p><img src=""https://i.imgur.com/dK3nWuS.png"" alt=""enter image description here"" /></p>
<ul>
<li>In last create a variable called increment as string and increment initial temp variable for every iteration by <code>@add(int(variables('temp1')),1)</code>
<img src=""https://i.imgur.com/Ei6Jstg.png"" alt=""enter image description here"" /></li>
</ul>
<p>By this process you can copy as many pages as you want or you set in Until loop as we don't have any specific condition to terminate loop.</p>
"
"75300123","Can a flowlet pass through all columns?","<p>We are using Azure Data Factory and are exploring if we could use Flowlets for transformations that occur in most Data flows.</p>
<p>Our first attempt was to create a flowlets that only add some columns (using a &quot;Derived Column&quot; step) to a stream. So in the &quot;Input&quot; step we don't require any column to be present in the received stream. Then the &quot;Derived Column&quot; followed by the &quot;Output&quot; step. And done... we thought.</p>
<p>When using this flowlet in a data flow we go from 25 columns back to only the column we added, all our original columns are no longer available.</p>
<p>Is it possible to use a flowlet to work on only a selection of all available columns but that all columns in the stream are &quot;passed through&quot; and thus will be available in the sink of the original data flow?</p>
","<azure-data-factory>","2023-01-31 16:25:12","84","0","1","75302106","<p>Be sure to select the Allow Schema Drift option on your Flowlet input settings</p>
<p><a href=""https://i.stack.imgur.com/VpR7u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VpR7u.png"" alt=""enter image description here"" /></a></p>
"
"75297691","How to increase performance of Azure Data Factory Pipeline with Integration Runtime","<p>I would like to increated the performance of our pipelines.</p>
<p>The pipelines currently run from an integration runtime.</p>
<p>I am running a single copy activity on tables held on our Source which is a SQL Database. Tables contain just under a million rows, with about 15 columns.</p>
<p>Currently the time it takes to copy a table from Source to Sink(ADLS) is approximately 20mins.</p>
<p>Is there a way to increase the DIU to increase performance?</p>
<p>My current copy settings are as follows:</p>
<p><a href=""https://i.stack.imgur.com/AQXpG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AQXpG.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/l3L3S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l3L3S.png"" alt=""enter image description here"" /></a></p>
<p>I'm thinking that if I made some changes to Settings, see below, I would improve performance, but I have never played around to settings before, any suggestions most welcomed.</p>
<p><a href=""https://i.stack.imgur.com/zWXR3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zWXR3.png"" alt=""enter image description here"" /></a></p>
<p>The activity details for a pipeline run is as follows:</p>
<p><a href=""https://i.stack.imgur.com/jpYJi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jpYJi.png"" alt=""enter image description here"" /></a></p>
<p>My link service is an Azure Synapse Link service, see below:</p>
<p><a href=""https://i.stack.imgur.com/pbnwZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pbnwZ.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-01-31 13:10:43","119","0","2","75298890","<p>From the output window, we can see that almost all the wait time was &quot;Time to first byte&quot;, which means your SQL server is slow to reply. It takes ~22 minutes for less than 90K rows. So changes on the ADF side will not help.
If your query is a simple &quot;select * from table&quot;, then maybe your SQL server is low on resources. You can check that in your database portal in Azure. Try to add more resources and see if copy times improve.
If this is a query from a view or other complicated query, maybe it needs some improvement (indexes, improve code). You can test that by writing the query result to a table in your SQL database, use that table as the data factory source, and see if this improves copy time.</p>
"
"75297691","How to increase performance of Azure Data Factory Pipeline with Integration Runtime","<p>I would like to increated the performance of our pipelines.</p>
<p>The pipelines currently run from an integration runtime.</p>
<p>I am running a single copy activity on tables held on our Source which is a SQL Database. Tables contain just under a million rows, with about 15 columns.</p>
<p>Currently the time it takes to copy a table from Source to Sink(ADLS) is approximately 20mins.</p>
<p>Is there a way to increase the DIU to increase performance?</p>
<p>My current copy settings are as follows:</p>
<p><a href=""https://i.stack.imgur.com/AQXpG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AQXpG.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/l3L3S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l3L3S.png"" alt=""enter image description here"" /></a></p>
<p>I'm thinking that if I made some changes to Settings, see below, I would improve performance, but I have never played around to settings before, any suggestions most welcomed.</p>
<p><a href=""https://i.stack.imgur.com/zWXR3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zWXR3.png"" alt=""enter image description here"" /></a></p>
<p>The activity details for a pipeline run is as follows:</p>
<p><a href=""https://i.stack.imgur.com/jpYJi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jpYJi.png"" alt=""enter image description here"" /></a></p>
<p>My link service is an Azure Synapse Link service, see below:</p>
<p><a href=""https://i.stack.imgur.com/pbnwZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pbnwZ.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-01-31 13:10:43","119","0","2","75305302","<p>Quick check , is the Azure SQL and storage account in the same region ? Also I see that your copy activity is set as parraleism as 1 , you can play with number and see if that helps .</p>
<p>How to setyp parallelism please read here : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#parallel-copy"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#parallel-copy</a></p>
<p>Please see the snaphot below</p>
<p><a href=""https://i.stack.imgur.com/GtYTo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GtYTo.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/sLwj8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sLwj8.png"" alt=""enter image description here"" /></a></p>
"
"75297637","ADF - Iterate through blob container containing JSON files, partition and push to various SQL tables","<p>I have a blob storage container in Azure Data Factory that contains many JSON files. Each of these JSON files contain data from an API. The data needs to be split into ~30 tables in my azure DWH.<a href=""https://i.stack.imgur.com/0nJQy.png"" rel=""nofollow noreferrer""><br />
ADF Processhere</a></p>
<p>I am hoping someone can provide some clarity on the best way to achieve this (I am new to the field of Data Engineering and trying to develop my skills through projects).</p>
<p>At present, I have written 1 stored procedure which contains code to extract data and insert data into 1 of the 30 tables. Is this the right approach? And if so, could you please advise how best to design my pipeline?</p>
<p>Thanks in advance.</p>
","<azure><azure-sql-database><azure-data-factory>","2023-01-31 13:06:44","78","0","1","75303683","<p>I am assuming that the you also have 30 folders in the containers and eachdata from each folder will land up in a table or may be the file are named in a such a way that you can dereive which table it lands on . Please read about Get meta data activity and it should give an idea as to how to get the filename/folder name : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>
<p>Once you have that I think you should read about paramterizing a dataset : <a href=""https://stackoverflow.com/questions/70315146/use-parameterized-dataset-for-dataflow"">Use parameterized dataset for DataFlow</a>  and this also <a href=""https://www.youtube.com/watch?v=9XSJih4k-l8"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=9XSJih4k-l8</a></p>
"
"75295998","Problems with rule-based mapping in Azure Dataflow","<p>I have multiple XMLs in an Azure Blob. I want to extract certain datapoints and sort them into an Azure SQL db.
For this I use Data Flow. The name of certain elements changes sometimes <code>[random name]</code> so i would like to set up a rule-based mapping, that fetches the right values every time .</p>
<p>I want to retrieve <code>IMPORTANT INFORMATION</code>, which is always located in the first randomly named child of category_a.</p>
<p>Apart from the randomly named object the rest of the structure always stays the same.</p>
<p>This is about the structure:</p>
<pre><code>&lt;title&gt;
&lt;category_a&gt;

    &lt;random_name_1&gt;
        &lt;object_a&gt;
            &lt;subobject_object_a&gt;
                &lt;p&gt;IMPORTANT INFORMATION&lt;/p&gt;
            &lt;/subobject_object_a&gt;
        &lt;/object_a&gt;
    &lt;/random_name_1&gt;

    &lt;random_name_2&gt;
        &lt;object_a&gt;
            &lt;subobject_object_a&gt;
                &lt;p&gt;IRRELEVANT INFORMATION&lt;/p&gt;
            &lt;/subobject_object_a&gt;
        &lt;/object_a&gt;
    &lt;/random_name_2&gt;

&lt;/category_a&gt;

&lt;category_b&gt;&lt;/category_b&gt;
</code></pre>

<p>How do I need to write the rule based mapping so that I always fetch this value no matter the random name in the middle of the path?</p>
<p>Thanks for your help</p>
","<azure><azure-data-factory>","2023-01-31 10:47:56","48","0","1","75306616","<p>There might be no option to find or use rule-based mapping to retrieve the important information. As a work around, I have used lookup and string manipulation activities to get the result. The following is the pipeline JSON:</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;XmlSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;XmlReadSettings&quot;,
                            &quot;validationMode&quot;: &quot;none&quot;,
                            &quot;namespaces&quot;: true
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;Xml1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;tp&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@replace(replace(split(string(activity('Lookup1').output.value[0].title.category_a),':')[0],'\&quot;',''),'{','')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Set variable2&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;tp2&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@replace(replace(split(string(activity('Lookup1').output.value[0].title.category_a[variables('tp')]),':')[0],'\&quot;',''),'{','')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Set variable3&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable2&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;tp3&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@replace(replace(split(string(activity('Lookup1').output.value[0].title.category_a[variables('tp')][variables('tp2')]),':')[0],'\&quot;',''),'{','')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Set variable4&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable3&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;tp4&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@replace(replace(split(string(activity('Lookup1').output.value[0].title.category_a[variables('tp')][variables('tp2')][variables('tp3')]),':')[0],'\&quot;',''),'{','')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Set variable5&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable4&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;final&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@activity('Lookup1').output.value[0].title.category_a[variables('tp')][variables('tp2')][variables('tp3')][variables('tp4')]&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;tp&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;tp2&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;tp3&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;tp4&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;final&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<ul>
<li><p>The lookup has the source xml file. Since we know the <code>category_a</code> is a child element, I have started from there to obtain the child element names (in each set variable activity).</p>
</li>
<li><p>The following is the output image for reference:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/LaOlXMU.png"" alt=""enter image description here"" /></p>
"
"75295465","How to Access Azure Data Factory if we disable public access for keyvault","<p>So how we can fetch the secrets or keys for Azure Data Factory , logic apps, Azure Synapse and Azure DataBricks if we disable the public access for keyvault.</p>
<p>I found a solution for AppServices and FunctionApps by using outbound IP Addresses and i need a solution for accessing the ADF,synapse,logicapps and databricks if we disable public access for keyvault.
I tried using Service Principal and grant permissions but its not working.</p>
<p>Please help me with the solution.</p>
","<azure><azure-active-directory><azure-data-factory><azure-databricks>","2023-01-31 10:03:16","219","0","2","75296273","<p>Even if you disable public access, you can still leave &quot;Allow trusted Microsoft services to bypass this firewall&quot; on, and so allow the MS services you mention to have access.
<a href=""https://i.stack.imgur.com/NOrlC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NOrlC.png"" alt=""public access"" /></a>
You can also create a private endpoint, and so add the key vault to your private Vnet.</p>
<p>All of this is related to networking - not being blocked by a firewall. You also need to grant permission to the service you use to access key vault, for example, with service principle or managed identity.</p>
"
"75295465","How to Access Azure Data Factory if we disable public access for keyvault","<p>So how we can fetch the secrets or keys for Azure Data Factory , logic apps, Azure Synapse and Azure DataBricks if we disable the public access for keyvault.</p>
<p>I found a solution for AppServices and FunctionApps by using outbound IP Addresses and i need a solution for accessing the ADF,synapse,logicapps and databricks if we disable public access for keyvault.
I tried using Service Principal and grant permissions but its not working.</p>
<p>Please help me with the solution.</p>
","<azure><azure-active-directory><azure-data-factory><azure-databricks>","2023-01-31 10:03:16","219","0","2","75298137","<p><strong>I tried to reproduce the same in my environment to access Azure key Vault with Private Endpoint:</strong></p>
<p>I have created <strong>Vnet</strong> with required configuration.</p>
<pre><code>Subnet and AddressSpace
</code></pre>
<p><strong>Azure Portal &gt; Virtual networks &gt; Create</strong></p>
<p><img src=""https://i.imgur.com/OJeGseC.png"" alt=""enter image description here"" /></p>
<p>Create <strong>key-vault</strong> with private endpoint.</p>
<p><strong>Azure Portal &gt; Key vaults &gt; Create a key vault</strong></p>
<blockquote>
<p>Note under Network section uncheck public access.</p>
</blockquote>
<p><img src=""https://i.imgur.com/CNW9DmP.png"" alt=""enter image description here"" /></p>
<p>once create the <strong>Keyvault</strong>, check the private endpoint provisioning status, like below.</p>
<p><img src=""https://i.imgur.com/h5Q0tEl.png"" alt=""enter image description here"" /></p>
<p>If you are trying to access the <strong>Azure Keyvault</strong> from public internet, you will get unauthorized error, like below.</p>
<p><img src=""https://i.imgur.com/L47u3oC.png"" alt=""enter image description here"" /></p>
<p><strong>Azure Keyvault</strong> is accessible with private network, like below.</p>
<p><img src=""https://i.imgur.com/uN6730d.png"" alt=""enter image description here"" /></p>
<p>For accessing <strong>Azure Datafactory</strong> using Azure keyvault, Assign service principal.</p>
<pre><code>Required Role: Key Vault Reader
</code></pre>
<p><strong>Keyvault</strong> access policy is assigned to ADF managed Identity.</p>
<pre><code>Ex: hellotestdata
</code></pre>
<p><img src=""https://i.imgur.com/KuhV8LY.png"" alt=""enter image description here"" /></p>
<p>You can add Azure Key vault as a linked service in the Azure Data factory. the managed identity of the ADF that has access to key vault can be used for connecting ADF to Azure Key vault like below.</p>
<p><img src=""https://i.imgur.com/uhjkMzm.png"" alt=""enter image description here"" /></p>
<p><strong>Azure Key</strong> Vault is successfully linked to <strong>ADF</strong>.</p>
<p><img src=""https://i.imgur.com/cywXlHk.png"" alt=""enter image description here"" /></p>
<p><strong>Reference:</strong>
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">Store credentials in Azure Key Vault</a></p>
"
"75293211","Which is the best method to push dataframe into sql server?","<p>I am very new to Azure, I need to know what is the best method to push my pandas dataframe of around 10gb into sql server and what services are better to handle 10-13 gb of data. Currently I am using Python Azure func, where I do some Etl and finally the clean Data of 10 gb needs to be pushed to sql server?</p>
<p>I am confused with the following points</p>
<ol>
<li><p>Is Azure Databricks is better than Azure function in terms of time and cost, to process 10GB-15GB Data?</p>
</li>
<li><p>For pushing data, what is the better method:
Pushing from python by using: <code>df.to_sql('products', conn, if_exists='replace', index = False)</code> or Using ADF Copy activity? -- Any other suggestions are welcome.</p>
</li>
<li><p>Should I choose PySpark over Python in Azure data brick, In order to have faster process?</p>
</li>
</ol>
","<python><sql-server><azure><pyspark><azure-data-factory>","2023-01-31 06:00:53","84","0","1","75294559","<p>As some of the comments have mentioned, it is faster to load data to SQL using Pyspark.</p>
<p>Code to load the pyspark dataframe to SQL</p>
<pre><code>from datetime import datetime
servername = &quot;jdbc:sqlserver://mytestserver.database.windows.net:1433&quot;
dbname = &quot;mydbname&quot;
url = servername + &quot;;&quot; + &quot;databaseName=&quot; + dbname + &quot;;&quot;
dbtable = &quot;Mytable&quot;
user = &quot;xxxxxxx&quot;
password = &quot;xxxxxxxxxxxxx&quot; # Please specify password here
 
def writeToSQL(df_raw):
# Adding an id column with unique values
    uuidUdf= udf(lambda : str(uuid.uuid4()),StringType())
    nowUdf= udf(lambda : int(time.time() * 1000),LongType())
    df_raw = df1 \
      .withColumn(&quot;id&quot;, uuidUdf()) \
      .withColumn(&quot;insertedAt&quot;, nowUdf()) \
 
    try:
        df_raw.write \
        .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \
        .mode(&quot;overwrite&quot;) \
        .option(&quot;url&quot;, url) \
        .option(&quot;dbtable&quot;, dbtable) \
        .option(&quot;user&quot;, user) \
        .option(&quot;password&quot;, password) \
        .save()
    except ValueError as error :
        print(&quot;Connector write failed&quot;, error)

df_raw=spark.createDataFrame(pandasDF)  #convert pandas to pyspark
starttime = datetime.utcnow()
print(&quot;Starting ingestion: &quot;, datetime.utcnow().strftime(&quot;%Y-%m-%d %H:%M:%S.%f&quot;))
writeToSQL(df_raw)
endtime = datetime.utcnow()
print(&quot;Finished ingestion: &quot;, datetime.utcnow().strftime(&quot;%Y-%m-%d %H:%M:%S.%f&quot;))
print(&quot;Time taken :&quot; + str(endtime-starttime))
</code></pre>
"
"75292273","How to undo action like ctrl+Z in azure data factory","<p>I am new to azure data factory.
By mistake I deleted for-each loop while developing pipeline.
How can I do  undo this ?</p>
","<azure-data-factory><undo>","2023-01-31 02:55:05","87","-1","1","75292491","<p>Is your ADF GIT enabled?
for GIT:
Right clicking on a commit, and clicking 'revert commit' reverses the selected change. Push it back to your remote git repository, and refresh your page on ADF to see the changes reflected.</p>
<p>If not, then there is no undo option in ADF
<a href=""https://i.stack.imgur.com/hPxIY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hPxIY.png"" alt=""enter image description here"" /></a></p>
<p>this would discard all the changes and not the latest one unlike CTRL+Z</p>
"
"75291430","ADLS Gen2 operation failed for: An error occurred while sending the request. User error 2011","<p>Hi I have the above error coming up when accessing storage container folder where I am trying to get the metadata of a folder and its files. It can't access the folders for some reason. checked linked service and storage container where public access is enabled and private end point is also set.</p>
<p>Please let me know what else is missing.</p>
","<azure-data-factory><azure-data-lake-gen2>","2023-01-31 00:00:18","154","0","1","75293306","<p>I tried to reproduce the error and got similar error.</p>
<p><img src=""https://i.imgur.com/Wb3NbS7.png"" alt=""enter image description here"" /></p>
<p><strong>The cause of error was the I am trying to access the ADLS gen 2 which is not available or present.</strong></p>
<p>After providing correct information I am successfully able to connect ADLS Gen 2</p>
<p><img src=""https://i.imgur.com/HPhwpDL.png"" alt=""enter image description here"" /></p>
"
"75289265","How to select object attribute in ADF using variable","<p>I'm trying to parametrize a pipeline in Azure Data Factory in order to enable a certain functionality to mulptiple environments. The idea is that the current environment is always available through a global parameter. I'd like to use this parameter to look up an array of environments to process data to. Example:</p>
<p><code>targetEnvs =  [{ &quot;dev&quot;: [&quot;dev&quot;], &quot;test&quot;: [&quot;dev&quot;, &quot;test&quot;], &quot;acc&quot;: [], &quot;prod&quot;: [&quot;acc&quot;, &quot;prod&quot;] }] </code></p>
<p>Then one should be able to select the targetEnv array with something like <code>targetEnvs[environment]</code> or targetEnvs.environment. Subsequently a ForEach is used to execute some logic on these target environments.</p>
<p>I tried setting this up with targetEnvs as a pipeline parameter (with default value mapping each env directly to targetEnv, as follows: {&quot;dev&quot;: [&quot;dev&quot;], &quot;test&quot;: [&quot;test&quot;]}) Then I have a Set variable step to take value from the targetEnvs parameter, as follows:<a href=""https://i.stack.imgur.com/LPqIO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LPqIO.png"" alt=""Set variable"" /></a>.</p>
<p>I'm now looking for a way to use the current environment (stored in a global parameter) instead of hardcoding &quot;dev&quot; in the Set Variable expression, but I'm not sure how to do this.</p>
<p><a href=""https://i.stack.imgur.com/6JjVj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6JjVj.png"" alt=""enter image description here"" /></a>.</p>
<p>Using this expression won't even start the pipeline.
<a href=""https://i.stack.imgur.com/lhm6B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lhm6B.png"" alt=""enter image description here"" /></a>.</p>
<p>Question: how do I select this attribute of the object? Any other suggestions on how to do tackle this problem are welcome as well!</p>
<p>(Python analogy would be to have a dictionary target_envs and taking a value from it by using the key &quot;current_env&quot;: target_envs[current_env].)</p>
","<azure-data-factory>","2023-01-30 19:33:24","137","2","1","75292663","<p>When I tried to access the object same as you, the same error occurred. I have taken the parameter <code>targetEnv</code> (given array) and global parameter <code>environment</code> with value as <strong>dev</strong>.</p>
<p><img src=""https://i.imgur.com/a7ncdta.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can use the following dynamic content to access the key value.</li>
</ul>
<pre><code>@pipeline().parameters.targetEnv[0][pipeline().globalParameters.environment]
</code></pre>
<p><img src=""https://i.imgur.com/qU7XLrN.png"" alt=""enter image description here"" /></p>
"
"75288517","ADF activity succeeded does not move on to the next activity","<p>I have a Web activity that performs a GET operation to an endpoint. I have set it up to do something On Failure (which works just fine) and another set of action upon Success. But this last part gets skipped entirely even though the output of the Web action is Succeeded.
Here is shown the succeeded status and my config
<a href=""https://i.stack.imgur.com/GekWZ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried removing and re-adding the On Success connector, but yielded no different results.</p>
","<azure><azure-data-factory>","2023-01-30 18:15:05","58","0","3","75290310","<p>Both dependencies of <code>set runtime status</code> cannot end as succeeded. You need to use a combination of skipped/completed states: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling</a></p>
"
"75288517","ADF activity succeeded does not move on to the next activity","<p>I have a Web activity that performs a GET operation to an endpoint. I have set it up to do something On Failure (which works just fine) and another set of action upon Success. But this last part gets skipped entirely even though the output of the Web action is Succeeded.
Here is shown the succeeded status and my config
<a href=""https://i.stack.imgur.com/GekWZ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried removing and re-adding the On Success connector, but yielded no different results.</p>
","<azure><azure-data-factory>","2023-01-30 18:15:05","58","0","3","75291345","<p>From the pipeline image  , I think it should move on with success .
Can you please cross check what is status code of web activity and is it returning something back ? Is the status code be 2xx ?  If I were you could have tried to check with the &quot;Upon Completion&quot; option and see it works .</p>
"
"75288517","ADF activity succeeded does not move on to the next activity","<p>I have a Web activity that performs a GET operation to an endpoint. I have set it up to do something On Failure (which works just fine) and another set of action upon Success. But this last part gets skipped entirely even though the output of the Web action is Succeeded.
Here is shown the succeeded status and my config
<a href=""https://i.stack.imgur.com/GekWZ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried removing and re-adding the On Success connector, but yielded no different results.</p>
","<azure><azure-data-factory>","2023-01-30 18:15:05","58","0","3","75292555","<p>Based on your flow, it would never proceed ahead:
<a href=""https://i.stack.imgur.com/uX7Mw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uX7Mw.png"" alt=""enter image description here"" /></a></p>
<p>Because as per your logic to proceed to the Set run time, Web activity 1 AND Web activity 2 should be success but that can never be the case since web activity 2 can be a success only if web activity 1 has failed thereby not proceeding further.</p>
<p>The below blog explains how to handle this :
<a href=""https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory/</a></p>
"
"75285128","How to transform a column into an array using ADF","<p>I need to read a column on a db on ADF and use all it´s values as parameters in a foreach.</p>
<p>I tried reading the column using a dataflow and a cache sink to then in a pipeline use Set Variable and then the foreach...but instead of an array of values I get an array with one value that contains all the others I want (but i cant iterate over)
I am using:
<code>@array(activity('myDataflow').output.runStatus.output.columName</code></p>
<p>Any help is appreciated, seems simple enough (column to array) but I am stuck</p>
","<database><azure><azure-data-factory>","2023-01-30 13:31:00","364","-1","2","75287745","<p>Use a lookup activity to get the data from SQL server, and run for each loop on the output of the lookup.
Example:</p>
<ol>
<li><p>Create a new pipeline</p>
</li>
<li><p>Add a lookup activity</p>
</li>
<li><p>Choose your source dataset (in this example, an Azure SQL database)</p>
</li>
<li><p>Remove the checkbox from “First row only”</p>
</li>
<li><p>Choose a table, stored procedure or type in a query</p>
<p>SELECT 1 AS result UNION ALL
SELECT 2 AS result UNION ALL
SELECT 3 AS result UNION ALL
SELECT 4 AS result
<a href=""https://i.stack.imgur.com/1AzQK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1AzQK.png"" alt=""lookup activity"" /></a></p>
</li>
<li><p>Add a foreach activity</p>
</li>
<li><p>In the foreach activity, under settings tab: “Items” -
@activity('Lookup SQL query').output.value – where 'Lookup SQL
query' is the name of the lookup activity
<a href=""https://i.stack.imgur.com/plXoO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/plXoO.png"" alt=""for each activity"" /></a></p>
</li>
<li><p>Inside the foreach loop, add a wait activity</p>
</li>
<li><p>In the settings tab, “Wait time in seconds” : @item().result .
item() is the current loop, and result is the name of the SQL column
<a href=""https://i.stack.imgur.com/nOBNv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nOBNv.png"" alt=""wait activity"" /></a></p>
</li>
</ol>
<p>debug the pipeline. You can see that the foreach activity iterates 4 times, for every row returned from the sql query.
<a href=""https://i.stack.imgur.com/VSATn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VSATn.png"" alt=""lookup activity results"" /></a></p>
"
"75285128","How to transform a column into an array using ADF","<p>I need to read a column on a db on ADF and use all it´s values as parameters in a foreach.</p>
<p>I tried reading the column using a dataflow and a cache sink to then in a pipeline use Set Variable and then the foreach...but instead of an array of values I get an array with one value that contains all the others I want (but i cant iterate over)
I am using:
<code>@array(activity('myDataflow').output.runStatus.output.columName</code></p>
<p>Any help is appreciated, seems simple enough (column to array) but I am stuck</p>
","<database><azure><azure-data-factory>","2023-01-30 13:31:00","364","-1","2","75293479","<p>You can use append variable activity also, inside <strong>ForEach</strong> after lookup.</p>
<p>First create an array variable in the pipeline.</p>
<p><img src=""https://i.imgur.com/p2qejdZ.png"" alt=""enter image description here"" /></p>
<p>Then use append variable activity inside ForEach and give</p>
<pre><code>@item.&lt;your_column_name&gt;
</code></pre>
<p><img src=""https://i.imgur.com/uSzvsb2.png"" alt=""enter image description here"" /></p>
<p>Result variable stored in a sample variable:</p>
<p><img src=""https://i.imgur.com/qOURsAF.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/TZgfO1n.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>I tried reading the column using a dataflow and a cache sink to then in a pipeline use Set Variable and then the foreach</p>
</blockquote>
<p>If you want to do it with dataflows instead of lookup, use the same above procedure and give the below dynamic content in the ForEach.</p>
<p><code>@activity('Data flow1').output.runStatus.output.sink1.value</code></p>
<p><img src=""https://i.imgur.com/2ltBU3Q.png"" alt=""enter image description here"" /></p>
"
"75285045","AzureDataFactory - 403 Forbidden With an Odata linked service","<p>Azure Data Factory
Linked Service : OData
Integration Runtime : Local network
Target : Microsoft Project Odata (hosted in the same local network)</p>
<p>I've got a 403 - Forbidden error while I try to reach my Microsoft Project Odata target..</p>
<p><a href=""https://i.stack.imgur.com/OzYmK.png"" rel=""nofollow noreferrer"">Odata Configuration</a></p>
<p><a href=""https://i.stack.imgur.com/sZ58v.png"" rel=""nofollow noreferrer"">Error Message</a></p>
<p>Whereas, it works when I do the same link with the Odata connector in Visual Studio (inside a SSAS Project for example).</p>
<p>I already test the network link beetween the server who host the integration runtime and my target point. It's ok.</p>
<p>Do you have an idea ?</p>
<p>Regards,</p>
<p>I already test the network link beetween the server who host the integration runtime and my target point. It's ok.</p>
","<azure-data-factory><project><olap>","2023-01-30 13:23:42","63","0","1","75291403","<p>Since you are getting the 403 (Forbidden Error) , so my best guess is that you are not authenticated .
I also see and the you are using domain\username for authentication with Windows authentication . I think you you should try out username@yourcompanyname.com and see if that works.</p>
"
"75279841","Why am I not be able to post json array from Azure Function activity at Azure Data Factory?","<p>I have created Azure Function activity on Azure Data Factory.
The Azure Function activity calls a function of Azure Function App (Python, App Service Plan).</p>
<p>The Python code looks like as follows:</p>
<pre class=""lang-py prettyprint-override""><code>  # ...
  def get_request(self, req: func.HttpRequest):
    request_body_json = req.get_json()
  # ...
</code></pre>
<p>When I pass a json array as request body (like <code>[{&quot;key1&quot;: &quot;value1&quot;}, {&quot;key2&quot;: &quot;value2&quot;}]</code>) of the Azure Function activity, <code>ValueError</code> occurs.</p>
<p>However, when I pass the same json array to Web activity, the error does not occur. The Web activity calls the same Python code as Azure Function activity.</p>
<p>Why am I not be able to post json array from Azure Function activity at Azure Data Factory?</p>
","<python><azure-functions><azure-data-factory>","2023-01-30 02:39:41","104","0","1","75280678","<p>I have reproduced in my environment and the below process worked for me:
Firstly, I have reproduced in web activity and I have succeeded in my attempt with your code as below:</p>
<p><img src=""https://i.imgur.com/gL3zHHv.png"" alt=""enter image description here"" /></p>
<p>When I tried with Function Activity i have got the error.</p>
<p>Function Activity in adf do not pass the body as Json it passes as Text. so, we need to use an array variable for that.</p>
<p>So, I have set a variable as below:</p>
<p><img src=""https://i.imgur.com/b3P3oy9.png"" alt=""enter image description here"" /></p>
<p>After Set Variable, then I have created an azure function activity as below:</p>
<pre><code>{

&quot;rithwik&quot;:@{variables('emo')}

}
</code></pre>
<p><img src=""https://i.imgur.com/nmXuhp6.png"" alt=""enter image description here"" /></p>
<p>Then in function code:</p>
<pre><code>import logging
import azure.functions as func

def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')
    request_body_json = req.get_json()
    return func.HttpResponse(f&quot;Received the message: {request_body_json['rithwik']}&quot;, status_code=200)
</code></pre>
<p><code>request_body_json['rithwik']</code> as it will print the Value.</p>
<p>Output:
<img src=""https://i.imgur.com/07nbkRj.png"" alt=""enter image description here"" /></p>
<p><strong>Reference:</strong></p>
<ul>
<li><p><a href=""https://learn.microsoft.com/en-us/answers/questions/923069/issue-while-passing-content-as-a-body-to-azure-fun"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/923069/issue-while-passing-content-as-a-body-to-azure-fun</a></p>
</li>
<li><p><a href=""https://learn.microsoft.com/en-us/answers/questions/917644/how-to-pass-string-output-of-set-varibale-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/917644/how-to-pass-string-output-of-set-varibale-activity</a></p>
</li>
</ul>
"
"75277701","i can't connect to input container. but the container is acccessible and the file is there","<p>I learning azure, specifically datafactory, so in a basic exercice.</p>
<p>1 - I should create a input container, and a output container (using azure sorage 2).</p>
<p>2 - After that, i created the datasets for input and output.</p>
<p>3 - And finally. I should connect the dataflow to my input dataset.</p>
<p>but</p>
<p>i can test conections on the datasets to prove that i created it without problems. but i cant test the connection on my dataflow to the input dataset.</p>
<p><a href=""https://i.stack.imgur.com/hO49h.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>i tryed</p>
<ul>
<li>recreating it with different names.</li>
<li>keep only the needed file in the storage</li>
<li>use different input file (i am using a sample similar to the &quot;movies.csv&quot; expected to the exercise.</li>
</ul>
","<azure><azure-data-factory>","2023-01-29 19:20:31","50","0","2","75281097","<p>I created azure blob container and uploaded file</p>
<p><img src=""https://i.imgur.com/PzrKeLo.png"" alt=""enter image description here"" /></p>
<p>I created linked service with azure storage account</p>
<p><img src=""https://i.imgur.com/ntrOU0z.png"" alt=""enter image description here"" /></p>
<p>I created a dataset with above linked service following below procedure:</p>
<p><img src=""https://i.imgur.com/k0X1zl3.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/scxD9aV.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/lqs2jeu.png"" alt=""enter image description here"" /></p>
<p>I tested the connection, it connected successfully.</p>
<p><img src=""https://i.imgur.com/RfA7OUC.png"" alt=""enter image description here"" /></p>
<p>I didn't get any error. The error which you mentioned above is related to dynamic content. If you assign any parameters in dataset provide the values of parameters correctly. I added parameters in dataset as below</p>
<p><img src=""https://i.imgur.com/4CIJeSK.png"" alt=""enter image description here"" /></p>
<p>I try to test the dataset I got error:</p>
<p><img src=""https://i.imgur.com/4F4kWdv.png"" alt=""enter image description here"" /></p>
<p>I added values for parameters in debug settings</p>
<p><img src=""https://i.imgur.com/hPl3S6s.png"" alt=""enter image description here"" /></p>
<p>Tested the connection, it connected successfully</p>
<p><img src=""https://i.imgur.com/2hN7bSe.png"" alt=""enter image description here"" /></p>
<p>Otherwise add the sink to the dataflow and try to debug it, it may work.</p>
"
"75277701","i can't connect to input container. but the container is acccessible and the file is there","<p>I learning azure, specifically datafactory, so in a basic exercice.</p>
<p>1 - I should create a input container, and a output container (using azure sorage 2).</p>
<p>2 - After that, i created the datasets for input and output.</p>
<p>3 - And finally. I should connect the dataflow to my input dataset.</p>
<p>but</p>
<p>i can test conections on the datasets to prove that i created it without problems. but i cant test the connection on my dataflow to the input dataset.</p>
<p><a href=""https://i.stack.imgur.com/hO49h.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>i tryed</p>
<ul>
<li>recreating it with different names.</li>
<li>keep only the needed file in the storage</li>
<li>use different input file (i am using a sample similar to the &quot;movies.csv&quot; expected to the exercise.</li>
</ul>
","<azure><azure-data-factory>","2023-01-29 19:20:31","50","0","2","75286605","<p>I think I found the solution.</p>
<p>when i am working with &quot;debug on&quot; and for some reason i create another &quot;data flow&quot;, you cant connect to the new datasets.</p>
<p>But</p>
<p>if I restart the debug (put off and on again), the connections start working again.</p>
"
"75275828","While creating a Dataset for ADF sometimes we import Schema and sometimes we not. What is the reason behind it?","<p>I am creating a dataset of Azure blob storage type. While watching youtube videos of ADF i saw sometimes we import schema and sometimes we not
can you please guide me when we have to click on import schema</p>
<p><a href=""https://i.stack.imgur.com/PGrE4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGrE4.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-01-29 14:56:05","68","0","2","75276984","<p>If you import schema you can easily map columns in the mapping tab of copy activity. If you don't import schema, your dataset can be generic, and you can use it with different tables, without the need to create a dataset for each table.</p>
"
"75275828","While creating a Dataset for ADF sometimes we import Schema and sometimes we not. What is the reason behind it?","<p>I am creating a dataset of Azure blob storage type. While watching youtube videos of ADF i saw sometimes we import schema and sometimes we not
can you please guide me when we have to click on import schema</p>
<p><a href=""https://i.stack.imgur.com/PGrE4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGrE4.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-01-29 14:56:05","68","0","2","75291456","<p>Just to add to what Chen said .
If you know that the your only focus is some few columns from source and there is a chance that your source can have few new columns in future , please go with mapping option . This saves from the any changes made on the source side .</p>
<p>If you want to make the pipeline more generic , please plan for going without mapping .</p>
"
"75274712","How to map Data Flow parameters to Sink SQL Table","<p>I need to store/map one or more data flow parameters to my Sink (Azure SQL Table).</p>
<p>I can fetch other data from a REST Api and is able to map these to my Sink columns (see below). I also need to generate some UUID's as key fields and add these to the same table.</p>
<p>I would like my EmployeeId column to contain my Data Flow Input parameter, e.g. named param_test. In addition to this I need to insert UUID's to other columns which are not part of my REST input fields.</p>
<p>How to I acccomplish that?</p>
<p><a href=""https://i.stack.imgur.com/UEGNd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UEGNd.png"" alt=""ADF Data Flow Mapping"" /></a></p>
","<azure-data-factory>","2023-01-29 12:01:27","103","0","2","75276733","<p>You need to use a derived column transformation, and there edit the expression to include the parameters.
<a href=""https://i.stack.imgur.com/NruUv.png"" rel=""nofollow noreferrer"">derived column transformation</a>
<a href=""https://i.stack.imgur.com/FhBHo.png"" rel=""nofollow noreferrer"">expression builder</a></p>
"
"75274712","How to map Data Flow parameters to Sink SQL Table","<p>I need to store/map one or more data flow parameters to my Sink (Azure SQL Table).</p>
<p>I can fetch other data from a REST Api and is able to map these to my Sink columns (see below). I also need to generate some UUID's as key fields and add these to the same table.</p>
<p>I would like my EmployeeId column to contain my Data Flow Input parameter, e.g. named param_test. In addition to this I need to insert UUID's to other columns which are not part of my REST input fields.</p>
<p>How to I acccomplish that?</p>
<p><a href=""https://i.stack.imgur.com/UEGNd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UEGNd.png"" alt=""ADF Data Flow Mapping"" /></a></p>
","<azure-data-factory>","2023-01-29 12:01:27","103","0","2","75281557","<p>Adding to @<a href=""https://stackoverflow.com/users/21079004/chen-hirsh"">Chen Hirsh</a>, <strong>use the same derived column to get uuid values to the columns after REST API Source.</strong></p>
<p><img src=""https://i.imgur.com/ZgILfCM.png"" alt=""enter image description here"" /></p>
<p>They will come into sink mapping:</p>
<p><img src=""https://i.imgur.com/Nh4Nwem.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/HVYsLqs.png"" alt=""enter image description here"" /></p>
"
"75265279","How to export Adf pipelines,dataflows,datasets into Azure Synapse","<p>I have 100 dataflows ,50 pipelines and their related datasets variables etc.
Now i want to use Synapse service and want my all pipelines amd stuff of ADF into Synapse. My Adf is git configured
Can we export them in one go??</p>
","<azure><azure-data-factory><azure-synapse>","2023-01-28 04:01:56","319","0","2","75285010","<p>As far as I know, there is no way to do this out of the box.
There is an option to download the object's JSON and then use PowerShell to recreate the objects in Synapse workspace.
I haven't tried that myself, but the process is explained <a href=""https://learn.microsoft.com/en-us/answers/questions/403992/azure-data-factory-v2-arm-template-to-synpase-data?childtoview=404618#answer-404618"" rel=""nofollow noreferrer"">here</a>.</p>
"
"75265279","How to export Adf pipelines,dataflows,datasets into Azure Synapse","<p>I have 100 dataflows ,50 pipelines and their related datasets variables etc.
Now i want to use Synapse service and want my all pipelines amd stuff of ADF into Synapse. My Adf is git configured
Can we export them in one go??</p>
","<azure><azure-data-factory><azure-synapse>","2023-01-28 04:01:56","319","0","2","75296132","<p>To move data from data factory to synapse we need to download the supported module files in ADF.</p>
<p><img src=""https://i.imgur.com/KczBnx5.png"" alt=""enter image description here"" /></p>
<p>I tried to migrate data from ADF to Synapse workspace using PowerShell following below procedure:</p>
<p>I connected to Azure account using below command:</p>
<pre><code>Connect-AzAccount
</code></pre>
<p>I set my subscription using below command:</p>
<pre><code>Select-AzSubscription -SubscriptionName &quot;&lt;SubscriptionName&gt;&quot;
</code></pre>
<p><img src=""https://i.imgur.com/gMiPneG.png"" alt=""enter image description here"" /></p>
<p>I created a linked service for source and sink using below comand:</p>
<pre><code>Set-AzSynapseLinkedService -WorkspaceName &lt;synapseworkspace&gt; -Name &lt;linkedservicename&gt; -DefinitionFile &quot;&lt;json file path&gt;&quot;
</code></pre>
<p>I created dataset for source and sink in synapse workspace using below command:</p>
<pre><code>Set-AzSynapseDataset  -WorkspaceName &lt;synapseworkspace&gt; -Name &lt;datasetname&gt; -DefinitionFile &quot;&lt;json file path&gt;&quot;
</code></pre>
<p>I created pipeline in synapse workspace using below command:</p>
<pre><code>Set-AzSynapsePipeline -WorkspaceName &lt;synapseworkspace&gt; -Name &lt;pipelinename&gt; -DefinitionFile &quot;&lt;json file path&gt;&quot;
</code></pre>
<p>They are created successfully in synapse workspace:</p>
<p><img src=""https://i.imgur.com/lVSRK73.png"" alt=""enter image description here"" /></p>
<p>In your case you are having bulk data in your data factory, you can use Azure Data Factory to Synapse Analytics Migration Tool.</p>
<p>You can migrate Azure Data Factory pipelines, datasets, linked service, integration runtime and triggers to a Synapse Workspace through below PowerShell script:</p>
<pre><code>  .\importADFtoSynapseTool.ps1 -sourceADFResourceId &quot;/subscriptions/&lt;SubscriptionID&gt;/resourcegroups/&lt;ADFResourceGroupName&gt;/providers/Microsoft.DataFactory/factories/&lt;ADFName&gt;&quot; -destSynapseResourceId &quot;/subscriptions/&lt;SubscriptionID&gt;/resourcegroups/&lt;SynapseResourceGroupName&gt;/providers/Microsoft.Synapse/workspaces/&lt;SynapseName&gt;&quot; -TenantId &lt;tenantId&gt;
</code></pre>
<ul>
<li>Resource group name for Azure Data Factory</li>
<li>Resource group name for Azure Synapse Workspace</li>
<li>Subscription ID</li>
<li>Tenant ID (you can find this by click Azure Active Directory in the Azure Portal)</li>
</ul>
<p>For more clarification you can visit <a href=""https://github.com/Azure-Samples/Synapse/tree/main/Pipelines/ImportADFtoSynapse"" rel=""nofollow noreferrer"">this</a>.</p>
"
"75264485","passing the parameters from adf to notebook","<p>Im ingesting a data with the api calls and would like to <code>widgets</code> to parametirze. In azure I have the following set up:
<a href=""https://i.stack.imgur.com/EFGdS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EFGdS.png"" alt=""enter image description here"" /></a></p>
<p>I have the list of <code>attribute_code</code>s, reading them with <code>lookup</code> activtiy and passing these parameter inside the databricks notebook code. Code inside the databricks:</p>
<pre><code>data, response = get_data_url(url=f&quot;https://p.cloud.com/api/rest/v1/attributes/{attribute_code}/options&quot;,access_token=access_token)
#Removing the folder in Data Lake
dbutils.fs.rm(f'/mnt/bronze/attribute_code/{day}',True)
#Creating the folder in the Data Lake
dbutils.fs.mkdirs(f'/mnt/bronze/attribute_code/{day}')

count = 0
#Putting the response inside of the Data Lake folder
dbutils.fs.put(f'/mnt/bronze/attribute_code/{day}/data_{count}.json', response.text)
</code></pre>
<p>My problem is that, since its in the <code>ForEach</code> loop, eveytime new parameter is passed, it deletes the entire folder with previosly, loaded data. Now someone can come and say to remove line where I drop and create the spacific daily folder but pipeline should run multiple times a day and I need to drop previously loaded data on that day and load new one.</p>
<p>My goal is to iterte over the entire list of the <code>attribute_code</code> and load them all in one folder with the name &quot;data_{count}.json</p>
","<azure><foreach><parameters><azure-data-factory>","2023-01-28 00:07:19","155","0","1","75283071","<ul>
<li><p>Instead of using <code>dbutils.fs.rm</code> in your notebook, you can use delete activity before for each activity to get desired results.</p>
</li>
<li><p>Using <code>dbutils.fs.rm</code>, the folder is being deleted each time the notebook is triggered inside for each loop deleting previously created files as well.</p>
</li>
<li><p>So, using a delete activity only before for each loop to delete the folder (deletes only if it exists), you can load data as per requirement.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/CIo9oG5.png"" alt=""enter image description here"" /></p>
<ul>
<li>For path, I have used the following dynamic content:</li>
</ul>
<pre><code>attribute/@{formatDateTime(utcNow(),'yyyy-MM-dd')}
</code></pre>
<p><img src=""https://i.imgur.com/sdW1yQC.png"" alt=""enter image description here"" /></p>
<ul>
<li>And using the following code in my databricks notebook:</li>
</ul>
<pre><code>#I used similar code
data, response = get_data_url(url=f&quot;https://p.cloud.com/api/rest/v1/attributes/{attribute_code}/options&quot;,access_token=access_token)


#Creating the folder in the Data Lake
dbutils.fs.mkdirs(f'/mnt/bronze/attribute_code/{day}')

count = 0
#Putting the response inside of the Data Lake folder
dbutils.fs.put(f'/mnt/bronze/attribute_code/{day}/data_{count}.json', response.text)
</code></pre>
<ul>
<li>Lets say I have the following output from my look up activity:</li>
</ul>
<p><img src=""https://i.imgur.com/PPItpDs.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I run the pipeline, it would run successfully. Only the latest look up data would be loaded.</li>
</ul>
<p><img src=""https://i.imgur.com/oINRpMj.png"" alt=""enter image description here"" /></p>
"
"75256322","How to get modified date as column in table while ingesting all files from year/month/date directories of storage account?","<p>I have some json files in ADLS account. The files are ingested in multiple Year/Month/Day directory structure. I want to copy all the files from ADLS to Azure SQL DB using azure data flow.<br />
I am able to ingest the data from using data flow but I want to include the file path, file ingestion date along with the file name in three separate columns but I do not know how to get these values.</p>
<p><strong>Please note that each Day directory has more than one file as following:</strong></p>
<p><code>container_name/Dataset/Year/Month/Day/file1.json.file2.json,file3.json </code></p>
<p>Could any one help me , how do I ingest the modified date column in table with data of each files</p>
<p>tried using getmedata to copy each file on by one also in dataflow derived column for any modified date</p>
","<azure><azure-data-lake><azure-data-factory>","2023-01-27 09:42:21","131","0","1","75266265","<p>I have reproduced the above and able to get the desired file by using combination of <strong>addional column option in copy activity</strong>, <strong>lookup</strong> and <strong>Get Meta data activity</strong>.</p>
<p>In this these are my datasets which I have used at various activities with dataset parameters.</p>
<p><strong>Source_files_wild_path:</strong></p>
<p><img src=""https://i.imgur.com/dwgJcX7.png"" alt=""enter image description here"" /></p>
<p><strong>temporary_filepaths:</strong></p>
<p><img src=""https://i.imgur.com/7b9skIS.png"" alt=""enter image description here"" /></p>
<p><strong>Each_file:</strong></p>
<p><img src=""https://i.imgur.com/gFjg6rl.png"" alt=""enter image description here"" /></p>
<p><strong>intermediate:</strong></p>
<p><img src=""https://i.imgur.com/P2TJ7F9.png"" alt=""enter image description here"" /></p>
<p><strong>target_folder:</strong></p>
<p><img src=""https://i.imgur.com/fsftuTi.png"" alt=""enter image description here"" /></p>
<p><strong>AFAIK</strong>, in ADF we can get the last modified date of files either by <strong>REST APIs or Get Meta data</strong>. But Get Meta data won't work with dynamic file paths with a folder structure like yours.</p>
<p>Also, we can get the file path of a blob file either <strong>from triggers or additonal column option of copy activity only</strong>. Here, as there is no usage of triggers, I have used the 2nd method.</p>
<ul>
<li><p>So, First I have used a <strong>copy activity</strong> with wild card path to all source files and added <strong><code>$$FILEPATH</code></strong> as column and copied to a temporary file <code>temp1.csv</code> with <code>Merge files</code> as copy behavior.</p>
</li>
<li><p>Then I have used a <strong>lookup activity</strong> to <code>temp1.csv</code> to get the file as array of objects by which I can get the file paths list.</p>
</li>
<li><p>Here I have created two variables of array type.</p>
<p><img src=""https://i.imgur.com/uZHSY1l.png"" alt=""enter image description here"" /></p>
</li>
<li><p>As it is lookup output is an array objects, to get only the <code>filename</code> object array, use a for loop and append the <code>@item().filepath</code> to <code>path_list</code> array.</p>
</li>
<li><p>Then use the below expression to get the unique list of all file paths in <code>unique_path_list</code> array.</p>
<p><code>@union(variables('path_list'),variables('path_list'))</code></p>
</li>
<li><p>Now, use this array in a ForEach and inside <strong>Foreach</strong>, use a <strong>Get Meta data activity</strong> with <code>each_file</code> dataset and <code>@item()</code> as filename and add the <strong>filedsList</strong> like <code>Item name</code> and <code>Last modified</code>.</p>
</li>
<li><p>Then use <strong>copy activity</strong> inside Foreach, and use the same dataset. Here add the additional columns like filename, filepath and last modified and give those values.</p>
</li>
<li><p>In sink of this copy activity use another temporary folder and staging(dataset <code>intermediate</code>). give random file name using date function.</p>
</li>
<li><p>After ForEach, use another copy activity with <strong><code>intermediate</code> dataset as source</strong>(use wild card path <code>*.csv</code> and give any empty string to dataset parameter) and <code>target_folder</code> folder as sink to get the result file by using merge files.</p>
</li>
</ul>
<p><strong>My pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;last_modifed_pipeline_copy1&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;for_paths_columns&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;additionalColumns&quot;: [
                        {
                            &quot;name&quot;: &quot;filepath&quot;,
                            &quot;value&quot;: &quot;$$FILEPATH&quot;
                        }
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;wildcardFolderPath&quot;: &quot;*/*/*&quot;,
                        &quot;wildcardFileName&quot;: &quot;*.csv&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;,
                        &quot;copyBehavior&quot;: &quot;MergeFiles&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                        &quot;quoteAllText&quot;: true,
                        &quot;fileExtension&quot;: &quot;.txt&quot;
                    }
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;Source_files_wild_card_path&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;temporary_filepaths&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        },
        {
            &quot;name&quot;: &quot;Lookup1&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;for_paths_columns&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;temporary_filepaths&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;append filepaths array&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Lookup1').output.value&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;path_list&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@item().filepath&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;get_unique_paths array&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;append filepaths array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;unique_path_list&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@union(variables('path_list'),variables('path_list'))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;adds_last modifed column&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;get_unique_paths array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('unique_path_list')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Get Metadata1&quot;,
                        &quot;type&quot;: &quot;GetMetadata&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;dataset&quot;: {
                                &quot;referenceName&quot;: &quot;Each_file&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                &quot;parameters&quot;: {
                                    &quot;filename&quot;: {
                                        &quot;value&quot;: &quot;@item()&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            },
                            &quot;fieldList&quot;: [
                                &quot;itemName&quot;,
                                &quot;lastModified&quot;
                            ],
                            &quot;storeSettings&quot;: {
                                &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                &quot;enablePartitionDiscovery&quot;: false
                            },
                            &quot;formatSettings&quot;: {
                                &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Copy data2&quot;,
                        &quot;type&quot;: &quot;Copy&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Get Metadata1&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;source&quot;: {
                                &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                &quot;additionalColumns&quot;: [
                                    {
                                        &quot;name&quot;: &quot;file_path&quot;,
                                        &quot;value&quot;: &quot;$$FILEPATH&quot;
                                    },
                                    {
                                        &quot;name&quot;: &quot;file_name&quot;,
                                        &quot;value&quot;: {
                                            &quot;value&quot;: &quot;@activity('Get Metadata1').output.itemName&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    },
                                    {
                                        &quot;name&quot;: &quot;last_modifed&quot;,
                                        &quot;value&quot;: {
                                            &quot;value&quot;: &quot;@activity('Get Metadata1').output.lastModified&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                ],
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                    &quot;recursive&quot;: true,
                                    &quot;enablePartitionDiscovery&quot;: false
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                }
                            },
                            &quot;sink&quot;: {
                                &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                                &quot;storeSettings&quot;: {
                                    &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                },
                                &quot;formatSettings&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                                    &quot;quoteAllText&quot;: true,
                                    &quot;fileExtension&quot;: &quot;.txt&quot;
                                }
                            },
                            &quot;enableStaging&quot;: false,
                            &quot;translator&quot;: {
                                &quot;type&quot;: &quot;TabularTranslator&quot;,
                                &quot;typeConversion&quot;: true,
                                &quot;typeConversionSettings&quot;: {
                                    &quot;allowDataTruncation&quot;: true,
                                    &quot;treatBooleanAsNumber&quot;: false
                                }
                            }
                        },
                        &quot;inputs&quot;: [
                            {
                                &quot;referenceName&quot;: &quot;Each_file&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                &quot;parameters&quot;: {
                                    &quot;filename&quot;: {
                                        &quot;value&quot;: &quot;@item()&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            }
                        ],
                        &quot;outputs&quot;: [
                            {
                                &quot;referenceName&quot;: &quot;intermediate&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                &quot;parameters&quot;: {
                                    &quot;file_name&quot;: {
                                        &quot;value&quot;: &quot;@concat(utcNow(),'.csv')&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    }
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Copy data3&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;adds_last modifed column&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;wildcardFileName&quot;: &quot;*.csv&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;,
                        &quot;copyBehavior&quot;: &quot;MergeFiles&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                        &quot;quoteAllText&quot;: true,
                        &quot;fileExtension&quot;: &quot;.txt&quot;
                    }
                },
                &quot;enableStaging&quot;: false,
                &quot;translator&quot;: {
                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                    &quot;typeConversion&quot;: true,
                    &quot;typeConversionSettings&quot;: {
                        &quot;allowDataTruncation&quot;: true,
                        &quot;treatBooleanAsNumber&quot;: false
                    }
                }
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;intermediate&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;,
                    &quot;parameters&quot;: {
                        &quot;file_name&quot;: &quot;No value&quot;
                    }
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;target_folder&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        }
    ],
    &quot;variables&quot;: {
        &quot;path_list&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;unique_path_list&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: [],
    &quot;lastPublishTime&quot;: &quot;2023-01-27T12:40:51Z&quot;
},
&quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p><strong>My pipeline:</strong></p>
<p><img src=""https://i.imgur.com/Vl3v4GM.png"" alt=""enter image description here"" /></p>
<p><strong>Result file:</strong></p>
<p><img src=""https://i.imgur.com/v266vGg.png"" alt=""enter image description here"" /></p>
<p><em><strong>NOTE:</strong></em></p>
<p>If you want run this on a regular basis, use Storage event trigger by which you can use trigger parameters like <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code>. you can give these to Get Meta data to get last modified time and then pass it to copy activity or dataflow to add as additonal column as per your requirement.</p>
"
"75255663","Invoking Web Activity failed with HttpStatusCode","<p>I got ADF that triggers Azure Functions by posting on HTTP url. After last support changes - function has been downgraded from v6 to v4 I got:</p>
<pre><code>Invoking Web Activity failed with HttpStatusCode - '404 : NotFound', 
message - 'The requested resource does not exist on the server. Please verify the request server and retry'
</code></pre>
<p>Could it be somehow connected with Storage Accounts changes or Networking?</p>
<p>EDIT:
Also, I have opened it in vscode, in bin directory I have something like this:
<a href=""https://i.stack.imgur.com/2PlRD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2PlRD.png"" alt=""enter image description here"" /></a></p>
","<azure-functions><azure-data-factory>","2023-01-27 08:33:36","150","0","1","75256377","<p>I have reproduced the above and got same error.</p>
<p><a href=""https://i.stack.imgur.com/oOVtm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oOVtm.png"" alt=""enter image description here"" /></a></p>
<p>My Sample function name is <code>HttpTrigger1</code> and I got the above error when I gave it as <code>HttpTrigger2</code>.</p>
<p>Recheck the function name in the web activity URL.</p>
<p><a href=""https://i.stack.imgur.com/2DRh0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2DRh0.png"" alt=""enter image description here"" /></a></p>
<p>Here, for sample demo I have used Get.</p>
<p><strong>Web activity executed successfully</strong></p>
<p><a href=""https://i.stack.imgur.com/S6WJM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S6WJM.png"" alt=""enter image description here"" /></a></p>
"
"75253446","ADF Dataflow stuck IN progress and fail with below errors","<p>ADF Pipeline DF task is Stuck in Progress. It was working seamlessly last couple of months but suddenly Dataflow stuck in progress and Time out after certain time. We are using IR managed Virtual Network. I am using forereach loop to run data flow for multiple entities parallel, it always randomly get stuck on last Entity.</p>
<p>What can I try to resolve this?</p>
<p>Error in Dev Environment</p>
<pre><code>Error Code 4508
Spark cluster not found
</code></pre>
<p>Error in Prod Environment:</p>
<pre><code>Error code
5000
Failure type
User configuration issue
Details
[plugins.*** ADF.adf-ir-001 WorkspaceType:&lt;ADF&gt; CCID:&lt;f289c067-7c6c-4b49-b0db-783e842a5675&gt;] [Monitoring] Livy Endpoint=[https://hubservice1.eastus.azuresynapse.net:8001/api/v1.0/publish/815b62a1-7b45-4fe1-86f4-ae4b56014311]. Livy Id=[0] Job failed during run time with state=[dead].
</code></pre>
<p>Images:</p>
<p><img src=""https://i.stack.imgur.com/rgSOM.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/frCjT.png"" alt=""enter image description here"" /></p>
<p>I tried below steps:</p>
<ol>
<li>By changing IR configuring as below</li>
</ol>
<p><img src=""https://i.stack.imgur.com/bF2IL.png"" alt=""enter image description here"" /></p>
<ol start=""2"">
<li>Tried DF Retry and retry Interval</li>
</ol>
<p><img src=""https://i.stack.imgur.com/ClMWo.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>Also, tried For each loop one batch at a time instead of 4 batch parallel. None of the above trouble-shooting steps worked. These PL is running last 3-4 months without a single failure, suddenly they started to fail last 3 days consistently. DF flow always stuck in progress randomly for different entity and times out in one point by throwing above errors.</li>
</ol>
","<azure-data-factory>","2023-01-27 02:00:15","193","0","1","75257642","<blockquote>
<p>Error Code 4508 Spark cluster not found.</p>
</blockquote>
<p>This error can cause because of two reasons.</p>
<ul>
<li>The debug session is getting closed till the dataflow finish its transformation in this case recommendation is to restart the debug session
<img src=""https://i.imgur.com/XkJVJKH.png"" alt=""enter image description here"" /></li>
<li>the second reason is due to resource problem, or an outage in that particular region.</li>
</ul>
<blockquote>
<p>Error code 5000 Failure type User configuration issue Details [plugins.*** ADF.adf-ir-001 WorkspaceType: CCID:] [Monitoring] Livy Endpoint=[https://hubservice1.eastus.azuresynapse.net:8001/api/v1.0/publish/815b62a1-7b45-4fe1-86f4-ae4b56014311]. Livy Id=[0] Job failed during run time with state=[dead].</p>
</blockquote>
<ul>
<li>A temporary error is one that says &quot;Livy job state dead caused by unknown error.&quot; At the backend of the dataflow, a spark cluster is used, and this error is generated by the spark cluster. to get the more information about error go to <code>StdOut</code> of sparkpool execution.</li>
<li>The backend cluster may be experiencing a network problem, a resource problem, or an outage.</li>
</ul>
<p>If error persist my suggestion is to raise Microsoft support ticket <a href=""https://azure.microsoft.com/en-us/support/create-ticket"" rel=""nofollow noreferrer"">here</a></p>
"
"75248657","Azure Synapse Copy Data from BigQuery, Source ERROR [HY000] [Microsoft][BigQuery] (131) Unable to authenticate with Google BigQuery Storage API","<p>I am getting this error at the Source tab at the Use query (Table, Query) Query, when doing a copy data activity at the Azure Synapse pipeline.</p>
<p>Unable to authenticate with Google BigQuery Storage API:</p>
<p><a href=""https://i.stack.imgur.com/v3o5S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v3o5S.png"" alt=""Unable to authenticate with Google BigQuery Storage API"" /></a>
.</p>
<p>The strange thing is I can preview data at the Source dataset, I can also preview data when select the Use query Table option.</p>
<p><a href=""https://i.stack.imgur.com/H71QE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H71QE.png"" alt=""can preview table"" /></a></p>
<p>I can even run query to select the table's schema</p>
<p><a href=""https://i.stack.imgur.com/uHw2u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uHw2u.png"" alt=""can query schema"" /></a></p>
<pre><code>SELECT
*
FROM
`3082`.INFORMATION_SCHEMA.TABLES
WHERE table_type = 'BASE TABLE'
</code></pre>
<p>but I get this authentication error when selecting columns</p>
<pre><code>SELECT
*
FROM
`3082.gcp_billing_export_v1_019F74_6EA5E8_C96548`;
</code></pre>
","<google-bigquery><google-api><azure-data-factory><azure-synapse>","2023-01-26 16:01:58","137","0","1","75305412","<blockquote>
<p>ERROR [HY000] [Microsoft][BigQuery] (131) Unable to authenticate with Google BigQuery Storage API. Check your account permissions</p>
</blockquote>
<p>The above error is due to issue in authentication of BigQuery Storage API. The permission required to access data from BigQuery are,</p>
<ul>
<li>bigquery.readsessions.create</li>
<li>bigquery.readsessions.getData</li>
<li>bigquery.readsessions.update</li>
</ul>
<p>The role <em><strong>BigQuery User</strong></em> will help in giving above permissions.</p>
<p><strong>Reference:</strong></p>
<ol>
<li><strong>Google cloud doc</strong> on  <a href=""https://cloud.google.com/bigquery/docs/access-control#bigquery.user"" rel=""nofollow noreferrer"">Access Control - BigQuery User</a>.</li>
<li><strong>MS doc</strong> on <a href=""https://learn.microsoft.com/en-us/power-query/connectors/google-bigquery#unable-to-authenticate-with-google-bigquery-storage-api"" rel=""nofollow noreferrer"">Google BigQuery connector issue</a></li>
</ol>
"
"75245231","Getting an error while copying data from one folder to another in Azure Data Factory","<p>This query used to work in Azure Data Factory pipeline but stopped working few days ago. Nothing changed in case of file names/ formats etc in Azure Blob storage. Getting error in this line:</p>
<pre><code>SELECT * FROM OPENROWSET ( 
  BULK 
'/filepath.csv@snapshot=*', FORMAT = 'CSV' 
) 
</code></pre>
<p>The error says .csv@snapshot=* has URL suffix which is not allowed.
Full code:</p>
<pre><code>-- CREATE OR REPLACE VIEW clean.barriers AS 
IF EXISTS (SELECT * FROM sys.tables t 
    JOIN sys.schemas s ON (t.schema_id = s.schema_id) 
    WHERE s.name = 'clean' AND t.name = 'barriers') 
EXEC('DROP EXTERNAL TABLE [clean].[barriers]')  

CREATE EXTERNAL TABLE [clean].[barriers]
WITH 
( 
   LOCATION = 'clean/synapse/barriers',
   DATA_SOURCE = &quot;&quot;, 
   FILE_FORMAT = [SynapseParquetFormat] 
) 
AS  

SELECT * FROM OPENROWSET ( 
  BULK 
'/filepath.csv@snapshot=*', FORMAT = 'CSV' 
) 
WITH( 
-- Schema adjusted to what we have in clean/barriers in Bigquery 
mshp_id INT, 
prog_name NVARCHAR(256), 
barrier_name NVARCHAR(256), 
days INT 
) AS load_clean_data
</code></pre>
","<sql><azure><azure-data-factory><azure-synapse>","2023-01-26 11:01:17","89","0","1","75321887","<p>As per the <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=sql-server-ver16#e-use-create-external-table-as-select-from-delta-table-to-parquet"" rel=""nofollow noreferrer"">Official Documentation</a>, you should have a Data source for the source file also from which you are trying to copy the data.</p>
<p><img src=""https://i.imgur.com/mmMGFjw.png"" alt=""enter image description here"" /></p>
<p>So, try to create a data source for the source CSV file and check, it may work.</p>
<p>Also, as you are executing the above script using ADF, first try to execute it without ADF and if the error occurs then problem can be with the script not ADF. If not try to change the activity of ADF and check.</p>
<p>You can try this trouble shoot also in your BULK path. As you want the data from that csv files folder give the path like below and check.</p>
<pre class=""lang-sql prettyprint-override""><code>/folder/*.csv
</code></pre>
"
"75244728","Handling Null Values in ADF Pipeline","<p>I am performing an incremental load using timestamp as watermark column. I have few null values in the date column in my source. When I am replicating the data using copy activity, only rows whose date column is not null are getting copied, but I want to copy all other rows too where date column has null values.</p>
<p>I have tried using IsNull and Coalesce in the lookup activites but that does not work . is there a way where I can handle those null values in ADF</p>
","<null><azure-data-factory><coalesce><isnull><incremental-load>","2023-01-26 10:13:25","217","0","1","75253975","<p>If you are not replacing null watermark values in source table and copying the data as is, it is not incremental load. For every pipeline run, same rows with null value will be copied. If null values data can be copied in every run, you can use the same query. In that query, change <code>(@{item().WaterMark_Column}= null)</code> as <code>(@{item().WaterMark_Column} is null)</code> .</p>
<p><strong>Corrected Code:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>select * from @{item().TABLE_NAME} where 
(@{item().WaterMark_Column} &gt;= '@{activity('Oldwatermark').output.firstRow.WatermarkValue}'and
@{item().WaterMark_Column} &lt;= '@{activity('Newwatermark').output.firstRow.NewWatermarkvalue}') 
or (@{item().WaterMark_Column} is null)
</code></pre>
<p>Reference: <strong>MS document</strong> on <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/is-null-transact-sql?view=sql-server-ver16#remarks"" rel=""nofollow noreferrer"">IS NULL or IS NOT NULL instead of comparison operators</a></p>
"
"75243872","Install azure integration runtime on linux and move data from Linux to Azure blob storage","<p>I want to move data from <strong>Linux RHEL 8.5</strong> to <strong>Azure blob storage</strong>.
I have a server of <strong>postgreSQL</strong> running on that linux vm and I want to move some table periodically from vm to azure blob storage.
After reading their official documentation, Azure IR only support windows environment.
Is there any way that I can use <strong>Azure data factory</strong> to move data from <strong>linux RHEL 8.5</strong> to Azure blob storage?</p>
","<azure><azure-blob-storage><azure-data-factory><redhat><rhel>","2023-01-26 08:48:13","33","0","1","75244217","<p>Azure data factory has a connector to PostgreSQL. If your server can be connected from the internet, then you can use data factory directly. If your server is behind a firewall, you need to install self hosted IR, which you will need to install on another (windows) server, in the same network of your PostgreSQL server.</p>
"
"75243392","Transform JSON Data of one column from excel csv in Azure data factory","<p>Have an excel csv as a source in that one particular entity (500)  contains JSON formatted data with hierarchy like below</p>
<pre class=""lang-json prettyprint-override""><code>500 7   30600052764 30128903357 {&quot;type&quot;: &quot;reportingData&quot;, &quot;merStoreId&quot;: &quot;0099703&quot;, &quot;productData&quot;: [{&quot;productCode&quot;: &quot;107&quot;, &quot;totalAmount&quot;: &quot;47.92&quot;, &quot;quantity&quot;: &quot;1.000&quot;, &quot;unitPrice&quot;: &quot;47.92&quot;, &quot;tax1Amount&quot;: &quot;0.00&quot;},{&quot;productCode&quot;: &quot;963&quot;, &quot;totalAmount&quot;: &quot;2.40&quot;, &quot;quantity&quot;: &quot;1.000&quot;, &quot;unitPrice&quot;: &quot;2.40&quot;, &quot;tax1Amount&quot;: &quot;0.00&quot;}, {&quot;productCode&quot;: &quot;913&quot;, &quot;totalAmount&quot;: &quot;20.00&quot;, &quot;quantity&quot;: &quot;1.000&quot;, &quot;unitPrice&quot;: &quot;20.00&quot;, &quot;tax1Amount&quot;: &quot;0.00&quot;}]}
</code></pre>
<p>Has to convert this data in excel csv to my sink in meaningful format , can any one help me</p>
","<json><excel><csv><azure-data-factory>","2023-01-26 07:53:04","129","-1","1","75255302","<p>To unroll the Json column as you want you can follow below steps in Data flow activity:</p>
<ul>
<li><p>Sample data at source
<img src=""https://i.imgur.com/LeXW4wq.png"" alt=""enter image description here"" /></p>
</li>
<li><p>As it is taking the Json column as string first, I took Parse transformation and unfold the Json. In parse setting select <strong>Document form as <code>single document</code> and column as name of column you want to give Expression as column you want to unfold and expression as</strong></p>
</li>
</ul>
<pre><code>(type as string,        merStoreId as integer,      productData as string[])
</code></pre>
<p><img src=""https://i.imgur.com/WEgfpzb.png"" alt=""enter image description here"" />
<strong>Data preview:</strong>
<img src=""https://i.imgur.com/sS0QZvF.png"" alt=""enter image description here"" /></p>
<ul>
<li>After that create derived column transformation and create column with the respective value of column unfolded in previous transformation
<img src=""https://i.imgur.com/a4LyHaR.png"" alt=""enter image description here"" />
<strong>Data preview:</strong>
<img src=""https://i.imgur.com/TvhIb5l.png"" alt=""enter image description here"" /></li>
</ul>
<p>-As you have one object with array in Json column to unfold that take flatten transformation and flattern that array column. <img src=""https://i.imgur.com/ZCNOrYE.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, unfold that array Json in another parse transformation parse setting select <strong>Document form as <code>single document</code> and column as name of column you want to give Expression as column you want to unfold and expression as</strong></li>
</ul>
<pre><code>(productCode as integer,        totalAmount as double,      quantity as double,     unitPrice as double,        tax1Amount as double)
</code></pre>
<p><img src=""https://i.imgur.com/LHilO8L.png"" alt=""enter image description here"" />
<strong>Data preview:</strong>
<img src=""https://i.imgur.com/ZKosDgA.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>After that create derived column transformation and create column with the respective value of column unfolded in previous transformation
<img src=""https://i.imgur.com/2eUKzYQ.png"" alt=""enter image description here"" />
<strong>Data preview:</strong>
<img src=""https://i.imgur.com/LR50EZq.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Now take union transformation after derived column 1 and with derived column 2
<img src=""https://i.imgur.com/UyCvJsB.png"" alt=""enter image description here"" />
<strong>Data preview:</strong>
<img src=""https://i.imgur.com/jHt8Fdj.png"" alt=""enter image description here"" /></p>
</li>
<li><p>now take select transformation and select the columns and store the columns into the sink.
<img src=""https://i.imgur.com/avWgRj7.png"" alt=""enter image description here"" /></p>
</li>
</ul>
"
"75239023","Trouble with GET request to Copy Data from REST API to Data Lake","<p>I will provide some context: my pipeline makes a GET Request to a REST API (Auth type: OAuth2 Client Credential) in order to import data to the Data Lake (ADLSGen2) in parquet file format. Later, a Stored Procedure creates a View which includes every file in a predefined directory.</p>
<p>I am looking forward to requesting data to the API on an hourly basis (or maybe every 30 minutes) in order to get information of the previous hour. The thing is: almost 36 million records are brought per hour as a response.</p>
<p>In the body of the response there is no reference to the number or the total of pages. There is only data (keys and values).</p>
<p>On the other hand, the Headers include &quot;first-page&quot; and &quot;next-page&quot; (this one appears only if there are further pages in the response, but also makes no reference to the total of pages).</p>
<p>I was wondering if there are any useful suggestions to make my Copy Data activity work differently. Right now, and because of what I mentioned above, the pagination rule is set to RFC5988. I would like my requested data to be partitioned in some way.</p>
<p>Also, I was wondering if there is another way to approach this issue (like using another activity, for example).</p>
<p>Thanks!</p>
<p>Mateo</p>
","<azure><azure-data-factory><azure-synapse>","2023-01-25 19:38:11","82","0","1","75281510","<p>You need to replace the <strong>Header</strong> placeholder with your header_name(Link).</p>
<p><img src=""https://i.imgur.com/VYHyLvW.png"" alt=""enter image description here"" /></p>
<p>Or you can directly use like this dynamic content.</p>
<p><img src=""https://i.imgur.com/Qf3Nip7.png"" alt=""enter image description here"" /></p>
"
"75236948","Can we use Azure PaaS resources like Azure Data Factory or Azure Functions in a private cloud?","<p>I'm new to cloud computing. I'm trying to figure out the difference between public and private clouds. I understand that in a private cloud, the organization owns and maintains the infrastructure. Is this to say that private clouds are always IAAS and cannot use PaaS services?</p>
","<azure><azure-data-factory><private-cloud>","2023-01-25 16:26:12","50","0","1","75237242","<p>I worked on a project that was using Azure Functions on Azure Stack (private cloud). The only thing we had to do, was making sure the Storage Account was v1 not the v2. Besides that, everything worked as expected.</p>
<p>PS: I'm not sure about Azure Data Factory.</p>
"
"75233423","How are changes count calculated in Azure Data Factory - Change Data Capture","<p>With the announcing of change data capture in ADF comes various questions. I tried hand's on the same, and came across various scenarios.</p>
<ul>
<li>Implemented multiple tables from source to target, where source was <strong>On-premises SQL Server</strong> and sink was <strong>Azure SQL Database</strong>.</li>
<li>In monitor tab I tried to read changes read and write but didn't get how that are counted while INSERT, UPDATE, DELETE operation.</li>
<li>If I'm inserting the single data in the source table, the changes read in the monitor tab is displaying 4 changes.</li>
<li>And when I perform Delete operation, that change is not read and written.</li>
</ul>
<p>So, overall, I'm facing difficulty how the changes count are calculated. Can anybody explain this process of count calculation.</p>
<p>Please find below the screenshot for the same:-
(<a href=""https://i.stack.imgur.com/iLtT5.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/iLtT5.png</a>)</p>
","<sql-server><azure><azure-sql-database><azure-data-factory><change-data-capture>","2023-01-25 11:37:50","157","1","1","75233765","<ol>
<li>To support <strong>upsert/delete</strong> operations you need to choose keys columns in column mapping, can you try selecting these options and try.</li>
<li>In Monitoring tab, currently we aggregate the entire changes read/written across sources/sinks.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/2nsaM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2nsaM.png"" alt=""enter image description here"" /></a></p>
"
"75232938","How to add null value in Azure Datafactory Derived columns expression builder","<p>I am currently using Azure Datafactory in that I am creating a Derived column and since the field will always will be blank, so I want the value to be NULL</p>
<p>currently Derived Column I am doing this for adding the expression e.g. toString(&quot;null&quot;) and toString(null()) but this is appearing as string. I only want null to appear without quotes in Json document</p>
","<azure-data-factory>","2023-01-25 10:54:45","119","0","1","75233684","<p>I have reproduced the above and got below results.</p>
<p>I tried to give <code>null()</code> to a column and it gave the error like below.</p>
<p><img src=""https://i.imgur.com/WNFvQHx.png"" alt=""enter image description here"" /></p>
<p>So, in ADF Dataflow, there should be any wrap over <code>null()</code> with the functions like <strong>toInteger()</strong> or <strong>toString()</strong></p>
<p>When I give <code>toString(null())</code> when <strong>id is 4</strong> in derived column of dataflow and the <strong>sink is a JSON</strong>, it gave me the below output.</p>
<p><img src=""https://i.imgur.com/x5opG03.png"" alt=""enter image description here"" /></p>
<p><strong>You can see the row with <strong><code>id==4</code></strong> skipped the null valued key in JSON.</strong> If you give <code>toString(null())</code> same key in every row will be skipped.</p>
<p>You can go through <a href=""https://learn.microsoft.com/en-us/answers/questions/469915/mapping-data-flow-null-value-issue-in-json-which-r"" rel=""nofollow noreferrer"">this link</a> by @<a href=""https://learn.microsoft.com/en-us/users/na/?userid=025a0b1b-b2ce-4dec-bd9b-0c3db34a196d"" rel=""nofollow noreferrer"">ShaikMaheer-MSFT</a> to understand more about this.</p>
<p><strong>AFAIK</strong>, The workaround for this can be to store the null as <code>'null'</code> string to get that key in JSON like this and later use this as per your requirement.</p>
<p><img src=""https://i.imgur.com/9JmyiPQ.png"" alt=""enter image description here"" /></p>
"
"75231644","Azure Data Factory uploads data to database when pipeline run fails","<p>I have a pipeline running in my ADF where I take fields from a json file and save them to multiple tables in my database. The pipeline starts running when a file is uploaded to a container in my storage account, and after the run the files are deleted.
The problem at hand is that when the pipeline fails for whatever reason, the files don't get deleted (because it doesn't make it to the end of the run), and for some reason the SQL records are still saved to some tables which means my tables get spammed with data. Is there any way to only upload the data if the pipeline succeeded?</p>
<p>I didn't see any option that allows me to do that and have to manually take care of the problem when it happens</p>
","<azure><azure-devops><azure-data-factory>","2023-01-25 09:00:47","99","0","2","75232459","<p>Usually when we trigger one activity from another, we use the success route. You can also use a catch block in case of an activity failure to do some other activity. Refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#error-handling"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>So incase the failures happen in a certain activity, use this method to execute another activity which will clear out your SQL records.</p>
"
"75231644","Azure Data Factory uploads data to database when pipeline run fails","<p>I have a pipeline running in my ADF where I take fields from a json file and save them to multiple tables in my database. The pipeline starts running when a file is uploaded to a container in my storage account, and after the run the files are deleted.
The problem at hand is that when the pipeline fails for whatever reason, the files don't get deleted (because it doesn't make it to the end of the run), and for some reason the SQL records are still saved to some tables which means my tables get spammed with data. Is there any way to only upload the data if the pipeline succeeded?</p>
<p>I didn't see any option that allows me to do that and have to manually take care of the problem when it happens</p>
","<azure><azure-devops><azure-data-factory>","2023-01-25 09:00:47","99","0","2","75233347","<p>Adding to @Anupam Chand's answer,</p>
<p>There is no direct way to roll-back or delete records from SQL table which got inserted in the current pipeline run. You should have some flag or watermark column field in the SQL table to identify the records which got inserted in current run.  During failure of the pipeline activities, you can add the logic of deleting records in the failure path. I tried to repro this. Below is the approach.</p>
<ul>
<li>Initially a variable of string type is taken, and it is set using <strong>set variable activity</strong> to get the time at which the pipeline starts executing.</li>
</ul>
<p><img src=""https://i.imgur.com/bj37uFQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then copy activity is added. While copying, add the time when the records get inserted in SQL table.</li>
</ul>
<p><img src=""https://i.imgur.com/zWc8h1i.png"" alt=""enter image description here"" /></p>
<ul>
<li>If this activity is failed, script activity is added to the failure path of copy activity.</li>
</ul>
<p><img src=""https://i.imgur.com/vcMg0nU.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then Script activity is added to delete the records. Query is given as</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>Delete from tgt_table where Inserted_Datetime &gt;='@{variables('initial_time')}'
</code></pre>
<p>By this way, we can roll-back the records which got partially loaded during pipeline failure.</p>
"
"75230703","PostgreSQL to azuredata factory","<p>copy data from postgreSQL db  to azure sQl db</p>
<p>The source is in different server and i want to move the data from source server to destination server for that i need to install self hosted integration runtime but i am unable to install that is there another way to do that.</p>
","<sql><postgresql><azure-data-factory>","2023-01-25 07:14:28","46","0","1","75291961","<p>As I understand the ask here , user is not willing to install Self hosted IR , but the goal is to copy data from postgressql ( in prermise ) to Azure sql . I am quite sure without  SHIR , we cannot use azure data factory in this case  .</p>
<p>I suggest to use pg_dump to copy the data locally and then move the local file to a cloud storage and then copy data from cloud storage to Azure SQl using  ADF .</p>
"
"75230149","How to generate bearer token via azure data factory","<p>I followed this <a href=""https://medium.com/analytics-vidhya/azure-data-factory-retrieve-token-from-azure-ad-using-oauth-2-0-9a3ed3f55013"" rel=""nofollow noreferrer"">blog</a> for generating bearer token.</p>
<p>I have an API like this <code>https://login.microsoftonline.com/&lt;tenantid&gt;/oauth2/token</code>.</p>
<p>I tested it in postman it's working but it is not working in ADF.</p>
<p><strong>Error message</strong></p>
<pre><code>&quot;error&quot;: &quot;invalid request&quot;
&quot;error description&quot;: &quot;xxxx: The 'resource' request parameter is not supported. \r\nTrace ID: xxxxx\rnCorrelation ID: xxxx\r\nTimestamp: xxxx&quot;
&quot;error codes&quot;: [
901002
</code></pre>
","<azure><azure-data-factory>","2023-01-25 05:55:00","207","2","1","75232048","<p>Yes ,you can use both <strong>resource</strong> and <strong>scope</strong> depending upon endpoint.</p>
<p>If you are using endpoint with  <strong>oauth2/token</strong>: <code>https://login.microsoftonline.com/&lt;tenant id&gt;/oauth2/token</code></p>
<p>You need to use <code>resource=https://graph.microsoft.com/</code> inside the body</p>
<p><strong>Body:</strong> <code>grant_type=client_credentials&amp;client_id=&lt;client_id&gt;&amp;client_secret=&lt;client_secret&gt;&amp;resource=https://graph.microsoft.com/</code></p>
<p><img src=""https://i.imgur.com/65dt7rg.png"" alt=""enter image description here"" /></p>
<p>If you are using <strong>oauth2/v2.0/token</strong> endpoint
<code>https://login.microsoftonline.com/&lt;tenant id&gt;/oauth2/v2.0/token</code></p>
<p>You need to use <code>scope</code>:</p>
<p><strong>Body:</strong>  <code>grant_type=client_credentials&amp;client_id=&lt;client_id&gt;&amp;client_secret=&lt;client_secret&gt;&amp;scope=https://graph.microsoft.com/.default</code></p>
<p><img src=""https://i.imgur.com/kg5Pllt.png"" alt=""enter image description here"" /></p>
<p><strong>The pipeline successfully executed got the token:</strong></p>
<p><img src=""https://i.imgur.com/8SYjSHy.png"" alt=""enter image description here"" /></p>
"
"75229631","How to select characters between wildcards in Azure Data Factory expression for Switch Block","<p>I have 2 pipelines that are currently selected based on an IF condition, which works well.  I now need to add a third pipeline , so using a Switch block instead.  And instead of tablename as the deciding factor, ideally I would use logic something like this:</p>
<ol>
<li>does the sourcetablename value contain {FromDate} then goto pipeline A</li>
<li>does the sourcetablename value contain {passinid} then goto pipeline B</li>
<li>else goto pipeline C</li>
</ol>
<p>The sourcetablename value comes with the initial gettablestoload lookup.</p>
<p>An example sourcetablename value is used for the API call in this instance, and would look like this for example:  <strong>blahblah/content/{passinid}/user/blahblah</strong></p>
<p>I am struggling with the expression for the switchblock.  Previously I have matched on the last 10 characters of the tablename, this just seems a little bit tricky.</p>
<p>Here is an example of an expression to remove the last 46 characters, just to give you an idea where I am struggling up to:</p>
<pre><code>@substring(activity('GetTablesToLoad').SourceTableName,0,sub(length(activity('GetTablesToLoad').SourceTableName),46))
</code></pre>
<p>Would anyone have an idea please?</p>
<p>If this was SQL it would be something like this:</p>
<pre><code> DECLARE @text VARCHAR (500) = 'content/{passinid}/user'
    SELECT SUBSTRING(@Text, CHARINDEX('{', @Text)
, CHARINDEX('}',@text) - CHARINDEX('{', @Text) + Len('}'))
</code></pre>
<p><a href=""https://i.stack.imgur.com/9NvtN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9NvtN.png"" alt=""switchblock"" /></a></p>
<p>thankyou</p>
","<sql><switch-statement><azure-data-factory><azure-synapse>","2023-01-25 04:14:29","83","0","1","75230344","<p>You can use <code>if</code> function to check for required value directly. I have taken a parameter called <code>sourceTableName</code> for demonstration instead of lookup value.</p>
<ul>
<li>I have used the following dynamic content as expression value for switch activity. You can replace the <code>pipeline().parameters.sourceTableName</code> with lookup activity SourceTableName value. The name of switch case cannot start with <code>{</code> so, I have directly taken the name.</li>
</ul>
<pre><code>@if(contains(pipeline().parameters.sourceTableName,'{passinid}'),'passinid',if(contains(pipeline().parameters.sourceTableName,'{FromDate}'),'FromDate','execute default'))
</code></pre>
<p><img src=""https://i.imgur.com/y15CVH9.png"" alt=""enter image description here"" /></p>
<ul>
<li>When the parameter value is <code>blahblah/content/{passinid}/user/blahblah</code>, the corresponding <code>set variable3</code> is executed.</li>
</ul>
<p><img src=""https://i.imgur.com/ZKvwyJ2.png"" alt=""enter image description here"" /></p>
<ul>
<li>When the parameter value is <code>blahblah/content/{FromDate}/user/blahblah</code>, the corresponding <code>set variable4</code> is executed.</li>
</ul>
<p><img src=""https://i.imgur.com/TH66gVe.png"" alt=""enter image description here"" /></p>
<ul>
<li>If these values are not present (<code>blahblah/conten/user/blahblah</code>), then the default case would be executed (set variable2 activity).</li>
</ul>
<p><img src=""https://i.imgur.com/kP5kqWd.png"" alt=""enter image description here"" /></p>
"
"75228446","Automated email when connection to source fails in Azure","<p>sorry if the question is too basic. I am very new to Azure and I want to receive automated email if a connection to for example a SQL server is not established. In that regard, I was wondering if there is anything like get meta data activity for linked services that could output true or false if a connection is is not established? given that there is such activity, I know how to to the rest of job. I would appreciate it if someone could help.</p>
<p>Thanks in advance</p>
<p>I have searched about it and tried to find the option of linkedservice instead of dataset in some activities settings but have not figured it out yet.</p>
","<azure><azure-data-factory>","2023-01-24 23:49:47","72","-1","2","75233911","<p>You can make use of Logic App in Azure with your Azure Data factory to receive an email when there’s an error.</p>
<ol>
<li>I created a Logic app resource with the connectors as mentioned below:-</li>
</ol>
<p><img src=""https://i.imgur.com/2BPKL1r.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/bMDuouJ.png"" alt=""enter image description here"" /></p>
<ol start=""2"">
<li>I created an ADF pipeline and sent an email on my Outlook when I received an error like below:-</li>
</ol>
<p>My copy data activity failed like below:-</p>
<p><img src=""https://i.imgur.com/z0oK8gV.png"" alt=""enter image description here"" /></p>
<p>Now, I’ll add one control flow on Web Activity that will trigger our Logic app to send an Email alert.</p>
<p>Added my Logic app URL in the Web activity URL and content-type set to application/json :<br />
<img src=""https://i.imgur.com/5j1QEGN.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/CQluuQG.png"" alt=""enter image description here"" /></p>
<p>Added the dynamic content in the body to get the Error message and details in the email:-</p>
<p><img src=""https://i.imgur.com/hZImtfD.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/VyU8rvJ.png"" alt=""enter image description here"" /></p>
<p>As the Copy activity failed, I got an email alert like below:-</p>
<p><img src=""https://i.imgur.com/EtApqkP.png"" alt=""enter image description here"" /></p>
<p>Reference:-
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-send-email"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-send-email</a></p>
"
"75228446","Automated email when connection to source fails in Azure","<p>sorry if the question is too basic. I am very new to Azure and I want to receive automated email if a connection to for example a SQL server is not established. In that regard, I was wondering if there is anything like get meta data activity for linked services that could output true or false if a connection is is not established? given that there is such activity, I know how to to the rest of job. I would appreciate it if someone could help.</p>
<p>Thanks in advance</p>
<p>I have searched about it and tried to find the option of linkedservice instead of dataset in some activities settings but have not figured it out yet.</p>
","<azure><azure-data-factory>","2023-01-24 23:49:47","72","-1","2","75292013","<p>How about getting an text message with email ? :)
You can create monitoring on a pipeline and on failure it will send an email and also a text message . Please read more on the same here <a href=""https://azure.microsoft.com/en-us/blog/create-alerts-to-proactively-monitor-your-data-factory-pipelines/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/create-alerts-to-proactively-monitor-your-data-factory-pipelines/</a>.</p>
<p>Now for the pipeline , you can add a Lookup actvity and add a simple query like</p>
<pre><code>&quot;Select top 1 from someTable&quot; 
</code></pre>
<p>When the connection to the SQL will fail , the activity will fail and also the pipeline . You should get and email and text for the failures .</p>
"
"75226643","How to dynamically copy multiple datasets from a Google BigQuery project using Azure Synapse Analytics","<p>Is it possible to dynamically copy all datasets from a BigQuery Project to Azure Synapse Analytics, then dynamically copy all tables within each dataset? I know we can dynamically copy all tables within a BigQuery dataset reference to this answered question <a href=""https://stackoverflow.com/questions/57664561/loop-over-of-table-names-adfv2"">Loop over of table names ADFv2</a>, but is there a way to do it at the project level with the lookup activity to loop through all datasets? Is there a way to do a <code>SELECT *</code> to the datasets?</p>
<pre><code>SELECT
*
FROM
gcp_project_name.dataset_name.INFORMATION_SCHEMA.TABLES
WHERE table_type = 'BASE TABLE'
</code></pre>
<p>According to Microsoft's <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup activity in Azure Data Factory and Azure Synapse Analytics</a>, this only reaches the dataset level.
I also tried just putting in the GCP's project name into the Lookup activity's query, but it did not work, ref <a href=""https://stackoverflow.com/questions/72273707/understanding-the-not-found-dataset-was-not-found-in-location-us-error"">Understanding the &quot;Not found: Dataset ### was not found in location US&quot; error</a></p>
","<google-bigquery><azure-data-factory><azure-synapse><lookup-tables>","2023-01-24 19:57:40","100","0","1","75230012","<ul>
<li><p>This can be done using two-level pipeline. I tried to repro this and below is the approach.</p>
</li>
<li><p>Take a <strong>lookup activity</strong> and take the Google big query as source dataset. In Query text box, enter the below query.</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>SELECT schema_name
FROM  `project_name`.INFORMATION_SCHEMA.SCHEMATA
</code></pre>
<p>This query will list the datasets in the project.</p>
<p><img src=""https://i.imgur.com/XDDv4hN.png"" alt=""enter image description here"" /></p>
<ul>
<li>Add a for-each activity next to the lookup activity. In for-each settings' item , type <code>@activity('Lookup1').output.value</code> as a dynamic content.</li>
</ul>
<p><img src=""https://i.imgur.com/U5nxade.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then inside for-each activity, take another lookup activity with same big Query dataset as  source dataset. Type the below query as a dynamic content.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>SELECT
*
FROM
gcp_project_name.dataset_name.@{item().schema_name}.TABLES
WHERE table_type = 'BASE TABLE'
</code></pre>
<p>This will give list of all tables within each dataset.</p>
<p>Since you cannot nest a for-each inside for-each in ADF, you can design a two-level pipeline where the outer pipeline with the outer ForEach loop iterates over an inner pipeline with the nested loop.</p>
<p>Refer the NiharikaMoola-MT's answer on this <a href=""https://stackoverflow.com/questions/72684969/nested-foreach-in-adf-azure-data-factory"">SO thread</a> for Nested foreach in ADF.</p>
"
"75225902","What is the equivalent to Kusto's CountOf() function in Azure Data Factory?","<p>My requirement is to extract a string from filenames using a ADF variable, I need to extract the string until the final underscore '_' and the number of underscores vary in every filename as seen in the below example.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>abc_xyz_20221221.txt         --&gt; abc_xyz
abc_xyz_a1_20221221.txt      --&gt; abc_xyz_a1
abc_c_ab_a1_20221221.txt     --&gt; abc_c_ab_a1
abc_c_ab_a1_a11_20221221.txt --&gt; abc_c_ab_a1_a11</code></pre>
</div>
</div>
</p>
<p>I tried to get it done using indexof() to get the position of the final underscore but it does not accept negative values, so I got the below logic which works in KQL (Azure Data Explorer) but fails in ADF because there is no CountOf() in this tool. Is there any equivalent function in ADF or can you please suggest me how to achieve the same in ADF?</p>
<pre><code>substring(&quot;abc_xyz_20221221.txt&quot;, 0, 
                      indexof(&quot;abc_xyz_20221221.txt&quot;, &quot;_&quot;, 0, 
                              strlen(&quot;abc_xyz_20221221.txt&quot;), 
                                     countof(&quot;abc_xyz_20221221.txt&quot;, '_')))
</code></pre>
","<substring><azure-data-factory><filenames>","2023-01-24 18:43:48","79","0","2","75230163","<p>You can try like this also using split and join inside ForEach activity.</p>
<p><strong>Array for ForEach activity:</strong></p>
<pre><code>[&quot;abc_xyz_20221221.txt&quot;,&quot;abc_xyz_a1_20221221.txt&quot;,&quot;abc_c_ab_a1_20221221.txt&quot;,&quot;abc_c_ab_a1_a11_20221221.txt&quot;]
</code></pre>
<p><strong>Append variable inside ForEach:</strong></p>
<pre><code>@join(take(split(item(), '_'),add(length(split(item(), '_')),-1)),'_')
</code></pre>
<p><img src=""https://i.imgur.com/8kTVMH8.png"" alt=""enter image description here"" /></p>
<p><strong>Result in an array variable:</strong></p>
<p><img src=""https://i.imgur.com/7DooWV1.png"" alt=""enter image description here"" /></p>
<p>As mentioned by <strong>@Joel Cochran</strong>, use the below expression in the append variable inside ForEach with <code>lastIndexOf()</code>.</p>
<pre><code>@substring(item(),0,lastindexof(item(),'_'))
</code></pre>
"
"75225902","What is the equivalent to Kusto's CountOf() function in Azure Data Factory?","<p>My requirement is to extract a string from filenames using a ADF variable, I need to extract the string until the final underscore '_' and the number of underscores vary in every filename as seen in the below example.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>abc_xyz_20221221.txt         --&gt; abc_xyz
abc_xyz_a1_20221221.txt      --&gt; abc_xyz_a1
abc_c_ab_a1_20221221.txt     --&gt; abc_c_ab_a1
abc_c_ab_a1_a11_20221221.txt --&gt; abc_c_ab_a1_a11</code></pre>
</div>
</div>
</p>
<p>I tried to get it done using indexof() to get the position of the final underscore but it does not accept negative values, so I got the below logic which works in KQL (Azure Data Explorer) but fails in ADF because there is no CountOf() in this tool. Is there any equivalent function in ADF or can you please suggest me how to achieve the same in ADF?</p>
<pre><code>substring(&quot;abc_xyz_20221221.txt&quot;, 0, 
                      indexof(&quot;abc_xyz_20221221.txt&quot;, &quot;_&quot;, 0, 
                              strlen(&quot;abc_xyz_20221221.txt&quot;), 
                                     countof(&quot;abc_xyz_20221221.txt&quot;, '_')))
</code></pre>
","<substring><azure-data-factory><filenames>","2023-01-24 18:43:48","79","0","2","75292118","<p>This is a just a simpler form of what @Rakesh called out above . The only difference being , his implementation is iterating . In my case the file name is stored in a variable named foo</p>
<pre><code>@substring(variables('foo'),0,lastindexof(variables('foo'),'_'))
</code></pre>
<p>output</p>
<p><a href=""https://i.stack.imgur.com/TLnA5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TLnA5.png"" alt=""enter image description here"" /></a></p>
"
"75225600","Azure Data Factory DataFlow Error: Key partitioning does not allow computed columns","<p><a href=""https://i.stack.imgur.com/oJyQX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oJyQX.png"" alt=""Parameters"" /></a></p>
<p><a href=""https://i.stack.imgur.com/4lU4m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4lU4m.png"" alt=""Source Settings"" /></a></p>
<p><a href=""https://i.stack.imgur.com/6CvBD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6CvBD.png"" alt=""Optimize"" /></a></p>
<p>We have a generic dataflow that works for many tables, the schema is detected at runtime.
We are trying to add a Partition Column for the Ingestion or Sink portion of the delta.</p>
<p>We are getting error:
Azure Data Factory DataFlow Error: Key partitioning does not allow computed columns
Job failed due to reason: at Source 'Ingestion'(Line 7/Col 0): Key partitioning does not allow computed columns</p>
<p>Can we pass the partition column as a parameter to a generic dataflow?</p>
","<partitioning><azure-data-factory><delta>","2023-01-24 18:14:43","181","0","1","75230269","<blockquote>
<p>Can we pass the partition column as a parameter to a generic dataflow?</p>
</blockquote>
<p>I tried your scenario and got similar error.</p>
<p><img src=""https://i.imgur.com/FeHr3Nx.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>There is a limitation of key partition method is <strong>we cannot apply any calculation to the partition column while declaring it.</strong> Instead, this must be created in advanced, either using derived column or read in from source.</p>
</blockquote>
<p>To resolve this, you can try following steps -</p>
<ul>
<li><p>First, I created a pipeline parameter with datatype string and gave column name as value.
<img src=""https://i.imgur.com/cK4n5mv.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Click on Dataflow &gt;&gt; Go to Parameter &gt;&gt; In value of parameter select Pipeline expression &gt;&gt; and pass the above created parameter.
<img src=""https://i.imgur.com/4DfC3S9.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>OUTPUT:</strong></p>
<p>It is taking it as partition key column and partitioning data accordingly.
<img src=""https://i.imgur.com/Tr6ZdON.png"" alt=""enter image description here"" /></p>
<p><strong>Reference</strong> : <a href=""https://www.c-sharpcorner.com/article/how-to-use-data-flow-partitions-to-optimize-spark-performance-in-data-factory/"" rel=""nofollow noreferrer"">How To Use Data Flow Partitions To Optimize Spark Performance In Data Factor</a></p>
"
"75223981","How to make Copy Data work faster and have better performance (Azure Synapse)","<p>A bit of context: my Azure Synapse pipeline makes a GET Request to a REST API in order to import data to the Data Lake (ADLSGen2) in parquet file format.</p>
<p>I am looking forward to requesting data to the API on an hourly basis in order to get information of the previous hour. I have also considered to set the trigger to run every half an hour to get the data of the previous 30 minutes.</p>
<p>The thing is: this last GET request and Copy Data debug took a bit less than 20 minutes. The DUI used was set in &quot;Auto&quot;, and it equals 4 even if I set it manually to 8 on the activity settings.</p>
<p>I was wondering if there are any useful suggestions to make a Copy Data activity work faster, whatever the cost may be (I would really like info about it, if you consider it pertinent).</p>
<p>Thanks in advance!</p>
<p>Mateo</p>
","<azure><azure-data-factory><azure-synapse>","2023-01-24 15:52:11","138","0","1","75234517","<p>You need to check which part is running slow.
You can click on the glasses icon to see the copy data details.
If the latency is on &quot;Time to first byte&quot; or &quot;Reading from source&quot; the issue is on the REST API side.
If the latency is on &quot;Writing to sink&quot; the problem may be from writing to data lake.
If the issue is on the API side, try to contact the provider. Another option, if applicable, is to use a few copy data activities, each will copy a part of the data.
If the issue is on data lake, you should check the setting on the sink side.</p>
"
"75223728","Incremental load in Azure Data Factory","<p>I am replicating my data from Azure SQl DB TO Azure SQL DB. I have some tables with date columns and some tables  with just the ID columns which are assigning primary key. While performing incremental load in ADF, I can select date as watermark column for the tables which have date column and id as watermark column for the tables which has id column, But the issue is my id has guid values, So can I i take that as my watermark column ? and if yes while copy activity process it gives me following error in ADF</p>
<p>Please see the image for above reference</p>
<p><img src=""https://i.stack.imgur.com/hNQ2y.png"" alt=""Please see the image for above reference"" /></p>
<p>How can I overcome this issue. Help is appreciated</p>
<p>Thank you
Gp</p>
<p>I have tried dynamic mapping <a href=""https://martinschoombee.com/2022/03/22/dynamic-column-mapping-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://martinschoombee.com/2022/03/22/dynamic-column-mapping-in-azure-data-factory/</a> from here but it does not work it still gives me same error.</p>
","<azure><azure-sql-database><azure-data-factory><incremental-load><dynamic-mapping>","2023-01-24 15:34:00","258","0","2","75234042","<p>Regarding your question about watermak:
<strong>A watermark is a column that has the last updated time stamp or an incrementing key</strong>
So GUID column would not be a good fit.
Try to find a date column, or an integer identity which is ever incrementing, to use as watermark.
Since your source is SQL server, you can also use change data capture.
Links:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">Incremental loading in ADF</a>
<a href=""https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-ver16"" rel=""nofollow noreferrer"">Change data capture</a></p>
<p>Regards,
Chen</p>
"
"75223728","Incremental load in Azure Data Factory","<p>I am replicating my data from Azure SQl DB TO Azure SQL DB. I have some tables with date columns and some tables  with just the ID columns which are assigning primary key. While performing incremental load in ADF, I can select date as watermark column for the tables which have date column and id as watermark column for the tables which has id column, But the issue is my id has guid values, So can I i take that as my watermark column ? and if yes while copy activity process it gives me following error in ADF</p>
<p>Please see the image for above reference</p>
<p><img src=""https://i.stack.imgur.com/hNQ2y.png"" alt=""Please see the image for above reference"" /></p>
<p>How can I overcome this issue. Help is appreciated</p>
<p>Thank you
Gp</p>
<p>I have tried dynamic mapping <a href=""https://martinschoombee.com/2022/03/22/dynamic-column-mapping-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://martinschoombee.com/2022/03/22/dynamic-column-mapping-in-azure-data-factory/</a> from here but it does not work it still gives me same error.</p>
","<azure><azure-sql-database><azure-data-factory><incremental-load><dynamic-mapping>","2023-01-24 15:34:00","258","0","2","75292192","<p>The watermark logic takes advantange of the fact that all the new records which are inserted after the last watermark saved should only be considered for copying from source A to B , basically we are using &quot;&gt;=&quot; operator to our advantage here .</p>
<p>In case of guid you cannot use that logic as guid cann surely be unique but not &quot;&gt;=&quot; or &quot;=&lt;&quot; will not work.</p>
"
"75221833","get group members from azure ad via web activity","<p>I want to fetch list of all members from ad security group through adf pipeline</p>
<p>I came across this Api method : <a href=""https://graph.microsoft.com/v1.0/groups/%7Bgroup"" rel=""nofollow noreferrer"">https://graph.microsoft.com/v1.0/groups/{group</a> id}/members</p>
<p>can you guys help me how I can run this Api through web activity by adf pipeline.
Also any permissions or access I need to have before running this Api.</p>
<p>Thanks for your help and suggestions</p>
","<azure><azure-active-directory><azure-data-factory><azure-ad-graph-api>","2023-01-24 12:58:01","156","1","1","75222616","<p><em><strong>I tried to reproduce the same in my environment and got the results like below:</strong></em></p>
<p>I created an <strong>Azure AD Application and granted API permissions</strong> like below:</p>
<p><img src=""https://i.imgur.com/jqq03s4.png"" alt=""enter image description here"" /></p>
<p><strong>Generate the access token like below by creating the web activity:</strong></p>
<ul>
<li><p><strong>URL</strong>:  <code>https://login.microsoftonline.com/TenantID/oauth2/v2.0/token</code></p>
</li>
<li><p><strong>Method :</strong>  POST</p>
</li>
<li><p><strong>Body:</strong>  <code>grant_type=client_credentials&amp;client_id=&lt;client_id&gt;&amp;client_secret=&lt;client_secret&gt;&amp;scope=https://graph.microsoft.com/.default</code></p>
</li>
<li><p><strong>Header:</strong>  <code>Content-Type:application/x-www-form-urlencoded</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/hgDzBst.png"" alt=""enter image description here"" /></p>
<p><strong>I generated access token successfully like below:</strong></p>
<p><img src=""https://i.imgur.com/6efQb15.png"" alt=""enter image description here"" /></p>
<p>I created <strong>Azure AD security Group</strong> and added members:</p>
<p><img src=""https://i.imgur.com/0G5SCA9.png"" alt=""enter image description here"" /></p>
<p>To fetch list of all members from Azure AD security group, use the query in web activity 2 like below:</p>
<pre class=""lang-json prettyprint-override""><code>https://graph.microsoft.com/v1.0/groups/GroupID/members
</code></pre>
<p>In <strong>Authentication</strong>, use this Dynamic content <code>Bearer @{activity('Web1').output.access_token}</code></p>
<p><img src=""https://i.imgur.com/Fwtx0qB.png"" alt=""enter image description here"" /></p>
<p><strong>I am able to fetch list of all members from Azure AD security group successfully like below:</strong></p>
<p><img src=""https://i.imgur.com/RKtVPJV.png"" alt=""enter image description here"" /></p>
<p><strong>Reference:</strong></p>
<p><a href=""https://learn.microsoft.com/en-us/graph/api/group-list-members?view=graph-rest-1.0&amp;tabs=http"" rel=""nofollow noreferrer"">List group members - Microsoft Graph v1.0 | Microsoft Learn</a></p>
"
"75220256","ADFv2 Azure Restrict Access to the Publish button","<p>[Update]
Changes to ADF are deployed using the new CI/CD flow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-improvements#the-new-cicd-flow"" rel=""nofollow noreferrer"">here</a>. SO developers don't need to click publish to manually push changes to create a ARM template.</p>
<p>So developers need to have permissions to develop within ADF but don't need to access to the Publish buttons.</p>
<p>To restrict access to the Publish/Publish All buttons, based on this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-roles-permissions#custom-scenarios-and-custom-roles"" rel=""nofollow noreferrer"">RBAC page</a>, is to assign the Contributor role but remove the Data Factory Contributor role.</p>
<p>I've done that but the Publish button is still enabled.</p>
<p><a href=""https://i.stack.imgur.com/ch4c8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ch4c8.png"" alt=""enter image description here"" /></a></p>
<p>Is there something else I need to do?</p>
","<publish><azure-data-factory><azure-rbac>","2023-01-24 10:34:46","87","0","1","75224413","<p>This is the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control#editing-repo-settings"" rel=""nofollow noreferrer"">answer</a> I believe</p>
<blockquote>
<p>If you choose to disable the publish button from the studio, the
publish button will be grayed out in the studio</p>
</blockquote>
<p>In ADF, Go to &quot;Manage&quot; -&gt; Git Configuration -&gt; Edit</p>
<p><a href=""https://i.stack.imgur.com/PU3KQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PU3KQ.png"" alt=""enter image description here"" /></a></p>
"
"75219546","How do I add a row to a dataset in ADF dataflows","<p>New to ADF and could use some help, I have a 2 column data set I would like to add an additional row to the data set. I have Columns : &quot;CHANNEL&quot; and &quot;CHANNEL_ID&quot; with values as following <a href=""https://i.stack.imgur.com/13K47.png"" rel=""nofollow noreferrer"">Source Data set</a> and would like to add fields: '0' and 'blank' to those columns to produce the result below<a href=""https://i.stack.imgur.com/NP5WZ.png"" rel=""nofollow noreferrer"">Desired outcome</a>. Is this kind of transformation possible within my dataflow?</p>
<p>I've tried to pivot the columns and add a derived column for the '0' field and then pivot those columns again, but I was not certain that what I did was right and I believe there has to be a simpler way than that.</p>
","<azure><azure-data-factory>","2023-01-24 09:28:37","103","0","1","75220258","<p>In order to add a new row <code>channel_id=0 and channel='blank'</code>, I followed below approach.</p>
<ul>
<li>Two source transformations are taken with the same datasets as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/lVJbQq0.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then filter transformation is added to one of the source transformations and filter is given to select one of the rows from the dataset.
filter condition:<code>channel_id='52'</code></li>
</ul>
<p><img src=""https://i.imgur.com/NHwXNRV.png"" alt=""enter image description here"" /></p>
<ul>
<li>derived column transformation is added and settings are give as,</li>
</ul>
<pre><code>Column: Expression
channel='Blank'
channel_id='0'
</code></pre>
<p><img src=""https://i.imgur.com/C7uyUGa.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then Union transformation is added and derived column output and source1 output are given as input to the union transfromation.</li>
</ul>
<p><img src=""https://i.imgur.com/DAeSxwY.png"" alt=""enter image description here"" /></p>
<p><em>Result of Union transformation:</em>
<img src=""https://i.imgur.com/GT920GQ.png"" alt=""enter image description here"" /></p>
<p><strong>Dataflow script</strong></p>
<pre class=""lang-scala prettyprint-override""><code>source(output(
channel as string,
channel_id as string
),
allowSchemaDrift: true,
validateSchema: false,
ignoreNoFilesFound: false) ~&gt; source1
source(output(
channel as string,
channel_id as string
),
allowSchemaDrift: true,
validateSchema: false,
ignoreNoFilesFound: false) ~&gt; source2
filter1 derive(channel =  'blank',
channel_id =  '0') ~&gt; derivedColumn1
source1, derivedColumn1 union(byName: true)~&gt; union1
source2 filter(channel_id=='52') ~&gt; filter1
union1 sink(allowSchemaDrift: true,
validateSchema: false,
skipDuplicateMapInputs: true,
skipDuplicateMapOutputs: true) ~&gt; sink1
</code></pre>
<p>By this way, you can add a row in dataflow.</p>
"
"75219267","Azure Data factory: PostgreSQL to blob storage if PostgreSQL is in a private subnet (Azure)","<p>So I want to create a copy activity in Azure Data factory. From PostgreSQL to Azure blob storage.
As <strong>my vm (postgreSQL)</strong> is in a private subnet in Azure.
So my question is, is it possible to create a pipeline from a vm which is in a <strong>private subnet</strong>?</p>
<p><a href=""https://i.stack.imgur.com/2tqke.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2tqke.png"" alt=""enter image description here"" /></a></p>
<p><strong>updates</strong></p>
<p>So currently this is the situation.
I have created a private endpoint <strong>postgresql-2-data-storage</strong> and now I want to connect datafactory to my vm which is in a Azure vNet with a private ip address <strong>172.16.101.4</strong></p>
<p><a href=""https://i.stack.imgur.com/xZETL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xZETL.png"" alt=""enter image description here"" /></a></p>
<p>when I click on create new Linked service, I don't see Azure resource(vNet) or private endpoint.</p>
","<postgresql><azure><azure-data-factory>","2023-01-24 08:56:03","136","0","1","75220263","<blockquote>
<p>PostgreSQL to blob storage if PostgreSQL is in a private subnet</p>
</blockquote>
<p>AFAIK, to Assess the database from On-premises or from Azure private network you need to configure a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">self-hosted integration runtime</a> to connect to it.</p>
<p>Using Azure Private Link, you can connect to various platform as a service (PaaS) deployment in Azure via a private endpoint and to access data from Private network, you need to Create private endpoint on Azure data factory an add that endpoint to same virtual network where your VM is present.</p>
<p>Go to your ADF settings &gt;&gt; Networking &gt;&gt; Private endpoint connection &gt;&gt; Private endpoint.</p>
<p><img src=""https://i.imgur.com/jDHYeti.png"" alt=""enter image description here"" /></p>
<p>Then fill all details an configure it. after this install SHIR in your VM and connect your PostgreSQL to Data factory</p>
<p>Follow this document <a href=""https://www.techbrothersit.com/2022/01/how-to-install-self-hosted-integration.html"" rel=""nofollow noreferrer"">To Install Self-Hosted Integration Runtime on Azure VM by using Private EndPoint</a> for more information.</p>
"
"75215986","converting a DateTime column to string in ADF","<p>I am trying to build a fully parametrised pipeline template in ADF. With the work I have done so far, I can do a full load without any issues but when it comes to delta load, it seems like my queries are not working. I believe the reason for this is that my &quot;where&quot; statement looks somewhat like this:</p>
<pre><code>SELECT @{item().source_columns} FROM @{item().source_schema}.@{item().source_table} 
WHERE @{item().source_watermarkcolumn} &gt; @{item().max_watermarkcolumn_loaded} AND @{item().source_watermarkcolumn} &lt;= @{activity('Watermarkvalue').output.firstRow.max_watermarkcolumn_loaded}
</code></pre>
<p>where the 'max_watermarkcolumn_loaded' is a datetime format and the 'activity' output is obviously a string format.</p>
<p>Please correct me if my assumption is wrong and let me know what I can do to fix.</p>
<p>EDIT:
screenshot of the error
<a href=""https://i.stack.imgur.com/RXnYb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RXnYb.png"" alt=""enter image description here"" /></a></p>
<p>ADF is picking a date from SQL column 'max_watermarkcolumn_loaded' in this format '&quot;2021-09-29T06:11:16.333Z&quot;' and I think thats where the problem is.</p>
","<sql><azure-data-factory><delta-lake>","2023-01-23 23:09:34","79","0","1","75217714","<p>I tried to repro this error. I gave the parameter without single quotes to a sample Query.
<img src=""https://i.imgur.com/i0urXrz.png"" alt=""enter image description here"" /></p>
<p>Wrap the date parameters with single quotes.</p>
<p><strong>Corrected Query</strong></p>
<pre class=""lang-sql prettyprint-override""><code>SELECT @{item().source_columns} FROM 
@{item().source_schema}.@{item().source_table} 
WHERE @{item().source_watermarkcolumn} &gt; 
'@{item().max_watermarkcolumn_loaded}' AND 
@{item().source_watermarkcolumn} &lt;= 
'@{activity('Watermarkvalue').output.firstRow.max_watermarkcolumn_loaded}'
</code></pre>
<p>With this query, pipeline is run successfully.
<img src=""https://i.imgur.com/j1U0Qwz.png"" alt=""enter image description here"" /></p>
"
"75214054","Azure data factory API","<p>I have constructed a very simple pipeline to test the concept of a pipeline monitoring itself using the REST API, but I am constantly running into 404 Resource not found errors.</p>
<p>here is the general format of my GET request.</p>
<p><a href=""https://management.azure.com/subscriptions/_MY"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/_MY</a> SUB_/resourceGroups/<em>MY_RG</em>/providers/Microsoft.DataFactory/factories/@{pipeline().DataFactory}/pipelineruns/@{pipeline().RunId}?api-version=2018-06-01</p>
<p>Even using the static try it functionality from learn.microsoft.com using hard coded known values also results in a 404.</p>
<p>The run id being generated from @{pipeline().RunId} matches that seen from the monitor.</p>
","<azure-data-factory>","2023-01-23 19:17:35","71","0","1","75252165","<p>Issue solved.
in ADF you cannot utilize the get run id capability of the REST API on runid's that are from debug runs.
The runID must be from a 'triggered' pipeline.</p>
"
"75213508","Copy Files from a folder to multiple folders based on the file name in Azure Data Factory","<p>I have a parent folder in ADLS Gen2 called <strong>Source</strong> which has number of subfolders and these subfolders contain the actual data files as shown in in the below example...</p>
<p>***Source: ***</p>
<p><strong>Folder Name:</strong> 20221212</p>
<p><code>A_20221212.txt B_20221212.txt C_20221212.txt</code></p>
<p><strong>Folder Name:</strong> 20221219</p>
<p><code>A_20221219.txt B_20221219.txt C_20221219.txt</code></p>
<p><strong>Folder Name:</strong> 20221226</p>
<p><code>A_20221226.txt B_20221226.txt C_20221226.txt</code></p>
<p>How can I copy files from subfolders to name specific folders (should create a new folder if it does not exist) using Azure Data Factory, please see the example below...</p>
<p>***Target: ***</p>
<p><strong>Folder Name:</strong> A</p>
<p><code>A_20221212.txt A_20221219.txt A_20221226.txt</code></p>
<p><strong>Folder Name:</strong> B</p>
<p><code>B_20221212.txt B_20221219.txt B_20221226.txt</code></p>
<p><strong>Folder Name:</strong> C</p>
<p><code>C_20221212.txt C_20221219.txt C_20221226.txt</code></p>
<p>Really appreciate your and help.</p>
","<azure><copy><azure-data-factory><move>","2023-01-23 18:22:20","202","0","1","75217950","<p>I have reproduced the above and got below results.</p>
<p>You can follow the below procedure using Get Meta data activity if you have the folder directories at same level.</p>
<p>This is my source folder structure.</p>
<pre><code>data
    20221212
        A_20221212.txt
        B_20221212.txt
        C_20221212.txt`
    20221219
        A_20221219.txt
        B_20221219.txt
        C_20221219.txt
    20221226
        A_20221226.txt
        B_20221226.txt
        C_20221226.txt
</code></pre>
<p><strong>Source dataset:</strong></p>
<p><img src=""https://i.imgur.com/BmlIdBC.png"" alt=""enter image description here"" /></p>
<p>Give this to Get Meta data activity and use <code>ChildItems</code>.</p>
<p>Then Give the ChildItems array from Get Meta data activity to a ForEach activity. Inside ForEach I have used set variable for storing folder name.</p>
<pre><code>@split(item().name,'_')[0]
</code></pre>
<p><img src=""https://i.imgur.com/5tV8VVG.png"" alt=""enter image description here"" /></p>
<p>Now, use copy activity and in source use wild card path like below.</p>
<p><img src=""https://i.imgur.com/ZrrWdTj.png"" alt=""enter image description here"" /></p>
<p>For sink create dataset parameters and give it copy activity sink like below.</p>
<p><img src=""https://i.imgur.com/YAiNnUL.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/qg6SsfM.png"" alt=""enter image description here"" /></p>
<p><strong>My pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Get Metadata1&quot;,
                &quot;type&quot;: &quot;GetMetadata&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;sourcetxt&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;fieldList&quot;: [
                        &quot;childItems&quot;
                    ],
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Get Metadata1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Get Metadata1').output.childItems&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy data1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;Set variable1&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                        &quot;recursive&quot;: true,
                                        &quot;wildcardFolderPath&quot;: &quot;*&quot;,
                                        &quot;wildcardFileName&quot;: {
                                            &quot;value&quot;: &quot;@item().name&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;enablePartitionDiscovery&quot;: false
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                    }
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                                    &quot;storeSettings&quot;: {
                                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                    },
                                    &quot;formatSettings&quot;: {
                                        &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                                        &quot;quoteAllText&quot;: true,
                                        &quot;fileExtension&quot;: &quot;.txt&quot;
                                    }
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;translator&quot;: {
                                    &quot;type&quot;: &quot;TabularTranslator&quot;,
                                    &quot;typeConversion&quot;: true,
                                    &quot;typeConversionSettings&quot;: {
                                        &quot;allowDataTruncation&quot;: true,
                                        &quot;treatBooleanAsNumber&quot;: false
                                    }
                                }
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;sourcetxt&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;targettxts&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;folder_name&quot;: {
                                            &quot;value&quot;: &quot;@variables('folder_name')&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;file_name&quot;: {
                                            &quot;value&quot;: &quot;@item().name&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        }
                                    }
                                }
                            ]
                        },
                        {
                            &quot;name&quot;: &quot;Set variable1&quot;,
                            &quot;type&quot;: &quot;SetVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;folder_name&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@split(item().name,'_')[0]&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            }
        ],
        &quot;variables&quot;: {
            &quot;folder_name&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/ILQIMAb.png"" alt=""enter image description here"" /></p>
"
"75210391","how to use Lookup activity and if activity together","<p>i have a file on storage account emp.csv that contains. i want that from storage account a file and b file would go to database table.</p>
<pre><code>emp_id,filename
   1,a
   2,b
   3,anubhav
</code></pre>
<p>so for this i pass emp.csv file on lookup activity as source dataset then i use foreach activity
<a href=""https://i.stack.imgur.com/F5HIX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F5HIX.png"" alt=""enter image description here"" /></a></p>
<p>Inside foreach activity i used a if condition on expression</p>
<pre><code>@equals(item().filename,'anubhav' )
</code></pre>
<p><a href=""https://i.stack.imgur.com/ZfNVo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZfNVo.png"" alt=""enter image description here"" /></a></p>
<p>if this expression is true then wait activity will come and wait for 1 sec. if this expression false then
<a href=""https://i.stack.imgur.com/XFyGd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XFyGd.png"" alt=""enter image description here"" /></a></p>
<p>but this pipeline is failing</p>
","<azure><azure-pipelines><azure-data-factory>","2023-01-23 13:45:07","112","0","1","75212862","<p>The pipeline is failing because inside copy activity dataset properties it should be a string, but you have given it as an array value <code>@activity('Lookup1').output.value</code>. because of that, you getting an error.</p>
<p>Try to replace the array value with the string <code>@item().filename</code> as you can see, I reproduce the same thing in my environment and got this output.</p>
<p><img src=""https://i.imgur.com/X9rM5oh.png"" alt=""enter image description here"" /></p>
<p>You can use this <strong>Json pipeline activity</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobStorageReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;DelimitedText1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Lookup1').output.value&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;If Condition1&quot;,
                            &quot;type&quot;: &quot;IfCondition&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;expression&quot;: {
                                    &quot;value&quot;: &quot;@equals(item().filename, 'anubhav.csv')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                },
                                &quot;ifFalseActivities&quot;: [
                                    {
                                        &quot;name&quot;: &quot;Copy data1&quot;,
                                        &quot;type&quot;: &quot;Copy&quot;,
                                        &quot;dependsOn&quot;: [],
                                        &quot;policy&quot;: {
                                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                            &quot;retry&quot;: 0,
                                            &quot;retryIntervalInSeconds&quot;: 30,
                                            &quot;secureOutput&quot;: false,
                                            &quot;secureInput&quot;: false
                                        },
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;source&quot;: {
                                                &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                                &quot;storeSettings&quot;: {
                                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                                    &quot;recursive&quot;: true,
                                                    &quot;enablePartitionDiscovery&quot;: false
                                                },
                                                &quot;formatSettings&quot;: {
                                                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                                }
                                            },
                                            &quot;sink&quot;: {
                                                &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                                                &quot;storeSettings&quot;: {
                                                    &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                                },
                                                &quot;formatSettings&quot;: {
                                                    &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                                                    &quot;quoteAllText&quot;: true,
                                                    &quot;fileExtension&quot;: &quot;.txt&quot;
                                                }
                                            },
                                            &quot;enableStaging&quot;: false,
                                            &quot;translator&quot;: {
                                                &quot;type&quot;: &quot;TabularTranslator&quot;,
                                                &quot;typeConversion&quot;: true,
                                                &quot;typeConversionSettings&quot;: {
                                                    &quot;allowDataTruncation&quot;: true,
                                                    &quot;treatBooleanAsNumber&quot;: false
                                                }
                                            }
                                        },
                                        &quot;inputs&quot;: [
                                            {
                                                &quot;referenceName&quot;: &quot;abcsv&quot;,
                                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                                &quot;parameters&quot;: {
                                                    &quot;file&quot;: {
                                                        &quot;value&quot;: &quot;@item().filename&quot;,
                                                        &quot;type&quot;: &quot;Expression&quot;
                                                    }
                                                }
                                            }
                                        ],
                                        &quot;outputs&quot;: [
                                            {
                                                &quot;referenceName&quot;: &quot;DelimitedText2&quot;,
                                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                                &quot;parameters&quot;: {
                                                    &quot;file&quot;: {
                                                        &quot;value&quot;: &quot;@item().filename&quot;,
                                                        &quot;type&quot;: &quot;Expression&quot;
                                                    }
                                                }
                                            }
                                        ]
                                    }
                                ],
                                &quot;ifTrueActivities&quot;: [
                                    {
                                        &quot;name&quot;: &quot;Wait1&quot;,
                                        &quot;type&quot;: &quot;Wait&quot;,
                                        &quot;dependsOn&quot;: [],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;waitTimeInSeconds&quot;: 1
                                        }
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ],
        &quot;annotations&quot;: []
    }
} 
</code></pre>
<blockquote>
<p><strong>Pipeline successfully executed</strong></p>
</blockquote>
<p><img src=""https://i.imgur.com/nmVSAub.png"" alt=""enter image description here"" /></p>
"
"75207663","Data Factory repoConfiguration in Bicep - lastCommitId","<p>I have been working with bicep for a short time and have the following problem.</p>
<p>I make bicep deplyoment for an already existeirende Data Factory (Connected with Git). I have configured it as follows:</p>
<pre><code>resource name_resource 'Microsoft.DataFactory/factories@2018-06-01' {
  name: 'adf-...'
  location: location
  properties: {
    publicNetworkAccess: 'Enabled'
    repoConfiguration: isDEV ? {
      accountName: '...'
      repositoryName: '...'
      disablePublish: false
      collaborationBranch: '...'
      rootFolder: '...'
      type: 'FactoryVSTSConfiguration'
      lastCommitId:
      projectName: '...'
      tenantId: subscription().tenantId
      } : null 
  }
  tags: tags
  identity: {
    type: 'SystemAssigned'
  }
}
</code></pre>
<p>Everything looks good except <strong>lastCommitId</strong>.</p>
<p>If I do not configure it, it will be deleted according to Bicep What-if (logical).</p>
<p>I can't hardcode it because the commitId changes with every adf-publish.</p>
<p>Is there a way to dynamically read lastCommitId and pass it to the bicep deplyoment?</p>
<p>Something like with output? I have tried myself, but unfortunately I can't get any further.</p>
<pre><code>lastCommitId: id

output id string = name_resource.properties.repoConfiguration.lastCommitId

ERROR
The expression is involved in a cycle (&quot;id&quot; -&gt; &quot;name_resource&quot;).
</code></pre>
<p>I am grateful for any advice.</p>
","<git><azure-devops><terraform><azure-data-factory><azure-bicep>","2023-01-23 09:35:35","114","0","1","75400664","<p>Unfortunately, I haven't found a way to solve it directly in the bicep. But I have a workaround, you can read it dynamically during the deployment, in the deplyoment pipeline. Here is a link to it:</p>
<p><a href=""https://stackoverflow.com/questions/75245712/parse-set-variables-output-to-template-azure-yml-pipeline/75260173#75260173"">Parse Set variables output to template - Azure yml pipeline</a></p>
"
"75206080","Invoking REST API through SQL Stored Proc","<p>We are working with Azure Data Factory and there is this pipeline in which I am trying to check the response of an <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipeline-runs/query-by-factory?tabs=HTTP"" rel=""nofollow noreferrer"">API</a> which reports a pipeline status. If it returns 'InProgress', I want to wait for some time before doing the same thing until the status is either 'Succeeded' or 'Failed'. I can easily achieve this in ADF but we are being asked to reduce the pipeline cost to as low as possible. I have done some research and found that implementing a wait functionality in SP compared to the Wait-Until construct in ADF is significantly cheaper (Stored Procedure activity being an external activity). And thus, I am interested to see how to achieve this in stored proc.</p>
","<api><stored-procedures><azure-sql-database><azure-data-factory>","2023-01-23 06:00:41","93","0","1","75206151","<p>You can leverage Azure SQL Database External REST Endpoints Integration
<a href=""https://datasharkx.wordpress.com/2022/12/02/event-trigger-azure-data-factory-synapse-pipeline-via-azure-sql-database/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2022/12/02/event-trigger-azure-data-factory-synapse-pipeline-via-azure-sql-database/</a>
wherein directly access the API via Azure SQL database within an SP.</p>
<p>Sample blog for some other use case:
<a href=""https://datasharkx.wordpress.com/2022/12/02/event-trigger-azure-data-factory-synapse-pipeline-via-azure-sql-database/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2022/12/02/event-trigger-azure-data-factory-synapse-pipeline-via-azure-sql-database/</a></p>
"
"75204206","How to map the iterator value to sink in adf","<p>A question concerning Azure Data Factory.</p>
<p>I need to persist the iterator value from a lookup activity (an Id column from a sql table) to my sink together with other values.</p>
<p>How to do that?</p>
<p>I thought that I could just reference the iterator value as @{item().id} as source and a destination column name from from my sql table sink. That doesn’t seems to work. The resulting value in the destination column is NULL.</p>
","<azure-data-factory>","2023-01-22 21:57:50","56","0","1","75218069","<ul>
<li>I have used 2 look up activities, one for id values and the other for remaining values. Now, to combine and insert these values to sink table, I have used the following:</li>
<li>The ids look up activity output is as following:</li>
</ul>
<p><img src=""https://i.imgur.com/2gfqK2m.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have one more column to combine with above id values. The following is the look up output for that:</li>
</ul>
<p><img src=""https://i.imgur.com/N0lnXZk.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have given the following dynamic content as the items value in for each as following:</li>
</ul>
<pre><code>@range(0,length(activity('ids').output.value))
</code></pre>
<p><img src=""https://i.imgur.com/fmXCNQ2.png"" alt=""enter image description here"" /></p>
<ul>
<li>Inside for each activity, I have given the following script activity query to insert data as required into sink table:</li>
</ul>
<pre><code>insert into t1 values(@{activity('ids').output.value[item()].id},'@{activity('remaining rows').output.value[item()].gname}')
</code></pre>
<ul>
<li>The data would be inserted successfully and the following is the reference image of the same:</li>
</ul>
<p><img src=""https://i.imgur.com/y2XIYLS.png"" alt=""enter image description here"" /></p>
"
"75203244","ADF Column Name Validation and Data Validation","<p>I am trying to add some validation to my ADF pipeline. Is there a way to achieve the following validation in ADF?</p>
<ol>
<li>Validate column header and return error message. There is a list of required column names that I need to check against the raw Excel file. For example, the raw file might have column A,B,C,D, but the required columns are A,B,E. So is there a way to validate and return an error message that the column E is missing in the raw file?</li>
<li>Validate the data type in data mapping flow, if column A should be a numeric field but some of the cells have text in it, or column B should be datetime type but has a number in it. Is there a way to check values in each row and return error message if the data validation fails on that row?</li>
</ol>
","<azure-data-factory><azure-mapping-data-flow>","2023-01-22 19:23:32","200","0","2","75205493","<p>you can use a lookup activity on the dataset and return 1st row(with dataset header property disabled)this would give you the list of columns present in the excel file which you can then compare against the expected values, if the values/sequence match you can proceed further else you can thro error.
Note: you can also use Get meta data activity to get the column details</p>
<p>For data type, you can use column patterns in dataflows:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern</a></p>
<p>@rakeshGovindula: any more thoughts?</p>
"
"75203244","ADF Column Name Validation and Data Validation","<p>I am trying to add some validation to my ADF pipeline. Is there a way to achieve the following validation in ADF?</p>
<ol>
<li>Validate column header and return error message. There is a list of required column names that I need to check against the raw Excel file. For example, the raw file might have column A,B,C,D, but the required columns are A,B,E. So is there a way to validate and return an error message that the column E is missing in the raw file?</li>
<li>Validate the data type in data mapping flow, if column A should be a numeric field but some of the cells have text in it, or column B should be datetime type but has a number in it. Is there a way to check values in each row and return error message if the data validation fails on that row?</li>
</ol>
","<azure-data-factory><azure-mapping-data-flow>","2023-01-22 19:23:32","200","0","2","75209795","<p>Adding to <strong>@Nandan</strong>, you can use Get Meta data activity structure like below.</p>
<p>This is my repro for your reference:</p>
<p>First, I have used 2 parameters for column names and Data types.</p>
<p><img src=""https://i.imgur.com/0QJMCBc.png"" alt=""enter image description here"" /></p>
<p><strong>Get Meta data activity:</strong></p>
<p><img src=""https://i.imgur.com/rFMXHiX.png"" alt=""enter image description here"" /></p>
<p>Get Meta activity output array:</p>
<p><img src=""https://i.imgur.com/gHwGImC.png"" alt=""enter image description here"" /></p>
<p>Then I have created two arrays to get the above names and columns using forEach.</p>
<p>Then I have used two filter activities to filter the above parameter arrays.</p>
<p>The used if activity to check the parameter arrays length and filter activity output arrays lengths.</p>
<p>If its true, the inside True activities you can use your copy activity or Data flow as per your requirement. Inside False activities, use a fail activity.</p>
<p><strong>My pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline1&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Get Metadata1&quot;,
            &quot;type&quot;: &quot;GetMetadata&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Excel1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;fieldList&quot;: [
                    &quot;structure&quot;
                ],
                &quot;storeSettings&quot;: {
                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                    &quot;enablePartitionDiscovery&quot;: false
                }
            }
        },
        {
            &quot;name&quot;: &quot;Filtering names&quot;,
            &quot;type&quot;: &quot;Filter&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Getting names and columns as list&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@pipeline().parameters.names&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;condition&quot;: {
                    &quot;value&quot;: &quot;@contains(variables('namesvararray'),item())&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Filtering types&quot;,
            &quot;type&quot;: &quot;Filter&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Filtering names&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('typevararray')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;condition&quot;: {
                    &quot;value&quot;: &quot;@contains(variables('typevararray'), item())&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Getting names and columns as list&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Get Metadata1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Get Metadata1').output.structure&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append names&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;namesvararray&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@item().name&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;Append types&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;Append names&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;typevararray&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@item().type&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;If Condition1&quot;,
            &quot;type&quot;: &quot;IfCondition&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Filtering types&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;expression&quot;: {
                    &quot;value&quot;: &quot;@and(equals(length(pipeline().parameters.names),activity('Filtering names').output.FilteredItemsCount),equals(length(pipeline().parameters.columns),activity('Filtering types').output.FilteredItemsCount))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;ifFalseActivities&quot;: [
                    {
                        &quot;name&quot;: &quot;Fail1&quot;,
                        &quot;type&quot;: &quot;Fail&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;message&quot;: &quot;Some of the headers or types are not as required&quot;,
                            &quot;errorCode&quot;: &quot;240&quot;
                        }
                    }
                ],
                &quot;ifTrueActivities&quot;: [
                    {
                        &quot;name&quot;: &quot;Set variable1&quot;,
                        &quot;type&quot;: &quot;SetVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;sample&quot;,
                            &quot;value&quot;: &quot;All good&quot;
                        }
                    }
                ]
            }
        }
    ],
    &quot;parameters&quot;: {
        &quot;names&quot;: {
            &quot;type&quot;: &quot;array&quot;,
            &quot;defaultValue&quot;: [
                &quot;A&quot;,
                &quot;B&quot;,
                &quot;C&quot;
            ]
        },
        &quot;columns&quot;: {
            &quot;type&quot;: &quot;array&quot;,
            &quot;defaultValue&quot;: [
                &quot;String&quot;,
                &quot;String&quot;,
                &quot;String&quot;
            ]
        }
    },
    &quot;variables&quot;: {
        &quot;namesvararray&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;typevararray&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;sample&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>My pipeline failed and got error:</strong></p>
<p><img src=""https://i.imgur.com/AnJbhGG.png"" alt=""enter image description here"" /></p>
"
"75193816","Azure SQL database with MFA login to connect from Azure ADF","<p>I have an Azure SQL server and database which have MFA login and I am the admin. But when I try to establish a connection via a new linked service from ADF to this database using System Managed Identity option, it throws error -
&quot;Cannot connect to SQL Database. Please contact SQL server team for further support. Server: 'Server details', Database: 'database name', User: ''. Check the linked service configuration is correct, and make sure the SQL Database firewall allows the integration runtime to access.</p>
<p><img src=""https://i.stack.imgur.com/l7l3Z.jpg"" alt=""enter image description here"" />
<img src=""https://i.stack.imgur.com/37jzo.jpg"" alt=""enter image description here"" />
I have already given contributor role access to ADF in SQL database using system managed Identity. Also, I have tried to access this database using Autoresolve runtime and azure runtime. But still the error is coming.</p>
","<azure><azure-active-directory><azure-sql-database><azure-data-factory><azure-sql-managed-instance>","2023-01-21 13:49:32","211","0","2","75194289","<p>It sounds like you are missing the user creation and role assignment within the SQL database:</p>
<p>Connect to the database with your account and create an account for the data factory:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE USER [&lt;datafactory-name&gt;] FROM EXTERNAL PROVIDER;
</code></pre>
<p>Then grant it the required role for your task:</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER ROLE [&lt;roleName&gt;] ADD MEMBER [&lt;datafactory-name&gt;]
</code></pre>
<p>Some available role names are:</p>
<ul>
<li>db_accessadmin</li>
<li>db_backupoperator</li>
<li>db_datareader</li>
<li>db_datawriter</li>
<li>db_ddladmin</li>
<li>db_denydatareader</li>
<li>db_denydatawriter</li>
<li>db_owner</li>
<li>db_securityadmin</li>
<li>public</li>
</ul>
"
"75193816","Azure SQL database with MFA login to connect from Azure ADF","<p>I have an Azure SQL server and database which have MFA login and I am the admin. But when I try to establish a connection via a new linked service from ADF to this database using System Managed Identity option, it throws error -
&quot;Cannot connect to SQL Database. Please contact SQL server team for further support. Server: 'Server details', Database: 'database name', User: ''. Check the linked service configuration is correct, and make sure the SQL Database firewall allows the integration runtime to access.</p>
<p><img src=""https://i.stack.imgur.com/l7l3Z.jpg"" alt=""enter image description here"" />
<img src=""https://i.stack.imgur.com/37jzo.jpg"" alt=""enter image description here"" />
I have already given contributor role access to ADF in SQL database using system managed Identity. Also, I have tried to access this database using Autoresolve runtime and azure runtime. But still the error is coming.</p>
","<azure><azure-active-directory><azure-sql-database><azure-data-factory><azure-sql-managed-instance>","2023-01-21 13:49:32","211","0","2","75208415","<p>I created Azure SQL database in portal and created linked service in azure data factory with managed identity authentication I got below error:</p>
<p><img src=""https://i.imgur.com/rOBa7Ij.png"" alt=""enter image description here"" /></p>
<p>I followed below procedure to resolve this:
I turned on the managed identity of data factory</p>
<p><img src=""https://i.imgur.com/pbGIR9Z.png"" alt=""enter image description here"" /></p>
<p>I set admin for azure SQL database:</p>
<p><img src=""https://i.imgur.com/HTtapdM.png"" alt=""enter image description here"" /></p>
<p>Login with Admin to sql database Create User username as data factory name using below code:</p>
<pre><code>CREATE USER [DATAFACTORY NAME] FROM eXTERNAL PROVIDER
</code></pre>
<p><img src=""https://i.imgur.com/CWkrMbq.png"" alt=""enter image description here"" /></p>
<p>Added rules to the user using below code:</p>
<pre><code>ALTER ROLE db_datareader ADD MEMBER [DATA FACTORY NAME];
</code></pre>
<p><img src=""https://i.imgur.com/4T8iB2k.png"" alt=""enter image description here"" /></p>
<p>I tested linked service again, tested successfully</p>
<p><img src=""https://i.imgur.com/QTbVvRE.png"" alt=""enter image description here"" /></p>
<p>It worked for me, once check from your end.</p>
"
"75192309","How to truncate the data to first 3 letter in data flow?","<p>I want to truncate data if unit=code.</p>
<p>Input:</p>
<p>Country, unit</p>
<p>India, code</p>
<p>Bangladesh, money</p>
<p>China, code</p>
<p>Output:</p>
<p>Country, unit</p>
<p>Ind, code</p>
<p>Bangladesh, money</p>
<p>Chi, code</p>
<p>What I tried?</p>
<p>I used case expression in dataflow but not able to truncate data to 3 letter code</p>
","<azure><azure-data-factory>","2023-01-21 09:25:00","63","0","1","75192601","<ul>
<li>You can use left() function in dataflow to get the first three characters of data.</li>
<li>I repro'd this with sample input.</li>
</ul>
<p><strong>Source data:</strong></p>
<p><img src=""https://i.imgur.com/L8XfpIT.png"" alt=""enter image description here"" /></p>
<ul>
<li>Derived column transformation is taken and expression for country column is given as <code>case(unit=='code',left(Country,3) , Country)</code></li>
</ul>
<p><strong>Derived column settings:</strong></p>
<p><img src=""https://i.imgur.com/KH6PSc2.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/9nUNTWB.png"" alt=""enter image description here"" /></p>
"
"75191676","Migrate data to Azure SQL DB","<p>How to migrate data from Azure file storage to Azure SQL DB</p>
<p>Error I am getting
Error 40615: Cannot connect to &lt; servername</p>
","<azure><azure-data-factory>","2023-01-21 07:04:28","74","0","1","75192833","<ul>
<li><p>I tried to repro this and got the error <code>40615: Cannot open server</code>
<img src=""https://i.imgur.com/yVeiwA4.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Check the connection strings - server name, password, and database name.</p>
</li>
<li><p>Add your client ip to the firewall. Below are the steps.</p>
<ol>
<li>Open the Azure SQL Database in azure portal.</li>
<li>Click Networking</li>
</ol>
</li>
</ul>
<p><img src=""https://i.imgur.com/5uloN4q.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>Click +Add a firewall rule</li>
<li>Add the client ip address.</li>
<li>You can also check allow azure services and resources to access this server. This will allow all azure resources to access your database.</li>
</ol>
<p><img src=""https://i.imgur.com/0CytWkg.png"" alt=""enter image description here"" /></p>
<p>Reference: MS Document on <a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/firewall-configure?view=azuresql#manage-firewall-rules-using-the-azure-portal"" rel=""nofollow noreferrer"">IP firewall rules - Azure SQL Database and Azure Synapse Analytics</a></p>
"
"75191667","Azure Synapse Pipeline copy data from the BigQuery, where the source schema is hierarchical with nested columns","<p>Please help me with copying data from Google BigQuery to Azure Data Lake Storage Gen2 with Serverless SQL Pool.</p>
<p>I am using Azure Synapse's Copy data pipeline. The issue is I cannot figure out how to handle source table from the BigQuery with hierarchical schema.          This result in missing columns and inaccurate datetime value at the sink.</p>
<p>The source is a Google BigQuery table, it is made of Google Cloud Billing export of a project's standard usage cost. The source table's schema is hierarchical with nested columns, such as service.id; service.description; sku.id; sku.description; Project.labels.key; Project.labels.value, etc.</p>
<p>When I click on Preview data from the Source tab of the Copy data pipeline, it only gives me the top of the column hierarchy, for example: It would only show the column name of [service] and with value of {\v&quot;:{&quot;f&quot;:[{&quot;v&quot;:&quot;[service.id]&quot;},{&quot;v&quot;:&quot;[service.descrpition]&quot;}]}}
<a href=""https://i.stack.imgur.com/Q4ttr.png"" rel=""nofollow noreferrer"">image description: Source with nested columns result in issues with Synapse Copy Data Pipline</a></p>
<p>I have tried to configure the Copy Pipline with the following:<br />
Source Tab:
Use query - I think the solution lays in here, but I cannot figure out the syntax of selecting the proper columns.  I watched a Youtube video from TechBrothersIT <a href=""https://www.youtube.com/watch?v=6_I-PX27fNA"" rel=""nofollow noreferrer"">How to Pass Parameters to SQL query in Azure Data Factory - ADF Tutorial 2021</a>, but still unable to do it.</p>
<p>Sink Tab:</p>
<p>1.Sink dataset in various format of csv, json and parquet - with csv and parquet getting similar result, and json format failed</p>
<p>2.Sink dataset to Azure SQL Database - failed because it is not supported with Serverless SQL Pool</p>
<p>3.<strong>Mapping Tab</strong>:  note: edited on Jan22 with screenshot to show issue.</p>
<ol>
<li>Tried with Import schemas, with Sink Tab copy behavior of <code>None, Flatten Hierarchy and Preserve Hierarchy</code>, but still unable to get source column to be recognized as Hierarchical. Unable to get the Collection reference nor the Advanced Editor configurations to show up. Ref: <a href=""https://i.stack.imgur.com/PLyCn.png"" rel=""nofollow noreferrer"">Screenshot of Source columns not detected as Hierarchical</a>    <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#hierarchical-source-to-tabular-sink"" rel=""nofollow noreferrer"">MS Doc on Schema and data type mapping in copy activity</a></li>
</ol>
<p>I have also tried with the Data flow pipeline, but it does not support Google BigQuery<a href=""https://i.stack.imgur.com/xmM56.png"" rel=""nofollow noreferrer"">Data Flow Pipe Source do not support BigQuery yet</a></p>
<p>Here are the steps to reproduce / get to my situation:</p>
<ol>
<li>Register Google cloud, setup billing export (of standard usage cost) to BigQuery.</li>
<li>At Azure Synapse Analytics, create a Linked service with user authentication. Please follow Data Tech's Youtube video
<a href=""https://www.youtube.com/watch?v=0CdvAc_jR9Y"" rel=""nofollow noreferrer"">&quot;Google BigQuery connection (or linked service) in Azure Synapse analytics&quot;</a></li>
<li>At Azure Synapse Analytics, Integrate, click on the &quot;+&quot; sign -&gt; Copy Data Tool</li>
</ol>
<p>I believe the answer is at the Source tab with Query and Functions, please help me figure this out, or point me to the right direction.
<img src=""https://i.stack.imgur.com/RbK9e.png"" alt=""Img: Source Tab"" /></p>
<p>Looking forward to your input. Thanks in advance!</p>
","<tsql><google-bigquery><azure-data-factory><azure-synapse>","2023-01-21 07:02:26","251","0","1","75205866","<p>ADF allows you to write the query in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-bigquery?tabs=data-factory#googlebigquerysource-as-a-source-type"" rel=""nofollow noreferrer"">google bigquery source dataset</a>. Therefore write the query to unnest the nested columns using <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#unnest_operator"" rel=""nofollow noreferrer"">unnest operator</a> and then map it to the sink.</p>
<p>I tried to repro this with sample nested table.
<img src=""https://i.imgur.com/63AUMa5.png"" alt=""enter image description here"" />
img:1 nested table</p>
<p><img src=""https://i.imgur.com/q52aazL.png"" alt=""enter image description here"" />
img:2 sample data of nested table</p>
<p><strong>Script to flatten the nested table:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>select
user_id,
a.post_id,
a.creation_date
from  `ds1.stackoverflow_nested`  
cross  join unnest(comments) a
</code></pre>
<p><img src=""https://i.imgur.com/vBhW0eR.png"" alt=""enter image description here"" />
img:3 flattened table.</p>
<ul>
<li>Use this query in copy activity source dataset.
<img src=""https://i.imgur.com/CRSqLBB.png"" alt=""enter image description here"" />
img:4 Source settings of copy activity.</li>
<li>Then take the sink dataset, do the mapping and execute the ADF pipeline.</li>
</ul>
<p>Reference:</p>
<ol>
<li>MS document on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-bigquery?tabs=data-factory#googlebigquerysource-as-a-source-type"" rel=""nofollow noreferrer"">google bigquery as a source - ADF</a></li>
<li>GC document on <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#unnest_operator"" rel=""nofollow noreferrer"">unnest operator</a></li>
</ol>
"
"75189158","Azure Data Factory: Incrementing a number in the REST API request body","<p>In <a href=""https://en.wikipedia.org/wiki/Microsoft_Azure#Data_management"" rel=""nofollow noreferrer"">ADF</a>, I am currently working on a <em>copy data</em> function from a REST API URL. In the <a href=""https://en.wikipedia.org/wiki/POST_%28HTTP%29"" rel=""nofollow noreferrer"">POST</a> request body there are parameters like this:</p>
<pre class=""lang-none prettyprint-override""><code>{
    &quot;anyStateChange&quot;: true,
    &quot;timeInterval&quot;: {
        &quot;field&quot;: &quot;requestedOn&quot;,
        &quot;operator&quot;: &quot;lastndays&quot;,
        &quot;value&quot;: &quot;1&quot;
    },
    &quot;paginationStartIndex&quot;: 1,
    &quot;numberOfRecords&quot;: 20
}
</code></pre>
<p>API Returns 3 Fields like this :</p>
<pre class=""lang-none prettyprint-override""><code>{
    &quot;totalResults&quot;: X
    &quot;startIndex&quot; : Y
    &quot;itemsPerPage&quot; : Z
}
</code></pre>
<p>I am trying to determine the best way to increment the paginationStartIndex until it runs out of records in the loop.</p>
<p>I tried doing some adjustments to <em>pagination</em> rules, and other options which want to increment the page in the URL which doesn't work here.</p>
","<rest><azure-data-factory>","2023-01-20 20:42:31","146","0","1","75193063","<p>For incrementing a number in a REST API request body, you can use a <em>ForEach activity</em> as below.</p>
<p>In the <em>for each</em> activity settings, pass the value as a range of numbers you want to pass as below:</p>
<p><img src=""https://i.imgur.com/yZxJtaW.png"" alt=""Enter image description here"" /></p>
<p>Then in the request body, pass the <code>@{item()}</code> for the parameter you want to increment sequentially.</p>
<p><img src=""https://i.imgur.com/CxagYRk.png"" alt=""Enter image description here"" /></p>
<p>This will also act as pagination.</p>
"
"75188175","azure file share map to folder based on clients?","<p>Hello Every azure file share map to folder based on clients ?</p>
<p>I have 300 clients and each have there own files to upload on ADFS.
I want to map drive but folder which is only associated with that client not root folder.</p>
<p>so will this be possible with ADSF or any option ?</p>
","<azure-blob-storage><azure-data-factory><azure-data-lake><azure-file-share><azure-file-copy>","2023-01-20 18:42:12","59","0","1","75206047","<blockquote>
<p>Azure file share map to folder based on clients.</p>
</blockquote>
<p>We can do this in two ways.</p>
<p><strong>Approach 1</strong></p>
<p>One way is to map a drive for each client with their own files on ADFS by creating a folder for each client and has to use the ADFS drive mapping feature to map the client's folder as the root directory for their drive.</p>
<p>And this can be done by using the <code>net use</code> command in <code>Command Prompt</code> by specifying the client's folder as the target directory.</p>
<p>net use [drive letter:] [\server name\share name] [password] /user:[username]</p>
<p>For example,
To map the drive letter X to the folder &quot;\adfs-server\client1files&quot; for a user named &quot;client1&quot; with the password &quot;client1password&quot;, you would use the following command:</p>
<pre><code>net use X: \\adfs-server\client1files client1password /user:client1
</code></pre>
<p><img src=""https://i.imgur.com/MaREpwt.png"" alt=""enter image description here"" /></p>
<p><strong>Approach 2</strong></p>
<p>Another way is to use a script to automate the process, in this way you do not need to manually map the drive for each client.</p>
<p>for this you need to create the script into a .bat file and have run based on requirement.</p>
<pre><code>Script Example

net use P: &quot;\\server\foldername\foldername&quot;

@echo off
setlocal

rem Define variables for the server name, share name, and client information
set server=\\adfs-server
set share=clientfiles
set username=client
set password=clientpassword

rem Iterate through the clients and map their drive
for /l %%i in (1,1,300) do (
    set clientnum=%%i
    set driveletter=%%i

    rem Map the drive for the current client
    net use %driveletter%: %server%\%share%%clientnum% %password% /user:%username%%clientnum%

    rem Confirm the drive mapping
    echo Drive %driveletter%: has been mapped to %server%\%share%%clientnum% for %username%%clientnum%
)
</code></pre>
<p>References taken from</p>
<p><a href=""https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/cc771865%28v=ws.11%29"" rel=""nofollow noreferrer"">Net User</a></p>
<p><a href=""https://lazyadmin.nl/it/net-use-command/"" rel=""nofollow noreferrer"">Net Use Command</a></p>
<p>Thanks @ Ruud for the  lazyadmin blog.</p>
"
"75186375","How to pass special character as parameter in Azure Data Factory?","<p>I am trying to parametrize the Column Delimiter field in CSV dataset in Azure Data Factory.
(<a href=""https://i.stack.imgur.com/JGkD5.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/JGkD5.png</a>)</p>
<p>This unfortunately doesn't work when I pass a special character as a parameter.</p>
<p>When I hardcode the special character in the column delimiter field all works as expected.
<a href=""https://i.stack.imgur.com/5pay3.png"" rel=""nofollow noreferrer"">This works</a></p>
<p>However, when I have \u0006 as a parameter in SQL DB (varchar(10) type)
(<a href=""https://i.stack.imgur.com/GyTdr.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/GyTdr.png</a>)</p>
<p>and I pass it in the pipeline
(<a href=""https://i.stack.imgur.com/CUeRf.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/CUeRf.png</a>)</p>
<p>The Copy Data activity doesn't detect this special character as a delimiter.</p>
<p>My guess is that when I use a parameter it passes \u0006 as a string, but I can't find anywhere how to bypass that.</p>
","<azure-data-factory>","2023-01-20 15:52:50","165","0","1","75205568","<p>I tried to pass <code>\u0006</code> in column delimiter as a dynamic content. It didn't consider that as column delimiter. All data are shown as a single column.
<img src=""https://i.imgur.com/In4zBXl.png"" alt=""enter image description here"" /></p>
<p>Therefore, I tried to pass equivalent symbol of \u0006 <code>ACK</code>() as a dynamic value to that column delimiter and it worked. I tried to convert the \u0006 into the special character using SQL script. Below are the steps to do this.</p>
<ul>
<li><p>File delimiters are stored in a table.<img src=""https://i.imgur.com/aYbBF0E.png"" alt=""enter image description here"" /></p>
</li>
<li><p>To convert this column into equivalent characters, \u is removed from the column and the resultant hexadecimal value is converted into an integer. Then nchar() function is used to the integer data.</p>
</li>
</ul>
<p><code>select nchar(cast(right(file_delimiter,4) as int)) as file_delimiter from t5</code></p>
<ul>
<li>The above SQL query is used in Lookup activity in ADF.</li>
</ul>
<p><img src=""https://i.imgur.com/v9wzigF.png"" alt=""enter image description here"" /></p>
<ul>
<li>When this value is passed as a dynamic content to column delimiter to that dataset, values are properly delimited.</li>
<li>Once pipeline is run, data is copied successfully.
<img src=""https://i.imgur.com/fAMkQ45.png"" alt=""enter image description here"" /></li>
</ul>
"
"75184415","Azure Data Factory development with multiple users","<p>can any one help me  how to lock pipeline in ADF, is there any option when one developer is working other should not work, as with multiple developers are working on same pipeline  without using Source Control</p>
","<azure-data-factory>","2023-01-20 13:05:28","32","-1","1","75185764","<p>unfortunately there is no feature in Azure portal for Azure data factory to lock the pipeline changes if 2 or more are working on the same pipeline. You would have to create a clone of existing pipeline and work on those clones else the best way is to use source control like git</p>
"
"75183265","How to Convert an Object received from an SQL Lookup into a string in Azure Data Factory","<p>I'm new to Stack and ADF, so please excuse me if my question is lacking some information. I am looking up a datetime value in our database using an Lookup Activity in ADF. This the result of the lookup:</p>
<pre><code> &quot;value&quot;: [
        {
            &quot;&quot;: &quot;2023-01-19T10:16:34Z&quot;
        }
    ],
</code></pre>
<p>I want to put 2023-01-19T10:16:34Z into a variable as a string. What is the best practise for this? The type of the value is Object.</p>
<p>Thanks in advance.</p>
<p>I was able to convert the value into an array. (see below) But that is not what I need.</p>
<pre><code>@array(activity('Lookup1').output.value[0])
</code></pre>
","<azure-data-factory>","2023-01-20 11:14:30","87","0","1","75183644","<p>You can use <strong>string()</strong> instead of array function.
I tried to repro this with sample input.</p>
<p><strong>Sample input (Lookup table):</strong></p>
<pre class=""lang-json prettyprint-override""><code>&quot;value&quot;:
 [
     { &quot;col1&quot;: &quot;2023-01-19T10:16:34Z&quot; }
 ]
</code></pre>
<ul>
<li>Set variable activity is taken and variable s1 of string type is set using this activity. <code>@string(activity('Lookup1').output.value[0].col1)</code> is given as an expression to convert the lookup activity output to string.</li>
</ul>
<p><strong>Result</strong>
<img src=""https://i.imgur.com/xioTQSp.png"" alt=""enter image description here"" /></p>
"
"75182315","Azure Data Factory, how to pass parameters from trigger/pipeline in to data source","<p>I need help. I've create a pipeline for data processing, which is importing csv and copy data to DB. I've also configure a Blob storage trigger, which is triggering pipeline with dataflow, when speciffic file will be uploaded in to container. For the moment, this trigger is set to monitor one container, however I would like to set it to be more universal. To monitor all containers in desired Storage Account and if someone will send some files, pipeline will be triggered. But for that I need to pass container name to the pipeline to be used in datasource file path. for now I've create something like that:</p>
<p>in the pipeline, I've add this parameter @pipeline().parameters.sourceFolder:</p>
<p><a href=""https://i.stack.imgur.com/NMTf9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NMTf9.png"" alt=""enter image description here"" /></a></p>
<p>Next in Trigger, I've set this:</p>
<p><a href=""https://i.stack.imgur.com/oTGb9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oTGb9.png"" alt=""enter image description here"" /></a></p>
<p>Now what should I set here, to pass this folder path?</p>
<p><a href=""https://i.stack.imgur.com/5WDbD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5WDbD.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><pipeline>","2023-01-20 09:50:01","215","0","2","75182900","<p>You need to use <strong>dataset parameters</strong> for this.</p>
<p>Like folderpath parameter in pipeline create another pipeline parameter for the file name also and give <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code> to those when creating trigger.</p>
<p><strong>Pipeline parameters:</strong></p>
<p><img src=""https://i.imgur.com/LsLd3Yl.png"" alt=""enter image description here"" /></p>
<p>Make sure you give <code>all containers</code> in storage event trigger while creating trigger.</p>
<p><strong>Assiging trigger parameters to pipeline parameters:</strong></p>
<p><img src=""https://i.imgur.com/algGQ48.png"" alt=""enter image description here"" /></p>
<p>Now, create two dataset parameters for the folder and file name like below.</p>
<p><strong>Source dataset parameters:</strong></p>
<p><img src=""https://i.imgur.com/x7buhEV.png"" alt=""enter image description here"" /></p>
<p>Use these in the file path of the dataset dynamic content.</p>
<p><img src=""https://i.imgur.com/CcpXWFM.png"" alt=""enter image description here"" /></p>
<p>If you use <strong>copy activity</strong> for this dataset, then <strong>assign the pipeline parameters values(which we can get from trigger parameters) to dataset parameters like below.</strong></p>
<p><img src=""https://i.imgur.com/5LEohru.png"" alt=""enter image description here"" /></p>
<p>If you use dataflows for the dataset, you can assign these in the dataflow activity itself like below after giving dataset as source in the dataflow.</p>
<p><img src=""https://i.imgur.com/BcScxYO.png"" alt=""enter image description here"" /></p>
"
"75182315","Azure Data Factory, how to pass parameters from trigger/pipeline in to data source","<p>I need help. I've create a pipeline for data processing, which is importing csv and copy data to DB. I've also configure a Blob storage trigger, which is triggering pipeline with dataflow, when speciffic file will be uploaded in to container. For the moment, this trigger is set to monitor one container, however I would like to set it to be more universal. To monitor all containers in desired Storage Account and if someone will send some files, pipeline will be triggered. But for that I need to pass container name to the pipeline to be used in datasource file path. for now I've create something like that:</p>
<p>in the pipeline, I've add this parameter @pipeline().parameters.sourceFolder:</p>
<p><a href=""https://i.stack.imgur.com/NMTf9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NMTf9.png"" alt=""enter image description here"" /></a></p>
<p>Next in Trigger, I've set this:</p>
<p><a href=""https://i.stack.imgur.com/oTGb9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oTGb9.png"" alt=""enter image description here"" /></a></p>
<p>Now what should I set here, to pass this folder path?</p>
<p><a href=""https://i.stack.imgur.com/5WDbD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5WDbD.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><pipeline>","2023-01-20 09:50:01","215","0","2","75207840","<p>Thank you Rakesh</p>
<p>I need to process few speciffic files from package that will be send to container. Each time user/application will send same set of files so in trigger I'm checking does new drive.xml file was send to any container. This file defines type of the data that was send, so if it comes, I know that new datafiles has been send as well and they will be present in lover folder.
F.eg. drive.xml was found in <em><strong>/container/data/somefolder/2022-01-22/drive.xml</strong></em> and then I know that in <em><strong>/container/data/somefolder/2022-01-22/datafiles/</strong></em>, are located 3 files that I need to process.
Therefor in parameters, I need to pass only file path, file names will be always the same.</p>
<p>The Dataset configuration looks like that:
<a href=""https://i.stack.imgur.com/BnLeE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BnLeE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/jYjGo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jYjGo.png"" alt=""enter image description here"" /></a>
and the event trigger like that:
<a href=""https://i.stack.imgur.com/sf7vh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sf7vh.png"" alt=""enter image description here"" /></a></p>
"
"75181834","How to process signed numbers in azure data factory","<p>I am processing a .txt file
in source and  I have a data +120.2894.
I want to push the data to sql db using adf dataflow activity.
Automatically it takes string datatype and makes the output as +120.29 which is precise upto two decimal places.
I have tried decimal l(15,4) using a cast activity but it remives the + sign and gives the output as 120.2894.</p>
<p>I need the data in as usual format and the output shouold be +120.2894.
Please let me know how I can do the cast in adf?</p>
<p>Ps:-The column also contains negative records.</p>
","<azure><azure-data-factory>","2023-01-20 09:07:47","47","0","1","75183368","<p>In SQL or data factory If it detects Number with <code>+</code> sign It will not show it the sign is only visible for <code>-</code> numbers.</p>
<p>Sample data:
<img src=""https://i.imgur.com/21zamnG.png"" alt=""enter image description here"" /></p>
<p>Go to projection of source &gt;&gt; Import projection it will detect schema of column.
<img src=""https://i.imgur.com/y7Mgscd.png"" alt=""enter image description here"" /></p>
<p>Data preview for it:( It will only show  <code>-</code> sign for numbers)
<img src=""https://i.imgur.com/U6Rh74S.png"" alt=""enter image description here"" /></p>
<p>Output: (same in sql database)
<img src=""https://i.imgur.com/gKCwLuM.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>If you still want data as <code>+120.2894</code> you need to store it as String (varchar).</p>
</blockquote>
<p>Go to projection of source &gt;&gt; Import projection it will detect schema of column &gt;&gt; then select string type for respective column.
<img src=""https://i.imgur.com/rp15fIz.png"" alt=""enter image description here"" /></p>
<p>Data preview for it:
<img src=""https://i.imgur.com/LykuG4n.png"" alt=""enter image description here"" /></p>
<p>Output:
<img src=""https://i.imgur.com/hnLGRv1.png"" alt=""enter image description here"" /></p>
"
"75179463","How to write expression in expression builder in data flow of ADF","<p>I need an expression in the expression builder to transform the customer Name as below
Take first character of word in the name followed by * . Customer name may contain 1 or more words
Name can be Tim or Tim John or Tim John Zac or Tim John Mike Zac</p>
","<regex><azure><azure-data-factory>","2023-01-20 00:00:10","247","1","1","75179995","<p>I have reproduced above and got below results using derived column.</p>
<p>I have used the same data that you have given in a single column and used the below dataflow expression in derived column.</p>
<pre><code>dropLeft(toString(reduce(map(split(Name, ' '),regexReplace(#item, concat('[^',left(#item,1),']'), '*')), '', #acc +  '  '  + #item, #result)), 2)
</code></pre>
<p>Here, some general regular expressions were given errors for me in dataflow, that's why used the above approach.</p>
<p>First, I have used <code>split()</code> by <code>space</code> to get an array of strings. Then used regular expression on every item of array like above.</p>
<p>As we do not have <code>join</code> in dataflow expression, I have used the code from this <a href=""https://stackoverflow.com/a/75089327"">SO answer</a> by <strong>@Jarred Jobe</strong> to convert array to a string seperated by spaces.</p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/2duHR6f.png"" alt=""enter image description here"" /></p>
<p><strong>NOTE:</strong></p>
<p>Make sure you give two spaces in <code>toString()</code> of above code to get the required result. If we give only one space it will give the results like below.</p>
<p><img src=""https://i.imgur.com/7nIg4tT.png"" alt=""enter image description here"" /></p>
<p><strong>Update:</strong></p>
<blockquote>
<p>Thank you so much for sharing this. I have tried your solution but I
got few names wrong .Also I want to replace the rest of the characters
with just 5 '<em>' irrespective of how many characters the name has. Also
name : Mia hellah came as M*</em> h****h instead of M***** h*****. Another
one SAM &amp; JOHN TIBEH should be S***** &amp;***** J***** T*****. I tried to
update your expression but I couldn't get it right.</p>
</blockquote>
<p>If you want to do like above, you can directly use <code>concat</code> function dataflow expression.</p>
<pre><code>dropLeft(toString(reduce(map(split(Name, ' '),concat(left(#item,1), '*****')), '', #acc +  '  '  + #item, #result)), 2)
</code></pre>
<p><strong>Results:</strong></p>
<p><img src=""https://i.imgur.com/hfvEq6n.png"" alt=""enter image description here"" /></p>
"
"75179391","ADF Data Flow Remove empty JSON objects and arrays when building JSON files","<p>I am using a Data Flow in Azure Data Factory to transform rows of CSV files into JSON documents of a standard layout.
I can't figure out how to get rid of empty JSON objects and arrays when there is no data to populate.</p>
<p>For example if I have a CSV like below:</p>
<pre><code>firstName,lastName,Email,Address1,City,State,Zip
Bob,Smith,someemail@email.com,123 st,Somecity,TX,12345
</code></pre>
<p>I need to turn it into this:</p>
<pre><code>{
    &quot;firstName&quot;: &quot;Bob&quot;,
    &quot;lastName&quot;: &quot;Smith&quot;,
    &quot;contactData&quot;: [
        {
            &quot;contactType&quot;: &quot;postalAddress&quot;,
            &quot;contactData&quot;: {
                &quot;postalAddress1&quot;: &quot;123 st&quot;,
                &quot;postalCity&quot;: &quot;Somecity&quot;,
                &quot;postalState&quot;: &quot;TX&quot;,
                &quot;postalCode&quot;: &quot;12345&quot;
            }
        },
        {
            &quot;contactType&quot;: &quot;email&quot;,
            &quot;contactData&quot;: {
                &quot;emailAddress&quot;: &quot;someemail@email.com&quot;
            }
        }
    ]
}
</code></pre>
<p>I am using derived columns to build the subcolumns and arrays. I have been able to produce the JSON above.
The problem I run into is that if an email or address is null, I want to remove the object from the array.
If both are null, I want to remove the entire contactData object.</p>
<p>Example:</p>
<pre><code>firstName,lastName,Email,Address1,City,State,Zip
Bob,Smith,,,,,
</code></pre>
<p>I need to turn it into this:</p>
<pre><code>{
    &quot;firstName&quot;: &quot;Bob&quot;,
    &quot;lastName&quot;: &quot;Smith&quot;
}
</code></pre>
<p>If I set all of the child objects to NULL with IF statements I can produce something like this:</p>
<pre><code>{
    &quot;firstName&quot;: &quot;Bob&quot;,
    &quot;lastName&quot;: &quot;Smith&quot;,
    &quot;contactData&quot;: [
        {
            &quot;contactData&quot;: {}
        },
        {
            &quot;contactData&quot;: {}
        }
    ]
}
</code></pre>
<p>but I can't get rid of the entire section.
The Sink will get rid of the empty string objects, but not the nested JSON objects and arrays.
Is there any way to do this in ADF Data Flows?</p>
","<json><azure-data-factory>","2023-01-19 23:48:46","215","0","1","75179973","<ul>
<li>You can split the data and then apply union to get the desired result.</li>
<li>Since I don't have the previous data transformations, I have taken the following data as my source data.</li>
</ul>
<pre><code>[{
    &quot;firstName&quot;: &quot;Bob&quot;,
    &quot;lastName&quot;: &quot;Smith&quot;,
    &quot;contactData&quot;: [
        {
            &quot;contactType&quot;: &quot;postalAddress&quot;,
            &quot;contactData&quot;: {
                &quot;postalAddress1&quot;: &quot;123 st&quot;,
                &quot;postalCity&quot;: &quot;Somecity&quot;,
                &quot;postalState&quot;: &quot;TX&quot;,
                &quot;postalCode&quot;: &quot;12345&quot;
            }
        },
        {
            &quot;contactType&quot;: &quot;email&quot;,
            &quot;contactData&quot;: {
                &quot;emailAddress&quot;: &quot;someemail@email.com&quot;
            }
        }
    ]
},
{
    &quot;firstName&quot;: &quot;b1&quot;,
    &quot;lastName&quot;: &quot;s1&quot;,
    &quot;contactData&quot;: [
        {
            &quot;contactData&quot;: {}
        },
        {
            &quot;contactData&quot;: {}
        }
    ]
},
{
    &quot;firstName&quot;: &quot;Bob1&quot;,
    &quot;lastName&quot;: &quot;Smith1&quot;,
    &quot;contactData&quot;: [
        {
            &quot;contactType&quot;: &quot;postalAddress&quot;,
            &quot;contactData&quot;: {
                &quot;postalAddress1&quot;: &quot;123 st1&quot;,
                &quot;postalCity&quot;: &quot;Somecity1&quot;,
                &quot;postalState&quot;: &quot;TX1&quot;,
                &quot;postalCode&quot;: &quot;123456&quot;
            }
        },
        {
            &quot;contactType&quot;: &quot;email&quot;,
            &quot;contactData&quot;: {
                &quot;emailAddress&quot;: &quot;someemail1@email.com&quot;
            }
        }
    ]
},
{
    &quot;firstName&quot;: &quot;b2&quot;,
    &quot;lastName&quot;: &quot;s2&quot;,
    &quot;contactData&quot;: [
        {
            &quot;contactData&quot;: {}
        },
        {
            &quot;contactData&quot;: {}
        }
    ]
}]
</code></pre>
<ul>
<li>Now, I have taken a sample derived column to find the length of <code>contactType</code>. The rows without any <code>contactType</code> would have the same length (convert to string and find length).</li>
</ul>
<pre><code>tp : length(toString(contactData.contactType))
</code></pre>
<p><img src=""https://i.imgur.com/HItBrET.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, split the data based on whether the <code>contactType</code> (converted to string) length is equal to 2 or not. The split condition would be as shown below:</li>
</ul>
<pre><code>length(toString(contactData.contactType))!=2
</code></pre>
<p><img src=""https://i.imgur.com/IWcqSqe.png"" alt=""enter image description here"" /></p>
<ul>
<li>This will split the data as required. The <code>noContact</code> stream would have the following data:</li>
</ul>
<p><img src=""https://i.imgur.com/b6NNQs3.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now select only required columns. I have used rule based selection to <strong>select only columns where name is not contactData</strong>.</li>
</ul>
<pre><code>condition : name!='contactData'
column name : $$
</code></pre>
<p><img src=""https://i.imgur.com/pTDpzyH.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now apply union transformation by <code>Name</code> on <code>hasContact</code> and <code>select1</code> stream.</li>
</ul>
<p><img src=""https://i.imgur.com/VLzVzB8.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have configured the sink dataset as shown in the image below:</li>
</ul>
<p><img src=""https://i.imgur.com/AlH2GWS.png"" alt=""enter image description here"" /></p>
<ul>
<li>In sink, output to a single JSON file with selected name (under settings tab). The data preview in sink would be as:</li>
</ul>
<p><img src=""https://i.imgur.com/0Ux4Nwf.png"" alt=""enter image description here"" /></p>
<ul>
<li>Once the file is written, the null fields would not be written, so the data would be as per requirement. The following is an image for reference.</li>
</ul>
<p><img src=""https://i.imgur.com/6kg3NzH.png"" alt=""enter image description here"" /></p>
"
"75177827","git pull in Azure Data Factory","<p>When working with the regular source code, (Java, C++, etc..) there are things like</p>
<pre><code>git pull ..
git fetch .. 
git push .. 
</code></pre>
<p>to synch your remote git repo branch with your local branch.
What is the equivalent of such in the Azure Data Factory world ?</p>
<p>So, I am using azure data factory with the Azure git repo.
I am working in the particular feature branch - &quot;fefature branch&quot;
And my pipeline has a copy activity that hits a data set in its &quot;Sink&quot; stage.
Here is a screen shot but .. it's pretty simple and seems right</p>
<p><a href=""https://i.stack.imgur.com/3gDy1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3gDy1.png"" alt=""enter image description here"" /></a></p>
<p>I see that my code for Data set definition (Json) in the remote Git repository is different from what I see in the Azure portal gui (being pointed to that same remote branch).  ADF Gui in the Azure Portal is correct, the one in the git repo contains some stuff that I already deleted, but it does not gets deleted there (Why??)
So, when I 'Debug' pipeline I get errors which indicate this discrepancy as a problem.  I want ty sync the environments and .. given that <strong>I do not understand</strong> how the discrepancies came about, I don't know how to fix an issue?. Any help is appreciated.</p>
","<azure><azure-data-factory><azure-git-deployment>","2023-01-19 20:18:23","148","-1","1","75239384","<p>In the ADF world, we use publish and create a new pull request to merge the new changes from a feature branch to the main branch.</p>
<p>it seems like your git repository version is not up to date with the live ADF.</p>
<p>If there are any pending changes in your main branch, then you can click on Publish button to merge the changes</p>
<p>And if you are working on the feature branches, you can merge the changes using the new pull request.</p>
<p>If you have multiple feature branches, then you will need to manually compare the different versions to resolve these conflicts.</p>
<p><a href=""https://i.stack.imgur.com/drUgN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/drUgN.png"" alt=""ADF"" /></a></p>
"
"75177276","Trouble with Azure Synapse: pipeline cannot execute a stored procedure that works in Develop script","<p>I will give some context regarding our inconvenience in Azure Synapse:</p>
<ul>
<li>We created a Stored Procedure (it creates a VIEW which reads all the
parquet files in a certain folder) on a Develop script, and it ran
successfully.</li>
<li>We also created the VIEW manually, also successfully, in a database
created in Serverless SQL Pool.</li>
<li>The container where the data is currently has Private Access Level.
My user has Storage blob data contributor.</li>
<li>Moving on to the Azure Synapse pipeline, we can use Copy Data to get
new parquet files inserted into the Container (ADLS Gen2).</li>
<li>When we want to run everything on Integrate (Synapse pipeline) an
error pops up:</li>
</ul>
<p>{    &quot;errorCode&quot;: &quot;2402&quot;,    &quot;message&quot;: &quot;Execution fail against sql server. Please contact SQL Server team if you need further support. Sql error number: 13807. Error Message: Content of directory on path 'https://xxx.blob.core.windows.net/data/folder/*.parquet' cannot be listed.</p>
<p><strong>If we switch the Container's Access level to public, everything works smoothly, <em>but we want to keep it Private</em>.</strong></p>
<p>Is there anything else we should do in order to make our Synapse pipeline work correctly? Any additional permissions setup or else?</p>
<p>Thank you so much in advance.</p>
<p>Regards,</p>
<p>Mateo</p>
","<azure-active-directory><azure-data-factory><azure-synapse><azure-data-lake-gen2><azure-storage-account>","2023-01-19 19:19:35","392","1","1","75182141","<p>I tried to reproduce the issue and got similar error.</p>
<p><img src=""https://i.imgur.com/5AnAJl5.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>As per <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#content-of-directory-on-the-path-cant-be-listed"" rel=""nofollow noreferrer"">Microsoft documentation</a> the error it says that <em><strong>the user who's querying Azure Data Lake can't list the files in storage.</strong></em></p>
<blockquote>
<ul>
<li>The user of Azure AD who is utilizing pass-through authentication from Azure AD is not authorized to show the files in Data Lake Storage.</li>
<li>The shared access signature key or workspace managed identity being used by the Azure AD or SQL user to view data does not have authorization to list the files in storage.</li>
</ul>
</blockquote>
</blockquote>
<p>Give <code>Storage Blob Data Contributor</code> role to your synapse workspace.</p>
<p>Go to Storage account =&gt; Access Control (IAM) =&gt; Add role assignment =&gt; Select Role: Storage Blob Data Contributor Select: your workspace name =&gt; Click on save.</p>
<p>OR</p>
<p>To Get the data from <strong>Private containers you need to give authorize access of it to data DataSource</strong> when using it from pipeline. for this you can use <code>Access key</code>, <code>Shared access credentials</code> to give this credential to DataSource you need to create <code>scoped credential</code>. But you can't create Scoped credentials in Serverless SQL pool you have to use Dedicated SQL pool.</p>
<p>The work around is to use Dedicated SQL pool.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE  DATABASE  SCOPED  CREDENTIAL SasToken
WITH  IDENTITY = 'SHARED ACCESS SIGNATURE',
SECRET = 'SAS token';
GO

CREATE  EXTERNAL  DATA  SOURCE mysample1
WITH ( LOCATION = 'storage account',
CREDENTIAL = SasToken
)
</code></pre>
<p>now you can create stored procedure to create view and then execute it from stored procedure or scrip activity in pipeline.</p>
"
"75174174","Synapse Analytics RenameDataFactoryResourceError error when trying to publish a rename pipeline","<p>I'm trying to publish a reanaming pipeline and I'm getting this error and don't know how to deal with</p>
<pre><code>Error code: OK
Inner error code: RenameDataFactoryResourceError
Message: {&quot;code&quot;:&quot;InternalError&quot;,&quot;message&quot;:&quot;Internal error has occurred.&quot;,&quot;target&quot;:null,&quot;details&quot;:null,&quot;error&quot;:null}
</code></pre>
<p>I've already tried to rename the pipeline again but the error persists when I try to publish the pipeline.</p>
","<azure><azure-data-factory><azure-synapse>","2023-01-19 14:57:36","624","2","3","75213135","<p>I was having the same problem. I tried many ways, no luck. Except for one. I disconnected Github in Synapse, then reconnected. Working now.</p>
"
"75174174","Synapse Analytics RenameDataFactoryResourceError error when trying to publish a rename pipeline","<p>I'm trying to publish a reanaming pipeline and I'm getting this error and don't know how to deal with</p>
<pre><code>Error code: OK
Inner error code: RenameDataFactoryResourceError
Message: {&quot;code&quot;:&quot;InternalError&quot;,&quot;message&quot;:&quot;Internal error has occurred.&quot;,&quot;target&quot;:null,&quot;details&quot;:null,&quot;error&quot;:null}
</code></pre>
<p>I've already tried to rename the pipeline again but the error persists when I try to publish the pipeline.</p>
","<azure><azure-data-factory><azure-synapse>","2023-01-19 14:57:36","624","2","3","75226725","<p>I just ran into this issue myself. I fixed it by reverting the commit(s) that renamed the files. For the record, I <em>hate</em> this approach, but Synapse seems to have problems renaming things.</p>
"
"75174174","Synapse Analytics RenameDataFactoryResourceError error when trying to publish a rename pipeline","<p>I'm trying to publish a reanaming pipeline and I'm getting this error and don't know how to deal with</p>
<pre><code>Error code: OK
Inner error code: RenameDataFactoryResourceError
Message: {&quot;code&quot;:&quot;InternalError&quot;,&quot;message&quot;:&quot;Internal error has occurred.&quot;,&quot;target&quot;:null,&quot;details&quot;:null,&quot;error&quot;:null}
</code></pre>
<p>I've already tried to rename the pipeline again but the error persists when I try to publish the pipeline.</p>
","<azure><azure-data-factory><azure-synapse>","2023-01-19 14:57:36","624","2","3","75237041","<p>Same issue here. I used the Pending Changes log to revert the names back on Main branch and republished. That fixed it.</p>
<p>Anything highlighted in yellow, I renamed back with the old name. <a href=""https://i.stack.imgur.com/QFKHX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QFKHX.png"" alt=""enter image description here"" /></a></p>
"
"75174138","How to delete records from a sql database using azure data factory","<p>I am setting up a pipeline in data factory where the first part of the pipeline needs some pre-processing cleaning. I currently have a script set up to query these rows that need to be deleted, and export these results into a csv.</p>
<p>What I am looking for is essentially the opposite of an upsert copy activity. I would like the procedure to delete the rows in my table based on a matching row.</p>
<p>Apologies in advanced if this is an easy solution, I am fairly new to data factory and just need help looking in the right direction.</p>
","<azure><azure-data-factory>","2023-01-19 14:54:32","89","0","1","75174826","<p>Assuming the source from which you are initially getting the rows is different from the sink
There are multiple ways to achieve it.</p>
<ol>
<li><p>in case if the number of rows is less, we can leverage script activity or lookup activity to delete the records from the destination table</p>
</li>
<li><p>in case of larger dataset, limitations of lookup activity, you can copy the data into a staging table with in destination and leverage a script activity to delete the matching rows</p>
</li>
<li><p>in case if your org supports usage of dataflows, you can use that to achieve it</p>
</li>
</ol>
"
"75171036","Trigger Date for reruns","<p>My pipelines activities need the date of the run as a parameter. Now I get the current date in the pipeline from the utcnow() function. Ideally this would be something I could enter dynamically in the trigger so I could rerun a failed day and the parameter would be set right, now a rerun would lead to my pipeline being rerun but with the date of today not the failed run date.</p>
<p>I am used to airflow where such things are pretty easy to do, including scheduling reruns. Probably I think too much in terms of airflow but I can't wrap my head around a better solution.</p>
","<azure-data-factory>","2023-01-19 10:42:38","119","0","2","75172758","<ul>
<li>In ADF,it is not supported directly to pass trigger date at which pipeline got failed to trigger.</li>
<li>You can get the trigger time using <code>@pipeline().TriggerTime</code> .</li>
<li>This system variable will give the time at which the trigger triggers the pipeline to run.</li>
</ul>
<p><img src=""https://i.imgur.com/8e5GxWg.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can store this trigger value for every pipeline and use this as a parameter for the trigger which got failed and rerun the pipeline.</li>
</ul>
<p>Reference: Microsoft document on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables#pipeline-scope"" rel=""nofollow noreferrer"">System Variables on ADF</a></p>
"
"75171036","Trigger Date for reruns","<p>My pipelines activities need the date of the run as a parameter. Now I get the current date in the pipeline from the utcnow() function. Ideally this would be something I could enter dynamically in the trigger so I could rerun a failed day and the parameter would be set right, now a rerun would lead to my pipeline being rerun but with the date of today not the failed run date.</p>
<p>I am used to airflow where such things are pretty easy to do, including scheduling reruns. Probably I think too much in terms of airflow but I can't wrap my head around a better solution.</p>
","<azure-data-factory>","2023-01-19 10:42:38","119","0","2","75230564","<p>To resolve my problem I had to create a nested structure of pipelines, the top pipeline setting a variable for the date and then calling other pipelines passing that variable.
<a href=""https://i.stack.imgur.com/5FC7G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5FC7G.png"" alt=""enter image description here"" /></a></p>
<p>With this I still can't rerun the top pipeline but rerunning Execute Pipeline1/2/3 reruns them with the right variable set. It is still not perfect since the top pipeline run stays an error and it is difficult to keep track of what needs to be rerun, however it is a partial solution.</p>
"
"75169286","ADF Unpivot Dynamically With New Column","<p>There is an Excel worksheet that I wanted to unpivot all the columns after &quot;Currency Code&quot; into rows, the number of columns need to be unpivot might vary, new columns might be added after &quot;NetIUSD&quot;. Is there a way to dynamically unpivot this worksheet with unknown columns?</p>
<p><a href=""https://i.stack.imgur.com/sN6bG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sN6bG.png"" alt=""enter image description here"" /></a></p>
<p>It worked when I projected all the fields and define the datatype for all the numerical fields as &quot;double&quot; and set the unpivot column data type as &quot;double&quot; as well. However, the issue is there might be additional columns added to the source file, which I won't be able to define the datatype ahead, in this case, if the new column has different data type other than &quot;double&quot;, it will throw an error that the new column is not of the same unpivot datatype.</p>
","<azure-data-factory><azure-mapping-data-flow>","2023-01-19 08:08:57","177","0","2","75170204","<p>I tried to repro this in Dataflow with sample input details.</p>
<p><img src=""https://i.imgur.com/Yuvktzg.png"" alt=""enter image description here"" /></p>
<ul>
<li>Take the unpivot transformation and in unpivot settings do the following.</li>
</ul>
<pre><code>Ungroup by:  Code, Currency_code
Unpivot column: Currency
Unpivoted Columns: Column arrangement: Normal
                   Column name: Amount
                   Type: string
</code></pre>
<p><img src=""https://user-images.githubusercontent.com/113445679/213407139-defda8fa-b381-44fc-93c4-09d9fd0fcbe0.gif"" alt=""gif1112"" /></p>
<p><strong>Data Preview</strong></p>
<p><img src=""https://i.imgur.com/ijmdodV.png"" alt=""enter image description here"" /></p>
<p>All columns other than mentioned in ungroup by can be dynamically unpivoted even if you add additional fields.</p>
"
"75169286","ADF Unpivot Dynamically With New Column","<p>There is an Excel worksheet that I wanted to unpivot all the columns after &quot;Currency Code&quot; into rows, the number of columns need to be unpivot might vary, new columns might be added after &quot;NetIUSD&quot;. Is there a way to dynamically unpivot this worksheet with unknown columns?</p>
<p><a href=""https://i.stack.imgur.com/sN6bG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sN6bG.png"" alt=""enter image description here"" /></a></p>
<p>It worked when I projected all the fields and define the datatype for all the numerical fields as &quot;double&quot; and set the unpivot column data type as &quot;double&quot; as well. However, the issue is there might be additional columns added to the source file, which I won't be able to define the datatype ahead, in this case, if the new column has different data type other than &quot;double&quot;, it will throw an error that the new column is not of the same unpivot datatype.</p>
","<azure-data-factory><azure-mapping-data-flow>","2023-01-19 08:08:57","177","0","2","75182282","<p>I confirm an Aswin answer. Got the same issue: failed dataflow with dynamically new columns. The reason was in datatype of unpivoted columns. Changed that to string and all goes smoothly.
Imported projection does not affect this case i`ve tried both with imported and manually coded, both works with &quot;string&quot; datatype.</p>
"
"75168760","Get ADF pipelines status using az cli - bash script","<p>I have been trying to get adf pipelines status using az cli script.
I am using</p>
<p><code>az datafactory pipeline-run query-by-factory --factory-name &quot;adfname&quot; --filters operand=&quot;Status&quot; operator=&quot;Equals&quot; values=&quot;Failed&quot; --last-updated-after &quot;2023-01-17T00:00:00.3345758Z&quot; --last-updated-before &quot;2023-01-17T11:59:59.3686473Z&quot; --resource-group &quot;rgname&quot;</code></p>
<p>command and I am getting full json of pipelines but I only want name and status of these pipelines. I have tried using jQuery like --query &quot;pipelineName&quot;, --query &quot;status&quot;. Pipeline is succeeding but I am not getting any results.
Please help me for the issue if anyone have knowledge about it.</p>
<p>I am expecting result like pipelineName -- status.
e.g.,</p>
<p>pl_databricks -- Failed
pl_databricks_mq -- Succeeded.</p>
<p>If possible date and time also</p>
<p>pl_databricks    -- Failed    -- 23/12/22 10:29:27
pl_databricks_mq -- Succeeded -- 23/12/22 08:20:50</p>
","<bash><azure><shell><automation><azure-data-factory>","2023-01-19 07:06:57","104","0","1","75230489","<ul>
<li>I have  reproduced in my environment and got outputs as below:</li>
</ul>
<pre><code>$Target=@()

$x=az datafactory pipeline-run query-by-factory --factory-name &quot;adfname&quot; --filters operand=&quot;Status&quot; operator=&quot;Equals&quot; values=&quot;Succeeded&quot; --last-updated-after &quot;2023-01-15T00:00:00.3345758Z&quot; --last-updated-before &quot;2023-06-16T00:36:44.3345758Z&quot; --resource-group &quot;rgname&quot;

$r=$x | ConvertFrom-Json

$Target= $r.value.pipelinename +&quot; &quot;+ $r.value.status

$Target
</code></pre>
<p><img src=""https://i.imgur.com/zwVhfwk.png"" alt=""enter image description here"" /></p>
"
"75166632","How to mask existing datas in Azure Table Storage","<p>I have an azure table storage with thousands of records. I need to mask certain sensitive data eg :
CustomerName : Tim Captain as T***** C****
BSBNumber :0342 8765 as ***8765</p>
","<azure><azure-data-factory><azure-table-storage>","2023-01-19 00:23:49","78","0","1","75169044","<p>Good day!</p>
<p>Unlike Azure SQL databases/synapse which provides the functionality of Dynamic data masking, Azure table storage doesnt have any such functionality.
In case if you need masking aspect, you would have to have a custom logic while loading data to mask data during ingestion and based on access need use a reverse logic to unmask while extracting</p>
"
"75161692","Azure: How to provide limited Access Level to a Container in a Storage Account?","<p>Me and my team are using Azure Synapse Analytics to ingest data from a REST API to a Azure Data Lake Storage Gen2, in order to create views automatically.</p>
<p><strong>The only way we could manage to do this in our Workspace was by previously changing the Public Access Level to the Container inside our Storage Account to &quot;Container (anonymous read access for containers and blobs)&quot;.</strong></p>
<p><strong>Is there any way to avoid doing this, and <em><strong>just enable this level of access to specific containers for a limited amount of users / IPs, while keeping it &quot;Private (no anonymous access)&quot;</strong></em>?</strong></p>
<p><a href=""https://i.stack.imgur.com/6mSMU.png"" rel=""nofollow noreferrer"">Click to Azure Portal view of the Containers inside a certain Storage Account resource</a></p>
","<azure><azure-data-factory><azure-synapse>","2023-01-18 15:38:20","104","0","2","75162026","<p>Yes, you can use Shared Access Signatures for it. You can find more information in here:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview</a></p>
"
"75161692","Azure: How to provide limited Access Level to a Container in a Storage Account?","<p>Me and my team are using Azure Synapse Analytics to ingest data from a REST API to a Azure Data Lake Storage Gen2, in order to create views automatically.</p>
<p><strong>The only way we could manage to do this in our Workspace was by previously changing the Public Access Level to the Container inside our Storage Account to &quot;Container (anonymous read access for containers and blobs)&quot;.</strong></p>
<p><strong>Is there any way to avoid doing this, and <em><strong>just enable this level of access to specific containers for a limited amount of users / IPs, while keeping it &quot;Private (no anonymous access)&quot;</strong></em>?</strong></p>
<p><a href=""https://i.stack.imgur.com/6mSMU.png"" rel=""nofollow noreferrer"">Click to Azure Portal view of the Containers inside a certain Storage Account resource</a></p>
","<azure><azure-data-factory><azure-synapse>","2023-01-18 15:38:20","104","0","2","75167676","<p>I do agree with @Thiago Custodio and @Nandan, we can use SAS Token and URL for limited access to Container in the storage accounts.</p>
<p><strong>To get SAS token:</strong></p>
<ul>
<li>Firstly, open the storage account, then click on container.</li>
<li>Then click on your container and then click on Shared Access tokens as below.</li>
<li>Then you can select what access you want to give access as below. Then Generate the token.</li>
</ul>
<p><img src=""https://i.imgur.com/3QIMPgZ.png"" alt=""enter image description here"" /></p>
<p>The token comes as below, now you can send this token to whom you want to give access your container.</p>
<p><img src=""https://i.imgur.com/a0ITi5J.png"" alt=""enter image description here"" /></p>
<p>Alternatively, you can also create a private endpoint as below:</p>
<ul>
<li>Firstly, click on <strong>Networking</strong>, the click on <strong>Private endpoint connections</strong> and then <strong>(+)</strong> create a end point.</li>
</ul>
<p><img src=""https://i.imgur.com/J47pAF7.png"" alt=""enter image description here"" /></p>
<p>Now this Container can be accessed from the Virtual Machine integrated with this private endpoint.</p>
"
"75161076","Azure Data Factory SSIS Integration Runtime SSISDB not showing in location dropdown","<p>on Azure Data Factory (ADF) we have a SSISDB integration runtime (IR). We want to connect to Azure SQL cloud server / database to run SSIS packages from ADF. SSISDB already exists on this server. All about IR looks OK, its running, VNET/Subnet tests are green, IR test connection to Server endpoint is green. VNet injection method: Standard.</p>
<p>However, when creating a pipeline on ADF, activity &quot;Execute SSIS PAckage&quot;, in section &quot;Settings&quot; there is a dropdown for Package location. In this dropdown, the option &quot;SSISDB&quot; is missing.
This is how it should look like:
<a href=""https://i.stack.imgur.com/OxExv.png"" rel=""nofollow noreferrer"">enter image description here</a>
But this &quot;SSISDB&quot; is missing.</p>
<p>Already tried Self-Hosted Integration Runtime as a proxy, all green lights for all tests, but still SSISDB missing in dropdown.</p>
<p>I tried the same on synapse, with same results.</p>
<p>Could anyone give some advise on this? How can I get the &quot;SSISDB&quot; option into the location dropdown?</p>
","<sql-server><ssis><azure-data-factory>","2023-01-18 14:50:51","154","0","1","75182764","<p>As you would like to use an existing SSISDB in SSIS Integration Runtime.</p>
<p>But as per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-azure-ssis-integration-runtime#prerequisites"" rel=""nofollow noreferrer"">Prerequisites of Create an Azure-SSIS integration runtime</a></p>
<blockquote>
<p>Confirm that your database server does not have an SSISDB instance already. The provisioning of an Azure-SSIS IR does not support using an existing SSISDB instance.</p>
</blockquote>
<p>This might be the reason you are not able to view SSISDB option in the Package location dropdown.</p>
<p>Follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-azure-ssis-integration-runtime-portal"" rel=""nofollow noreferrer"">provision an Azure-SSIS IR using the Azure portal</a> this document to set up an Azure-SSIS IR.</p>
"
"75159338","Azure Data Factory: Dynamic path value for the Storage Event Trigger","<p>I have created an azure data factory pipeline to copy the data from one adls container to another adls container using copy data activity.
This copy activity will trigger using a storage event trigger.</p>
<p>So whenever a new file gets generated, it will trigger the activity.<br />
The source file is located in a nested directory structure having dynamic folders such as year, month, and day, which vary based on date.</p>
<p>In the trigger, I mentioned the path until the fixed folder path, but I don't know what value I should put for the dynamic path.<br />
Initially, I provided the path such as <code>my/fixed/directory/*/*/*/</code>,<br />
but at the time of execution, it throws the exception 'PathNotFound'.</p>
<p>So my question is - How can I provide the path to the storage event trigger with the dynamic folder structure?
Following is ADF copy data pipeline screenshot:<br />
<strong>Pipeline-</strong>
<a href=""https://i.stack.imgur.com/RxEpe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RxEpe.png"" alt=""enter image description here"" /></a></p>
<p><strong>Copy data activity source configuration-</strong>
<a href=""https://i.stack.imgur.com/MxTDO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MxTDO.png"" alt=""enter image description here"" /></a></p>
<p><strong>Copy data activity target configuration-</strong>
<a href=""https://i.stack.imgur.com/GJCbx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GJCbx.png"" alt=""enter image description here"" /></a></p>
<p><strong>Copy data activity source dataset configuration-</strong>
<a href=""https://i.stack.imgur.com/AWinj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AWinj.png"" alt=""enter image description here"" /></a></p>
<p><strong>Copy data activity target dataset configuration-</strong>
<a href=""https://i.stack.imgur.com/trzAX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/trzAX.png"" alt=""enter image description here"" /></a></p>
<p><strong>Storage event configuration-</strong>
<a href=""https://i.stack.imgur.com/2uLW2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2uLW2.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-storage><azure-data-factory>","2023-01-18 12:36:17","442","0","1","75159913","<ul>
<li>Wildcards are not supported for <code>blob path begins with</code> or <code>blob path ends with</code> in storage event triggers.</li>
<li>However, creating a storage event trigger on the fixed parent directory would trigger the pipeline for any file created/deleted in child directories as well.</li>
<li>Let's say I have the folder structure as shown below where <code>input/folder/2022</code> is my fixed directory (input is container name). I also have sub folders within each of the folders shown below.</li>
</ul>
<p><img src=""https://i.imgur.com/8GZSLAm.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have created a copy data activity. The folder name and file name dynamic content for source dataset is shown below (parameter values will be passed from pipeline):</li>
</ul>
<pre><code>folder path:  @replace(dataset().folder_name,'input/','')

file name:  @dataset().file_name
</code></pre>
<p><img src=""https://i.imgur.com/OUGXnOQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>The folder name and file name dynamic content for sink dataset is shown below. This is a different container named <code>data</code>:</li>
</ul>
<pre><code>folder path: @concat('output/',replace(dataset().folder,'input/folder/',''))

file name: @dataset().file
</code></pre>
<p><img src=""https://i.imgur.com/5ihPBUp.png"" alt=""enter image description here"" /></p>
<ul>
<li>After configuring the copy activity is done, create a storage event trigger.</li>
</ul>
<p><img src=""https://i.imgur.com/xorbbmU.png"" alt=""enter image description here"" /></p>
<ul>
<li>Here, the values from pipeline parameters <code>folderName</code> and <code>fileName</code> will be set while creating trigger as shown below:</li>
</ul>
<pre><code>fileName : @triggerBody().fileName
folderName : @triggerBody().folderPath
</code></pre>
<p><img src=""https://i.imgur.com/Fr1gh8k.png"" alt=""enter image description here"" /></p>
<ul>
<li>After you attach the trigger and create a pipeline, when ever any file is uploaded to any folder within the fixed directory <code>folder/2022</code> the pipeline will be triggered.</li>
<li>I have uploaded a file to <code>folder/2022/03/01/sample1.csv</code>. This triggered the pipeline successfully.</li>
</ul>
<p><img src=""https://i.imgur.com/sYKgdTG.png"" alt=""enter image description here"" /></p>
<ul>
<li>The file is successfully copied as well. The following is an image for reference:</li>
</ul>
<p><img src=""https://i.imgur.com/mwQWMMY.png"" alt=""enter image description here"" /></p>
<p>So, creating a storage event trigger for just the parent directory is sufficient to be able to trigger the pipeline for any file uploaded to child directories as well.</p>
"
"75157679","adf mapping parameters to json file keeping type","<p>My goal is to save to a json file, the parameters passed in a Pipeline.
I added the parameter to the pipeline
<a href=""https://i.stack.imgur.com/RTB4t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RTB4t.png"" alt=""enter image description here"" /></a>
And with a copy activity I'm trying to save the value to a json (in a blob storage). The problem is that I can save as string in a json, but I cannot as integer
<a href=""https://i.stack.imgur.com/oUJ4e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oUJ4e.png"" alt=""enter image description here"" /></a></p>
<p>Using dynamic content @int(pipeline.parameters...) doesn't work.
Is it possible to save in an integer format?</p>
<p>thanks</p>
<p>I tried to cast as dynamic content and also importing the schema from mapping schema, the type is still a string</p>
","<azure><azure-data-factory>","2023-01-18 10:19:18","116","0","1","75158483","<p>I tried same scenario and getting same error as <code>Expression of type int dose not match the field value</code>.</p>
<p><strong>Even we provided direct value int in additional column it is taking it as string.</strong></p>
<p>The work around is to use dataflow with derived column and data flow parameter and pipeline parameter as shown below:</p>
<ul>
<li><p>First create pipeline parameter with <code>integer</code> type.
<img src=""https://i.imgur.com/4t0DhT4.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Create data flow and add source file to it then take derived column and create column and give value as parameter1.
<img src=""https://i.imgur.com/PoLqMtl.png"" alt=""enter image description here"" />
Click on <code>Open expression builder</code> &gt;&gt; parameters &gt;&gt;create new.
<img src=""https://i.imgur.com/Y4JRrcL.png"" alt=""enter image description here"" />
Give name to parameter as <strong>parameter1</strong> and type as <code>integer</code> and click on create.
<img src=""https://i.imgur.com/JugPBxQ.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then come back to pipeline cand click on dataflow activity &gt;&gt; Parameters &gt;&gt; click on dynamic expression below value box &gt;&gt; select <code>Pipeline expression</code> and pass the pipeline parameter which we created.
<img src=""https://i.imgur.com/7qndFkd.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>OUTPUT</strong></p>
<p>Taking as <code>int</code>
<img src=""https://i.imgur.com/gSLlpwA.png"" alt=""enter image description here"" /></p>
"
"75155114","How to send the output values of a Lookup activity in an email in Data Factory?","<p>I'm trying to send a LookUp activity output values as part of a body parameter in a POST request using LogicApp, which uses three parameters: &quot;to&quot;, &quot;email_body&quot;, &quot;subject&quot;.</p>
<p>The LookUp activity depends on a query, and it may return from 2 rows up to 10 rows.
According to Azure, the output of the activity should look like this:</p>
<pre><code>{
    &quot;count&quot;: 2,
    &quot;value&quot;: [
        {
            &quot;column1&quot;:value1,
            &quot;column2&quot;:value2,
            &quot;column3&quot;:value3
        },
        {
            &quot;column1&quot;:value4,
            &quot;column2&quot;:value5,
            &quot;column3&quot;:value6
        }
    ]
}
</code></pre>
<p>In this case, the query returned 2 rows, but how can I attach every output value to the POST body without having to use <code> @activity('lookup_act').output.value[0].column1</code> and so on for every value?</p>
<p>The POST body is the following:</p>
<pre><code>{
    &quot;email_body&quot;: &quot;Hi, the following tables have been updated:
        @{activity('lookup_act').output.value[0].column1}
        @{activity('lookup_act').output.value[1].column1}&quot;,
    &quot;subject&quot;: &quot;Update on tables&quot;,
    &quot;to&quot;: &quot;email@domain.com&quot;
}
    
</code></pre>
<p>I've tried using <code>@activity('lookup_act').output.value</code> to bring every value but it won't work.</p>
<p>Is there a way to call every single output value? If so, how can it be done and paste into a table?</p>
<p>Thanks beforehand.</p>
","<azure><lookup><azure-data-factory>","2023-01-18 05:50:11","470","0","1","75155910","<p>There are two ways to get all values in mail:</p>
<p><strong>1. Get whole lookup output array in mail.</strong></p>
<ul>
<li>First get the results from Lookup activity and then pass the output of this activity by <strong>converting it into a string</strong> otherwise you will get error regarding deserialization.</li>
</ul>
<p><img src=""https://i.imgur.com/xYZsR33.png"" alt=""enter image description here"" /></p>
<pre class=""lang-json prettyprint-override""><code>{&quot;message&quot;:&quot;@string(activity('Lookup1').output.value)&quot;,
&quot;dataFactoryName&quot;:&quot;@{pipeline().DataFactory}&quot;,
&quot;pipelineName&quot;:&quot;@{pipeline().Pipeline}&quot;,
&quot;receiver&quot;:&quot;@{pipeline().parameters.receiver}&quot;}
</code></pre>
<p><strong>OUTPUT</strong></p>
<p><img src=""https://i.imgur.com/BI3cjS7.png"" alt=""enter image description here"" /></p>
<p><strong>2. Get all the respective values column wise.</strong></p>
<ul>
<li>First get the results from Lookup activity then take a foreach loop and create append variable for every column to store every column value in single array.</li>
</ul>
<p>ForEach activity setting:
<img src=""https://i.imgur.com/8hbdSDD.png"" alt=""enter image description here"" /></p>
<p>Took append variable activity and created <code>Idarray</code> variable. and gave <code>item().id</code> as value to store all id values in a single array.
<img src=""https://i.imgur.com/HiC64MI.png"" alt=""enter image description here"" /></p>
<p>Then in web activity passed below body for getting all arrays.
<img src=""https://i.imgur.com/qoYDc7t.png"" alt=""enter image description here"" /></p>
<pre class=""lang-json prettyprint-override""><code>{&quot;message&quot;:&quot;@{string(variables('Idarray'))} as Id, @{string(variables('Namearray'))} as Name, @{string(variables('ProfessionArray'))} as Profession&quot;,
&quot;dataFactoryName&quot;:&quot;@{pipeline().DataFactory}&quot;,
&quot;pipelineName&quot;:&quot;@{pipeline().Pipeline}&quot;,
&quot;receiver&quot;:&quot;@{pipeline().parameters.receiver}&quot;}
</code></pre>
<p><strong>OUTPUT</strong></p>
<p><img src=""https://i.imgur.com/k5eY5P2.png"" alt=""enter image description here"" /></p>
"
"75154112","ADF Check If Header Row Start with A1","<p>In the Azure Data Factory pipeline, if I have an Excel file that I am going to perform data transformation. Is there a way to check if the header row start with row 1 first, if yes, then goes to the next step for data mapping flow, and if not, will return a message note that the data doesn't not start with row 1? Thank you!</p>
<p>I tried with lookup activity, but seems like I have to create a dataset first for the Excel file which is not ideal in my use case.</p>
","<azure-blob-storage><azure-data-factory>","2023-01-18 02:17:19","90","0","1","75155428","<p>You can use First row as header in source options which takes the First row and check the columns names in Conditional split like below.</p>
<p><img src=""https://i.imgur.com/uW9BY0w.png"" alt=""enter image description here"" /></p>
<p>If the A1 row is total empty in excel it will take 2nd row as header, then use conditional split and check the column names.</p>
<pre><code>equals(columnNames()[1],'Id')
</code></pre>
<p><img src=""https://i.imgur.com/xnQQGps.png"" alt=""enter image description here"" /></p>
<p>Now, you will get the data which satisfies your condition like below.</p>
<p><img src=""https://i.imgur.com/WxP63z8.png"" alt=""enter image description here"" /></p>
<p>Use your next transformation after this(<code>check1</code>). If your header is not the one you want it will go to <code>check2</code> and stop the execution.</p>
"
"75152693","Trouble with Copy Data Mapping: cannot convert data from REST API Get Request","<p>My goal is to get data through a GET Request and, using Copy Data, introduce it in parquet file format in the Data Lake. My pipeline currently works, but I wish not to map manually all the variables and their respective types.</p>
<p>I would like to use the Copy Data without a specific variable mapping in order to keep it flexible, and for that purpose I believe I can select the option &quot;Map complex values to string&quot;. However, when I try Debugging my pipeline, the following message appears:</p>
<blockquote>
<p>{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
'Value' contains an invalid value '541.9481'. Cannot convert
'541.9481' to type
'Int64'.,Source=Microsoft.DataTransfer.Common,''Type=System.OverflowException,Message=Value
was either too large or too small for an Int64.,Source=mscorlib,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;Copy to Landing&quot;,
&quot;details&quot;: [] }</p>
</blockquote>
<p>How can I avoid this situation so that every variable in the response to the GET request flows into the parquet file without errors like the one mentioned above?</p>
<p>Is there another way to achieve the same goal (i.e., Copy Data without manually mapping the variables)?</p>
<p>Thanks!</p>
<p>Mateo</p>
","<azure-data-factory><azure-synapse>","2023-01-17 22:05:19","157","0","1","75158384","<p>Copy data with Auto Mapping will automatically detect the type for sink dataset. Therefore, this may result in error.</p>
<p><code>Message=Value was either too large or too small for an Int64.,</code></p>
<blockquote>
<p>Is there another way to achieve the same goal (i.e., Copy Data without manually mapping the variables)?</p>
</blockquote>
<p>You can give the mapping in the dynamic content.</p>
<ul>
<li>You need to give type, mapping and collection reference as a json document.</li>
</ul>
<p><em>Sample Json mapping document:</em></p>
<pre class=""lang-json prettyprint-override""><code> { 
 &quot;type&quot;: &quot;TabularTranslator&quot;, 
 &quot;mappings&quot;: [
  {
   &quot;source&quot;: 
      { &quot;name&quot;: &quot;Id&quot; },
      {&quot;type&quot;: &quot;int&quot;}
   &quot;sink&quot;:
      { &quot;name&quot;: &quot;CustomerID&quot; },
      {&quot;type&quot;: &quot;int&quot;} 
   },
   {
    &quot;source&quot;: 
      { &quot;name&quot;: &quot;Name&quot; },
      {&quot;type&quot;: &quot;string&quot;}
    &quot;sink&quot;: 
      { &quot;name&quot;: &quot;LastName&quot; },
      {&quot;type&quot;: &quot;string&quot;} 
    },
    { 
     &quot;source&quot;: 
       { &quot;name&quot;: &quot;LastModifiedDate&quot; },,
      {&quot;type&quot;: &quot;string&quot;}
     &quot;sink&quot;:
       { &quot;name&quot;: &quot;ModifiedDate&quot; } ,
      {&quot;type&quot;: &quot;string&quot;}
     }
 ] 
}
</code></pre>
<ul>
<li>Create a pipeline parameter of <em>object</em> type and assign the Json mapping document as default value or give the Json mapping document during runtime.</li>
<li>Add source and sink dataset.</li>
<li>In mapping, Click <em>Add dynamic content</em>.</li>
</ul>
<p><img src=""https://i.imgur.com/tOqjYyL.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Click the pipeline parameter name that is created.
<img src=""https://i.imgur.com/j1aush7.png"" alt=""enter image description here"" /></p>
</li>
<li><p>If we have multiple copy activities to be done in single pipeline, You can store the Mapping Json for each copy activity in Lookup table and call it using lookup activity and refer that value in mapping section of copy activity.</p>
</li>
</ul>
<p>Reference: <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/3e87a117-b3e8-4554-b3af-9434a15e9c66/how-to-do-a-dynamic-column-mapping-in-copy-activity?forum=AzureDataFactory"" rel=""nofollow noreferrer"">How to do a Dynamic Column mapping in Copy Activity (microsoft.com)</a></p>
"
"75138288","How to grab a cell value and write it as a new column. Excel to SQL copy activity Azure Data Factory","<p>I have a copy activity pipeline that simply copies cells A6:C9 into sql table.
<a href=""https://i.stack.imgur.com/jpTC1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jpTC1.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/xyJFO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xyJFO.png"" alt=""enter image description here"" /></a></p>
<p>However, now I need to grab the value in a cell A3 (date) and write it as a new column in sql table.</p>
<p><a href=""https://i.stack.imgur.com/USRHf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/USRHf.png"" alt=""enter image description here"" /></a></p>
<p>How can I achieve that?
Do I need to use another copy activity? Or it can be done in a single one?</p>
<p>UPDATE: value A3:A3 in Lookup activity.</p>
<p><a href=""https://i.stack.imgur.com/whh3I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/whh3I.png"" alt=""enter image description here"" /></a></p>
","<sql-server><azure-data-factory>","2023-01-16 18:29:22","57","0","1","75138723","<p>You may fetch the desired value with a Lookup activity, and then add it to the Copy Data activity as an additional column. Please see the example below.</p>
<p>Start by creating a range parameter on your Excel dataset, so you may provide it dynamically in the pipeline:</p>
<p><a href=""https://i.stack.imgur.com/OGLkO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OGLkO.png"" alt=""Parameterizing Excel dataset"" /></a></p>
<p>Make sure the Range parameter is added in the Connection tab as well.</p>
<p><a href=""https://i.stack.imgur.com/4ZaPn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4ZaPn.png"" alt=""enter image description here"" /></a></p>
<p>Next, create a pipeline with a Lookup activity, followed by a Copy Data activity. The lookup activity should provide the range for the cell you want to capture (in my case, B1)</p>
<p><a href=""https://i.stack.imgur.com/co80g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/co80g.png"" alt=""Lookup activity"" /></a></p>
<p>Finally, in the Copy Data activity, insert an additional column and provide it the Lookup activity output.</p>
<p><a href=""https://i.stack.imgur.com/jEU7k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jEU7k.png"" alt=""Copy Data activity"" /></a></p>
<p>The expression I used is <code>@activity('Lookup1').output.firstRow.Prop_0</code>. This will add a column called Date with the value in B1, for every row.</p>
"
"75135313","How to run Azure Data Factory pipeline on Java Eclipse","<p>I am doing a project to integrate ADF pipeline with Java project. Is there any possible way to run the ADF pipeline remotely using Java?</p>
<p>I am a newcomer to this field and I didn't find any valuable articles regarding the question. Pls let me know if there is any possible solution to do so.</p>
","<java><azure><azure-data-factory><bdd>","2023-01-16 14:02:11","98","-1","1","75137356","<p>You may use the REST API to trigger pipeline runs from external applications, including Java. It is a POST request, and you may find details here:</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run?tabs=HTTP"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run?tabs=HTTP</a></p>
<p>Alternatively, there is a Data Factory client library for Java. It appears to be in beta, but provides the functionality you are looking for:</p>
<p><a href=""https://learn.microsoft.com/en-us/java/api/overview/azure/resourcemanager-datafactory-readme?view=azure-java-preview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/java/api/overview/azure/resourcemanager-datafactory-readme?view=azure-java-preview</a></p>
"
"75134972","Is it possible to use Azure Data Factory on-premise without data running through the cloud?","<p>is it possible to use Azure Data Factory on-premise without letting the data run through the cloud? I know Talend got a prodcut, where the data is transfered only on our machines and not in the cloud.</p>
<p>Read documentation on Microsoft.com but didnt find any useful information</p>
","<azure><cloud><azure-data-factory>","2023-01-16 13:35:10","300","0","1","75136997","<p>You may use a self-hosted integration runtime to transfer data entirely through your on-premises infrastructure, as long as both the data source and sink are on-premises.</p>
<p>However, the control flow will still happen through the cloud, even if the data itself never leaves your data center. For this reason, setting up a self-hosted integration runtime will still require outbound network access from your infrastructure to Azure.</p>
<p>Check out this piece of documentation for more information: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#command-flow-and-data-flow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#command-flow-and-data-flow</a></p>
"
"75133521","Move Files from Multiple Folders to Multiple Folders in Azure Blob","<p>I have a folder structure in Azure Blob like this</p>
<p>Container/app_archive/app1/app1.csv</p>
<p>Container/app_archive/app2/app2.csv</p>
<p>Container/app_archive/app3/app3.csv</p>
<p>Container/app_archive/app4/app4.csv</p>
<p>Container/app_archive/app5/app5.csv</p>
<p>....</p>
<p>Container/app_archive/app150/app150.csv</p>
<p>These needs to be moved to Container/app_archive/app1/YYYY/MM/DD/app1.csv</p>
<p>Container/app_archive/app2/YYYY/MM/DD/app2.csv</p>
<p>.....</p>
<p>Container/app_archive/app150/YYYY/MM/DD/app150.csv</p>
<p>Whenever any file is placed in any folder, it has to trigger and copy the files accordingly. Also I need to capture this information in an audit table like Source File Name, Source File Path, Destination File Path etc etc. How to achieve this ?</p>
","<azure><azure-functions><azure-data-factory>","2023-01-16 11:22:41","117","0","1","75143712","<p>You can use <strong>Storage event triggers with Dataset parameters</strong> for this like below.</p>
<p>First Give the Root container and Blob path ends with as .csv in Storage event trigger.</p>
<p><img src=""https://i.imgur.com/dOu4Ehw.png"" alt=""enter image description here"" /></p>
<p>Create two pipeline parameters and assign the trigger values to those while creating trigger.</p>
<p><img src=""https://i.imgur.com/RDHt0jo.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/0n5LuGy.png"" alt=""enter image description here"" /></p>
<p>Now, create dataset parameters for folder name and file names for both source and sink datasets.</p>
<p><strong>Source:</strong></p>
<p><img src=""https://i.imgur.com/zGOz0w5.png"" alt=""enter image description here"" /></p>
<p><strong>Sink:</strong></p>
<p><img src=""https://i.imgur.com/nFQu7jc.png"" alt=""enter image description here"" /></p>
<p><strong>My pipeline JSON:</strong></p>
<pre><code> {

&quot;name&quot;: &quot;pipeline1&quot;,

&quot;properties&quot;: {

&quot;activities&quot;: [

{

&quot;name&quot;: &quot;Copy data1&quot;,

&quot;type&quot;: &quot;Copy&quot;,

&quot;dependsOn&quot;: [

{

&quot;activity&quot;: &quot;Set variable1&quot;,

&quot;dependencyConditions&quot;: [

&quot;Succeeded&quot;

]

}

],

&quot;policy&quot;: {

&quot;timeout&quot;: &quot;0.12:00:00&quot;,

&quot;retry&quot;: 0,

&quot;retryIntervalInSeconds&quot;: 30,

&quot;secureOutput&quot;: false,

&quot;secureInput&quot;: false

},

&quot;userProperties&quot;: [],

&quot;typeProperties&quot;: {

&quot;source&quot;: {

&quot;type&quot;: &quot;DelimitedTextSource&quot;,

&quot;storeSettings&quot;: {

&quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,

&quot;recursive&quot;: true,

&quot;enablePartitionDiscovery&quot;: false

},

&quot;formatSettings&quot;: {

&quot;type&quot;: &quot;DelimitedTextReadSettings&quot;

}

},

&quot;sink&quot;: {

&quot;type&quot;: &quot;DelimitedTextSink&quot;,

&quot;storeSettings&quot;: {

&quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;

},

&quot;formatSettings&quot;: {

&quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,

&quot;quoteAllText&quot;: true,

&quot;fileExtension&quot;: &quot;.txt&quot;

}

},

&quot;enableStaging&quot;: false,

&quot;translator&quot;: {

&quot;type&quot;: &quot;TabularTranslator&quot;,

&quot;typeConversion&quot;: true,

&quot;typeConversionSettings&quot;: {

&quot;allowDataTruncation&quot;: true,

&quot;treatBooleanAsNumber&quot;: false

}

}

},

&quot;inputs&quot;: [

{

&quot;referenceName&quot;: &quot;Source1&quot;,

&quot;type&quot;: &quot;DatasetReference&quot;,

&quot;parameters&quot;: {

&quot;filename&quot;: {

&quot;value&quot;: &quot;@pipeline().parameters.filename&quot;,

&quot;type&quot;: &quot;Expression&quot;

},

&quot;folderpath&quot;: {

&quot;value&quot;: &quot;@pipeline().parameters.path&quot;,

&quot;type&quot;: &quot;Expression&quot;

}

}

}

],

&quot;outputs&quot;: [

{

&quot;referenceName&quot;: &quot;target1&quot;,

&quot;type&quot;: &quot;DatasetReference&quot;,

&quot;parameters&quot;: {

&quot;sinkpath&quot;: {

&quot;value&quot;: &quot;@variables('var_path')&quot;,

&quot;type&quot;: &quot;Expression&quot;

},

&quot;sinkfilename&quot;: {

&quot;value&quot;: &quot;@pipeline().parameters.filename&quot;,

&quot;type&quot;: &quot;Expression&quot;

}

}

}

]

},

{

&quot;name&quot;: &quot;Set variable1&quot;,

&quot;type&quot;: &quot;SetVariable&quot;,

&quot;dependsOn&quot;: [],

&quot;userProperties&quot;: [],

&quot;typeProperties&quot;: {

&quot;variableName&quot;: &quot;var_path&quot;,

&quot;value&quot;: {

&quot;value&quot;: &quot;@concat(split(pipeline().parameters.path,'/')[2],'/',formatDateTime(utcNow(),'yyyy/MM/dd'),'/')&quot;,

&quot;type&quot;: &quot;Expression&quot;

}

}

}

],

&quot;parameters&quot;: {

&quot;path&quot;: {

&quot;type&quot;: &quot;string&quot;

},

&quot;filename&quot;: {

&quot;type&quot;: &quot;string&quot;

}

},

&quot;variables&quot;: {

&quot;var_path&quot;: {

&quot;type&quot;: &quot;String&quot;

},

&quot;var1&quot;: {

&quot;type&quot;: &quot;String&quot;

}

},

&quot;annotations&quot;: []

}

}
</code></pre>
<p><strong>Result when a file uploaded to app1 folder:</strong></p>
<p><img src=""https://i.imgur.com/NCumIdq.png"" alt=""enter image description here"" /></p>
"
"75132994","Issue with Copy Activity Metadata","<p>The Copy Data Activity Which used to show the number of rows written isnt showing up any more.</p>
<p><a href=""https://i.stack.imgur.com/fbqDE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fbqDE.png"" alt=""enter image description here"" /></a>
Is there any option in the copy activity to make sure it reflects the number of rows written.</p>
","<azure-data-factory>","2023-01-16 10:32:19","59","0","1","75133160","<ul>
<li><p>Be it debug of the pipeline, or a triggered pipeline run, you can check the output of the <code>copy data</code> activity to conclude whether the data read is equal to data written or not.</p>
</li>
<li><p>Let's say it is a pipeline run. Navigate to monitor section and click on the pipeline.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/WvqGDls.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, the activity run dialog opens up. There you can monitor from the activity debug output whether the data read is equal to data written or not:</li>
</ul>
<p><img src=""https://i.imgur.com/Vv6H1UO.png"" alt=""enter image description here"" /></p>
<p><strong>NOTE:</strong> The above is for blob to blob copy. For your source and sink, there will be similar activity output data that might contain the required information (like rows read and rows written). The following is an example for Azure SQL database to blob:</p>
<p><a href=""https://i.stack.imgur.com/T1PFw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T1PFw.png"" alt=""enter image description here"" /></a></p>
"
"75111981","Combine Date and Time in Azure Data Factory?","<p>I have two Columns. One Date Column and one Time Column.</p>
<p>ADF reads my Source Data like this.
DateColumn: 2022-01-01 (Date format)</p>
<p>TimeColumn: 1899-12-31T05:59:59.000Z (String Format)</p>
<p>In an old Use Case without ADF I was able to convert the Columns into Numbers, and add them to get a timestamp with the date and the time.</p>
<p>I don't know how to handle the Timestamp in ADF, every approach leads to only Null Values.</p>
","<timestamp><type-conversion><azure-data-factory>","2023-01-13 16:34:04","213","0","1","75130711","<p>I have reproduced the above and got below results in Dataflow.</p>
<p>This is my sample input:</p>
<p><img src=""https://i.imgur.com/Eee23LK.png"" alt=""enter image description here"" /></p>
<p>As you already had a date in the time column, when you try to combine the above it will null values only.</p>
<p>So, try to combine and form the result column from the time stamp time column(<strong>split from T</strong>) like below.</p>
<pre><code>toTimestamp(concat(toString(date),'T',split(time,'T')[2]), 'yyyy-MM-dd\'T\'HH:mm:ss')
</code></pre>
<p><strong>Result in Derived column:</strong></p>
<p><img src=""https://i.imgur.com/CnNc67H.png"" alt=""enter image description here"" /></p>
"
"75108349","Can I simply start a databricks cluster in Azure Data Factory without a job or notebook?","<p>I'm starting a Databricks notebook from ADF to do some preprocessing tasks.
The cluster of this notebook is usually not running and should only run, when the ADF pipeline is running as well.</p>
<p>But it takes several minutes for the compute cluster to start, which of course slows down the execution of the pipeline.</p>
<p>My question now is, if there is any possibility, to trigger the cluster in an earlier stage of the ADF pipeline, so it is already starting in the background, while earlier stages of the ADF pipeline are still running. Like this, I could speed up the pipeline in total.</p>
<p>I already searched the databricks menue and also the ADF menue and toolbars but didn't find a solution.</p>
<p>Thanks for your help!</p>
","<azure><azure-data-factory><azure-databricks>","2023-01-13 11:03:49","96","1","2","75108602","<p>I think there is a option called <strong>Existing Interactive Pool</strong>.
Please refer this video for more info : <a href=""https://www.youtube.com/watch?v=VZggcUdIO14"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=VZggcUdIO14</a>.</p>
<p>To utilize <strong>Existing Interactive Pool</strong> i think there should be some clusters in cluster pool</p>
<p>For info related to cluster pool refer this link: <a href=""https://learn.microsoft.com/en-us/azure/databricks/clusters/instance-pools/create?source=recommendations"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/databricks/clusters/instance-pools/create?source=recommendations</a></p>
"
"75108349","Can I simply start a databricks cluster in Azure Data Factory without a job or notebook?","<p>I'm starting a Databricks notebook from ADF to do some preprocessing tasks.
The cluster of this notebook is usually not running and should only run, when the ADF pipeline is running as well.</p>
<p>But it takes several minutes for the compute cluster to start, which of course slows down the execution of the pipeline.</p>
<p>My question now is, if there is any possibility, to trigger the cluster in an earlier stage of the ADF pipeline, so it is already starting in the background, while earlier stages of the ADF pipeline are still running. Like this, I could speed up the pipeline in total.</p>
<p>I already searched the databricks menue and also the ADF menue and toolbars but didn't find a solution.</p>
<p>Thanks for your help!</p>
","<azure><azure-data-factory><azure-databricks>","2023-01-13 11:03:49","96","1","2","75110959","<p>We can use cluster pool. Azure Databricks pools reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances. When a cluster is attached to a pool, cluster nodes are created using the pool’s idle instances. If the pool has no idle instances, the pool expands by allocating a new instance from the instance provider in order to accommodate the cluster’s request. When a cluster releases an instance, it returns to the pool and is free for another cluster to use. Only clusters attached to a pool can use that pool’s idle instances. You can check <a href=""https://learn.microsoft.com/en-us/azure/databricks/clusters/instance-pools/"" rel=""nofollow noreferrer"">link</a></p>
"
"75107909","Getting error while writing parquet files to Azure data lake storage gen 2","<p>Hi I have a usecase where I am reading parquet files and writing it to ADLG Gen 2. This is without any modification to data.</p>
<p>MY Code:</p>
<pre><code>val kustoLogsSourcePath: String = &quot;/mnt/SOME_FOLDER/2023/01/11/fe73f221-b771-49c9-ba7d-2e2af4fe4f2a_1_69fc119b888447efa9ed2ecd7a4ab647.parquet&quot; 
val outputPath: String = &quot;/mnt/SOME_FOLDER/2023/01/10/EventLogs1/&quot; 
val kustoLogData = spark.read.parquet(kustoLogsSourcePath) 
kustoLogData.write.mode(SaveMode.Overwrite).save(outputPath)
</code></pre>
<p>I am getting this error, any ideas how to solve it:
Here, I have shared all the exception related messages that I got.</p>
<p>org.apache.spark.SparkException: Job aborted.
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)
at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:192)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:110)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:108)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:128)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:143)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:183)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:180)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:131)
at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)
at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:114)
at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:690)
at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:690)
at</p>
<hr />
<p>Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 276 in stage 2.0 failed 4 times, most recent failure: Lost task 276.3 in stage 2.0 (TID 351, 10.139.64.13, executor 5): com.databricks.sql.io.FileReadException: Error while reading file dbfs:[REDACTED]/eventlogs/2023/01/10/[REDACTED-FILE-NAME].parquet.
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:272)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:256)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:197)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.scan_nextBatch_0$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)</p>
<hr />
<p>Caused by: java.lang.UnsupportedOperationException: Unsupported encoding: DELTA_BYTE_ARRAY
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.initDataReader(VectorizedColumnReader.java:584)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV2(VectorizedColumnReader.java:634)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.access$100(VectorizedColumnReader.java:49)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:557)
at</p>
<hr />
<p>Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:[REDACTED]/eventlogs/2023/01/11/fe73f221-b771-49c9-ba7d-2e2af4fe4f2a_1_69fc119b888447efa9ed2ecd7a4ab647.parquet.
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:272)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:256)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:197)
at</p>
<hr />
<p>Caused by: java.lang.UnsupportedOperationException: Unsupported encoding: DELTA_BYTE_ARRAY
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.initDataReader(VectorizedColumnReader.java:584)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV2(VectorizedColumnReader.java:634)
at</p>
","<apache-spark><azure-data-factory><parquet><azure-databricks><azure-data-lake-gen2>","2023-01-13 10:26:07","208","0","2","75122062","<p>It seems that some columns are DELTA_BYTE_ARRAY encoded, a workarround would be to turn off the vectorized reader property:</p>
<pre><code>spark.conf.set(&quot;spark.sql.parquet.enableVectorizedReader&quot;, &quot;false&quot;)
</code></pre>
"
"75107909","Getting error while writing parquet files to Azure data lake storage gen 2","<p>Hi I have a usecase where I am reading parquet files and writing it to ADLG Gen 2. This is without any modification to data.</p>
<p>MY Code:</p>
<pre><code>val kustoLogsSourcePath: String = &quot;/mnt/SOME_FOLDER/2023/01/11/fe73f221-b771-49c9-ba7d-2e2af4fe4f2a_1_69fc119b888447efa9ed2ecd7a4ab647.parquet&quot; 
val outputPath: String = &quot;/mnt/SOME_FOLDER/2023/01/10/EventLogs1/&quot; 
val kustoLogData = spark.read.parquet(kustoLogsSourcePath) 
kustoLogData.write.mode(SaveMode.Overwrite).save(outputPath)
</code></pre>
<p>I am getting this error, any ideas how to solve it:
Here, I have shared all the exception related messages that I got.</p>
<p>org.apache.spark.SparkException: Job aborted.
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)
at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:192)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:110)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:108)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:128)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:143)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:183)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:180)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:131)
at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)
at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:114)
at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:690)
at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:690)
at</p>
<hr />
<p>Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 276 in stage 2.0 failed 4 times, most recent failure: Lost task 276.3 in stage 2.0 (TID 351, 10.139.64.13, executor 5): com.databricks.sql.io.FileReadException: Error while reading file dbfs:[REDACTED]/eventlogs/2023/01/10/[REDACTED-FILE-NAME].parquet.
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:272)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:256)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:197)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.scan_nextBatch_0$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)</p>
<hr />
<p>Caused by: java.lang.UnsupportedOperationException: Unsupported encoding: DELTA_BYTE_ARRAY
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.initDataReader(VectorizedColumnReader.java:584)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV2(VectorizedColumnReader.java:634)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.access$100(VectorizedColumnReader.java:49)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:557)
at</p>
<hr />
<p>Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:[REDACTED]/eventlogs/2023/01/11/fe73f221-b771-49c9-ba7d-2e2af4fe4f2a_1_69fc119b888447efa9ed2ecd7a4ab647.parquet.
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:272)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:256)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:197)
at</p>
<hr />
<p>Caused by: java.lang.UnsupportedOperationException: Unsupported encoding: DELTA_BYTE_ARRAY
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.initDataReader(VectorizedColumnReader.java:584)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV2(VectorizedColumnReader.java:634)
at</p>
","<apache-spark><azure-data-factory><parquet><azure-databricks><azure-data-lake-gen2>","2023-01-13 10:26:07","208","0","2","75131454","<p><em><strong>Try to modify your code and also remove the string parameter in the font of the variable and also use <code>.format(&quot;delta&quot;)</code> for reading delta file.</strong></em></p>
<pre><code>%scala

val kustoLogsSourcePath = &quot;/mnt/SOME_FOLDER/2023/01/11/&quot; 
val outputPath = &quot;/mnt/SOME_FOLDER/2023/01/10/EventLogs1/&quot; 
val kustoLogData = spark.read.format(&quot;delta&quot;).load(kustoLogsSourcePath)
kustoLogData.write.format(&quot;parquet&quot;).mode(&quot;append&quot;).mode(SaveMode.Overwrite).save(outputPath)
</code></pre>
<p><em>For the demo, this is my FileStore location <code>/FileStore/tables/delta_train/</code>.</em></p>
<p><img src=""https://i.imgur.com/q0tfXUt.png"" alt=""enter image description here"" /></p>
<p><strong>I reproduce same in my environment as per above code .I got this output.</strong></p>
<p><img src=""https://i.imgur.com/5S0pR8H.png"" alt=""enter image description here"" /></p>
"
"75105345","Can't read REST API with an XML response using Synapse Pipeline's Copy Activity","<p>Trying to read REST API endpoints through Synapse pipeline and sinking it in a JSON format. The API response is XML and the run ends up erroring out.</p>
<p>--------Error---------
{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=JsonInvalidDataFormat,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error occurred when deserializing source JSON file ''. Check if the data is in valid JSON object format.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=Newtonsoft.Json.JsonReaderException,Message=Unexpected character encountered while parsing value: &lt;. Path '', line 0, position 0.,Source=Newtonsoft.Json,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;Copy REST API Data&quot;,
&quot;details&quot;: []
}
--------Error---------</p>
<p>Do not want to go back and use existing C# script-based code which is currently run through SSIS packages.</p>
<p>Any assistance will be appreciated.</p>
","<rest><azure-synapse><azure-data-factory>","2023-01-13 05:50:36","123","0","1","75105894","<p>I tried to repro this using Rest connector in ADF and got the same error. Rest connector supports only JSON file. Refer this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest?tabs=data-factory#supported-capabilities"" rel=""nofollow noreferrer"">Microsoft Document</a> on Supported capabilities of REST connector.</p>
<p><img src=""https://i.imgur.com/huSq8qr.png"" alt=""enter image description here"" /></p>
<p>Instead use <strong>HTTP connector</strong> and select <strong>XML dataset</strong>. Below is the approach to do it.</p>
<ul>
<li><p>Select HTTP in linked service.
<img src=""https://i.imgur.com/XUVBKVv.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Enter Base URL and authentication type and then click create.
<img src=""https://i.imgur.com/OUYfyn9.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Create new dataset for HTTP linked service. Select HTTP and then continue.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/YyUxusl.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Select XML format and then select continue.
<img src=""https://i.imgur.com/iPJy45t.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Give the linked service and relative url and then click OK.
<img src=""https://i.imgur.com/rmTrHMf.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Use this dataset as source dataset in copy activity. Once pipeline is run, data will be copied to sink.</p>
</li>
</ul>
"
"75104282","Azure Data Factory Copy XML Activity","<p>I'm copying XML to a database using a &quot;Copy Activity&quot;.</p>
<p>XML file has a nested structure so I have defined a &quot;Collection reference&quot; at &quot;Cond_Tbl_Data_Record&quot; level.
<a href=""https://i.stack.imgur.com/D9xVR.png"" rel=""nofollow noreferrer"">mapping</a>
the following is written to db correctly.</p>
<pre><code>`&lt;Cond_Tbl_Data_Set&gt;
    &lt;Cond_Tbl_Data_Record&gt;
        &lt;Base_Per_Quantity&gt;1&lt;/Base_Per_Quantity&gt;
        &lt;Base_UOM_Code&gt;GA&lt;/Base_UOM_Code&gt;
        &lt;Condition_Table_ID&gt;A02&lt;/Condition_Table_ID&gt;
        &lt;Condition_Type&gt;COCO&lt;/Condition_Type&gt;
        &lt;Condition_Value&gt;829&lt;/Condition_Value&gt;
        &lt;Currency_Code&gt;USC&lt;/Currency_Code&gt;
        &lt;Extraction_Time&gt;20230113 19:41:03&lt;/Extraction_Time&gt;
        &lt;Key_Values&gt;US/000001/001&lt;/Key_Values&gt;
        &lt;Valid_From_Date&gt;20230113&lt;/Valid_From_Date&gt;
        &lt;Valid_To_Date&gt;99991231&lt;/Valid_To_Date&gt;
        &lt;Effective_Start_Time&gt;13:22:42&lt;/Effective_Start_Time&gt;
        &lt;Condition_Change_Value&gt;300&lt;/Condition_Change_Value&gt;
    &lt;/Cond_Tbl_Data_Record&gt;
    &lt;Cond_Tbl_Data_Record&gt;
        &lt;Base_Per_Quantity&gt;1&lt;/Base_Per_Quantity&gt;
        &lt;Base_UOM_Code&gt;GA&lt;/Base_UOM_Code&gt;
        &lt;Condition_Table_ID&gt;A04&lt;/Condition_Table_ID&gt;
        &lt;Condition_Type&gt;COCO&lt;/Condition_Type&gt;
        &lt;Condition_Value&gt;829&lt;/Condition_Value&gt;
        &lt;Currency_Code&gt;USC&lt;/Currency_Code&gt;
        &lt;Extraction_Time&gt;20230113 19:41:03&lt;/Extraction_Time&gt;
        &lt;Key_Values&gt;US/000001/002&lt;/Key_Values&gt;
        &lt;Valid_From_Date&gt;20230113&lt;/Valid_From_Date&gt;
        &lt;Valid_To_Date&gt;99991231&lt;/Valid_To_Date&gt;
        &lt;Effective_Start_Time&gt;13:22:42&lt;/Effective_Start_Time&gt;
        &lt;Condition_Change_Value&gt;300&lt;/Condition_Change_Value&gt;
    &lt;/Cond_Tbl_Data_Record&gt;
&lt;/Cond_Tbl_Data_Set&gt;`
</code></pre>
<p>but the following is not written to db at all.</p>
<pre><code>`&lt;Cond_Tbl_Data_Set&gt;
    &lt;Cond_Tbl_Data_Record&gt;
        &lt;Base_Per_Quantity&gt;1&lt;/Base_Per_Quantity&gt;
        &lt;Base_UOM_Code&gt;GA&lt;/Base_UOM_Code&gt;
        &lt;Condition_Table_ID&gt;A02&lt;/Condition_Table_ID&gt;
        &lt;Condition_Type&gt;COCO&lt;/Condition_Type&gt;
        &lt;Condition_Value&gt;829&lt;/Condition_Value&gt;
        &lt;Currency_Code&gt;USC&lt;/Currency_Code&gt;
        &lt;Extraction_Time&gt;20230113 19:41:03&lt;/Extraction_Time&gt;
        &lt;Key_Values&gt;US/000001/001&lt;/Key_Values&gt;
        &lt;Valid_From_Date&gt;20230113&lt;/Valid_From_Date&gt;
        &lt;Valid_To_Date&gt;99991231&lt;/Valid_To_Date&gt;
        &lt;Effective_Start_Time&gt;13:22:42&lt;/Effective_Start_Time&gt;
        &lt;Condition_Change_Value&gt;300&lt;/Condition_Change_Value&gt;
    &lt;/Cond_Tbl_Data_Record&gt;
&lt;/Cond_Tbl_Data_Set&gt;`
</code></pre>
<p>I tried to reimport schema but still did not work</p>
","<azure><azure-data-factory>","2023-01-13 02:26:03","54","0","1","75105244","<blockquote>
<p>As per information you are providing when your file has single object in array Its not getting copied.</p>
</blockquote>
<p><strong>The cause of issue is when you have single object in array it will take it as object not as array.</strong></p>
<p>To resolve the issue, you have to first clear the mapping and then again import the mapping. So, it will take that as another object not an array as shown in below image:</p>
<p><img src=""https://i.imgur.com/34PzzbR.png"" alt=""enter image description here"" /></p>
<p><strong>My sample input:</strong></p>
<p><img src=""https://i.imgur.com/h751D0E.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/3DVQDHn.png"" alt=""enter image description here"" /></p>
"
"75098700","Split CSV file in Azure Data Factory based on additional headers in file","<p>I currently receive csv files in the following format:</p>
<pre><code>Col1, Col2, Col3, Col4
Header1, , ,
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
Header2, , ,
Val1, Val2, Val3, Val4
Header3, , ,
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
</code></pre>
<p>The number of rows per header can vary and the Headers can contain any words.</p>
<p>The expected result should be one of:
Option 1: Save headers to additional column in 1 file
File: abc/abc/complete_output</p>
<pre><code>Col1, Col2, Col3, Col4, Col5
Val1, Val2, Val3, Val4, Header1
Val1, Val2, Val3, Val4, Header1
Val1, Val2, Val3, Val4, Header1
Val1, Val2, Val3, Val4, Header2
Val1, Val2, Val3, Val4, Header3
Val1, Val2, Val3, Val4, Header3
</code></pre>
<p>Option 2: create different file per header:
File1: abc/abc/Header1</p>
<pre><code>Col1, Col2, Col3, Col4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
</code></pre>
<p>File2: abc/abc/Header2</p>
<pre><code>Col1, Col2, Col3, Col4
Val1, Val2, Val3, Val4
</code></pre>
<p>File3: abc/abc/Header3</p>
<pre><code>Col1, Col2, Col3, Col4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
</code></pre>
<p>The files should either be split from the received format to different files or the header rows should be mapped to an additional column. Can this be done in Azure Data Factory, including Data Flow options? There is no access to a Databricks cluster.</p>
<p>P.S. I know this would be easy with a Python script whatsoever, but I hope to be able to build the complete flow in ADF.</p>
<p>I tried splitting the file based on conditional split. However, this does not work, as this just allows to select rows. This could only be used if (one of) the row values gave an indication about the Header.</p>
<p>No other things seem usable to me.</p>
<p>Edit: added desired output options as asked</p>
","<csv><split><azure-data-factory>","2023-01-12 15:29:16","346","0","2","75132754","<p>If the input dataset is static, considering the second option as requirement then you can go with the following approach:</p>
<ol>
<li><p>Add Filter transformation after source with expression as : <code>!startsWith(Col1, 'Header')</code></p>
</li>
<li><p>Add surrogate key transformation to create the incremental identity column</p>
</li>
<li><p>Add conditional split transformation to split the data into three parts having these expressions:</p>
<p><code>stream1</code>: <code>Id&gt;=1 &amp;&amp; Id&lt;=3</code>
<code>stream2</code> : <code>Id==4</code>
<code>stream3 : </code> <code>Default</code></p>
</li>
<li><p>Use Select transformation to deselect 'Id' column</p>
</li>
<li><p>Add sink transformation to load the data to csv file</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/vfuZN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vfuZN.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/G8EQq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G8EQq.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/vwIbE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vwIbE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/LQrU4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LQrU4.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/bTkpo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bTkpo.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/e7qD7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e7qD7.png"" alt=""enter image description here"" /></a></p>
"
"75098700","Split CSV file in Azure Data Factory based on additional headers in file","<p>I currently receive csv files in the following format:</p>
<pre><code>Col1, Col2, Col3, Col4
Header1, , ,
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
Header2, , ,
Val1, Val2, Val3, Val4
Header3, , ,
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
</code></pre>
<p>The number of rows per header can vary and the Headers can contain any words.</p>
<p>The expected result should be one of:
Option 1: Save headers to additional column in 1 file
File: abc/abc/complete_output</p>
<pre><code>Col1, Col2, Col3, Col4, Col5
Val1, Val2, Val3, Val4, Header1
Val1, Val2, Val3, Val4, Header1
Val1, Val2, Val3, Val4, Header1
Val1, Val2, Val3, Val4, Header2
Val1, Val2, Val3, Val4, Header3
Val1, Val2, Val3, Val4, Header3
</code></pre>
<p>Option 2: create different file per header:
File1: abc/abc/Header1</p>
<pre><code>Col1, Col2, Col3, Col4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
</code></pre>
<p>File2: abc/abc/Header2</p>
<pre><code>Col1, Col2, Col3, Col4
Val1, Val2, Val3, Val4
</code></pre>
<p>File3: abc/abc/Header3</p>
<pre><code>Col1, Col2, Col3, Col4
Val1, Val2, Val3, Val4
Val1, Val2, Val3, Val4
</code></pre>
<p>The files should either be split from the received format to different files or the header rows should be mapped to an additional column. Can this be done in Azure Data Factory, including Data Flow options? There is no access to a Databricks cluster.</p>
<p>P.S. I know this would be easy with a Python script whatsoever, but I hope to be able to build the complete flow in ADF.</p>
<p>I tried splitting the file based on conditional split. However, this does not work, as this just allows to select rows. This could only be used if (one of) the row values gave an indication about the Header.</p>
<p>No other things seem usable to me.</p>
<p>Edit: added desired output options as asked</p>
","<csv><split><azure-data-factory>","2023-01-12 15:29:16","346","0","2","75132960","<ul>
<li>You can achieve this with in data factory using variables, loops and conditionals and copy data activity.</li>
<li>First, read the source file using look up activity without header and random row and column delimiters and <strong>without selecting</strong> <code>First row as header</code> option (So that you would get the output as shown in below image.) I have used <code>;</code> as column delimiter and <code>|</code> as row delimiter and.</li>
</ul>
<p><img src=""https://i.imgur.com/pYNDRXg.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have used multiple set variable activities. The <code>header</code> activity is to extract the header (col1,col2,col3,col4) from lookup output.</li>
</ul>
<pre><code>@first(array(split(activity('file as text').output.value[0]['Prop_0'],decodeUriComponent('%0A'))))
</code></pre>
<p><img src=""https://i.imgur.com/CeAWMPZ.png"" alt=""enter image description here"" /></p>
<ul>
<li><code>each file</code> set variable activity to store all the data for each file. I initialized it with <code>header</code> variable value.</li>
</ul>
<p><img src=""https://i.imgur.com/mka7IAM.png"" alt=""enter image description here"" /></p>
<ul>
<li><code>get first filename</code> is used to extract the name of first file (header1 in this case) using collection and string functions.</li>
</ul>
<pre><code>@replace(first(skip(array(split(activity('file as text').output.value[0]['Prop_0'],decodeUriComponent('%0A'))),1)),',,,','')
</code></pre>
<p><img src=""https://i.imgur.com/pS7gFqS.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, take the rest of the data (after header1 line), use it as items value in for each loop and then do further processing.</li>
</ul>
<pre><code>@skip(array(split(activity('file as text').output.value[0]['Prop_0'],decodeUriComponent('%0A'))),2)
</code></pre>
<p><img src=""https://i.imgur.com/Nx73kYG.png"" alt=""enter image description here"" /></p>
<ul>
<li>Inside for each, I have an <code>if condition</code> activity to check if the line is header (to be considered as filename) or not. Accordingly, I have concatenated each line accordingly and used copy data activity as per requirement.</li>
</ul>
<p><img src=""https://i.imgur.com/cd8i9TW.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following is the entire pipeline JSON (you can use this directly except that you have to create your datasets).</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;file as text&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;header&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;file as text&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;header&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@first(array(split(activity('file as text').output.value[0]['Prop_0'],decodeUriComponent('%0A'))))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;each file&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;header&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;each file&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@variables('header')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;get first filename&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;each file&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;filename&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@replace(first(skip(array(split(activity('file as text').output.value[0]['Prop_0'],decodeUriComponent('%0A'))),1)),',,,','')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;get first filename&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@skip(array(split(activity('file as text').output.value[0]['Prop_0'],decodeUriComponent('%0A'))),2)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;If Condition1&quot;,
                            &quot;type&quot;: &quot;IfCondition&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;expression&quot;: {
                                    &quot;value&quot;: &quot;@contains(item(),',,,')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                },
                                &quot;ifFalseActivities&quot;: [
                                    {
                                        &quot;name&quot;: &quot;each row&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;each row&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@concat(variables('each file'),decodeUriComponent('%0A'),item())&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    },
                                    {
                                        &quot;name&quot;: &quot;complete data&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [
                                            {
                                                &quot;activity&quot;: &quot;each row&quot;,
                                                &quot;dependencyConditions&quot;: [
                                                    &quot;Succeeded&quot;
                                                ]
                                            }
                                        ],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;each file&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@variables('each row')&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    }
                                ],
                                &quot;ifTrueActivities&quot;: [
                                    {
                                        &quot;name&quot;: &quot;create each file&quot;,
                                        &quot;type&quot;: &quot;Copy&quot;,
                                        &quot;dependsOn&quot;: [],
                                        &quot;policy&quot;: {
                                            &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                                            &quot;retry&quot;: 0,
                                            &quot;retryIntervalInSeconds&quot;: 30,
                                            &quot;secureOutput&quot;: false,
                                            &quot;secureInput&quot;: false
                                        },
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;source&quot;: {
                                                &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                                                &quot;additionalColumns&quot;: [
                                                    {
                                                        &quot;name&quot;: &quot;req&quot;,
                                                        &quot;value&quot;: {
                                                            &quot;value&quot;: &quot;@variables('each file')&quot;,
                                                            &quot;type&quot;: &quot;Expression&quot;
                                                        }
                                                    }
                                                ],
                                                &quot;storeSettings&quot;: {
                                                    &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                                                    &quot;recursive&quot;: true,
                                                    &quot;enablePartitionDiscovery&quot;: false
                                                },
                                                &quot;formatSettings&quot;: {
                                                    &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                                                }
                                            },
                                            &quot;sink&quot;: {
                                                &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                                                &quot;storeSettings&quot;: {
                                                    &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                                                },
                                                &quot;formatSettings&quot;: {
                                                    &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                                                    &quot;quoteAllText&quot;: true,
                                                    &quot;fileExtension&quot;: &quot;.txt&quot;
                                                }
                                            },
                                            &quot;enableStaging&quot;: false,
                                            &quot;translator&quot;: {
                                                &quot;type&quot;: &quot;TabularTranslator&quot;,
                                                &quot;mappings&quot;: [
                                                    {
                                                        &quot;source&quot;: {
                                                            &quot;name&quot;: &quot;req&quot;,
                                                            &quot;type&quot;: &quot;String&quot;
                                                        },
                                                        &quot;sink&quot;: {
                                                            &quot;type&quot;: &quot;String&quot;,
                                                            &quot;physicalType&quot;: &quot;String&quot;,
                                                            &quot;ordinal&quot;: 1
                                                        }
                                                    }
                                                ],
                                                &quot;typeConversion&quot;: true,
                                                &quot;typeConversionSettings&quot;: {
                                                    &quot;allowDataTruncation&quot;: true,
                                                    &quot;treatBooleanAsNumber&quot;: false
                                                }
                                            }
                                        },
                                        &quot;inputs&quot;: [
                                            {
                                                &quot;referenceName&quot;: &quot;demo&quot;,
                                                &quot;type&quot;: &quot;DatasetReference&quot;
                                            }
                                        ],
                                        &quot;outputs&quot;: [
                                            {
                                                &quot;referenceName&quot;: &quot;op_files&quot;,
                                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                                &quot;parameters&quot;: {
                                                    &quot;fileName&quot;: {
                                                        &quot;value&quot;: &quot;@variables('filename')&quot;,
                                                        &quot;type&quot;: &quot;Expression&quot;
                                                    }
                                                }
                                            }
                                        ]
                                    },
                                    {
                                        &quot;name&quot;: &quot;change filename&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [
                                            {
                                                &quot;activity&quot;: &quot;create each file&quot;,
                                                &quot;dependencyConditions&quot;: [
                                                    &quot;Succeeded&quot;
                                                ]
                                            }
                                        ],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;filename&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@replace(item(),',,,','')&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    },
                                    {
                                        &quot;name&quot;: &quot;re initialise each file value&quot;,
                                        &quot;type&quot;: &quot;SetVariable&quot;,
                                        &quot;dependsOn&quot;: [
                                            {
                                                &quot;activity&quot;: &quot;change filename&quot;,
                                                &quot;dependencyConditions&quot;: [
                                                    &quot;Succeeded&quot;
                                                ]
                                            }
                                        ],
                                        &quot;userProperties&quot;: [],
                                        &quot;typeProperties&quot;: {
                                            &quot;variableName&quot;: &quot;each file&quot;,
                                            &quot;value&quot;: {
                                                &quot;value&quot;: &quot;@variables('header')&quot;,
                                                &quot;type&quot;: &quot;Expression&quot;
                                            }
                                        }
                                    }
                                ]
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;for last file within csv&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;additionalColumns&quot;: [
                            {
                                &quot;name&quot;: &quot;req&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@variables('each file')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        ],
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;type&quot;: &quot;TabularTranslator&quot;,
                        &quot;mappings&quot;: [
                            {
                                &quot;source&quot;: {
                                    &quot;name&quot;: &quot;req&quot;,
                                    &quot;type&quot;: &quot;String&quot;
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;String&quot;,
                                    &quot;physicalType&quot;: &quot;String&quot;,
                                    &quot;ordinal&quot;: 1
                                }
                            }
                        ],
                        &quot;typeConversion&quot;: true,
                        &quot;typeConversionSettings&quot;: {
                            &quot;allowDataTruncation&quot;: true,
                            &quot;treatBooleanAsNumber&quot;: false
                        }
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;demo&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;op_files&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;,
                        &quot;parameters&quot;: {
                            &quot;fileName&quot;: {
                                &quot;value&quot;: &quot;@variables('filename')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        ],
        &quot;variables&quot;: {
            &quot;header&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;each file&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;filename&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;each row&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<ul>
<li>For copy data, the source data looks as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/qvXYCXn.png"" alt=""enter image description here"" /></p>
<ul>
<li>The sink of the copy data activity has the following dataset configurations (both source and sink dataset are same in 2 copy data activities):</li>
</ul>
<p><img src=""https://i.imgur.com/L48uyXM.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following are the outputs for each of the file for given sample data:</li>
</ul>
<p><img src=""https://i.imgur.com/AHuiK9U.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/6mOjmWk.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/R0NiWEU.png"" alt=""enter image description here"" /></p>
<ul>
<li>If the file ends with data (not header) then the file would be populated as required instead of empty file with just header.</li>
</ul>
"
"75091954","How to connect to DB2 database with SSL enabled in Azure Data Factory?","<p>I have an on-prem machine for DB2 connectivity to ADF where SHIR is installed, and I have been provided with the certificate zip file.
How to use SSL certificate for linked service of DB2 in data factory and how to get that certificate common name for linked service.</p>
<p><a href=""https://i.stack.imgur.com/1aak7.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-db2?tabs=data-factory#create-a-linked-service-to-db2-using-ui"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-db2?tabs=data-factory#create-a-linked-service-to-db2-using-ui</a> - following this but nothing in detail has been shared on how to setup SSL for source machine.</p>
","<db2><ssl-certificate><azure-data-factory><linked-service>","2023-01-12 05:26:56","227","0","1","75184164","<p>The procedures below must be followed in order to utilise an SSL certificate for a DB2 connected service in Azure Data Factory:</p>
<p>You should extract the.pem file from the given certificate file by unzipping it.</p>
<p>Go to the &quot;Author &amp; Monitor&quot; area of the Azure Data Factory, and then click the &quot;Author&quot; button.</p>
<p>Click the &quot;Author &amp; Monitor&quot; button on the left-hand menu.</p>
<p>Click the &quot;Linked services&quot; button in the &quot;Author&quot; section before selecting &quot;New.&quot;</p>
<p>As the type of connected service you wish to build, select &quot;DB21.&quot;</p>
<p><img src=""https://i.imgur.com/T5TgLAT.png"" alt=""enter image description here"" /></p>
<p>Fill out the appropriate data on the &quot;New Linked Service&quot; form, including the server name, port number, and username/password.</p>
<p><img src=""https://i.imgur.com/BXiBpDx.png"" alt=""enter image description here"" /></p>
<p>Select the &quot;Certificate&quot; button after selecting the &quot;Enable SSL&quot; checkbox in the &quot;Advanced&quot; section.</p>
<p><img src=""https://i.imgur.com/Jb784xn.png"" alt=""enter image description here"" /></p>
<p>Click the &quot;Add&quot; button in the &quot;Certificate&quot; dialogue box and then navigate to the location of the.pem file that you retrieved in step 1 before clicking.</p>
<p><img src=""https://i.imgur.com/IbGtlW3.png"" alt=""enter image description here"" />
The Common Name (CN) of the certificate will be automatically filled in once you've chosen the file.</p>
<p><img src=""https://i.imgur.com/ImvhhzK.png"" alt=""enter image description here"" /></p>
<p>To build the associated service, click &quot;Finish&quot; after clicking &quot;OK&quot; to exit the dialogue box.</p>
<p><img src=""https://i.imgur.com/A9tJ25D.png"" alt=""enter image description here"" /></p>
<p>The SSL certificate should now work with your Azure Data Factory linked service to connect to your DB2 source machine.</p>
"
"75091292","Dynamically running SQL scripts via ADF","<p>Is it possible to run simple select statements dynamically via ADF?</p>
<p>I want to copy tables from source to sink (sql to sql) using ADF. I am using the control table approach to load the data which allows me to dynamically load columns etc. However, for some tables I have where conditions etc or even something simple like</p>
<pre><code>SELECT 'Apple Sauce' as ColumnABC
FROM TableA
</code></pre>
<p>Can someone please either explain or guide me to how I can create a template pipeline using control table which fulfils this requirement.</p>
<p>Please do ask for more information if you need.</p>
","<sql><azure-data-factory><etl>","2023-01-12 03:32:46","67","0","1","75091948","<blockquote>
<p>Is it possible to run simple select statements dynamically via ADF?</p>
</blockquote>
<p>Yes, it is possible to run simple select statements dynamically. In copy activity there is query option which allow us to write query on table and get data from it.</p>
<p><strong>In copy activity source select <code>Query</code> type your query in query box.</strong></p>
<p><img src=""https://i.imgur.com/SuzVCiB.png"" alt=""enter image description here"" /></p>
"
"75088832","How to increase copy activity performance in ADF for ACDOCA table?","<p>I am copying data from SAP to Azure Synapse dedicated pool. The table name is ACDOCA from SAP side.</p>
<p>When I try to copy data from ACDOCA to Synapse, firstly it took around 8 minutes to first byte and then failed with internal memory error due to large amount of data in this table.</p>
<p>I have tried partition on calendar date for other tables like BKPF (On CPUDT column) and the performance was increased.</p>
<p>But, I am not able to decide which column should I consider for date partition in Copy Activity in ADF. FYI, AEDAT and BLDAT are not able to resolve the performance issue.</p>
<p><strong>Why I am doing this:</strong></p>
<p>I need the latest data of ACDOCA table from SAP to Synapse daily AND I want to implement a incremental load on this table using date column.</p>
<p>Thanks</p>
","<azure-data-factory><partitioning><sap-erp>","2023-01-11 20:45:22","129","0","1","75091910","<p>I would suggest using the Document number as Delta pointer as ACDOCA or ACDOCU table entries doesn't get modified at all, always new document number gets added when there is new posting happens</p>
"
"75087139","ADF: Using look up and foreach activities to get and array ouput","<p>I'm using <code>lookup</code> activity to scan my rows and <code>foreach</code> activity to concatinate my values into an array without any success.</p>
<p>The csv file that lookup is scanning looks like this:</p>
<pre><code>value
a
b
c
d
</code></pre>
<p>my <code>foreach</code> looks like this :
<a href=""https://i.stack.imgur.com/RioBa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RioBa.png"" alt=""enter image description here"" /></a></p>
<p>and inside my <code>foreach</code> I have an <code>set variable</code> activity which I'm aware that the datatype is not matching but do not know how to solve</p>
<p><a href=""https://i.stack.imgur.com/3yJYn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3yJYn.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-01-11 17:50:40","307","0","2","75092281","<blockquote>
<p>concatinate my values into an array without any success.</p>
</blockquote>
<p>Use <strong>append variable</strong> inside ForEach activity.</p>
<p><strong>My lookup activity:</strong></p>
<p><img src=""https://i.imgur.com/6uNQXB1.png"" alt=""enter image description here"" /></p>
<p>create an array variable. The second array variable is just for showing output.</p>
<p><img src=""https://i.imgur.com/LVFwmhv.png"" alt=""enter image description here"" /></p>
<p><strong>append variable:</strong></p>
<pre><code>@item().value
</code></pre>
<p><img src=""https://i.imgur.com/GQskmLg.png"" alt=""enter image description here"" /></p>
<p><strong>Output</strong>:</p>
<p><img src=""https://i.imgur.com/sQJQ0fH.png"" alt=""enter image description here"" /></p>
"
"75087139","ADF: Using look up and foreach activities to get and array ouput","<p>I'm using <code>lookup</code> activity to scan my rows and <code>foreach</code> activity to concatinate my values into an array without any success.</p>
<p>The csv file that lookup is scanning looks like this:</p>
<pre><code>value
a
b
c
d
</code></pre>
<p>my <code>foreach</code> looks like this :
<a href=""https://i.stack.imgur.com/RioBa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RioBa.png"" alt=""enter image description here"" /></a></p>
<p>and inside my <code>foreach</code> I have an <code>set variable</code> activity which I'm aware that the datatype is not matching but do not know how to solve</p>
<p><a href=""https://i.stack.imgur.com/3yJYn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3yJYn.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2023-01-11 17:50:40","307","0","2","75093420","<p>The error you are seeing is because you are trying to pass an array value to a string type variable.</p>
<p>For your requirement you will have to use AppendVariable activity inside your ForEach and load all the iteration values to your AppendVariable of type array. Then outside of you ForEach, have setVariable activity of type array variable and map the AppendVariable to SetVariable</p>
<p>Below is sample pipeline JSON for the same requirement. You can reuse it by replacing the source dataset</p>
<pre><code>{
&quot;name&quot;: &quot;pl_LookupToArray&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Lookup1&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;DelimitedText28&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;ForEach1&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@activity('Lookup1').output.value&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;appendVarArray&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@item().value&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Set variable2&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;ForEach1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;finalArrayValue&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@variables('appendVarArray')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        }
    ],
    &quot;variables&quot;: {
        &quot;appendVarArray&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;finalArrayValue&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: []
}
</code></pre>
<p>}</p>
<p>Here is how the pipeline flow looks:</p>
<p><a href=""https://i.stack.imgur.com/UdUil.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UdUil.png"" alt=""enter image description here"" /></a></p>
<p>Here is how your final output value of your array type variable looks like :</p>
<pre><code>{
&quot;name&quot;: &quot;finalArrayValue&quot;,
&quot;value&quot;: [
    &quot;a&quot;,
    &quot;b&quot;,
    &quot;c&quot;,
    &quot;d&quot;
]
</code></pre>
<p>}</p>
"
"75084911","Is there a way to do IntervalMatch in Azure Data Factory?","<p>Im trying to do an IntervalMatch from the following link in ADF:
<a href=""https://help.qlik.com/en-US/qlikview/May2022/Subsystems/Client/Content/QV_QlikView/Scripting/ScriptPrefixes/IntervalMatch.htm"" rel=""nofollow noreferrer"">https://help.qlik.com/en-US/qlikview/May2022/Subsystems/Client/Content/QV_QlikView/Scripting/ScriptPrefixes/IntervalMatch.htm</a></p>
<p>Is there an activity (Join, ...) or another way to achieve this?</p>
","<azure><azure-data-factory><transformation>","2023-01-11 14:50:03","60","0","1","75091848","<p>I tried to repro this in ADF data flow with sample inputs and below is the approach.</p>
<p><strong>Input tables:</strong>
<em>Event_log:</em></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Time</th>
<th>Event</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>0:00</td>
<td>0</td>
<td>Start of shift 1</td>
</tr>
<tr>
<td>1:18</td>
<td>1</td>
<td>Line stop</td>
</tr>
<tr>
<td>2:23</td>
<td>2</td>
<td>Line restart 50%</td>
</tr>
<tr>
<td>4:15</td>
<td>3</td>
<td>Line speed 100%</td>
</tr>
<tr>
<td>8:00</td>
<td>4</td>
<td>Start of shift 2</td>
</tr>
<tr>
<td>11:43</td>
<td>5</td>
<td>End of production</td>
</tr>
</tbody>
</table>
</div>
<p><em>Order_log:</em></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Start</th>
<th>End</th>
<th>Order</th>
</tr>
</thead>
<tbody>
<tr>
<td>1:00</td>
<td>03:35</td>
<td>A</td>
</tr>
<tr>
<td>2:30</td>
<td>07:58</td>
<td>B</td>
</tr>
<tr>
<td>3:04</td>
<td>10:27</td>
<td>C</td>
</tr>
<tr>
<td>7:23</td>
<td>11:43</td>
<td>D</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Source transformations (source1 and source2) are taken for the above tables.</li>
<li>Join transformation is taken. In Join settings
<ul>
<li>Source1 <em>(Event Log)</em> is taken as Left stream and Source2 <em>(Order_log)</em> is taken as right stream.</li>
<li>Join type is given as <em>Right Outer</em>.</li>
<li>Joining conditions are <strong>Start&lt;=Time and End&gt;=Time</strong>.</li>
</ul>
</li>
</ul>
<p><img src=""https://i.imgur.com/UIU7sMw.png"" alt=""enter image description here"" /></p>
<p><strong>Output of Join Transformation:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Start</th>
<th>End</th>
<th>Order</th>
<th>Time</th>
<th>Event</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
<td>0:00</td>
<td>0</td>
<td>Start of shift 1</td>
</tr>
<tr>
<td>1:00</td>
<td>3:35</td>
<td>A</td>
<td>1:18</td>
<td>1</td>
<td>Line stop</td>
</tr>
<tr>
<td>1:00</td>
<td>3:35</td>
<td>A</td>
<td>2:23</td>
<td>2</td>
<td>Line restart 50%</td>
</tr>
<tr>
<td>2:30</td>
<td>7:58</td>
<td>B</td>
<td>4:15</td>
<td>3</td>
<td>Line speed 100%</td>
</tr>
<tr>
<td>3:04</td>
<td>10:27</td>
<td>C</td>
<td>4:15</td>
<td>3</td>
<td>Line speed 100%</td>
</tr>
<tr>
<td>3:04</td>
<td>10:27</td>
<td>C</td>
<td>8:00</td>
<td>4</td>
<td>Start of shift 2</td>
</tr>
<tr>
<td>7:23</td>
<td>11:43</td>
<td>D</td>
<td>8:00</td>
<td>4</td>
<td>Start of shift 2</td>
</tr>
<tr>
<td>7:23</td>
<td>11:43</td>
<td>D</td>
<td>11:43</td>
<td>5</td>
<td>End of production</td>
</tr>
</tbody>
</table>
</div>
<p><img src=""https://i.imgur.com/kU4VbB5.png"" alt=""enter image description here"" /></p>
"
"75084399","Data Factory aggregate function array within array not allowed","<p>Is it possible to do something like this in Data Factory (data flow expression builder)? I am trying to create an array within an array but with a filter so it doesn't create an array with an empty item.</p>
<p>When I do this, it works however produces an array that looks like <code>fooBar: [{ }]</code> note the array contains a single item.</p>
<pre><code>collect(@(
optionNumber=OptionNo,
price=OptionPrice,
salePricePeriods=array(@(
salePrice=OptionSalePrice,
priceActiveFrom=toString(OptionSaleFrom),
priceActiveTo=toString(OptionSaleTo)))))
</code></pre>
<p>Ideally, I want to filter this data by using an expression:</p>
<pre><code>collect(@(
    optionNumber=OptionNo,
    price=OptionPrice,
    salePricePeriods=
    filter(
       collect(@(
          salePrice=OptionSalePrice, 
          priceActiveFrom=toString(OptionSaleFrom),
          priceActiveTo=toString(OptionSaleTo))),
       and(not(isNull(#item.salePrice)), and(not(isNull(#item.priceActiveFrom)), not(isNull(#item.priceActiveTo)))))))
</code></pre>
<p>When I do the above I get an error stating</p>
<p><strong>Job failed due to reason: It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;
Aggregate [958abb16-5236-430c-9af6-497495d60469#23243], [958abb16-5236-430c-9af6-497495d60469#23243, first(DataSet#22183, false) AS DataSet#23295, first(Realm#22184, false) AS Realm#23297, first(Territory#22185, false) AS Territory#23299, first(ItemNo#22186, false) AS ItemNo#23301, first(PriceGroupCode#22187, false) AS PriceGroupCode#23303, first(MinOptionPrice#22196, false) AS MinOptionPrice#23305, first(MaxOptionPrice#22197, false) AS MaxOptionPrice#23307, min(MinOptionSalePriceForPeriod#22198) AS MinOptionSalePrice#23309, max(MaxOptionSalePriceForPeriod#22199) AS MaxOptionSalePrice#23311, first(OldestDatePointMinPrice#22203, false) AS OldestDatePointMinPrice#23313, first(OldestDatePointMaxPrice#22204, false) AS OldestDatePointMaxPrice#23315, collect_list(named_struct(optionNumber, OptionNo#22189, price, OptionPrice#22191, salePrice</strong></p>
","<azure-data-factory>","2023-01-11 14:12:50","62","0","1","75098566","<p>Figured it out, annoyingly the filter should have been on the incoming field so like so. The filter expression seems to get evaluated first before adding it to the array.</p>
<pre><code>collect(@(
   optionNumber=OptionNo,
   price=OptionPrice,
   salePricePeriods=
filter(
      array(@(
        salePrice=OptionSalePrice, 
        priceActiveFrom=toString(OptionSaleFrom),
        priceActiveTo=toString(OptionSaleTo))),
      not(isNull(toString(OptionSaleFrom))))))
</code></pre>
"
"75081080","filter by last modified is not working copying files from azure blob","<p>I have setup a copy pipeline, which is working fine.
Now I have tried to add a filter with last modified, but it is not working.</p>
<p>my filter
<a href=""https://i.stack.imgur.com/JIDgf.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>a sample file it should detect
<a href=""https://i.stack.imgur.com/eC9QI.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>it doesn't detect any files
<a href=""https://i.stack.imgur.com/qp4EP.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>this is the pipeline without the filter
<a href=""https://i.stack.imgur.com/1uHt5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory>","2023-01-11 09:49:01","116","0","2","75083581","<p>I got similar kind issue with last modified filter using copy activity.</p>
<ul>
<li><p>Make sure your date is of type UTC as like below, if not convert it into UTC like below :</p>
</li>
<li><p>The above will only read the files if they are in that time period of UTC.</p>
</li>
</ul>
<p>Add this dynamic content filter by modified as per your required date.</p>
<pre><code>@convertToUtc('01/10/23', 'Pacific Standard Time')
</code></pre>
<p><img src=""https://i.imgur.com/tMr07M3.png"" alt=""enter image description here"" /></p>
<p><strong>Try this alterative approach using get metadata.Follow this <a href=""https://stackoverflow.com/questions/74531579/data-factory-synapse-copy-task-fails-with-staging-enabled-when-no-files-found/74545887#74545887"">SO</a> thread by Rakesh Govindula .</strong></p>
"
"75081080","filter by last modified is not working copying files from azure blob","<p>I have setup a copy pipeline, which is working fine.
Now I have tried to add a filter with last modified, but it is not working.</p>
<p>my filter
<a href=""https://i.stack.imgur.com/JIDgf.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>a sample file it should detect
<a href=""https://i.stack.imgur.com/eC9QI.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>it doesn't detect any files
<a href=""https://i.stack.imgur.com/qp4EP.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>this is the pipeline without the filter
<a href=""https://i.stack.imgur.com/1uHt5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory>","2023-01-11 09:49:01","116","0","2","75097514","<p>Hi you can use Azure Dynamic Functions like Addmintues so if your file was written two minutes ago it can detect by searching in Minutes and adding utcnow and -2 represent last two minutes. You can also add Days and Hours as well.</p>
<p>Let me know if this help otherwise same can be done with Get metadata Activity in azure</p>
<p><a href=""https://i.stack.imgur.com/jrnzU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jrnzU.png"" alt=""enter image description here"" /></a></p>
"
"75079994","How to add columns in GroupBy in Aggregate transformation dynamically in Azure Data Flow","<p>I have 200 columns ,out of those 100 columns need to be in GroupBy .So its very painful to add the 100 columns manually ,clicking the dropdown,selecting the column .It takes more time to add columns in this manner.</p>
<p>Is there any way ,to lessen the time to add up 100 cols in groupBy all at once.</p>
","<azure><azure-data-factory>","2023-01-11 08:13:35","166","0","1","75081656","<ul>
<li>AFAIK, there is no option within the aggregate transformation to dynamically select all the required columns that are of different types to apply group by operation.</li>
<li>One way you can try is to manually enter the names of all the columns to which you have to apply group by on, as an array.</li>
<li>Let's say I have the following data (all columns of same type):</li>
</ul>
<p><img src=""https://i.imgur.com/gotqS3R.png"" alt=""enter image description here"" /></p>
<ul>
<li>Considering I want to apply group by on <code>id and test</code> columns, the following is how I have given that. I used <code>collect</code> as my aggregate function on <code>team</code> column.</li>
</ul>
<p><img src=""https://i.imgur.com/NsoTrk1.png"" alt=""enter image description here"" /></p>
<ul>
<li>This will give the result as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/inS4vTS.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>But this requires us to convert all columns into the same type (string) to perform operations. And also converts the grouped data into an array of values as shown in the above picture (not separate columns but an array).</p>
</li>
<li><p>Another manual way that you can try is to alter the dataflow JSON to add the column names. If I select 2 columns using drop down,</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/H5dVmMo.png"" alt=""enter image description here"" /></p>
<ul>
<li>The JSON would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/dftxShV.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, if you want to include another column, it will change as shown below. You can build a similar string for required columns and edit the dataflow JSON.</li>
</ul>
<p><img src=""https://i.imgur.com/oR0EUDr.png"" alt=""enter image description here"" /></p>
"
"75078333","How to configure authentication from Azure data factory to Google Sheets","<p>I am trying to fetch data from Google Sheets via the Azure Data Factory.  I have a data flow configured, and a Google Sheets Linked Service configured as well.  The Google Sheets Linked Service is using an API key that has no Application Restrictions, and for API restrictions, it is restricted to Google Sheets. When I test the connection, it is successful.  However, when I try to fetch a spreadsheet using the data flow in debug mode, I get the error:</p>
<pre><code>    at Source 'GoogleSheetsOutput': Failure to read most recent page request: DF-REST_001 - Error response from server: Some({
&quot;error&quot;: {
&quot;code&quot;: 403,
&quot;message&quot;: &quot;The caller does not have permission&quot;,
&quot;status&quot;: &quot;PERMISSION_DENIED&quot;
}
}), Status code: 403. Please check your request url and body. (url:https://sheets.googleapis.com/v4/spreadsheets/&lt;my_spreadsheet_id&gt;/values/Sheet1!A2:B3,request body: None, request method: GET)
</code></pre>
<p>I made a service account, and shared the google sheet with the service account e-mail, and I also shared it with the admin on the account, which is where the API Key was made.</p>
<p>Any ideas what I need to do here?  I would also prefer to use oauth / service account instead if possible, so I am happy to go down that route if there is some documentation I can follow.</p>
","<google-api><google-oauth><azure-data-factory>","2023-01-11 04:25:47","158","0","1","75079270","<blockquote>
<p>&quot;error&quot;: { &quot;code&quot;: 403, &quot;message&quot;: &quot;The caller does not have permission&quot;, &quot;status&quot;: &quot;PERMISSION_DENIED&quot; }</p>
</blockquote>
<p>I tried to repro this and got the same error.</p>
<p><img src=""https://i.imgur.com/4U9I74B.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>To solve this, I changed access of my google sheets from restricted to <strong>Anyone with the link</strong>
<img src=""https://i.imgur.com/8F4DzKj.png"" alt=""enter image description here"" /></p>
</li>
<li><p>It worked and data is previewed without error.
<img src=""https://i.imgur.com/FdJ0bH4.png"" alt=""enter image description here"" /></p>
</li>
</ul>
"
"75075624","Copy subdirs + files from storage acct to ADX using Data Factory","<p>I'm trying to copy files from Az Storage blob to ADX using Data factory, but I can't find a solution to do this using json datasets (not binary), so I can map schema. I would love to have a template to do this.</p>
<p>I have tried to follow the resolution mentioned here (<a href=""https://stackoverflow.com/questions/72688135/get-metadata-from-blob-storage-with-folder-like-structure-using-azure-data-fac"">Get metadata from Blob storage with &quot;folder like structure&quot; using Azure Data Factory pipeline</a>), but I'm lacking some more guidance (this is my first project using ADF)</p>
<p><a href=""https://i.stack.imgur.com/fGrMp.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/H6I2k.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/8esWL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory><azure-data-explorer><file-copying>","2023-01-10 20:44:06","67","0","3","75077349","<p>From the error message you have shared, it seems like your dynamic expression for passing the childItems of your get metadata activity to ForEach activity <code>items</code> is causing this problem.</p>
<p>You might be using <code>Items</code> = <code>@activity('GetMetadataActivity').output</code> instead of <code>@activity('GetMetadataActivity').output.childItems</code>.</p>
<p>Please use <code>Items</code> = <code>@activity('GetMetadataActivity').output.childItems</code> in your ForEach activity which should help resolve your error.</p>
<p>Here is a video demonstration by a community volunteer where this error has been explained in detail: <a href=""https://www.youtube.com/watch?v=XP9xmt5pzQ0"" rel=""nofollow noreferrer"">ADF Error Function Length expects its parameter to be array or string</a></p>
"
"75075624","Copy subdirs + files from storage acct to ADX using Data Factory","<p>I'm trying to copy files from Az Storage blob to ADX using Data factory, but I can't find a solution to do this using json datasets (not binary), so I can map schema. I would love to have a template to do this.</p>
<p>I have tried to follow the resolution mentioned here (<a href=""https://stackoverflow.com/questions/72688135/get-metadata-from-blob-storage-with-folder-like-structure-using-azure-data-fac"">Get metadata from Blob storage with &quot;folder like structure&quot; using Azure Data Factory pipeline</a>), but I'm lacking some more guidance (this is my first project using ADF)</p>
<p><a href=""https://i.stack.imgur.com/fGrMp.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/H6I2k.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/8esWL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory><azure-data-explorer><file-copying>","2023-01-10 20:44:06","67","0","3","75079612","<p>Now I am getting another error as shown below</p>
<p>I'm actually also looking for a COMPLETE guide setting this up.</p>
<p>Here is my overall use-case target <a href=""https://learn.microsoft.com/en-us/azure/sentinel/store-logs-in-azure-data-explorer?tabs=azure-storage-azure-data-factory"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sentinel/store-logs-in-azure-data-explorer?tabs=azure-storage-azure-data-factory</a> - but the documentation is missing the detailed steps - in step 6 Create a data pipeline with a copy activity, based on when the blob properties were last modified. This step requires an extra understanding of Azure Data Factory. For more information, see Copy activity in Azure Data Factory and Azure Synapse Analytics.</p>
<p><a href=""https://i.stack.imgur.com/FRXlq.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/ylLzc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
"
"75075624","Copy subdirs + files from storage acct to ADX using Data Factory","<p>I'm trying to copy files from Az Storage blob to ADX using Data factory, but I can't find a solution to do this using json datasets (not binary), so I can map schema. I would love to have a template to do this.</p>
<p>I have tried to follow the resolution mentioned here (<a href=""https://stackoverflow.com/questions/72688135/get-metadata-from-blob-storage-with-folder-like-structure-using-azure-data-fac"">Get metadata from Blob storage with &quot;folder like structure&quot; using Azure Data Factory pipeline</a>), but I'm lacking some more guidance (this is my first project using ADF)</p>
<p><a href=""https://i.stack.imgur.com/fGrMp.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/H6I2k.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/8esWL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory><azure-data-explorer><file-copying>","2023-01-10 20:44:06","67","0","3","75079919","<p>It seems as I was going in a wrong direction.</p>
<p>I have actually found a simple solution for setting this up. I have tried the copy data tool, and it seems to be doing what I want. So case closed :-)</p>
"
"75074218","Azure Log Analytics - Data Factory - track Linked Service creation","<p>I'm trying to figure out when a Linked Service was added to a Data Factory instance.</p>
<p>We capture all logs and metrics in Log Analytics but so far nothing seems to show when a Linked Service was created.</p>
<p>The closest I've got is querying the Azure Activity table for this ADF querying for Linked Services/Write and the specific Linked service as shown below:</p>
<pre><code>| where OperationNameValue == &quot;MICROSOFT.DATAFACTORY/FACTORIES/LINKEDSERVICES/WRITE
| where Properties contains &quot;Name of my link service&quot;
</code></pre>
<p>This is not telling me the info I need, which is to know when this linked service was created. Doing a distinct search on the OperationNameValue does not yield anything else related to Linked Services. Perhaps the data I'm looking for is somewhere else?</p>
<p>Thanks for any help you may provide.</p>
","<azure><azure-data-factory><kql><azure-log-analytics-workspace>","2023-01-10 18:16:40","94","0","1","75081034","<p>To test this, I have created a ADF, linked service with blob storage and exported the activity logs to one of the log analytics workspaces.
Using the below KQL query I am able to pull the linked service creation time and followed by the caller who has created it.</p>
<pre><code>AzureActivity
| where OperationNameValue contains &quot;MICROSOFT.DATAFACTORY/FACTORIES/LINKEDSERVICES/WRITE&quot; and ActivityStatusValue contains &quot;Success&quot;
| extend linkedservicename=tostring(Properties_d[&quot;resource&quot;])
| where linkedservicename contains &quot;&lt;pass specific linked servicename&gt;&quot;
| project EventSubmissionTimestamp,_ResourceId,Caller
</code></pre>
<p><strong>Here is the sample output for reference:</strong></p>
<p><a href=""https://i.stack.imgur.com/28zvN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/28zvN.png"" alt=""enter image description here"" /></a></p>
<p>Alternatively, you can use the below PowerShell cmdlet as well to pull creation time of linked service.</p>
<pre><code>Get-AzActivityLog -StartTime (get-date).AddDays(-90) -EndTime (get-date)| Where-Object {$_.Authorization.Action -like &quot;MICROSOFT.DATAFACTORY/FACTORIES/LINKEDSERVICES/WRITE&quot; -and $_.Status -like &quot;Succeeded&quot; } | Select EventTimestamp,SubmissionTimestamp,Caller,ResourceId| ConvertTo-Json
</code></pre>
<p><strong>Here is the sample output of reference:</strong></p>
<p><a href=""https://i.stack.imgur.com/nPgEY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nPgEY.png"" alt=""enter image description here"" /></a></p>
"
"75073507","Unzip zip files within a zip file in Azure Data Factory","<p>The situation is that a zip file contains multiple zip files inside of it, I would like to essentially unzip 2 layers within ADF. Nearly all videos/guides only have one zip file that needs to unzipped so I am not sure what to do.</p>
<p>When I unzip the outer zip file inside the folder contains a binary file that is not useful</p>
","<azure><azure-data-factory>","2023-01-10 17:09:24","134","0","1","75172327","<p>In ADF, currently it is not supported to unzip two level zip files at the same time. So, you have to use copy activity two times to unzip the child level zip files like below- using Binary dataset.</p>
<p><img src=""https://i.imgur.com/JCZc2MF.png"" alt=""enter image description here"" /></p>
<p>give the temporary folder for sink of 1st copy activity.</p>
<p><img src=""https://i.imgur.com/YNaPJhh.png"" alt=""enter image description here"" /></p>
<p>Then use Get Meta data activity to list the child items and give that to a  ForEach and inside ForEach use another copy activity.</p>
<p><img src=""https://i.imgur.com/WyQKYs9.png"" alt=""enter image description here"" /></p>
<p>For this source use dataset parameters and give <code>@item().name</code> and in sink binary dataset give <code>file_name.csv</code>.</p>
<p><img src=""https://i.imgur.com/upg9bpz.png"" alt=""enter image description here"" /></p>
"
"75071316","Azure Data factory Unauthorized Issues While Connect Azure DataBricks Notebook Trigger PipeLine","<p>We are trying to execute notebook in ADF pipeline and it is throwing the error because of we are trying to connect azure key vault to fetch the credentials to authenticate our API's but if we run in ADB level, we are not get any issues. only happening while trigger in ADF pipeline level. also we enabled all the required permission like get and list but not able to resolve the issues, please help us.</p>
<blockquote>
<p>com.databricks.common.client.UnexpectedHttpError: HTTP request failed
with status: HTTP/1.1 401 Unauthorized</p>
</blockquote>
","<azure-data-factory><azure-databricks>","2023-01-10 14:20:56","151","0","1","75077429","<p>Seems like you may have enabled <code>Enable admin protection for “No isolation shared” clusters on your account</code> feature across the workspace. If yes, then that could be the reason why jobs are failing as it was hitting limitation with this admin protection. For more info about the limitation please refer to this document: <a href=""https://learn.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/no-isolation-shared#limitations"" rel=""nofollow noreferrer"">Enable the account-level admin protection setting - Limitations</a></p>
<p><a href=""https://i.stack.imgur.com/NBrA1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NBrA1.png"" alt=""enter image description here"" /></a></p>
<p>Then, to overcome this issue, Azure Databricks recommends that admins do one of the following:</p>
<ul>
<li>Use a different cluster type other than “No isolation shared” cluster access type or its equivalent legacy cluster types.</li>
<li>Create a non-admin user when using No Isolation Shared clusters.</li>
</ul>
"
"75067091","Use the reserved variable $$Filepath in dataset paths in Data Factory","<p>I am investigating whether it's possible to use <code>Copy activity</code> in bulk and dynamically decide the sink path based on the current file iterated upon?</p>
<p>I know that the <code>Foreach activity</code> makes this possible with the use of <code>@item().name</code>. However, when looping through say, 10 files, each loop kicks off a new <code>Copy activity</code>. This means x10 <code>Copy activities</code> which is billed a minimum of 1 minute each. Over the course of one month, this can drastically affect the cost depending on the amount of files and triggers.</p>
<p>This information if fetched from Microsoft docs. It says that the variable <code>$$Filepath</code> can be used to set an additional column value.</p>
<p><a href=""https://i.stack.imgur.com/o01ir.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o01ir.png"" alt=""enter image description here"" /></a></p>
<p>So if below sink path is possible to set, for my specific requirement, it would be possible to dynamically move files from one folder to another based on their filename.</p>
<p><a href=""https://i.stack.imgur.com/ELtmN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ELtmN.png"" alt=""enter image description here"" /></a></p>
<p>The files could look like blow in source:</p>
<ul>
<li>ABCXX.csv</li>
<li>ABCYY.csv</li>
<li>ABCZZ.csv</li>
</ul>
<p>And I would like to have below, dynamically without <code>Foreach</code>, in sink:</p>
<ul>
<li>ABC/XX/ABCXX.csv</li>
<li>ABC/YY/ABCYY.csv</li>
<li>ABC/ZZ/ABCZZ.csv</li>
</ul>
<p>I've tried to set the sink path to <code>$$Filepath</code> but it rendered as a string and not the dynamic value it would be evaluated to in an additional column.</p>
","<azure-data-factory>","2023-01-10 08:07:19","70","0","1","75080287","<blockquote>
<p>It says that the variable <code>$$Filepath</code> can be used to set an additional column value.</p>
</blockquote>
<p>AFAIK, <strong><code>$$FILEPATH</code> is the reserved variable to store the file path as an additional column</strong> and it is only accessible in the Additional column only everywhere else it will work as string.</p>
<blockquote>
<p>it would be possible to dynamically move files from one folder to another based on their filename.</p>
</blockquote>
<p>As you want to Iterate and compare the files based on names to fulfill that you cannot create and add dynamic expression with <code>$$FILEPATH</code> It will throw error.</p>
<p>To Iterate and compare the files based on names we don't have any other option than <strong>For-each and until loop</strong>. Both are working same.</p>
"
"75061995","Use date parameter in Azure Pipeline FetchXML query","<p>in my Azure Data Factory pipeline, I have a parameter, let's call it '<em>pDate</em>', and it has a default value of <em>2023-01-01</em>.</p>
<p>But every time I run the pipeline (copy data), I want to be able to change it.</p>
<p>I can do that with a variable as shown below (<em>date1</em> variable), but then I have to change the value of the variable.</p>
<p>How could I insert a parameter here instead?</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>&lt;fetch version=""1.0"" output-format=""xml-platform"" mapping=""logical"" distinct=""false""&gt; 
&lt;entity name=""my_attachment""&gt; 
&lt;all-attributes/&gt;  
&lt;filter type=""and""&gt; 
&lt;condition attribute=""createdon"" operator=""on-or-after"" value=""@{variables('date1')}"" /&gt; 
&lt;/filter&gt;  
&lt;/entity&gt; 
&lt;/fetch&gt;</code></pre>
</div>
</div>
</p>
","<azure><parameters><azure-data-factory><fetchxml>","2023-01-09 19:18:06","98","0","1","75066591","<ul>
<li>Parameters are constants across the particular pipeline. So, you can't change its value within the pipeline.</li>
<li>A variable with static value (say <code>2023-01-01</code>) is a parameter itself. Every time you execute the pipeline, the same value will be assigned to the variable making its functionality similar to parameter.</li>
<li>If there is a pattern for your date like created on or before 10 days (from today's date), then you can build a dynamic pipeline expression for this value. So, each time you run the pipeline, the value would be dynamic as required.</li>
<li>If there is no such pattern, then you have to make use of parameters. You can still change the value each time you run the pipeline.</li>
<li>When you try to trigger or debug the pipeline, the <code>pipeline run</code> tab will prompt you to enter the value for your parameter. You can simply give the required value and proceed accordingly.</li>
</ul>
<pre><code>&lt;fetch version=&quot;1.0&quot; output-format=&quot;xml-platform&quot; mapping=&quot;logical&quot; distinct=&quot;false&quot;&gt;
&lt;entity name=&quot;my_attachment&quot;&gt;
&lt;all-attributes/&gt;
&lt;filter type=&quot;and&quot;&gt;
&lt;condition attribute=&quot;createdon&quot; operator=&quot;on-or-after&quot; value=&quot;@{pipeline().parameters.req_date}&quot; /&gt;
&lt;/filter&gt;
&lt;/entity&gt;
&lt;/fetch&gt;
</code></pre>
<p><img src=""https://i.imgur.com/Fh7ekoK.png"" alt=""enter image description here"" /></p>
"
"75061917","Azure Data Factory ETL Process","<p>I would like to merge data from different data sources (ERP system, Excel files) with the ADF and make it available in an AzureSQLDB for further analyzing.</p>
<p>I'm not sure where and when I do the transformations and joins between the tables. Can I run all of this directly in the pipeline and then load the data into AzureDB, or do I need to stage the data first?</p>
<p>My understanding is to load the data into the ADF using Copy Activities and Datasets. Transforming and merging the datasets there with MappingDataFlows or similar activities. Then they are loaded into the AzureSQLDB</p>
","<azure><azure-sql-database><azure-data-factory><etl>","2023-01-09 19:10:45","95","0","1","75065457","<p>Your question is fully requirement based. You can go for either ETL or ELT process. Since your sink is AzureSQLDB , I would suggest to go with ELT , as you can handle lots of transformations in SQL itself by creating views on the landing tables.</p>
<p>If you have complex transformations to handle , then go with ETL and use Dataflow instead.</p>
<p>Also, regarding staging tables, if your requirement is to perform daily incremental load after the first full load, then you should opt for staging table.</p>
<p>Checkout <a href=""https://www.youtube.com/watch?v=FXw1gPaa2-M"" rel=""nofollow noreferrer"">this</a> video for full load and incremental load .</p>
"
"75060495","How to dump SQL query result from LookUp activity into a dict or array variable?","<p><a href=""https://i.stack.imgur.com/p0pui.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p0pui.png"" alt=""enter image description here"" /></a>
like this process or others, read SQL query result and set into a collection variable.</p>
<p>Add dynamic content but failed</p>
","<azure-data-factory>","2023-01-09 16:50:50","103","0","1","75066436","<p>This is my sample SQL table:</p>
<p><img src=""https://i.imgur.com/bbwEGMe.png"" alt=""enter image description here"" /></p>
<p>Use lookup activity and give your SQL query in query option like below sample.</p>
<p><img src=""https://i.imgur.com/K6MLTUY.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>read SQL query result and set into a collection variable.</p>
</blockquote>
<p>Lookup will give the query result as array of objects. Give this array to array variable in set variable activity.</p>
<pre><code>@activity('Lookup1').output.value
</code></pre>
<p><img src=""https://i.imgur.com/XJQcjnT.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/wJq8ZSm.png"" alt=""enter image description here"" /></p>
"
"75059074","Azure Synapse Copy Activity error: CREATE TABLE dbo is not supported","<p>I am having trouble using the Table Option named <strong>Auto Create Table</strong> on a Copy Data activity.<br />
The source I use is a REST API which can be requested correctly, and actually data can be previewed normally.<br />
However, <strong>even though I count with every permission available (including blob storage contributor), the following message is received after Debug given the settings of Sink section of Copy Data:</strong></p>
<p><em>{<br />
&quot;errorCode&quot;: &quot;2200&quot;,<br />
&quot;message&quot;: &quot;ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'CREATE TABLE dbo is not supported.',Source=,''Type=System.Data.SqlClient.SqlException,Message=CREATE TABLE dbo is not supported.,Source=.Net SqlClient Data Provider,SqlErrorNumber=15868,Class=16,ErrorCode=-2146232060,State=5,Errors=[{Class=16,Number=15868,State=5,Message=CREATE TABLE dbo is not supported.,},],'&quot;,<br />
&quot;failureType&quot;: &quot;UserError&quot;,<br />
&quot;target&quot;: &quot;From API to Data Lake&quot;,<br />
&quot;details&quot;: []<br />
}</em></p>
<p><strong>Below I attach screenshots of the Copy Data settings. I use Azure SQL Database as our Sink dataset, and the same for Linked Service. Even though we create a table beforehand, the process does not work. Given that I wish to use the Serverless SQL pool, are there any suggestions or alternatives for this pipeline to work properly?</strong></p>
<p><a href=""https://i.stack.imgur.com/B5c5P.png"" rel=""nofollow noreferrer"">Azure Synapse copy data config1</a></p>
<p><a href=""https://i.stack.imgur.com/PX14U.png"" rel=""nofollow noreferrer"">Azure Synapse copy data config2</a></p>
<p>I am trying to make a GET Request to the API I am working with, and to send the data from the generated parquet file to a table using the Copy Data activity, using Azure SQL Database as Sink dataset.</p>
","<azure-data-factory><azure-synapse>","2023-01-09 14:56:23","325","0","2","75065951","<p>Please check if 'dbo' schema exist or not in the database. If it doesn't exist you can create it by using the following query in precopy script of sink settings.</p>
<p><code>IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo') BEGIN EXEC sp_executesql N'CREATE SCHEMA dbo'; END</code></p>
"
"75059074","Azure Synapse Copy Activity error: CREATE TABLE dbo is not supported","<p>I am having trouble using the Table Option named <strong>Auto Create Table</strong> on a Copy Data activity.<br />
The source I use is a REST API which can be requested correctly, and actually data can be previewed normally.<br />
However, <strong>even though I count with every permission available (including blob storage contributor), the following message is received after Debug given the settings of Sink section of Copy Data:</strong></p>
<p><em>{<br />
&quot;errorCode&quot;: &quot;2200&quot;,<br />
&quot;message&quot;: &quot;ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'CREATE TABLE dbo is not supported.',Source=,''Type=System.Data.SqlClient.SqlException,Message=CREATE TABLE dbo is not supported.,Source=.Net SqlClient Data Provider,SqlErrorNumber=15868,Class=16,ErrorCode=-2146232060,State=5,Errors=[{Class=16,Number=15868,State=5,Message=CREATE TABLE dbo is not supported.,},],'&quot;,<br />
&quot;failureType&quot;: &quot;UserError&quot;,<br />
&quot;target&quot;: &quot;From API to Data Lake&quot;,<br />
&quot;details&quot;: []<br />
}</em></p>
<p><strong>Below I attach screenshots of the Copy Data settings. I use Azure SQL Database as our Sink dataset, and the same for Linked Service. Even though we create a table beforehand, the process does not work. Given that I wish to use the Serverless SQL pool, are there any suggestions or alternatives for this pipeline to work properly?</strong></p>
<p><a href=""https://i.stack.imgur.com/B5c5P.png"" rel=""nofollow noreferrer"">Azure Synapse copy data config1</a></p>
<p><a href=""https://i.stack.imgur.com/PX14U.png"" rel=""nofollow noreferrer"">Azure Synapse copy data config2</a></p>
<p>I am trying to make a GET Request to the API I am working with, and to send the data from the generated parquet file to a table using the Copy Data activity, using Azure SQL Database as Sink dataset.</p>
","<azure-data-factory><azure-synapse>","2023-01-09 14:56:23","325","0","2","75066551","<p><strong>I tried to reproduce your scenario and I got similar error.</strong></p>
<p><img src=""https://i.imgur.com/IOBqg7u.png"" alt=""enter image description here"" /></p>
<p>The Cause as per <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview#t-sql-support"" rel=""nofollow noreferrer"">Microsoft Document</a></p>
<blockquote>
<p>The Serverless SQL pool has no local storage, only metadata objects are stored in databases. Therefore, T-SQL related to the following concepts isn't supported:</p>
<blockquote>
<ul>
<li>Tables</li>
<li>Triggers</li>
<li>Materialized views</li>
<li>DDL statements other than ones related to views and security</li>
<li>DML statements</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Given that I wish to use the Serverless SQL pool, are there any suggestions or alternatives for this pipeline to work properly?</strong></p>
</blockquote>
<p>As we can not create table in Serverless SQL, but we can create external table in serverless SQL.
The workaround is to store the data from API in the <code>ADLS</code>  of the synapse workspace as csv file and then create external table on it as below:</p>
<ul>
<li><p>Storing the API data in the <code>ADLS</code>  of the synapse workspace as csv file
<img src=""https://i.imgur.com/PAW1mzn.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then Go to synapse workspace And <strong>Data &gt;&gt; Linked &gt;&gt; select your ADLS account and container &gt;&gt; click on file &gt;&gt; New SQL script &gt;&gt; Create External Table.</strong>
<img src=""https://i.imgur.com/IwywSOF.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Check all settings and click on continue.
<img src=""https://i.imgur.com/xraUYwi.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Give the table name you want and click on Open script it will generate the script.
<img src=""https://i.imgur.com/8xkfnX4.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Run the script</p>
</li>
</ul>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/BtmVpml.png"" alt=""enter image description here"" /></p>
"
"75058012","Wildcard in blob ends with path - ADF trigger blob storage event","<p>I have a blob structure like this:</p>
<pre><code>&gt; data/
&gt;     folder1/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
&gt;     dir2/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
&gt;     source3/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
</code></pre>
<p>I want to trigger the blob storage event when any csv file is uploaded to source3/dirC only.
<a href=""https://i.stack.imgur.com/VxhDl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VxhDl.png"" alt=""enter image description here"" /></a></p>
<p>The problem is adf doesnt support wildcard path here. I want something like this:</p>
<p><strong>Blob_path_ends_with:</strong> <code>any_dir(exclude folder1 include dir2,source3)/dirC/*.csv (any csv file in dirC in any main directory)</code></p>
<p><strong>So I want to ignore any csv uploads in the folder1 but trigger event on upload of files in dir2 and source3.</strong></p>
","<azure-data-factory>","2023-01-09 13:30:23","505","1","3","75061486","<p>As mentioned by Rakesh Govindula, path <strong>begins with</strong> and <strong>ends with</strong> are the only pattern matching <strong>allowed in Storage Event Trigger</strong>. Other types of wildcard matching aren't supported for the trigger type.</p>
<p>However you can workaround this with a logic app as follows:
<a href=""https://i.stack.imgur.com/yp2mG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yp2mG.png"" alt=""Architecture"" /></a></p>
<p>Steps to reproduce:</p>
<ol>
<li><p>Create an Event Grid System Topic (<a href=""https://learn.microsoft.com/en-us/azure/event-grid/create-view-manage-system-topics#create-a-system-topic"" rel=""nofollow noreferrer"">Documentaion</a>)<br />
Select <em>Storage Accounts (Blob &amp; GPv2)</em> for topic type. You can choose any name for the topic.</p>
</li>
<li><p>Create a Logic app with http trigger type<br />
Paste the following <a href=""https://learn.microsoft.com/en-us/azure/event-grid/event-schema-blob-storage?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&amp;tabs=event-grid-event-schema#microsoftstorageblobcreated-event"" rel=""nofollow noreferrer"">Event Grid Event Schema</a> to the Logic App's http trigger's <em>Request Body JSON Schema</em> field:</p>
<pre><code>[{
  &quot;topic&quot;: &quot;/subscriptions/{subscription-id}/resourceGroups/Storage/providers/Microsoft.Storage/storageAccounts/my-storage-account&quot;,
  &quot;subject&quot;: &quot;/blobServices/default/containers/test-container/blobs/new-file.txt&quot;,
  &quot;eventType&quot;: &quot;Microsoft.Storage.BlobCreated&quot;,
  &quot;eventTime&quot;: &quot;2017-06-26T18:41:00.9584103Z&quot;,
  &quot;id&quot;: &quot;831e1650-001e-001b-66ab-eeb76e069631&quot;,
  &quot;data&quot;: {
    &quot;api&quot;: &quot;PutBlockList&quot;,
    &quot;clientRequestId&quot;: &quot;6d79dbfb-0e37-4fc4-981f-442c9ca65760&quot;,
    &quot;requestId&quot;: &quot;831e1650-001e-001b-66ab-eeb76e000000&quot;,
    &quot;eTag&quot;: &quot;\&quot;0x8D4BCC2E4835CD0\&quot;&quot;,
    &quot;contentType&quot;: &quot;text/plain&quot;,
    &quot;contentLength&quot;: 524288,
    &quot;blobType&quot;: &quot;BlockBlob&quot;,
    &quot;url&quot;: &quot;https://my-storage-account.blob.core.windows.net/testcontainer/new-file.txt&quot;,
    &quot;sequencer&quot;: &quot;00000000000004420000000000028963&quot;,
    &quot;storageDiagnostics&quot;: {
      &quot;batchId&quot;: &quot;b68529f3-68cd-4744-baa4-3c0498ec19f0&quot;
    }
  },
  &quot;dataVersion&quot;: &quot;&quot;,
  &quot;metadataVersion&quot;: &quot;1&quot;
}]
</code></pre>
</li>
</ol>
<p>Save the logic app in designer and copy the content of the <em>HTTP POST URL</em> field. You will use it in the next.</p>
<ol start=""3"">
<li><p>Create an Event Subscription for the storage account (<a href=""https://learn.microsoft.com/en-us/azure/data-explorer/ingest-data-event-grid-manual#create-an-event-grid-subscription"" rel=""nofollow noreferrer"">Documentation</a>)<br />
Type in the System Topic Name field the event grid system topic you created in step 1. Select Webhook endpoint type and paste in the HTTP POST URL you coped in step 2.
Once you are ready, you can test if the logic app is triggered if you upload a file to the storage. Go to the logic app and check the logs (raw outputs of the http trigger)</p>
</li>
<li><p>Add a condition step to the Logic app to stop the workflow if the http request's <code>body().data.url</code> parameter doesn't contains the path you need.</p>
</li>
<li><p>Add a Data Factory pipeline run step to the Logic App. (<a href=""https://gbamezai.medium.com/how-to-trigger-azure-data-factory-from-logic-apps-c3132aa24649"" rel=""nofollow noreferrer"">Useful blogpost</a>)<br />
You can pass the path string as pipeline parameter from the http body: <code>body().data.url</code></p>
</li>
</ol>
<p>I hope you can follow the steps I described without screenshots.</p>
"
"75058012","Wildcard in blob ends with path - ADF trigger blob storage event","<p>I have a blob structure like this:</p>
<pre><code>&gt; data/
&gt;     folder1/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
&gt;     dir2/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
&gt;     source3/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
</code></pre>
<p>I want to trigger the blob storage event when any csv file is uploaded to source3/dirC only.
<a href=""https://i.stack.imgur.com/VxhDl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VxhDl.png"" alt=""enter image description here"" /></a></p>
<p>The problem is adf doesnt support wildcard path here. I want something like this:</p>
<p><strong>Blob_path_ends_with:</strong> <code>any_dir(exclude folder1 include dir2,source3)/dirC/*.csv (any csv file in dirC in any main directory)</code></p>
<p><strong>So I want to ignore any csv uploads in the folder1 but trigger event on upload of files in dir2 and source3.</strong></p>
","<azure-data-factory>","2023-01-09 13:30:23","505","1","3","75069989","<p>You can use the below workaround using ADF.</p>
<p>This my folder structure.</p>
<pre><code>input
    source1
        dirC
    source2
        dirC
    source3
        dirC
            ##file uploading 
</code></pre>
<p>Create two parameters for uploading file name and folder path.</p>
<p>and another parameter of array type and give your folder names that you want the file to be uploaded.</p>
<p><img src=""https://i.imgur.com/bMvnuq7.png"" alt=""enter image description here"" /></p>
<p>I have created the trigger like below for the container <code>input</code></p>
<p><img src=""https://i.imgur.com/Pi76Ouq.png"" alt=""enter image description here"" /></p>
<p>give the <code>@triggerBody().fileName</code> and <code>@triggerBody().folderPath</code> while creating trigger.</p>
<p><img src=""https://i.imgur.com/MCLk2ry.png"" alt=""enter image description here"" /></p>
<p>We will get the trigger file folder path value like <code>input/source3/dirC</code>. So, take this and check whether our folder names array values are exists or not in this string in if activity.</p>
<pre><code>@contains(pipeline().parameters.folder_list, split(pipeline().parameters.folderpath, '/')[1])
</code></pre>
<p><img src=""https://i.imgur.com/rzG5nb0.png"" alt=""enter image description here"" /></p>
<p>Then inside True of if you can give your activities or if there are more activities you can use Execute pipeline activity.</p>
<p><strong>Sample copy activity:</strong></p>
<p><img src=""https://i.imgur.com/LAU8yvb.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/X9x8Oh6.png"" alt=""enter image description here"" /></p>
"
"75058012","Wildcard in blob ends with path - ADF trigger blob storage event","<p>I have a blob structure like this:</p>
<pre><code>&gt; data/
&gt;     folder1/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
&gt;     dir2/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
&gt;     source3/
&gt;            dirA/
&gt;            dirB/
&gt;            dirC/
&gt;                 file1.csv
&gt;                 file2.csv
&gt;                 file3.csv
</code></pre>
<p>I want to trigger the blob storage event when any csv file is uploaded to source3/dirC only.
<a href=""https://i.stack.imgur.com/VxhDl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VxhDl.png"" alt=""enter image description here"" /></a></p>
<p>The problem is adf doesnt support wildcard path here. I want something like this:</p>
<p><strong>Blob_path_ends_with:</strong> <code>any_dir(exclude folder1 include dir2,source3)/dirC/*.csv (any csv file in dirC in any main directory)</code></p>
<p><strong>So I want to ignore any csv uploads in the folder1 but trigger event on upload of files in dir2 and source3.</strong></p>
","<azure-data-factory>","2023-01-09 13:30:23","505","1","3","75188525","<p>I ended up changing the folder structure to</p>
<blockquote>
<p>data
source1
dirA
dirB
source2
dirA
dirB
source3
dirA
dirB
So all the files that I want to trigger the pipeline are always in <strong>source3/dirB</strong>
and using BlobPathStartsWith as <strong>data/source3/dirB</strong> and BlobPathEndsWith <strong>.csv</strong></p>
</blockquote>
"
"75056607","Data factory Copy Task - Using Stored Procedure containing temporary tables","<p>In Data Factory we're using Copy Data task to move some data. The source is a SQL Stored Procedure and within the stored procedure some temp tables are being created. The Sink is an auto created physical SQL table (not a temp table).</p>
<p>When running the pipeline we get an error complaining about invalid object which is the temp table.</p>
<pre><code> Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Invalid object name `'#w_activity'.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Invalid object name '#w_activity'.,Source=.Net SqlClient Data Provider,SqlErrorNumber=208,Class=16,ErrorCode=-2146232060,State=0,Errors=[{Class=16,Number=208,State=0,Message=Invalid object name '#w_activity'.,},],'`
</code></pre>
<p>The same sproc executes perfectly via SSMS. Having done some digging it looks like one solution is to re write the stored procedure to use table variables instead of temp tables.</p>
<p>Can anyone explain if there is a way that temp tables can be retained so that we don't have to re write all our stored procs, or if not can anyone explain why they can't/won't work.  I understand about different sessions between the ADF activities, however these are being created/used within the same stored proc within a single activity.</p>
<p>Many Thanks</p>
","<sql-server><azure-data-factory>","2023-01-09 11:26:21","146","0","2","75058131","<p>I tried to reproduce your scenario and got the similar error.</p>
<p><img src=""https://i.imgur.com/r2zIxbz.png"" alt=""enter image description here"" /></p>
<p>Using stored procedures, global temporary tables (##T), a workaround for doing this is to write stored procedure for getting data from temp table and execute it will give you a result.</p>
<ul>
<li>Created stored procedure to fetch data from temp table.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code> create proc testtemp as
 begin
 select * from ##test
 end
</code></pre>
<p><img src=""https://i.imgur.com/96efsrx.png"" alt=""enter image description here"" /></p>
<ul>
<li>Using that stored procedures in copy activity.<img src=""https://i.imgur.com/IDEM3VG.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Successful execution</strong>
<img src=""https://i.imgur.com/M8GQCZ4.png"" alt=""https://i.imgur.com/M8GQCZ4.png"" /></p>
"
"75056607","Data factory Copy Task - Using Stored Procedure containing temporary tables","<p>In Data Factory we're using Copy Data task to move some data. The source is a SQL Stored Procedure and within the stored procedure some temp tables are being created. The Sink is an auto created physical SQL table (not a temp table).</p>
<p>When running the pipeline we get an error complaining about invalid object which is the temp table.</p>
<pre><code> Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Invalid object name `'#w_activity'.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Invalid object name '#w_activity'.,Source=.Net SqlClient Data Provider,SqlErrorNumber=208,Class=16,ErrorCode=-2146232060,State=0,Errors=[{Class=16,Number=208,State=0,Message=Invalid object name '#w_activity'.,},],'`
</code></pre>
<p>The same sproc executes perfectly via SSMS. Having done some digging it looks like one solution is to re write the stored procedure to use table variables instead of temp tables.</p>
<p>Can anyone explain if there is a way that temp tables can be retained so that we don't have to re write all our stored procs, or if not can anyone explain why they can't/won't work.  I understand about different sessions between the ADF activities, however these are being created/used within the same stored proc within a single activity.</p>
<p>Many Thanks</p>
","<sql-server><azure-data-factory>","2023-01-09 11:26:21","146","0","2","75897763","<p>I've just had the same issue, I had copied an existing copy activity, I found when I cleared the mappings it worked for me, without any changes to the stored proc</p>
"
"75056078","Azure data factory with a copy activity using a binary dataset fails to copy folder contents if parameterized","<p>In my Azure data factory I need to copy data from an SFTP source that has structured the data into date based directories with the following hierarchy
year -&gt; month -&gt; date -&gt; file</p>
<p>I have created a linked service and a binary dataset where the dataset &quot;filesystem&quot; points to the host and &quot;Directory&quot; points to the folder that contains the year directories. Ex: host/exampledir/yeardir/</p>
<p>with yeardir containing the year directories.</p>
<p>When I manually write into the dataset that I want the folder &quot;2015&quot; it will copy the entirety of the 2015 folder, however if I put a parameter for the directory and then input the same folder path from a copy activity it creates a file called &quot;2015&quot; inside of my blob storage that contains no data.</p>
<p>My current workaround is to make a nested sequence of get metadata for loops that drill into each folder and subfolder and copy the individual file ends. However the desired result is to instead have the single binary dataset copy each folder without the need for get metadata.</p>
<p>Is this possible within the scope of the data factory?</p>
<p>edit:</p>
<p><a href=""https://i.stack.imgur.com/8R7Vf.png"" rel=""nofollow noreferrer"">manual filepath that works</a></p>
<p><a href=""https://i.stack.imgur.com/ZSSDM.png"" rel=""nofollow noreferrer"">parameterized filepath</a></p>
<p><a href=""https://i.stack.imgur.com/rsvUz.png"" rel=""nofollow noreferrer"">properties used in copy activity</a></p>
<p>To add further context I have tried manually writing the filepath into the copy activity as shown in the photo, I have also attempted to use variables, dynamic content for the parameter (using base filepath and concat) and also putting the base filepath into the dataset alongside @dataset().filePath. None of these solutions have worked for me so far and either copy nothing or create the empty file I mentioned earlier.</p>
<p>The sink is a binary dataset linked to Azure Data Lake Storage Gen2.</p>
<p><a href=""https://i.stack.imgur.com/Bmmkm.png"" rel=""nofollow noreferrer"">sink filepath</a></p>
<p>Update:</p>
<p>The accepted answer is the solution. My problem was that the source dataset when retrieved would have a newline at the end when passed as a parameter. I used concat to clean this up and this has worked since then.</p>
","<azure><azure-data-factory><linked-service>","2023-01-09 10:41:19","130","0","1","75057541","<p>Since giving <code>exampledir/yeardir/2015</code> worked perfectly for you and you want to copy all the folders present in <code>exampledir/yeardir</code>, you can follow the below procedure:</p>
<ul>
<li>I have taken a <code>get metadata</code> activity to get the child items of the folder <code>exampledir/yeardir/</code> (In my demonstration, I have taken path as 'maindir/yeardir'.).</li>
</ul>
<p><img src=""https://i.imgur.com/1ylpBFt.png"" alt=""enter image description here"" /></p>
<ul>
<li>This will give you all the year folders present. I have taken only 2020 and 2021 as an example.</li>
</ul>
<p><img src=""https://i.imgur.com/1wy7yMT.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, with only one for each activity with items value as the child items output of get metadata activity, I have directly used copy activity.</li>
</ul>
<pre><code>@activity('Get Metadata1').output.childItems
</code></pre>
<p><img src=""https://i.imgur.com/wuAPUKj.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, inside for each I have my copy data activity. For both source and sink, I have created a dataset parameter for paths. I have given the following dynamic content for source path.</li>
</ul>
<pre><code>maindir/yeardir/@{item().name}
</code></pre>
<p><img src=""https://i.imgur.com/wEHGAqX.png"" alt=""enter image description here"" /></p>
<ul>
<li>For sink, I have given the output directory as follows:</li>
</ul>
<pre><code>outputDir/@{item().name}
</code></pre>
<p><img src=""https://i.imgur.com/ISMU01M.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Since giving path manually as <code>exampledir/yeardir/2015</code> worked, we have got the list of year folders using get metadata activity. We looped through each of this and copy each folder with source path as <code>exampledir/yeardir/&lt;current_iteration_year_folder&gt;</code>.</p>
</li>
<li><p>Based on how I have given my sink path, the data will be copied with contents. The following is a reference image.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/lysi2Mq.png"" alt=""enter image description here"" /></p>
"
"75054673","How to build OR-behavior into Azure Synapse pipelines?","<p>I have a pipeline in Synapse that reloads data from a number of REST-endpoints. It looks as follows: <a href=""https://i.stack.imgur.com/PXUFH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PXUFH.png"" alt=""enter image description here"" /></a></p>
<p>What I want to achieve is that the 'CheckAccessTokenValidity' activity checks whether my API-token is still valid. If yes, all is fine, proceed to ForEachDivision activity. If no, refresh tokens, Proceed to ForEachDivision activity.</p>
<p>I thought I had implemented this logic in the screenshot provided. However, this is not the case. My pipeline is now deadlocked because the ForEachDivision apparently expects <em>all three</em> preceding activities to be succesful before running (Which is impossible by design).</p>
<p>How do I implement the logic described above in Azure Synapse?</p>
","<azure-data-factory><azure-synapse>","2023-01-09 08:22:05","66","0","1","75054996","<p>You can modify your pipeline and accommodate following changes:</p>
<ol>
<li>Create a variable 'var1' , by default set it to 'False'</li>
<li>After 'CheckAccessTokenValidity' , add set variable activity 'set variable1' pointing to 'var1' and set its value as 'True'</li>
<li>After SetRefreshToken , add another set variable activity 'set variable2' pointing to same 'var1' and set its value as 'True' , so that either of the step 2 or 3 can turn the value of variable to 'True'.</li>
<li>Remove all activities starting ForEach block from this pipeline and cut paste those activities and keep in another new pipeline 'pipeline2'</li>
<li>After 'set variable2' activity, attach an If activity with 'success' and 'Skipped' conditional path . If step 2 gets successful, 'set variable2' would be skipped and it will call the next activity that is 'Step6' . If step 2 fails, and step 3 is called, it will call 'set variable2' and after its success, Step6 will be completed.</li>
<li>In this if activity, write the condition to check the value of var1 . If var1=='True' , then execute the 'pipeline2'</li>
</ol>
"
"75041423","reduce function not working in derived column in adf mapping data flow","<p>I am trying to create the derived column based on the condition that met the value and trying to do the summation of multiple matching column values dynamically. So I am using reduce function in ADF derived column mapping data flow. But the column is not getting created even the transformation is correct.</p>
<p>Columns from source</p>
<p><a href=""https://i.stack.imgur.com/rVjri.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rVjri.png"" alt=""enter image description here"" /></a></p>
<p>Derived column logic</p>
<p><a href=""https://i.stack.imgur.com/JFWGt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JFWGt.png"" alt=""enter image description here"" /></a></p>
<p>Derived column data preview without the new columns as per logic</p>
<p><a href=""https://i.stack.imgur.com/OU58H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OU58H.png"" alt=""enter image description here"" /></a></p>
<p>I could see only the fields from source but not the derived column fields. If I use only the array($$) I could see the fields getting created.</p>
<p>Derived column data preview with logic only array($$)</p>
<p><a href=""https://i.stack.imgur.com/0F6Sb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0F6Sb.png"" alt=""enter image description here"" /></a></p>
<p>How to get the derived column with the summation of all the fields matching the condition?</p>
<p>We are getting data of 48 weeks forecast and the data to be prepared on monthly basis.</p>
<p>eg: Input data</p>
<p><a href=""https://i.stack.imgur.com/Osj2Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Osj2Q.png"" alt=""enter image description here"" /></a></p>
<p>Output data:</p>
<pre><code>JAN
----
506  -- This is for first record i.e. (94 + 105 + 109 + 103 + 95)
</code></pre>
","<azure-data-factory>","2023-01-07 15:13:47","100","0","1","75043406","<p>The problem is that the <code>array($$)</code> in the reduce function has only one element, so that the reduce function can not accumulate the content of the matching columns correctly.</p>
<p>You can solve this by using two derived columns and a data flow parameter as follows:</p>
<ol>
<li><p><strong>Create derived columns with pattern matching for each month-week</strong> you did it before, but put the reference $$ into the value field, instead of the reduce(...) function. <br><br />
This will create derived columns like jan0, jan1, etc. containing the copy of the original values. For example <strong>Week 0 (1 Jan - 7 Jan) =&gt; 0jan</strong> with value 95.
This step gives you a predefined set of column names for each week, which you can use to summarize the values with specific column names.
<br></p>
</li>
<li><p><strong>Define Data Flow parameters for each month</strong> containing the month-week column names in a string array, like this:<br><br />
<code>ColNamesJan=['0jan' ,'1jan', etc.] ColNamesFeb=['0feb' ,'1feb', etc.] and so on.</code><br><br />
You will use these column names in a reduce function to summarize the month-week columns to monthly column in the next step.</p>
</li>
<li><p><strong>Create a derived column for each month</strong>, which will contain the monthly totals, and use the following reduce function to sum the weekly values: <br><br />
<code>reduce(array(byNames($ColNamesJan)), 0, #acc + toInteger(toString(#item)),#result)</code><br />
<em>Replace the parameter name accordingly.</em></p>
</li>
</ol>
<p>I was able to summarize the columns dynamically with the above solution.<br />
Please let me know if you need more information (e.g. screenshots) to reproduce the solution.</p>
<p><strong>Update</strong> -- Here are the screenshots from my test environment.<br><br />
Data source (data preview):
<a href=""https://i.stack.imgur.com/B9tPI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B9tPI.png"" alt=""Data source"" /></a><br><br />
Derived columns with pattern matching (settings)<br />
<a href=""https://i.stack.imgur.com/oDSwN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oDSwN.png"" alt=""derived columns with pattern matching settings"" /></a><br><br />
Derived columns with pattern matching (data preview)<br />
<a href=""https://i.stack.imgur.com/KA1U8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KA1U8.png"" alt=""derived columns with pattern matching preview"" /></a><br />
Data flow parameter:<br />
<a href=""https://i.stack.imgur.com/mhVyM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mhVyM.png"" alt=""data flow parameter"" /></a><br><br />
Derived column for monthly sum (settings):<br />
<a href=""https://i.stack.imgur.com/OawNC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OawNC.png"" alt=""derived column for monthly sum settings"" /></a><br><br />
Derived column for monthly sum (data preview):<br />
<a href=""https://i.stack.imgur.com/SPFtA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SPFtA.png"" alt=""derived column for monthly sum data preview"" /></a></p>
"
"75038150","Identify partial text in the column and create new column in Azure Data Flow?","<p>I have a below text column called &quot;Type&quot; in my data</p>
<p><a href=""https://i.stack.imgur.com/zweEa.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I wanted to find if column has Annual or Monthly and create new column called &quot;Season&quot;. Season column should have Annual, Monthly or None</p>
<p>Expected Output</p>
<p><a href=""https://i.stack.imgur.com/PhM5k.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am using derived column but not sure how to extract the partial text in the column. Can anyone advise how to do this please?</p>
","<azure><azure-data-factory>","2023-01-07 04:37:46","71","1","1","75038750","<p>You can use the following data flow expression in the derived column:</p>
<pre><code>iif( contains(split(Type, ' '), #item == 'Annual'), 'Annual', 
  iif( contains(split(Type, ' '), #item == 'Monthly'), 'Monthly',
  'None'))
</code></pre>
<p>Explanation:</p>
<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#split"" rel=""nofollow noreferrer"">split()</a> functions convert each row in <em>Type</em> column into array. (i.e. the &quot;Founding Annual Member&quot; will be converted to [&quot;Founding Annual Member&quot;] )</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#contains"" rel=""nofollow noreferrer"">contains()</a> functions return <em>true</em> if the column contains the word <em>&quot;Annual&quot;</em> (first contains function), or <em>&quot;Monthly&quot;</em> (second contains function)</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#iif"" rel=""nofollow noreferrer"">iif()</a> functions evaluate the result of the contains() functions by returning the word <em>&quot;Annual&quot;</em> (iif in the first row), or <em>&quot;Monthly&quot;</em> (iif in the second row). If no matches found then the second iif() function returns the word <em>&quot;None&quot;</em>. (last row)</li>
</ul>
<p><a href=""https://i.stack.imgur.com/rZXiT.png"" rel=""nofollow noreferrer"">Result in Data Factory Studio</a></p>
"
"75036756","How to Load files with the same name in data flow Azure data factory","<p>I use data flow in Azure data factory And I set as source dataset files with the same name. The files have named “name_date1.csv” end “name_date2.csv”. I set path “name_*.csv”. I want that data flow load in sink db only data of  “name_date1”. How is it possible?</p>
","<azure-data-factory>","2023-01-06 22:41:24","79","0","1","75054320","<p>I have reproduced the above and able to get the desired file to sink using <code>Column to store file name</code> option in source options.</p>
<p>These are my source files in storage.</p>
<p><img src=""https://i.imgur.com/rzha8b2.png"" alt=""enter image description here"" /></p>
<p>I have given name_*.csv in wild card of source as same as you to read multiple files.</p>
<p><img src=""https://i.imgur.com/1K8HAr7.png"" alt=""enter image description here"" /></p>
<p>In source options, go to <code>Column to store file name</code> and give a name and this will store the file name of every row in new column.</p>
<p><img src=""https://i.imgur.com/LIwEoai.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/ajKE3WH.png"" alt=""enter image description here"" /></p>
<p>Then use <strong>filter transformation</strong> to get the row only from a particular file.</p>
<p><code>notEquals(instr(filename,'name_date1'),0)</code></p>
<p><img src=""https://i.imgur.com/3b57tMk.png"" alt=""enter image description here"" /></p>
<p>After this give your sink and you can get the rows from your desired file only.</p>
"
"75031644","Load files with the same name in data flow Azure data factory","<p>I use data flow in Azure data factory And I set as source dataset files with the same name. The files have named “name_date1” end “name_date2”. I want that data flow  load in sink db only data of file  “name_date1”.
How is it possible?</p>
","<azure-data-factory>","2023-01-06 13:36:32","127","0","1","75044787","<p>If you have multiple files being read in your ADF data flow source transformation, you can use the property called &quot;Column to store file name&quot; under Source Options. Provide a name to store the filename and it will become part of your metadata. You can then use a Filter transformation to include only source files with a specific filename by matching on that column that has the file name.</p>
"
"75028376","ADF Scheduling when existing Job not yet finished","<p>Having read <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-scheduling-and-execution"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-scheduling-and-execution</a>, it is unclear to me if:</p>
<ul>
<li>A schedule is made every hr for a job to run,
<ul>
<li>can we stop the concurrent execution of the next job at hr+1 if the job for hr+0 is still running?
<ul>
<li>It looks if concurrency = 1 means this,
<ul>
<li>But is that invocation simply not start until concurrent execution is finished?</li>
<li>Or will it be discarded?</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
","<azure-data-factory><job-scheduling>","2023-01-06 08:01:42","60","0","1","75036374","<p>When we set the concurrency 1, only one instance will be allowed to run at a time. When the scheduled trigger runs again and tries to run the pipeline, If the pipeline is already running, the next invocation will be queued. It will start after finishing the current instance.</p>
<p>For your question, the following invocation will be queued. After the first run finishes, the next run will start.</p>
"
"75026466","Data Factory: how to stop pipeline execution without logic and without failure?","<p>I have a simple pipeline consisting of:</p>
<ul>
<li>A notebook, when completed successfully followed by;</li>
<li>A ForEach loop.</li>
</ul>
<p>When executed successfully, the notebook either outputs:</p>
<ol>
<li>An array of values for the ForEach loop (no issue here).</li>
<li>A message that some conditions are not met (troublesome part).</li>
</ol>
<p>In situation two a string is passed to the ForEach loop, which causes the activity to fail. This is not the desired result, because the pipeline should run successfully.</p>
<p>I have tried to solve the issue with the Switch activity, but you can not put a ForEach activity in a Switch. The same issue arises when I try to use the If Condition activity.</p>
<p>Any solutions and workarounds are welcome.</p>
","<azure><azure-data-factory>","2023-01-06 02:48:56","105","0","1","75027056","<p>As per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#limitations-and-workarounds"" rel=""nofollow noreferrer"">Microsoft Document</a>, For-each activity <strong>cannot</strong>  be nested inside <strong>if activity or switch activity</strong> in Azure Data Factory (ADF). Instead, use the  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">Execute Pipeline</a>  activity to create nested pipelines, where the parent has If activity and the child pipeline has for-each activity.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Limitation</th>
<th>Workaround</th>
</tr>
</thead>
<tbody>
<tr>
<td>You can't nest a ForEach loop inside another ForEach loop (or an Until loop).</td>
<td>Design a two-level pipeline where the outer pipeline with the outer ForEach loop iterates over an inner pipeline with the nested loop.</td>
</tr>
</tbody>
</table>
</div>
<p>Refer the NiharikaMoola-MT's answer on this <a href=""https://stackoverflow.com/questions/72684969/nested-foreach-in-adf-azure-data-factory"">SO thread</a>.
In the parent pipeline, keep <strong>Notebook activity</strong> and <strong>If activity</strong>. Then invoke the child pipeline with <strong>for-each activity</strong> inside the true section of If activity.</p>
"
"75024656","How to send an email with query output result as attachment from Data Factory?","<p>I have a data  into the SSMS table that I need to send it as an email attachment using data factory. any suggestion to set up a pipeline such way so I can query the data from the table and attach that output and send it in the email would be appreciate.</p>
<ul>
<li>I tried using logic APP to send an email , I am able to send an email but I was not able to find out the way I can attach the query output data as an attachment in that email , 1st have a lookup activity to get the data from the table then I have used web activity to set the trigger to send an email. inside the web activity I have used Logic app URL , post as method ,
BODY : { &quot;message&quot; : &quot;@{activity(Lookup1').output}&quot;, &quot;DatafactoryName&quot; : &quot;@{pipeline().DataFactory}&quot;,      &quot;pipelineName&quot; : &quot;@{pipeline().PipelineName}&quot;,&quot;receiver&quot; : &quot;@{variables('receiver')}&quot;</li>
<li>I have declare variable called receiver to store email ids.</li>
</ul>
","<azure-data-factory>","2023-01-05 21:41:39","210","0","1","75027709","<p>To send an Output of lookup activity as an attachment follow the below procedure:</p>
<p>In logic app For the <strong>Send Email (V2)</strong> action, customize how you wish to format the email, using the properties and to add attachment <strong>select <code>Attachment</code> from <code>Add new parameter</code> dropdown give Attachment name you want and add Attachment Content as <code>message</code> from Dynamic content.</strong></p>
<p><img src=""https://i.imgur.com/q4YetIe.png"" alt=""enter image description here"" /></p>
<p><strong>For the Body, enter the following JSON:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;message&quot; : &quot;@{string(activity('Lookup1').output.value)}&quot;,
    &quot;dataFactoryName&quot; : &quot;@{pipeline().DataFactory}&quot;, 
    &quot;pipelineName&quot; : &quot;@{pipeline().Pipeline}&quot; ,
    &quot;receiver&quot; : &quot;@{pipeline().parameters.receiver}&quot;
}
</code></pre>
<p><img src=""https://i.imgur.com/7EPOguy.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/5D8mG1h.png"" alt=""enter image description here"" /></p>
"
"75022126","Is it possible to export activity logs on self-hosted IR in Azure Data Factory?","<p>I have a Web Activity module in an Azure Data Factory (ADF) pipeline that calls an API, and it's been running for some time. I want to export the &quot;Activity Log&quot; of all instances of this activity.</p>
<p>I can see the Activity Log for a single instance through monitor tab in ADF. When trying to access the log I see this message  first:<br />
<a href=""https://i.stack.imgur.com/F3QyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F3QyX.png"" alt=""enter image description here"" /></a></p>
<p>Then when I click &quot;confirm&quot; at the bottom right corner, a log table like this is shown:<br />
<a href=""https://i.stack.imgur.com/jHyoT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jHyoT.png"" alt=""enter image description here"" /></a></p>
<p>I was wondering if it's possible to export this log for all instances of execution of this Web Activity module.</p>
","<azure><azure-data-factory>","2023-01-05 17:18:54","124","1","1","75035164","<p>I also asked the question in Microsoft forum and got the answer:</p>
<p>We want to export the logs of you Self-Hosted Integration Runtime (SHIR). This is doable. The tricky part is sorting through the logs to find the exact events you want.
The Logs are stored on the same machine as your SHIR. Go to that computer and open/run &quot;Microsoft Integration Runtime&quot;</p>
<p><a href=""https://i.stack.imgur.com/rO4uD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rO4uD.png"" alt=""277000-image.png"" /></a></p>
<p>This will open the Microsoft Integration Runtime Configuration Manager. You will likely be prompted to allow it to make changes.</p>
<p>Click on Diagnostics
<a href=""https://i.stack.imgur.com/WfxSI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WfxSI.png"" alt=""276919-image.png"" /></a></p>
<p>Then click &quot;View Logs&quot;.
<a href=""https://i.stack.imgur.com/c7Qgb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c7Qgb.png"" alt=""enter image description here"" /></a></p>
<p>This will open the &quot;Event Viewer&quot; which is part of Windows. The Event Viewer is not part of Azure Data Factory. Event Viewer comes with windows.
logs
As Event Viewer is part of Microsoft Windows, it can show you things not part of the Integration Runtime. After clicking on &quot;View Logs&quot;, and waiting for the Event Viewer to load, it should be showing &quot;Connectors -- Integration Runtime&quot;</p>
<p><a href=""https://i.stack.imgur.com/DxuUJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DxuUJ.png"" alt=""Event Viewer"" /></a></p>
<p>You may want to Filter the log before doing the &quot;Save as&quot;.</p>
<p>There is also a way to do this from Powershell.</p>
<pre><code>Get-EventLog -LogName &quot;Integration Runtime&quot; -Source &quot;Integration Runtime (Self-hosted) Host Service&quot; -After (Get-Date).AddMinutes(-15) | Sort-Object Time | Export-CSV &quot;C:/last15minutes_$dataTimeNow.CSV&quot;
</code></pre>
"
"75010668","Azure Data Factory REST call shows all datetime values as null","<p>I have an ADF data flow which uses a pre-defined data set of type rest as the source. From the moment the get request is made, ADF shows all datetime values as null. Questions such as <a href=""https://stackoverflow.com/questions/63625287/azure-data-factory-data-flow-silently-nulling-date-column"">Azure data factory data flow silently NULLing date column</a> suggest that it is an issue in how adf is casting the value, and that I should intercept the value before that point and apply my own casting logic. Since the values are presented as null from the get, I can't see how I could intercept the value before it is cast.</p>
<p>I know these values to be non-null when I make this rest call in another environment (curl, postman etc). Hoping someone has some insight. I would like to avoid processing the data outside of ADF as an intermediate step if possible.</p>
<p>Here is an example json body which shows the format the date objs are returned in any alternative environment:</p>
<pre><code>       &quot;stats&quot;: {
        &quot;ticket_id&quot;: 100000549322,
        &quot;resolved_at&quot;: null,
        &quot;created_at&quot;: &quot;2023-01-04T19:09:19Z&quot;,
        &quot;updated_at&quot;: &quot;2023-01-04T19:09:22Z&quot;
    }
</code></pre>
","<json><rest><azure-data-factory><freshworks>","2023-01-04 19:59:23","105","0","1","75013237","<p>Under the projection tab of the source tile you can select the default formatting of a data type
<a href=""https://i.stack.imgur.com/ACIcG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ACIcG.png"" alt=""enter image description here"" /></a></p>
"
"75008735","How to Perform for loop type of logic in Azure Data FLow","<p>Here is my requirement:</p>
<p>Basically in a for loop there are 3 things: initialisation, condition, loop</p>
<p>So i have to generate a row basing on for loop logic for every row like:</p>
<ul>
<li><strong>initialisation</strong> : <code>{Start_Rcvd Date}</code> (col in my table)</li>
<li><strong>condition:</strong> <code>DateBetweenDRO</code> (New Column) <code>&lt;= {WIP AGE DATE}</code> (Col in my table)</li>
<li><strong>loop:</strong> <code>addDays({DateBetweenDRO},1)</code> (Col in my table)</li>
</ul>
<p>Output: In the output, for every row it loops through the condition and generate a column and fills from initialsion value to loop ending condition.</p>
<p>Let us consider 2 date cols
NewDate : 31/01/2022
OldDate: 01/05/2022</p>
<p>Now I am mentioning :</p>
<p><strong>initialization expression : NewDate(31/01/2022)
conditional expression</strong> : NewDate&lt;=OldDate
<strong>loop</strong> : NewDate+1month</p>
<p><strong>Output can be :</strong>
<a href=""https://i.stack.imgur.com/BN4na.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BN4na.png"" alt=""enter image description here"" /></a></p>
<p>So For every row its generating 3 rows and adding 1 month every time as value in NewDate col.</p>
<p>Can you help please how to achieve this kind of thing in Azure dataflow</p>
","<azure><azure-data-factory>","2023-01-04 16:56:32","185","0","1","75066274","<blockquote>
<p>How to Perform for loop type of logic in Azure Data FLow
I tried to repro this scene in Azure data flow and below is the approach.</p>
</blockquote>
<ul>
<li>An initial value for <strong>new_date</strong> field is assigned in a dataflow parameter
<code>initialnewParameter=&quot;2022-01-05&quot;</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/omLJcuX.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Source data is taken as in below image.
<img src=""https://i.imgur.com/yRcNlNa.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In order to generate multiple rows, first total number of months between old date and new date is computed.
<code>toInteger(round(monthsBetween(old_date,toDate($initialnewParameter)),0,1))</code>. This will give the rounded-up value of months between old and new date.</p>
</li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#maploop"" rel=""nofollow noreferrer"">Maploop</a> function is used to create an array starting from 1 to value of months between old and new date.</p>
</li>
<li><p>In derived column transformation column named <strong>monthsbw[]</strong> is created and expression is given as <code>mapLoop(toInteger(round(monthsBetween(old_date,toDate($initialnewParameter)),0,1)),#index)</code>.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/d3m4Snz.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>monthsbw[] is created and values are from 1 to months between old and new date as in below image.
<img src=""https://i.imgur.com/iQ8ZJEF.png"" alt=""enter image description here"" /></p>
</li>
<li><p><strong>Flatten transform</strong> is used and this data is unrolled by <strong>monthsbw</strong> array.
<img src=""https://i.imgur.com/GWhexQQ.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Output of the flattened data
<img src=""https://i.imgur.com/ILYoGuw.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Derived column transformation is taken and monthbw is added to the dataflow parameter <code>initialnewParameter</code>  which stores the initial value of new date.
<strong>Expression for new_date column:</strong>
<code>addMonths(toDate($initialnewParameter),toInteger(monthbw)-1)</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/6MBXA8o.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Output of the derived column activity:
<img src=""https://i.imgur.com/BhOQ0EK.png"" alt=""enter image description here"" /></p>
</li>
<li><p>By this way, you can generate multiple rows from a row in azure data flow. You can use select transformation and select only required columns.</p>
</li>
</ul>
"
"75006692","Monitoring Azure Data Factory access","<p>Kind of a simple question, but puzzling...</p>
<p>Is there a stat in Azure services to monitor how many times data factory is / was accessed ?</p>
<p>So, as an example if an automated system is set up to make persistent API calls to ADF with the malicious intent exhaust it is there a way to monitor for that and gather some kind of stats?</p>
","<azure><azure-data-factory><azure-security>","2023-01-04 14:12:04","39","0","1","75009800","<p>The monitoring built into the Azure Data Factory PaaS itself only monitors legitimate, authenticated usage. You can see this on the <code>https://adf.azure.com/en/monitoring/pipelineruns?factory=%2Fsubscriptions%...</code> dashboard.
<a href=""https://i.stack.imgur.com/GXp4z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GXp4z.png"" alt=""ADF Monitoring Dashboard"" /></a></p>
<p>Notice how the root domain is <code>adf.azure.com</code> - this is the same for all tenants using data factory around the world. Your specific subscription / instance are mere query parameters in the URL. Microsoft Azure is <strong>fully managing</strong> the actual hosting of this PaaS, which means they are entirely responsible for subverting any DDOS or similar bad-actor attempts on this service. It's not something you have to worry about, and therefore not something you have much visibility into.</p>
<p>If you ever needed or wanted to check in on how microsoft is doing with this, head on over to <a href=""https://status.azure.com/status"" rel=""nofollow noreferrer"">https://status.azure.com/status</a> and search for the &quot;Azure Data Factory&quot; row:
<a href=""https://i.stack.imgur.com/Jjl4L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jjl4L.png"" alt=""Azure Status Page"" /></a></p>
<p>This is really one of the biggest selling points of using a fully-hosted cloud PaaS such as Data Factory. You are no longer responsible for the hardware, or even range of ip addresses that back this service. No more than you have to worry about someone DDOS'ing <code>outlook.office.com</code> which probably services your entire organisation's email. I could happen, but if it did, it affects all of Microsoft's customers around the world, not just you personally, so there should be no expectation that you personally are doing anything special to mitigate against it.</p>
<hr />
<p>Note that more generically if you want to monitor network traffic within your NSGs, iterfaces, VNETs etc in general on Azure, the thing to use is the Application Insights' Network Monitoring at <a href=""https://portal.azure.com/#view/Microsoft_Azure_Monitoring/AzureMonitoringBrowseBlade/%7E/networkInsights"" rel=""nofollow noreferrer"">https://portal.azure.com/#view/Microsoft_Azure_Monitoring/AzureMonitoringBrowseBlade/~/networkInsights</a>
This is more generically applicable to all provisioned resources and services on Azure though, not something specific to Azure Data Factory.</p>
"
"75005717","Error when creating view on pipeline (problem with BULK path)","<p>Good morning everybody!</p>
<p>Me and my team managed to create part of an Azure Synapse pipeline which selects the database and creates a data source named 'files'.
Now we want to create a view in the same pipeline using a Script activity. However, this error comes up:</p>
<p><a href=""https://i.stack.imgur.com/Rqnn2.png"" rel=""nofollow noreferrer"">Error message here</a></p>
<p>Even if we hardcoded the folder names and the file name on the path, the pipeline won't recognise the existance of the file in question.</p>
<p>This is our query. If we run it manually on a script in the Develop section everything works smoothly:</p>
<p><a href=""https://i.stack.imgur.com/5klBe.png"" rel=""nofollow noreferrer"">CREATE VIEW query here</a></p>
<p>We expected to get every file with &quot;.parquet&quot; extension inside every folder available on our data_source named 'files'. However, running this query on the Azure Synapse Pipeline won't work. If we run it on a script in Develop section, it works perfectly. We want to achieve that result.</p>
<p>Could anyone help us out?
Thanks in advance!</p>
","<azure><azure-synapse><azure-data-factory>","2023-01-04 12:52:40","72","0","1","75015201","<p>I tried to reproduce the same thing my environment and got error.</p>
<p><img src=""https://i.imgur.com/WdqWwvl.png"" alt=""enter image description here"" /></p>
<p>The cause of error can be the Your synapse service principal or the user who is accessing the storage account does not have the role of <code>Storage Blob data Contributor</code> role assigned to it or your External data source have some issue. try with creating new external data source with <code>SAS token</code>.</p>
<p><strong>Sample code:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>CREATE  DATABASE  SCOPED  CREDENTIAL SasToken
WITH  IDENTITY = 'SHARED ACCESS SIGNATURE',
SECRET = 'SAS token';
GO

CREATE  EXTERNAL  DATA  SOURCE mysample1
WITH ( LOCATION = 'storage account',
CREDENTIAL = SasToken
)

CREATE  VIEW [dbo].[View4] AS  SELECT [result].filepath(1) as [YEAR], [result].filepath(2) as [MONTH], [result].filepath(3) as [DAY], *
FROM
OPENROWSET(
BULK  'fsn2p/*-*-*.parquet',
DATA_SOURCE = 'mysample1',
FORMAT = 'PARQUET'
) AS [result]
</code></pre>
<p><strong>Execution:</strong></p>
<p><img src=""https://i.imgur.com/AeiNKtY.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/2tE2ua0.png"" alt=""enter image description here"" /></p>
"
"75005100","Azure data factory Pass data between activities without storing anywhere","<p>I am new to Azure data Factory.
I am trying to get data from an API using Web Activity and then will use some activity and then data flow and then some other activity and at end will store in BLOB. In this whole transformation Do I need to have any persistency layer like blob or sql database between the passes of activities and Data Flow. Or these activities can hold data. The data size may vary from MB to GB.</p>
","<azure-data-factory>","2023-01-04 11:57:04","128","0","1","75014934","<blockquote>
<p>In this whole transformation Do I need to have any persistency layer
like blob or sql database between the passes of activities and Data
Flow</p>
</blockquote>
<p>This depends on your requirement.</p>
<p><strong>AFAIK</strong>, in ADF <strong>Web activity, lookup activity, dataflow sink cache, script activity and set variable activity (only array)</strong> will give the result as activity output. (Also, Notebook activity if we pass it as JSON dumps from notebook code)</p>
<p><strong>And all will give the output as array of objects apart from web</strong>.</p>
<ul>
<li><p>we can get the output of <strong>lookup</strong> using dynamic content <code>@activity('activity name').output.value</code></p>
</li>
<li><p>For <strong>web activity</strong> it is <code>@activity('activity name').output</code>. In this, it will give the output as JSON object.</p>
<p><img src=""https://i.imgur.com/59U5b9N.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In dataflow sink, use <strong>sink cache</strong> and check <strong>write to activity output</strong>. Then set the logging level to None in dataflow activity.</p>
<p><img src=""https://i.imgur.com/vdFVRSO.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/QROzTEF.png"" alt=""enter image description here"" /></p>
<p>Check the data in dataflow activity output and give use appropriate     dynamic content to get it.</p>
<p><img src=""https://i.imgur.com/lyMiL6g.png"" alt=""enter image description here"" /></p>
</li>
<li><p>You can get the <strong>Script activity</strong> output like <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script#activity-output"" rel=""nofollow noreferrer"">this</a> and the dynamic content according to it.</p>
</li>
<li><p><strong>Set variable activity</strong> can be used to store these results as array of objects(to avoid complex dynamic content expression).
<img src=""https://i.imgur.com/REJ5VLQ.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<blockquote>
<p>The data size may vary from MB to GB.</p>
</blockquote>
<p><strong>But all the above activities data size limit is 5000 rows only i.e., 4MB</strong>. So, you need intermediate storage SQL or blob when using more size.</p>
"
"75004205","Syncing incremental data with ADF","<p>In Synapse I've setup 3 different pipelines. They all gather data from different sources (SQL, REST and CSV) and sink this to the same SQL database.</p>
<p>Currently they all run during the night, but I already know that the question is coming of running it more frequently. I want to prevent that my pipelines are going to run through all the sources while nothing has changed in the source.</p>
<p>Therefore I would like to store the last succesfull sync run of each pipeline (or pipeline activity). Before the next start of each pipeline I want to create a new pipeline, a 4th one, which checks if something has changed in sources. If so, it triggers the execution of one, two or all three the pipelines to run.</p>
<p>I still see some complications in doing this, so I'm not fully convinced on how to do this. So all help and thoughts are welcome, don't know if someone has experience in doing this?</p>
","<azure><azure-data-factory><etl><data-analysis><azure-synapse>","2023-01-04 10:39:47","98","1","1","75010356","<p>This is (at least in part) the subject of the following Microsoft tutorial:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal"" rel=""nofollow noreferrer"">Incrementally load data from Azure SQL Database to Azure Blob storage using the Azure portal</a></p>
<p>You're on the correct path - the crux of the issue is creating and persisting &quot;watermarks&quot; for each source from which you can determine if there have been any changes. The approach you use may be different for different source types. In the above tutorial, they create a stored procedure that can store and retrieve a &quot;last run date&quot;, and use this to intelligently query tables for only rows modified after this last run date. Of course this requires the cooperation of the data source to take note of when data is inserted or modified.</p>
<p>If you have a source that cannot by intelligently queried in part (e.g. a CSV file) you still have options to use things like the <a href=""https://learn.microsoft.com/en-ca/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get Metadata Activity</a> to e.g. query the <code>lastModified</code> property of a source file (or even its <code>contentMD5</code> if using blob or ADLGen2) and compare this to a value saved during your last run (You would have to pick a place to store this, e.g. an operational DB, Azure Table or small blob file) to determine whether it needs to be reprocessed.</p>
<p>If you want to go crazy, you can look into streaming patterns (might require dabbling in HDInsights or getting your hands dirty with Azure Event Hubs to trigger ADF) to move from the scheduled trigger to an automatic ingestion as new data appears at the sources.</p>
"
"75003754","How to add leading zeros in ADF data flow from the expression builder","<p>How to add leading zeros in ADF data flow from the expression builder</p>
<p>For example – have column with numeric value as “000001” but it is coming as 1 only in SQL DB , if I put in  entire value in single quotes it is coming but I need dynamic way of implementation with out hard coding.</p>
","<sql-server><azure-data-factory>","2023-01-04 09:58:04","302","0","1","75005585","<p>I agree with <strong>@Larnu</strong>'s comments that even if we give <code>00001</code> to an int type column it will give as <code>1</code> only.</p>
<p>So, we have to give those in single quotes (<code>'00001'</code>) to use like that or import the incoming data as string instead of int.</p>
<p>As you are using ADF dataflow, if you want to use the <code>00001</code>, you can generate those using <strong>derived column transformation</strong> from SQL source. But this depends on your requirement like how your leading 0's varies. So, use according to it.</p>
<p><strong>Sample demo:</strong></p>
<pre><code>concat('0000', toString(id))
</code></pre>
<p><img src=""https://i.imgur.com/JFUhdii.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/x0OnWNn.png"" alt=""enter image description here"" /></p>
<p>Use that column as per your requirement, after that you can convert it back to the same input by <code>toInteger(id)</code>.</p>
"
"75003532","ADF Tumbling Window Trigger Dependency","<p>I went over the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tumbling-window-trigger-dependency"" rel=""nofollow noreferrer"">link</a> but I could not understand what is the diagram trying to convey. The diagram is not conclusive enough. There is no explanation.</p>
<p>Consider the following diagram from the doc:
<a href=""https://i.stack.imgur.com/JM7RT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JM7RT.png"" alt=""enter image description here"" /></a></p>
<p>The <em><strong>Dependency Offset</strong></em> part seems clear. The second diagram in Dependency Offset tells me that Trigger A's 10 o clock run should depend on Trigger B's 9 o clock run.</p>
<p>The <em><strong>Dependency Size</strong></em> part does not quite make sense. How to make sense out of it/interpret it? e.g. the second example in the Dependency Size section where it says Offset is -1 hr and Size is 2 hrs. What is the <strong>size</strong> referring to here? The size 2hrs defined there is causing to have an overlap of 1 hr (i.e. between 10 and 11) with the complete window of Trigger A which effectively leaves Trigger A with no time to execute. I am sure I am interpreting it wrong. Could someone please help?</p>
<p>.</p>
","<azure><azure-data-factory>","2023-01-04 09:37:48","75","1","1","75010623","<p>Imagine you have a job that runs every hour to produce &quot;usage metrics&quot; for the past hour. Imagine &quot;Trigger B&quot; runs this job hourly.</p>
<p>Now you want a job (Trigger A) that creates a &quot;rolling average&quot; of usage metrics over the past two hours. To reduce computational requirements, you can produce this by taking the average of the usage metrics produced for the last two hours (calculations output by Trigger B).</p>
<p>You still run this &quot;rolling average&quot; (&quot;Trigger A&quot;) job every hour, but it relies not only on the most recent &quot;Trigger B&quot; run that ran on this hour, but the previous one as well. The &quot;Dependency Size&quot; of 2 hours (with offset of -1 hour) means this trigger A will not run unless the 2 &quot;Trigger B&quot; jobs (from this hour and last hour) have <em>both</em> completed successfully.</p>
<p>Agreed that the diagram conveys the concept poorly for a number of reasons. For example in my opinion, Trigger B should be named A and vice-versa to make the dependency order more intuitively obvious.</p>
<p>Also, the individual/multiple pipeline runs should be visualized within the windows. Something like this might have been better:
<a href=""https://i.stack.imgur.com/S9yBs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S9yBs.png"" alt=""enter image description here"" /></a></p>
"
"75002794","How to copy the file in ADF if target container doesn't have files?","<p>I want to copy data from source to sink if target container does not have any file in it.</p>
<p>I used get meta data activity and filter activity. But this doesn't give me expected output.</p>
","<azure><azure-data-factory>","2023-01-04 08:32:36","96","0","1","75003171","<p>Try using <code>IF activity</code> and give the expression as <code>@equals(length(activity('Get Metadata1').output.childItems),0)</code> to check if the target has no file. I reproduced this and below is the approach.</p>
<ul>
<li>Get Metadata activity is taken and child items is taken as an argument. This will give the list of files or folders present in the datastore.</li>
</ul>
<p><img src=""https://i.imgur.com/cYoUERJ.png"" alt=""enter image description here"" /></p>
<ul>
<li>If activity is taken and checked if the length of the list generated by metadata activity is greater than zero.
<code>@equals(length(activity('Get Metadata1').output.childItems),0)</code></li>
</ul>
<p><img src=""https://i.imgur.com/xUgYPeZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>In true section, you can add copy activity to copy data from source to sink.</li>
</ul>
<p>By this way, you can get your requirement.</p>
"
"75001712","array of input tables as pipeline parameter","<p>With Continuation from the question which is already resolved :
<a href=""https://stackoverflow.com/questions/74996159/how-to-concatenate-with-parameter-in-azure-data-factory-passing-dynamic-table"">how to concatenate with parameter in azure data factory / passing dynamic table names in source query</a></p>
<p>i have created a pipeline parameter and passing array of tables to perform multiple copy (incremental) operation using control table (watermark table). i have successfully tested the pipeline with single table as a parameter which is of type string. However when i am changing the parameter from String to Array and passing 2 tables [&quot;table1&quot;,&quot;table2&quot;] i am getting error as:</p>
<ul>
<li>&quot;concat does not have an overload that supports the arguments given:(StringLiteral,Array.........)&quot; *</li>
</ul>
<p>However i have updated the parameterized query in following way (using converion to string).</p>
<p><code>@concat('select * from DW_GL.',string(pipeline().parameters.p_param_input_table),' where updated_on &gt; ''',activity('Old_Lookup1').output.firstRow.date_value,''' and updated_on &lt;= ''',activity('Old_Lookup1').output.firstRow.date_value_new, '''')</code></p>
<p>i am getting error now as follows:</p>
<p><em>ErrorCode=UserErrorInvalidValueInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to convert the value in 'table' property to 'System.String' type. Please make sure the payload structure and value are correct.,Source=Microsoft.DataTransfer.DataContracts,''Type=System.InvalidCastException,Message=Object must implement IConvertible.,Source=mscorlib,' –</em></p>
<p>However i have updated the parameterized query in following way (using converion to string).</p>
<p><code>@concat('select * from DW_GL.',string(pipeline().parameters.p_param_input_table),' where updated_on &gt; ''',activity('Old_Lookup1').output.firstRow.date_value,''' and updated_on &lt;= ''',activity('Old_Lookup1').output.firstRow.date_value_new, '''')</code></p>
","<azure-data-factory>","2023-01-04 06:34:09","159","0","1","75002949","<ul>
<li><p>You have an array of tables, and trying to use it in the same query just by replacing string with an array of tables directly.</p>
</li>
<li><p>If you want to apply the process which worked for each table separately, then you can use a for each loop. Lets say I have the following array of table names:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/jtYFxt9.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I have configured the for each loop where I have given the items value as the above parameter. Inside for each I have taken a sample script. I have used a query similar to the one given below:</li>
</ul>
<pre><code>@concat('select * from DW_GL.',item(),' where updated_on &gt; ''',activity('Old_Lookup1').output.firstRow.id,''' and updated_on &lt;= ''',activity('Old_Lookup1').output.firstRow.id, '''')
</code></pre>
<p><a href=""https://i.stack.imgur.com/bdjXR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bdjXR.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Now within this for each, you have to keep all the activities that you have used in the case of single table name for parameter (string).</li>
<li>If you are trying to do it simultaneously, then you can check what query is being implemented in the debug input of script activity. Giving same query as yours would build the query in the following way:</li>
</ul>
<p><img src=""https://i.imgur.com/zZRa3om.png"" alt=""enter image description here"" /></p>
<ul>
<li>To select multiple tables at a time, you can use <code>join()</code> instead of directly converting it to string. Use the following dynamic content instead:</li>
</ul>
<pre><code>@concat('select * from DW_GL.',join(pipeline().parameters.p_param_input_table,',DW_GL.'),' where updated_on &gt; ''',activity('Old_Lookup1').output.firstRow.date_value,''' and updated_on &lt;= ''',activity('Old_Lookup1').output.firstRow.date_value_new, '''')
</code></pre>
<p><img src=""https://i.imgur.com/tCjGqsD.png"" alt=""enter image description here"" /></p>
<p><strong>NOTE:</strong> My activity failed as I just wanted to demonstrate what query will be executed based on the given dynamic content. Use accordingly.</p>
"
"74996159","how to concatenate with parameter in azure data factory / passing dynamic table names in source query","<p>I have created in pipeline parameter and i wanted to pass array of tables to perform multiple copy activity. BUT before that i am testing with only one table as a parameter.
I have created below query in expression (sink side of copy activity).
(Please note i am copying from synapse to synapse (from one schema to another))</p>
<pre><code>@concat('select * from DW_GL.',pipeline().parameters.p_param_input_table,' where updated_on &gt;',activity('Old_Lookup1').output.firstRow.date_value,' and updated_on &lt;=',activity('Old_Lookup1').output.firstRow.date_value_new)
</code></pre>
<p>My table is in the schema &quot;DW_GL&quot; so i tried to remove it from above expression and still its not working.</p>
<p>i am getting error:</p>
<blockquote>
<p>Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Parse error at line: 1, column: 75: Incorrect syntax near '03'.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Parse error at line: 1, column: 75: Incorrect syntax near '03'.,Source=.Net SqlClient Data Provider,SqlErrorNumber=103010,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=103010,State=1,Message=Parse error at line: 1, column: 75: Incorrect syntax near '03'.,},],'</p>
</blockquote>
<p>i tried removing empty white spaces from expression .
also tried to use direct parameter without concatenate as '@{pipeline().parameters.p_param_input_table}' still not worked.</p>
","<azure-data-factory>","2023-01-03 16:38:51","226","0","2","74998412","<p>I think the error is because this concat statement does not include single quotes (') around the date values. You need to use '' to include them, which translates to ''' to end (or begin) a string with ', and '''' to insert JUST a single ':</p>
<pre><code>@concat('select * from DW_GL.',pipeline().parameters.p_param_input_table,' where updated_on &gt; ''',activity('Old_Lookup1').output.firstRow.date_value,''' and updated_on &lt;= ''',activity('Old_Lookup1').output.firstRow.date_value_new, '''')
</code></pre>
"
"74996159","how to concatenate with parameter in azure data factory / passing dynamic table names in source query","<p>I have created in pipeline parameter and i wanted to pass array of tables to perform multiple copy activity. BUT before that i am testing with only one table as a parameter.
I have created below query in expression (sink side of copy activity).
(Please note i am copying from synapse to synapse (from one schema to another))</p>
<pre><code>@concat('select * from DW_GL.',pipeline().parameters.p_param_input_table,' where updated_on &gt;',activity('Old_Lookup1').output.firstRow.date_value,' and updated_on &lt;=',activity('Old_Lookup1').output.firstRow.date_value_new)
</code></pre>
<p>My table is in the schema &quot;DW_GL&quot; so i tried to remove it from above expression and still its not working.</p>
<p>i am getting error:</p>
<blockquote>
<p>Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Parse error at line: 1, column: 75: Incorrect syntax near '03'.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Parse error at line: 1, column: 75: Incorrect syntax near '03'.,Source=.Net SqlClient Data Provider,SqlErrorNumber=103010,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=103010,State=1,Message=Parse error at line: 1, column: 75: Incorrect syntax near '03'.,},],'</p>
</blockquote>
<p>i tried removing empty white spaces from expression .
also tried to use direct parameter without concatenate as '@{pipeline().parameters.p_param_input_table}' still not worked.</p>
","<azure-data-factory>","2023-01-03 16:38:51","226","0","2","75000808","<p>Below query can also work :</p>
<pre><code>select * from DW_GL.@{pipeline().parameters.p_param_input_table} where updated_on &gt; '@{activity('Old_Lookup1').output.firstRow.date_value}' and updated_on &lt;=' @{activity('Old_Lookup1').output.firstRow.date_value_new}'
</code></pre>
"
"74995940","How to filter/map an array in a data factory copy activity from a hierarchy JSON to tabular file?","<p><strong>The context</strong></p>
<p>I'm new to the Azure environment, so please bare with me if the question is simple. I have called a REST API with pagination. The data has multiple arrays stored in a hierachy. The arrays contains the same value translated in different languages. So in theory if I only want one language from that array the data is already in a tabular format. However, i'm having trouble with filtering the data to the correct language in the mapping part of the copy activity.</p>
<p><strong>Sample data</strong></p>
<p>Below is a sample of the data. I have added 3 different 'rows' for the tabular format. There are 3 different arrays in the data:</p>
<ol>
<li>['stage']['localization']</li>
<li>['disqualifyReason']['localization']</li>
<li>['title']['localization']</li>
</ol>
<p>As I work for a dutch company, we only want the value where locale == 'nl-NL' to be returned.</p>
<pre><code>[
    {
        &quot;id&quot;: &quot;f2597aa9-45b3-4142-a343-b1ec27fbfcea&quot;,
        &quot;email&quot;: &quot;some@email.com&quot;,
        &quot;firstName&quot;: &quot;Name&quot;,
        &quot;lastName&quot;: &quot;Name&quot;,
        &quot;middleName&quot;: null,
        &quot;created&quot;: &quot;2023-01-03T13:29:15.7452993Z&quot;,
        &quot;status&quot;: 1,
        &quot;stage&quot;: {
            &quot;localization&quot;:[
                {
                    &quot;locale&quot;: &quot;da-DK&quot;,
                    &quot;value&quot;: &quot;Ansøgt&quot;
                },
                {
                    &quot;locale&quot;: &quot;de-DE&quot;,
                    &quot;value&quot;: &quot;Beworben&quot;
                },
                {
                    &quot;locale&quot;: &quot;en-GB&quot;,
                    &quot;value&quot;: &quot;Applied&quot;
                },
                {
                    &quot;locale&quot;: &quot;nl-NL&quot;,
                    &quot;value&quot;: &quot;Gesolliciteerd&quot;
                }
            ]
        },
        &quot;disqualifyReason&quot;: {
            &quot;localization&quot;:[
                {
                    &quot;locale&quot;: &quot;nl-NL&quot;,
                    &quot;value&quot;: &quot;Geen match&quot;
                },
                {
                    &quot;locale&quot;: &quot;da-DK&quot;,
                    &quot;value&quot;: &quot;Ikke et match&quot;
                },
                {
                    &quot;locale&quot;: &quot;de-DE&quot;,
                    &quot;value&quot;: &quot;Absage - Screening&quot;
                },
                {
                    &quot;locale&quot;: &quot;en-GB&quot;,
                    &quot;value&quot;: &quot;Not a match&quot;
                }
            ]
        },
        &quot;source&quot;:{
            &quot;media&quot;:{
                &quot;id&quot;: &quot;c0772eab-09dd-4c7c-86b5-ee9b65ed8398&quot;, 
                &quot;title&quot;: {
                    &quot;localization&quot;:[
                        {
                            &quot;locale&quot;: &quot;nl-NL&quot;,
                            &quot;value&quot;: &quot;Tegel voor URL&quot;
                        }
                    ]
                }
            }
        }
    },
    {
        &quot;id&quot;: &quot;a72b856e-8000-4e51-b475-9e6af5cf9e19&quot;,
        &quot;email&quot;: &quot;some@email.com&quot;,
        &quot;firstName&quot;: &quot;Name&quot;,
        &quot;lastName&quot;: &quot;Name&quot;,
        &quot;middleName&quot;: null,
        &quot;created&quot;: &quot;2023-01-03T13:29:15.7452993Z&quot;,
        &quot;status&quot;: 1,
        &quot;stage&quot;: {
            &quot;localization&quot;:[
                {
                    &quot;locale&quot;: &quot;nl-NL&quot;,
                    &quot;value&quot;: &quot;Afwijzen op CV&quot;
                }
            ]
        },
        &quot;disqualifyReason&quot;: null,
        &quot;source&quot;:{
            &quot;media&quot;:{
                &quot;id&quot;: &quot;c0772eab-09dd-4c7c-86b5-ee9b65ed8398&quot;, 
                &quot;title&quot;: {
                    &quot;localization&quot;:[
                        {
                            &quot;locale&quot;: &quot;nl-NL&quot;,
                            &quot;value&quot;: &quot;Tegel voor URL&quot;
                        }
                    ]
                }
            }
        }
    },
    {
        &quot;id&quot;: &quot;f3898ebd-d6d6-4d9e-979e-348fe79325dc&quot;,
        &quot;email&quot;: &quot;some@email.com&quot;,
        &quot;firstName&quot;: &quot;Name&quot;,
        &quot;lastName&quot;: &quot;Name&quot;,
        &quot;middleName&quot;: null,
        &quot;created&quot;: &quot;2023-01-03T14:36:04.4517426Z&quot;,
        &quot;status&quot;: 1,
        &quot;stage&quot;: {
            &quot;localization&quot;:[
                {
                    &quot;locale&quot;: &quot;nl-NL&quot;,
                    &quot;value&quot;: &quot;1e interview&quot;
                },
                {
                    &quot;locale&quot;: &quot;da-DK&quot;,
                    &quot;value&quot;: &quot;1. samtale&quot;
                },
                {
                    &quot;locale&quot;: &quot;en-GB&quot;,
                    &quot;value&quot;: &quot;1st Interview&quot;
                },
                {
                    &quot;locale&quot;: &quot;nl-NL&quot;,
                    &quot;value&quot;: &quot;1. Interview&quot;
                }
            ]
        },
        &quot;disqualifyReason&quot;: null,
        &quot;source&quot;:{
            &quot;media&quot;:{
                &quot;id&quot;: &quot;c0772eab-09dd-4c7c-86b5-ee9b65ed8398&quot;, 
                &quot;title&quot;: {
                    &quot;localization&quot;:[
                        {
                            &quot;locale&quot;: &quot;nl-NL&quot;,
                            &quot;value&quot;: &quot;Tegel voor URL&quot;
                        }
                    ]
                }
            }
        }
    }
]

</code></pre>
<p><strong>What did I try</strong></p>
<p>Lots of google, and microsoft learn pages. However, I thought the following dynamic function would work in the mapping part of the copy activity
<code>@filter($['stage']['localization']['locale'] == 'nl-NL')</code>, which it doens't. I can't use the filter function in the copy activity pipeline. I believe I can save the API call to a JSON file, then use data flows to filter it out in a data flow activity, which then stores it to a tabular format. However, isn't there a way to directly filter the data in the copy activity?</p>
<p>Many thanks for any help!</p>
","<azure><azure-functions><azure-data-factory>","2023-01-03 16:21:59","203","0","1","75003037","<p>In copy activity mapping, there is a dynamic content option.</p>
<p><img src=""https://i.imgur.com/Mz2qH8E.png"" alt=""enter image description here"" /></p>
<p>But <strong>AFAIK</strong>, <strong>this will only apply to filter specific columns from source</strong>. But in your case, you are trying to filter the records which might not be possible using copy activity.</p>
<blockquote>
<p>I believe I can save the API call to a JSON file, then use data flows
to filter it out in a data flow activity, which then stores it to a
tabular format.</p>
</blockquote>
<p>Yes, using dataflows is the solution for it. And <strong>dataflows also support REST API source. You can directly use dataflows and give pagination</strong> like copy activity.</p>
<p><img src=""https://i.imgur.com/tXWntXj.png"" alt=""enter image description here"" /></p>
<p>Then use filter transformation with your condition.</p>
<p><img src=""https://i.imgur.com/p3c3O7B.png"" alt=""enter image description here"" /></p>
<p>You will get the desired result in debug.</p>
"
"74995849","Dropping columns in Azure Data Factory based on values in columns","<p>I'm working with a dataset where I need to drop some columns which contain only <code>NULL</code> values. The issue is that the column names are not consistent or similar, and can change with time. I was wondering if there is a way in ADF to drop a column if all instances are <code>NULL</code> without having drifted columns?</p>
<p>I have tried unpivoting, removing rows, then re-pivoting, however after I pivot the data back to its original format, I get the following message:</p>
<blockquote>
<p>&quot;This drifted column is not in the source schema and therefore can only be referenced with pattern matching expressions&quot;</p>
</blockquote>
<p>The drifted columns don't seem to join on subsequent join functions. I have also tried setting derived columns with regex column patters to make all the drifted columns explicit, however, the <code>byName()</code> function doesn't seem to work with the <code>$$</code> syntax; namely:</p>
<pre><code>toString(byName($$))
</code></pre>
<p>Any ideas of how to solve this within Azure Data Factory - Data Flows would be very much appreciated!</p>
","<azure><azure-data-factory>","2023-01-03 16:13:23","283","0","2","74998178","<p>If the source column names will change, then you have to use column patterns. When you match columns based on patterns, you can project those into columns using the Select transformation. Use the rule-based mapping option in the Select transformation with true() as the matching expression and $$ as the Name As property like this:</p>
<p><a href=""https://i.stack.imgur.com/ij5xu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ij5xu.png"" alt=""enter image description here"" /></a></p>
"
"74995849","Dropping columns in Azure Data Factory based on values in columns","<p>I'm working with a dataset where I need to drop some columns which contain only <code>NULL</code> values. The issue is that the column names are not consistent or similar, and can change with time. I was wondering if there is a way in ADF to drop a column if all instances are <code>NULL</code> without having drifted columns?</p>
<p>I have tried unpivoting, removing rows, then re-pivoting, however after I pivot the data back to its original format, I get the following message:</p>
<blockquote>
<p>&quot;This drifted column is not in the source schema and therefore can only be referenced with pattern matching expressions&quot;</p>
</blockquote>
<p>The drifted columns don't seem to join on subsequent join functions. I have also tried setting derived columns with regex column patters to make all the drifted columns explicit, however, the <code>byName()</code> function doesn't seem to work with the <code>$$</code> syntax; namely:</p>
<pre><code>toString(byName($$))
</code></pre>
<p>Any ideas of how to solve this within Azure Data Factory - Data Flows would be very much appreciated!</p>
","<azure><azure-data-factory>","2023-01-03 16:13:23","283","0","2","75053797","<ul>
<li><p>I have used combination of both data factory pipeline activities and dataflow to achieve the requirement.</p>
</li>
<li><p>First, I have taken dataflow to output a file. I have added a new column with all values as <code>1</code> so that I can use aggregate on all other rows using this new column to group.
<img src=""https://i.imgur.com/RP7D2AW.png"" alt=""enter image description here"" /></p>
</li>
<li><p>I have used collect() to create an array for each of the column where group by is on above created column.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/cepmeHQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now create another derived column to replace the array by converting array to string and calculating length. If length is 2 it indicates that column contains all nulls.</li>
</ul>
<p><img src=""https://i.imgur.com/EBT7VjK.png"" alt=""enter image description here"" /></p>
<ul>
<li>Write this dataflow output to a file. The data preview of the sink will be as follows:</li>
</ul>
<p><img src=""https://i.imgur.com/Ob7C83L.png"" alt=""enter image description here"" /></p>
<ul>
<li>Create a dataflow activity to run the above dataflow and pass the following dynamic content in execute pipeline activity to filter out and write data of only required columns.</li>
</ul>
<pre><code>@activity('Data flow1').output.runstatus.profile.sink1.total
</code></pre>
<p><img src=""https://i.imgur.com/6ldC1Z7.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>In pipeline2, I have used activities to get columns that are not entirely nulls, create a dynamic schema and then use this schema as mapping and write to a file only the required columns.</p>
</li>
<li><p>First, I have read the file written at the end of dataflow without header (even though the file has header). The dataset looks as shown below:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/mUIf1w3.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can directly use the following pipeline JSON to build the pipeline:</li>
</ul>
<pre><code>{
    &quot;name&quot;: &quot;pipeline2&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Lookup1&quot;,
                &quot;type&quot;: &quot;Lookup&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;cols&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;firstRowOnly&quot;: false
                }
            },
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Lookup1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@range(0,pipeline().parameters.count_of_rows)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Append variable1&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;props&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;Prop_@{item()}&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;ForEach2&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@variables('props')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Append variable2&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;req_cols&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@if(and(not(equals(activity('Lookup1').output.value[0][item()],'tp')),not(equals(activity('Lookup1').output.value[1][item()],'2'))),activity('Lookup1').output.value[0][item()],'')&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Filter1&quot;,
                &quot;type&quot;: &quot;Filter&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach2&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@variables('req_cols')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;condition&quot;: {
                        &quot;value&quot;: &quot;@not(equals(item(),''))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;ForEach3&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Filter1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@activity('Filter1').output.Value&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Append variable3&quot;,
                            &quot;type&quot;: &quot;AppendVariable&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;variableName&quot;: &quot;mapping&quot;,
                                &quot;value&quot;: {
                                    &quot;value&quot;: &quot;@json(concat('{\&quot;source\&quot;:{\&quot;name\&quot;:\&quot;',item(),'\&quot;},\&quot;sink\&quot;:{\&quot;name\&quot;:\&quot;',item(),'\&quot;}}'))&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;ForEach3&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;dynamic_schema&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@concat('{\&quot;type\&quot;:\&quot;TabularTranslator\&quot;,\&quot;mappings\&quot;:',string(variables('mapping')),'}}')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Copy data1&quot;,
                &quot;type&quot;: &quot;Copy&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;source&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSource&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                            &quot;recursive&quot;: true,
                            &quot;enablePartitionDiscovery&quot;: false
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextReadSettings&quot;
                        }
                    },
                    &quot;sink&quot;: {
                        &quot;type&quot;: &quot;DelimitedTextSink&quot;,
                        &quot;storeSettings&quot;: {
                            &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                        },
                        &quot;formatSettings&quot;: {
                            &quot;type&quot;: &quot;DelimitedTextWriteSettings&quot;,
                            &quot;quoteAllText&quot;: true,
                            &quot;fileExtension&quot;: &quot;.txt&quot;
                        }
                    },
                    &quot;enableStaging&quot;: false,
                    &quot;translator&quot;: {
                        &quot;value&quot;: &quot;@json(variables('dynamic_schema'))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                },
                &quot;inputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;csv1&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ],
                &quot;outputs&quot;: [
                    {
                        &quot;referenceName&quot;: &quot;req_file&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    }
                ]
            }
        ],
        &quot;parameters&quot;: {
            &quot;count_of_rows&quot;: {
                &quot;type&quot;: &quot;int&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;props&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;req_cols&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;test&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;mapping&quot;: {
                &quot;type&quot;: &quot;Array&quot;
            },
            &quot;dynamic_schema&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/r6do2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r6do2.png"" alt=""enter image description here"" /></a></p>
<p><strong>NOTE:</strong> In the copy data activity, the source is the original file.</p>
"
"74993812","Postman Get Azure Synapse Bearer Token","<p>I am trying to get a Azure Synapse Bearer Token using Postman.  I've followed this Postman link/video and have successfully called:</p>
<p>Azure Management Rest API: <a href=""https://management.azure.com/subscriptions/%7B%7BsubscriptionId%7D%7D/resourcegroups?api-version=2020-09-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups?api-version=2020-09-01</a></p>
<p>Per Microsoft documentation and PowerShell, I have successfully obtained a Synapse Bearer Token and called:</p>
<p><a href=""https://mysynapseworkspace.dev.azuresynapse.net/pipelines?api-version=2020-12-01"" rel=""nofollow noreferrer"">https://mysynapseworkspace.dev.azuresynapse.net/pipelines?api-version=2020-12-01</a></p>
<p>PowerShell: NOTE: The Endpoint is <a href=""https://dev.azuresynapse.net/"" rel=""nofollow noreferrer"">https://dev.azuresynapse.net/</a>
Create Azure APP Service Principal</p>
<p>az login --service-principal --username &quot;appid&quot; --password &quot;pasword&quot; --tenant &quot;tenantid&quot;</p>
<p>az account get-access-token --resource <a href=""https://dev.azuresynapse.net/"" rel=""nofollow noreferrer"">https://dev.azuresynapse.net/</a></p>
<p>In the Postman link example, the Pre-request-Script</p>
<pre><code>pm.test(&quot;Check for collectionVariables&quot;, function () {
let vars = ['clientId', 'clientSecret', 'tenantId', 'subscriptionId'];
vars.forEach(function (item, index, array) {
    console.log(item, index);
    pm.expect(pm.collectionVariables.get(item), item + &quot; variable not set&quot;).to.not.be.undefined;
    pm.expect(pm.collectionVariables.get(item), item + &quot; variable not set&quot;).to.not.be.empty; 
});

if (!pm.collectionVariables.get(&quot;bearerToken&quot;) || Date.now() &gt; new Date(pm.collectionVariables.get(&quot;bearerTokenExpiresOn&quot;) * 1000)) {
    pm.sendRequest({
        url: 'https://login.microsoftonline.com/' + pm.collectionVariables.get(&quot;tenantId&quot;) + '/oauth2/token',
        method: 'POST',
        header: 'Content-Type: application/x-www-form-urlencoded',
        body: {
            mode: 'urlencoded',
            urlencoded: [
                { key: &quot;grant_type&quot;, value: &quot;client_credentials&quot;, disabled: false },
                { key: &quot;client_id&quot;, value: pm.collectionVariables.get(&quot;clientId&quot;), disabled: false },
                { key: &quot;client_secret&quot;, value: pm.collectionVariables.get(&quot;clientSecret&quot;), disabled: false },
                { key: &quot;resource&quot;, value: pm.collectionVariables.get(&quot;resource&quot;) || &quot;https://management.azure.com/&quot;, disabled: false }
            ]
        }
    }, function (err, res) {
        if (err) {
            console.log(err);
        } else {
            let resJson = res.json();
            pm.collectionVariables.set(&quot;bearerTokenExpiresOn&quot;, resJson.expires_on);
            pm.collectionVariables.set(&quot;bearerToken&quot;, resJson.access_token);
        }
    });
}
</code></pre>
<p>});</p>
<p>I modified the Pre-Request-Script statement:</p>
<p>The PostMan Variable: resource = dev.azuresynapse.net</p>
<pre><code>{ key: &quot;resource&quot;, value: pm.collectionVariables.get(&quot;resource&quot;) || &quot;https://dev.azuresynapse.net/&quot;, disabled: false }
</code></pre>
<p>Post Failed:</p>
<pre><code>{
&quot;code&quot;: &quot;AuthenticationFailed&quot;,
&quot;message&quot;: &quot;Token Authentication failed - IDX12741: JWT: '[PII of type 'System.String' is hidden. For more details, see https://aka.ms/IdentityModel/PII.]' must have three segments (JWS) or five segments (JWE).&quot;
</code></pre>
<p>}</p>
<p>What is the problem?  Thx</p>
","<postman><azure-data-factory><azure-powershell><azure-synapse><postman-pre-request-script>","2023-01-03 13:10:00","367","1","1","75014935","<p>I try to reproduce same thing in my environment  and got below result:</p>
<p><strong>PowerShell with Bearer Token:</strong></p>
<blockquote>
<p>Shell Script:</p>
</blockquote>
<pre><code>$applicationID = '4381xxxxxxxxxxxfa606b3edf' 
$clientSecret = '4KI8Qxxxxxxxxxxxxxxxx6sQcKw'
$tenantId = 'fb134080xxxxxxxxxxxxx3a0c218f3b0'          
$url = &quot;https://login.microsoftonline.com/$tenantId/oauth2/token&quot; 
$resource = &quot;https://graph.microsoft.com/&quot; 
$restbody = @{
        grant_type = 'client_credentials' 
        client_id = $applicationID 
        client_secret = $clientSecret 
        resource = $resource 
} 
# Get the return Auth Token 
$token = Invoke-RestMethod -Method POST -Uri $url -Body $restbody Write-Host &quot;Authenticated - token retrieved of type &quot; $($token.token_type)
$token
</code></pre>
<p><img src=""https://i.imgur.com/PzmBHdD.png"" alt=""enter image description here"" /></p>
<p><strong>Or</strong></p>
<blockquote>
<p><strong>I generated Bearer token via postman with below parameters:</strong></p>
</blockquote>
<pre><code>POST
https://login.microsoftonline.com/fb1380-e4d2-xxxxx-f3a0c218f3b0/oauth2/v2.0/token
  
client_id:4KIxxxxxx2CS2UasSu9zTXhrMNQy6sQcKw  
scope:https://management.azure.com/.default 
client_secret: 4KI8Q~7xxxxxxxxxzTXhrMNQy6sQcKw
grant_type:client_credentials
</code></pre>
<p><strong>Response:</strong></p>
<p><img src=""https://i.imgur.com/wR3HX0P.png"" alt=""enter image description here"" /></p>
"
"74991498","Variable value in the mail in script in ADF","<p>I have a Adf flow , in which, i have stored the data in a array Variable(namely 'VariOutput'), that thing is present in a If Condition Activity.</p>
<p>and in IF condition False Activity , i have given the Web activity link. the code used is</p>
<p>'''{
&quot;RunId&quot;:&quot;Dummy&quot;,
&quot;applicationName&quot;: &quot;@{pipeline().parameters.AppName}&quot;,
&quot;ErrorCode&quot;:&quot;error&quot;,
&quot;ErrorMessage&quot;:@{concat('File/Files not found-', string(variables('VariOutput')))},
}'''</p>
<p>But the webactivity is not processing the Array Variable <strong>VariOutput</strong> and throwing an error.</p>
<p><a href=""https://i.stack.imgur.com/R6b20.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R6b20.png"" alt=""enter image description here"" /></a></p>
<p>error :: error&quot;: {
&quot;code&quot;: &quot;InvalidRequestContent&quot;,
&quot;message&quot;: &quot;The request content is not valid and could not be deserialized: 'Unexpected character encountered while parsing value: F. Path 'ErrorMessage</p>
<p><strong>ignore the line number 7 in the error , as i have deleted part of the error.</strong>
<a href=""https://i.stack.imgur.com/UsNsy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UsNsy.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-synapse>","2023-01-03 09:40:41","163","0","1","74993513","<blockquote>
<p>The request content is not valid and could not be deserialized: 'Unexpected character encountered while parsing value: F. Path 'ErrorMessage</p>
</blockquote>
<ul>
<li>I tried to repro this and got the same error.</li>
</ul>
<p><img src=""https://i.imgur.com/IlrDWW8.png"" alt=""enter image description here"" /></p>
<ul>
<li>This is because of double quotes in the variable. Replace the double quotes with single quotes and pass it in the web activity.</li>
<li>Give the below expression for <strong>error message</strong>
<code>&quot;ErrorMessage&quot;:&quot;@{concat('File/Files not found-', replace(string(variables('VariOutput')),'&quot;',''''))}&quot;</code></li>
</ul>
<p><img src=""https://i.imgur.com/jibBZha.png"" alt=""enter image description here"" /></p>
<ul>
<li>When the pipeline is debugged, it is executed successfully, and mail is sent.</li>
</ul>
<p><img src=""https://i.imgur.com/qKCtvSP.png"" alt=""enter image description here"" /></p>
"
"74989778","In Azure ADF trying to copy data from ADLS to SQLDB, however, few of the JSON array data at the sink side is missing","<p>I am trying to copy data from ADLS to SQL-DB, however in source I have JSON file <a href=""https://i.stack.imgur.com/I4HHM.png"" rel=""nofollow noreferrer"">refer link for JSON file that need to be copied</a>
However after debugging successfully, the data is not fully copied <a href=""https://i.stack.imgur.com/wVuI9.png"" rel=""nofollow noreferrer"">refer link for the reference image</a> - please guide</p>
<p>I tried to copy JSON array data using copy activity from ADLS to SQL-DB, and was expecting to get complete data copied into SQL-DB from ADLS</p>
","<json><azure><azure-sql-database><azure-data-factory>","2023-01-03 06:37:15","77","0","1","75000848","<p>Datasize variation while copying data is acceptable since data gets compressed according to source and sink dataset type. If there is really data truncation happening at the sink side, it might be because of the length limitations for a nvarchar() field i.e. 4000 characters. Check :<a href=""https://learn.microsoft.com/en-us/sql/t-sql/data-types/nchar-and-nvarchar-transact-sql?view=sql-server-ver16#nvarchar---n--max--"" rel=""nofollow noreferrer"">nvarchar [ ( n | max ) ]</a></p>
"
"74984178","Azure Data Flows isolate-find substrings form text field","<p>I am trying to isolate several substrings from a specific column of a parquet file that contains text (string). The substrings are all in an array and I want to keep only those rows that contain one or more of these substrings - words, while I keep a new column with the substrings that where found at the text.</p>
<p>I have currently used the following transformations:</p>
<ol>
<li>source: which is the parquet file I use</li>
<li>derived column: where I create a new column (words) which contains an array of the words-substrings that are contained in the text, by using the following expression
<strong>intersect(split(text_column, ' '), ['array','of','words'])</strong></li>
<li>filter: where I want to filter the derived column that was created at the previous transformation and exclude those rows that are either Null or contain an empty array</li>
<li>sink</li>
</ol>
<p>I have currently stuck to the 3rd transformation where I cannot filter and discard those rows that the 2 arrays do not intersect. I think that when intersect doesn't find any common element it returns an empty string array which I have not find the right condition that filters it out.
I have tried:
<strong>1. not(isNull(words))
2. word != array('')
3. not(isNull(word[1]))</strong>
but none of them worked.</p>
<p>Any suggestions regarding the whole process or the filtering of the empty string array will be perfect.
Thank you in advance.</p>
<p>I was expecting to get back only the rows that contain at least one of the substrings, but I get all the rows regardless if they contain one of the substrings.</p>
","<azure><data-manipulation><azure-synapse><azure-data-factory>","2023-01-02 15:26:40","71","0","1","74991059","<p>You can check the size of the array and remove the rows with array size=0. In filter transformation, filter on  <code>size(words)!=0</code>.</p>
<ul>
<li><p>I repro'd this with sample inputs.  <img src=""https://i.imgur.com/zdTRXPV.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Derived column transformation with same expression is given.
<code>intersect(split(text_column, ' '), ['array','of','words'])</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/lDUJapZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then in Filter transformation, condition is given as filter on <code>size(words)!=0</code></li>
</ul>
<p><img src=""https://i.imgur.com/nsmNiDQ.png"" alt=""enter image description here"" /></p>
<p>By this way, we can remove the empty array.</p>
<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#size"" rel=""nofollow noreferrer"">MS document</a> on size expression.</p>
"
"74982694","How to make a parameterized Azure Data Factory production ready","<p>I have a parameterized ADF pipeline that is feeded data from a metadata table that is stored in SQL db.
One of the columns in the metadata table is 'IsActiveFlag.' Whilst developing the pipeline I used this parameter as condition to filter out records in the metadata table where IsActiveFlag = N.</p>
<p>As I am preparing to deploy to production I am trying to find a way I can improve my pipeline so that I won't be dependent on the IsActiveFlag filter. Ideally I would be able to  update the IsActiveFlag some other dynamic way instead of having to change the values manually and then deploy to production based on user demand.</p>
<p>Does anyone have any tips or recommendations on how I can improve my pipeline?</p>
<p>Thanks in advance!</p>
","<deployment><azure-data-factory><production><parameterization>","2023-01-02 12:56:49","61","0","1","75000817","<p>Having 'IsActiveFlag' is a good way to filter out the tablenames you don't want to ingest. If you would perform delete the table entries which you don't want to ingest in , it would be kind of hard delete which is not recommended. Better to go with IsActiveFlag .</p>
"
"74982308","How to make sure that cancelling an Azure Data Factory pipeline terminates the SQL Server stored procedure that it is running?","<p>I have a number of ADF pipelines that execute an Azure SQL Database stored procedure. I noticed that cancelling an ADF pipeline run (using the 'Cancel Recursive' button in the monitoring pane) does not stop the execution of the Azure SQL Database procedure.
ADF takes a couple of seconds to process a cancellation request, after which it displays the pipeline run as 'Cancelled'. The stored procedure however keeps on running, as is evident by the log rows that continue to be written by the procedure.</p>
<p>How can I make sure that cancelling an ADF pipeline effectively also stops the stored procedure that it is running, as one would expect? Stopping a stored procedure that was started by a pipeline now requires me to kill the relevant process on the server, or to stop and restart the SQL Server database altogether.</p>
<p>I am using the latest versions of Azure SQL Database and Azure Data Factory.</p>
<p>The definition of the pipeline looks like this:</p>
<pre><code>{
    &quot;name&quot;: &quot;some_pipeline_name&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;some_activity_name&quot;,
                &quot;description&quot;: &quot;&quot;,
                &quot;type&quot;: &quot;SqlServerStoredProcedure&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;storedProcedureName&quot;: &quot;some_procedure_name&quot;,
                    &quot;storedProcedureParameters&quot;: {
                        &quot;fqn_pattern&quot;: {
                            &quot;value&quot;: &quot;some_value&quot;,
                            &quot;type&quot;: &quot;String&quot;
                        }
                    }
                },
                &quot;linkedServiceName&quot;: {
                    &quot;referenceName&quot;: &quot;name_of_some_linked_service_pointing_to_an_azure_sql_database&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                }
            }
        ],
        &quot;concurrency&quot;: 1,
        &quot;folder&quot;: {
            &quot;name&quot;: &quot;some_folder_name&quot;
        },
        &quot;annotations&quot;: [],
        &quot;lastPublishTime&quot;: &quot;2022-07-31T19:56:30Z&quot;
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
","<sql-server><stored-procedures><azure-sql-database><azure-data-factory>","2023-01-02 12:13:42","351","0","1","75001679","<p><strong>There is no direct way to kill the stored procedure if pipeline run is cancelled.</strong></p>
<p>you can do following workaround:</p>
<p>First, I ran <code>Stored procedure</code> activity in <strong>Pipeline 1</strong> to run the stored procedure.</p>
<p><img src=""https://i.imgur.com/gMokmD7.png"" alt=""enter image description here"" /></p>
<p>Then I created another Pipeline <strong>Pipeline2</strong> with <code>execute pipeline</code> invoking pipeline 1 and <em><strong><code>script activity</code> for the killing the stored procedure if pipeline 1 is cancelled</strong></em>.</p>
<p><img src=""https://i.imgur.com/0VxnGyl.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/3fedZhP.png"" alt=""enter image description here"" /></p>
<p><strong>Execution:</strong></p>
<p>when we are triggering pipeline 2 it will run pipeline 1 also if we cancel the pipeline run It will consider it as failed and will execute the script and stop the stored procedure.</p>
<p><img src=""https://i.imgur.com/STAUveo.png"" alt=""enter image description here"" /></p>
"
"74981694","data factor alter row iif condition error: data flow expression should return boolean","<p>I have provided below if condition in update if block. I am getting error: data flow expression should return Boolean.
iif(Column_8 == 'AutomÃ¡tica','Automatic','Manual')</p>
<p>I tried updating rows present in a column based on a condition.</p>
","<azure-data-factory>","2023-01-02 11:11:18","91","1","1","74990029","<ul>
<li><p>You have to specify only condition in the alter row update if i.e., there has to be a Boolean value for these fields. The expression that you have used returns a string value and hence the error.</p>
</li>
<li><p>I have taken the following data in my table:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/nRaaURD.png"" alt=""enter image description here"" /></p>
<ul>
<li>If the data you want to update is your dataflow source, then use derived column transformation with the same <code>iff</code> condition.</li>
</ul>
<pre><code>iif(col1=='AutomÃ¡tica','Automatic','Manual')
</code></pre>
<p><img src=""https://i.imgur.com/mz7aZBZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>If this table is your sink and you want to update the column values based on the condition, then try using pre-SQL scripts/ post-SQL scripts.</li>
<li><code>Update if</code> will update the entire sink row based on the source row data (after checking the condition given in alter row). It would not update from any foreign value (as you have tried). So, using the following post SQL scripts to update the values as per requirement.</li>
</ul>
<pre><code>Update demo set col1='Manual' where col1!= 'AutomÃ¡tica';
Update demo set col1='Automatic' where col1='AutomÃ¡tica';
</code></pre>
<p><img src=""https://i.imgur.com/RmqhSTN.png"" alt=""enter image description here"" /></p>
<ul>
<li>The final data in table would be as shown below (Id with value 4 and 5 are inserted first and then post SQL script will be applied).</li>
</ul>
<p><img src=""https://i.imgur.com/Y2jOuRr.png"" alt=""enter image description here"" /></p>
"
"74979313","How to compare output of two get metadata","<p>I am quite new in adf and I have the following condition.
I have a source and a destination.
I want to build a logic where if file is prent in target it should not copy the file from source else it should copy the source file.
The source and sink are different so with two getmetadata I am trying to get the files with childite.
With a set variable activity I am passing the output of get metadatas for comparison.
I am failing at the logic stage.
Please let me know if any other better approach is possible</p>
<p>Logic:If file is present in destination it should not copy file from source else it should copy</p>
","<azure><azure-data-factory>","2023-01-02 05:47:50","77","0","1","74979325","<p>You can use Filter activity</p>
<p>In items : @activity('files from source').output.childItems
Conditions: @not(contains(activity('files from target').output.childItems,item()))</p>
<p>similar thread: <a href=""https://stackoverflow.com/questions/73833488/adf-how-to-compare-two-get-metadata-activity-results-and-skip-if-the-same-name-e"">ADF-how to compare two get metadata activity results and skip if the same name exists</a></p>
"
"74978209","Copy Data from REST API to Azure SQL DB with Data Factory","<p>Beginner here, I want to copy some stock data from a REST API into my Azure SQL database. I've set up a pipeline with a Look-up activity to get the needed strings (with stock symbols) and use them in <code>For Each</code> to call the API and copy the data to SQL database.</p>
<p><strong>Problem:</strong> the copy activity works for some stock symbols, but not for all.</p>
<p><a href=""https://i.stack.imgur.com/AZBdV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AZBdV.png"" alt=""Pipeline Activities"" /></a></p>
<p>My Look-Up concatenates the stock symbols into a comma-separated string, for concatenating the string with the API call URL.</p>
<p>Here is the concatenate statement and the final output of the pipeline:</p>
<pre><code>SELECT STRING_AGG(Symbol, ',') AS Symbollist
FROM [dbo].[Tab_Symbols]
GROUP BY GroupOfTen;
</code></pre>
<ul>
<li>green symbols --&gt; appear in SQL database</li>
<li>red symbols --&gt; <strong>do not</strong> appear in SQL database</li>
</ul>
<p><a href=""https://i.stack.imgur.com/oH1uV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oH1uV.png"" alt=""enter image description here"" /></a></p>
<p>Why are the first and last symbols always missing?</p>
<p>I've tested the API call with the strings and I receive all stock symbols data in JSON format as it should. Maybe the problems is my mapping or sink SQL table?</p>
<p>Thank you! Fabian</p>
<p>Here are screenshots of my copy activity:</p>
<p>REST dataset settings and dynamic URL string:</p>
<p><a href=""https://i.stack.imgur.com/IkBSX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IkBSX.png"" alt=""enter image description here"" /></a></p>
<p>For Each settings:</p>
<p><a href=""https://i.stack.imgur.com/2tQmg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2tQmg.png"" alt=""enter image description here"" /></a></p>
<p>Source settings and use of dynamic string (use for each item):</p>
<p><a href=""https://i.stack.imgur.com/sIcPL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sIcPL.png"" alt=""enter image description here"" /></a></p>
<p>Sink settings:</p>
<p><a href=""https://i.stack.imgur.com/sHVJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sHVJS.png"" alt=""enter image description here"" /></a></p>
<p>Mapping:</p>
<p><a href=""https://i.stack.imgur.com/agLdr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/agLdr.png"" alt=""enter image description here"" /></a></p>
","<azure-sql-database><azure-data-factory><azure-rest-api>","2023-01-02 00:27:49","163","0","1","75010184","<p>I solved it. In the source settings of my copy activity the column reference '.Symbollist' of the lookup-output was missing.</p>
<pre><code>@item().Symbollist
</code></pre>
"
"74974871","Azure Synapse Create Pipeline Rest API","<p>I am calling Azure Synapse Pipeline Create Pipeline REST API with a Databricks Notebook JSON activity.</p>
<p>Error message in the Synapse Pipeline screen: Could not load resource 'PLDB2'. Please ensure no mistakes in the JSON and that referenced resources exist. Status: Cannot read properties of undefined (reading 'toLowerCase'), Possible reason: Cannot read properties of undefined (reading 'toLowerCase')</p>
<p>PostMan: <a href=""https://mysynapseworkspace.dev.azuresynapse.net/pipelines/PLDB2?api-version=2020-12-01"" rel=""nofollow noreferrer"">https://mysynapseworkspace.dev.azuresynapse.net/pipelines/PLDB2?api-version=2020-12-01</a></p>
<p>Links:
<a href=""https://learn.microsoft.com/en-us/rest/api/synapse/data-plane/pipeline/create-or-update-pipeline?tabs=HTTP"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/synapse/data-plane/pipeline/create-or-update-pipeline?tabs=HTTP</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-notebook?context=%2Fazure%2Fsynapse-analytics%2Fcontext%2Fcontext&amp;tabs=synapse-analytics"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-notebook?context=%2Fazure%2Fsynapse-analytics%2Fcontext%2Fcontext&amp;tabs=synapse-analytics</a></p>
<p>Create Pipeline JSON in Postman Body</p>
<pre><code>{
&quot;name&quot;: &quot;PLDB2&quot;,
&quot;properties&quot;: {
    &quot;description&quot;: &quot;Databricks Notebook&quot;,
    &quot;activities&quot;: [
        {
            &quot;activity&quot;: {
                &quot;name&quot;: &quot;MyActivity&quot;,
                &quot;description&quot;: &quot;MyActivity description&quot;,
                &quot;type&quot;: &quot;DatabricksNotebook&quot;,
                &quot;linkedServiceName&quot;: {
                    &quot;referenceName&quot;: &quot;LS_Databricks&quot;,  
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                },
                &quot;typeProperties&quot;: {            
                    &quot;notebookPath&quot;: &quot;/Users/abc@myemail.com/Test_Job_1&quot;
                }
            }
        }
    ]
}
</code></pre>
<p>}</p>
<p>Synape Pipeline item is created with above error message:</p>
<pre><code>{
&quot;id&quot;: &quot;/subscriptions/mysubscription/resourceGroups/myresourcegroup/providers/Microsoft.Synapse/workspaces/mysynapseworkspace/pipelines/PLDB2&quot;,
&quot;name&quot;: &quot;PLDB2&quot;,
&quot;type&quot;: &quot;Microsoft.Synapse/workspaces/pipelines&quot;,
&quot;etag&quot;: &quot;9400d55f-0000-0100-0000-63b180e40000&quot;,
&quot;properties&quot;: {
    &quot;description&quot;: &quot;Databricks Notebook&quot;,
    &quot;activities&quot;: [
        {
            &quot;activity&quot;: {
                &quot;name&quot;: &quot;MyActivity&quot;,
                &quot;description&quot;: &quot;MyActivity description&quot;,
                &quot;type&quot;: &quot;DatabricksNotebook&quot;,
                &quot;linkedServiceName&quot;: {
                    &quot;referenceName&quot;: &quot;LS_Databricks_Personify&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                },
                &quot;typeProperties&quot;: {
                    &quot;notebookPath&quot;: &quot;/Users/nlaw@quaerainsights.com/Test_Job_1&quot;
                }
            }
        }
    ],
    &quot;lastPublishTime&quot;: &quot;2023-01-01T12:47:32Z&quot;
}
</code></pre>
<p>}</p>
<p>In Azure Synapse,  I can manually create and run a pipeline calling the Databricks &quot;Test_Job_1&quot; notebook described in the JSON.</p>
<p>What is the problem?  Thx for help.</p>
","<azure><azure-databricks><azure-synapse><azure-data-factory>","2023-01-01 13:27:39","275","1","1","75021134","<p>I found the problem. In the JSON, delete these 2 lines: &quot;activity&quot;: { and its closing bracket &quot;}&quot;</p>
<pre><code>{
&quot;name&quot;: &quot;Correct JSON&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;MyActivity&quot;,
            &quot;description&quot;: &quot;MyActivity description&quot;,
            &quot;type&quot;: &quot;DatabricksNotebook&quot;,
            &quot;linkedServiceName&quot;: {
                &quot;referenceName&quot;: &quot;LS_Databricks&quot;,
                &quot;type&quot;: &quot;LinkedServiceReference&quot;
            },
            &quot;typeProperties&quot;: {
                &quot;notebookPath&quot;: &quot;/Users/abc@myemail.com/Test_Job_1&quot;
            }
        }
    ]
}
</code></pre>
<p>}</p>
"
"74967032","Requiremet of Error logging mail(mail should contain all the missing file details). in the ADF flow","<p>the reqirement is simple , i have a folder having 4 txt files(1.txt,2.txt,3.txt,4.txt) . the Flow is controlled by a parameter called <strong>all or some</strong> which is of string type.
<a href=""https://i.stack.imgur.com/J0AWt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J0AWt.png"" alt=""This is the adf pipeline, with the activities"" /></a></p>
<p>If i select all in the parameter, all 4 file should be processed. the requirement starts here &gt;&gt;</p>
<ol>
<li><p>IF any file is missing from the folder(for ex <strong>2.txt and 3.txt</strong> is not present and i selected <strong>ALL</strong> in the parameter) , i need a mail saying file is 2.txt and 3.txt is missing.</p>
</li>
<li><p>If i select some in the parameter, for ex <strong>1.txt and 4.txt</strong> and if any of the file is missing 1.txt and 4.txt is missing(for example 1.txt is missing) , i need a mail with the missing file name(i.e 1.txt in our case).</p>
</li>
</ol>
","<azure-synapse><azure-data-factory>","2022-12-31 01:32:37","116","0","1","74979336","<blockquote>
<p>capture missing file details in one variable</p>
</blockquote>
<p>I tried to repro this capturing missing files using azure data factory. Below is the approach.</p>
<ul>
<li><p>Take a parameter of <strong>array</strong> type in the pipeline. During runtime, you can give the list of file names in a folder to be processed in this array parameter.
<img src=""https://i.imgur.com/3JsNf8j.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Take a <strong>get metadata</strong> activity and add a dataset in the activity. Click <strong>+New</strong> in the field list and select <strong>child items</strong> as an argument.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/RDLhp0Z.png"" alt=""enter image description here"" /></p>
<ul>
<li>Take a filter activity and give the array parameter value in items and write the condition to filter the missing files in condition box</li>
</ul>
<p><strong>item:</strong>
<code>@pipeline().parameters.AllorSome</code></p>
<p><strong>condition:</strong>
<code>@not(contains(string(activity('Get Metadata1').output.childItems),item()))</code></p>
<ul>
<li><p>I tried to run this pipeline. During run time, four file names are given to the array parameter.
<img src=""https://i.imgur.com/SfUe3VE.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Get Metadata activity output has three file names.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/vHbAgnz.png"" alt=""enter image description here"" /></p>
<ul>
<li>Parameter has 4 filenames and Get meta data activity has 3 filenames. Missing file names are to be filtered out.</li>
<li><strong>Output of filter activity:</strong></li>
</ul>
<p><img src=""https://i.imgur.com/6WFLBFR.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Use this output and send it in email.</p>
</li>
<li><p>Refer the  <strong>MS document</strong>  on  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-send-email"" rel=""nofollow noreferrer"">How to send email - Azure Data Factory &amp; Azure Synapse | Microsoft Learn</a>  for sending email.</p>
</li>
</ul>
"
"74965312","Create a dictionary object in Azure Data Factory","<p>I am trying to build a dictionary in Azure Data Factory using the object data type by converting the output of a Lookup activity and running a FOR EACH loop to build it. The append activity works only for arrays. I am wondering if this is even possible.</p>
<p><a href=""https://i.stack.imgur.com/zGOXk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zGOXk.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/om8AK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/om8AK.png"" alt=""enter image description here"" /></a></p>
<p><strong>From Lookup output:</strong></p>
<pre><code>&quot;Value&quot;:[
        {
            &quot;name&quot;: &quot;key1&quot;,
            &quot;value&quot;: &quot;value1&quot;
        },
        {
            &quot;name&quot;: &quot;key2&quot;,
            &quot;value&quot;: &quot;value2&quot;
        }
        ]
</code></pre>
<p><strong>I need to create the following object to use it as a dictionary:</strong></p>
<pre><code>{
  &quot;values&quot;: {
    &quot;key1&quot;: &quot;value1&quot;,
    &quot;key2&quot;: &quot;value2&quot;
  }
}
</code></pre>
","<azure><azure-data-factory>","2022-12-30 19:51:10","158","1","1","75115702","<p>Here are the steps. Store the lookup output in json file in datalake.</p>
<ol>
<li>Point to the same in source transformation in dataflow.</li>
<li>Add pivot transformation and select 'name' as the pivot key and <code>first(value)</code> as the pivot column expression .</li>
<li>Click on map drifted option in data preview tab of pivot transformation.</li>
<li>Use derived column transformation to create a column called 'values' and create sub columns underneath with expression as <code>@(key1=key1,      key2=key2)</code></li>
<li>Then , use select transformation to remove the pivoted columns 'key1' and 'key2' and only keep 'values' column.</li>
<li>Use Sink transformation and load the data to a json file.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/mguEp.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mguEp.gif"" alt=""enter image description here"" /></a></p>
"
"74962170","How to disable the mapping between two activity in Azure Data Factory","<p><a href=""https://i.stack.imgur.com/FOW3R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FOW3R.png"" alt=""Filter Emp File from Metadata activity"" /></a></p>
<p>There are 4 Files on my Folder name Company Filter1 will filter file starting from Emp, Filter2 will filter file starting from Payroll.
By mistake i connect filter2 activity with Get Metadata1 activity. Can you please tell me how to disconnect them</p>
","<azure><azure-data-factory>","2022-12-30 13:39:50","36","0","1","74963632","<p>Select that link and click delete from key board. That would delete the link between them.</p>
<p>Another way would be right click on that link :</p>
<p><a href=""https://i.stack.imgur.com/YEL4v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YEL4v.png"" alt=""enter image description here"" /></a></p>
"
"74956891","Bicep ADF LinkedService","<p>I'm having a heck of a time trying to deploy a simple Azure BlobFS linked service into an ADF using Bicep (which I have only really started to learn).</p>
<p>The bicep I have thus far is:</p>
<pre><code>//---Data Factory
resource datafactory 'Microsoft.DataFactory/factories@2018-06-01' = {
  name: adf_name
  location: loc_name
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
    globalParameters: {}
    publicNetworkAccess: 'Enabled'
  }
}

//--- Data Factory Linked Service
resource adls_linked_service 'Microsoft.DataFactory/factories/linkedservices@2018-06-01' = {
  name: 'ls_adf_to_adls'
  parent: datafactory
  properties: {
    annotations: []
    connectVia: {
      parameters: {}
      referenceName: 'AutoResolveIntegrationRuntime'
      type: 'IntegrationRuntimeReference'
    }
    description: 'linked_service_for_adls'
    parameters: {}
    type: 'AzureBlobFS'
    typeProperties: {
      accountKey: datafactory.identity.principalId
      azureCloudType: 'AzurePublic'
      credential: {
        referenceName: 'string'
        type: 'CredentialReference'
      }
      servicePrincipalCredentialType: 'SecureString'
      servicePrincipalId: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
      servicePrincipalKey: {
        type: 'SecureString'
        value: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
      }
      tenant: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
      url: bicepstorage.properties.primaryEndpoints.blob
    }
  }
}
</code></pre>
<p>The ADF resource deploys fine by itself as does the ADLS (symbolic name is: bicepstorage). The issue is when I added the linkedservice resource block. I get:</p>
<pre><code>New-AzResourceGroupDeployment: /home/vsts/work/1/s/psh/deploy_main.ps1:12
Line |
  12 |  New-AzResourceGroupDeployment `
     |  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | 22:46:27 - The deployment 'main' failed with error(s). Showing 1 out of
     | 1 error(s). Status Message: Input is malformed. Reason: Could not get
     | integration runtime details for AutoResolveIntegrationRuntime
     | (Code:InputIsMalformedDetailed)  CorrelationId:
     | f77ef878-5314-46ea-9de6-65807845a104
</code></pre>
<p>The only integration runtime in the ADF is the 'AutoResolveIntegrationRuntime'. When I inspect it in the portal it's green, running and healthy.</p>
<p>I'm using task: AzurePowerShell@5 on ubuntu-latest in ADF, but I get the same error when I try to deploy the template directly from vscode.</p>
<p>I'm out of ideas and would really appreciate some assistance. I found the documentation for the 'connectVia' block (actually all the documentation on bicep linked services!) to be really confusing; if anyone could tell me exactly what is supposed to go in there, I'd really appreciate it.</p>
<p>Thanks.</p>
","<azure><azure-data-factory><azure-bicep><linked-service>","2022-12-29 23:23:28","231","0","1","74958615","<p>As mentioned in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-azure-integration-runtime?tabs=data-factory#default-azure-ir"" rel=""nofollow noreferrer"">this documentation</a>, If you want to create a linked service to adls(blobfs) with default Azure IR (autoresolveintegrationruntime) then you can remove the <code>ConnectionVia</code> property in linked service block in your bicep template.</p>
<p><a href=""https://i.stack.imgur.com/WaU9q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WaU9q.png"" alt=""enter image description here"" /></a></p>
<p>To test this I have created a bicep template which will deploy adlsgen2 storage account, data factory and a linked service to it using the service principal based authentication.</p>
<p><strong>Here is the sample template for your reference</strong>:</p>

<pre class=""lang-html prettyprint-override""><code>param location string='westus'

//---Data Factory
resource storage 'Microsoft.Storage/storageAccounts@2022-09-01'={
  name:'&lt;storageAccountName&gt;'
  location:location
  kind:'StorageV2'
  sku:{
    name:'Standard_GRS'
  }
  properties:{
    accessTier:'Hot'
    supportsHttpsTrafficOnly:true
    isHnsEnabled:true
  }
}
resource datafactory 'Microsoft.DataFactory/factories@2018-06-01' = {
  name: '&lt;AdfName&gt;'
  location: location
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
    globalParameters: {}
    publicNetworkAccess: 'Enabled'
  }
}
//--- Data Factory Linked Service
resource adls_linked_service 'Microsoft.DataFactory/factories/linkedservices@2018-06-01' = {
  name: '&lt;linkedserviceName&gt;'
  parent: datafactory
  properties: {
    annotations: []
    description: 'linked_service_for_adls'
    parameters: {}
    type: 'AzureBlobFS'
    typeProperties: {
      url: storage.properties.primaryEndpoints.dfs
      //encryptedCredential:storage.listKeys(storage.id).keys[0].value
      servicePrincipalCredential: {
        type: 'SecureString'
        value: '&lt;serviceprincipalKey&gt;'
      }
      servicePrincipalId:'&lt;serviceprincipalappId&gt;'
      servicePrincipalCredentialType:'ServicePrincipalKey'
      azureCloudType:'AzurePublic'
      servicePrincipalKey: {
        type: 'SecureString'
        value: '&lt;serviceprincipalKey&gt;'
      }
      tenant: '&lt;tenantId&gt;'      
    }
  }
}
</code></pre>
"
"74950296","How to apply pagination rule when next url is not present","<p>How to apply pagination rule when web page gives me data as below</p>
<p>{
data:[],
paging:
{
page:0,
size:100,
total_count:1190
}
}</p>
<p>baseurl?page=0&amp;size=100&amp;respose_type=json</p>
<p>baseurl?page=1&amp;size=100&amp;respose_type=json</p>
<p>tried few solutions by dividing total_count/size to get the pages but missed few records.Is there any option in Query parameter ?</p>
","<azure-data-factory><azure-devops-rest-api><jquery-pagination>","2022-12-29 11:04:22","166","0","1","74959145","<p><strong>AFAIK</strong>, You can use the <strong>Range option</strong> in the pagination.</p>
<p>Create a variable in the relative URL of the REST API and use that in pagination rules.</p>
<p><strong>My sample:</strong></p>
<p><strong>Relative URL:</strong> <code>users?take=100&amp;skip={offest}</code></p>
<p><img src=""https://i.imgur.com/dlwrQW2.png"" alt=""enter image description here"" /></p>
<p>In Pagination rules, Give the Absolute URI and give the variable <code>{offset}</code> in pagination like below based on your requirement of page size. Here it will take 100 records and offset increases by 100 till 1190.</p>
<p><code>AbsoluteUrl</code>  <code>{offset}</code>  <code>Range</code>  <code>0</code>  <code>1190</code>  <code>100</code></p>
<p><img src=""https://i.imgur.com/IyehgqC.png"" alt=""enter image description here"" /></p>
<p>For more information can go through this <a href=""https://learn.microsoft.com/en-us/answers/questions/723352/azure-data-factory-pagination.html"" rel=""nofollow noreferrer"">ans</a> by <a href=""https://learn.microsoft.com/answers/users/85053/martinjaffer-msft.html"" rel=""nofollow noreferrer"">MartinJaffer-MSFT</a></p>
"
"74944602","How to prevent timeout in Dataflow in Azure Data Factory when ingesting data into Kusto?","<p>My source is a directory in Azure data lake with roughly 70,000 JSON files.
Each file has certain properties and one of them is an array of complex <code>Node</code> elements.
In total, the JSON files have around 13 million of such <code>Node</code> elements.</p>
<p>Using Azure Data Factory and a DataFlow I want to flatten the arrays in each JSON file and insert them as rows into a Kusto DB.</p>
<p>On a &quot;General purpose&quot; integration unit I'm getting an out of memory error. On a memory optimized instance the job runs for about 40 minutes and then fails with the message below.</p>
<p>What I tried:</p>
<ul>
<li>Increase the timeout in the Kusto sink. It's set to 36,000 seconds (10 hours).</li>
<li>Increase the compute size to 8+8 memory optimized.</li>
<li>The Kusto cluster is configured to a min instance count of 2 with a max of 4 but I cannot see any scale out events.</li>
</ul>
<p>What are my options to optimize the ingestion?
If other information is required, please let me know in the comments.</p>
<blockquote>
<p>Operation on target ExportNodesToKusto failed:
{&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to
reason: at Sink 'KustoNodesSink': Timed out trying to ingest
requestId:
'a23025d4-f1e7-48cd-a5f9-a4d8dbaec64e'&quot;,&quot;Details&quot;:&quot;shaded.msdataflow.com.microsoft.kusto.spark.exceptions.TimeoutAwaitingPendingOperationException:
Timed out trying to ingest requestId:
'a23025d4-f1e7-48cd-a5f9-a4d8dbaec64e'\n\tat
shaded.msdataflow.com.microsoft.kusto.spark.datasink.KustoWriter$$anonfun$ingestRowsIntoKusto$1.apply(KustoWriter.scala:201)\n\tat
shaded.msdataflow.com.microsoft.kusto.spark.datasink.KustoWriter$$anonfun$ingestRowsIntoKusto$1.apply(KustoWriter.scala:198)\n\tat
scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat
scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat
scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat
scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat
shaded.msdataflow.com.microsoft.kusto.spark.datasink.KustoWriter$.ingestRowsIntoKusto(KustoWriter.scala:198)\n\tat
shaded.msdataflow.com.microsoft.kusto.spark.datasink.KustoWriter$.ingestToTemporaryTableByWorkers(KustoWriter.scala:247)\n\tat
shaded.msdataflow.com.microsoft.kusto.spark.datasink.KustoWriter$.ingestRowsIntoTem&quot;}</p>
</blockquote>
","<azure-data-explorer><azure-data-factory>","2022-12-28 20:12:43","110","1","1","75644158","<p>Seems like its getting bottlenecked at the sink, so resources on Kusto side is not enough. Try further adding resources on Kusto side. If it doesnt work, try creating a support ticket because those runIDs need to be looked at deeper, potentially requiring further help from Data Explorer team.</p>
"
"74940848","SAP Table Connector - How to querying SAP table using RFC from Azure Synapse","<p>We are using SAP Table Connector on Azure Synapse to extract SAP tables. However, we would like to filter the data in the copy activity.
I tried using the RFC table options using the COLUMN EQ 'SOME VALUE' pattern,</p>
<p><img src=""https://i.stack.imgur.com/IjVll.png"" alt=""Azure Synapse config field exemple"" /></p>
<p>This worked, but we would like to apply more filters like &quot;AND&quot; and &quot;OR&quot;, like this &quot;COLUMN EQ 'SOME VALUE' AND COLUMN1 EQ 'SOME VALUE' &quot;, I don't know if this is possible, or if there are better ways to do this type of filter.</p>
<p>How can we overcome this issue?</p>
<p>Thanks for listening.</p>
<p>I tried with AND, &amp;&amp;, space, comma, but none worked, I think this might not be possible</p>
","<azure-data-factory><azure-synapse><saprfc>","2022-12-28 13:27:49","199","0","1","74996434","<p>I feel like it should be possible to AND conditionals, but so far everything indicates it is not supported by default.  A custom function module would be able to support it.</p>
"
"74930946","How to flatten a nested JSON array in Azure Data Factory and output the original JSON object as string?","<p>My input (simplified) is coming from many JSON files structured like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
  Type : &quot;Root&quot;,
  Id: &quot;R1&quot;,
  Nested : [
    { Type : &quot;NestedType1&quot;, Id: &quot;N1&quot;, SharedAttribute: 1 },
    { Type : &quot;NestedType1&quot;, Id: &quot;N2&quot;, SharedAttribute: 2 },
    { Type : &quot;NestedType2&quot;, Id: &quot;N3&quot;, SharedAttribute: 3, NestedType2SpecificAttribute = &quot;foo&quot; }
  ]
}
</code></pre>
<p>The important bit is that the nested elements share certain attributes (eg <code>SharedAttribute</code>), but they can have various other attributes (eg <code>NestedType2SpecificAttribute</code>). I cannot capture all attributes in the input schema as they are changing over time.</p>
<p>I want the nested array to be transformed so that it outputs a Kusto table with all common/shared attributes and an additional column with a string representing the nested array's items' JSON.</p>
<p>I'm using a DataFlow to read these JSON files from Data Lake.
To extract the array, I added a &quot;Flatten&quot; formatter and selected to unroll by &quot;Nested&quot; and made the unroll root the same. This gives me the expected data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Type</th>
<th style=""text-align: center;"">Id</th>
<th style=""text-align: right;"">SharedAttribute</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N1</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N2</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType2</td>
<td style=""text-align: center;"">N3</td>
<td style=""text-align: right;"">3</td>
</tr>
</tbody>
</table>
</div>
<p>But I have no idea how to add an additional column called &quot;RawJson&quot; which should contain the source element of the unrolled array, essentially <code>toString(currentItem)</code> to produce a result like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Type</th>
<th style=""text-align: center;"">Id</th>
<th style=""text-align: right;"">SharedAttribute</th>
<th>RawJson</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N1</td>
<td style=""text-align: right;"">1</td>
<td>{ Type : &quot;NestedType1&quot;, Id: &quot;N1&quot;, SharedAttribute: 1 }</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N2</td>
<td style=""text-align: right;"">2</td>
<td>{ Type : &quot;NestedType1&quot;, Id: &quot;N2&quot;, SharedAttribute: 2 }</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType2</td>
<td style=""text-align: center;"">N3</td>
<td style=""text-align: right;"">3</td>
<td>{ Type : &quot;NestedType2&quot;, Id: &quot;N3&quot;, SharedAttribute: 3, NestedType2SpecificAttribute = &quot;foo&quot; }</td>
</tr>
</tbody>
</table>
</div>","<azure-data-factory>","2022-12-27 15:05:01","568","0","2","74935259","<p>In the Flatten transformation's &quot;Input Columns&quot; section, you can include the original array. You should be able to choose the array name that you set to unroll by and you can then rename it to &quot;RawJson&quot; in the Input Columns using the Name As property.</p>
<p>In my example, &quot;coordinates&quot; is the array that I unrolled as well as generated it as part of the output.</p>
<p><a href=""https://i.stack.imgur.com/sBwLN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sBwLN.png"" alt=""enter image description here"" /></a></p>
"
"74930946","How to flatten a nested JSON array in Azure Data Factory and output the original JSON object as string?","<p>My input (simplified) is coming from many JSON files structured like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
  Type : &quot;Root&quot;,
  Id: &quot;R1&quot;,
  Nested : [
    { Type : &quot;NestedType1&quot;, Id: &quot;N1&quot;, SharedAttribute: 1 },
    { Type : &quot;NestedType1&quot;, Id: &quot;N2&quot;, SharedAttribute: 2 },
    { Type : &quot;NestedType2&quot;, Id: &quot;N3&quot;, SharedAttribute: 3, NestedType2SpecificAttribute = &quot;foo&quot; }
  ]
}
</code></pre>
<p>The important bit is that the nested elements share certain attributes (eg <code>SharedAttribute</code>), but they can have various other attributes (eg <code>NestedType2SpecificAttribute</code>). I cannot capture all attributes in the input schema as they are changing over time.</p>
<p>I want the nested array to be transformed so that it outputs a Kusto table with all common/shared attributes and an additional column with a string representing the nested array's items' JSON.</p>
<p>I'm using a DataFlow to read these JSON files from Data Lake.
To extract the array, I added a &quot;Flatten&quot; formatter and selected to unroll by &quot;Nested&quot; and made the unroll root the same. This gives me the expected data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Type</th>
<th style=""text-align: center;"">Id</th>
<th style=""text-align: right;"">SharedAttribute</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N1</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N2</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType2</td>
<td style=""text-align: center;"">N3</td>
<td style=""text-align: right;"">3</td>
</tr>
</tbody>
</table>
</div>
<p>But I have no idea how to add an additional column called &quot;RawJson&quot; which should contain the source element of the unrolled array, essentially <code>toString(currentItem)</code> to produce a result like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Type</th>
<th style=""text-align: center;"">Id</th>
<th style=""text-align: right;"">SharedAttribute</th>
<th>RawJson</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N1</td>
<td style=""text-align: right;"">1</td>
<td>{ Type : &quot;NestedType1&quot;, Id: &quot;N1&quot;, SharedAttribute: 1 }</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType1</td>
<td style=""text-align: center;"">N2</td>
<td style=""text-align: right;"">2</td>
<td>{ Type : &quot;NestedType1&quot;, Id: &quot;N2&quot;, SharedAttribute: 2 }</td>
</tr>
<tr>
<td style=""text-align: left;"">NestedType2</td>
<td style=""text-align: center;"">N3</td>
<td style=""text-align: right;"">3</td>
<td>{ Type : &quot;NestedType2&quot;, Id: &quot;N3&quot;, SharedAttribute: 3, NestedType2SpecificAttribute = &quot;foo&quot; }</td>
</tr>
</tbody>
</table>
</div>","<azure-data-factory>","2022-12-27 15:05:01","568","0","2","74936058","<p>Adding to <strong>@Mark Kromer MSFT</strong>, after flatten transformation, you will get the array of objects in the output.</p>
<blockquote>
<p>additional column with a string representing the nested array's items'
JSON.</p>
</blockquote>
<p>Then use <strong>derived column transformation</strong> to get the output as String.</p>
<p><strong>Flatten:</strong></p>
<p><img src=""https://i.imgur.com/3FFz6By.png"" alt=""enter image description here"" /></p>
<p><strong>Flatten Output as array of objects:</strong></p>
<p><img src=""https://i.imgur.com/yOIg6FG.png"" alt=""enter image description here"" /></p>
<p><strong>Derived column:</strong></p>
<p><img src=""https://i.imgur.com/Hy1VAx4.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/LEsGeT4.png"" alt=""enter image description here"" /></p>
"
"74927249","How to remove last 2 positions in split and get remaining first value.?","<p>Let's say string is a variable file name like few examples below:</p>
<ol>
<li>file1_name_cr_001.csv</li>
<li>file2_name1_name2.nn.123.456_updt_000.csv</li>
<li>filename_2012.444.1234_utc_del_004.csv</li>
</ol>
<p>The length of last 8 string values will always remain fixed i.e. (_001.csv,_000.csv,_004.csv). We need to only extract values = <strong>cr, updt, del</strong></p>
<p>How can we get the value as single value before _cr,_updt,_del.?
any suggetions.?</p>
<p>output should get like this:
file1_name/cr/001
file2_name1_name2.nn.123.456/updt/000
filename_2012.444.1234_utc/del/004</p>
","<azure-data-factory>","2022-12-27 08:44:01","87","0","1","74927783","<p>I have reproduced the above and got the below results.</p>
<p>First, I took a sample file name in set variable.</p>
<p><img src=""https://i.imgur.com/EVRIr5Y.png"" alt=""enter image description here"" /></p>
<p>Then, I got the string from start to <strong>length-8</strong>.</p>
<pre><code>@substring(variables('sample'),0,sub(length(variables('sample')),8))
</code></pre>
<p><img src=""https://i.imgur.com/x05ScoQ.png"" alt=""enter image description here"" /></p>
<p><strong>For end folder:</strong></p>
<pre><code>@replace(split(substring(variables('sample'),sub(length(variables('sample')),8), 8),'.')[0],'_','')
</code></pre>
<p><img src=""https://i.imgur.com/cq8tU7b.png"" alt=""enter image description here"" /></p>
<p><strong>For Start folder:</strong></p>
<pre><code>@substring(variables('before_8'), 0, lastIndexOf(variables('before_8'), '_'))
</code></pre>
<p><img src=""https://i.imgur.com/HzoQi8c.png"" alt=""enter image description here"" /></p>
<p><strong>For middle folder:</strong></p>
<pre><code>@split(variables('before_8'), '_')[sub(length(split(variables('before_8'), '_')), 1)]
</code></pre>
<p><img src=""https://i.imgur.com/yq4SSnG.png"" alt=""enter image description here"" /></p>
<p><strong>Result folder structure:</strong></p>
<pre><code>@concat(variables('start'),'/',variables('middle'),'/',variables('end'))
</code></pre>
<p><img src=""https://i.imgur.com/ejDD7bt.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/jQikQHY.png"" alt=""enter image description here"" /></p>
<p><strong>Give this variable in copy activity source folder path and it will generate the folder structure for you.</strong></p>
<p>For multiple file names, first store all file names in an array then use a ForEach and inside ForEach do the same operations as above.</p>
"
"74926445","How to connect to POWER BI DATSET in Azure data factory","<p>We have a requirement where we have to fetch data from Power BI dataset and transform the data and then produce the data again in Power BI Workspace so that I may consume that in PBI.</p>
<p>-&gt; Is there any way I may connect my Power BI Dataset in ADF.</p>
<p>-&gt; Once the data is fetched can I use Power Query to work on dataset in ADF.</p>
<p>-&gt; Where to store that data: in ADLS or in Azure BLOB or may I save this data in SQL directly?</p>
<p>-&gt; Once I have transformed the data can i publish my gold data in Power BI Workspace?</p>
","<powerbi><azure-data-factory>","2022-12-27 06:53:25","744","0","1","74926497","<p>There is PowerBI REST API to execute queries:
Datasets – Execute Queries</p>
<p>you can access the power bi dataset via managed identity or service principal authentication.</p>
<p>The below blog :
<a href=""https://datasharkx.wordpress.com/2022/11/03/copy-data-from-power-bi-through-azure-data-factory-synapse-pipeline-via-managed-identity-authentication-part-4/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2022/11/03/copy-data-from-power-bi-through-azure-data-factory-synapse-pipeline-via-managed-identity-authentication-part-4/</a></p>
<p>You can leverage Dataflows to do the transformation</p>
<p>The query w.r.t 3rd aspect really depends on your architecture.You can preserve it anywhere</p>
"
"74906757","Azure Data factory - How to convert data time to some specific format in pipeline expression builder?","<p>Input:
&quot;firstRow&quot;: {
&quot;flag&quot;: false,
&quot;WindowStart&quot;: &quot;2022-12-23:18:00:000Z&quot;</p>
<p>I'm using the below format,
formatDateTime(activity('Lookup1').output.firstRow.windowStart,'dd-MM-yyyy hh:mm')</p>
<p>but its throwing an error as</p>
<p>Operation on target Copy_staging_to_raw failed: In function 'formatDateTime', the value provided for date time string '2022-12-23:18:00:000Z' was not valid. The datetime string must match ISO 8601 format</p>
<p>Could you please help me what was the mistake on this?</p>
","<azure-data-factory><expressionbuilder>","2022-12-24 09:38:47","306","0","2","74916858","<blockquote>
<p>Operation on target Copy_staging_to_raw failed: In function
'formatDateTime', the value provided for date time string
'2022-12-23:18:00:000Z' was not valid. The datetime string must match
ISO 8601 format</p>
</blockquote>
<p>The error message shows your provided date time string '2022-12-23:18:00:000Z'is not valid for ISO 8601 format.</p>
<p>the ISO 8601 timestamp normally looks like this:</p>
<pre><code>String dateString = &quot;2019-09-26T07:58:30.996+0200&quot;
</code></pre>
<p>This string is now structured in the date format YYYY-MM-DD. The delimiter for the time is “T” and the time format is hh:mm:ss plus the UTC suffix .sssz. The complete format is therefore: YYY-MM-DD “T” hh:mm:ss.SSSZ.</p>
<p>You could use this <a href=""https://www.timestamp-converter.com/"" rel=""nofollow noreferrer"">converter tool</a> for the convert.</p>
"
"74906757","Azure Data factory - How to convert data time to some specific format in pipeline expression builder?","<p>Input:
&quot;firstRow&quot;: {
&quot;flag&quot;: false,
&quot;WindowStart&quot;: &quot;2022-12-23:18:00:000Z&quot;</p>
<p>I'm using the below format,
formatDateTime(activity('Lookup1').output.firstRow.windowStart,'dd-MM-yyyy hh:mm')</p>
<p>but its throwing an error as</p>
<p>Operation on target Copy_staging_to_raw failed: In function 'formatDateTime', the value provided for date time string '2022-12-23:18:00:000Z' was not valid. The datetime string must match ISO 8601 format</p>
<p>Could you please help me what was the mistake on this?</p>
","<azure-data-factory><expressionbuilder>","2022-12-24 09:38:47","306","0","2","74980041","<p>You can use the following dynamic content instead, to get the desired result.</p>
<ul>
<li>The following is the sample of lookup output same as yours:</li>
</ul>
<p><img src=""https://i.imgur.com/GfhXz60.png"" alt=""enter image description here"" /></p>
<p>Using string and collection functions, I have converted the lookup output data to ISO 8601 format string and converted it to <code>dd-MM-yyyy hh:mm</code> format:</p>
<pre><code>@formatDateTime(concat(first(array(split(activity('Lookup1').output.firstRow.windowStart,':'))),'T',join(take(skip(array(split(activity('Lookup1').output.firstRow.windowStart,':')),1),2),':')),'dd-MM-yyyy hh:mm')
</code></pre>
<p><img src=""https://i.imgur.com/0EjfLh3.png"" alt=""enter image description here"" /></p>
"
"74903136","Azure Data Factory Cosmos DB sql api 'DateTimeFromParts' is not a recognized built-in function name","<p>I am using Copy Activity in my Datafactory(V2) to query Cosmos DB (NO SQL/SQLAPI). I have a where clause to build datetime from parts using DateTimeFromParts datetime function. THis query works fine when I execute it on the Cosmos DB data explorer query window. But when i use the same query from my copy activity I get the following error:</p>
<p>&quot;message&quot;:&quot;'DateTimeFromParts' is not a recognized built-in function name.&quot;}]}
ActivityId: ac322e36-73b2-4d54-a840-6a55e456e15e, documentdb-dotnet-sdk/2.5.1 Host/64-bit</p>
<p>I am trying convert a string attribute which is like this '20221231', this translates to  Dec 31,2022, to a date to compare it with current date, i use the DateTimeFromParts to build the date, is there another way to convert  this '20221231' to a valid date</p>
<p>Select * from c where
DateTimeFromParts(StringToNumber(LEFT(c.userDate, 4)), StringToNumber(SUBSTRING(c.userDate,4, 2)), StringToNumber(RIGHT(c.userDate, 2))) &lt; GetCurrentDateTime()</p>
<p>I suspect the error might be because the documentdb-dotnet-sdk might be an old version. Is there way to specify which sdk to use in the activity?</p>
","<azure-cosmosdb><azure-data-factory><azure-cosmosdb-sqlapi>","2022-12-23 18:58:39","160","0","2","74906760","<p>I tried to repro this and got the same error.
Instead of changing the format of <code>userDate</code> column using <code>DateTimeFromParts</code> function, try changing the <strong>GetCurrentDateTime()</strong> function to <code>userDate</code> column format.</p>
<p><strong>Workaround query:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>SELECT  *  FROM c
where c.userDate &lt;  
replace(left(GetCurrentDateTime(),10),'-','')
</code></pre>
<p><strong>Input data</strong></p>
<pre class=""lang-json prettyprint-override""><code>[
    {
        &quot;id&quot;: &quot;1&quot;,
        &quot;userDate&quot;: &quot;20221231&quot;
    },
    {
        &quot;id&quot;: &quot;2&quot;,
        &quot;userDate&quot;: &quot;20211231&quot;,
    }
]
</code></pre>
<p><strong>Output data</strong></p>
<pre class=""lang-json prettyprint-override""><code>[
    {
        &quot;id&quot;: &quot;2&quot;,
        &quot;userDate&quot;: &quot;20211231&quot;
    }
]
</code></pre>
<p><img src=""https://i.imgur.com/zN2t2Ea.png"" alt=""enter image description here"" /></p>
"
"74903136","Azure Data Factory Cosmos DB sql api 'DateTimeFromParts' is not a recognized built-in function name","<p>I am using Copy Activity in my Datafactory(V2) to query Cosmos DB (NO SQL/SQLAPI). I have a where clause to build datetime from parts using DateTimeFromParts datetime function. THis query works fine when I execute it on the Cosmos DB data explorer query window. But when i use the same query from my copy activity I get the following error:</p>
<p>&quot;message&quot;:&quot;'DateTimeFromParts' is not a recognized built-in function name.&quot;}]}
ActivityId: ac322e36-73b2-4d54-a840-6a55e456e15e, documentdb-dotnet-sdk/2.5.1 Host/64-bit</p>
<p>I am trying convert a string attribute which is like this '20221231', this translates to  Dec 31,2022, to a date to compare it with current date, i use the DateTimeFromParts to build the date, is there another way to convert  this '20221231' to a valid date</p>
<p>Select * from c where
DateTimeFromParts(StringToNumber(LEFT(c.userDate, 4)), StringToNumber(SUBSTRING(c.userDate,4, 2)), StringToNumber(RIGHT(c.userDate, 2))) &lt; GetCurrentDateTime()</p>
<p>I suspect the error might be because the documentdb-dotnet-sdk might be an old version. Is there way to specify which sdk to use in the activity?</p>
","<azure-cosmosdb><azure-data-factory><azure-cosmosdb-sqlapi>","2022-12-23 18:58:39","160","0","2","75086163","<p>Apologies for the slow reply here. Holidays slowed getting an answer for this.</p>
<p>There is a workaround that allows you to use the SDK v3 which would then allows you to access the DateTimeFromParts() system function which was released in .NET SDK v.3.13.0.</p>
<p>Option 1: Use AAD authentication (i.e Service Principal or System or User Managed Identity) for the Linked Service object in ADF to Cosmos DB. This will automatically pick up the .NET SDK v3.</p>
<p>Option 2: Modify the linked service template. First, click on Manage in ADF designer, next click on Linked Services, then select the connection and click the {} to open the JSON template, you can then modify and set <code>useV3</code> to true. Here is an example.</p>
<pre><code>{
  &quot;name&quot;: &quot;&lt;CosmosDbV3&gt;&quot;,
  &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
  &quot;properties&quot;: {
    &quot;annotations&quot;: [],
    &quot;type&quot;: &quot;CosmosDb&quot;,
    &quot;typeProperties&quot;: {
      &quot;useV3&quot;: true,
      &quot;accountEndpoint&quot;: &quot;&lt;https://sample.documents.azure.com:443/&gt;&quot;,
      &quot;database&quot;: &quot;&lt;db&gt;&quot;,
      &quot;accountKey&quot;: {
        &quot;type&quot;: &quot;SecureString&quot;,
        &quot;value&quot;: &quot;&lt;account key&gt;&quot;
      }
    }
  }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/pXze1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pXze1.png"" alt=""enter image description here"" /></a></p>
"
"74901607","Synapse Pipeline - How do I specify identity for Pipeline Run? (SP/UAMI etc)","<p>I am working with Synpase Spark Pools in a controlled corporate environment. I have limited permission to query AAD but I can create UAMIs and assign them to Resources.</p>
<p>When I access my Synpase workspace I can create a Spark Job Definition to read some data from ADLS. Looking at the Apache Spark Applications list under the Monitor tab I can see that these jobs use my identity (tim.davies@work.com) as the 'Submitter', and since I have given myself rx access to the data store these succeed.</p>
<p>Now if i create a Pipeline, and configure it to run my Spark Job Definition, it fails with an authorisation error. Going back to Apache Spark Applications list under Monitor I see that my Pipeline has a different Identity used as Submitter, which would explain why it is not authorised to access the data.</p>
<p>Firstly, I'm not sure which identity is now being used as Submitter, I don't recognise the UUID as either my Synapse Workspace SAMI or UAMI, (but I can't query AAD for more info).</p>
<p>However in general it occurs to me that I would probably like to be able to assign explicit UAMIs for my Pipelines to run under. Is this possible? Or is there a different model for managing this?</p>
","<azure><azure-data-factory><azure-synapse>","2022-12-23 15:58:22","223","1","2","74998146","<p>As I understand the ask here is to know how to read the data from ADLS from a spark job . Since you have the access the the ADLS , so works fine . I thnk you will have to set up the permission for the Synapse Workspace on the ADLS and it should work fine .</p>
"
"74901607","Synapse Pipeline - How do I specify identity for Pipeline Run? (SP/UAMI etc)","<p>I am working with Synpase Spark Pools in a controlled corporate environment. I have limited permission to query AAD but I can create UAMIs and assign them to Resources.</p>
<p>When I access my Synpase workspace I can create a Spark Job Definition to read some data from ADLS. Looking at the Apache Spark Applications list under the Monitor tab I can see that these jobs use my identity (tim.davies@work.com) as the 'Submitter', and since I have given myself rx access to the data store these succeed.</p>
<p>Now if i create a Pipeline, and configure it to run my Spark Job Definition, it fails with an authorisation error. Going back to Apache Spark Applications list under Monitor I see that my Pipeline has a different Identity used as Submitter, which would explain why it is not authorised to access the data.</p>
<p>Firstly, I'm not sure which identity is now being used as Submitter, I don't recognise the UUID as either my Synapse Workspace SAMI or UAMI, (but I can't query AAD for more info).</p>
<p>However in general it occurs to me that I would probably like to be able to assign explicit UAMIs for my Pipelines to run under. Is this possible? Or is there a different model for managing this?</p>
","<azure><azure-data-factory><azure-synapse>","2022-12-23 15:58:22","223","1","2","75323294","<p>Slightly slow update to this but I've arrived at something of an answer in terms of understanding, if not quite a solution. Will be useful to share here for anyone following or looking into the same questions.</p>
<p>First of all, when accessing the Synapse Workspace through the Portal/UI the actionable Identity that is used by Notebooks or a Standalone 'Apache Spark Job Definition', is the Identity of the User that is logged in, (via '<a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/active-directory-authentication#azure-ad-pass-through-in-azure-synapse-analytics"" rel=""nofollow noreferrer"">AAD Passthrough</a>'). This is great for user experience, especially in Notebooks, and you just need to make sure that you as the individual have personal access to any data sources you use. In some cases, where your user identity doesn't have this access, you could make use of a <strong>Workspace Linked Service</strong> identity instead, but not always! (keep reading)</p>
<p>Once you switch to using Pipelines however, the Identity used is the <a href=""https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/understand-the-identity-used-during-component-execution-in/ba-p/2567863"" rel=""nofollow noreferrer"">System Assigned Managed Identity (SAMI) of the workspace</a>, which is created and assigned at resource creation. This is ok, but it is important to understand the granularity, ie. it is the Workspace that has access to resources, not individual Pipelines. Therefore if you want to run Pipelines with different levels of access, you will need to deploy them to segregated Synapse Workspaces, (with distinct SAMIs).</p>
<p>One aside on this is the identity of the '<strong>Submitter</strong>' that I mentioned in my original question, which is visible under the monitor tab of the Synapse workspace for all Apache Spark applications. When running as the user (eg. Notebooks), this submitter ID is my AAD username, which is straightforward. However when running as a pipeline the Submitter ID is '<strong>ee20d9e7-6295-4240-ba3f-c3784616c565</strong>', and I mean literally this same UUID for <strong>EVERYONE</strong>. It turns out this is the id of ADF as an enterprise application. Not very useful, compared to putting the Workspace SAMI in here for example, but that's what it is in case anyone else is drifting down that rabbit hole!</p>
<p>You can create and assign an additional User Assigned Managed Identity (UAMI) to the Workspace, but this will be not be used by an executing pipeline. The UAMI can be used by a Workspace Linked Service, but that has some of its own limitations (mentioned below). Also my experience is that a UAMI assigned at workspace creation will not be correctly 'associated' to the Workspace until I manually create a 2nd UAMI in the portal. I haven't gone deep into this as turns out UAMIs are no good to me but seems like a straightforward bug.</p>
<p>Now my specific use case is for running Apache Spark Applications in Synapse Pipelines, and the straightforward way to make this work is to make sure the Workspace SAMI has access to required resources and you're good to go. If you just want to make it work then do this and stop here, but if you want to look a little deeper carry on...</p>
<p>The suggestion in some of the <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-secure-credentials-with-tokenlibrary?pivots=programming-language-python#adls-gen2-storage-with-linked-services"" rel=""nofollow noreferrer"">Microsoft documentation</a> is that you should be able to use a Workspace Linked Service within a Spark Application in order to get access to Resources. However this doesn't work, I've been discussing the same with Microsoft and they have confirmed the same and are investigating. So at this point it's worth noting the date (<strong>02/02/2023</strong> - handily unambiguous for American readers ;-)), because the issue may later be resolved. But right now your only option in your Spark code is to fall back on the user/workspace identities.</p>
<p>Just a thought on why this matters, it is not really for segregation since any resource running in the Workspace can access any Linked Service. It is really more a question of Identity and Resource Management, ie. it would be better to separate the Identities being used and assigned to Resources for access from the Resources themselves. In most cases we'd rather do this with groups that individual identities, and if the management processes are long-winded (mine are) then I'd rather not have to repeat them every time I create a resource.</p>
<p>Anyway that's enough for now, will update if this changes while I'm still paying attention...</p>
"
"74899101","Attempting to flatten JSON in Azure Data Factory","<p>I have a JSON file in the following format:</p>
<p><code>{ &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Doe&quot;, &quot;userid&quot;: 12345, “profile”: { “annoying_field_name_10”: {“field_id”: 15, “field_name”: “Gender”, “value”: “Female”,               “applicable”: 1 }, “annoying_field_name_11”: {“field_id”: 16, “field_name”: “Interests”, “value”: “Baking”, “applicable”: 1 } } }</code></p>
<p>In this file i have 5000 users, with around 150 profile fields each, the &quot;annoying_field_name...&quot; is completely unique for every user and field name combination.
I would like to parse and flatten the file so my results look like this:</p>
<p>table</p>
<p><img src=""https://i.stack.imgur.com/XCzPY.png"" alt=""1"" /></p>
<p>I have not been able to figure out how to use a wildcard or dynamics expressions in either parsing or flatten and unroll functions in ADF.</p>
<p>Is anyone able to advise further on this please?</p>
<p>I have tried using both the parse and flatten function in ADF, I have attempted to use the dynamic expressions in these but was unable to get this to run successfully. I have followed this question <a href=""https://stackoverflow.com/questions/58090889/how-to-flatten-multiple-child-nodes-using-jsonnodereference-in-azure-data-factor"">how to flatten multiple child nodes using jsonNodeReference in azure data factory</a> and guide :<a href=""https://adatis.co.uk/converting-json-with-nested-arrays-into-csv-in-azure-logic-apps-by-using-array-variable/"" rel=""nofollow noreferrer"">https://adatis.co.uk/converting-json-with-nested-arrays-into-csv-in-azure-logic-apps-by-using-array-variable/</a> but the changing object name &quot;annoying_field_name...&quot; is proving restrictive.</p>
","<arrays><json><azure-data-factory>","2022-12-23 11:25:57","155","0","2","74919259","<p>Flattening the nested JSON where key names are different is difficult to achieve in mapping dataflow as in dataflow, any transformation works on the defined source schema .</p>
<p>Either you can go for writing custom code in C#,JAVA and use custom activity in ADF or you can change the JSON to defined format like below and apply flatten transformation directly on top of it:</p>
<pre><code>{
&quot;first_name&quot;:&quot;Jane&quot;,
&quot;last_name&quot;:&quot;Doe&quot;,
&quot;userid&quot;:12345,
&quot;profile&quot;:{
   &quot;annoying_field_name&quot;:[
      {
         &quot;field_id&quot;:15,
         &quot;field_name&quot;:&quot;Gender&quot;,
         &quot;value&quot;:&quot;Female&quot;,
         &quot;applicable&quot;:1
      },
      {
         &quot;field_id&quot;:16,
         &quot;field_name&quot;:&quot;Interests&quot;,
         &quot;value&quot;:&quot;Baking&quot;,
         &quot;applicable&quot;:1
      }
   ]
}
}
</code></pre>
"
"74899101","Attempting to flatten JSON in Azure Data Factory","<p>I have a JSON file in the following format:</p>
<p><code>{ &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Doe&quot;, &quot;userid&quot;: 12345, “profile”: { “annoying_field_name_10”: {“field_id”: 15, “field_name”: “Gender”, “value”: “Female”,               “applicable”: 1 }, “annoying_field_name_11”: {“field_id”: 16, “field_name”: “Interests”, “value”: “Baking”, “applicable”: 1 } } }</code></p>
<p>In this file i have 5000 users, with around 150 profile fields each, the &quot;annoying_field_name...&quot; is completely unique for every user and field name combination.
I would like to parse and flatten the file so my results look like this:</p>
<p>table</p>
<p><img src=""https://i.stack.imgur.com/XCzPY.png"" alt=""1"" /></p>
<p>I have not been able to figure out how to use a wildcard or dynamics expressions in either parsing or flatten and unroll functions in ADF.</p>
<p>Is anyone able to advise further on this please?</p>
<p>I have tried using both the parse and flatten function in ADF, I have attempted to use the dynamic expressions in these but was unable to get this to run successfully. I have followed this question <a href=""https://stackoverflow.com/questions/58090889/how-to-flatten-multiple-child-nodes-using-jsonnodereference-in-azure-data-factor"">how to flatten multiple child nodes using jsonNodeReference in azure data factory</a> and guide :<a href=""https://adatis.co.uk/converting-json-with-nested-arrays-into-csv-in-azure-logic-apps-by-using-array-variable/"" rel=""nofollow noreferrer"">https://adatis.co.uk/converting-json-with-nested-arrays-into-csv-in-azure-logic-apps-by-using-array-variable/</a> but the changing object name &quot;annoying_field_name...&quot; is proving restrictive.</p>
","<arrays><json><azure-data-factory>","2022-12-23 11:25:57","155","0","2","74919934","<p>I agree with <strong>@AnnuKumari-MSFT</strong> as in ADF we can only flatten the array of objects JSON.</p>
<p>I have tried to reproduce this, but Your JSON includes different keys so convert into array of objects like above.</p>
<p>Then, apart from flatten transformation in dataflow, you can also try copy activity to flatten.</p>
<p>Give the source and sink JSONs and Go to <strong>Mapping</strong>.</p>
<p><img src=""https://i.imgur.com/30glUFQ.png"" alt=""enter image description here"" /></p>
<p>Give the respective names to the columns. you can delete the unwanted column like above.</p>
<p><strong>Desired Result:</strong></p>
<p><img src=""https://i.imgur.com/upP9eZG.png"" alt=""enter image description here"" /></p>
"
"74898350","ADF Track Pipeline Changes","<p>Is that possible to see that changes and whom made the changes to the ADF pipeline? In QA some pipeline changes are made and the code has been published. IS that possible to track?</p>
","<azure-data-factory>","2022-12-23 09:58:28","133","0","1","74918949","<p>To track the changes in Azure Data Factory, configure git repository with GitHub or Azure-repos.</p>
<ul>
<li>This will help to track the changes done in Pipeline.</li>
<li>Also, you can revert the changes done in code and bring back the previous versions of code.</li>
<li>You cannot track who made changes directly in ADF. Instead, you can configure the code-review process and allow few members to make changes in code via git and only limited members can publish those changes to data factory.</li>
</ul>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control"" rel=""nofollow noreferrer"">Microsoft document</a> on Source control in Azure Data Factory</p>
"
"74897070","How to use timestamp type parameters set in pipeline with timestamp type in data flow","<p>I can't send the question due to some mysterious error, so I'll share a screenshot of the question.</p>
<p>Can anyone help me solve this?</p>
<p><a href=""https://i.stack.imgur.com/dc58O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dc58O.png"" alt=""ss of the error"" /></a></p>
","<azure><azure-data-factory>","2022-12-23 07:17:18","118","0","1","74918537","<p>I have reproduced the above and got same error when the Expression checkbox in checked.</p>
<p><img src=""https://i.imgur.com/U9NfF0V.png"" alt=""enter image description here"" /></p>
<p><strong>Remove the Expression checkbox check</strong> in dataflow pipeline assignment and pass it as a string. Now it won't give the error.</p>
<p><img src=""https://i.imgur.com/4Li5MHJ.png"" alt=""enter image description here"" /></p>
<p>It will take the Dataflow parameter like this.</p>
<p><img src=""https://i.imgur.com/MbdMvr2.png"" alt=""enter image description here"" /></p>
<p>Also, along with the Date time string pass the format in <code>toTimestamp()</code> function to avoid null values.</p>
<p><strong>This is my sample input data:</strong></p>
<p><img src=""https://i.imgur.com/SXfCuT6.png"" alt=""enter image description here"" /></p>
<p><strong>sample filter condition:</strong></p>
<pre><code>toTimestamp(start_date,'yyyy-MM-dd\'T\'HH:mm:ss')
</code></pre>
<p><img src=""https://i.imgur.com/ynXG3Cz.png"" alt=""enter image description here"" /></p>
<p><strong>Filtered Result:</strong></p>
<p><img src=""https://i.imgur.com/K7MPZMi.png"" alt=""enter image description here"" /></p>
"
"74896637","Count of orders - ADF- aggregation","<p>How to convert this CSV file input to the given output in ADLS using ADF?</p>
<h4>Input data:</h4>
<pre><code>order_id,city,country
L10,Sydney,Australia
L11,Annecy,France
L12,Montceau,France
L13,Paris,France
L14,Montceau,Canada
L15,Ste-Hyacinthe,Canada
</code></pre>
<h4>Output data:</h4>
<pre><code>COUNTRY,CITY,TOTAL_Order
Australia,Sydney,1
Australia,Total,1
Canada,Montréal,1
Canada,Ste-Hyacinthe,1
Canada,Total,2
France,Annecy,1
France,Montceau,1
France,Paris,1
France,Total,3
Total,Total,6
</code></pre>
<p>I want to find the count of order ids city wise and country wise using Data Flow. This is similar to roll-up aggregation.</p>
","<azure><azure-data-factory>","2022-12-23 06:07:22","74","0","1","74897558","<ul>
<li><p>Take three aggregate transforms in dataflow to do this. First is to calculate the count of <code>orderid</code> for every country and city combination. Second aggregate transform is to calculate the count of <code>orderid</code> for every country. Third aggregate transform is to calculate the count <code>orderid</code> for the full table. Below are the detailed steps.</p>
</li>
<li><p>Same input data is taken as source.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/ZIUuwSq.png"" alt=""enter image description here"" />
img:1 source data preview</p>
<ul>
<li>Create two new additional branches by clicking + symbol near to Source transformation and click new branch.</li>
<li>In each branch add aggregate transformation.</li>
<li>Aggregate transformation1 settings:</li>
</ul>
<pre><code> group by : country, city
 aggregates: total_order=count(order_id)
</code></pre>
<p><img src=""https://i.imgur.com/oYghSQd.png"" alt=""enter image description here"" /></p>
<p>img:2 aggregate transform1 data preview</p>
<ul>
<li>Aggregate transorm2 settings:</li>
</ul>
<pre><code>group by: country
aggregates: total_order=count(order_id)
</code></pre>
<p><img src=""https://i.imgur.com/8CbaP0L.png"" alt=""enter image description here"" />
img:3  aggregate transform 2 data preview.</p>
<ul>
<li>Aggregate transorm3 settings: No column in group by.</li>
</ul>
<pre><code>group by: 
aggregates: total_order=count(order_id)
</code></pre>
<p><img src=""https://i.imgur.com/8CbaP0L.png"" alt=""enter image description here"" />
img:4 aggregate transform3 data preview.</p>
<ul>
<li>Next step is to union all these tables. Since all of these are not in the same structure, Add derived columns transformation to aggregate2 and aggregate3 and create columns with empty string.</li>
<li>Join aggregate1,derived1 and derived2 transformations data using Union transformation.</li>
</ul>
<p><img src=""https://i.imgur.com/CtOsbQZ.png"" alt=""enter image description here"" />
img:5 Data preview after all transformations.</p>
<p><img src=""https://i.imgur.com/d1sxZBl.png"" alt=""enter image description here"" />
img: 6 Complete dataflow with all transformations.</p>
"
"74893607","Azure data factory merge files and set content type","<p>I am using a SQL Server query which would return the last 3 months since a customer last purchased a product. For instance, There's a customer 100 that last made a purchase in August 2022. The SQL query will return June, July, August. Which would be in the format 062022, 072022, 082022. Now I need to be able to pass these values to the Copy data activity REST api dataset Relative URL (/salemonyr/062022) in the ForEach activity. So, I'm now able to store the json document per iteration but even though I use <code>Copy behavior-&gt; Merge files</code> it runs over 3 iterations to over-write the files and not merge files with 3 months of data.</p>
<p><a href=""https://i.stack.imgur.com/O0AQV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O0AQV.png"" alt=""enter image description here"" /></a></p>
<p>Also, I want the <code>content-type</code> to be <code>application/json</code>.</p>
<p><a href=""https://i.stack.imgur.com/YgCwG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YgCwG.png"" alt=""enter image description here"" /></a></p>
<p>How would we be able to merge files and set the right content-type?</p>
","<azure-data-lake-gen2><azure-data-factory>","2022-12-22 20:34:50","105","0","1","74895906","<p>Since you are using ForEach and looping through each iteration it will generate individual files. So, first step would be to copy those 3 files (don't use merge files feature here) to an intermediate location and once all three are copied, have a subsequent copy activity outside of ForEach, which will point source to the folder which has 3 files copied earlier and sink to your desired location and use merge files option which will generate a single data file. Then have a subsequent web activity and call <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/set-blob-properties"" rel=""nofollow noreferrer"">Set Blob Properties</a> API to update the content type property of the merged file.</p>
<p><a href=""https://i.stack.imgur.com/n4RFu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n4RFu.png"" alt=""enter image description here"" /></a></p>
"
"74890368","How to use ""dynamic content"" as a filter in SAP CDC connector in Azure Data factory","<p>I'm trying to use SAP CDC linked service in my ADF pipeline to import some data. And also there is an option to filter out some data on source side using &quot;Selection&quot; part of Copy activity source configuration. This part gives possibility to select from drop-down list column and logical operator to use for filtering value. Also there is an option to replace it with entire expression as a dynamic content, but when I try add there any logical expression it fails with error:</p>
<blockquote>
<p>The value of property 'selection' is in unexpected type 'List`1'.</p>
</blockquote>
<p>So the question is - what is the syntax of that expression that can be used as a filter on SAP side?
<a href=""https://i.stack.imgur.com/833wd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/833wd.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><sap><azure-synapse><sap-bw>","2022-12-22 15:07:24","331","1","1","74947938","<ul>
<li>One way that you can check about how to give dynamic content for selection is to make a static selection first using any sample condition.</li>
<li>As soon as this selection is made, the pipeline JSON would automatically be generated. Using this, you can understand where to replace the static values with dynamic content (parameters, variables or activity outputs).</li>
<li>Look at the following example for SAP CDC source in a dataflow (same approach but different syntax). In optimize tab, I selected partition type as <code>source</code>. And then for condition I have first used a static data as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/4fDIjek.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, when I open the JSON, I can see how the condition is actually being used.</li>
</ul>
<p><img src=""https://i.imgur.com/LRtp4pH.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now let's say instead of 100, I want to use dynamic content (a parameter called <code>x</code>), then I can directly edit the above json by replacing <code>'low' -&gt; ('100')</code> to <code>'low' -&gt; ($x)</code> which would change the condition as:</li>
</ul>
<p><img src=""https://i.imgur.com/3NVu57y.png"" alt=""enter image description here"" /></p>
<ul>
<li>Similarly, you can try to change the pipeline JSON directly to understand the syntax of the how to give selection condition.</li>
</ul>
<p><strong>NOTE:</strong> Dataflow SAP CDC allows dynamic content only for value but not operator and fieldname. Check if this is also the case in Copy data pipeline activity.</p>
"
"74889444","Get Output of Copy Activity present Inside the Switch in Azure Data Factory","<p>I have a Copy Activity Inside a switch Activity. I need to pass the rowsWritten to a notebook present outside the switch activity.</p>
<p>How Will I be available to pass the output of copy activity to the notebook present outside the switch activity.</p>
<p>I have already used Pipeline Variables to achieve this. Exploring for an alternate solution without the use of Pipeline variables</p>
","<azure-data-factory>","2022-12-22 13:50:04","325","1","1","74899753","<p><em><strong>AFAIK</strong></em>, We cannot get the activity outputs which are inside Switch in the outside of it by referencing it.</p>
<p>As the ADF and we don’t know which case will be executed inside Switch, that’s why when we reference those outside it will say <code>The output of activity 'Lookup1' can't be referenced since it is either not an ancestor to the current activity or does not exist</code>.</p>
<p>Here I have used two lookups inside two Switch cases and when I get the output of one Lookuo activity outside Switch, you can see the error.</p>
<p><img src=""https://i.imgur.com/4qb5XcQ.png"" alt=""enter image description here"" /></p>
<p>Also, it will fail on Validation of the pipeline.</p>
<p><img src=""https://i.imgur.com/0f7l75h.png"" alt=""enter image description here"" /></p>
<p>As you said, <strong>Using a Pipeline variable and set variable activities inside every case and storing the activity output(rowsWritten) and using that variable outside Switch activity</strong> is the workaround for it.</p>
"
"74886821","Change string size in Azure Data Factory","<p>In my Azure Data Factory project I have created a pipeline to load data from an API source to Azure SQL. However, there is one issue that I would like to fix but cannot find a solution for. I want to cast my strings to restrain their size. Right now all my strings in SQL are (MAX), which is going to slow down my performance later on.</p>
<p>Is there a way to specify the length of my string in ADF pipelines when I map my data.</p>
<p>Right now I have tried it through mapping within the Copy Data activity in pipelines. It is here that I have not found an option for string length. Do I have to make the ETL process in Data flows in order for me to be able to change the string length?</p>
<p><a href=""https://i.stack.imgur.com/SdTpZ.png"" rel=""nofollow noreferrer"">Here you can see that I map it with string in ADF</a></p>
<p><a href=""https://i.stack.imgur.com/mNqlq.png"" rel=""nofollow noreferrer"">This is what it is in SSMS</a></p>
<p>I want to limit it so that the datatype in the server is for example nvarchar(50).</p>
","<azure><azure-data-factory>","2022-12-22 09:57:35","206","0","1","74897550","<p>It is not possible to specify the length of the string columns in copy activity. - Try to design SQL sink table with datatype as varchar (50). Otherwise, like @Joel Cochran commented, you can create stored procedure to alter the data type. Execute that stored procedure using pre-copy script in copy activity.</p>
<p><img src=""https://i.imgur.com/emmzd77.png"" alt=""enter image description here"" /></p>
"
"74884948","Azure Data Factory Getting error AccessToOnPremFileSystemDenied when trying to reach the self hosted IR disk","<p>I had been using the F: of my self hosted IR in my ADF pipelines when all of a sudden I started getting an error:</p>
<p>Error code
AccessToOnPremFileSystemDenied
Failure type
User configuration issue
Details
ErrorCode=AccessToOnPremFileSystemDenied,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Access to 'F:' is not allowed.,Source=Microsoft.DataTransfer.Common</p>
<p>This is pretty bad of Microsoft to change stuff on the fly.</p>
<p>Found this link here but nothing on stack overflow so I thought I would put something here to help others.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-troubleshoot-file-system"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-troubleshoot-file-system</a></p>
","<azure-data-factory>","2022-12-22 06:40:35","93","1","1","74884976","<p>So in the end I found out that MS introduced this as a safety feature and these were the steps to disable it:</p>
<ol>
<li>Log on to the Machine where the IR is hosted as an admin.</li>
<li>Run CMD as an admin.</li>
<li>Run cd C:\Program Files\Microsoft Integration Runtime\5.0\Shared</li>
<li>Run dmgcmd -DisableLocalFolderPathValidation</li>
</ol>
"
"74883251","Azure data factory pass activity output to a dataset","<p>I am using a SQL Server query which would return the last 3 months since a customer last purchased a product. For instance, There's a customer 100 that last made a purchase in August 2022. The SQL query will return June, July, August. Which would be in the format 062022, 072022, 082022. Now I need to be able to pass these values to the Copy data activity REST api dataset Relative URL (/salemonyr/062022) in the ForEach activity.</p>
<p><a href=""https://i.stack.imgur.com/jHMPj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jHMPj.png"" alt=""enter image description here"" /></a></p>
<p>So during the first iteration the Relative URL should be set to <code>/salemonyr/062022</code> the second would be <code>/salemonyr/072022</code> and third <code>/salemonyr/082022</code>.</p>
<p><strong>Error:</strong> <code>The expression 'length(activity('MonYear').output.value)' cannot be evaluated because property 'value' doesn't exist, available properties are 'resultSetCount, recordsAffected, resultSets, outputParameters, outputLogs, outputLogsLocation, outputTruncated, effectiveIntegrationRuntime, executionDuration, durationInQueue, billingReference</code></p>
<p><em><strong>Script activity json:</strong></em></p>
<pre><code>{
    &quot;resultSetCount&quot;: 1,
    &quot;recordsAffected&quot;: 0,
    &quot;resultSets&quot;: [
        {
            &quot;rowCount&quot;: 3,
            &quot;rows&quot;: [
                {
                    &quot;MonYear&quot;: 062022
                },
                {
                    &quot;MonYear&quot;: 072022
                },
                {
                    &quot;MonYear&quot;: 082022
                }
            ]
        }
    ],
    &quot;outputParameters&quot;: {},
    &quot;outputLogs&quot;: &quot;&quot;,
    &quot;outputLogsLocation&quot;: &quot;&quot;,
    &quot;outputTruncated&quot;: false,
    &quot;effectiveIntegrationRuntime&quot;: &quot;&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 3
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p>How would I accomplish this to read the values dynamically from the SQL query.</p>
","<azure-data-factory>","2022-12-22 01:03:13","135","0","2","74886454","<p>You can use <code>@split(item().colname,',')[0]</code> , <code>split(item().colname,',')[1]</code> and <code>split(item().colname,',')[2]</code> in the relative URL path.</p>
<p>Check the below video for details:</p>
<p><a href=""https://i.stack.imgur.com/WGBIR.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WGBIR.gif"" alt=""enter image description here"" /></a></p>
"
"74883251","Azure data factory pass activity output to a dataset","<p>I am using a SQL Server query which would return the last 3 months since a customer last purchased a product. For instance, There's a customer 100 that last made a purchase in August 2022. The SQL query will return June, July, August. Which would be in the format 062022, 072022, 082022. Now I need to be able to pass these values to the Copy data activity REST api dataset Relative URL (/salemonyr/062022) in the ForEach activity.</p>
<p><a href=""https://i.stack.imgur.com/jHMPj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jHMPj.png"" alt=""enter image description here"" /></a></p>
<p>So during the first iteration the Relative URL should be set to <code>/salemonyr/062022</code> the second would be <code>/salemonyr/072022</code> and third <code>/salemonyr/082022</code>.</p>
<p><strong>Error:</strong> <code>The expression 'length(activity('MonYear').output.value)' cannot be evaluated because property 'value' doesn't exist, available properties are 'resultSetCount, recordsAffected, resultSets, outputParameters, outputLogs, outputLogsLocation, outputTruncated, effectiveIntegrationRuntime, executionDuration, durationInQueue, billingReference</code></p>
<p><em><strong>Script activity json:</strong></em></p>
<pre><code>{
    &quot;resultSetCount&quot;: 1,
    &quot;recordsAffected&quot;: 0,
    &quot;resultSets&quot;: [
        {
            &quot;rowCount&quot;: 3,
            &quot;rows&quot;: [
                {
                    &quot;MonYear&quot;: 062022
                },
                {
                    &quot;MonYear&quot;: 072022
                },
                {
                    &quot;MonYear&quot;: 082022
                }
            ]
        }
    ],
    &quot;outputParameters&quot;: {},
    &quot;outputLogs&quot;: &quot;&quot;,
    &quot;outputLogsLocation&quot;: &quot;&quot;,
    &quot;outputTruncated&quot;: false,
    &quot;effectiveIntegrationRuntime&quot;: &quot;&quot;,
    &quot;executionDuration&quot;: 0,
    &quot;durationInQueue&quot;: {
        &quot;integrationRuntimeQueue&quot;: 3
    },
    &quot;billingReference&quot;: {
        &quot;activityType&quot;: &quot;PipelineActivity&quot;,
        &quot;billableDuration&quot;: [
            {
                &quot;meterType&quot;: &quot;&quot;,
                &quot;duration&quot;: 0.016666666666666666,
                &quot;unit&quot;: &quot;Hours&quot;
            }
        ]
    }
}
</code></pre>
<p>How would I accomplish this to read the values dynamically from the SQL query.</p>
","<azure-data-factory>","2022-12-22 01:03:13","135","0","2","74887561","<p>You can use <strong>REST</strong> Dataset parameter and use it in the <strong>Relative URL</strong>.</p>
<p><img src=""https://i.imgur.com/LBwfPEd.png"" alt=""enter image description here"" /></p>
<p>Relative URL:</p>
<p><img src=""https://i.imgur.com/PeCfIf9.png"" alt=""enter image description here"" /></p>
<p>Give lookup output to ForEach. use your query in lookup.</p>
<p><img src=""https://i.imgur.com/SJR9Zwa.png"" alt=""enter image description here"" /></p>
<p>Give this to ForEach and inside ForEach, in copy sink(REST DATASET) use the below expression for the dataset parameter.</p>
<pre><code>/salemonyr/@{item().sample_date}
</code></pre>
<p><img src=""https://i.imgur.com/0gkWXvN.png"" alt=""enter image description here"" /></p>
<p>In source, you can give your source.</p>
<p>By this, you can copy the data to the respective Relative URL.</p>
"
"74879369","How to copy huge data from oracle db to Aure blob storage using copy activity","<p>I am pretty new to azure. I want to copy data from on-premise oracle db to azure blob storage using the copy activity.</p>
<p>I am already using a single copy activity that has a SQL query, but the query return rows in millions. And it takes more than 5 hrs to copy the data</p>
<p>I read some docs suggesting doing the incremental copy. But what steps I need to follow to achieve this ?</p>
","<azure><azure-data-factory>","2022-12-21 16:59:58","178","0","1","74926759","<ul>
<li>Select a watermark column in the source oracle table. Watermark column should be a column keeps increasing whenever new rows are inserted, or rows get updated.</li>
<li>Create a watermark table and store a minimum value of watermark column. By doing this, all data will be moved to target in the first run.</li>
<li>Write a stored procedure in database to update the watermark value of watermark table. Stored Procedure should have an input parameter and watermark value should get updated with this input parameter, whenever it is run.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>CREATE  PROCEDURE usp_update_watermark 
@LastModifiedtime datetime 
AS 
 BEGIN  
 UPDATE watermarktable 
 SET [WatermarkValue] = @LastModifiedtime
  WHERE [TableName] = @TableName 
END
</code></pre>
<ul>
<li>Take two lookup activities in ADF.</li>
<li>Lookup activity1 is to read the value from watermark table.</li>
<li>Lookup activity2 is to read the maximum watermark value from the source table.</li>
<li>Copy data activity is added in sequence to Lookup activities 1 and 2. So that it will run after Both Lookup1 and 2.</li>
<li>Write a query in copy data source settings to copy data between lookup activity1 watermark value and lookup activity2 watermark value.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>select * from data_source_table where 
LastModifytime &gt; '@{activity('LookupLastWaterMarkActivity').output.firstRow.WatermarkValue}' 
and LastModifytime &lt;= '@{activity('LookupCurrentWaterMarkActivity').output.firstRow.NewWatermarkvalue}'
</code></pre>
<ul>
<li><p>Then take a execute stored procedure activity and include the stored procedure which updates watermark table.</p>
</li>
<li><p>Pass the lookup activity2 output as a parameter to Stored procedure.
<img src=""https://i.imgur.com/paywCpj.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Publish and run the pipeline.</p>
</li>
<li><p>In first run all data will be copied and watermark table gets updated with new value.</p>
</li>
<li><p>From next run, data will be copied from new watermark value only.</p>
</li>
</ul>
<p><strong>Reference:</strong> Microsoft document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-powershell"" rel=""nofollow noreferrer"">Incrementally copy a table using PowerShell - Azure Data Factory | Microsoft Learn</a></p>
"
"74879344","Azure Release Pipeline failed to download the template file from the given path","<p>I'm creating a release pipeline and it should be picking up the ARM Templates from my built artifacts but it keeps giving the below error. I'm not sure what I'm doing wrong here</p>
<p><a href=""https://i.stack.imgur.com/mpGaN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mpGaN.png"" alt=""enter image description here"" /></a></p>
<p>The way this has been configured is:</p>
<p><strong>1. Artifact is picked up from my build pipeline:</strong>
<a href=""https://i.stack.imgur.com/x2nOY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x2nOY.png"" alt=""enter image description here"" /></a></p>
<p><strong>2. Build pipeline has successfully created the artifact:</strong>
<a href=""https://i.stack.imgur.com/Dwq7t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dwq7t.png"" alt=""[1]: https://i.stack.imgur.com/kyrLl.png
[2]: https://i.stack.imgur.com/k53JK.png"" /></a></p>
<p><strong>3. Artifacts are selected here</strong></p>
<p><a href=""https://i.stack.imgur.com/xuDSx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xuDSx.png"" alt=""enter image description here"" /></a></p>
<p>Any help would be much appreciated</p>
<p><strong>EDIT:</strong></p>
<p>Have changed my CI pipeline to use PublishBuildArtifacts but not getting an error about 2MB limit:</p>
<p><a href=""https://i.stack.imgur.com/fdejL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fdejL.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-pipelines-release-pipeline><cicd>","2022-12-21 16:58:19","116","0","2","74886237","<p>I reproduced your issue when i used the task <code>PublishPipelineArtifact@1</code> to publish aritfact in build pipeline.The same warning poped up when i selected <code>Override template parameters</code>. Then i changed the task to <code>PublishBuildArtifacts@1</code> and it worked.</p>
<p>RESULT</p>
<p><a href=""https://i.stack.imgur.com/xRJvk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xRJvk.png"" alt=""enter image description here"" /></a></p>
<p>You can refer to this doc to understand the two types of publish artifact task:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/publish-build-artifacts-v1?view=azure-pipelines&amp;viewFallbackFrom=azure-devops#usage"" rel=""nofollow noreferrer"">PublishBuildArtifacts</a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/publish-pipeline-artifact?view=azure-devops&amp;tabs=yaml"" rel=""nofollow noreferrer"">Publish Pipeline Artifacts</a></p>
<p>Here are suggestions to check:</p>
<p>1 check the type of publish artifact task in your build pipeline.</p>
<p>2 check your template that should follow Syntax and expressions in ARM templates</p>
"
"74879344","Azure Release Pipeline failed to download the template file from the given path","<p>I'm creating a release pipeline and it should be picking up the ARM Templates from my built artifacts but it keeps giving the below error. I'm not sure what I'm doing wrong here</p>
<p><a href=""https://i.stack.imgur.com/mpGaN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mpGaN.png"" alt=""enter image description here"" /></a></p>
<p>The way this has been configured is:</p>
<p><strong>1. Artifact is picked up from my build pipeline:</strong>
<a href=""https://i.stack.imgur.com/x2nOY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x2nOY.png"" alt=""enter image description here"" /></a></p>
<p><strong>2. Build pipeline has successfully created the artifact:</strong>
<a href=""https://i.stack.imgur.com/Dwq7t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dwq7t.png"" alt=""[1]: https://i.stack.imgur.com/kyrLl.png
[2]: https://i.stack.imgur.com/k53JK.png"" /></a></p>
<p><strong>3. Artifacts are selected here</strong></p>
<p><a href=""https://i.stack.imgur.com/xuDSx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xuDSx.png"" alt=""enter image description here"" /></a></p>
<p>Any help would be much appreciated</p>
<p><strong>EDIT:</strong></p>
<p>Have changed my CI pipeline to use PublishBuildArtifacts but not getting an error about 2MB limit:</p>
<p><a href=""https://i.stack.imgur.com/fdejL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fdejL.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-pipelines-release-pipeline><cicd>","2022-12-21 16:58:19","116","0","2","75058438","<p>Managed to do using PublishBuildArtifacts and linked ARM templates to overcome the 2MB issue:  learn.microsoft.com/en-us/azure/data-factory/….</p>
"
"74879157","Azure Data Factory Trigger Azure Notebook Failure","<p>I am trying to execute the notebook via azure datafactory to Azure Databricks notebook but unable to success my ADF pipeline, if I run the azure databricks notebook separately on my pyspark scripts, there is no error but if run via the ADF pipeline, i am getting below like.</p>
<blockquote>
<p>ModuleNotFoundError: No module named 'prophet'</p>
<p>ModuleNotFoundError                       Traceback (most recent call
last)  in 
6 import pandas as pd
7 import pyspark.pandas as ps
----&gt; 8 from prophet import Prophet
9 from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, DateType, IntegerType
10</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/NFL2c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NFL2c.png"" alt=""enter image description here"" /></a></p>
<p>I am not sure, if we installed everting in ADB cluster but throwing the error in ADF pipeline. i tried to restart the cluster and all the possibility. Kindly provide your advice.</p>
","<python><pyspark><azure-databricks><azure-data-factory>","2022-12-21 16:41:48","233","0","1","74980968","<p>I tried to reproduce your issue and faced similar error.</p>
<p><img src=""https://i.imgur.com/1sR80Up.png"" alt=""enter image description here"" /></p>
<p>To resolve this I Installed prophet PYPI library on cluster.</p>
<blockquote>
<p>go to your cluster &gt;&gt; libraries &gt;&gt; Install library &gt;&gt; PyPI &gt;&gt; Packge:&quot;prophet&quot; &gt;&gt; install</p>
</blockquote>
<p><img src=""https://i.imgur.com/QaL9Lu8.png"" alt=""enter image description here"" /></p>
<p>Now ADF executing Databricks notebook</p>
<p><img src=""https://i.imgur.com/tSqZRjz.png"" alt=""enter image description here"" /></p>
"
"74876657","How to user pipeline parameters to change sink dataset paths in Synapse Analytics","<p>I've a Azure Synapse Pipeline that has some copy activities with parquets sinking datasets. My pipeline that runs the copy activities has a string date parameter. How to I pass this date parameters to the sink dataset and use it in the path of the blob as dynamic expression?</p>
","<azure-pipelines><azure-synapse><azure-data-factory>","2022-12-21 13:16:49","182","0","1","74879176","<ul>
<li><p>If these multiple sink datasets are present in different activities of different pipelines, and you want to use the date <code>2022-12-01</code> for your path, then you can use global parameter.</p>
</li>
<li><p>Since global parameter is a constant that can be used across multiple pipelines, it would fit the requirement.</p>
</li>
<li><p>If these multiple sink activities are present in the same pipeline, then you can use the pipeline parameter itself.</p>
</li>
<li><p>You can use string interpolation to include the parameter value wherever required. The following is a demonstration of the same:</p>
</li>
<li><p>I passed the parameter value to dataset (through a parameter created in dataset called <code>dt</code>).</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/x6gs5xd.png"" alt=""enter image description here"" /></p>
<ul>
<li>Using the following dynamic content, I can create a folder with name as <code>filename_2022-12-01</code></li>
</ul>
<pre><code>filename_@{dataset().dt}
</code></pre>
<p><img src=""https://i.imgur.com/LaCcNAU.png"" alt=""enter image description here"" /></p>
"
"74874889","Can't trigger a wheel in azure data factory","<p>I created and tested multiple times a python wheel on my local machine and on Azure Databricks as a Job and it worked fine.</p>
<p>Now, I'm trying to create an Azure Data Factory Pipeline that triggers the wheel stored in Azure Databricks (<code>dbfs:/..</code>) everytime a new file is stored in a blob storage container.</p>
<p>The wheel takes a parameter (<code>-f</code>) and the values is new file name. I passed it to the wheel using <code>argparse</code> inside the script.py and parameters section of databricks job in the previous tests.</p>
<p>I created the pipeline and setted two parameters <code>param</code> and <code>value</code> that I want to pass to the wheel whose values are <code>-f</code> and <code>new-file.txt</code>. <a href=""https://i.stack.imgur.com/7GGry.png"" rel=""nofollow noreferrer"">See image here</a></p>
<p>Then I created a Databricks Python file in ADF workspace and paste wheel path into <code>Python file</code> section. Now I'm wondering if this is the right way to do this.</p>
<p>I passed the parametes in the way you can see in the image below and I didn't add any library as I've already attacched the wheel in the upper section (I've tried to add the wheel also as library but notthing changed). <a href=""https://i.stack.imgur.com/Bk4LS.png"" rel=""nofollow noreferrer"">See image here</a></p>
<p>I've created the trigger for blob storage and I've checked that in the trigger json file the parameters exists. Trying to trigger the pipeline I received this error: <a href=""https://i.stack.imgur.com/AL3k3.png"" rel=""nofollow noreferrer"">See image here</a></p>
<p>I checked if there are errors in code and I changed to UTF-8 the encoding as suggested in other questions of the community but notthing changes.</p>
<p>At this point, I think that I didn't trigger correctly the blob storage or the wheel can't be attached in the way I've done. I didn't add other resources in the workspace, hence I've only Databricks Python file.</p>
<p>Any advice is really appreciate,
thanks for the help!</p>
","<azure-databricks><python-wheel><azure-data-factory>","2022-12-21 10:48:27","114","0","1","74879093","<p>If I understand your goal is to launch a wheel package from a databricks python notebook using Azure data factory and calling the notebook via the activity python databricks.</p>
<p>I think the problem that you are facing would be when calling the python wheel from the notebook.</p>
<p>Here is an example that I tried to use which is close to your needs and it worked fine.</p>
<ol>
<li>I created a hello.py script and put it on the path <code>/dbfs/FileStore/jars/</code></li>
</ol>
<p>Here is the content of hello.py (just prints the provided arguments)</p>
<pre class=""lang-py prettyprint-override""><code>import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-f', help='file', type=str)
args = parser.parse_args()
print('You provided the value : ', args.f)
</code></pre>
<ol start=""2"">
<li>I created a python notebook on databricks that takes arguments and passes them to the hello.py script.<br />
This code defines the parameter that the notebook can take (which refers to the parameters you pass via Azure Data Factory while calling the activity databricks)</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>    dbutils.widgets.text(&quot;value&quot;, &quot;new_file.txt&quot;)  
    dbutils.widgets.text(&quot;param&quot;, &quot;-f&quot;)
</code></pre>
<p>This code retrieves the parameters passed to the databricks notebook</p>
<pre class=""lang-py prettyprint-override""><code>param = dbutils.widgets.get(&quot;param&quot;)
value = dbutils.widgets.get(&quot;value&quot;)
</code></pre>
<p>And finally we call the python hello.py script to execute our custom code as follows :</p>
<pre><code>!python /dbfs/FileStore/jars/hello.py $param $value
</code></pre>
<p>Pay attention to the ! at the begining.</p>
<p>Hope this helps your needs and don't forget to mark the answer :) .</p>
"
"74870557","How to filter by 3 or more criteria in azure dataflow","<p>I want to filter rows using more than 3 conditions in Azure data flow, but I get an error in the expression.
How can I resolve this?
I would like to filter the company_code to those that are colored yellow.</p>
<p><img src=""https://i.stack.imgur.com/lGGYt.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/tSj6z.png"" alt=""enter image description here"" /></p>
","<azure><azure-data-factory>","2022-12-21 01:37:08","80","0","1","74876624","<p>I reproduced the above and got the below results.</p>
<p><code>company_code=='4901</code> in this there is no single quote in your filter condition. This might be the cause of the error.</p>
<p>This is input data.</p>
<p><img src=""https://i.imgur.com/pWhr9n3.png"" alt=""enter image description here"" /></p>
<p>I have used the below filter condition.</p>
<pre><code>company_code=='7833'||company_code=='7828'||company_code=='4904'||company_code=='4901'||company_code=='7832'
</code></pre>
<p><strong>Filter transformation:</strong></p>
<p><img src=""https://i.imgur.com/7mR3ZBM.png"" alt=""enter image description here"" /></p>
<p><strong>Desired Result:</strong></p>
<p><img src=""https://i.imgur.com/rPlIZFI.png"" alt=""enter image description here"" /></p>
"
"74868788","Azure blob storage - SAS - Data Factory","<p>I was able to blob test connection and it's successful, but when I attempt to look for the storage path it shows this error. <a href=""https://i.stack.imgur.com/HG8sZ.png"" rel=""nofollow noreferrer"">screenshot</a></p>
<p>Full error:</p>
<blockquote>
<p>Failed to load
Blob operation failed for: Blob Storage on container '' and path '/' get failed with 'The remote server returned an error: (403) Forbidden.'. Possible root causes: (1). Grant service principal or managed identity appropriate permissions to do copy. For source, at least the “Storage Blob Data Reader” role. For sink, at least the “Storage Blob Data Contributor” role. For more information, see <a href=""https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#service-principal-authentication"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#service-principal-authentication</a>. (2). It's possible because some IP address ranges of Azure Data Factory are not allowed by your Azure Storage firewall settings. Azure Data Factory IP ranges please refer <a href=""https://docs.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses</a>. If you allow trusted Microsoft services to access this storage account option in firewall, you must use <a href=""https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#managed-identity"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#managed-identity</a>. For more information on Azure Storage firewalls settings, see <a href=""https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal."" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal.</a>. The remote server returned an error: (403) Forbidden.StorageExtendedMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.</p>
</blockquote>
<p>Context: I'm trying to copy data from SQL db to Snowflake and I am using Azure Data Factory for that. Since this doesn't publish, I enable the staged copy and connect blob storage.</p>
<p>I already tried to check network and it's set for all network. I'm not sure what I'm missing here because I found a youtube video that has it working but they didn't show an issue related/similar to this one. <a href=""https://www.youtube.com/watch?v=5rLbBpu1f6E"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=5rLbBpu1f6E</a>.</p>
<p>I also tried to retain empty storage path but trigger for copy data pipeline isn't successfully to.</p>
<p>Full error from trigger:</p>
<blockquote>
<p>Operation on target Copy Contacts failed: Failure happened on 'Sink' side. ErrorCode=FileForbidden,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error occurred when trying to upload a blob, detailed message: dbo.vw_Contacts.txt,Source=Microsoft.DataTransfer.ClientLibrary,''Type=Microsoft.WindowsAzure.Storage.StorageException,Message=The remote server returned an error: (403) Forbidden.,Source=Microsoft.WindowsAzure.Storage,StorageExtendedMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.</p>
</blockquote>
","<azure><azure-sql-database><snowflake-cloud-data-platform><azure-blob-storage><azure-data-factory>","2022-12-20 20:55:03","500","1","2","74872343","<p>I created Blob storage and generated SAS token for that. I created a blob storage linked service using SAS URI It created successfully.
Image for reference:</p>
<p><img src=""https://i.imgur.com/ghlOE1H.png"" alt=""enter image description here"" /></p>
<p>When I try to retrieve the path I got below error</p>
<p><img src=""https://i.imgur.com/37u6pKZ.png"" alt=""enter image description here"" /></p>
<p>I changed the networking settings of storage account by enabling enabled from all networks of storage account
Image for reference:</p>
<p><img src=""https://i.imgur.com/Ld6SGyo.png"" alt=""enter image description here"" /></p>
<p>I try to retrieve the path again in data factory. It worked successfully. I was able to retrieve the path.
Image for reference:</p>
<p><img src=""https://i.imgur.com/yOgRKZR.png"" alt=""enter image description here"" /></p>
<p>Another way is by whitelisting the IP addresses we can resolve this issue.</p>
"
"74868788","Azure blob storage - SAS - Data Factory","<p>I was able to blob test connection and it's successful, but when I attempt to look for the storage path it shows this error. <a href=""https://i.stack.imgur.com/HG8sZ.png"" rel=""nofollow noreferrer"">screenshot</a></p>
<p>Full error:</p>
<blockquote>
<p>Failed to load
Blob operation failed for: Blob Storage on container '' and path '/' get failed with 'The remote server returned an error: (403) Forbidden.'. Possible root causes: (1). Grant service principal or managed identity appropriate permissions to do copy. For source, at least the “Storage Blob Data Reader” role. For sink, at least the “Storage Blob Data Contributor” role. For more information, see <a href=""https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#service-principal-authentication"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#service-principal-authentication</a>. (2). It's possible because some IP address ranges of Azure Data Factory are not allowed by your Azure Storage firewall settings. Azure Data Factory IP ranges please refer <a href=""https://docs.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/data-factory/azure-integration-runtime-ip-addresses</a>. If you allow trusted Microsoft services to access this storage account option in firewall, you must use <a href=""https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#managed-identity"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory#managed-identity</a>. For more information on Azure Storage firewalls settings, see <a href=""https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal."" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal.</a>. The remote server returned an error: (403) Forbidden.StorageExtendedMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.</p>
</blockquote>
<p>Context: I'm trying to copy data from SQL db to Snowflake and I am using Azure Data Factory for that. Since this doesn't publish, I enable the staged copy and connect blob storage.</p>
<p>I already tried to check network and it's set for all network. I'm not sure what I'm missing here because I found a youtube video that has it working but they didn't show an issue related/similar to this one. <a href=""https://www.youtube.com/watch?v=5rLbBpu1f6E"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=5rLbBpu1f6E</a>.</p>
<p>I also tried to retain empty storage path but trigger for copy data pipeline isn't successfully to.</p>
<p>Full error from trigger:</p>
<blockquote>
<p>Operation on target Copy Contacts failed: Failure happened on 'Sink' side. ErrorCode=FileForbidden,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error occurred when trying to upload a blob, detailed message: dbo.vw_Contacts.txt,Source=Microsoft.DataTransfer.ClientLibrary,''Type=Microsoft.WindowsAzure.Storage.StorageException,Message=The remote server returned an error: (403) Forbidden.,Source=Microsoft.WindowsAzure.Storage,StorageExtendedMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.</p>
</blockquote>
","<azure><azure-sql-database><snowflake-cloud-data-platform><azure-blob-storage><azure-data-factory>","2022-12-20 20:55:03","500","1","2","74875568","<p>From the error message:</p>
<blockquote>
<p>'The remote server returned an error: (403) Forbidden.'</p>
</blockquote>
<p>It's likely the authentication method you're using doesn't have enough permissions on the blob storage to list the paths. I would recommend using the Managed Identity of the Data Factory to do this data transfer.</p>
<ol>
<li>Take the name of the Data Factory</li>
<li>Assign the Blob Data Contributor role in the context of the container or the blob storage to the ADF Managed Identity (step 1).</li>
<li>On your blob linked service inside of Data Factory, choose the managed identity authentication method.</li>
</ol>
<p>Also, if you stage your data transfer on the blob storage, you have to make sure the user can write to the blob storage, and also bulk permissions on SQL Server.</p>
"
"74866789","Pass number function in azure pipeline dinamically","<p>Trying to pass sum or multiply number to subtract from date dynamically in blue pipeline as below:</p>
<pre><code>@concat(
    'RANGE:',
    1+((1-1)*(variables('totalcount')/20)),
    ':',
    variables('totalcount'),
    ':50'
)
</code></pre>
<p><a href=""https://i.stack.imgur.com/Ad42e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ad42e.png"" alt=""enter image description here"" /></a></p>
<p>The above expression says Unrecognized expression: 1+((1-1)*(variables('totalcount')/20))</p>
","<azure-data-factory><pipeline>","2022-12-20 17:32:35","58","1","1","74867277","<p>The math functions in Data Factory are add, mul, div, etc.  You cannot use the *,+,/ literals.  Here is the converted expression</p>
<pre><code>@concat(
    'RANGE:',
    string(add(1,mul(sub(1,1),div(int(variables('totalcount')),20)))),
    ':',
    variables('totalcount'),
    ':50'
)
</code></pre>
<p>which gives the result &quot;RANGE:1:18000:50&quot;.  You did not specify the expected result so don't know if that is what you want.  The expression I changed will always result in a value of '1' because of the 1-1 part.</p>
"
"74866073","ADF pipeline - How can I check pipeline run status via c# code (pipeline triggered via a blob trigger)","<p>I'm trying to check the pipeline run status of a specific pipeline via c# code.
The pipeline is triggered via a Blob Events Trigger (Blob Created/File Uploaded via c# code) rather than an API call or DataFactoryManagementClient.Pipelines.CreateRunWithHttpMessagesAsync.</p>
<p>If the pipeline was triggered via code and I had the runId then I'd have simply captured the status from the response using DataFactoryManagementClient.PipelineRuns.GetAsync() method. But i'm trying to get the status from a pipeline triggered via file upload to a Storage container.</p>
<p>I tried to use the answer suggested on <a href=""https://stackoverflow.com/questions/49405267/how-to-get-pipeline-runid"">this Question</a> but it mentions PipelineRunFilterParameters which I can't find the namespace for.</p>
<p>Thanks in advance</p>
","<c#><automated-tests><azure-data-factory>","2022-12-20 16:29:25","153","0","1","74866280","<p>You can use datafactory API</p>
<p>First get runs on the pipeline by calling the pipeline API</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipeline-runs/query-by-factory?tabs=HTTP"" rel=""nofollow noreferrer"">Pipeline Runs - Query By Factory</a></p>
<p>Get your id from this call, you can pass filters to limit number of runs received to get your target one.</p>
<p>Then call the pipeline runs api passing your runId and you get the status of the run.</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipeline-runs/get?tabs=HTTP"" rel=""nofollow noreferrer"">Pipeline Runs - Get</a></p>
<p>Sample response</p>
<pre><code>{
  &quot;runId&quot;: &quot;2f7fdb90-5df1-4b8e-ac2f-064cfa58202b&quot;,
  &quot;pipelineName&quot;: &quot;examplePipeline&quot;,
  &quot;parameters&quot;: {
    &quot;OutputBlobNameList&quot;: &quot;[\&quot;exampleoutput.csv\&quot;]&quot;
  },
  &quot;invokedBy&quot;: {
    &quot;id&quot;: &quot;80a01654a9d34ad18b3fcac5d5d76b67&quot;,
    &quot;name&quot;: &quot;Manual&quot;
  },
  &quot;runStart&quot;: &quot;2018-06-16T00:37:44.6257014Z&quot;,
  &quot;runEnd&quot;: &quot;2018-06-16T00:38:12.7314495Z&quot;,
  &quot;durationInMs&quot;: 28105,
  &quot;status&quot;: &quot;Succeeded&quot;,
  &quot;message&quot;: &quot;&quot;,
  &quot;lastUpdated&quot;: &quot;2018-06-16T00:38:12.7314495Z&quot;,
  &quot;annotations&quot;: []
}
</code></pre>
<p>You can see the status is &quot;Succeeded&quot;
It would be one of InProgress, Succeeded, and Failed.</p>
"
"74865724","Can we process an EDI 852 format file in Azure Data Factory?","<p>I want to check if converting an EDI 852 formatted file into a CSV file through ADF or logic apps is possible.</p>
","<csv><azure-data-factory><azure-logic-apps><edi>","2022-12-20 15:57:55","141","1","2","74871566","<p>Assuming that you had an Integration Account linked with the Logic App so that you could make use of the built-in BizTalk 852 to XML parsing and then XSLT to transform to CSV (or to a flat XML structure - which you can then convert to JSON in the Logic App using <code>json(xml(body('Convert_852')))</code> in a Compose action and then use a Create CSV Table action to convert to CSV.</p>
<p>If you don't have an Integration Account, it'll be quite a bit harder, but should still be possible in a Logic App, splitting the 852 into an array of strings first by the segment terminator (which will be the character at position 105 of the ISA segment - possibly with a CR and/or LF appended) and then splitting each of those strings into elements using the element separator at position 3 of the ISA segment.</p>
<p>The header data will be in the first two or three elements of the XQ segment (3 if the 852 represents a date range, 2 if there's only a handling code and reporting date).</p>
<p>Line-level data is arranged like this:</p>
<ul>
<li>LIN : product data
<ul>
<li>ZA : product activity type (eg. qty sold, qty on order, qty lost, etc)
<ul>
<li>SDQ : location/quantity reporting (optional)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>so you'd need to consolidate SDQ data into an array inside a ZA object and then consolidate an array of ZA objects into its parent LIN object and then roll all of the LIN objects up into an array inside the top level 852 object probably using nested For Each loops containing Switch and Append To Array Variable actions.</p>
<p>You can then use Select actions inside more For Each loops to flatten the data to CSV - or just push the resulting JSON to ADF or CosmosDB</p>
"
"74865724","Can we process an EDI 852 format file in Azure Data Factory?","<p>I want to check if converting an EDI 852 formatted file into a CSV file through ADF or logic apps is possible.</p>
","<csv><azure-data-factory><azure-logic-apps><edi>","2022-12-20 15:57:55","141","1","2","74871614","<p>EDI 852 are NOT STANDARDIZED.</p>
<p>Although the various available segments and outer control structures are (ISA/IEA, GS, GE), the internals are NOT.</p>
<p>One company's 852 can be wildly different than another's.</p>
<p>You'll need the company's EDI852 specification before it can be parsed properly, regardless of what tools you have available.</p>
<p>A sample specification looks like <a href=""https://vendor.nsgrocery.com/downloads/NSG_UCS_852.pdf"" rel=""nofollow noreferrer"">this</a>:</p>
<p><a href=""https://i.stack.imgur.com/VNah0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VNah0.png"" alt=""enter image description here"" /></a></p>
"
"74862522","unable to load XML data using Copy data activity","<p>I am unable to load the XML data using copy data activity in sql server DB, but am able to achieve this with data flows using flatten hierarchy , while mapping the corresponding array is not coming properly in copy data even pipeline success also only partial  data is loading in DB.</p>
<p>and auto creation of table is also not allowing while doing copy activity for XML file , has to create table script first and load the data ...</p>
<p>as we are using SHIR this activity should be done in using copy data activity.</p>
","<sql-server><xml><azure-data-factory>","2022-12-20 11:37:59","170","0","1","74871331","<ul>
<li>Use the collection reference in mapping tab of copy activity to unroll by and extract the data. I repro'd this using copy activity with sample nested XML data.</li>
</ul>
<p><img src=""https://i.imgur.com/nohxNVP.png"" alt=""enter image description here"" />
img1: source dataset preview.</p>
<ul>
<li>In mapping tab, Select <strong>Import schemas</strong></li>
<li>Toggle on the <strong>Advanced editor</strong></li>
<li>Give the json path of the array from which data needs to be iterated and extracted.</li>
</ul>
<p><img src=""https://i.imgur.com/qnv1KpU.png"" alt=""enter image description here"" />
img:2 Mapping settings.</p>
<ul>
<li>When pipeline is run, data is copied successfully to database.
<img src=""https://i.imgur.com/uFqY7Mh.png"" alt=""enter image description here"" />
img:3 sink data after copying.</li>
</ul>
<p>Reference : <strong>MS document</strong> on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#hierarchical-source-to-tabular-sink"" rel=""nofollow noreferrer"">hierarchical source to tabular sink</a>.</p>
"
"74861157","how to do validation checks in ADF","<p>I want to perform some validation checks in ADF, if insertion of data fails  it should through error , it will automatically fix the issue in db and pipeline should run</p>
<p>i tired , but not getting , could you provide me with example</p>
","<azure-data-factory>","2022-12-20 09:46:25","73","1","1","74861489","<p>To perform data validation using ADF , the best transformation available in mapping dataflow is : <code>Assert transformation</code></p>
<p>If the check fails, you can either fail the dataflow by checking the 'Fail dataflow' option or you can use alter row transformation to delete the rows that are failing the checks.</p>
<p><a href=""https://i.stack.imgur.com/ODamh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ODamh.png"" alt=""enter image description here"" /></a></p>
<p>Checkout the following videos:</p>
<p><a href=""https://www.youtube.com/watch?v=ozhHsto-Dqg&amp;t=24s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ozhHsto-Dqg&amp;t=24s</a></p>
<p><a href=""https://www.youtube.com/watch?v=_NzWpTRxt0s&amp;t=503s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=_NzWpTRxt0s&amp;t=503s</a></p>
"
"74852506","Copy Data Sink Validation","<p>How to Use Copy data activity to check against sink values</p>
<p>My Data Sources:
SourceDataset : Source_SQL_DB
DestinationDataset : Destination_SQL_DB
SourceTable : SourceTableName
Column : Name,Age,Gender,Location
DestinationTable : DestinationTableName
Column : Name,Age,Gender,Location</p>
<p>Below is my scenario :
I have to validate Source before moving to sinkTable by checking Destination should not have the values
On Copy data, i can directly load data,</p>
<ol>
<li><p>How to pass the Location in Source Query since my source will be connecting to source dataset only</p>
<p><code>select * from SourceTableName where Location in (select distinct Location  from DestinationTableName)</code></p>
</li>
<li><p>How to check is the name present in the destination dataset table, If name is present, i should not insert data.</p>
<p><code>select * from SourceTableName where name not in (select distinct name from  DestinationTableName )</code></p>
</li>
</ol>
","<azure-data-factory>","2022-12-19 15:23:46","35","0","1","74853052","<p>assuming both your source and sink are sql, you can use a lookup activity to get the list of names and location as comma seperated and either save them in a variable or use it to directly in source query.</p>
<p>Another way would be to load the source dara as is in staging table and then leveraging a stored procedure activity.</p>
<p>The final way would be to use dataflows</p>
"
"74850045","Data Filter Expression in ADF","<p>I am trying to filter data in Azure Data Flow.
However, I do not know how to do this.
What I want to do is to extract only the records with the largest value in the &quot;seq_no&quot; column among those with duplicate IDs.
I just don't know what function to use to achieve this.
I await your answer.
Any answer would be appreciated.
Sorry for my bad English, I am Japanese.
Thanks for reading.</p>
<p><img src=""https://i.stack.imgur.com/9vZS6.png"" alt=""enter image description here"" />
<img src=""https://i.stack.imgur.com/O1Ake.png"" alt=""enter image description here"" /></p>
","<azure><azure-data-factory>","2022-12-19 11:44:24","159","0","1","74850660","<p>You can use <strong>aggregate transform</strong> and group by id and take the max(seq_no). I repro'd the same. Below are the steps.</p>
<ul>
<li>Sample data is taken as input.</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>seq_no</th>
<th>mark</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>1001</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>1001</td>
<td>2</td>
<td>20</td>
</tr>
<tr>
<td>1002</td>
<td>1</td>
<td>30</td>
</tr>
<tr>
<td>1002</td>
<td>2</td>
<td>20</td>
</tr>
<tr>
<td>1002</td>
<td>3</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p><img src=""https://i.imgur.com/t1al8Hr.png"" alt=""enter image description here"" />
img:1 Source Transformation data preview</p>
<ul>
<li>Then Aggregate transform is taken. In Aggregate settings,
<code>id</code> is given as group by column and aggregates expression is given for <code>seq_no</code> column as <code>max(seq_no)</code>.</li>
</ul>
<p><img src=""https://user-images.githubusercontent.com/113445679/208426498-0c8f691b-0e61-481e-8e96-20aee2ddba91.gif"" alt=""gif111"" /></p>
<ul>
<li>Aggregate transform output data</li>
</ul>
<p><img src=""https://i.imgur.com/buu6sa3.png"" alt=""enter image description here"" />
img:2 Data preview of Aggregate transform.</p>
<ul>
<li>In order to get the other column data corresponding to maximum of <code>seq_no</code> column, <strong>Join transformation</strong> is used.</li>
</ul>
<pre><code>Left stream: aggregate1
Right stream: source1
Join Type:Inner
Join conditions: source1@id==source2@id
                 source1@seq_no==source2@seq_no
</code></pre>
<p><img src=""https://i.imgur.com/Pi8TkIn.png"" alt=""enter image description here"" />
img:3 Join Transformation settings</p>
<p><img src=""https://i.imgur.com/SV2o7He.png"" alt=""enter image description here"" />
img:4 Join transformation data preview</p>
<ul>
<li>Select transformation is used and removed the extra columns.
<img src=""https://user-images.githubusercontent.com/113445679/208584333-e23ee8df-1d8f-46ea-bd5c-d37757ac414a.gif"" alt=""gif112"" /></li>
</ul>
"
"74849570","Prevent users from editing master branch in ADF","<p>I'm trying to prevent developers from directly editing and committing to the master branch in ADF.</p>
<p>There is a risk that users can accidently make changes in ADF by not creating a feature branch but instead working directly on master (I've done it myself!)</p>
<p>Is there a way to enforce this in ADF? I do have a branch policy on master that only allows merging via a pull request but don't think this stops developers from working directly on master:</p>
<p><a href=""https://i.stack.imgur.com/j8IZb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j8IZb.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-12-19 11:00:37","88","0","2","74860393","<p>You can deny the contribute permission for the user group in git repo. Check the following official documentation:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/devops/repos/git/branch-permissions?view=azure-devops"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/repos/git/branch-permissions?view=azure-devops</a></p>
"
"74849570","Prevent users from editing master branch in ADF","<p>I'm trying to prevent developers from directly editing and committing to the master branch in ADF.</p>
<p>There is a risk that users can accidently make changes in ADF by not creating a feature branch but instead working directly on master (I've done it myself!)</p>
<p>Is there a way to enforce this in ADF? I do have a branch policy on master that only allows merging via a pull request but don't think this stops developers from working directly on master:</p>
<p><a href=""https://i.stack.imgur.com/j8IZb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j8IZb.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-12-19 11:00:37","88","0","2","74864420","<p>Another way to achieve this is to require a reviewer.</p>
<p>Select &quot;Branch policies&quot;:</p>
<p><a href=""https://i.stack.imgur.com/FFInR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FFInR.png"" alt=""enter image description here"" /></a></p>
<p>Turn on the option &quot;Require a minimum number of reviewers&quot;:</p>
<p><a href=""https://i.stack.imgur.com/fYFYy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fYFYy.png"" alt=""enter image description here"" /></a></p>
<p>Set the number of reviewers to 1.</p>
<p>This example shows &quot;Allow requestors to approve their own changes&quot;. This setting is optional. If you set it, the user is still prevented from committing to main/master, but can complete their own PRs.</p>
"
"74849359","How to execute MariaDB stored procedure from azure data factory?","<p>I wanted to 'Call' MariaDB Procedure from Azure Data Factory.</p>
<p>How can this be achieved, are there any other service which can be integrated with ADF to call this MariaDB procedures</p>
<p>I tried calling the procedure by writing the query using lookup activity.</p>
<p>It fails while showing this error.
ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property 'columns' is invalid: 'Value cannot be null.
Parameter name: columns'.,Source=,''Type=System.ArgumentNullException,Message=Value cannot be null.
Parameter name: columns,Source=Microsoft.DataTransfer.Common,'</p>
","<azure-cosmosdb><azure-data-factory><azure-cosmosdb-mongoapi>","2022-12-19 10:41:21","118","0","2","74868995","<p>By selecting the 'query' option, you can call the stored procedure using lookup activity. From your error message, it looks like you are missing the parameter columns while calling the stored procedure.</p>
<p>Did you try executing the same code using the client tools like MySQL workbench? If you can execute the stored proc from other client tools, then you should be able to execute the same using the lookup activity.</p>
<p>I tested from my end and was able to execute the Stored procedure using lookup activity. Please see the below screenshot for your reference.</p>
<p><a href=""https://i.stack.imgur.com/cSZLf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cSZLf.png"" alt=""Lookup"" /></a></p>
<p><a href=""https://i.stack.imgur.com/coKAq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/coKAq.png"" alt=""output"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rtjDm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rtjDm.png"" alt=""mysqlqworkbench"" /></a></p>
"
"74849359","How to execute MariaDB stored procedure from azure data factory?","<p>I wanted to 'Call' MariaDB Procedure from Azure Data Factory.</p>
<p>How can this be achieved, are there any other service which can be integrated with ADF to call this MariaDB procedures</p>
<p>I tried calling the procedure by writing the query using lookup activity.</p>
<p>It fails while showing this error.
ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property 'columns' is invalid: 'Value cannot be null.
Parameter name: columns'.,Source=,''Type=System.ArgumentNullException,Message=Value cannot be null.
Parameter name: columns,Source=Microsoft.DataTransfer.Common,'</p>
","<azure-cosmosdb><azure-data-factory><azure-cosmosdb-mongoapi>","2022-12-19 10:41:21","118","0","2","74884498","<p>Lookup activity reads and returns the content of the query. I tried to repro this by creating three stored procedures in Azure SQL database for Maria DB.</p>
<p>First Stored procedure is written to update the data in the table.</p>
<pre class=""lang-sql prettyprint-override""><code>DELIMITER $$
CREATE PROCEDURE update_inventory()
BEGIN
  UPDATE inventory SET quantity = 150 
  WHERE id = 1;
END$$
DELIMITER ;
</code></pre>
<p>When this procedure is called in ADF lookup activity, error occurs.</p>
<p>Second stored procedure is written with select query.</p>
<pre class=""lang-sql prettyprint-override""><code>DELIMITER $$
CREATE PROCEDURE select_inventory()
BEGIN
 select * from inventory;
END$$
DELIMITER ;
</code></pre>
<p>When this SP is called, ADF pipeline is executed successfully.</p>
<p>In order to execute the stored procedure with update statements (or any statements), a select statement is added in the Stored procedure.</p>
<pre class=""lang-sql prettyprint-override""><code>DELIMITER $$
CREATE PROCEDURE update_select_inventory()
BEGIN
  UPDATE inventory SET quantity = 150 
  WHERE id = 1;
  select * from inventory;
END$$
DELIMITER ;
</code></pre>
<p>When this stored procedure is called through Lookup activity, it got executed successfully.</p>
<p><img src=""https://i.imgur.com/DnTy9Le.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/bfotN6o.png"" alt=""enter image description here"" /></p>
<p><strong>Try adding select statement in the stored procedure and execute it in Lookup activity. Or add Select statement after Call stored procedure statement.</strong></p>
"
"74848438","Data Factory copy from Azure Cognitive Search","<p>The Copy activity does not support Azure Cognitive Search as a source.  Sink is fine but not a source.  Makes transferring indexed documents from one index to another tedious: an until around get + post batches to the Search API with conditional variables to break the outer until iteration.</p>
<p>Easier way?</p>
","<azure><azure-data-factory>","2022-12-19 09:20:38","106","1","1","74921715","<p>As of now Azure cognitive search is not supported as source. Azure cognitive search can be used as Sink only in Azure data factory copy activity. Refer the official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">documentation</a> for supported data stores</p>
<p><img src=""https://i.imgur.com/gne0zze.png"" alt=""enter image description here"" /></p>
"
"74840978","Azure Data Factory - converting lookup result array","<p>I'm pretty new to Acure Data Factory - ADF and have stumbled into somthing I would have solved with a couple lines of code.</p>
<p><strong>Background</strong></p>
<p>Main flow:</p>
<ol>
<li>Lookup Activity fetchin an array of ID's to process</li>
<li>ForEach Activity looping over input array and uisng a Copy Activity pulling data from a REST API storing it into a database</li>
</ol>
<p><strong>Step #1</strong> would result in an array containing ID's</p>
<pre><code>{
    &quot;count&quot;: 10000,
    &quot;value&quot;: [
        {
            &quot;id&quot;: &quot;799128160&quot;
        },
        {
            &quot;id&quot;: &quot;817379102&quot;
        },
        {
            &quot;id&quot;: &quot;859061172&quot;
        },
        ... many more...
</code></pre>
<p><strong>Step #2</strong> When the lookup returns a lot of ID's - individual REST calls takes a lot of time. The REST API supports batching ID's using a comma spearated input.</p>
<p><strong>The question</strong>
How can I convert the array from the input into a new array with comma separated fields? This will reduce the number of Activities and reduce the time to run.</p>
<p>Expecting something like this;</p>
<pre><code>{
    &quot;count&quot;: 1000,
    &quot;value&quot;: [
        {
            &quot;ids&quot;: &quot;799128160,817379102,859061172,....&quot;
        },
        {
            &quot;ids&quot;: &quot;n,n,n,n,n,n,n,n,n,n,n,n,....&quot;
        }
        ... many more...
</code></pre>
<p><strong>EDIT 1 - 19th Des 22</strong>
Using &quot;Until Activity&quot; and keeping track of posistions, I managed to use plain ADF. Would be nice if this could have been done using some simple array manipulation in a code snippet.
<a href=""https://i.stack.imgur.com/HIsqu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HIsqu.png"" alt=""enter image description here"" /></a></p>
","<arrays><azure-data-factory>","2022-12-18 11:37:13","340","0","1","74847076","<p>The ideal response might be we have to do manipulation with Dataflow -</p>
<p><strong>My sample input:</strong></p>
<p><img src=""https://i.imgur.com/tRmONVW.png"" alt=""enter image description here"" /></p>
<ol>
<li>First, I took a Dataflow In that adding a key Generate (Surrogate key) after the source - Say new key field is 'SrcKey'</li>
</ol>
<p><img src=""https://i.imgur.com/8yxeVEZ.png"" alt=""enter image description here"" /></p>
<p>Data preview of <strong>Surrogate key 1</strong></p>
<p><img src=""https://i.imgur.com/qsvZdEQ.png"" alt=""enter image description here"" /></p>
<ol start=""2"">
<li>Add an aggregate where you group by <code>mod(SrcKey/3)</code>. This will group similar remainders into the same bucket.</li>
</ol>
<p><img src=""https://i.imgur.com/7V4PZ4h.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>Add a collect column in the same aggregator to collect into an array with expression <code>trim(toString(collect(id)),'[]')</code>.</li>
</ol>
<p><img src=""https://i.imgur.com/QYKGQHu.png"" alt=""enter image description here"" /></p>
<p>Data preview of <strong>Aggregate 1</strong></p>
<p><img src=""https://i.imgur.com/GXfSMU4.png"" alt=""enter image description here"" /></p>
<p>Store output in single file in blob storage.</p>
<p><img src=""https://i.imgur.com/rhUUrv6.png"" alt=""enter image description here"" /></p>
<p><strong>OUTPUT</strong></p>
<p><img src=""https://i.imgur.com/zPxL3lA.png"" alt=""enter image description here"" /></p>
"
"74837685","How to add a Where clause in Azure Data Factory Copy Activity","<p>I have the following Azure Data Factory copy activity,</p>
<p><a href=""https://i.stack.imgur.com/xp8wy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xp8wy.png"" alt=""enter image description here"" /></a></p>
<p>The query looks like the following:</p>
<pre><code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
</code></pre>
<p>I would like to add a where clause to the query similar to the following:</p>
<p>'SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
where parameters.ColumnName(mycolumn) = 'zyx'</p>
<p>I know the above is probably incorrect but is it possible to add a WHERE clause in the query to work with  <code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)</code></p>
","<azure-data-factory>","2022-12-17 21:47:38","282","1","2","74845842","<p>Here is a sample expression to form a valid SQL statement in ADF source using dynamic expression when there is a parameter for table name using a where clause.</p>
<pre><code>@concat('select * from ', pipeline().parameters.param_TableName, ' where Location = ',' ''Redmond'' ')
</code></pre>
<p>Or</p>
<pre><code>@concat('select * from ', pipeline().parameters.param_TableName, ' where Location ',' = ''Redmond'' ')
</code></pre>
<p>The above expression will form the below SQL statement at runtime:</p>
<pre><code>&quot;sqlReaderQuery&quot;: &quot;select * from Employee where Location = 'Redmond'&quot;,
</code></pre>
<p>In your case the query will look like below:</p>
<pre><code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName, 'where ', pipeline().parameters.ColumnName, ' = ''zyx'' ')
</code></pre>
<p>In simple terms in the where clause of your dynamic expression you will have to add two single quotes to form one single quote during the runtime.</p>
"
"74837685","How to add a Where clause in Azure Data Factory Copy Activity","<p>I have the following Azure Data Factory copy activity,</p>
<p><a href=""https://i.stack.imgur.com/xp8wy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xp8wy.png"" alt=""enter image description here"" /></a></p>
<p>The query looks like the following:</p>
<pre><code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
</code></pre>
<p>I would like to add a where clause to the query similar to the following:</p>
<p>'SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
where parameters.ColumnName(mycolumn) = 'zyx'</p>
<p>I know the above is probably incorrect but is it possible to add a WHERE clause in the query to work with  <code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)</code></p>
","<azure-data-factory>","2022-12-17 21:47:38","282","1","2","74846358","<ul>
<li><p>Using <code>concat</code> to build SQL queries would give desired result but you can also try using string interpolation <code>@{...}</code> to build queries instead. The following is an example of the same.</p>
</li>
<li><p>I am using the dynamic content in set variable activity as a demonstration. To build a query that you are expecting, you can give the following dynamic expression directly in the pipeline expression builder.</p>
</li>
</ul>
<pre><code>SELECT * FROM @{pipeline().parameters.domain}.@{pipeline().parameters.tablename} where @{pipeline().parameters.col} = 'zyx'
</code></pre>
<p><img src=""https://i.imgur.com/srZx5qG.png"" alt=""enter image description here"" /></p>
<ul>
<li>The above dynamic content would generate the query as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/inJHrKG.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, wherever you want to use dynamic content in the query, you can simply enclose it within <code>@{...}</code></li>
</ul>
"
"74833267","Read multiple parquet files and ignore _delta_log folder and save in a single csv file","<p>I have copied a source to adls using ADF in folder 2022-12-05(SchedulerunTime parameter) and the data looks like one _delta_log folder and many parquet files(refer screenshot) e.g.,
_delta_log,
part-0000-12...,
part-0000-25...,
.... so on</p>
<p>I want to read only the parquet files and write into a single csv file under the same 2022-12-05 and also delete the _delta_log folder.</p>
<p><strong>Problem :</strong> _delta_log,
part-0000-12...,
part-0000-25...,
.... so on</p>
<p><strong>Solution I need</strong> : csv_file.csv</p>
<p><strong>My approach :</strong>
saving in single csv file : DF0.coalesce(1).write.format(&quot;csv&quot;).mode(&quot;overwrite&quot;).save(path)</p>
<p>but the output is coming like 1 folder and 3 files  :
<em>delta_log folder,
SUCCESS</em> file,
_Committed file,
part-0000-36gdh....csv file.</p>
<p>I only want csv_file.csv removing the 1 folder and these upper 3 files.<a href=""https://i.stack.imgur.com/mT8ci.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mT8ci.jpg"" alt=""enter image description here"" /></a></p>
","<scala><apache-spark><pyspark><azure-data-factory><azure-databricks>","2022-12-17 10:09:46","337","0","1","74884504","<p>If you do the above operations in Pyspark, you can get a single csv file by using <code>pandas</code> in databricks.</p>
<p>In Scala, we cannot use <code>pandas</code> so you can try this alternative using ADF to get single csv file.</p>
<p>First in source use wildcard file paths and give like <code>*.parquet</code>.</p>
<p>These are my source parquet files.</p>
<p><img src=""https://i.imgur.com/P0iFgpP.png"" alt=""enter image description here"" /></p>
<p><strong>Copy activity:</strong></p>
<p><img src=""https://i.imgur.com/T968PT9.png"" alt=""enter image description here"" /></p>
<p>In sink use csv file and give <strong>merge</strong> as copy type.</p>
<p><img src=""https://i.imgur.com/C5JkrFv.png"" alt=""enter image description here"" /></p>
<p><strong>Result single csv file:</strong></p>
<p><img src=""https://i.imgur.com/URyrywD.png"" alt=""enter image description here"" /></p>
"
"74833159","Azure Data Factory Data Flow: how to filter input column with multiple values","<p>What I am trying to do is basically something that emulates SQL WHERE IN CLAUSE in a data flow. I want to pass comma separated string of values into my data flow, i.e ptf_proc_link_id = &quot;A, B, C&quot;.</p>
<p><a href=""https://i.stack.imgur.com/Arpda.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Arpda.png"" alt=""enter image description here"" /></a></p>
<p>Inside of the data flow, I want to use those values, to filter one of my input columns:</p>
<p><a href=""https://i.stack.imgur.com/yoZtN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yoZtN.png"" alt=""enter image description here"" /></a></p>
<p>It would be equal to writing in SQL SELECT * FROM my_delta_table where ptf_proc_link_id in (A, B, C).</p>
<p>However, data flow documentation only hints that I can filter single values i.e ptf_proc_link_id = A. I have tried to use intersect, but then both parameters needs to be arrays, and in my case one of them is a column. I could do something like ptf_proc_link_id = A OR ptf_proc_link_id = B OR ptf_proc_link_id = C, but I never know how many input values I would have, so I cannot hardcode it.</p>
<p>It seems like a very simple use case for a data flow, so I would be really thankful if someone could explain me what the correct approach would be.</p>
<p>Thank you!</p>
","<azure><azure-data-factory>","2022-12-17 09:53:23","607","0","1","74834750","<p>You can use <code>in()</code> function in ADF data flow to filter multiple values.</p>
<p>Syntax:
<code>in(array of items,item to find)</code></p>
<ul>
<li>I tried to repro this with sample input data.</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>key</th>
<th>class</th>
<th>name</th>
<th>mark</th>
<th>DOB</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>Arjuna</td>
<td>50</td>
<td>1/2/2015</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Basanta</td>
<td>47</td>
<td>5/1/2015</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>Subala</td>
<td>54</td>
<td>5/6/2015</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
<td>Gandharva</td>
<td>60</td>
<td>1/2/2014</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>Ujjvala</td>
<td>55</td>
<td>9/2/2014</td>
</tr>
<tr>
<td>6</td>
<td>2</td>
<td>Sanadhana</td>
<td>64</td>
<td>1/12/2014</td>
</tr>
<tr>
<td>7</td>
<td>3</td>
<td>Sridama</td>
<td>75</td>
<td>1/2/2013</td>
</tr>
<tr>
<td>8</td>
<td>3</td>
<td>Sudama</td>
<td>80</td>
<td>13/12/2013</td>
</tr>
<tr>
<td>9</td>
<td>3</td>
<td>Vasu</td>
<td>81</td>
<td>1/12/2013</td>
</tr>
</tbody>
</table>
</div>
<p><img src=""https://i.imgur.com/gm43gbl.png"" alt=""enter image description here"" />
img:1 Source data preview.</p>
<ul>
<li><p>I tried to filter the records with key=2,4,6</p>
</li>
<li><p>Filter condition can be given as
<code>in(['2','4','6'], key)</code>  or
<code>in(array('2','4','6'), key)</code>
<img src=""https://i.imgur.com/w2ibjxN.png"" alt=""enter image description here"" />
img:2 Filter condition settings</p>
</li>
<li><p><strong>Output data of the filter Transformation:</strong></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/T3Cb2zB.png"" alt=""enter image description here"" />
img:3 Filter Transformation data preview</p>
<ul>
<li><p>You can also create a dataflow parameter of array type and give that parameter in the filter condition of filter transformation.
<img src=""https://i.imgur.com/Eln6MEm.png"" alt=""enter image description here"" />
img: 4 dataflow parameters</p>
</li>
<li><p>Filter condition while using parameter will be <code>in($parameter1,key)</code></p>
</li>
</ul>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#in"" rel=""nofollow noreferrer"">Microsoft document</a> on <code>data flow expression - in</code></p>
"
"74831539","Error HTTP 401 when trying to sent a message from Azure Data Factory to Service Bus","<p>I have tried to reproduce <strong>Hugh Woods</strong> experiment (<em><a href=""https://medium.com/asos-techblog/sending-messages-from-azure-data-factory-to-service-bus-86d4be6dd357"" rel=""nofollow noreferrer"">https://medium.com/asos-techblog/sending-messages-from-azure-data-factory-to-service-bus-86d4be6dd357</a></em>) in my azure environment to sent messages from Azure DataFactory to Azure Service Bus.</p>
<p>I have followed the instructions, assigning the  &quot;<em>Azure Service Bus Data Sender</em>&quot; role to my <em>data factory’s managed identity</em>.</p>
<p>But when I tried to sent a message to Service Bus I got this error:</p>
<pre><code>Error code   : 2108
Failure type : User configuration issue
Details      : Invoking Web Activity failed with HttpStatusCode - 
               '401 : Unauthorized', message - ''
Source       : Pipeline Service Bus REST API
</code></pre>
<p>What am I doing wrong in this case?  I have this configuration in my pipeline:</p>
<pre><code>   {
    &quot;name&quot;: &quot;Service Bus REST API&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Service Bus REST API&quot;,
                &quot;description&quot;: &quot;Teste&quot;,
                &quot;type&quot;: &quot;WebActivity&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: &quot;https://sb-namespace-dv.servicebus.windows.net/sbt-azure-adf-ntt-data-test/messages&quot;,
                    &quot;method&quot;: &quot;POST&quot;,
                    &quot;headers&quot;: {
                        &quot;CustomHeader-Version&quot;: &quot;\&quot;1.0\&quot;&quot;,
                        &quot;Content-Type&quot;: &quot;application/json&quot;,
                        &quot;BrokerProperties&quot;: {
                            &quot;value&quot;: &quot;@concat('{\&quot;CorrelationId\&quot;: \&quot;',pipeline().RunId,'\&quot;}')&quot;,
                            &quot;type&quot;: &quot;Expression&quot;
                        }
                    },
                    &quot;body&quot;: {
                        &quot;value&quot;: &quot;{\&quot;message\&quot;:{\&quot;value\&quot;:\&quot;@variables('OutputDetails')\&quot;,\&quot;type\&quot;:\&quot;Expression\&quot;}}&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;httpRequestTimeout&quot;: &quot;00:10:00&quot;,
                    &quot;authentication&quot;: {
                        &quot;type&quot;: &quot;MSI&quot;,
                        &quot;resource&quot;: &quot;https://servicebus.azure.net&quot;
                    }
                }
            }
        ],
        &quot;folder&quot;: {
            &quot;name&quot;: &quot;999_Others/9910_DevTest/TesteServiceBusADF&quot;
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>I have followed all steps of <strong>Hugh Woods</strong> article. I have expecting to get the same results of his experiment (below), but I did not have success.</p>
<pre><code>{
message : &quot;Snapshot Avaliable&quot;
}
</code></pre>
","<azure><azure-data-factory><azureservicebus><http-status-code-401><azure-servicebus-topics>","2022-12-17 03:08:34","282","0","1","74837958","<p>The message you are getting is HTTP 401 Unauthorized. So the call to write data to the service bus is being refused.</p>
<p>There are 2 possibilities:</p>
<ul>
<li>The authentication is not set up correctly</li>
<li>You tested it before the role assignments had propagated (can take 5 mins)</li>
</ul>
<p>See: <a href=""https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-managed-service-identity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-managed-service-identity</a></p>
"
"74829441","How to set up blob storage Property before web activity drop a file into Blob storage?","<p>I have created pipeline in Azure Data factory in which I'm Have web activity which copy the excel(xlsx) file from Dropbox App console and have another web activity which copy the file into Blob Storage, Pipeline is executing successfully, it is copying the file in same xlsx format in blob Storage as well but when I open the excel file from blob storage getting error that &quot;The file myfilename.xlsx may not render correctly as it contains an unrecognized extension&quot;</p>
<p>when the web activity copy the file I see it has content-Type = Applicatoin/octent-stream, I did try to change the content-type = application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, Any help would be appreciate to set the blob storage property before I copy my excel file from web activity output.</p>
","<azure-data-factory>","2022-12-16 20:25:43","134","0","1","74886509","<p>The web activity's associated service and dataset characteristics are to be applied. I'll give you a solution that I know will work in the interim. Use two online activities. The data is initially retrieved from the blob. The first web activity's output is used as the body of the second web activity. In the example below, I substitute another blob for the website. (It essentially duplicates the blob.)</p>
<p>Here, the URL looks like this to obtain the blob:</p>
<p>https://.blob.core.windows.net///;</p>
<p>As well, I employ MSI authentication. You must assign the Data Factory rights in the storage account (or container) access control for this to function.</p>
<p><a href=""https://i.stack.imgur.com/9etYN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9etYN.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/UB0jG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UB0jG.png"" alt=""enter image description here"" /></a></p>
<p>The header x-ms-version must then be added, along with the value 2017-11-09. (A picture of an earlier solution illustrates this.)</p>
<p>You should use <a href=""https://storage.azure.com"" rel=""nofollow noreferrer"">https://storage.azure.com</a> as the resource.</p>
<p>If an error message appears,</p>
<p><a href=""https://i.stack.imgur.com/cfUAz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cfUAz.png"" alt=""enter image description here"" /></a></p>
<p>AuthorizationPermissionMismatch</p>
<p>This operation cannot be carried out under the terms of this request.Then you might need to visit your storage account and grant additional permissions, as seen below. Wait for the permissions to spread over a few minutes; otherwise, you risk getting a false negative.</p>
<p><a href=""https://i.stack.imgur.com/r4rqp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r4rqp.png"" alt=""enter image description here"" /></a></p>
"
"74822357","Azure Data Factory Validation the Header row and checksum of a fixed length txt file and insert","<p>I am currently using the Azure Data Factory to retrieve a fixed-length file from blob storage and trying to import the record into my database.</p>
<p>Fixed-length.txt</p>
<pre><code>0202212161707
1Tom
1Kelvin
1Michael
23
</code></pre>
<p>The first row is the header record, which is start with '0' and comes up with the creation time.
The following row are the detail record, started with '1' and comes up with user name.
The last row is the end record, which started with '2' and comes up with the sum of the detail record.</p>
<p>However, I want to validate that the data of the file is correct before I insert those records. I would like to check if the checksum is correct first, and then only insert all those record started with 1.</p>
<p>Currently, I insert all those record line by line into SQL DB and run a stored procedures to perform the tasks. Is it possible to utlize Azure Data Factory to do it?? Thank you.</p>
","<azure><azure-data-factory>","2022-12-16 09:15:06","101","0","1","74824005","<p><strong>I reproduced your issue follow below steps.</strong></p>
<p>First take one look up activity to view all the data from file and apply filter on that data.</p>
<p><img src=""https://i.imgur.com/BTnvfAX.png"" alt=""enter image description here"" /></p>
<p>Then take one set variable activity and get the last row's last element <strong>e.g. 23 as 3</strong> with below dynamic expression.</p>
<pre><code>@last(activity('Lookup1').output.value[sub(length(activity('Lookup1').output.value),1)].Prop_0)
</code></pre>
<p><img src=""https://i.imgur.com/5jeCkaM.png"" alt=""enter image description here"" /></p>
<p>Then take one Filter activity to filter rows with 1 prefix with below items value and condition</p>
<pre><code>items : @activity('Lookup1').output.value
condition : @startswith(item().Prop_0,'1')
</code></pre>
<p><img src=""https://i.imgur.com/niyaEy1.png"" alt=""enter image description here"" /></p>
<p>after filter take ForEach activity to Append those values in an array</p>
<p><img src=""https://i.imgur.com/46o7E83.png"" alt=""enter image description here"" /></p>
<p>Then inside for each activity take Append variable activity it will create an array with filtered values.</p>
<p><img src=""https://i.imgur.com/BZUFrdl.png"" alt=""enter image description here"" /></p>
<p>Now take If condition with expression which checking value of set variable 1 and length of Append array variable is same or not.</p>
<pre><code>@equals(int(variables('sum')),length(variables('username')))
</code></pre>
<p><img src=""https://i.imgur.com/v2a2STx.png"" alt=""enter image description here"" /></p>
<p>Then inside true condition, add your copy activity to copy data if condition is true</p>
<p><strong>My Sample Output:</strong></p>
<pre><code>0202212161707
1Tom
1Kelvin
23
</code></pre>
<p>for above data control is going to false condition.</p>
<p><img src=""https://i.imgur.com/sTsZW3s.png"" alt=""enter image description here"" /></p>
<pre><code>0202212161707
1Tom
1Kelvin
1Michael
23
</code></pre>
<p>for above data control is going to true condition.</p>
<p><img src=""https://i.imgur.com/Tfl1b9o.png"" alt=""enter image description here"" /></p>
"
"74821797","Can we use output of look up activity directly in Data flow activity without giving any dataset in the dataflow?","<p>I was using the look up activity to read my tables in Oracle .I want the output of Look up to be the Source of DataFlow ,Because at dataflow designer i could not connect Oracle at source side,Oracle connector is not available in Dataflow level.</p>
<p>So is it possible this way or is there any efficient way to do it .</p>
","<azure><azure-data-factory>","2022-12-16 08:18:46","112","0","1","74828221","<p>For this pattern, use a Copy Activity in your pipeline prior to your Data Flow activity and stage the data in a Blob or ADLS folder, then use that dataset as the source in your data flow next in the pipeline.</p>
"
"74821726","How to connect to azure SQL database without IP whitelisting","<p>Someone at the interview asked me a question.
Lets assume there is a new SQL Server being provisioned in the resource group. The question was how would you establish a connectin using azure data factory without whitelisting the IP address.</p>
<p>I responded that we could use Managed Identity.</p>
<p>Could someone guide me to the best practices around it. The securest way to connect to Azure SQL DB.</p>
<p>I answered Managed Identity. However not sure about this.</p>
","<azure-sql-database><azure-data-factory>","2022-12-16 08:11:21","119","0","1","74821912","<p>Managed identity is just a type of authentication to access the Database and the best secure way to access it. But that is not the answer for your query w.r.t whitelisting .</p>
<p><a href=""https://i.stack.imgur.com/bHF7Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bHF7Z.png"" alt=""enter image description here"" /></a></p>
<p>you need to enable this property</p>
"
"74821626","Importing CSV file(s) into SQL server with ADF","<p>I've a CSV file stored in a blob storage and want to upsert the records in a on-premise SQL server.
Currently I'm getting errors on inserting date fields. In the CSV file those date fields are typed as a date field. In my SQL table they are marked as DateTime2 fields. In the CSV file I have about 1000 records, 500 of them are seen as correct and 500 as incorrect.</p>
<p>When I check the file I really don't see a difference in the date fields. There is no time hidden or whatever which could actually cause my pipeline to fail.</p>
<p>What would be a good way to handle those fields? In the current situation, as a work around so that the business could continue, I've inserted all the fields as nvarchar(500) fields. So the imports works, but it's not ideal...</p>
<p>-- update</p>
<p>I've uploaded an image of the file which I'm importing, removed the fields where I'm not getting errors on. As you can see in the csv file the cells where there is some data are marked as Date fields.</p>
<p>Excel example
<img src=""https://i.stack.imgur.com/P45h6.png"" alt=""Excel example"" /></p>
<p>In ADF all fields are identified as a 'string' field, so maybe that's the problem. Never the less I can't change the schema in the import settings.</p>
<p>ADF identifying them as string
<img src=""https://i.stack.imgur.com/sJ8Nd.png"" alt=""ADF identifying them as string "" /></p>
<p>In SQL they are set to DateTime2 (setting it to dateTime or date gave the same error).</p>
<p>Sink field schema
<img src=""https://i.stack.imgur.com/Am0YG.png"" alt=""Sink field schema"" /></p>
<p>Here the mapping itself:</p>
<p>Source and Sink mapping
<img src=""https://i.stack.imgur.com/ZQCxS.png"" alt=""Source and Sink mapping"" /></p>
<p>Eventually the error that I'm getting, is saying that it can't insert a string value into a datetime field.</p>
<pre><code>{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;ErrorCode=TypeConversionFailure,Er is een uitzondering opgetreden bij het converteren van de waarde 29-8-2017 voor de kolomnaam TT_STARTDATE van het type String (Precision: , Scale: ) naar type DateTime (Precision: 255, Scale: 7). Aanvullende informatie: String was not recognized as a valid DateTime.&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;EMP&quot;,
&quot;details&quot;: [
    {
        &quot;errorCode&quot;: 0,
        &quot;message&quot;: &quot;'Type=System.FormatException,Message=String was not recognized as a valid DateTime.,Source=mscorlib,'&quot;,
        &quot;details&quot;: []
    }
]}
</code></pre>
<p>Thanks in advance!</p>
","<sql><csv><azure-data-factory><data-warehouse><azure-synapse>","2022-12-16 08:01:20","170","1","1","74999295","<p>Well i think it is the fomat of the date which is different in excel and what SQL is expecting is creating the error . I think in Excel you are providing dd-MM-YYYY , may be your SQL is expecting MM-DD-YYYY. I suggest please check this on the SQL side by inserting a record from TSQL using SSMS or other IDE . Once you are sure you can change the format in EXCEL accordingly .</p>
"
"74819744","Convert SQL table data to Excel file in ADF","<p>I am creating a workflow in Azure data factory and I wanted to create a excel file with data from the SQL table (Azure SQL server) in any one below scenario:</p>
<ol>
<li>Create excel and upload into blob storage.</li>
<li>Create excel and upload into sharepoint.
But I am unable to find the excel connection in copy activity sink for copying the data into blob. Is there anyway to do that? Please advice.</li>
</ol>
","<azure-blob-storage><azure-data-factory>","2022-12-16 02:59:11","375","0","2","74820077","<p>There is no out of the box feature in ADF to support Excel as a sink.
You would have to use logic app or write your own custom logic via Azure function, Databricks, Azure batch etc to do the above tasks</p>
"
"74819744","Convert SQL table data to Excel file in ADF","<p>I am creating a workflow in Azure data factory and I wanted to create a excel file with data from the SQL table (Azure SQL server) in any one below scenario:</p>
<ol>
<li>Create excel and upload into blob storage.</li>
<li>Create excel and upload into sharepoint.
But I am unable to find the excel connection in copy activity sink for copying the data into blob. Is there anyway to do that? Please advice.</li>
</ol>
","<azure-blob-storage><azure-data-factory>","2022-12-16 02:59:11","375","0","2","75406344","<p>Write a SQL stored procedure Python in SQL Server Machine learning Services (will only work if SQL Server Machine Learning Services is configured). ADF can then run the stored procedure and place the file into the blob storage. The Blob storage Python libraries are not available on SQL Managed Instance, so the script below uses the Blob REST API's instead.</p>
<pre><code>EXECUTE sp_execute_external_script @language = N'Python'
,@input_data_1 = N'SELECT A.* FROM MyTable A'
,@input_data_1_name = N'SQLQueryOutput' 
,@script = N'

import pyodbc
import pandas as pd
import requests


# Execute the query to retrieve the data from the SQL Server table
df = SQLQueryOutput

# Write the data to an Excel file
df.to_excel(FileName, index=False)

url = &quot;https://&quot; + AccountName + &quot;.blob.core.windows.net/&quot; + BlobContainerPath + &quot;/&quot; + FileName + &quot;?&quot; + BlobSASToken

payload= open(FileName, &quot;rb&quot;)
headers = {
  ''x-ms-blob-type'': ''BlockBlob'',
  ''Content-Type'': ''application/vnd.openxmlformats-officedocument.spreadsheetml.sheet''
}

response = requests.request(&quot;PUT&quot;, url, headers=headers, data=payload)

print(response.text)

'
,@params = N'@BlobSASToken VARCHAR(200), @AccountName VARCHAR(100), @BlobContainerPath VARCHAR(200), @FileName VARCHAR(100)'
,@BlobSASToken = N'sp=xxxxxxxxxxxxx&amp;st=xxxxxxxxxxxx&amp;se=xxxxxxxxxxxx&amp;spr=https&amp;sv=xxxxxxxxxxxxxxx&amp;sr=c&amp;sig=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
,@AccountName = N'myaccountname'
,@BlobContainerPath = N'containername/containerpath'
,@FileName = N'myexcelfilename.xlsx'
</code></pre>
"
"74817661","What is best practice for aggregating multiple congruent databases for use by Power BI","<p>I am consolidating data from multiple, identically-structured databases that make frequent use of bigint key fields. What is best practice for ensuring uniqueness in the aggregate tables and ensuring they can still be related to the foreign keys in other aggregate tables <strong>once they're in Power BI</strong>?</p>
<p>I ask because it is my understanding that Power BI won't allow joins using multiple columns.</p>
<p>I have created the following illustrative case:</p>
<p><a href=""https://i.stack.imgur.com/Cbc0F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cbc0F.png"" alt=""Source databases"" /></a></p>
<p>If Power BI were okay with me joining the aggregate Customers table to the aggregate Orders table using <strong>multiple</strong> fields, I'd simply add a source field (e.g. src) and do this:
<a href=""https://i.stack.imgur.com/efPzc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/efPzc.png"" alt=""Aggregate database with new field &quot;src&quot;"" /></a>
Note that the join between the two tables uses two fields: src and CustId</p>
<p>But if, as I understand it, Power BI requires that those be joined by a single field, I'd be tempted to create a new value by merging the <strong>src</strong> and <strong>CustId</strong> fields into, say, <strong>SrcCustId</strong> and joining on that:
<a href=""https://i.stack.imgur.com/arDhD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/arDhD.png"" alt=""Aggregate with new helper field"" /></a></p>
<p>Finally, if the answer is merging the two columns into a helper column, can I do that using a computed column in SQL Server (or SQL Database) or do I need to handle that when loading the source tables in the first place?</p>
<p>I would prefer the computed column solution because there may be multiple foreign keys in my actual tables and loading helper columns for all of them will blow up the amount of work I need to do each time I spin up new Azure Data Factory pipelines for a new source database.</p>
","<sql-server><powerbi><azure-sql-database><azure-data-factory><data-modeling>","2022-12-15 21:17:17","88","0","1","74921282","<p><strong>Explanation: -</strong> Instead of merging or creating helper column you can use CombineValues function of DAX. The CombineValues function supports multi-column relationships in DirectQuery models.</p>
<p><strong>Reference: -</strong> <a href=""https://learn.microsoft.com/en-us/dax/combinevalues-function-dax"" rel=""nofollow noreferrer"">COMBINEVALUES function (DAX) - DAX | Microsoft Learn</a></p>
<p>Other option to use user defined aggregations. Aggregations in PowerBI is used to improve the performance of large DirectQuery datasets.</p>
<p>Power BI desktop app has provision of “Manage Aggregations” to create aggregation based on tables.</p>
<p><strong>Reference: -</strong> <a href=""https://learn.microsoft.com/en-us/power-bi/transform-model/aggregations-advanced"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/power-bi/transform-model/aggregations-advanced</a></p>
"
"74816119","Azure Data Factory Global variables","<p>If I define a global variable at the management panel level.  Is that variable accessible by other concurrently running data factories?</p>
<p>I am looking into using a global as a validation flag as it will be available as a return value from child pipelines. And I do not want a concurrent data factory invocation to have scope into that variable.</p>
","<azure><azure-data-factory>","2022-12-15 18:40:45","480","0","2","74818624","<p>I believe you are referring to Global parameters in Azure Data factory. If that is the case, then the answer is <code>No</code>. Global parameters are constants across a particular Data Factory that can be consumed by a pipeline in any expression. You cannot use that global parameter in other data factories except the one in which it was created. But it can be referenced in other pipelines within the same data factory.</p>
"
"74816119","Azure Data Factory Global variables","<p>If I define a global variable at the management panel level.  Is that variable accessible by other concurrently running data factories?</p>
<p>I am looking into using a global as a validation flag as it will be available as a return value from child pipelines. And I do not want a concurrent data factory invocation to have scope into that variable.</p>
","<azure><azure-data-factory>","2022-12-15 18:40:45","480","0","2","74822146","<p>Adding to the <strong>@KranthiPakala-MSFT</strong>, Yes Global parameters are constant, and we cannot use a Global parameter of one ADF in another ADF.</p>
<p>But if you want to use it, store it in a file in blob using copy activity source additional column.</p>
<p>Then use this file value by lookup activity in another ADF workspace.</p>
<p>Or <strong>store it in a set variable</strong>.</p>
<p><img src=""https://i.imgur.com/wniSJZA.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/FbeL5zm.png"" alt=""enter image description here"" /></p>
<p>Run this pipeline and in another ADF get this activity run details by using the below <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/activity-runs/query-by-pipeline-run?tabs=HTTP"" rel=""nofollow noreferrer"">REST API</a> in web activity.</p>
<pre><code>https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelineruns/{runId}/queryActivityruns?api-version=2018-06-01
</code></pre>
"
"74815080","Error getting authentication to Azure data factory","<p>I need to hit the data factory pipeline with the REST call ...
So, I</p>
<ol>
<li>Registered application in the Azure Active Directory.</li>
<li>Obtained Client_ID and client_Secret</li>
</ol>
<p>Then I try to use Postman to get the authentication token and get an error, which I do not understand.  Please, help.  How can I get authentication token to my ADF using Postman ?</p>
<p><a href=""https://i.stack.imgur.com/soGx7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/soGx7.png"" alt=""enter image description here"" /></a></p>
","<rest><oauth-2.0><azure-data-factory>","2022-12-15 17:00:07","240","0","2","74818999","<p>In body section, instead of <code>raw</code>, use - <code>x-www-form-urlencoded</code> as below and it should help resolve the issue.</p>
<p><a href=""https://i.stack.imgur.com/4CQEn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4CQEn.png"" alt=""enter image description here"" /></a></p>
<p>Ideally, when you select <code>x-www-form-urlencoded</code> radio button in the <code>Body</code> section, it automatically sets the <code>Content-Type</code>=<code>application/x-www-form-urlencoded</code> under the Headers tab as shown below.</p>
<p><a href=""https://i.stack.imgur.com/Earzs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Earzs.png"" alt=""enter image description here"" /></a></p>
"
"74815080","Error getting authentication to Azure data factory","<p>I need to hit the data factory pipeline with the REST call ...
So, I</p>
<ol>
<li>Registered application in the Azure Active Directory.</li>
<li>Obtained Client_ID and client_Secret</li>
</ol>
<p>Then I try to use Postman to get the authentication token and get an error, which I do not understand.  Please, help.  How can I get authentication token to my ADF using Postman ?</p>
<p><a href=""https://i.stack.imgur.com/soGx7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/soGx7.png"" alt=""enter image description here"" /></a></p>
","<rest><oauth-2.0><azure-data-factory>","2022-12-15 17:00:07","240","0","2","74822980","<p><strong>Please follow below steps for generating bearer token In ADF :</strong></p>
<p><strong>Step 1:</strong> Create  a web activity to get the Access Token from ADF.</p>
<p><strong>URL:</strong> <code>https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/token</code></p>
<p><img src=""https://i.imgur.com/TvLHycd.png"" alt=""enter image description here"" /></p>
<ul>
<li><p><strong>Method :</strong> POST</p>
</li>
<li><p><strong>Body:</strong> <code>grant_type=client_credentials&amp;client_id=&lt;client_id&gt;&amp;client_secret=&lt;client_secret&gt;&amp;resource=https://management.azure.com/</code></p>
</li>
<li><p><strong>Header:</strong> <code>Content-Type:application/x-www-form-urlencoded</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/OvNyPIb.png"" alt=""enter image description here"" /></p>
<p><strong>Step2:</strong> Create Set variable :</p>
<ul>
<li>Add dynamic content -&gt;  <code>@activity('Web1').output.access_token</code></li>
</ul>
<p><img src=""https://i.imgur.com/apmfsvO.png"" alt=""enter image description here"" /></p>
<p><strong>Pipeline successfully executed and got the token</strong></p>
<p><img src=""https://i.imgur.com/wqzEB5U.png"" alt=""enter image description here"" /></p>
"
"74813503","How to take max value and replace it in drived colomn in data factory","<p>I am new to azure data factory. I need a help in the following implementation -</p>
<p>input:</p>
<p>Key,  col3</p>
<p>1,5</p>
<p>2,6</p>
<p>3,7</p>
<p>Desired output:</p>
<p>Key, col3</p>
<p>1,3</p>
<p>2,6</p>
<p>3,7</p>
<p>Here key=1, col3 value is replaced with 3 (maximum of key)</p>
<p>I have a requirement to replace one column data with maximum value of other column. I want to take the maximum of key column and replace the value in column3 with that max value. This should be done only for the row key=1.</p>
<p>I used derived column transformation and iif expression. But how to take maximum value?</p>
","<azure><azure-data-factory>","2022-12-15 15:00:26","133","0","1","74814695","<ul>
<li><p><strong>Max()</strong> or any aggregate functions cannot be used directly in derived column transformation. Therefore, use Window transformation and take the maximum of the column key. Use this value in derived column transformation.</p>
</li>
<li><p>I tried to repro this with sample data and below is the approach.</p>
</li>
<li><p>Sample source data is taken.
<img src=""https://i.imgur.com/ORaGipf.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p>img:1 Source data preview</p>
<ul>
<li>Window transformation is taken next to source transformation. Window columns is selected. New column named <strong>max_key</strong> is given as column name. Expression is <code>max(key)</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/fqw3uVP.png"" alt=""enter image description here"" /></p>
<p>img:2 Window Transformation settings</p>
<ul>
<li>maximum value of key at the entire table data is assigned to <strong>max_key</strong> column.</li>
</ul>
<p><img src=""https://i.imgur.com/SiZrqbD.png"" alt=""enter image description here"" /></p>
<p>img:3 Windows transformation data preview</p>
<ul>
<li>Derived column transformation is taken. Expression for the column <strong>mark</strong> is
<code>iif(key=='1',max_key,mark)</code></li>
</ul>
<p><img src=""https://i.imgur.com/Bg31iJu.png"" alt=""enter image description here"" /></p>
<p>img:4 Derived column transformation settings</p>
<ul>
<li>Here, I tried to assign the maximum value of key to mark where key=1 and remaining values of the column is assigned as original mark value.</li>
</ul>
<p><img src=""https://i.imgur.com/lBBVSlz.png"" alt=""enter image description here"" /></p>
<p>img:5 Derived Column transformation data preview.</p>
<ul>
<li>By this way you can use the maximum value in derived column transformation.</li>
</ul>
"
"74807222","Date provided for the Azure Data Factory pipeline should sink with the Azure Databricks Notebook query","<p>Whenever I am providing the date for the pipeline, it creates a new folder with the same date in ADLS. I can read the data by providing the hardcoded date, but when I again debug the pipeline it create new folder.</p>
<p>Problem - the date I have provided in the query is hardcoded, I need to make it flexible like whenever I debug the pipeline, the query automatically get the new folder.
<a href=""https://i.stack.imgur.com/uisKX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uisKX.jpg"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/nmdCt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nmdCt.jpg"" alt=""enter image description here"" /></a>
Attaching the screenshot for the better understanding.enter image description here</p>
","<scala><apache-spark><azure-data-factory><azure-databricks><azure-data-lake-gen2>","2022-12-15 05:40:47","86","0","1","74808810","<p>Please follow below approach. You can create a new folder automatically by using dynamic content.</p>
<p><strong>I tried to reproduce the same in my environment and got below results by using copy activity</strong>:</p>
<ol>
<li><p><em>Open sink dataset in the copy activity .</em>
<img src=""https://i.imgur.com/e8tO37c.png"" alt=""enter image description here"" /></p>
</li>
<li><p><em>Go to parameter and  create <strong>Parameter Name <code>folder</code></strong>.</em>
<img src=""https://i.imgur.com/tkV9Zr9.png"" alt=""enter image description here"" /></p>
</li>
<li><p><em>Go to connection , Add dynamic content <code>@dataset().folder</code> in the folder path .</em></p>
</li>
</ol>
<p><img src=""https://i.imgur.com/PFccP2N.png"" alt=""enter image description here"" /></p>
<ol start=""4"">
<li><em>when you created a parameter name, the same name will be displayed in dataset properties as shown below image.</em></li>
</ol>
<ul>
<li><em>Add dynamic content inside Value:</em> <code>@concat(formatDateTime(utcnow(), 'yyyy/MM/DD'))</code></li>
</ul>
<p><img src=""https://i.imgur.com/8k9wLbm.png"" alt=""enter image description here"" /></p>
<p><em><strong>Now , you can check Pipeline successfully executed and got the output:</strong></em></p>
<p><img src=""https://i.imgur.com/cCYz59U.png"" alt=""enter image description here"" /></p>
"
"74806001","Azure Data Factory - Web activity with header value containing comma failing","<p>We are trying to ingest data from Amazon Selling Partner API.  We currently have an azure function that signs our request and returns the header values.  The authorization header contains commas in it and the comma causes the activity to fail on the client side.</p>
<p>Here is an exampleof the authorization header:</p>
<pre><code>AWS4-HMAC-SHA256 Credential=AKIAIOSFODNN7EXAMPLE/20130524/us-east-1/s3/aws4_request,SignedHeaders=date;host;x-amz-content-sha256;x-amz-date;x-amz-storage-class,Signature=98ad721746da40c64f1a55b78f14c238d841ea1380cd77a1b5971af0ece108bd
</code></pre>
<p>To reproduce, create a new pipeline and add a web activity.  Enter &quot;http://www.google.com&quot; for the url with the method GET.  Add a header like above.
<a href=""https://i.stack.imgur.com/IEnha.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IEnha.png"" alt=""enter image description here"" /></a></p>
<p>You should get the following error:</p>
<pre><code>Error calling the endpoint 'http://www.google.com'. Response status code: 'NA - Unknown'. More details: Exception message: 'NA - Unknown [ClientSideException] The format of value 'AWS4-HMAC-SHA256 Credential=AKIAIOSFODNN7EXAMPLE/20130524/us-east-1/s3/aws4_request,SignedHeaders=date;host;x-amz-content-sha256;x-amz-date;x-amz-storage-class,Signature=98ad721746da40c64f1a55b78f14c238d841ea1380cd77a1b5971af0ece108bd' is invalid.'.

Request didn't reach the server from the client. This could happen because of an underlying issue such as network connectivity, a DNS failure, a server certificate validation or a timeout. 
</code></pre>
<p>Any work arounds for this?
Thanks,
Scott</p>
","<azure-data-factory>","2022-12-15 01:40:01","179","1","1","74807913","<p>If you are using Authorization as a header to get data make sure to use bearer in front of the Token.</p>
<blockquote>
<p><strong>Authorization</strong>    <code>Bearer &lt; Access Token &gt;</code></p>
</blockquote>
<p><em><strong>I tried to reproduce the same in my environment and got below results</strong></em>:</p>
<p><img src=""https://i.imgur.com/gopfJkQ.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/xnLyXky.png"" alt=""enter image description here"" /></p>
<p>For more information <strong>refer</strong> this <a href=""https://stackoverflow.com/questions/73760418/how-to-pass-bearer-token-api-in-azure-data-factory/73763796#73763796"">SO</a> thread .</p>
"
"74804913","Azure data factory Dynamic Content","<p>I have the following output from a web activity .</p>
<pre><code>{
    &quot;value&quot;: [
        {
            &quot;id&quot;: &quot;/subscriptions/xy_csv&quot;,
            &quot;name&quot;: &quot;xy_csv&quot;,
            &quot;type&quot;: &quot;Microsoft.code&quot;,
            &quot;etag&quot;: &quot;6200&quot;,
            &quot;properties&quot;: {
                &quot;folder&quot;: {
                    &quot;name&quot;: &quot;samplecodes&quot;
                },
                &quot;content&quot;: {
                    &quot;query&quot;: &quot;select * from table 1&quot;,
                    &quot;metadata&quot;: {
                        &quot;language&quot;: &quot;sql&quot;
                    },
                    &quot;currentConnection&quot;: {
                        &quot;databaseName&quot;: &quot;demo&quot;,
                        &quot;poolName&quot;: &quot;Built-in&quot;
                    },
                    &quot;resultLimit&quot;: 5000
                },
                &quot;type&quot;: &quot;SqlQuery&quot;
            }
        },
        {
            &quot;id&quot;: &quot;/subscriptions/ab_csv&quot;,
            &quot;name&quot;: &quot;ab_csv&quot;,
            &quot;type&quot;: &quot;Microsoft.code&quot;,
            &quot;etag&quot;: &quot;6200&quot;,
            &quot;properties&quot;: {
                &quot;folder&quot;: {
                    &quot;name&quot;: &quot;livecode&quot;
                },
                &quot;content&quot;: {
                    &quot;query&quot;: &quot;select * from table 2&quot;,
                    &quot;metadata&quot;: {
                        &quot;language&quot;: &quot;sql&quot;
                    },
                    &quot;currentConnection&quot;: {
                        &quot;databaseName&quot;: &quot;demo&quot;,
                        &quot;poolName&quot;: &quot;Built-in&quot;
                    },
                    &quot;resultLimit&quot;: 5000
                },
                &quot;type&quot;: &quot;SqlQuery&quot;
            }
        }
]
</code></pre>
<p>I would like to create filter activity after the web activity just to filter out items that are saved under the folder name &quot;livecode&quot;.</p>
<p>On the filter activity item field I have -<em><strong>@activity('Web1').output.value</strong></em></p>
<p>On the condition field I have -- <em><strong>@startswith(item().properties.folder.name,'livecode')</strong></em></p>
<p>The web activity is successful but the filter activity is failed with this error.</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;InvalidTemplate&quot;,
    &quot;message&quot;: &quot;The execution of template action 'FilterFilter1' failed: The evaluation of 'query' action 'where' expression '@startswith(item().properties.folder.name,'sql')' failed: 'The expression 'startswith(item().properties.folder.name,'sql')' cannot be evaluated because property 'folder' doesn't exist, available properties are 'content, type'.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Filter1&quot;,
    &quot;details&quot;: &quot;&quot;
}
</code></pre>
<p>it feels like I am going wrong on how i have written the Condition Dynamic Content filter to navigate to properties.folder.name. I am not sure what is missing in my condition. Can anyone help? thanks Much appreciated.</p>
","<azure-data-factory><dynamic-expression>","2022-12-14 22:38:34","174","0","1","74807954","<p>The error is because of the web activity output's <code>properties</code> object might not contain the <code>folder</code> sometimes.</p>
<ul>
<li>I have taken the following json and got the same error:</li>
</ul>
<pre><code>{
   &quot;value&quot;:[
      {
         &quot;id&quot;:&quot;/subscriptions/xy_csv&quot;,
         &quot;name&quot;:&quot;xy_csv&quot;,
         &quot;type&quot;:&quot;Microsoft.code&quot;,
         &quot;etag&quot;:&quot;6200&quot;,
         &quot;properties&quot;:{
            &quot;content&quot;:{
               &quot;query&quot;:&quot;select * from table 1&quot;,
               &quot;metadata&quot;:{
                  &quot;language&quot;:&quot;sql&quot;
               },
               &quot;currentConnection&quot;:{
                  &quot;databaseName&quot;:&quot;demo&quot;,
                  &quot;poolName&quot;:&quot;Built-in&quot;
               },
               &quot;resultLimit&quot;:5000
            },
            &quot;type&quot;:&quot;SqlQuery&quot;
         }
      },
      {
         &quot;id&quot;:&quot;/subscriptions/ab_csv&quot;,
         &quot;name&quot;:&quot;ab_csv&quot;,
         &quot;type&quot;:&quot;Microsoft.code&quot;,
         &quot;etag&quot;:&quot;6200&quot;,
         &quot;properties&quot;:{
            &quot;folder&quot;:{
               &quot;name&quot;:&quot;livecode&quot;
            },
            &quot;content&quot;:{
               &quot;query&quot;:&quot;select * from table 2&quot;,
               &quot;metadata&quot;:{
                  &quot;language&quot;:&quot;sql&quot;
               },
               &quot;currentConnection&quot;:{
                  &quot;databaseName&quot;:&quot;demo&quot;,
                  &quot;poolName&quot;:&quot;Built-in&quot;
               },
               &quot;resultLimit&quot;:5000
            },
            &quot;type&quot;:&quot;SqlQuery&quot;
         }
      }
   ]
}
</code></pre>
<p><img src=""https://i.imgur.com/oG2UEKZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, you have to modify the filter condition to check whether it contains a <code>folder</code> key or not using the following dynamic content. I have taken your web activity output as a parameter value and took out folder key from <code>properties</code> object:</li>
</ul>
<pre><code>@startswith(if(contains(item().properties,'folder'),item().properties.folder.name,''),'livecode')
</code></pre>
<p><img src=""https://i.imgur.com/Lg2hLXs.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I debug the pipeline, we get desired result:</li>
</ul>
<p><img src=""https://i.imgur.com/vFnPbyo.png"" alt=""enter image description here"" /></p>
"
"74799740","Email errors from all activities in ADF at once","<p>In Azure Data Factory I created a pipeline with several copy data activities. As soon as one of them crashes I want that it continues to the next step and that it sends an e-mail with the error.</p>
<p>The mail error action is correct. When I link it to just one activity and let that one crash on purpose it does send the mail correctly. But when I link it to multiple and it crashes (even the last step) it doesn't send an e-mail.</p>
<p>What am I doing wrong here?</p>
<p><a href=""https://i.stack.imgur.com/GU6rA.png"" rel=""nofollow noreferrer"">see image</a></p>
<p>The e-mail step is quite simple, it's set that it triggers an Logic App which sends the actual mail via office365.</p>
","<azure><azure-data-factory><azure-logic-apps><azure-synapse>","2022-12-14 14:25:25","308","0","2","74801996","<p>In adf, the flows act as AND condition. So based on your above image, an email will be sent only when all activities tagged to the web activity will fail.
So to achieve your scenario, you would have to use a combination of on skip, on completion and on failure flows.
Reference blog:
<a href=""https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2021/08/19/error-logging-and-the-art-of-avoiding-redundant-activities-in-azure-data-factory</a></p>
"
"74799740","Email errors from all activities in ADF at once","<p>In Azure Data Factory I created a pipeline with several copy data activities. As soon as one of them crashes I want that it continues to the next step and that it sends an e-mail with the error.</p>
<p>The mail error action is correct. When I link it to just one activity and let that one crash on purpose it does send the mail correctly. But when I link it to multiple and it crashes (even the last step) it doesn't send an e-mail.</p>
<p>What am I doing wrong here?</p>
<p><a href=""https://i.stack.imgur.com/GU6rA.png"" rel=""nofollow noreferrer"">see image</a></p>
<p>The e-mail step is quite simple, it's set that it triggers an Logic App which sends the actual mail via office365.</p>
","<azure><azure-data-factory><azure-logic-apps><azure-synapse>","2022-12-14 14:25:25","308","0","2","74809865","<blockquote>
<p>As soon as one of them crashes I want that it continues to the next step and that it sends an e-mail with the error.</p>
</blockquote>
<p>All the connections marked &quot;on failure&quot; are the problem. Although what you created makes logical. When an activity receives connections from several dependents, it will not begin to run until all of the dependencies have reported in. for this you can use pipeline dependency called <code>on completion</code> to run next activity even if previous activity is failed.</p>
<p>And another problem in your approach is if one activity is executed successfully and you are trying to fetch error from that in web activity msg It will throw an error</p>
<p>You have to give web activity to each activity to send mail if they fail and give On Completion to all activities It will run all activities irrespective previous activity is failing or not.</p>
<p><img src=""https://i.imgur.com/9YJE26c.png"" alt=""enter image description here"" /></p>
<p>Or <strong>If your main intension is to get mail if activities failed in pipeline</strong> you could create alert on particular pipeline with all activities with <code>Failed activity runs metrics</code> It would send you mail if any activity failed in that particular pipeline</p>
<p><img src=""https://i.imgur.com/xSxTlFz.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/KQ3uC2E.png"" alt=""enter image description here"" /></p>
"
"74797337","How to get creation datetime of a blob file from Azuredatafactory or Azure Dataflow","<p><a href=""https://i.stack.imgur.com/f1pIM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f1pIM.png"" alt=""enter image description here"" /></a>I can fetch lastmodified file from get metadata activity but the requirement is the creation date of the file .I can see no option to fetch that in Azure Data Factory . Besides I need to fetch:</p>
<pre><code>First row from   fileCreationDateTime --- Descending Order. So i need the CreationDateTime functionality as LastModified .
</code></pre>
<p>As shown in the picture , there are two things CREATION TIME and LAST MODIFIED . So i need to fetch all the creationtimes and sort it to decending and pick the first row.</p>
","<azure><azure-data-factory>","2022-12-14 11:09:42","330","0","1","74804161","<p>You can use Web Activity in ADF and make a call to <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/get-blob"" rel=""nofollow noreferrer"">Get Blob REST API</a> which will return the blob creation date in Web activity Response headers section, and you can use that value as per your requirement.</p>
<p>Get Blob REST API sample URI: <a href=""https://myaccount.blob.core.windows.net/mycontainer/myblob"" rel=""nofollow noreferrer"">https://myaccount.blob.core.windows.net/mycontainer/myblob</a></p>
<p>Ref document: <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/get-blob#response-headers"" rel=""nofollow noreferrer"">Get Blob - Response Headers</a></p>
<p><a href=""https://i.stack.imgur.com/7Iuhn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Iuhn.png"" alt=""enter image description here"" /></a></p>
<p>You can capture the value of Blob creation datetime from the REST API response headers in ADF using below dynamic expression:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@activity('WebGetBlobDetails').output.ADFWebActivityResponseHeaders['x-ms-creation-time']</code></pre>
</div>
</div>
</p>
<p>Here is a sample demo.  I have used the hardcoded bearer token in the headers for a quick demo, but you can automate and pass dynamically by having another web activity to get the bearer token before <code>WebGetBlobDetails</code> activity.</p>
<p>Sample GIF:</p>
<p><a href=""https://i.stack.imgur.com/HvV6e.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HvV6e.gif"" alt=""enter image description here"" /></a></p>
"
"74795084","Incremental Load without Last Modified Date and Primary Key field in Azure Data Factory","<p>I am trying to do incremental load in azure data factory. Most of the tables in the database doesn't have last modified date column. I don't have rights to add watermark columns in the tables. Is there any way to do incremental loading without last modified date and primary key column?</p>
<p>I don't know which approach I can use. so kindly help me. Thanks in advance.</p>
","<azure><azure-data-factory><etl>","2022-12-14 07:55:40","479","0","1","74804313","<p>If you source database support native Change Data Capture feature, then you can use ADF Mapping data flow with no timestamp or ID columns are required to identify the changes since it uses the native change data capture technology in the databases.</p>
<p>For complete demonstration, please refer to this public documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-change-data-capture"" rel=""nofollow noreferrer"">Change data capture in Azure Data Factory and Azure Synapse Analytics</a></p>
<p>Another possible approach is if you can access both old data (Previously loaded data to your sink) and latest data (source) with changes then you can use mapping data flow in ADF and implement hashing to compare the both the datasets and pick the changed data as per your requirement.</p>
<p>You can refer to this demonstration for the same implementation: <a href=""https://www.youtube.com/watch?v=GACpvMjOJgE"" rel=""nofollow noreferrer"">Data Flows: How to capture changed data</a></p>
"
"74791413","Data factory - copy data with mongodb","<p>I have the following doubt in a datafactory pipeline, I have the cosmosbdmongobd connection and I am trying to know how many documents I have before copying to the datalake but I have not been able to do the filter in data factory, in visual studio I know what to do this way db.rips.find().count(); but in data factory this doesn't work for me, is there a way to do it from the pipeline?</p>
<p><img src=""https://i.stack.imgur.com/5eej6.png"" alt=""enter image description here"" /></p>
","<mongodb><azure-data-factory>","2022-12-13 22:09:52","129","0","1","74794472","<blockquote>
<p>I am trying to know how many documents I have before copying to the datalake.</p>
</blockquote>
<p>There is no proper way to query and fond the no of document in collection from ADF.</p>
<p>And the <code>Lookup</code>, <code>Get metadata</code> activity which can be used to retrieve metadata from database or files are also not supported for <code>Cosmos DB MongoDB API</code>.</p>
<p><strong>Filter</strong> option in <code>Cosmos DB MongoDB API</code> is mainly used to filter the documents based on specific value or condition.</p>
<blockquote>
<p>Specifies selection filter using query operators. To return all documents in a collection.</p>
</blockquote>
<p>The workaround can be possible by upserting data in same collection and get the data read from source.</p>
<p><img src=""https://i.imgur.com/tbVzQHF.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/bctU6tk.png"" alt=""enter image description here"" /></p>
<p>set variable to get read rows from output of copy activity.</p>
<p><img src=""https://i.imgur.com/LvFBklD.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/vHrvs7v.png"" alt=""enter image description here"" /></p>
<p>Then you can perform your copy activity to copy data from <code>Cosmos DB MongoDB API</code> to <code>data lake</code>.</p>
"
"74784598","How do I create a copy activity for a file whose name keeps changing?","<p>I have to copy a file from an HTTP source to Azure Blob Storage (ABS) using a copy activity in Azure Data Factory (ADF).</p>
<p>The fully-qualified path to the file has a date-stamp in it, so it keeps changing (e.g., <code>http://www.example.com/files/2022-12-13.zip</code>). Further I want to expand it into a directory in ABS that is also named based on the date (e.g., <code>&lt;blob&gt;/2022-12-13/</code>).</p>
<p>Is there a way to do this in ADF (preferably one that doesn't involve writing code)?</p>
","<azure><azure-data-factory>","2022-12-13 12:04:03","166","1","2","74786536","<p>I had a similar requirement lately, and ended up solving this with code. You can either use an azure function to get the list of files from your data lake folder, or use a Synapse Notebook. Based on your requirements, you can select the latest/earliest/some other criterion in that specific blob --&gt; folder. Here's how I did it:</p>
<pre><code># Use DataLakeServiceClient class from ADLS2 data lake API. 
# Can probably use similar API for blob storage. 
from azure.storage.filedatalake import DataLakeServiceClient


# Function that initializes a connection to the data lake
def initialize_storage_account_connection(storage_account_name, storage_account_key):
    service_client = DataLakeServiceClient(account_url=f&quot;https://{storage_account_name}.dfs.core.windows.net&quot;,
                                           credential=storage_account_key)
    return service_client


# Function that returns the file paths of files in a certain folder
def list_directory_contents():
    # Initialize a file system client for blob container &quot;raw&quot;
    file_system_client = service_client.get_file_system_client(file_system=&quot;raw&quot;)
    # Get the path objects of respective parquet files in specific table folders under the &quot;raw&quot; blob container
    paths = file_system_client.get_paths(path=path_to_folder)
    # Parse paths into a proper list
    path_list = [path.name for path in paths]
    return path_list


# Function that determines the most recent change file (I Needed most recent file but perhaps adapt according to needs)
def get_most_recent_timestamp():
    # Example of a path: 'change_data/test_table/changes_test_table2022-10-13T17:57:30.parquet'
    # Determine prefix length of path that has to be stripped away (for example: &quot;change_data/test_table/changes_test_table&quot; has a length of 41)
    prefix_length = len(path_to_change_table) + len('changes_') + len(table_name) + 1
    # Determine suffix length of path that has to be stripped away
    suffix_length = len('.parquet')
    # Strip away prefix and suffix for each path so only the timestamp remains. In example, only 2022-10-13T17:57:30 would remain.
    # Do this for all paths in directory_path_list
    time_stamp_list = [i[prefix_length:-suffix_length] for i in directory_path_list]
    # Sort the time stamps
    sorted_time_stamp_list = sorted(time_stamp_list, reverse=True)
    # Get and return most recent timestamp
    most_recent_timestamp = sorted_time_stamp_list[0]
    return most_recent_timestamp
</code></pre>
<p>And then just call the function:</p>
<pre><code>path_to_change_table = f'change_data/{table_name}'
#TODO: get key from key-vault or use a managed identity
service_client = initialize_storage_account_connection('your_sa', 'your_sa_key')
directory_path_list = list_directory_contents()
most_recent_timestamp = get_most_recent_timestamp()
print(most_recent_timestamp)
</code></pre>
"
"74784598","How do I create a copy activity for a file whose name keeps changing?","<p>I have to copy a file from an HTTP source to Azure Blob Storage (ABS) using a copy activity in Azure Data Factory (ADF).</p>
<p>The fully-qualified path to the file has a date-stamp in it, so it keeps changing (e.g., <code>http://www.example.com/files/2022-12-13.zip</code>). Further I want to expand it into a directory in ABS that is also named based on the date (e.g., <code>&lt;blob&gt;/2022-12-13/</code>).</p>
<p>Is there a way to do this in ADF (preferably one that doesn't involve writing code)?</p>
","<azure><azure-data-factory>","2022-12-13 12:04:03","166","1","2","74793889","<p>Since, your source is HTTP, you can build the URL dynamically like <code>http://www.example.com/files/yyyy-MM-dd.zip</code> where <code>yyyy-MM-dd</code> is today's date.</p>
<ul>
<li>Using <code>copy data</code> activity, create a source dataset for <code>HTTP</code> source with configurations set as below. Give the base URL as <code>http://www.example.com/files/</code> and relative URL as shown below:</li>
</ul>
<pre><code>@{formatDateTime(utcNow(),'yyyy-MM-dd')}.zip
</code></pre>
<p><img src=""https://i.imgur.com/hz7tffl.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Don't select <code>Preserve zip file name as folder</code> option.</p>
</li>
<li><p>Now for sink, you can create your dataset for blob storage. Since you want to store it in a folder with <code>yyyy-MM-dd</code>, configure your blob storage sink dataset as shown below:</p>
</li>
</ul>
<pre><code>#folder structure here would be &quot;op/yyyy-MM-dd/&quot; 
#remove or add before the dynamic content to define your folder structure.
op/@{formatDateTime(utcNow(),'yyyy-MM-dd')}
</code></pre>
<p><img src=""https://i.imgur.com/ASjs7C4.png"" alt=""enter image description here"" /></p>
<ul>
<li>Also change the file extension as required in sink settings:</li>
</ul>
<p><img src=""https://i.imgur.com/4fDoIK1.png"" alt=""enter image description here"" /></p>
"
"74783617","Ecodage Data factory DelimitedText source","<p>I have data factory pipeline with source (DelimitedText) on SFTP with ecodage ISO-8859-13, it was working without any problems with the special characetrs, but yesterday it's blocked with many errors, with the special characters,
Have you any solution for this problem ? whitch kind of encodage you use on ADF to read special characters (like this: commerçant)</p>
<p>Best regards,</p>
<p>I try to copy data with data factory pipeline</p>
","<azure><encoding><dataset><azure-data-factory>","2022-12-13 10:48:47","32","0","1","74791911","<p>ADF delimited text format supports below encoding types to read/write files.</p>
<p><a href=""https://i.stack.imgur.com/aoOjK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aoOjK.png"" alt=""enter image description here"" /></a></p>
<p>Ref doc:  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delimited-text#dataset-properties"" rel=""nofollow noreferrer"">ADF Delimited format dataset properties</a></p>
<p>I just tried with default encoding type <code>UTF-8</code> for the special character you have provided (like this: commerçant) and it works fine without any issue.</p>
<p><a href=""https://i.stack.imgur.com/xeYpF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xeYpF.png"" alt=""enter image description here"" /></a></p>
<p>I have also tried with encoding type ISO-8859-13 and didn't notice any issue for the character type you have provided.</p>
<p>Hence assuming some other special characters might be causing the problem. But I would recommend trying to default <code>UTF-8</code> and see if that helps. In case if the issue still persists, then try using the Skip incompatible rows, which will help filter down the bad records and log them to a log file from where you can get the exact rows which are causing the pipeline to fail and based on those records you can choose the encoding type in your dataset settings.</p>
"
"74780249","In Azure Data Factory I have Created self hosted IR. But from few days it's status is showing as Running(Limited). Can I know the reason Why?","<p>In Azure Data Factory I have Created self-hosted IR. But from few days its status is showing as Running (Limited). I have Three nodes in it. Can I know the reason why its status is showing as Running (Limited).</p>
<p><a href=""https://i.stack.imgur.com/bMLo6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bMLo6.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/yqk6F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yqk6F.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><ssis-2012>","2022-12-13 05:10:59","126","0","1","74784777","<p>Nodes may be unable to communicate with each other, resulting in this error.</p>
<p>Go to particular node and get into <strong>Integration runtime &gt;&gt; Diagnostic &gt;&gt; view logs</strong></p>
<p><img src=""https://i.imgur.com/QWpn5nf.png"" alt=""enter image description here"" /></p>
<p>now you will be redirected to event viewer and filter the error logs.</p>
<p><img src=""https://i.imgur.com/FmK8cot.png"" alt=""enter image description here"" /></p>
<p>If there's an <code>System.ServiceModel.EndpointNotFoundException</code> and <code>System.Net.Sockets.SocketException</code>  these errors contain in above error logs.</p>
<p>run the following command in a Command Prompt window:</p>
<pre><code>telnet 10.2.4.10 8060
</code></pre>
<p>As <a href=""https://learn.microsoft.com/en-us/azure/data-factory/self-hosted-integration-runtime-troubleshoot-guide?tabs=data-factory#error-message-self-hosted-integration-runtime-nodelogical-self-hosted-ir-is-in-inactive-running-limited-state"" rel=""nofollow noreferrer"">Microsoft official Documentation</a> states,</p>
<blockquote>
<p><strong>To resolve this, make sure that all the nodes in your domain are using the same IP address, and you will need to add the IP address to the host files for all the hosted virtual machines.</strong></p>
</blockquote>
"
"74779690","Right anti join in Dataflow","<p>Is it possible to perform right anti join in Dataflow?</p>
<p>I can see joins in Dataflow but I didn't find right anti join .</p>
<p>Any help is appreciated</p>
","<azure><azure-data-factory>","2022-12-13 03:32:39","113","0","1","74783925","<p><strong>AFAIK</strong>, <strong>Currently Anti joins are not supported in Data flows</strong>. Only the following joins are supported.</p>
<p><img src=""https://i.imgur.com/WNYq7WO.png"" alt=""enter image description here"" /></p>
<p>If your Source tables are from SQL, its good if you perform a stored procedure at source for Right anti join in Dataflow.</p>
<p>If not, you can try the below workaround using <strong>Exists transformation</strong>.</p>
<p><strong>Exists transformation</strong> takes left and Right streams and <strong>gives the records of left records which are not in Right stream</strong>.(<em><strong>Left Anti join</strong></em>)</p>
<p>To achieve Right anti join, you can change the incoming streams.</p>
<p><strong>Sample demo:</strong></p>
<p>Left source data:</p>
<p><img src=""https://i.imgur.com/bUDzjo6.png"" alt=""enter image description here"" /></p>
<p>Right source data:</p>
<p><img src=""https://i.imgur.com/IK1Rh7D.png"" alt=""enter image description here"" /></p>
<p><strong>Exists transformation:</strong></p>
<p><img src=""https://i.imgur.com/VfxSPWd.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/xGibDc7.png"" alt=""enter image description here"" /></p>
"
"74777793","If condition while joining data in ADF","<p>Im tyring to write expression that</p>
<p>check for Column_A - If its ABCD then the join will be on DepartmentName and isActive = 1</p>
<p>if Column_A is other than ABCD then join will be on DepartmentCode and isActive =1</p>
<p>is there any way this can be accomplished in ADF</p>
<p>I have tried to derive nested iif statement but failing to accomplish above task</p>
","<azure-data-factory>","2022-12-12 21:58:32","68","0","2","74783907","<p>Use case expression in matching condition of join transformation <code>case(column_name=='ABCD',Department_name,Department_code)</code>
to achieve your requirement.</p>
<p>I tried to repro this with sample inputs. Below is the approach.</p>
<ul>
<li>Source 1 and source2 are taken as in below images respectively.</li>
</ul>
<p><img src=""https://i.imgur.com/wyCnjDk.png"" alt=""enter image description here"" />
img1: Source 1 data</p>
<p><img src=""https://i.imgur.com/OZ7Jx69.png"" alt=""enter image description here"" />
img2: source 2 data.</p>
<ul>
<li>Join transformation is added and matching condition is given as
<code>case(column_name=='ABCD',Department_name,Department_code) == Department</code>
(Here, inner join type is used)</li>
</ul>
<p><img src=""https://i.imgur.com/NbAtD3C.png"" alt=""enter image description here"" />
img 3:  Join transformation settings</p>
<ul>
<li>Then filter transformation is used to filter only the records with isactive=1.</li>
</ul>
<p><img src=""https://i.imgur.com/vrZAMOf.png"" alt=""enter image description here"" />
img 4: filter transformation settings.</p>
<p><strong>Output Data:</strong></p>
<p><img src=""https://i.imgur.com/b7Gxhye.png"" alt=""enter image description here"" />
img5: Output data after join and filter transformation.</p>
"
"74777793","If condition while joining data in ADF","<p>Im tyring to write expression that</p>
<p>check for Column_A - If its ABCD then the join will be on DepartmentName and isActive = 1</p>
<p>if Column_A is other than ABCD then join will be on DepartmentCode and isActive =1</p>
<p>is there any way this can be accomplished in ADF</p>
<p>I have tried to derive nested iif statement but failing to accomplish above task</p>
","<azure-data-factory>","2022-12-12 21:58:32","68","0","2","74813595","<p>I ended up using split function to split data for the conditino mentioned and then finally joining it back</p>
"
"74777605","DataFlow Query Not Replacing Variable","<p>I have a workable sql query</p>
<pre><code> ... where AcctPD &gt;= '@{variables('StartDate')}'
</code></pre>
<p>and want to use a variable such as <code>StartDate</code> value of <code>202211</code>, a <code>varchar(6)</code>.</p>
<p>If I put <code>... where AcctPD &gt;= '202211'</code> in the sql, the query works, and data is returned. But if I run the above for a variable substitution, I get</p>
<blockquote>
<p>at Source 'XXXXx': Parse error at line: 36, column: 41: Incorrect syntax near 'StartDate'.
SQL Server error encountered while reading from the given table or
while executing the given query.</p>
</blockquote>
<p>Why?</p>
<hr />
<p><a href=""https://i.stack.imgur.com/A2LM7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A2LM7.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-12-12 21:36:26","153","0","1","74779841","<ul>
<li><p>ADF pipeline variables and pipeline parameters cannot be used directly in dataflow. In order to use the value of pipeline variable, a dataflow parameter is to be created and value of the pipeline variable should be passed to that dataflow parameter. I tried to repro this.</p>
</li>
<li><p>Pipeline variable <code>StartDate</code> is created and value is assigned.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/cUeqJbi.png"" alt=""enter image description here"" /></p>
<ul>
<li>Dataflow Query is run without using the variable. Data is read from the SQL table without any error.</li>
</ul>
<p><img src=""https://user-images.githubusercontent.com/113445679/207209400-d018c26b-f3ae-4091-839c-a715c7f8720c.gif"" alt=""gif53"" /></p>
<ul>
<li><p>When variable is given in the Query, same error is produced.
<img src=""https://user-images.githubusercontent.com/113445679/207199100-43e120fd-1bd5-4e41-8541-1e61e1142981.gif"" alt=""gif51"" /></p>
</li>
<li><p>In order to solve this, dataflow parameter named  <code>par_StartDate</code> is created.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/L7sqi1N.png"" alt=""enter image description here"" /></p>
<ul>
<li>In Source transformation Query, open expression builder is selected.</li>
</ul>
<p><img src=""https://i.imgur.com/v4rjAfN.png"" alt=""enter image description here"" /></p>
<ul>
<li>Query is written as <code>&quot;select * from Target_merged_table where createdat='{$par_StartDate}'&quot;</code> in dataflow expression builder.</li>
</ul>
<p><img src=""https://i.imgur.com/lhZlYkA.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/lhZlYkA.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Sink transformation is added with csv file as a sink dataset.</p>
</li>
<li><p>This dataflow is added in dataflow activity of pipeline. Value of pipeline variable <code>StartDate</code> is passed to dataflow parameter <code>par_StartDate</code>.
<img src=""https://user-images.githubusercontent.com/113445679/207207956-ddba49e9-4b93-41f5-811f-b4ee6008b173.gif"" alt=""gif51"" /></p>
</li>
<li><p>When pipeline is run, dataflow is executed successfully.</p>
</li>
</ul>
"
"74767594","Azure Data Factory: Pagination in Data Flow with Rest API Source","<p>I have a API source in an ADF DataFlow task. The API source gives me the current page and the toatl number of pages in the body of the response. I want to use that information to paginate through my API source. I'm able to paginate through it just fine outside of a DataFlow activity using the range function. The issue is that the Rest transformation in a DataFlow activity does not support the range function. I've been trying to use the AbsoluteUrl function plus an expression to do add one to the current page returned by the body but either pagination does not accept expressions or I cannot figure out the syntax</p>
<p>I have a url like this:</p>
<p>BaseURL/fabricationcodes?facets=relatedArticles:Not%20Empty&amp;page={PageNumber}.</p>
<p>In this example my rest linked service URL has everything I need minus the &amp;page=pageNumber. So I'm trying to add that part with the key/value pair function of AbsoluteUrl. The Key being &amp;page= and the value should be currentPage +1. My desire is for it to get the first page, page 0 and then add +1 to that to formulate the next pages url. the end condition being when body.totalPages == body.currentPage</p>
<p>I've tried a bunch of different expression formaulations but none seem to work and debugging in a Data flow is tough b/c the logging and error messaging is poor
<a href=""https://i.stack.imgur.com/fMkKP.png"" rel=""nofollow noreferrer"">What I have right now</a>.</p>
","<azure-data-factory>","2022-12-12 06:57:10","300","0","1","74772531","<p>As data flow don't support Range option or you cannot use dynamic expression to get page from API response.</p>
<p>To work around the issue, you can use Data Flow activity within ForEach loop using range function in dynamic expression.</p>
<p>First take a web activity and pass the URL of the Rest API as below Ito get the total no of Pages from API response
<img src=""https://i.imgur.com/AmF3Xu8.png"" alt=""enter image description here"" /></p>
<p>then take a for each activity to iterate on API like pagination give the Dynamic expression as <code>@range(1,activity('Web1').output.total_pages)</code></p>
<p><img src=""https://i.imgur.com/OXrH43n.png"" alt=""enter image description here"" /></p>
<p>I will iterate the API till the respective range in sequential manner.</p>
<p>create parameter with type string in source DataSource.</p>
<p><img src=""https://i.imgur.com/vgtOJhE.png"" alt=""enter image description here"" /></p>
<p>give that parameter as dynamic value in relative URL.</p>
<p><img src=""https://i.imgur.com/M163FHk.png"" alt=""enter image description here"" /></p>
<p>after this gave parameter value as <code>?page=@{item()}</code> to give the no coming from range to the page.</p>
<p><img src=""https://i.imgur.com/EIXmfP2.png"" alt=""enter image description here"" /></p>
<p><strong>OUTPUT:</strong></p>
<p><img src=""https://i.imgur.com/pfunPh6.png"" alt=""enter image description here"" /></p>
"
"74767515","ADF prod object pipeline parametrization not working","<p>I have created an object param in the dev environment in my ADF instance like {&quot;SuperSet&quot;:&quot;SuperSet/SuperSet.csv&quot;}, and calling the object like <code>pipeline().parameters.DWH.SuperSet</code> it is working fine, but when deployed on production instance using Azure CI/CD, it gives below mentioned error when triggered on set time.</p>
<p><code>Operation on target SuperSet failed: The expression 'pipeline().parameters.DWH.SuperSet' cannot be evaluated because property 'SuperSet' cannot be selected. Property selection is not supported on values of type 'String'.</code></p>
<p>my arm-templete-parameters-definition.json file has:</p>
<pre><code>&quot;Microsoft.DataFactory/factories/pipelines&quot;: {
    &quot;properties&quot;: {
      &quot;activities&quot;: [
        {
          &quot;policy&quot;: {
            &quot;retry&quot;: &quot;-&quot;,
            &quot;retryIntervalInSeconds&quot;: &quot;-&quot;
          }
        }
      ],
      &quot;parameters&quot;: {
        &quot;*&quot;: {
          &quot;defaultValue&quot;: &quot;-::string&quot;
        }
      }
    }
  }
</code></pre>
","<azure><azure-data-factory>","2022-12-12 06:49:00","77","0","1","74775877","<p>Had to update arm-templete-parameters-definition.json, and it worked.</p>
<pre><code>&quot;Microsoft.DataFactory/factories/pipelines&quot;: {
    &quot;properties&quot;: {
      &quot;activities&quot;: [
        {
          &quot;policy&quot;: {
            &quot;retry&quot;: &quot;-&quot;,
            &quot;retryIntervalInSeconds&quot;: &quot;-&quot;
          }
        }
      ],
      &quot;parameters&quot;: {
        &quot;*&quot;: {
          &quot;defaultValue&quot;: &quot;-::string&quot;
        },
        &quot;DWH&quot;: {
          &quot;type&quot;: &quot;object&quot;,
          &quot;defaultValue&quot;: &quot;=::object&quot;
        }
      }
    }
  }
</code></pre>
<p>After the adf_publish branch update you will find before and after mentioned below.</p>
<p>Before:</p>
<pre><code>&quot;Aport_Import_properties_parameters_DWH&quot;: {
            &quot;type&quot;: &quot;string&quot;
        }
</code></pre>
<p>After:</p>
<pre><code>&quot;Aport_Import_properties_parameters_DWH&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;defaultValue&quot;: {
                &quot;FILENAME&quot;: &quot;techbit.csv&quot;
            }
        }
</code></pre>
<p><strong>&quot;FILENAME&quot;: &quot;techbit.csv&quot;</strong> is the object that i had declared in the parameters in pipeline.</p>
"
"74766715","How to POST a batch of objects from a whole JSON file to REST API using Azure Data Factory","<p>I have a JSON file that I need to POST to a REST API. The REST API has a limit of posting 2 records per batch. How can I do it? Sample JSON file provided below. If anybody can help me with this.</p>
<pre><code>[{
        &quot;name&quot;: &quot;sa&quot;,
        &quot;salary&quot;: 1000,
        &quot;age&quot;: 10
    },
    {
        &quot;name&quot;: &quot;sb&quot;,
        &quot;salary&quot;: 2000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sc&quot;,
        &quot;salary&quot;: 3000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sd&quot;,
        &quot;salary&quot;: 4000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;se&quot;,
        &quot;salary&quot;: 5000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sf&quot;,
        &quot;salary&quot;: 6000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sg&quot;,
        &quot;salary&quot;: 7000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sh&quot;,
        &quot;salary&quot;: 8000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;si&quot;,
        &quot;salary&quot;: 9000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sj&quot;,
        &quot;salary&quot;: 1000,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sk&quot;,
        &quot;salary&quot;: 1100,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sl&quot;,
        &quot;salary&quot;: 1200,
        &quot;age&quot;: 20
    },
    {
        &quot;name&quot;: &quot;sm&quot;,
        &quot;salary&quot;: 1300,
        &quot;age&quot;: 20
    }
]
</code></pre>
<p>I am able to post the data from JSON file using lookup activity and to pass the output to Web Activity using <code>post</code> method, but I can't control the number of objects to be processed during the pipeline run.</p>
","<json><azure><azure-data-factory>","2022-12-12 04:49:24","96","0","1","74770730","<p>I have reproduced the above using <strong>Set variables,lookup and until activities</strong> and got below results.</p>
<p>First, I have taken a string set variable and given <code>'0'</code> value for iteration intialization.</p>
<p><img src=""https://i.imgur.com/5fpkehR.png"" alt=""enter image description here"" /></p>
<p>Then after lookup used Until activity. Inside until activity I have taken 3 set variable activities. as ADF does not support self-referencing variables I have used a temporary string variable.</p>
<p><img src=""https://i.imgur.com/qH6QaSq.png"" alt=""enter image description here"" /></p>
<p>Next set variable creates an array of 2 objects with the following dynamic content using <code>iter</code> variable as index.</p>
<pre><code>@createArray(activity('Lookup1').output.value[int(variables('iter'))],activity('Lookup1').output.value[add(int(variables('iter')),1)])
</code></pre>
<p><img src=""https://i.imgur.com/a0E10a5.png"" alt=""enter image description here"" /></p>
<p>3rd set variable increments the <code>iter</code> by 2 with the following dynamic content.</p>
<pre><code>@string(add(int(variables('temp')),2))
</code></pre>
<p><img src=""https://i.imgur.com/FM5OvWE.png"" alt=""enter image description here"" /></p>
<p><strong>Until activity</strong></p>
<pre><code>@equals(div(int(variables('iter')),2),div(activity('Lookup1').output.count,2))
</code></pre>
<p><img src=""https://i.imgur.com/CSHE232.png"" alt=""enter image description here"" /></p>
<p>**Inside until activity you can use web activity and give that array variable to web activity body. **</p>
<p>The until stops for the last even numbered record.</p>
<p>You can see it iterated till 12th record.</p>
<p><img src=""https://i.imgur.com/XbZP7NS.png"" alt=""enter image description here"" /></p>
<p><strong>Then after until check the total data count (even or odd) using if activity</strong>. If even pass the last two records by index using length of the lookup(like <code>length-1</code> and <code>length-2</code>) to web. If odd pass only last one record(<code>length-1</code>) to the web.**</p>
"
"74763018","Difference between azure integration runtime & self hosted integration runtime in azure data factory?","<p>I would like to know real life examples/use cases for Azure Data Factory(ADF) with below configurations:</p>
<ul>
<li>Azure Integration Runtime(AIR) default</li>
<li>Self Hosted Integration Runtime(SHIR)</li>
</ul>
<p>Additional questions:</p>
<ol>
<li>Are there any additional costs associated with either of these configurations(AIR &amp; SHIR)?</li>
<li>If I need to connect to a private network/on-prem system from ADF, would SHIR be suffice and serve the need?</li>
<li>By having AIR as additional configuration, does it bother? If it is not needed, just don’t configure AIR?</li>
</ol>
<p>Please share your thoughts/suggestions. Thanks!</p>
","<azure><azure-data-factory>","2022-12-11 17:30:28","673","0","1","74766312","<p>One usually leverages an SHIR whenever there is need to access a source or sink hosted within a vnet like On Prem databases, SFTP, IaaS databases etc else you leverage AIR.</p>
<p>Now to answer ot your queries:</p>
<ol>
<li><p>SHIR charges are less than AIR since you are paying separately for your engine by setting up your own server whereas in case of AIR, MSFT provides the engine directly thereby having more charge as compared to SHIR
Note: this is in accordance to ADF costing and not overall costing</p>
</li>
<li><p>yes, to access anything hosted within a Vnet, you would need SHIR and it would suffice</p>
</li>
<li><p>AIR is a default configuration unlike SHIR wherein you have to set up server and install gateway. there is no action needed by individual for AIR</p>
</li>
</ol>
"
"74761942","Unable to Publish Azure Data Factory Publish","<p>I am trying to pulish an Azure Data Factory pipeline, however I'm getting the error:</p>
<blockquote>
<p>Error The document creation or update failed because of invalid
reference 'master'. Please ensure 'master' exists in data factory mode
and recreate it in Git mode if already present.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/YoOKA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YoOKA.png"" alt=""enter image description here"" /></a></p>
<p>I am familiar with the error. However, I'm can't find reference 'master'. Can someone let me know how to go about tracking down the reference 'master'?</p>
<p>Thanks</p>
","<azure-data-factory>","2022-12-11 15:10:39","439","0","1","75136059","<p>This issue is commonly caused by a mismatch between Data Factory mode and Git mode. It may happen when Git is first configured, or when changes are added directly in Git or in Live mode.</p>
<p>If you are unable to find and fix the conflict manually, you may re-sync the content in the Git Configuration page, by using either:</p>
<ul>
<li>Overwrite Live Mode (which I recommend): Makes Data Factory mode (published) version match Git.</li>
<li>Import Resources: Makes a Git branch match Data Factory mode.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/LduB4.png"" rel=""nofollow noreferrer"">Git configuration page</a></p>
<p>Please be advised that overwriting live mode may result in losing changes not currently in Git. You may use Import Resources to persist changes prior to this.</p>
"
"74761195","How azure data factory -Dataflow can create CRM related entities records in single transaction?","<p>I am trying to implement Azure Data factory DATAFLOW to create CRM entity records into multiple entities in single transaction. If any error occurred in the second entity, then first entity record should be rollback. Please share your idea.</p>
<p>I tried with Json file as input with multiple hierarchy, representing multiple CRM entity. I used Data flow source json dataset and 3 CRM sinks. But, i am unable to achieve single transaction when an error occurred.</p>
","<azure><azure-data-factory><dynamics-crm-365-v9>","2022-12-11 13:24:02","145","0","1","74767986","<p>ADF does not support roll back option. You can have any Watermark column or flag in the target table which indicates the records which got inserted during the current pipeline run and delete only those records if any error occurred.</p>
<p>Watermark column is the column which can have the timestamp at which the row got inserted or it can be incrementing key. Before running the pipeline, maximum value of the watermark column is noted. Whenever the pipeline is failed, rows inserted after the maximum watermark value can be deleted.</p>
<p>Instead of deleting all records from current pipeline run, if records which are not copied in some entities only need to be deleted then based on the key field, we can delete the rows. Below is the approach.</p>
<ul>
<li>Source1 and source2 are taken with entity1 and entity2 data respectively.</li>
</ul>
<p><img src=""https://i.imgur.com/u2MwZ0S.png"" alt=""enter image description here"" />
img1: entity1 data</p>
<p><img src=""https://i.imgur.com/6tHRc6v.png"" alt=""enter image description here"" /></p>
<p>img2: entity2 data</p>
<ul>
<li>Id=6 is not copied to entity2. So, this should be deleted from entity1.</li>
<li>Exist transformation is added and left and right stream are given as source1 and 2 respectively. Exists type is <strong>doesn't exist</strong>. Exists conditions: <code>source1@id = source2@id</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/clmEudi.png"" alt=""enter image description here"" />
img3: exists transformation settings</p>
<ul>
<li>Alter row transformation is added and condition is given as <code>delete if</code>  <code>true()</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/AeqvJSF.png"" alt=""enter image description here"" />
img4: Alter Row transformation settings</p>
<ul>
<li>In sink settings, allow delete is selected and key column is selected as <code>id</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/mO4OhCy.png"" alt=""enter image description here"" />
Img5: Sink settings</p>
<p><img src=""https://i.imgur.com/r6jqoLS.png"" alt=""enter image description here"" />
img6: Sink data preview.</p>
<p>When pipeline with this dataflow is run, all rows which are in entity1 but not in entity2 are deleted.</p>
"
"74757187","Data Factory Dynamic Expression using CRM 365 FetchXML","<p>I am trying to convert this dynamic expression in ADF</p>
<pre><code>&lt;fetch mapping=&quot;logical&quot; version=&quot;1.0&quot;&gt;
  &lt;entity name=&quot;account&quot;&gt;
    &lt;attribute name=&quot;createdon&quot; /&gt;
    &lt;filter&gt;
      &lt;condition attribute=&quot;createdon&quot; operator=&quot;ge&quot; value=&quot;2020-10-20T00:00:00Z&quot; /&gt;
      &lt;condition attribute=&quot;createdon&quot; operator=&quot;le&quot; value=&quot;2020-10-20T23:59:59Z&quot; /&gt;
    &lt;/filter&gt;
  &lt;/entity&gt;
&lt;/fetch&gt;
</code></pre>
<p>Replacing the Date with a variable or possibly a parameter, if someone can help with this please on how he expression should look like ?</p>
<p>Thank you</p>
","<azure-data-factory>","2022-12-10 23:08:27","177","0","1","74766526","<p>Replace the date value in the script as <code>@{variable('variable_name')}</code> . I tried to repro this. Two variables date1 and date2 of string type are created in ADF and assigned default value. Value should be given within single quotes or double quotes.</p>
<p><img src=""https://i.imgur.com/F7zpiWQ.png"" alt=""enter image description here"" /></p>
<p><code>@{variable('variable_name')}</code>  is inserted into the expression and the expression looks as in below snippet.</p>
<p><strong>Query</strong></p>
<pre class=""lang-html prettyprint-override""><code>&lt;filter&gt;
&lt;condition attribute=&quot;createdon&quot; operator=&quot;ge&quot; value=@{variables('date1')} /&gt;
&lt;condition attribute=&quot;createdon&quot; operator=&quot;le&quot; value=@{variables('date2')} /&gt;
&lt;/filter&gt;
</code></pre>
<p>Instead of giving the value for the variable within quotes,
<code>@{variables('date1')}</code> can be wrapped within quotes like <code>&quot;@{variables('date1')}&quot;</code>.</p>
<p>This way, variables can be used in dynamic expression in ADF.</p>
"
"74754418","Call stored procedure from ADF and sink to Azure SQL database","<p>I'm struggling with calling the stored procedure (create in SSMS, Azure serverless pools) in ADF and sink to AZURE database.</p>
<p>I have a copy data activity, my <code>Source dataset</code> is linked to my <code>synapse analytics</code> serverless pools:</p>
<p><a href=""https://i.stack.imgur.com/kxnzx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kxnzx.png"" alt=""enter image description here"" /></a></p>
<p>My <code>Sink</code> is connected to an Azure SQL database  parameter in the <code>Sink</code> coming from the  Azure SQL database <code>Dataset</code>.</p>
<p><a href=""https://i.stack.imgur.com/TWmYF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TWmYF.png"" alt=""enter image description here"" /></a></p>
<p>This is where I want to write output from the stored procedure. Problem is that I could not figure out how I could <code>TRUNCATE TABLE</code> in <code>Pre-copy-Script</code>.</p>
","<azure><azure-sql-database><azure-data-factory><azure-synapse>","2022-12-10 15:54:52","180","0","1","74755823","<p>There is a text box for pre-copy script on your 2nd screen dump.</p>
<p>Just add TRUNCATE TABLE YOURTABLENAME in the box. Replacing YOURTABLENAME with the actual name of your table.</p>
"
"74745788","How to get pipeline id of the first pipeline if first pipeline trigger another pipeline","<p>In our ADF application pipeline A is invoking pipeline B. I want to capture the run ID so I used the @pipeline().RunId to capture it. But while executing the two run ID  Which is as per design. But I want to capture the run ID of pipeline A from pipeline B.</p>
<p>Any idea how to capture these details?</p>
","<azure><azure-data-factory>","2022-12-09 16:28:00","141","0","2","74746106","<p>There is a system variable like TriggeredByPipelineRunId</p>
"
"74745788","How to get pipeline id of the first pipeline if first pipeline trigger another pipeline","<p>In our ADF application pipeline A is invoking pipeline B. I want to capture the run ID so I used the @pipeline().RunId to capture it. But while executing the two run ID  Which is as per design. But I want to capture the run ID of pipeline A from pipeline B.</p>
<p>Any idea how to capture these details?</p>
","<azure><azure-data-factory>","2022-12-09 16:28:00","141","0","2","74750390","<p>Just to add to Nandan points , you also have System variable named TriggeredByPipelineName . I do find name more helpful then guid :) .</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a></p>
<p><a href=""https://i.stack.imgur.com/rTD2w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rTD2w.png"" alt=""enter image description here"" /></a></p>
"
"74741006","Multiple lines in Azure Synapse/ADF expression builder","<p>I would like to have all lines showed inside of the expression builder. I want to avoid the following:</p>
<p><img src=""https://i.stack.imgur.com/k22R2.png"" alt=""Loses overview"" /></p>
<p>This works in pipeline runs, but is hard to debug and check.</p>
<p>I want the following, which is easy to read and debug:</p>
<p><img src=""https://i.stack.imgur.com/mmElM.png"" alt=""Multiline dynamic content"" /></p>
<p>Unfortunately this leads to newline characters being introduced in the resulting expression, which breaks the pipeline.</p>
<p>Any suggestions to achieve the above solution?</p>
","<azure-data-factory><azure-synapse>","2022-12-09 09:17:11","149","0","1","74741700","<p><strong>String interpolation</strong> treats the <code>enter</code> as new line in result. As an alternative you can use <code>concat()</code> in dynamic content.</p>
<p><strong>Example:</strong></p>
<p><img src=""https://i.imgur.com/KzMDOfr.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/OkWiLJJ.png"" alt=""enter image description here"" /></p>
"
"74740677","How to copy many csv files using Synapse Pipelines from an online source with the date in the file name?","<p>There is this git repository publicly available. It's being refreshed daily. There are several csv files with the structure like &quot;DA-01-12-2022&quot;, &quot;DA-02-12-2022&quot;, &quot;DA-03-12-2022&quot; and so on. The date is in the file name. It's also in the githublink, so I can copy one file without problem but since there are many CSV files in the git folder how can I use Synapse pipelines to copy all the files in the git repository to a storage in azure. I feel like I have to use loops but how can I tell it to use the date?</p>
<p>Thanks and best regards!</p>
","<azure-synapse><azure-data-factory>","2022-12-09 08:45:12","157","0","1","74743864","<p>You can use a copy activity to load all csv and store in one parquet files</p>
<p><a href=""https://i.stack.imgur.com/kV80w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kV80w.png"" alt=""enter image description here"" /></a></p>
<p>You can also use ITER activity to make loop with date</p>
"
"74737183","Can you use JDBC sources via Azure Linked Services using the Azure Integration Runtime on a remote machine?","<p>All,</p>
<p>We are in the process of migrating our clients to ADF using Integration Runtimes. The clients have the Azure Integration Runtime installed on an on-premise server and we are needing to pull data from a remote database through that connection.  The &quot;legacy&quot; product allowed for JDBC connections so we have JDBC drivers installed for the proprietary databases (Progress DataDirect, Pervasive, etc.) but there isn't an option in the Azure Linked Services option for JDBC.  We can use ODBC, but that would require purchasing and installing an ODBC driver on each of the client machines.  We'd like to use JDBC, but can't find anything that would allow us to do it.  Has anyone else run into this and are there any work arounds?</p>
<p>We would prefer to not have each of the clients that used JDBC purchase and install ODBC drivers.  We also aren't interested in developing our own ODBC driver that wraps the JDBC driver.  We are specifically looking for a way to use the existing JDBC drivers that are installed on the client systems to pull data from their database to create an Azure Dataset.</p>
","<azure><jdbc><odbc><azure-data-factory><linked-service>","2022-12-08 22:51:06","171","0","1","74999940","<p>The Progress DataDirect/Pervasive etc can spit the data in the txt/csv file locally and since you plan to install slef hosted runtime ADF can read the text file and write that any supported sink .</p>
"
"74736165","Is Possible To Schedule Azure Data Factory Triggers In Synapse While Not in Live Mode","<p>Is it possible to schedule Azure Data Factory Triggers in Git hub mode? My understanding is that Scheduled Triggers will on work in Synapse Live Mode.</p>
<p>As you can see from the image, I'm in Git Hub mode. Is it possible schedule triggers while in this mode?</p>
<p><a href=""https://i.stack.imgur.com/2e2IC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2e2IC.png"" alt=""enter image description here"" /></a></p>
<p>In an update to the question, I am attempting to create a Trigger in Git Hub mode, however I'm getting the message &quot;Make sure to &quot;Publish&quot; for trigger to be activated after clicking &quot;Save&quot;'. However, when I view my Triggers in Monitor, I can see the Triggers have started even though I haven't published them. Can you let me know if the trigger is actually working or not?</p>
<p><a href=""https://i.stack.imgur.com/vUfn8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vUfn8.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/TCHnv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TCHnv.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-12-08 20:47:31","178","0","1","74740174","<p>Yes, it is possible Schedule a Triggers in Azure synapse with Git Hub mode.</p>
<p>Go to New trigger -&gt; select your type <code>schedule</code>, <code>start time</code> and <code>time zone.</code></p>
<p><img src=""https://i.imgur.com/ZTOwFwi.png"" alt=""enter image description here"" /></p>
<p>You can check the <strong>Trigger, Pipeline successfully executed and got the output:</strong></p>
<p><img src=""https://i.imgur.com/85nznem.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/hzXJlLE.png"" alt=""enter image description here"" /></p>
"
"74733156","How use output of data flow in the copy data activty in azure data factory","<p>I have a excel file which I transform in a azure data flow in adf. I have added some column and transformed some values. As next step I want to copy the new data into a cosmos db. How can I achieve this? It's not clear how do I get the result of the data flow into the copyData activity. I have a sink in the data flow which will store the transformed data into a csv. As I understand the adf will create multiple files for performance reason. Or is there a way to make the changes &quot;on the fly&quot; and work with the transformed file further</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2022-12-08 16:07:27","499","0","1","74739706","<ul>
<li>If your sink is <code>Cosmos DB for No SQL</code>, then there is a direct sink connecter available in azure dataflows. After applying your transformations, you can create a dataset and directly move the data.</li>
</ul>
<p><img src=""https://i.imgur.com/RBnLWly.png"" alt=""enter image description here"" /></p>
<ul>
<li>If your sink is not for No SQL, then as you have done, write your data as csv files to your storage account. And if you choose to write the data to a single file, you can choose the <code>Output to single file</code> option in sink settings and give a filename.</li>
</ul>
<p><img src=""https://i.imgur.com/AtimP8m.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can directly select this file to copy to your sink. But if you already have data written as multiple files in a folder, you can use wild card path option as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/pcwCuC1.png"" alt=""enter image description here"" /></p>
"
"74730601","Exclude blank column headers when sinking to sql-database","<p>I have an dynamic CSV file with alot of columns. I mapped the columns I want to get from the source side and sink it into an sql-database. I did this through a copy data activity in Azure Data Factory. However I'm getting the following error:</p>
<p><strong>ErrorCode=DelimitedTextColumnNameNotAllowNull,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The name of column index 236 is empty. Make sure column name is properly specified in the header</strong></p>
<p>There are blank column headers in the file. How can I exclude this? I tried searching Google and here for an similiar problem, but I couldn't find anything. The solutions I found was about excluding rows with empty values, however I only want to exclude the column headers with an empty value. It doesn't matter if the column itself has values in the rows.</p>
","<azure-data-factory>","2022-12-08 12:52:57","52","0","2","74731691","<p>I can think of two different ways of how I handled this in the past</p>
<ul>
<li>Use a data flow where you can parse the CSV and leave out specific columns. (this requires a data flow component and hence extra costs, which is why I prefer the second option)</li>
<li>You can unmark &quot;First Row as Header&quot; in the dataset for the CSV in combination with &quot;Skip Line Count&quot; = 1 in the copy data activity. In this way, ADF won't read the first line and won't complain about it.</li>
</ul>
<p>In the schema, you can now define which columns (by number) you want to map to which column in the database &amp; which one you want to skip. In my example, the third column is skipped.</p>
<p>Schema:</p>
<p><img src=""https://i.stack.imgur.com/tEYa6.png"" alt=""Schema"" /></p>
"
"74730601","Exclude blank column headers when sinking to sql-database","<p>I have an dynamic CSV file with alot of columns. I mapped the columns I want to get from the source side and sink it into an sql-database. I did this through a copy data activity in Azure Data Factory. However I'm getting the following error:</p>
<p><strong>ErrorCode=DelimitedTextColumnNameNotAllowNull,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The name of column index 236 is empty. Make sure column name is properly specified in the header</strong></p>
<p>There are blank column headers in the file. How can I exclude this? I tried searching Google and here for an similiar problem, but I couldn't find anything. The solutions I found was about excluding rows with empty values, however I only want to exclude the column headers with an empty value. It doesn't matter if the column itself has values in the rows.</p>
","<azure-data-factory>","2022-12-08 12:52:57","52","0","2","74750489","<p>I am not sure if I understand the ask , but I just want to share that even if you do not have a header in a column in a CSV file and you select the first row is header , ADF will be add a header . See the snapshot below</p>
<p><a href=""https://i.stack.imgur.com/UwBbS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UwBbS.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/N1y2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N1y2Z.png"" alt=""enter image description here"" /></a></p>
<p>So one solution which you may think of changing to pipeline from</p>
<p>CSV -&gt; SQL</p>
<p>to</p>
<p>CSV -&gt; CSV ( to add the header ) -&gt; SQL</p>
<p>Now coming back to the addition of column  say column 54 . This is anyways a bigger problem as you will have to alter the SQL table also to accomdate this new column 54 correct ?</p>
<p>If I were you I could have used a AUTO CREATE Table option , that way ADF will create table as per the CSV  . Once I have the data in the staging table &lt; i can compare the same with the actual table and make some logic ( since the data is in SQL may a proc will do the trick )</p>
"
"74729296","Azure data factory connecting to MongoDB via linked service connection timeout","<p>I am trying to create a linked service in ADF to connect to a MongoDB and I am getting 30 second server timeouts.</p>
<p>I have the connection string and I can connect using Compass - my computer IP address is whitelisted - but I cannot connect through Azure linked service using their MongoDB connector with this connection string.</p>
<p>The Azure IP address ranges for my region have been added to the whitelist as well using the latest set published by Microsoft.  I am using an azurehostedingegrationruntime that is in the same Region the MongoDB is hosted in.</p>
<p>Problem is the MongoDB is hosted by a software house and I am not convinced they know what they are doing.  SSL is NOT enabled on the MongoDB and they are using the community edition v1.34.1, database is small &lt; 0.75Gb.  The MongoDB instance is installed on a Linux box - I was looking at a selfhostedintegrationruntime but that requires a gateway installing on the server that in turn needs the use of a windows server.</p>
<p>If anybody has any experience of connecting to a MongoDB through Azure data factory your help would be appreciated.  The only option from the Azure end is the connection string and I know that is correct as I can connect using Compass with it, but it times out when trying to connect using Azure linked service so looks like it cannot see the MongoDB.</p>
<p>Connects ok with the given connection using Compass, just not using Azure even though the Azure IP addresses have been whitelisted.</p>
","<mongodb><azure-data-factory><linked-service>","2022-12-08 11:04:27","193","0","1","74731979","<p>Solved by the software house, so they do actually know what they are doing.</p>
<p>Don't need to use SelfHostedIntegrateionRuntime, the AzureHostedIntegrationRuntime works just fine.  Also no need to whitelist the Azure IPs - these are subject to revision anyway.</p>
<p>&quot;, but on the instance firewall, I have the option to allow the exact service and this should cover any future ip changes. For now, I have allowed access only for the &quot;</p>
<p>Hope this makes sense.</p>
"
"74728342","Starting azure data factory with incoming parameter which is a json","<p>I want to start my data factory with parameters. For that I defined a parameter &quot;param&quot; in my factory. The parameter is of type string and contains a json object like this</p>
<pre><code>{
  &quot;url&quot;: &quot;http://mySpecialFile.csv&quot;,
  &quot;name&quot;: &quot;testing&quot;
  &quot;destination: &quot;farAway&quot;
}
</code></pre>
<p>Now how can I a access these values in my factory. Is it necessary to define for each attribue a variable? And can I extract for example the value of url? I tried it like this</p>
<pre><code>(@json(@pipeline().parameters.param).url)
</code></pre>
<p>But this does not work, it will write this value into the variable?
Is there better way to access the incoming json object? Also if I define for each json entry a variable I need to change my factory if the json changes?</p>
<p>Thanks for any advice</p>
","<azure><azure-data-factory>","2022-12-08 09:50:44","81","0","1","74729610","<p>In Azure Data factory pipeline parameter for <code>Json object type</code> of data there is Data type called <strong>Object</strong>.</p>
<p>Here I created Pipeline parameter named <strong>json</strong> with <code>Object data type</code> and your sample value.</p>
<p><img src=""https://i.imgur.com/U8HGcQB.png"" alt=""enter image description here"" /></p>
<p>from this parameter to access particular value you can use dynamic parameter like <code>@pipeline().parameters.json['value_which_you_want']</code></p>
<p>e.g. <code>@pipeline().parameters.json['url']</code>,<code>@pipeline().parameters.json['name']</code>,<code>@pipeline().parameters.json['destination']</code></p>
<p><img src=""https://i.imgur.com/UfV8rfZ.png"" alt=""enter image description here"" /></p>
<p>And it is fetching proper data.</p>
<p><img src=""https://i.imgur.com/V5VYhJK.png"" alt=""enter image description here"" /></p>
"
"74728080","Dataflow integration with foreach loop in azure data factory","<p>We have a data lake container weith three folders a,b,c. Each folder has 3 files a1,a2,a3,b1,b2,b3,c1,C2,c3. Now we need to design a pipeline which will dynamically do incremental load from the folders to a blob stroarge with same name file as souce. Incremental load is implemented by me in dataflow. We have other dataflow dependancy as well so we can't use copy activity but dataflow. I am unable to integrate get metadata activity with the dataflow where I am expecting some help.</p>
<p>We have a data lake container weith three folders a,b,c. Each folder has 3
I tried with parameters and variables.But I did not got the desired output. I used get metadata child item. Then a foreach loop. Inside foreach I tried with another fireaceach to get the files. I have used an append variable to append the data. I have already implemented the upsert logic for a single table in dataflow. If I am passing second get matadata active output (inside foreach) to dataflow it does not accepts. The main problem I am facing is to integrate the dataflow with foreach in dataset level. Because the dataset of the dataflow will be dependent on get metadata's output.</p>
","<azure><azure-data-factory>","2022-12-08 09:28:00","493","0","1","74729634","<p>Nested for-each is not possible in Azure data factory. Work around is to use execute pipeline inside for-each activity. To pass the output of metadata activity to dataflow, create the dataflow parameters and pass the value to that parameter. I tried to repro this scene in my environment, below is the approach.</p>
<p><strong>Outer Pipeline:</strong></p>
<ul>
<li><strong>Get Metadata activity</strong> is taken and only container name is given in the dataset file path. <strong>+ New</strong> is selected in field list and Child item argument is added. This activity will provide the list of all the directories that are present in the container.</li>
</ul>
<p><img src=""https://i.imgur.com/Gi2ymfU.png"" alt=""enter image description here"" /></p>
<ul>
<li><strong>For each activity</strong> is taken and in items Output of GetMetadata activity is given. <code>@activity('Get Metadata1').output.childItems</code></li>
</ul>
<p><img src=""https://i.imgur.com/eaB7SiP.png"" alt=""enter image description here"" /></p>
<ul>
<li>Inside for-each activity, <strong>execute pipeline</strong> activity is added.</li>
<li>A new child pipeline is created, and a parameter called <strong>FolderName</strong> is created in that pipeline.</li>
<li>The child pipeline name is given in execute pipeline activity. Value for the parameter is given as <code>@item().name</code>, to pass directory names as input to the child pipeline.</li>
</ul>
<p><img src=""https://i.imgur.com/4kBiOIE.png"" alt=""enter image description here"" /></p>
<p><strong>Child Pipeline:</strong></p>
<ul>
<li><p>In child pipeline, another Get meta data activity is taken and in the dataset file path, container name is given and for folder, dataset parameter is created and value of pipeline parameter FolderName is passed. <code>@pipeline().parameters.FolderName</code></p>
</li>
<li><p><strong>Child items</strong> is selected as an argument in the field list. This activity will give the list of files that are available in the directory.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/NoN6gsI.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Then for-each activity is added and in items output of the meta data activity is given. <code>@activity('Get_Metadata_inner').output.childItems</code></p>
</li>
<li><p>Inside for-each, dataflow is added.</p>
</li>
</ul>
<p><strong>Dataflow</strong></p>
<ul>
<li><p>In dataflow, parameter called filename is created.
<img src=""https://i.imgur.com/wIMwZia.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In Source dataset, dataset parameter is created for filename and foldername as <strong>fileName</strong> and <strong>folderName</strong> respectively.</p>
</li>
</ul>
<p><img src=""https://user-images.githubusercontent.com/113445679/206422842-f4443df4-65aa-4b30-8887-01394548406b.gif"" alt=""gif51"" /></p>
<ul>
<li><p>Then all other transformations are added in data flow.</p>
</li>
<li><p>In sink dataset of sink transformation, dataset parameter for folder is created and file name is left blank in dataset.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/1icm4Jy.png"" alt=""enter image description here"" /></p>
<ul>
<li>File name is given in sink settings. Value is the dataflow parameter <code>$filename.</code></li>
</ul>
<p><img src=""https://i.imgur.com/V2J77og.png"" alt=""enter image description here"" /></p>
<ul>
<li>In child pipeline, dataflow activity settings is given as in below image.
fileName : <code>@item().name</code>
folderName (for both source and sink parameter): <code>@pipeline().parameters.FolderName</code></li>
</ul>
<p><img src=""https://i.imgur.com/S5eUZ1I.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>In Parameters tab, filename value is given as <code>@item().name</code>
<img src=""https://i.imgur.com/dU3jWs4.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In this repro, simple select transformation is taken. This can be extended to any transformation in data flow. By this way, we can pass the values to dataflow.</p>
</li>
</ul>
"
"74726202","Is there any way to set up a trigger in ADF in such a way that if the file is updated or a new file is added it will run the pipeline","<p>Let's say I have a file called test.csv in a container called TestFolder, I want to set the trigger in such a way that whenever there is a new file or if the current test.csv file is edited, the pipeline should be triggered for copying data to the sql table</p>
","<sql-server><azure><azure-data-factory>","2022-12-08 06:14:16","89","0","1","74727431","<p>You can use Storage Event trigger
<a href=""https://i.stack.imgur.com/4rRsT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4rRsT.png"" alt=""enter image description here"" /></a></p>
<p>It would trigger an ADF pipeline whenever any one adds a new file or updates/edits an existing file</p>
<p>new edit:
<a href=""https://i.stack.imgur.com/DGBBr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DGBBr.png"" alt=""enter image description here"" /></a></p>
"
"74724644","Check if a Column Exists and Add it When Doesn't Exist","<p>In the Azure Data Factory data mapping flow, is there a way to check if a column <code>Date</code> exists in the input file? If <code>true</code>, select the <code>Date</code> column, if not then create a <code>Date</code> column but leave the column blank in the output?</p>
<p>I tried with conditional <code>select</code> that <code>if name=='Date'</code>, name the column as <code>Date</code>, but it the workflow fail with the &quot;Date&quot; column doesn't exist.</p>
","<azure><azure-data-factory><azure-mapping-data-flow>","2022-12-08 01:42:55","483","1","1","74726810","<p>You can use <code>byName()</code> in the derived column transformation.</p>
<p>This is my sample input data with <code>Date</code> column.</p>
<p><img src=""https://i.imgur.com/OUXdWeX.png"" alt=""enter image description here"" /></p>
<p>In derived column, use the below dataflow expression.</p>
<pre><code>toDate(byName('Date'))
</code></pre>
<p><img src=""https://i.imgur.com/FLDniSs.png"" alt=""enter image description here"" /></p>
<p><strong>The above <code>byName()</code> will search for the given column name and if it is there in the columns list then it gives those values and if it not there it will give null values to the column.</strong></p>
<p><strong>Result when Date column present in source:</strong></p>
<p><img src=""https://i.imgur.com/fhlQRqK.png"" alt=""enter image description here"" /></p>
<p><strong>Source without <code>Date</code> column:</strong></p>
<p><img src=""https://i.imgur.com/uC05HIO.png"" alt=""enter image description here"" /></p>
<p><strong>Result with <code>Date</code> column and values as <code>NULL</code>:</strong></p>
<p><img src=""https://i.imgur.com/dLdKgsb.png"" alt=""enter image description here"" /></p>
<p>After derived column transformation, use select transformation to select your desired columns.</p>
"
"74721818","How to put Excel as a sink in Azure Data Factory /Azure mapping dataflow","<p>Its kind of weird that i could not find Excel as a sink. How to write the file to Excel using DataFactory?</p>
","<azure><azure-data-factory>","2022-12-07 19:08:32","448","2","2","74725571","<p>Excel is yet not a supported sink in Azure data factory.
Its is not an out of box functionality
You would have to write your own custom logic either via Azure function,logic app etc</p>
"
"74721818","How to put Excel as a sink in Azure Data Factory /Azure mapping dataflow","<p>Its kind of weird that i could not find Excel as a sink. How to write the file to Excel using DataFactory?</p>
","<azure><azure-data-factory>","2022-12-07 19:08:32","448","2","2","75427289","<p>I have never used, but it seems this company is sharing for free a package that you can install in your ADF instance to export Excel files.</p>
<p><a href=""https://invati.ai/software-development-accelerators/excel-writer-for-azure-data-factory/"" rel=""nofollow noreferrer"">https://invati.ai/software-development-accelerators/excel-writer-for-azure-data-factory/</a></p>
<p>Note that although the package is free, hosting it in your Azure instance may incur in charges.</p>
"
"74720950","Publishing to Azure Data Factory from GitHub","<p>Can someone remind why when I publish from Gitbhub my Synapse Live workspace isn't updated with with the Pipelines created in Github?</p>
","<azure-data-factory>","2022-12-07 17:49:44","125","1","2","74731661","<p>Only when a pipeline is created in azure synapse analytics workspace's GIT mode, the pipeline would also appear in Synapse Live mode. Look at this <a href=""https://learn.microsoft.com/en-gb/azure/synapse-analytics/cicd/source-control#best-practices-for-git-integration:%7E:text=Best%20practices%20for%20Git%20integration"" rel=""nofollow noreferrer"">official Microsoft documentation's best practices section</a> which indicates the same.</p>
<p>But when the pipeline exists in your GIT repository, then after integrating your GIT, this pipeline would only be seen in the GIT branch but not in the Synapse Live mode.</p>
<ul>
<li>Let's say I have the following pipeline in my repo.</li>
</ul>
<p><img src=""https://i.imgur.com/rH6l0sf.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I integrate this repo with my synapse workspace, this pipeline would only appear in GIT branch.</li>
</ul>
<p><img src=""https://i.imgur.com/GrTnpUP.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can see that it is not visible in synapse live mode.</li>
</ul>
<p><img src=""https://i.imgur.com/bzG0q0N.png"" alt=""enter image description here"" /></p>
<ul>
<li>If you want your synapse live mode pipelines to GIT, you can check this option while configuring GIT and then publish to get them to your GIT.</li>
</ul>
<p><img src=""https://i.imgur.com/zWkkU8A.png"" alt=""enter image description here"" /></p>
"
"74720950","Publishing to Azure Data Factory from GitHub","<p>Can someone remind why when I publish from Gitbhub my Synapse Live workspace isn't updated with with the Pipelines created in Github?</p>
","<azure-data-factory>","2022-12-07 17:49:44","125","1","2","74731693","<p>This kind of things can happen when objects in your git repository get out of synch with the publish branch. (changes to linked services for example can be created imediately and create this kind of bad situation).
Meanwhile, there is at the moment a strange situation in Synapse that could also explain &quot;weird behaviours&quot;. There seems to be a general bug causing 2 things :</p>
<ul>
<li>problems when publishing to synapse live.</li>
<li>problems with notebook activity in pipelines : the reference to the notebook is lost and we are unable to select notebook in the pipeline activity</li>
</ul>
<p>In my case i am waiting microsoft feedback on this</p>
"
"74717938","Azure Data Flow expression builder not able to add string","<p>Error when connecting to Azure dataflow</p>
<p>I have given in Dataflow expression builder in sync</p>
<p>value of parameterdomain is coming from source column name</p>
<p>File Name Option: output to single file
File name : concat($parameterdomain,'.csv')</p>
<p>During Debug i get below error</p>
<pre><code>Failure type  User configuration issue
</code></pre>
<p>Details
Job failed due to reason: at Sink 'sinksource'(Line 17/Col 12): Column operands are not allowed in literal expressions
Source   Pipeline</p>
","<azure-data-factory>","2022-12-07 14:07:06","119","0","1","74809274","<p>Make sure you have checked the expression checkbox in the parameters tab of dataflow activity.</p>
<p><a href=""https://i.stack.imgur.com/kWdnM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kWdnM.png"" alt=""enter image description here"" /></a></p>
"
"74715883","Azure Data Factory Failing with Bulk Load","<p>I am trying to extract data from a Azure SQL Database, however I'm getting the</p>
<pre><code>Operation on target Copy Table to EnrDB failed: Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Cannot bulk load because the file &quot;https://xxxxxxx.dfs.core.windows.net/dataverse-xxxxx-org5a2bcccf/appointment/2022-03.csv&quot; could not be opened. Operating system error code 12(The access code is invalid.).
</code></pre>
<p>You might be thinking this is permission issue, but if you take a look at the error code 12 you will see the issue is related to Bulk Load.. a related answer can be found here..</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/988935/cannot-bulk-load-file-error-code-12-azure-synapse.html"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/988935/cannot-bulk-load-file-error-code-12-azure-synapse.html</a></p>
<p>I thought I might be able to fix the issue by selecting Bulk lock see image.</p>
<p><a href=""https://i.stack.imgur.com/2IEvP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2IEvP.png"" alt=""enter image description here"" /></a></p>
<p>But I still get the error.</p>
<p>Any thoughts on how to resolve this issue?</p>
","<azure-data-factory>","2022-12-07 11:26:59","99","0","1","74734955","<p>As I see that the error is refering to a source side (2022-03.csv) , so I am not sure as to why are you making changes on the sink side  . As explained in the threads which you referd , it appears the the CSV file is getting updated once the you pipeline starts execute by some other process . Refering back to the same thread .https://learn.microsoft.com/en-us/answers/questions/988935/cannot-bulk-load-file-error-code-12-azure-synapse.html
The changes suggested below should be made on the pipeline/process which is writing to 2022-03.csv .</p>
<p>[![enter image description here][1]][1]</p>
<p>HTH
[1]: <a href=""https://i.stack.imgur.com/SSzwt.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/SSzwt.png</a></p>
"
"74715258","Data Factory - Getting last Saturday from a current date","<p>How can I find the date of the last Saturday from a current date in ADF (set as a variable) ?</p>
<p>For example in SQL I can run this query and it will return the last Saturday:</p>
<p><a href=""https://i.stack.imgur.com/XIRUw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XIRUw.png"" alt=""enter image description here"" /></a></p>
<p>but I don't want to create a lookup activity to connect to the database and run that query. I want to define it as a variable.</p>
<p>Is it possible to create a variable that always returns the last Saturday?</p>
<p>Kind regards</p>
","<azure-data-factory>","2022-12-07 10:36:44","107","0","1","74716071","<p>You can use below expression to get the last saturday always in set variable activity.</p>
<pre><code>@FormatDateTime(addDays(subtractFromTime(utcnow(),dayOfWeek(utcnow()),'Day'),-1),'yyyy/MM/dd')
</code></pre>
<p><strong>Set variable:</strong></p>
<p><img src=""https://i.imgur.com/U8rmlnh.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/8ZpQ7CO.png"" alt=""enter image description here"" /></p>
"
"74714418","Azure Data Factory: ErrorCode=TypeConversionFailure,Exception occurred when converting value : ErrorCode: 2200","<p>Can somoene let me know why Azure Data Factory is trying to convert a value from String to type Double.</p>
<p>I am getting the error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=TypeConversionFailure,Exception occurred when converting value '+44 07878 44444' for column name 'telephone2' from type 'String' (precision:255, scale:255) to type 'Double' (precision:15, scale:255). Additional info: Input string was not in a correct format.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy Table to EnrDB&quot;,
    &quot;details&quot;: [
        {
            &quot;errorCode&quot;: 0,
            &quot;message&quot;: &quot;'Type=System.FormatException,Message=Input string was not in a correct format.,Source=mscorlib,'&quot;,
            &quot;details&quot;: []
        }
    ]
}
</code></pre>
<p>My Sink looks like the following:</p>
<p><a href=""https://i.stack.imgur.com/ccB34.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ccB34.png"" alt=""enter image description here"" /></a></p>
<p>I don't have any mapping set</p>
<p><a href=""https://i.stack.imgur.com/GzPxe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GzPxe.png"" alt=""enter image description here"" /></a></p>
<p>The column setting for the the field 'telephone2' is as follows:</p>
<p><a href=""https://i.stack.imgur.com/ycafx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ycafx.png"" alt=""enter image description here"" /></a></p>
<p>I changed the 'table option' to none, however I got the following error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: 'Internal system error occurred.\r\nStatement ID: {C2C38377-5A14-4BB7-9298-28C3C351A40E} | Query hash: 0x2C885D2041993FFA | Distributed request ID: {6556701C-BA76-4D0F-8976-52695BBFE6A7}. Total size of data scanned is 134 megabytes, total size of data moved is 102 megabytes, total size of data written is 0 megabytes.',Source=,''Type=System.Data.SqlClient.SqlException,Message=Internal system error occurred.\r\nStatement ID: {C2C38377-5A14-4BB7-9298-28C3C351A40E} | Query hash: 0x2C885D2041993FFA | Distributed request ID: {6556701C-BA76-4D0F-8976-52695BBFE6A7}. Total size of data scanned is 134 megabytes, total size of data moved is 102 megabytes, total size of data written is 0 megabytes.,Source=.Net SqlClient Data Provider,SqlErrorNumber=75000,Class=17,ErrorCode=-2146232060,State=1,Errors=[{Class=17,Number=75000,State=1,Message=Internal system error occurred.,},{Class=0,Number=15885,State=1,Message=Statement ID: {C2C38377-5A14-4BB7-9298-28C3C351A40E} | Query hash: 0x2C885D2041993FFA | Distributed request ID: {6556701C-BA76-4D0F-8976-52695BBFE6A7}. Total size of data scanned is 134 megabytes, total size of data moved is 102 megabytes, total size of data written is 0 megabytes.,},],'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy Table to EnrDB&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>Any more thoughts</p>
","<azure-data-factory>","2022-12-07 09:31:24","283","0","1","74715800","<p>The issue was resolved by changing the column DataType on the database to match the DataType recorded in Azure Data Factory i.e StringType</p>
"
"74714228","Expression Definition in Azure","<p>I want to copy data from SharePoint using Synapse's Copy Activity HTTP link service.</p>
<p>I want to put an expression using a function in the formula (1) at the URL of the source data set, but it is not working as expected.</p>
<p>Please let me know how I can fix this.
(1)<code>https://sharepoint.com/teams/Data/Share/Planning/_api/web/GetFileByServerRelativeUrl('/teams/Data/Share/Planning/Shared%20Documents/05_master/502_outlet_list/2022/2022_monthly_master/outlet_info_20221101_Outlet List _.xlsx')/$value</code></p>
<p>Here is the statement that results in an error
(2)<code>@concat('https://sharepoint.com/teams/Data/Share/Planning/_api/web/GetFileByServerRelativeUrl('/teams/Data/Share/Planning/_api/web/GetFileByServerRelativeUrl('/teams/Data/Share/Planning/Shared%20Documents/05_master/502_outlet_list/',formatDateTime(addHours(utcNow(),9),'yyyy'),'/',formatDateTime(addHours(utcNow(),9),'yyyy'),'_monthly_master/outlet_info_',formatDateTime(addHours(utcNow(),9),'yyyyMMdd'),'_Outlet List _.xlsx')/$value')</code></p>
","<azure><azure-functions><azure-data-factory>","2022-12-07 09:16:08","54","0","1","74714776","<ul>
<li><p>The error is because of the ambiguous usage of single quotes inside <code>concat</code> function. Instead, you can use string interpolation <code>@{...}</code>.</p>
</li>
<li><p>You can replace your dynamic content with the below dynamic content which makes use of string interpolation. I have used this in a set variable activity for demonstration.</p>
</li>
</ul>
<pre><code>https://sharepoint.com/teams/Data/Share/Planning/_api/web/GetFileByServerRelativeUrl('/teams/Data/Share/Planning/Shared%20Documents/05_master/502_outlet_list/@{formatDateTime(addHours(utcNow(),9),'yyyy')}/@{formatDateTime(addHours(utcNow(),9),'yyyy')}_monthly_master/outlet_info_@{formatDateTime(addHours(utcNow(),9),'yyyyMMdd')}_Outlet List _.xlsx')/$value
</code></pre>
<p><img src=""https://i.imgur.com/ncLDixJ.png"" alt=""enter image description here"" /></p>
<ul>
<li>This would give the result as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/e65sPBX.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can modify the <code>formatDateTime()</code> function as per your requirement where you can use <code>2022-11-01</code> instead of utcNow().</li>
</ul>
"
"74709436","How can i split a column delimiter with spaces?","<p>I have a csv delimited with spaces that can change to 9-10-11, it s there a way for split the column in two with Azure data factory?</p>
<p>Examples:</p>
<p>This is my CSV</p>
<p><img src=""https://i.stack.imgur.com/VXyeo.png"" alt=""enter image description here"" /></p>
<p>I try using dataflows:</p>
<p><img src=""https://i.stack.imgur.com/OjG7n.png"" alt=""enter image description here"" /></p>
<p>but when I execute the dataflow, it throw me this error:</p>
<p><img src=""https://i.stack.imgur.com/SvFKF.png"" alt=""enter image description here"" /></p>
<p>PD: the csv has 4.000.000 rows</p>
<p>Solve the problem using azure data factory, the csv needs to finish in my DW</p>
","<azure-data-factory>","2022-12-06 21:53:47","174","0","1","74712738","<ul>
<li>I have the following data in my sample csv file with either 9, 10 or 11 spaces in between the value.</li>
</ul>
<p><img src=""https://i.imgur.com/ZScufWN.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, after reading the column, use <code>derived column</code> transformation to split the required column using 9 spaces (minimum number of spaces).</li>
</ul>
<pre><code>req1 : split(col1,'         ')[1]
req2 : split(col1,'         ')[2]
</code></pre>
<ul>
<li>This will split the data into an array of 2 elements, where 1st index will have no spaces in its value and the 2nd index element has trailing spaces.</li>
</ul>
<p><img src=""https://i.imgur.com/digBWp4.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now apply <code>ltrim</code> on the req2 column. Check the length of this column before and after the transformation to confirm that we are eliminating the trailing spaces.</li>
</ul>
<pre><code>req2 : ltrim(req2)
</code></pre>
<p><img src=""https://i.imgur.com/nPFHMj3.png"" alt=""enter image description here"" /></p>
<ul>
<li>After doing this, you can check the length of the <code>req2</code> and it would be 1.</li>
</ul>
<p><img src=""https://i.imgur.com/diFwo1E.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, select only the required columns and then write it to required sink.</li>
</ul>
"
"74707193","How to consume a storage queue from data factory pipeline?","<p>I am working on a project where I need to consume the entries of the storage queue from a data factory pipeline.</p>
<p>Files will be uploaded to a blob storage which triggers a azure function. This azure function writes into a storage queue. Now I want to consume the entries of this storage queue. Due to the fact that the storage queue provide a rest api to consume data, I can use a web client in the azure data factory which can be scheduled every few minutes. But I would prefere a more direct way, so that when the storage queue has been filled, my pipeline should be starting.</p>
<p>I am quite new to the azure world, so now I am searching for solution. Is there a way to subscribe to the storage queue? I can see that there is the possibilty to create custom triggers in the data factory how can I connect to a storage queue there? Or is there another way?</p>
","<azure><azure-data-factory><azure-logic-apps>","2022-12-06 18:04:49","194","0","1","74729827","<p>Thank you @Scott Mildenberger for pointing out in the right direction. After taking the inputs and reproducing from my end, this was working when I used Queue Trigger called <code>When a specified number of messages are in a given queue (V2)</code> where we can specify the Threshold of the Queue to get the flow triggered. Below is the flow of my Logic App.</p>
<p><img src=""https://i.imgur.com/SaQfJcW.png"" alt=""enter image description here"" /></p>
<p><em><strong>RESULTS:</strong></em></p>
<p><em>In Logic App</em></p>
<p><img src=""https://i.imgur.com/FAkfayi.png"" alt=""enter image description here"" /></p>
<p><em>In ADF</em></p>
<p><img src=""https://i.imgur.com/skYhDBS.png"" alt=""enter image description here"" /></p>
"
"74704349","ADF Understanding the Case Statement","<p>Given the following Derived column Expression</p>
<pre><code>case(Rolling =='A'||Rolling == 'B'||Rolling == 'C'|| Rolling ==&quot;S&quot;
     , &quot;
     , case(Alpha== 'EE'
         , toString(toDate(Manu_Date, 'yyyy-MM-dd'))
         , case(Alpha=='CW', Del_Date,&quot;))
)
</code></pre>
<p>2 questions</p>
<ol>
<li>Is there a Better way to write this code?</li>
<li>What is this code trying to do ?</li>
</ol>
<p>I am trying to understand what they are trying to achieve with this expression?</p>
","<azure><azure-data-factory><azure-synapse>","2022-12-06 14:40:10","706","1","1","74711638","<ul>
<li>In the given expression, after Rolling==&quot;S&quot;, it is not double Quotes <code>&quot;</code>. It should be two single quotes<code>''</code></li>
<li>Similarly, after Del_date, also it should be two single Quotes.</li>
</ul>
<pre class=""lang-scala prettyprint-override""><code>case(Rolling =='A'||Rolling ==  'B'||Rolling ==  'C'|| Rolling ==&quot;S&quot;, '',
case(Alpha==  'EE', toString(toDate(Manu_Date, 'yyyy-MM-dd')),
case(Alpha=='CW', Del_Date,'' )))
</code></pre>
<blockquote>
<ol>
<li>What is this code trying to do ?</li>
</ol>
</blockquote>
<ul>
<li><p>Syntax for case statement is
<code>case(condition,true_expression,false_expression)</code></p>
</li>
<li><p>Initially, this expression checks if Rolling is 'A' or 'B' or 'C' or 'S' and then assign the value as <code>''</code> (empty string) for the derived column.</p>
</li>
<li><p>When the above condition is false, then checks if Alpha is 'EE' and assign the value of <strong>Manu_Date</strong> in string format.</p>
</li>
<li><p>When the second condition also fails, it checks if Alpha='CW' and assign the value of <strong>Del_Date</strong> column.</p>
</li>
<li><p>When all the above conditions are not met, <code>''</code> (empty string) is assigned. This will be the default value.</p>
</li>
<li><p>I repro'd this with sample input.
<img src=""https://i.imgur.com/X1HlMAL.png"" alt=""enter image description here"" />
img1: input data</p>
</li>
<li><p>In derived column transformation, new column is added, and the expression is given as in below script.</p>
</li>
</ul>
<pre class=""lang-scala prettyprint-override""><code>case(Rolling =='A'||Rolling ==  'B'||Rolling ==  'C'|| Rolling ==&quot;S&quot;, '',
case(Alpha==  'EE', toString(toDate(Manu_Date, 'yyyy-MM-dd')),
case(Alpha=='CW', Del_Date,'' )))
</code></pre>
<p><img src=""https://i.imgur.com/fXePf5S.png"" alt=""enter image description here"" />
img2: Derived column transformation output</p>
<blockquote>
<ol start=""2"">
<li>Is there a Better way to write this code?</li>
</ol>
</blockquote>
<ul>
<li>Since the order of condition is important to assign the values to the new column, case statement is better way to do.</li>
<li>But, instead of using nested case statements, we can use single case statement to achieve the same.
Syntax:
<code> case( condition_1, expression_1, condition_2, expression_2,.......... condition_n,expression_n,default_expression)</code>.
Null will be the default value, when the default expression is omitted.</li>
</ul>
<p><strong>Modified expression</strong></p>
<pre class=""lang-scala prettyprint-override""><code>case(Rolling =='A'||Rolling ==  'B'||Rolling ==  'C'|| Rolling ==&quot;S&quot;, '',
Alpha==  'EE', toString(toDate(Manu_Date, 'yyyy-MM-dd')),
Alpha=='CW', Del_Date,'' )
</code></pre>
<p><img src=""https://i.imgur.com/5TOXEOQ.png"" alt=""enter image description here"" />
img 3:  Results of both case statements</p>
<p>Both the expressions are added in the derived column transformation and results are same in both cases.</p>
"
"74702778","How to send activity output as email attachment in logic app","<p>I have an ADF pipeline, I want to send the output of my activity as an email attachment in the logic app.</p>
<p>I have a lookup activity followed by a For each activity and an Inside For each activity I have a web activity to call the logic app.</p>
<p>I want to send the output of the lookup activity as an email attachment to the logic app. I am not able to think about this integration part.</p>
","<azure-data-factory>","2022-12-06 12:45:58","135","0","2","74720736","<p>Create Logic app event trigger with HTTP and Outlook.</p>
<p><img src=""https://i.imgur.com/hGsKuHh.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>Inside <strong>HTTP  request is received</strong>:</p>
</blockquote>
<p><img src=""https://i.imgur.com/APwlRmL.png"" alt=""enter image description here"" /></p>
<ol>
<li><p>Copy HTTP POST URL</p>
</li>
<li><p>Request Body JSON Schema</p>
<p><code>{ &quot;properties&quot;: { &quot;dataFactoryName&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;message&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;pipelineName&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;receiver&quot;: { &quot;type&quot;: &quot;string&quot; } }, &quot;type&quot;: &quot;object&quot; }</code></p>
</li>
<li><p>POST Method</p>
</li>
</ol>
<blockquote>
<p><strong>Send an Email</strong></p>
</blockquote>
<p><img src=""https://i.imgur.com/j0K15ll.png"" alt=""enter image description here"" /></p>
<ol start=""4"">
<li><p>Connect Your outlook email .</p>
</li>
<li><p>Use HTTP POST URL as shown in step1</p>
</li>
</ol>
<p><img src=""https://i.imgur.com/3bWRbFo.png"" alt=""enter image description here"" /></p>
<p>Create parameter name <strong>receiver</strong></p>
<p><img src=""https://i.imgur.com/wuJcGAJ.png"" alt=""enter image description here"" /></p>
<ol start=""6"">
<li>Add dynamic this content</li>
</ol>
<p>s</p>
<pre><code>{

&quot;message&quot; : &quot;This is the row from lookup item @{item().customerID},@{item().gender},@{item().SeniorCitizen},@{item().Partner}.&quot;,

&quot;dataFactoryName&quot; : &quot;@{pipeline().DataFactory}&quot;,

&quot;pipelineName&quot; : &quot;@{pipeline().Pipeline}&quot;,

&quot;receiver&quot; : &quot;@{pipeline().parameters.receiver}&quot;

}
</code></pre>
<p><img src=""https://i.imgur.com/VltJDHl.png"" alt=""enter image description here"" /></p>
<p><strong>Pipeline successfully executed and got the output:</strong></p>
<p><img src=""https://i.imgur.com/jvhn4UM.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/oZmbGQ2.png"" alt=""enter image description here"" /></p>
"
"74702778","How to send activity output as email attachment in logic app","<p>I have an ADF pipeline, I want to send the output of my activity as an email attachment in the logic app.</p>
<p>I have a lookup activity followed by a For each activity and an Inside For each activity I have a web activity to call the logic app.</p>
<p>I want to send the output of the lookup activity as an email attachment to the logic app. I am not able to think about this integration part.</p>
","<azure-data-factory>","2022-12-06 12:45:58","135","0","2","74720965","<p>There is no direct or easy way to send email attachment from ADF.</p>
<p>But as a workaround first you will have to save the output of your lookup activity to a file and then follow the approach described in this video by a community volunteer where logic apps come into play to send the lookup activity output data file as an attachment. <a href=""https://www.youtube.com/watch?v=wNJhhT0DDMM"" rel=""nofollow noreferrer"">How To Send File as Attachment From Azure Data Factory - Azure Data Factory Tutorial 2021</a></p>
<p>In order to save the lookup output data to a file you can follow this approach: <a href=""https://stackoverflow.com/questions/66024162/get-output-of-lookup-activity-in-a-file"">Get Output of lookup activity in a file</a></p>
"
"74702237","Azure Data Factory2's pagination rule (for HTTP API)","<p>The concerning data is updated daily through an HTTP API, and no metadata is available. It consists of +/- 28 columns and 10,000 rows. There is a pagination of 30 applied to the API. It retrieves JSON data and is converted to Parquet through mapping.</p>
<p>Retrieving the data was done via this loop, created by @HimanshuSinha-MSFT and explained via: <a href=""https://learn.microsoft.com/en-us/answers/questions/468561/azure-data-factory-pagination-issue.html#answer-470345"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/468561/azure-data-factory-pagination-issue.html#answer-470345</a></p>
<p>However, there are 3 problems to solve:</p>
<p><strong>No metadata from HTTP API</strong>
The total number of iterations is needed, based on total rows, which is unavailable. I temporarily solved this by putting in a large number for total iterations, so ADF continues getting the empty rows until the iterations are over. This takes extremely long. This can be fixed by getting the metadata, but it is not available for an HTTP gateway. How can I retrieve the metadata?</p>
<p><strong>Method is time consuming</strong>
Iterating through the loop takes 20 seconds, for 30 rows. The iterations are sequential. When a part in the loop fails, the whole loop fails (which takes time). Can this process go faster?</p>
<p><strong>JSON columns not recognised</strong>
Not all column names from the JSON endpoint are recognised by ADF. This is due to the nested arrays. Is there a solution to this?</p>
<p>Thank you!</p>
","<azure><pagination><azure-data-factory><azure-synapse><httpapi>","2022-12-06 12:07:49","360","0","1","74713513","<p>To paginate Http API, you can use Pagination rules by Azure Data Factory rather than ForEach loop.</p>
<p>I took one sample APi and applied Pagination to it</p>
<p><img src=""https://i.imgur.com/FsQSQ0x.png"" alt=""enter image description here"" /></p>
<p>Here I passed range to <code>Offset</code> From 1 and the end I left blank and because I don't know when the number of total rows, an gave offset as 20</p>
<blockquote>
<ol>
<li><strong>No metadata from HTTP API</strong> The total number of iterations is needed, based on total rows, which is unavailable.</li>
</ol>
</blockquote>
<p>For this situation in Azure Data Factory Pagination rules there are end condition based on object value:</p>
<ul>
<li>Empty: The pagination ends when the value of the specific node in response is empty.</li>
<li>Exist: The pagination ends when the value of the specific node in response exists.</li>
<li>Not Exist: The pagination ends when the value of the specific node in response dose not exists.</li>
<li>Const: The pagination ends when the value of the specific node in response is a user-defined const value.</li>
</ul>
<p>Here I used End Condition as <code>$.results.name</code> is no exists.</p>
<p><img src=""https://i.imgur.com/CWHRDD9.png"" alt=""enter image description here"" /></p>
<blockquote>
<ol start=""2"">
<li><strong>Method is time consuming</strong> Iterating through the loop takes 20 seconds, for 30 rows.</li>
</ol>
</blockquote>
<p>using Azure Data Factory Pagination rules will might reduce the execution time.</p>
<p><img src=""https://i.imgur.com/aPaBSND.png"" alt=""enter image description here"" /></p>
<blockquote>
<ol start=""3"">
<li><strong>JSON columns not recognised</strong> Not all column names from the JSON endpoint are recognised by ADF. This is due to the nested arrays. Is there a solution to this?</li>
</ol>
</blockquote>
<p>To avoid this issue use mapping option in copy activity. Import the schemas of the <code>json</code> output and map it properly as below.</p>
<p><img src=""https://i.imgur.com/lZLxysf.png"" alt=""enter image description here"" /></p>
"
"74701965","ADF passing more than one array paramater to LogicApps","<p>I have an issue rearding the passing of more than one array parameter. I was able to do a &quot;for each&quot; cycle to execute my array parameter &quot;SPPATH&quot;, but unfortunately I can pass only one, here is my code:</p>
<pre><code>{&quot;SPPATH&quot;:&quot;@{item()}&quot;,&quot;SPFOLPATH&quot;:&quot;@{pipeline().parameters.SPFOLPATH}&quot;,&quot;updsppath&quot;:&quot;@{pipeline().parameters.updsppath}&quot;,&quot;Storageacct&quot;:&quot;@{pipeline().parameters.Storageacct}&quot;,&quot;sapath&quot;:&quot;@{pipeline().parameters.sapath}&quot;,&quot;saoppath&quot;:&quot;@{pipeline().parameters.saoppath}&quot;}
</code></pre>
<p><a href=""https://i.stack.imgur.com/HDLRX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HDLRX.png"" alt=""enter image description here"" /></a></p>
<p><strong>I want to pass &quot;updsppath&quot; also in the array because my output is on different locations, is it possible to do that, if so, how?</strong></p>
<p>thanks in advance</p>
","<azure-data-factory><azure-logic-apps>","2022-12-06 11:46:23","137","0","1","74713105","<p>I have reproduced the above and able to iterate multiple arrays inside ForEach.</p>
<p>For this the length of the all arrays should be same.</p>
<p>Use another array for indexes of these.</p>
<p>For Sample I have two array parameters like below.</p>
<p><img src=""https://i.imgur.com/W6fJkR4.png"" alt=""enter image description here"" /></p>
<p>I have created another array for index like below.</p>
<pre><code>@range(0,length(pipeline().parameters.arr1))
</code></pre>
<p><img src=""https://i.imgur.com/lkVjSDf.png"" alt=""enter image description here"" /></p>
<p>Give this <code>index_array</code> to ForEach.</p>
<p>Create a <code>res</code> array variable in pipeline and inside ForEach, use <code>append variable</code> with the below dynamic content.</p>
<pre><code>@json(concat('{&quot;arr1&quot;:&quot;',pipeline().parameters.arr1[item()],'&quot;,&quot;SPFOLPATH&quot;:&quot;',pipeline().parameters.arr2[item()],'&quot;}'))
</code></pre>
<p><img src=""https://i.imgur.com/IHgvonD.png"" alt=""enter image description here"" /></p>
<p>After ForEach if you look at variable result (for showing here I have assigned to another variable), it will give you the desired JSON.</p>
<p><img src=""https://i.imgur.com/XP9LSqE.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/nPifWwN.png"" alt=""enter image description here"" /></p>
<p>You can use this procedure to generate the desired array of objects and pass it to the logic apps as per your requirement.</p>
"
"74701956","Handle extra column while importing delimited file in adf","<p>I am having a csv file with delimiter '|'. For some of the rows the string itself contains '|'. At the end these rows are getting an additional column. So, when ever copying data using a copy activity, ADF is throwing an error. How to skip the copy activity for these particular rows ?</p>
<p>I have tried deleting these rows in file itself. But the main problem here is, I would be getting files every day that are to be loaded into db.</p>
","<csv><azure-data-factory>","2022-12-06 11:45:14","160","0","1","74707713","<p>This problem comes up frequently, usually with commas, and there aren't any good answers. Below are my recommendations in order of preference.</p>
<p>If you can control the input file format, I would recommend doing both of these:</p>
<ol>
<li><strong>Change the file delimiter</strong>. Change the file to use a delimiter that would not occur in your data. Again, this issue occurs most frequently with comma (,) delimiters because commas often show up in the underlying data. Pipestem (|) is usually a good option as it does not organically occur in text. Since that is not the case here, you may need to get more creative and use something like caret (^). Tabs (\t) are also a solid option and probably the easiest change to implement.</li>
<li><strong>Wrap the fields with Quotes</strong>. Doing this will allow the text inside the quotes to contain the delimiter character. This is a good practice regardless of the delimiter, but can add significant bloat to the file size depending on the number of rows and columns. You can also choose to only quote fields that contain the delimiter in the text.</li>
</ol>
<p>If you cannot change the input file, you'll need a preprocessor step to remove the offending rows. Basically, I would read each line of the original file as a single text value (not parsing) and count the delimiters. If a row has the proper delimiter count, then write it out to a secondary file. Then you can use the secondary file for your downstream processing. This would be my last resort because of the data loss, but it might be tolerable in your situation. You could use a Data Flow with a schema-less source dataset to accomplish this step.</p>
"
"74698942","Split Json array into table rows using azure data factory","<p>I'm using Azure Data Factory and I have Json files in azure Datalake. Each Json file contain an array of Jsons. I want to copy each Jsons of the array into Azure SQL row. I tried to do it with copy activity, but it automatically flatten the Json and I want to keep it original. I cannot use DataFlow.
The following pic describe what I want to achieve (don't mention the values of the table)</p>
<p><a href=""https://i.stack.imgur.com/si9Ua.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/si9Ua.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-sql-database><azure-data-factory>","2022-12-06 07:47:24","101","0","3","74700710","<p>I figured it out, I used Lookup activity with Json linked service. then, with script activity, I wrote each @item() to the azure sql table row
<a href=""https://i.stack.imgur.com/YiMXK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YiMXK.png"" alt=""enter image description here"" /></a></p>
"
"74698942","Split Json array into table rows using azure data factory","<p>I'm using Azure Data Factory and I have Json files in azure Datalake. Each Json file contain an array of Jsons. I want to copy each Jsons of the array into Azure SQL row. I tried to do it with copy activity, but it automatically flatten the Json and I want to keep it original. I cannot use DataFlow.
The following pic describe what I want to achieve (don't mention the values of the table)</p>
<p><a href=""https://i.stack.imgur.com/si9Ua.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/si9Ua.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-sql-database><azure-data-factory>","2022-12-06 07:47:24","101","0","3","74700894","<ul>
<li><p>In order to copy each Json array into a single column of a row, you can
use <code>openjson</code> in script activity. Below is the approach.</p>
</li>
<li><p>Lookup activity is taken, and Json file is taken as the dataset in the activity.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/N28ogxV.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Then the output of the lookup activity is stored in the variable <code>Json</code>
of string type. The value is assigned using <strong>set variable</strong> activity.
Value for variable Json: <code>@string(activity('Lookup1').output.value)</code>
<img src=""https://i.imgur.com/RhbSkCH.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Script activity is taken and linked service for Azure SQL database is given. Script is given as</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>declare @json nvarchar(4000)=N'@{variables('Json')}';
INSERT INTO test_tgt
SELECT * FROM OPENJSON(@json)
WITH (
col1 nvarchar(max) '$' AS JSON
);
</code></pre>
<p>This script will insert the data into the table <strong>test_tgt</strong> which is already created.</p>
<p><img src=""https://i.imgur.com/HGHz6vE.png"" alt=""enter image description here"" /></p>
<p><strong>SQL table output</strong></p>
<p><img src=""https://i.imgur.com/Pcdj3gi.png"" alt=""enter image description here"" /></p>
"
"74698942","Split Json array into table rows using azure data factory","<p>I'm using Azure Data Factory and I have Json files in azure Datalake. Each Json file contain an array of Jsons. I want to copy each Jsons of the array into Azure SQL row. I tried to do it with copy activity, but it automatically flatten the Json and I want to keep it original. I cannot use DataFlow.
The following pic describe what I want to achieve (don't mention the values of the table)</p>
<p><a href=""https://i.stack.imgur.com/si9Ua.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/si9Ua.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-sql-database><azure-data-factory>","2022-12-06 07:47:24","101","0","3","74701087","<p>I think you have to use dynamic expressions here.
Try to parse the output of your select activity ( the one that fetch the json file from your datalake).</p>
<p>Here is an exemple that would extract two seperate json variables from an array of json on a datalake.</p>
<p><a href=""https://i.stack.imgur.com/qxG5v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qxG5v.png"" alt=""my pipeline"" /></a></p>
<p>the json file is like :</p>
<p><a href=""https://i.stack.imgur.com/qlaHR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qlaHR.png"" alt=""my json file"" /></a></p>
<p>lookup activity would be as below :</p>
<p><a href=""https://i.stack.imgur.com/p5vO7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p5vO7.png"" alt=""lookup activity"" /></a></p>
<p>variables are :</p>
<p><a href=""https://i.stack.imgur.com/RtXwH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RtXwH.png"" alt=""variabes"" /></a></p>
<p>output of the lookup activity is :</p>
<p><a href=""https://i.stack.imgur.com/0KrZ7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0KrZ7.png"" alt=""output of lookup activity"" /></a></p>
<p>To parse the array of json (output of the lookup activity) we use the following :</p>
<pre><code>@array(activity('Lookup1').output.value[0])

NOTE : [number] would give the item number of the array
[0] : first item
[1] scond item
..
</code></pre>
<p><a href=""https://i.stack.imgur.com/een7G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/een7G.png"" alt=""setvariable"" /></a></p>
<p>Results :</p>
<p>variable 1 gets :</p>
<pre><code>@array(activity('Lookup1').output.value[0])
</code></pre>
<p><a href=""https://i.stack.imgur.com/T0C5V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T0C5V.png"" alt=""var 1"" /></a></p>
<p>variable 2 gets:</p>
<pre><code>@array(activity('Lookup1').output.value[1])
</code></pre>
<p><a href=""https://i.stack.imgur.com/qT7YU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qT7YU.png"" alt=""var 2"" /></a></p>
<p>Hope this would help.</p>
"
"74697739","I am Migrating the data from one server to other server with SSIS Package and Triggered that package in Azure Data Factory(ADF)","<p>I am Migrating the data from one server (azuredatabase) server to other server with SSIS Package. I am trigerring this in Azure Data Factory (ADF). But for 1 table data has arrount 1.3 million data. So, to complete that load it is taking arround 130 min of time in Azure. Is there any solution to complete the load faster.</p>
<p>(<a href=""https://i.stack.imgur.com/nHIyw.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/nHIyw.png</a>)</p>
<p>I am Migrating the data from one server (azuredatabase) server to other server with SSIS Package. I am trigerring this in Azure Data Factory (ADF). But for 1 table data has arrount 1.3 million data. So, to complete that load it is taking arround 130 min of time in Azure. Is there any solution to complete the load faster.</p>
","<ssis><azure-data-factory>","2022-12-06 05:38:50","65","0","1","74876615","<blockquote>
<p>According to <strong>shubs</strong> mentioned in <a href=""https://www.sqlservercentral.com/articles/the-defaultbuffermaxrows-and-defaultbuffersize-properties-in-ssis"" rel=""nofollow noreferrer"">The DefaultBufferMaxRows and DefaultBufferSize Properties in SSIS</a>, it says:</p>
<blockquote>
<p>for large data volumes the default values would not necessarily give you the best performance and hence a certain level of manipulation is required for improving the data flow performance.</p>
</blockquote>
</blockquote>
<p><code>DefaultBufferMaxRows</code> and <code>DefaultBufferSize</code> are two of the Data Flow Task settings that might be utilized to improve data extractions.</p>
<p><strong>DefaultBufferMaxRows:</strong> The number of rows that may be kept in these buffers is indicated by this parameter.</p>
<p><strong>DefaultBufferSize:</strong> The default buffer size for storing rows momentarily is indicated by this parameter (in bytes).</p>
<p>also, there is property called <a href=""https://technet.microsoft.com/en-us/library/ms141031.aspx"" rel=""nofollow noreferrer"">AutoAdjustBufferSize</a> which, if set to &quot;true,&quot; disregards the <code>DefaultBufferSize</code> option and determines the actual buffer size using the <code>DefaultBufferMaxRows</code>.</p>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/sql/integration-services/data-flow/data-flow-performance-features?redirectedfrom=MSDN&amp;view=sql-server-ver16"" rel=""nofollow noreferrer"">Data Flow Performance Features</a></p>
"
"74690361","Azure DataFactory Foreach Copy Upsert, howto use key column","<p>I'm using azure data factory to copy a table from a mysql source to a sql server destination. I'm using the default 'record' functionality. In the copy step, I want to enable <strong>upsert</strong>. I then need to enter the key columns, and I'm wondering how to make sure that each table can have its own key column(s).</p>
<p><img src=""https://i.stack.imgur.com/CRQGF.png"" alt=""See image here"" /></p>
<p><img src=""https://i.stack.imgur.com/Ee4AV.png"" alt=""And key column definition"" /></p>
<p>Tried entering column names, however the end result looks confusing, what is the key then for which table?</p>
","<azure><azure-data-factory>","2022-12-05 15:14:30","295","0","1","74697773","<p>In order to give the key columns dynamically, in lookup table, a field called <strong>key_column</strong> is added for every table_name. Below is the detailed approach.</p>
<ul>
<li>Lookup table is taken with fileds table name and key column. In ADF, lookup activity dataset is taken with the lookup table.</li>
</ul>
<p><img src=""https://i.imgur.com/zuYobDj.png"" alt=""enter image description here"" /></p>
<ul>
<li>In for-each activity, lookup table output is taken.</li>
</ul>
<p><img src=""https://i.imgur.com/RZgdHCK.png"" alt=""enter image description here"" /></p>
<ul>
<li>Copy activity is taken in for-each activity.  Source dataset is taken like below image.</li>
</ul>
<p><img src=""https://i.imgur.com/Ay1e5s9.png"" alt=""enter image description here"" /></p>
<ul>
<li>In sink dataset, write behaviour is given as 'upsert' and key columns is given dynamically as
<code>@array(item().key_column</code></li>
</ul>
<p><img src=""https://i.imgur.com/1ehcIMC.png"" alt=""enter image description here"" /></p>
<ul>
<li>By this way key columns can be assigned dynamically and upsert can be performed in copy activity</li>
</ul>
"
"74683958","How to make file name as folder name in ADF","<p>I have Files in Source side as</p>
<p><code>ABCD_120320221200.csv</code>
<code>FAG_IQ11_12052022.csv</code>
<code>FAZ_OP10_210320221300.csv</code></p>
<p>I need to make <code>ABCD</code>, <code>FAG_IQ11</code>, <code>FAZ_OP10</code> as Folder names in ADLS and Files should move inside the respective folders.</p>
<p>I have used @split(item.name,'_')
but only ABCD folder got created.</p>
<p>How can I create 3 folders?</p>
<p>Thanks in advance.</p>
","<azure><azure-data-factory><azure-data-lake-gen2>","2022-12-05 06:00:19","123","0","1","74692698","<blockquote>
<p>I have used @split(item().name,'_') but only ABCD folder got created.</p>
</blockquote>
<p><code>@split(item().name,'_')</code> will only splits the string into multiple words and we cannot pick our folder name from the array of words.</p>
<p>To achieve the above requirement, I did like below.</p>
<p>These are my sample source csv files.</p>
<p><img src=""https://i.imgur.com/AJ0J0LO.png"" alt=""enter image description here"" /></p>
<p>First use Get Meta data activity to get the source folder ChildItems array and give it to the ForEach activity.</p>
<p>Inside ForEach use a set variable activity to get the Folder name. Use below dynamic content for it.</p>
<pre><code>@substring(item().name,0,lastIndexOf(item().name,'_'))
</code></pre>
<p><img src=""https://i.imgur.com/v5p3hiT.png"" alt=""enter image description here"" /></p>
<p>The above dynamic content first finds the last occurance of <code>_</code> and give that index as required string length to <code>substring()</code> function.</p>
<p>Then use copy activity.</p>
<p>In source use wild card path and <code>@item().name</code>.</p>
<p><img src=""https://i.imgur.com/bTcw0qD.png"" alt=""enter image description here"" /></p>
<p>For sink <strong>create two dataset parameters, one for file name and one for folder name and use those in file path of dataset</strong>.</p>
<p><img src=""https://i.imgur.com/rGJsBt9.png"" alt=""enter image description here"" /></p>
<p>In copy activity sink give the variable to the folder name parameter and <code>@item().name</code> to the file name parameter.</p>
<p><img src=""https://i.imgur.com/W1t7Q9X.png"" alt=""enter image description here"" /></p>
<p><strong>Execute this and files will be copied to the respective folders.</strong></p>
<p><img src=""https://i.imgur.com/QNC5CQ2.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/l8kGMcO.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/IXAxPNV.png"" alt=""enter image description here"" /></p>
"
"74680474","Azure Data Factory concating syntax","<p>I'm trying to run a pipeline that results a select with where be the system triggername system output.</p>
<p>I've tried to use :</p>
<p><a href=""https://i.stack.imgur.com/TT6Vq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TT6Vq.png"" alt=""enter image description here"" /></a></p>
<pre><code>@concat(concat(concat(concat('select * from ',item().DB_CARGA,'.',item().SC_CARGA,'.',item().tb_carga, 'where ' ,item().DS_CARGA ,'=', string(pipeline().TriggerName)))))
</code></pre>
<p>But I'm getting the following error:</p>
<p><a href=""https://i.stack.imgur.com/IyPe5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IyPe5.png"" alt=""enter image description here"" /></a></p>
<p>Could anyone help me with the right syntax?</p>
","<azure><azure-data-factory>","2022-12-04 20:08:03","74","0","1","74684151","<p>I reproduced this and got similar error when I used your syntax.</p>
<p><img src=""https://i.imgur.com/mgWnABl.png"" alt=""enter image description here"" /></p>
<p>In your dynamic expression, <strong>there should be a space between <code>table name</code> and <code>where</code></strong> and also <strong>trigger name should be enclosed in single quotes</strong>.</p>
<p>Please go through the below procedure to resolve the error.</p>
<p>First, I have a lookup activity which will give database name, table name and SQL table column(trigger column name). Give this to Foreach.</p>
<p><img src=""https://i.imgur.com/LK5u1lu.png"" alt=""enter image description here"" /></p>
<p>I have created sample tables with <code>trigger_column</code> column and gave some values.</p>
<p><img src=""https://i.imgur.com/YJAx7aW.png"" alt=""enter image description here"" /></p>
<p>In pipeline I have created trigger with same name <code>mytrigger</code>. Inside Foreach for my demo <code>I have used lookup query</code>. You can use your activity as per the requirement.</p>
<p><img src=""https://i.imgur.com/5TJChCb.png"" alt=""enter image description here"" /></p>
<p><strong>Dynamic content:</strong></p>
<p>You can do this with single <code>concat</code> function.</p>
<pre><code>@concat('select * from ',item().database,'.',item().table_name, ' where ' ,item().trigger_name ,'=','''',string(pipeline().TriggerName),'''')
</code></pre>
<p>This will give the SQL query like this when Executed.</p>
<p><img src=""https://i.imgur.com/zSDwwSS.png"" alt=""enter image description here"" /></p>
<p><strong>Output when triggered:</strong></p>
<p><img src=""https://i.imgur.com/O2zVv1f.png"" alt=""enter image description here"" /></p>
"
"74675962","Copy Each '.txt' File into respective date folder Based on Date in Filename using data factory","<p>``
I have to copy files from source folder to target folder both are in the same storage account(ADL). The files in the source folder are of in .txt format and have date appended in the file name,
eg: RAINBOW.IND.EXPORT.20221201.WIFI.NETWORK.SCHOOL.txt
and
RAINBOW.IND.EXPORT.20221202.WIFI.NETWORK.SCHOOL.txt
(20221201 and 20221202 is date in file name , date format: yyyymmdd)</p>
<p>I have to create a pipeline that will sort and store files in the folders in ADL's in this hierarchy
ex: adl/2022/12/01/RAINBOW.IND.EXPORT.20221201.WIFI.NETWORK.SCHOOL.txt
adl/2022/12/02/RAINBOW.IND.EXPORT.20221202.WIFI.NETWORK.SCHOOL.txt</p>
<p>even if we have multiple files on same date in file name based on that date in file name it has to create year(YYYY) folder and in year(YYYY) folder it should create month(MM) folder and in month(MM) folder it should create date(DD) folder like above example. Each File should copy into respective yyyy and respective mm and respective date folder.</p>
<pre><code>What I have done:
In Get Metadata -  Given argument to extract **childitems**

For each activity that contains a Copy activity. 
 In Copy activity source wildcard path is given as *.txt
for sink took concat expression using split and substring functions
Please check the screenshots of all activities and expressions
but this pipeline is creating the folders based on date in file name (like adl/2022/12/01) 
but problem is it was copying all files into all date(DD) folders 
(like adl/2022/12/01/RAINBOW.IND.EXPORT.20221201.WIFI.NETWORK.SCHOOL.txt
                     RAINBOW.IND.EXPORT.20221202.WIFI.NETWORK.SCHOOL.txt
      
      adl/2022/12/02/RAINBOW.IND.EXPORT.20221201.WIFI.NETWORK.SCHOOL.txt
                     RAINBOW.IND.EXPORT.20221202.WIFI.NETWORK.SCHOOL.txt)



1.[GET META to extract child items](https://i.stack.imgur.com/GVYgZ.png)
2.[Giving GET META output to FOREACH](https://i.stack.imgur.com/cbo30.png)
3.[Inside FOREACH using COPY ](https://i.stack.imgur.com/U5LK5.png)
4.[Source Data Set](https://i.stack.imgur.com/hyzuC.png)
5.[Sink Data Set](https://i.stack.imgur.com/aiYYm.png) Expression used in Data Set in Folder Path '@concat('adl','/'dataset().FolderName)
6.[Took parameter for Sink](https://i.stack.imgur.com/QihZR.png) 
7.[Sink in copy activity ](https://i.stack.imgur.com/4OzT5.png)
Expression used in sink for dynamic folders using split and substring function
@concat(substring(split(item().name,'.')[3],0,4),'/',
        substring(split(item().name,'.')[3],4,2),'/',
        substring(split(item().name,'.')[3],6,2)
       )


**OUTPUT for this pipeline**

adl/2022/12/01/RAINBOW.IND.EXPORT.20221201.WIFI.NETWORK.SCHOOL.txt
               RAINBOW.IND.EXPORT.20221202.WIFI.NETWORK.SCHOOL.txt
      
adl/2022/12/02/RAINBOW.IND.EXPORT.20221201.WIFI.NETWORK.SCHOOL.txt
               RAINBOW.IND.EXPORT.20221202.WIFI.NETWORK.SCHOOL.txt


**Required Output is**

adl/2022/12/01/RAINBOW.IND.EXPORT.20221201.WIFI.NETWORK.SCHOOL.txt
      
adl/2022/12/02/RAINBOW.IND.EXPORT.20221202.WIFI.NETWORK.SCHOOL.txt

(i.e each file should copy to respective date folders only even if we have multiple files in same date, they should copy to date folders based on date in file name)


</code></pre>
","<azure-data-lake><azure-data-factory>","2022-12-04 13:14:23","77","0","1","74684688","<p>I have reproduced the above and got same result when I followed the steps that you have given.</p>
<p><img src=""https://i.imgur.com/Gh8e0dM.png"" alt=""enter image description here"" /></p>
<p>Copy activity did like this because, in source or sink you did not gave <code>@item().name</code>(<strong>file name for that particular iteration</strong>) and you have given *<strong>.txt</strong> in the wildcard path of source in copy activity.</p>
<p>It means <strong>for every iteration(for every file name)</strong> it copies all .txt files from source into that particular target folder(same happened for you).</p>
<p>To avoid this,</p>
<ol>
<li><strong>Give <code>@item().name</code> in source wild card file name</strong></li>
</ol>
<p><img src=""https://i.imgur.com/WGrNSz5.png"" alt=""enter image description here"" /></p>
<p>It means we are giving only one that iteration file name in the source for the copy.</p>
<p><strong>(OR)</strong></p>
<ol start=""2"">
<li>Keep the wildcard file name in source as it is(<code>*.txt</code>) and create a sink dataset parameter for file name.</li>
</ol>
<p><img src=""https://i.imgur.com/0vsSsEl.png"" alt=""enter image description here"" /></p>
<p>and give <code>@item().name</code> to it in copy activity sink.</p>
<p><img src=""https://i.imgur.com/Djvps88.png"" alt=""enter image description here"" /></p>
<p>You can do any of the above and if you want you can do both at a time. I have checked all the 3 scenarios like</p>
<blockquote>
<p>1.<code>@item().name</code> in wild card sink file name.<br>
2. <code>@item().name</code> in dataset file name by keeping wildcard path same.<br>
3. combining both 1 and 2(<code>@item().name</code> in wild card file name and in sink dataset parameter).</p>
</blockquote>
<p>All are working fine and giving desired result.</p>
<p><img src=""https://i.imgur.com/kcWVcyi.png"" alt=""enter image description here"" /></p>
"
"74668868","Data Factory Copy Activity: Error found when processing 'Csv/Tsv Format Text' source 'xxx.csv' with row number 6696: found more columns than expected","<p>I am trying to perform a simply copy activity in Azure Data Factory from CSV to SQL Table, but I'm getting the following error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=DelimitedTextMoreColumnsThanDefined,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error found when processing 'Csv/Tsv Format Text' source 'organizations.csv' with row number 6696: found more columns than expected column count 41.,Source=Microsoft.DataTransfer.Common,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Copy data1&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>The copy activity is as follows</p>
<p>Source
<a href=""https://i.stack.imgur.com/Nakad.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nakad.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/jd0Fz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jd0Fz.png"" alt=""enter image description here"" /></a></p>
<p>My Sink is as follows:</p>
<p><a href=""https://i.stack.imgur.com/3uNBJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3uNBJ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/V070i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V070i.png"" alt=""enter image description here"" /></a></p>
<p>As preview of the data in source is as follows:</p>
<p><a href=""https://i.stack.imgur.com/euDw3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/euDw3.png"" alt=""enter image description here"" /></a></p>
<p>This seems like a very straight forward copy activity. Any thoughts on what might be causing the error?</p>
<p>My row 6696 looks like the following:</p>
<p>3b1a2e5f-d08b-166b-4b91-eb53009b2377    Compassites Software Solutions  organization    compassites-software    <a href=""https://www.crunchbase.com/organization/compassites-software"" rel=""nofollow noreferrer"">https://www.crunchbase.com/organization/compassites-software</a>    318375  17/07/2008 10:46    05/12/2022 12:17        company compassitesinc.com  <a href=""http://www.compassitesinc.com"" rel=""nofollow noreferrer"">http://www.compassitesinc.com</a>   IND     Karnataka   Bangalore   &quot;Pradeep Court&quot;, #163/B, 6th Main 3rd Cross, JP Nagar 3rd phase 560078  operating   Custom software solution experts    Big Data,Cloud Computing,Information Technology,Mobile,Software Data and Analytics,Information Technology,Internet Services,Mobile,Software                 01/11/2005          51-100  info@compassitesinc.com 080-42032572    <a href=""http://www.facebook.com/compassites"" rel=""nofollow noreferrer"">http://www.facebook.com/compassites</a> <a href=""http://www.linkedin.com/company/compassites-software-solutions"" rel=""nofollow noreferrer"">http://www.linkedin.com/company/compassites-software-solutions</a>  <a href=""http://twitter.com/compassites"" rel=""nofollow noreferrer"">http://twitter.com/compassites</a>  <a href=""https://res.cloudinary.com/crunchbase-production/image/upload/v1397190270/c3e5acbde40f36eaf4f8c6f6eda3f803.png"" rel=""nofollow noreferrer"">https://res.cloudinary.com/crunchbase-production/image/upload/v1397190270/c3e5acbde40f36eaf4f8c6f6eda3f803.png</a>              company</p>
<p>No commas</p>
","<azure-data-factory>","2022-12-03 17:10:46","443","0","1","74698930","<p>As the error message indicates, there is a record at row number 6696 where there is a value containing <code>,</code> as a character in it.</p>
<ul>
<li>Look at the following demonstration where I have taken a similar case. I have 3 columns in my source. The data looks as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/NcFx2lA.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I run use similar dataset settings and read these values, the same error would be thrown.</li>
</ul>
<p><img src=""https://i.imgur.com/JLlJv6w.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, the value <code>T1,OG</code> is being considered as if they belong to 2 different columns since they have dataset delimiter within the value.</li>
<li>Such values would throw an error as it is ambiguous to read. One way to avoid this is to enclose such values with quote character (double quote in this case).</li>
</ul>
<p><img src=""https://i.imgur.com/ZZdjcAp.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now when I run the copy activity, it would give the desired output.</li>
</ul>
<p><img src=""https://i.imgur.com/hXzAeye.png"" alt=""enter image description here"" /></p>
<ul>
<li>The table data would look like this:</li>
</ul>
<p><img src=""https://i.imgur.com/5GOmuZR.png"" alt=""enter image description here"" /></p>
"
"74661868","Azure Data Factory: Return Identifier values from a Copy Data activity","<p>I am updating an on-premises SQL Server database table with data from a csv file using a Copy Data activity. There is an int identity Id column on the sink table that gets generated when I do the Upsert. I would like to retrieve the Id value generated in that table to use later in the pipeline.</p>
<p>Is there a way to do this?</p>
<p>I can't use a data flow as I am using a self-hosted Integration Runtime.</p>
<p>Hi @Nick.McDermaid, I am loading about 7,000 rows from the file to the database. I want to store the identities in the database the file comes from.</p>
<p>Edit:
I have 2 databases (source/target). I want to upsert (using MERGE SQL below, with the OUTPUT clause) into the target db from the source db and then return the Ids (via the OUTPUT resultset) to the source db. The problem I have is that the upsert (MERGE) SQL gets it's SELECT statement from the same target db that the target table is in (when using a Copy Data activity), but I need to get the SELECT from the source db. Is there a way to do this, maybe using the Script activity?</p>
<p>Edit 2: To clarify, the 2 databases are on different servers.</p>
<p>Edit 3 (MERGE Update):</p>
<pre><code>MERGE Product AS target
USING (SELECT [epProductDescription]
      ,[epProductPrimaryReference]
         FROM [epProduct]
        WHERE [epEndpointId] = '438E5150-8B7C-493C-9E79-AF4E990DEA04') AS source
ON target.[Sku] = source.[epProductPrimaryReference]
WHEN MATCHED THEN
    UPDATE SET [Name] = source.[epProductDescription]
      ,[Sku] = source.[epProductPrimaryReference]
WHEN NOT MATCHED THEN
    INSERT ([Name]
      ,[Sku]
    VALUES (source.[epProductDescription]
      ,source.[epProductPrimaryReference]
OUTPUT $action, inserted.*, updated.*;
</code></pre>
<p>Edit 3 (sample data):</p>
<p><a href=""https://i.stack.imgur.com/1C4Yv.png"" rel=""nofollow noreferrer"">source sample:</a></p>
<p><a href=""https://i.stack.imgur.com/pVbhE.png"" rel=""nofollow noreferrer"">target output</a></p>
","<sql-server><azure-data-factory>","2022-12-02 21:54:00","227","0","1","74743058","<blockquote>
<p>Is there a way to do this, maybe using the Script activity?</p>
</blockquote>
<p>Yes, you can execute this script using Script activity in ADF</p>
<p>As your tables are on different SQL servers first you have to create Linked server with source database on target Database.</p>
<p><strong>go to &gt;&gt; Server Objects &gt;&gt; Linked Server &gt;&gt; New Linked server</strong> and create linked server with source database on target Database as below.</p>
<p><strong>While creating linked server make sure same user must exist on both databases.</strong></p>
<p><img src=""https://i.imgur.com/Uuhwr2p.png"" alt=""enter image description here"" /></p>
<p>then I wrote <code>Merge</code> Query using this linked sever source.</p>
<p><strong>My Sample Query:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>    MERGE INTO PersonsTarget as trg
    USING (SELECT [LastName],[FirstName],[State]
    FROM [OP3].[sample1].[dbo].[Personssource]) AS src
    ON trg.[State] = src.[State]
    WHEN MATCHED THEN
    UPDATE SET [LastName] = src.[LastName]
          ,[FirstName] = src.[FirstName]
    WHEN NOT MATCHED THEN
    INSERT ([LastName],[FirstName],[State])
    VALUES (src.[FirstName],src.[LastName],src.[State])
    OUTPUT $action, inserted.*;
</code></pre>
<p>Then In Script activity I provided the script</p>
<p><img src=""https://i.imgur.com/YwdDv2X.png"" alt=""enter image description here"" /></p>
<p><strong>Note: In linked service for on premises target table use same user which you used in linked service</strong></p>
<p>Executed successfully and returning Ids:</p>
<p><img src=""https://i.imgur.com/kWHj2hZ.png"" alt=""enter image description here"" /></p>
"
"74656320","Azure data factory - Dashboard Log Query - Filter failed pipelines who successfully rerun","<p>I've been tasked with reducing monitor overhead of a data lake (~80TiB) with multiple ADF pipelines running (~2k daily). Currently we are logging Failed pipeline runs by doing a query on <code>ADFPipelineRun</code>. I do not own these pipelines, nor do I know the inner workings of existing and future pipes, I cannot make assumptions on how to filter these by custom logic in my queries. Currently the team is experiencing fatigue with these, most failed piperuns are solved during their reruns.</p>
<p>How can I filter these failures so they dont show up when a rerun succeeds?</p>
<p>The logs exposes a few id's that initially looks interesting, like <code>Id</code>, <code>PipelineRunId</code>, <code>CorrelationId</code>, <code>RunId</code>, but none of these will link a failed pipe to a successful one.</p>
<p>The logs does however show an interesting column, <code>UserProperties</code>, that apparently can be dynamically populated during the pipeline run. There may be a solution to be found here, however it would require time and friction for all existing factories to be reconfigured.</p>
<p>Are there any obvious solutions I have overlooked here? Preferably Azure native solutions. I can see that reruns and failures are linked inside ADF Studio, but I cannot see a way to query it externally.</p>
","<azure><azure-data-factory>","2022-12-02 13:09:41","44","0","1","74717045","<p>After a discussion with the owner of the ADF pipes we realized the naming convention of the pipelines would allow me to filter out the noisy failing pipes that would later succeed. It's not a universal solution but it will work for us as the naming convention is enforced across the business unit I am supporting</p>
"
"74653637","How to get number of activity runs in Azure Data Factory","<p>I see total cost of ADF Activities from Cost Analysis.</p>
<p>But where I can see how many time activities have run in last month?</p>
","<azure-data-factory>","2022-12-02 09:39:37","162","0","1","74688157","<p>I created Azure data factory and created pipeline and implemented copy data activity.
Image for reference:</p>
<p><img src=""https://i.imgur.com/1ikKZfq.png"" alt=""enter image description here"" /></p>
<p>I want to show the activity runs of pipeline. For that I followed below steps:</p>
<p>Go to Monitor -&gt;pipelineType(Triggered/debug) -&gt;Click on pipline which we want to see the activity runs as mentioned below</p>
<p><img src=""https://i.imgur.com/y7O1LKc.png"" alt=""enter image description here"" /></p>
<p>When click on the pipeline I got activity runs of pipeline as below</p>
<p><img src=""https://i.imgur.com/zm9d0IK.png"" alt=""enter image description here"" /></p>
<p>if we want to get consumption of pipeline, click on consumption will get as below</p>
<p><img src=""https://i.imgur.com/vjMpuKG.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/uNGOWJo.png"" alt=""enter image description here"" /></p>
<p>How many days we want to search we can select as mentioned below:</p>
<p><img src=""https://i.imgur.com/d3aI10S.png"" alt=""enter image description here"" /></p>
<p>In this way we can get the activity runs of azure data factory.</p>
"
"74646745","ADF pipeline not able to read DECIMAL(36,0) value from Parquet file","<p>We're using a copy activity to copy parquet file data into our managed instance SQL server.</p>
<p>The source is using a SQL Serverless query to read the parquet files.
There's a new column coming through that is bringing in large values and causing failures e.g. 28557632721941551956925858310928928</p>
<p>There isn't any problem querying it straight out of Azure Data Studio using SQL Serverless.</p>
<p>Here's the error message:</p>
<p>{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read data from source.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.OverflowException,Message=<strong>Conversion overflows</strong>.,Source=System.Data,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;Stage Parquet File Data&quot;,
&quot;details&quot;: []
}</p>
<p>I also tried using a parquet file dataset for my source. This is the failure I received:</p>
<p>{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;ErrorCode=ParquetBridgeInvalidData,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column gwcbi___seqval of primitive type FixedLenByteArray, original type Decimal contained an invalid value for the given original type.,Source=Microsoft.DataTransfer.Richfile.ParquetTransferPlugin,'&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;Stage Parquet File Data&quot;,
&quot;details&quot;: []
}</p>
<p>This looks like a serious limitation of Synapse/ADF pipelines. Any ideas?</p>
<p>Thanks,
Jason</p>
","<parquet><azure-data-factory><azure-synapse>","2022-12-01 18:43:55","218","0","1","74792303","<p>A conversion overflow means that the value was too big for the datatype it's trying to be stored in. Decimals with precision greater than 28 (BigDecimals) are not supported in ADF copy activity which is why the above issue.</p>
<p>As a workaround you may try casting/converting the datatype to other (for example <code>String/varchar</code>)</p>
<p>But if you have feedback to improve the ADF product, please feel free to log it in ADF IDEAS forum here -   <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c</a></p>
"
"74644349","Azure Synapse mapping dataflow - REST source dataset not paginating from ITGlue API","<p>This is my first post here so forgive me if I'm in the wrong place.</p>
<p>I'm running a mapping data flow in Azure Synapse to query ITGlue's REST API. It is only returning one page of results, vs all of them.</p>
<p>Here's the endpoint I'm querying: <a href=""https://api.itglue.com/configurations"" rel=""nofollow noreferrer"">https://api.itglue.com/configurations</a></p>
<p>Here's the response body for that endpoint (truncated to keep it brief):</p>
<pre><code>
{
    &quot;data&quot;: [
        {
            &quot;id&quot;: &quot;1234567&quot;,
            &quot;type&quot;: &quot;configurations&quot;,
            &quot;attributes&quot;: {
               ......
            },
            &quot;relationships&quot;: {
                &quot;adapters-resources&quot;: {
                    &quot;data&quot;: []
                }
            }
        }
    ],
    &quot;meta&quot;: {
        &quot;current-page&quot;: 1,
        &quot;next-page&quot;: 2,
        &quot;prev-page&quot;: null,
        &quot;total-pages&quot;: 1000,
        &quot;total-count&quot;: 1000,
        &quot;filters&quot;: {}
    },
    &quot;links&quot;: {
        &quot;self&quot;: &quot;https://api.itglue.com/configurations?page%5Bnumber%5D=1&amp;page%5Bsize%5D=1&quot;,
        &quot;next&quot;: &quot;https://api.itglue.com/configurations?page%5Bnumber%5D=2&amp;page%5Bsize%5D=1&quot;,
        &quot;last&quot;: &quot;https://api.itglue.com/configurations?page%5Bnumber%5D=1714&amp;page%5Bsize%5D=1&quot;
    }
}

</code></pre>
<p>Here's what I think is the relevant configuration for ITGlue:</p>
<p><a href=""https://i.stack.imgur.com/aTVuj.png"" rel=""nofollow noreferrer"">Dataset source options</a></p>
<p>As far as I can tell, this is the correct syntax for the pagination rule. The only thing I can think that is messing this up is the characters in the 'next' link, which are http encoded [ and ] characters.</p>
<p>The IT Glue API docs for this endpoint <a href=""https://api.itglue.com/developer/#configurations-index"" rel=""nofollow noreferrer"">here</a> confirm this - with page[number] instead.</p>
<p>Has anyone had this issue before?</p>
<p>Here's what I've tried with pagination rules in Azure Synapse - all to no success (dataflow only returns one page of data)</p>
<ol>
<li>AbsoluteUrl - Body - {links.next} (pictured)</li>
<li>AbsoluteUrl - Body - links.next</li>
<li>AbsoluteUrl - Body - $.{links.next}</li>
<li>AbsoluteUrl - Body - ['links']['next']</li>
<li>AbsoluteUrl - None - body.{links.next}</li>
<li>AbsoluteUrl - None - body.links.next</li>
<li>Query - page%5Bnumber%5D - Body - {meta.next-page}</li>
<li>Query - page[number] - Body - {meta.next-page}</li>
</ol>
<p>When testing this behavior with Postman or Powershell <code>Invoke-RestMethod</code>, it seems to work correctly.</p>
","<azure><rest><azure-data-factory><azure-synapse>","2022-12-01 15:29:25","75","0","1","74684314","<p>I tried to reproduce your scenario in my environment with your API I am getting some error so tried with my sample API.</p>
<p>My sample API returns the response as,</p>
<pre class=""lang-json prettyprint-override""><code>{{&quot;_embedded&quot;:
.
.
.},
&quot;_links&quot;:{
&quot;first&quot;:{&quot;href&quot;:&quot;https://www.ebi.ac.uk/ols/api/ontologies?page=0&amp;size=20&quot;},
&quot;last&quot;:{&quot;href&quot;:&quot;https://www.ebi.ac.uk/ols/api/ontologies?page=13&amp;size=20&quot;},
&quot;next&quot;:{&quot;href&quot;:&quot;https://www.ebi.ac.uk/ols/api/ontologies?page=1&amp;size=20&quot;},
&quot;self&quot;:{&quot;href&quot;:&quot;https://www.ebi.ac.uk/ols/api/ontologies&quot;}},
&quot;page&quot;:{&quot;number&quot;:&quot;0&quot;,&quot;size&quot;:&quot;20&quot;,&quot;totalElements&quot;:&quot;280&quot;,&quot;totalPages&quot;:&quot;14&quot;}},
&quot;headers&quot;:{.......}
</code></pre>
<p>In <strong>Pagination rules</strong> I gave <strong>AbsoluteUrl</strong> value as <code>body._links.next.href</code> and in <strong>Document form</strong> I gave <code>Document per line</code>.</p>
<p><img src=""https://i.imgur.com/9n2Fzgk.png"" alt=""enter image description here"" /></p>
<p>With the above settings I am able to retrieve all the pages from My sample API.</p>
<p>In your API, the pagination should look like</p>
<p><img src=""https://i.imgur.com/VChibJ6.png"" alt=""enter image description here"" /></p>
<p><strong>Output</strong></p>
<p><img src=""https://i.imgur.com/Da49Bvf.png"" alt=""enter image description here"" /></p>
"
"74639178","The folder name is invalid on selecting SFTP path in Azure data factory?","<p>I am trying to create a dataset for a SFTP linked to service in the azure data factory pipeline. After selecting the path I am getting the error &quot;The folder name is invalid&quot;.
Here is the Image
<a href=""https://i.stack.imgur.com/S5S91.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S5S91.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><linked-service>","2022-12-01 08:53:23","110","0","1","74739095","<p>I created storage account with sftp preview enabled and I created local user to sftp with container named sftp.
Image for reference:
<img src=""https://i.imgur.com/oUCDM54.png"" alt=""enter image description here"" /></p>
<p>I uploaded a csv file into that container.
Image for reference:</p>
<p><img src=""https://i.imgur.com/ZkNCn7o.png"" alt=""enter image description here"" /></p>
<p>I created a sftp linked service in data factory by filling required details.
Image for reference:</p>
<p><img src=""https://i.imgur.com/rSTZFXF.png"" alt=""enter image description here"" /></p>
<p>I created sftp dataset using above linked service and uploaded file successfully.
To upload a file, I click on browse option in file path and my container is opened. In my container I am having input directory.
Image for reference:</p>
<p><img src=""https://i.imgur.com/XKiFAfg.png"" alt=""enter image description here"" /></p>
<p>I click on input directory, and I selected my file.
Image for reference:</p>
<p><img src=""https://i.imgur.com/r5X04sd.png"" alt=""enter image description here"" /></p>
<p>My file is uploaded successfully.
Image for reference:</p>
<p><img src=""https://i.imgur.com/17zwb0e.png"" alt=""enter image description here"" /></p>
<p>dataset is created successfully in data flow.
data preview of dataset:</p>
<p><img src=""https://i.imgur.com/e9Gl1IO.png"" alt=""enter image description here"" /></p>
<p>Once check your directory if the folder is there or not.</p>
"
"74635547","pass SQL Query values to Data Factory variable as array for foreachloop","<p>similar to this question <a href=""https://stackoverflow.com/questions/62900297/how-to-pass-variables-to-azure-data-factory-rest-urls-query-stirng"">how to pass variables to Azure Data Factory REST url&#39;s query stirng</a></p>
<p>However, I have a pipeline to query against graphapi, where I need to pass in a userid as part of the Url to get their manager to build an ActiveDirectory staff hierarchy, this is fine on an individual basis, or even as a predefined array variable where I insert[&quot;xx&quot;,&quot;xxx&quot;] into the pipeline variable etc. My challenge is that I need to pass the results of a SQL query to be the array variable.  So, instead of defining the list of users, I need to pass into the foreach loop the results from a SQL query.</p>
<p>I can use a lookup to a set variable, but the url seems to be misconstructed and has extra characters added in for some reason.</p>
<p>returning graph.microsoft.com/v1.0/users/%7B%7B%22id%22:%22xx9e7878-bwbbb-bwbwbwr-7897-414a8e60c78c%22%7D%7D/?$expand=xxxxxx      where the &quot;%7B%7B%22id%22:%&quot; and &quot;%22%7D%7D/&quot; is all unnecessary and appears to come from the json rather than just utilising the value.
<a href=""https://i.stack.imgur.com/lLSO9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lLSO9.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>The lookup runs the query from SQL</li>
<li>The Set variable uses the lookup value's (below) to  assign to a pipeline variable as an array.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/RKcBO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RKcBO.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><p>then the foreachloop uses the variable value in the source</p>
<p>@concat('users/{',item(),'}/?$expand=manager($levels=max;$select=id,displayName,userPrincipalName,createdDate)')</p>
</li>
</ul>
<p>If anyone can suggest how to construct the array value dynamically that would be great.<br />
I have used</p>
<pre><code>SELECT '[&quot;'+STRING_AGG(CONVERT(NVARCHAR(MAX),t.[id]),'&quot;,&quot;')+'&quot;]' AS id  FROM 
stage.extract_msgraphapi_users t LEFT JOIN stage.extract_msgraphapi_users s ON s.id = t.id 
</code></pre>
<p>and this returns something that looks like an array [&quot;xx&quot;,&quot;xxx&quot;] but data factory still interpreted this as a string and not an array.  Any help would be appreciated.</p>
","<sql><arrays><azure><azure-data-factory>","2022-12-01 00:11:10","183","0","1","74635721","<p>10 minutes later:</p>
<pre><code>@concat('users/{',item().id,'}/?$expand=manager($levels=max;$select=id,displayName,userPrincipalName,createdDate)')
</code></pre>
<p>note the reference to item().id to use the id level of the array.  Works like a dream for anyone else facing the same issue</p>
"
"74633162","Can we Connect On premis Oracle server using Azure hosted IR into DataFactory and use it as a Source and sink?","<p>Here is my situation ,</p>
<p>My Org Oracle server is onpremis .</p>
<p>I wanted to connect it as a source and sink in my Datafactory using Azure Hosted IR .</p>
<p>People recommended to use Self Hosted IR .</p>
<p>But I wanted to use Azure Hosted IR.</p>
<p>Can we do that ? I found some documentation but its not helpful.</p>
<p>Can anyone clearly depict how to do this ?</p>
","<azure><azure-data-factory>","2022-11-30 19:27:18","61","0","1","74636693","<p>Anything that is within a vnet would be accessible to Azure data factory via a bridge that links both the ADF and the object and that bridge is what is the self hosted IR.
So basically you would have to create a server within the same vnet as the database and set up self IR on that server and leverage it in ADF to access that database(both as a source or sink) since it would be a common linked service.</p>
<p><a href=""https://www.techbrothersit.com/2022/01/how-to-install-self-hosted-integration.html"" rel=""nofollow noreferrer"">https://www.techbrothersit.com/2022/01/how-to-install-self-hosted-integration.html</a></p>
"
"74631002","Azure Data Factory Copy Activity for JSON to Table in Azure SQL DB","<p>I have a copy activity that takes a bunch of JSON files and merges them into a singe JSON.</p>
<p><a href=""https://i.stack.imgur.com/zfrSZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zfrSZ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/HBRJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HBRJS.png"" alt=""enter image description here"" /></a></p>
<p>I would now like to copy the merged single JSON to Azure SQL DB. Is that possible?</p>
<p>Ok, it appears to be working however the output in SQL is just countryCode and CompanyId</p>
<p><a href=""https://i.stack.imgur.com/qzEvF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qzEvF.png"" alt=""enter image description here"" /></a></p>
<p>However, I need to retrieve all the financial information in the JSON as well</p>
<p><a href=""https://i.stack.imgur.com/VosM7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VosM7.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-11-30 16:21:37","125","0","1","74637459","<blockquote>
<p>Azure Data Factory Copy Activity for JSON to Table in Azure SQL DB</p>
</blockquote>
<p>I repro'd the same and below are the steps.</p>
<ul>
<li><p>Two json files are taken as source.
<img src=""https://i.imgur.com/Ke6wSWJ.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Those files are merged into single file using copy activity.
<img src=""https://i.imgur.com/N5xrzV8.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/CJypj9M.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then Merged Json data is taken as source dataset in another copy activity.</li>
</ul>
<p><img src=""https://i.imgur.com/uGdk3ii.png"" alt=""enter image description here"" /></p>
<ul>
<li>In sink, dataset for Azure SQL db is created and Auto create table option is selected.</li>
</ul>
<p><img src=""https://i.imgur.com/ZAXlqly.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>In sink dataset, edit checkbox is selected and sink table name is given.
<img src=""https://i.imgur.com/xgHvZEA.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Once the pipeline is run, data is copied to table.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/Iwi2BOZ.png"" alt=""enter image description here"" /></p>
"
"74629505","How do I pull the last modified file with data flow in azure data factory?","<p>I have files that are uploaded into an onprem folder daily, from there I have a pipeline pulling it to a blob storage container (input), from there I have another pipeline from blob (input) to blob (output), here is were the dataflow is, between those two blobs. Finally, I have output linked to sql. However, I want the blob to blob pipeline to pull only the file that was uploaded that day and run through the dataflow. The way I have it setup, every time the pipeline runs, it doubles my files. I've attached images below</p>
<p>[![Blob to Blob Pipeline][1]][1]</p>
<p>Please let me know if there is anything else that would make this more clear
[1]: <a href=""https://i.stack.imgur.com/24Uky.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/24Uky.png</a></p>
","<azure><etl><azure-data-factory>","2022-11-30 14:34:45","340","0","2","74632422","<p>I was able to solve this by selecting &quot;Delete source files&quot; in dataflow. This way the the first pipeline pulls the new daily report into the input, and when the second pipeline (with the dataflow) pulls the file from input to output, it deletes the file in input, hence not allowing it to duplicate <a href=""https://i.stack.imgur.com/FUvje.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUvje.png"" alt=""Dataflow"" /></a></p>
"
"74629505","How do I pull the last modified file with data flow in azure data factory?","<p>I have files that are uploaded into an onprem folder daily, from there I have a pipeline pulling it to a blob storage container (input), from there I have another pipeline from blob (input) to blob (output), here is were the dataflow is, between those two blobs. Finally, I have output linked to sql. However, I want the blob to blob pipeline to pull only the file that was uploaded that day and run through the dataflow. The way I have it setup, every time the pipeline runs, it doubles my files. I've attached images below</p>
<p>[![Blob to Blob Pipeline][1]][1]</p>
<p>Please let me know if there is anything else that would make this more clear
[1]: <a href=""https://i.stack.imgur.com/24Uky.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/24Uky.png</a></p>
","<azure><etl><azure-data-factory>","2022-11-30 14:34:45","340","0","2","74638009","<blockquote>
<p><strong>I want the blob to blob pipeline to pull only the file that was uploaded that day and run through the dataflow.</strong></p>
</blockquote>
<p>To achieve above scenario, you can use <code>Filter by last Modified date</code> by passing the dynamic content as below:</p>
<ul>
<li><code>@startOfDay(utcnow())</code> : It will take start of the day for the current timestamp.</li>
<li><code>@utcnow()</code> : It will take current timestamp.</li>
</ul>
<p><img src=""https://i.imgur.com/RbPV2HK.png"" alt=""enter image description here"" /></p>
<p>Input and Output of Get metadata activity: (Its filtering file for that day only)</p>
<p><img src=""https://i.imgur.com/XOPND59.png"" alt=""enter image description here"" /></p>
<p>If the files are multiple for particular day, then you have to use for each activity and pass the output of Get metadata activity to foreach activity as</p>
<pre><code>@activity('Get Metadata1').output.childItems
</code></pre>
<p><img src=""https://i.imgur.com/XzpJeDo.png"" alt=""enter image description here"" /></p>
<p>Then add Dataflow activity in Foreach and create source dataset with filename parameter</p>
<p><img src=""https://i.imgur.com/pimGawm.png"" alt=""enter image description here"" /></p>
<p>Give filename parameter which is created as dynamic value in filename
<img src=""https://i.imgur.com/vJZwasg.png"" alt=""enter image description here"" /></p>
<p>And then pass source parameter filename as <code>@item().name</code>
<img src=""https://i.imgur.com/27M1OLs.png"" alt=""enter image description here"" /></p>
<p>It will run dataflow for each file get metadata is returning.</p>
"
"74628559","Timeout issue for http connector and web activity on adf","<p>Timeout issue for http connector and web activity</p>
<p>Web activity and http connector on adf</p>
<p>We have tried loading data through Copy Activity using REST API with Json data some columns are getting skipped which is having no data at its first row. We have also tried REST API with cv data but it's throwing error. We have tried using Web Activity but its payload size is 4MB, so it is getting failed with timeout issue. We have tried using HTTP endpoint but its payload size is 0.5 MB, so it is also getting failed with timeout issue</p>
","<azure-data-factory><httpconnection><trusted-web-activity>","2022-11-30 13:27:07","141","1","1","74638440","<ul>
<li><p>In Mapping settings, <strong>Toggle on</strong> the advanced editor and give the respective value in collection reference to cross apply the value for nested Json data. Below is the approach.</p>
</li>
<li><p>Rest connector is used in source dataset. Source Json API is taken as in below image.
<img src=""https://i.imgur.com/3PvzxWg.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then Sink dataset is created for Azure SQL database. Once the pipeline is run, few columns are not copied to database.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/OqElyyJ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Therefore, In Mapping settings of copy actvity,
1. Schema is imported
2. Advanced editor is turned on
3. Collection reference given.</li>
</ul>
<p><img src=""https://i.imgur.com/upx7zkI.png"" alt=""enter image description here"" /></p>
<ul>
<li>When pipeline is run after the above changes, all columns are copied in SQL database.</li>
</ul>
<p><img src=""https://i.imgur.com/ZZzlkyx.png"" alt=""enter image description here"" /></p>
"
"74625737","How to assign json file's content to a variable using powershell","<p>I trying to run below PowerShell script to add azure data factory data sets. But im getting motioned error.
<strong>Json File</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;DSNAME&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;REGNAME1&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureDataExplorerTable&quot;,
        &quot;schema&quot;: [],
        &quot;typeProperties&quot;: {
            &quot;table&quot;: &quot;TABLE_TEST&quot;
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/datasets&quot;
}
</code></pre>
<p><strong>Powershell</strong></p>
<pre><code>az config set extension.use_dynamic_install=yes_without_prompt
Get-ChildItem &quot;ADF_DATASETS/&quot; -Filter *.json | 
Foreach-Object {
    $content = Get-Content $_.FullName

az datafactory dataset create --properties $content --name &quot;DATASETNAME&quot; --factory-name &quot;ADFNAME&quot; --resource-group &quot;RG_TEST&quot;

}
</code></pre>
<p><strong>Error:</strong>
Error detail: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
Please provide a valid JSON file path or JSON string.
The provided JSON string may have been parsed by the shell. See <a href=""https://docs.microsoft.com/cli/azure/use-cli-effectively#use-quotation-marks-in-arguments"" rel=""nofollow noreferrer"">https://docs.microsoft.com/cli/azure/use-cli-effectively#use-quotation-marks-in-arguments</a>
ERROR: Failed to parse string as JSON:</p>
","<azure><powershell><azure-devops><azure-data-factory>","2022-11-30 09:52:19","292","0","1","74629390","<p>What kind of text <code>--properties</code> expect?</p>
<p>It is expecting JSON string in double quotation <a href=""https://learn.microsoft.com/en-us/cli/azure/datafactory/dataset?view=azure-cli-latest#az-datafactory-dataset-create"" rel=""nofollow noreferrer"">marks</a>. So you have to put the value of <code>$content</code> inside the double quotation mark.</p>
<pre><code>az datafactory dataset create --properties &quot;{\&quot;type\&quot;:\&quot;AzureBlob\&quot;,\&quot;linkedServiceName\&quot;:{\&quot;type\&quot;:\&quot;LinkedServiceReference\&quot;,\&quot;referenceName\&quot;:\&quot;exampleLinkedService\&quot;},\&quot;parameters\&quot;:{\&quot;MyFileName\&quot;:{\&quot;type\&quot;:\&quot;String\&quot;},\&quot;MyFolderPath\&quot;:{\&quot;type\&quot;:\&quot;String\&quot;}},\&quot;typeProperties\&quot;:{\&quot;format\&quot;:{\&quot;type\&quot;:\&quot;TextFormat\&quot;},\&quot;fileName\&quot;:{\&quot;type\&quot;:\&quot;Expression\&quot;,\&quot;value\&quot;:\&quot;@dataset().MyFileName\&quot;},\&quot;folderPath\&quot;:{\&quot;type\&quot;:\&quot;Expression\&quot;,\&quot;value\&quot;:\&quot;@dataset().MyFolderPath\&quot;}}}&quot; --name &quot;exampleDataset&quot; --factory-name &quot;exampleFactoryName&quot; --resource-group &quot;exampleResourceGroup&quot;
</code></pre>
"
"74618203","Azure Data Factory with Data Bricks Notebook Execute Pipeline Error","<p>I am trying to execute my databricks note book and linked service as execution Pool type of connection, also I have upload the Append libraries option for wheel format library in ADF but unable to execute our notebook via ADF and getting below error.</p>
<p>Run result unavailable: job failed with error message Library installation failed for library due to user error for whl:</p>
<blockquote>
<p>&quot;dbfs:/FileStore/jars/xxxxxxxxxxxxxxxxxxxx/prophet-1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&quot;
. Error messages: Library installation attempted on the driver node of
cluster 1129-161441-xwjfzl6k and failed. Please refer to the following
error message to fix the library or contact Databricks support. Error
Code: DRIVER_LIBRARY_INSTALLATION_FAILURE. Error Message:
org.apache.spark.SparkException: Process List(bash,
/local_disk0/.ephemeral_nfs/cluster_libraries/python/python_start_clusterwide.sh,
/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/pip, install,
--upgrade, --find-links=/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages,
/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/prophet-1.1-cp38-cp38-manylinux_2_17_x86_6
... *<strong>WARNING: message truncated. Skipped 195 bytes of output</strong></p>
</blockquote>
<p>Kindly help us. and in linked in service, there is three types of option we have(Select cluster),
1.new job cluster
2.exixting interactive cluster
3.Existing instance pool</p>
<p>in production perspective which is the best, we do not have any job created in databricks and plan note book needs to trigger in adf to success the execution. please advice</p>
","<azure-databricks><azure-data-factory>","2022-11-29 17:47:09","457","0","2","74621233","<p>Make sure you install the wheel onto the interactive cluster (option 2).  This has nothing to do with Azure Data Bricks.</p>
<p><a href=""https://stackoverflow.com/questions/72778547/installing-local-whl-files-on-databricks-cluster"">Installing local .whl files on Databricks cluster</a></p>
<p>See the above article for details.</p>
"
"74618203","Azure Data Factory with Data Bricks Notebook Execute Pipeline Error","<p>I am trying to execute my databricks note book and linked service as execution Pool type of connection, also I have upload the Append libraries option for wheel format library in ADF but unable to execute our notebook via ADF and getting below error.</p>
<p>Run result unavailable: job failed with error message Library installation failed for library due to user error for whl:</p>
<blockquote>
<p>&quot;dbfs:/FileStore/jars/xxxxxxxxxxxxxxxxxxxx/prophet-1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&quot;
. Error messages: Library installation attempted on the driver node of
cluster 1129-161441-xwjfzl6k and failed. Please refer to the following
error message to fix the library or contact Databricks support. Error
Code: DRIVER_LIBRARY_INSTALLATION_FAILURE. Error Message:
org.apache.spark.SparkException: Process List(bash,
/local_disk0/.ephemeral_nfs/cluster_libraries/python/python_start_clusterwide.sh,
/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/pip, install,
--upgrade, --find-links=/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages,
/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/prophet-1.1-cp38-cp38-manylinux_2_17_x86_6
... *<strong>WARNING: message truncated. Skipped 195 bytes of output</strong></p>
</blockquote>
<p>Kindly help us. and in linked in service, there is three types of option we have(Select cluster),
1.new job cluster
2.exixting interactive cluster
3.Existing instance pool</p>
<p>in production perspective which is the best, we do not have any job created in databricks and plan note book needs to trigger in adf to success the execution. please advice</p>
","<azure-databricks><azure-data-factory>","2022-11-29 17:47:09","457","0","2","74650083","<p>Karthik from the error it is complaining about the library . This is what i could have done .</p>
<ol>
<li>Cross check &amp; make sure that the ADF is pointing the correct cluster .</li>
<li>If The cluster is correct , move on the cluster and open the notebook which you are trying to refer from ADF . try to execute that .</li>
<li>If the notebook works fine , go and stop the cluster and restart it again and run the notebook .
My guess is that once the cluster goes into the idle mode and shutsdown and then when ADF starts the cluster , it is not able to find the library it needs .</li>
</ol>
"
"74618111","How to convert missing value in csv file to database null value using ADF copy activity?","<p>I have a pipeline in Azure Data Factory to take in incoming CSV files and save them to SQL server database, and I use a copy activity to take the wrangled CSV file and call a stored procedure to save it the data base table.</p>
<p>However, it is not unusual that some records in the CSV file have missing value at some columns. Such missing value will fail copy activity and below is the error message:</p>
<blockquote>
<p>ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property '' is invalid: 'Cannot set Column 'col 1' to be null. Please use DBNull instead.'</p>
</blockquote>
<p>The copy activity runs correctly when there is no missing value in the incoming data.</p>
<p>Below is the snippet of the stored procedure that fails the execution when encounter missing value(s).</p>
<pre><code>INSERT INTO target_table(
    [Id],
    [col 1],
    [col 2],
    [col 3]
)    
SELECT
    [source Id],
    [column 1],
    [column 2],
    [column 3]
FROM source_table
</code></pre>
<p><strong>My question is what I can do to convert the missing value in CSV file into a null value that SQL server understand.</strong></p>
<p>I orignally thought the problem is at the database side, so I created a test table in SQL Server and put some test data intentionally with missing values into a test table, then I run the stored procedure. These records with missing value get saved to the target table correctly. So I realized that the problem lies when the copy activity takes in the CSV file and pass it to the stored procedure, and the missing values didn't get translated well into a null value that SQL Server can understand.</p>
","<sql-server><azure-data-factory>","2022-11-29 17:39:12","142","0","3","74626423","<p>You can use dataflow activity to set the value as NULL. Below is the approach</p>
<ul>
<li>In Dataflow, Source data is taken as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/9s48SnQ.png"" alt=""enter image description here"" /></p>
<ul>
<li><strong>Derived column transformation</strong> is taken and expression is given as <code>iifNull(id,toString(null()))</code></li>
</ul>
<p><img src=""https://i.imgur.com/v4jpF3O.png"" alt=""enter image description here"" /></p>
<ul>
<li><strong>Result</strong>
<img src=""https://i.imgur.com/1F3Ilxl.png"" alt=""enter image description here"" /></li>
</ul>
"
"74618111","How to convert missing value in csv file to database null value using ADF copy activity?","<p>I have a pipeline in Azure Data Factory to take in incoming CSV files and save them to SQL server database, and I use a copy activity to take the wrangled CSV file and call a stored procedure to save it the data base table.</p>
<p>However, it is not unusual that some records in the CSV file have missing value at some columns. Such missing value will fail copy activity and below is the error message:</p>
<blockquote>
<p>ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property '' is invalid: 'Cannot set Column 'col 1' to be null. Please use DBNull instead.'</p>
</blockquote>
<p>The copy activity runs correctly when there is no missing value in the incoming data.</p>
<p>Below is the snippet of the stored procedure that fails the execution when encounter missing value(s).</p>
<pre><code>INSERT INTO target_table(
    [Id],
    [col 1],
    [col 2],
    [col 3]
)    
SELECT
    [source Id],
    [column 1],
    [column 2],
    [column 3]
FROM source_table
</code></pre>
<p><strong>My question is what I can do to convert the missing value in CSV file into a null value that SQL server understand.</strong></p>
<p>I orignally thought the problem is at the database side, so I created a test table in SQL Server and put some test data intentionally with missing values into a test table, then I run the stored procedure. These records with missing value get saved to the target table correctly. So I realized that the problem lies when the copy activity takes in the CSV file and pass it to the stored procedure, and the missing values didn't get translated well into a null value that SQL Server can understand.</p>
","<sql-server><azure-data-factory>","2022-11-29 17:39:12","142","0","3","74650107","<p>Have you tried this option in Copy activity ?
<a href=""https://i.stack.imgur.com/vdl0M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vdl0M.png"" alt=""enter image description here"" /></a></p>
<p>This should do that trick .</p>
"
"74618111","How to convert missing value in csv file to database null value using ADF copy activity?","<p>I have a pipeline in Azure Data Factory to take in incoming CSV files and save them to SQL server database, and I use a copy activity to take the wrangled CSV file and call a stored procedure to save it the data base table.</p>
<p>However, it is not unusual that some records in the CSV file have missing value at some columns. Such missing value will fail copy activity and below is the error message:</p>
<blockquote>
<p>ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property '' is invalid: 'Cannot set Column 'col 1' to be null. Please use DBNull instead.'</p>
</blockquote>
<p>The copy activity runs correctly when there is no missing value in the incoming data.</p>
<p>Below is the snippet of the stored procedure that fails the execution when encounter missing value(s).</p>
<pre><code>INSERT INTO target_table(
    [Id],
    [col 1],
    [col 2],
    [col 3]
)    
SELECT
    [source Id],
    [column 1],
    [column 2],
    [column 3]
FROM source_table
</code></pre>
<p><strong>My question is what I can do to convert the missing value in CSV file into a null value that SQL server understand.</strong></p>
<p>I orignally thought the problem is at the database side, so I created a test table in SQL Server and put some test data intentionally with missing values into a test table, then I run the stored procedure. These records with missing value get saved to the target table correctly. So I realized that the problem lies when the copy activity takes in the CSV file and pass it to the stored procedure, and the missing values didn't get translated well into a null value that SQL Server can understand.</p>
","<sql-server><azure-data-factory>","2022-11-29 17:39:12","142","0","3","74658529","<p>After various attempts, here is my solution to the problem. It is not ideal but it works. The solution is that I created a permament staging table in SQL Server and use the copy actity to transfer the CSV data to this staging table. The trick is to use the insert option in the copy activity (see the picture) instead of using a stored procedure which is what I tried to do previously. <a href=""https://i.stack.imgur.com/M6sLE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M6sLE.png"" alt=""enter image description here"" /></a> It feels like there is some internal mechanism behind the scene between copy activity and SQL server that handles the missing value. Once the data is saved in the staging table in SQL Server, I can easily do whatever I want in the database world and missing value is no longer an issue.</p>
"
"74615088","multiple sessions are created when inserting data into SQL table using ADF Pipeline through Copy Activity","<p>i am using ADF Copy activity to move data from multiple Source SQL table into different Target SQL table. This is iterating parallel in loop using for each activity. but when it starts multiple sessions are generated (8-10 sessions) in the background for every copy activity. i am not using any parallelism/concurrent/bulk copy settings of copy Activity.
Please help me why this session is being created?</p>
","<sql><sql-server><azure><azure-data-factory>","2022-11-29 13:46:04","138","0","1","74627951","<p>When you don't select Sequential in ForEach, it will execute Copy activities inside ForEach parallelly at the same time like below.</p>
<p><img src=""https://i.imgur.com/76oXXFL.png"" alt=""enter image description here"" /></p>
<p>Check the <strong>Sequential in ForEach activity</strong> as suggested by <strong>@Scott Mildenberger</strong></p>
<p><img src=""https://i.imgur.com/hPNUCWg.png"" alt=""enter image description here"" /></p>
<p>Now, copy activities inside ForEach will be executed one after another in Sequential manner.</p>
<p><img src=""https://i.imgur.com/QnWdbYv.png"" alt=""enter image description here"" /></p>
"
"74614576","How to disable Auto update of Integration Runtime in Azure Data Factory?","<p>I have self-hosted integration runtime.
&quot;Edit integration runtime&quot; in Azure Portal says that &quot;Auto update&quot; is &quot;Enabled&quot;.
However I cannot change it to disabled. Schedule times are also gray and not editable.</p>
","<azure-data-factory>","2022-11-29 13:07:57","242","0","1","74625773","<p>You can go to Integration runtime tab under manage section and Edit your self hosted integration runtime . Go to Auto Update tab and select disable and click apply.</p>
<p>If the buttons are showing greyed out for you, it means you are lacking some required permissions. Check with your administrator if they can disable.</p>
<p>Here is a short video of the steps:</p>
<p><a href=""https://i.stack.imgur.com/PMvVS.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PMvVS.gif"" alt=""enter image description here"" /></a></p>
"
"74614368","I'm unable to map drift columns in Azure Data Factory (ADF)","<p>I can't map drift my columns in ADF data flow. I'm able to manually, but this isn't possible as I have 1020 columns. File is .csv</p>
<p>I see a message: 'This drifted column is not in the source schema and therefore can only be referenced with pattern matching expressions'</p>
<p>I was hoping to have a map drifted data flow from my source data.</p>
","<azure><azure-data-factory>","2022-11-29 12:49:25","151","0","1","74623362","<p>With &gt; 1k columns, you should consider NOT mapping those columns. Just use column patterns inside your transformation expressions to access columns. Otherwise, ADF will have to materialize the entire 1k+ columns as a physical projection.</p>
"
"74613108","Issue in converting String datetime to timestamp In Azure Data Flow","<p>I have done the following in derived column to convert string column to timestamp</p>
<p><strong>toTimestamp({QIR Date},'yyyy-MM-dd HH:mm:ss')</strong></p>
<p>i/p col has 2021-01-26 11:44:45
I am getting O/p as 2021-01-26 11:44:45.000
I need to eliminate last zeros coming.I know we can do like :
<strong>toString(toTimestamp({QIR Date},'yyyy-MM-dd HH:mm:ss'),'yyyy-MM-dd HH:mm:ss')</strong></p>
<p>But i need the datatype as timestamp at the end . Conversion is adding 000 at the end . Can anyone help or am I approaching this wrong.</p>
","<azure><azure-data-factory>","2022-11-29 11:09:08","115","0","1","74638564","<p>You can try rtrim(toTimestamp({QIR Date},'yyyy-MM-dd HH:mm:ss'),'000')</p>
<p><a href=""https://i.stack.imgur.com/LDxkG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LDxkG.png"" alt=""enter image description here"" /></a></p>
"
"74611149","Terraform - ADF to DB connectivity issue when tenant_id is provided in LS configuration - azurerm_data_factory_linked_service_azure_sql_database","<h3>Terraform Version</h3>
<p>1.2.3</p>
<h3>AzureRM Provider Version</h3>
<p>v3.13.0</p>
<h3>Affected Resource(s)/Data Source(s)</h3>
<p>Azure data factory, SQL Database</p>
<h3>Terraform Configuration Files</h3>
<pre><code>resource &quot;azurerm_data_factory_linked_service_azure_sql_database&quot; &quot;sqldatabase_linked_service_10102022&quot; {
  count = (var.subResourcesInfo.sqlDatabaseName != &quot;&quot;) ? 1 : 0
  depends_on = [azurerm_data_factory_integration_runtime_azure.autoresolve_integration_runtime,
  azurerm_data_factory_managed_private_endpoint.sqlserver_managed_endpoint]

  name            = &quot;AzureSqlDatabase10102022&quot;
  data_factory_id = azurerm_data_factory.datafactory.id

  integration_runtime_name = &quot;AutoResolveIntegrationRuntime&quot;
  use_managed_identity     = true
  connection_string = format(&quot;Integrated Security=False;Data Source=%s.database.windows.net;Initial Catalog=%s;&quot;,
    var.subResourcesInfo.sqlServerName,
  var.subResourcesInfo.sqlDatabaseName)
}
</code></pre>
<h3>Expected Behaviour</h3>
<p>Issue is ADF to DB connectivity, error:</p>
<p>Operation on target DWH_DF_aaa failed: {'StatusCode':'DFExecutorUserError','Message':'Job failed due to reason: com.microsoft.dataflow.broker.InvalidOperationException: Only one valid authentication should be used for AzureSqlDatabase. ServicePrincipalAuthentication is invalid. One or two of servicePrincipalId/key/tenant is missing.','Details':''}</p>
<p>When we created this LS using TF, we get tenant=&quot;&quot; in ADF LS Json file which we suspect that causing issue of above error.</p>
<p>When we created the same LS directly on ADF UI, then there is no field of tenant=&quot;&quot; in its json file, and if we use this LS in dataflow/pipeline then communication works from ADF to DB.</p>
<p>Expected behavior should be, if we don't provide tenant_id parameter in TF code then in json also should not show tenant=&quot;&quot; which then works for connectivity.</p>
","<azure><terraform><azure-data-factory><terraform-provider-azure>","2022-11-29 08:35:51","140","0","1","74652378","<p><em><strong>I tried to reproduce the scenario in my environment:</strong></em></p>
<p>With below code , I could create a Linked Service (connection) between Azure SQL Database and Azure Data Factory.</p>
<p><em><strong>Code:</strong></em></p>
<pre><code>resource &quot;azurerm_data_factory&quot; &quot;example&quot; {
  name                            = &quot;kaADFexample&quot;
  location                        = data.azurerm_resource_group.example.location
  resource_group_name             = data.azurerm_resource_group.example.name
  managed_virtual_network_enabled = true
}

resource &quot;azurerm_storage_account&quot; &quot;example&quot; {
  name                     = &quot;kaaaexample&quot;
  resource_group_name      = data.azurerm_resource_group.example.name
  location                 = data.azurerm_resource_group.example.location
  account_kind             = &quot;BlobStorage&quot;
  account_tier             = &quot;Standard&quot;
  account_replication_type = &quot;LRS&quot;
}

resource &quot;azurerm_data_factory_managed_private_endpoint&quot; &quot;example&quot; {
  name               = &quot;example&quot;
  data_factory_id    = azurerm_data_factory.example.id
  target_resource_id = azurerm_storage_account.example.id
  subresource_name   = &quot;blob&quot;
}


resource &quot;azurerm_user_assigned_identity&quot; &quot;main&quot; {
    depends_on = [data.azurerm_resource_group.example]
    name = &quot;kasupports01-mid&quot;
    resource_group_name = data.azurerm_resource_group.example.name
    location = data.azurerm_resource_group.example.location
}


resource &quot;azurerm_data_factory_integration_runtime_azure&quot; &quot;test&quot; {
  name                    = &quot;AutoResolveIntegrationRuntime&quot;
  data_factory_id      = azurerm_data_factory.example.id
  location                = &quot;AutoResolve&quot;
  virtual_network_enabled = true
}

resource &quot;azurerm_data_factory_linked_service_azure_sql_database&quot; &quot;linked_service_azure_sql_database&quot; {
  name                     = &quot;kaexampleLS&quot;
  data_factory_id          = azurerm_data_factory.example.id
  connection_string        = &quot;data source=serverhostname;initial catalog=master;user id=testUser;Password=test;integrated security=False;encrypt=True;connection timeout=30&quot;
  use_managed_identity     = true
  integration_runtime_name = azurerm_data_factory_integration_runtime_azure.test.name
  
  

depends_on = [azurerm_data_factory_integration_runtime_azure.test,
azurerm_data_factory_managed_private_endpoint.example]

}



output &quot;id&quot; {
  value = azurerm_data_factory_linked_service_azure_sql_database.linked_service_azure_sql_database.id
}
</code></pre>
<p>Executed:  terraform plan</p>
<p><a href=""https://i.stack.imgur.com/jBipK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jBipK.png"" alt=""enter image description here"" /></a></p>
<p><em><strong>Output:</strong></em></p>
<pre><code>id = &quot;/subscriptions/xxxxxxxxx/resourceGroups/xxxxxx/providers/Microsoft.DataFactory/factories/kaADFexample/linkedservices/kaexampleLS&quot;
</code></pre>
<p>If the error persists in your case ,try  removing the  <em><strong><code>tenant</code></strong></em> attribute in the data_factory just after deployment is done in terraform.</p>
<p><a href=""https://i.stack.imgur.com/Bquo9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bquo9.png"" alt=""enter image description here"" /></a></p>
<p>Please check this known issue and mentioned by  @chgenzel in <a href=""https://github.com/hashicorp/terraform-provider-azurerm/issues/14577"" rel=""nofollow noreferrer"">terraform-provider-azurerm issues | Github</a></p>
<p><strong>ADF</strong>:</p>
<p><a href=""https://i.stack.imgur.com/dN42N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dN42N.png"" alt=""enter image description here"" /></a></p>
<p><strong>Managed Identity</strong></p>
<p><a href=""https://i.stack.imgur.com/OGFyw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OGFyw.png"" alt=""enter image description here"" /></a></p>
<p><strong>Linked service</strong> : azure sql</p>
<p><a href=""https://i.stack.imgur.com/Z4Pt5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z4Pt5.png"" alt=""enter image description here"" /></a></p>
<p><strong>Reference:</strong> <a href=""https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/data_factory_linked_service_azure_sql_database#integration_runtime_name"" rel=""nofollow noreferrer"">data_factory_linked_service_azure_sql_database | terraformregistry</a></p>
"
"74607242","Azure Data Factory: Lookup varbinary column in SQL DB for use in a Script activity to write to another SQL DB - ByteArray is not supported","<p>I'm trying to insert into an on-premises SQL database table called PictureBinary:</p>
<p><a href=""https://i.stack.imgur.com/x99lr.png"" rel=""nofollow noreferrer"">PictureBinary table</a></p>
<p>The source of the binary data is a table in another on-premises SQL database called DocumentBinary:</p>
<p><a href=""https://i.stack.imgur.com/zwxBk.png"" rel=""nofollow noreferrer"">DocumentBinary table</a></p>
<p>I have a file with all of the Id's of the DocumentBinary rows that need copying. I feed those into a ForEach activity from a Lookup activity. Each of these files has about 180 rows (there are 50 files fed into a new instance of the pipeline in parallel).</p>
<p><a href=""https://i.stack.imgur.com/WBgsJ.png"" rel=""nofollow noreferrer"">Lookup and ForEach Activities</a></p>
<p>So far everything is working. But then, inside the ForEach I have another Lookup activity that tries to get the binary info to pass into a script that will insert it into the other database.</p>
<p><a href=""https://i.stack.imgur.com/ou2NA.png"" rel=""nofollow noreferrer"">Lookup Binary column</a></p>
<p>And then the Script activity would insert the binary data into the table PictureBinary (in the other database).</p>
<p><a href=""https://i.stack.imgur.com/mXsP9.png"" rel=""nofollow noreferrer"">Script to Insert Binary data</a></p>
<p>But when I debug the pipeline, I get this error when the binary column Lookup is reached:</p>
<blockquote>
<p>ErrorCode=DataTypeNotSupported,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column: coBinaryData,The data type ByteArray is not supported from the column named coBinaryData.,Source=,'</p>
</blockquote>
<p>I know that the accepted way of storing the files would be to store them on the filesystem and just store the file path to the files in the database. But we are using a NOP database that stores the files in varbinary columns.</p>
<p>Also, if there is a better way of doing this, please let me know.</p>
","<sql-server><azure-data-factory>","2022-11-28 22:41:37","216","0","1","74611256","<p>I tried to reproduce your scenario in my environment and got similar error</p>
<p><img src=""https://i.imgur.com/fMKSZj8.png"" alt=""enter image description here"" /></p>
<p>As per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Microsoft document</a> Columns with datatype Byte Array Are not supported in lookup activity is might be the main cause of error.</p>
<p><img src=""https://i.imgur.com/RFgfK0F.png"" alt=""enter image description here"" /></p>
<p><strong>To workaround this as Follow below steps:</strong></p>
<p>As you explained your case you have a file in which all the Id's of the DocumentBinary rows that need copy in destination are stored. To achieve this, <strong>you can simply use Copy activity with the Query</strong> where you copy records where the <strong><code>DocumentBinary</code>  in column is equal to the <code>Id</code> stored in file</strong></p>
<p>First, I took lookup activity from where I can get Id's of the DocumentBinary rows stored in file</p>
<p><img src=""https://i.imgur.com/yW8jwCl.png"" alt=""enter image description here"" /></p>
<p>Then I took ForEach I passed the output of lookup activity to ForEach activity.</p>
<p><img src=""https://i.imgur.com/GeOtppd.png"" alt=""enter image description here"" /></p>
<p>After this I took Copy activity in forEach activity</p>
<pre class=""lang-sql prettyprint-override""><code>Select * from DocumentBinary
where coDocumentBinaryId = '@{item().PictureId}'
</code></pre>
<p>In source of copy activity <strong>select Use query as</strong> <code>Query</code> and pass above query with your names</p>
<p><img src=""https://i.imgur.com/82QP9wG.png"" alt=""enter image description here"" /></p>
<p>Now <strong>go to Mapping Click</strong> on <code>Import Schema</code> then delete unwanted columns and map the columns accordingly.</p>
<blockquote>
<p><strong>Note</strong>: For this, columns in both tables are of similar datatypes either both <code>uniqueidenntifier</code> or both should be <code>int</code></p>
</blockquote>
<p><img src=""https://i.imgur.com/UsOM0Bh.png"" alt=""enter image description here"" /></p>
<p>Sample Input in file:</p>
<p><img src=""https://i.imgur.com/0tM7Zki.png"" alt=""enter image description here"" /></p>
<p><strong>Output (Copied only picture id contend in file from source to destination):</strong></p>
<p><img src=""https://i.imgur.com/BTlth01.png"" alt=""enter image description here"" /></p>
"
"74606698","ADF Copy data from Azure Data Bricks Delta Lake to Azure Sql Server","<p>I'm trying to use the data copy activity to extract information from azure databricks delta lake, but I've noticed that it doesn't pass the information directly from the delta lake to the SQL server I need, but must pass it to an azure blob storage, when running it, it throws the following error</p>
<pre><code>ErrorCode=AzureDatabricksCommandError,Hit an error when running the command in Azure Databricks. Error details: Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key Caused by: Invalid configuration value detected for fs.azure.account.key
</code></pre>
<p>Looking for information I found a possible solution but it didn't work.</p>
<p><a href=""https://stackoverflow.com/questions/72724911/invalid-configuration-value-detected-for-fs-azure-account-key-copy-activity-fail"">Invalid configuration value detected for fs.azure.account.key copy activity fails</a></p>
<p>Does anyone have any idea how the hell to pass information from an azure databricks delta lake table to a table in Sql Server??</p>
<p>These are some images of the structure that I have in ADF:</p>
<p><a href=""https://i.stack.imgur.com/JuZgl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JuZgl.jpg"" alt=""enter image description here"" /></a></p>
<p>In the image I get a message that tells me that I must have a Storage Account to continue</p>
<p>These are the configuration images, and execution failed:</p>
<p>Conf:
<a href=""https://i.stack.imgur.com/6tFgB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6tFgB.jpg"" alt=""enter image description here"" /></a></p>
<p>Fail:
<a href=""https://i.stack.imgur.com/aq0Kp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aq0Kp.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ZQUnl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZQUnl.jpg"" alt=""enter image description here"" /></a></p>
<p>Thank you very much</p>
","<azure-databricks><azure-data-factory>","2022-11-28 21:35:04","936","0","2","74613423","<blockquote>
<p>Does anyone have any idea how the hell to pass information from an azure databricks delta lake table to a table in Sql Server??</p>
</blockquote>
<p>To achieve Above scenario, follow below steps:</p>
<p><strong>First go to your Databricks cluster Edit it</strong> and under <strong>Advance options &gt;&gt; spark &gt;&gt; spark config</strong> Add below code if you are using blob storage.</p>
<pre><code>spark.hadoop.fs.azure.account.key.&lt;storageaccountname&gt;.blob.core.windows.net &lt;Accesskey&gt;
spark.databricks.delta.optimizeWrite.enabled true 
spark.databricks.delta.autoCompact.enabled true 
</code></pre>
<p><img src=""https://i.imgur.com/CpUP6Ds.png"" alt=""enter image description here"" /></p>
<p>After that as you are using SQL Database as a sink.
<code>Enable staging</code> and give same blob storage account linked service as <code>Staging account linked service</code> give storage path from your blob storage.</p>
<p><img src=""https://i.imgur.com/GxJpeME.png"" alt=""enter image description here"" /></p>
<p>And then debug it. make sure you complete Prerequisites from official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-databricks-delta-lake?tabs=data-factory#prerequisites"" rel=""nofollow noreferrer"">document</a>.</p>
<p><strong>My sample Input:</strong></p>
<p><img src=""https://i.imgur.com/YFsqWx8.png"" alt=""enter image description here"" /></p>
<p><strong>Output in SQL:</strong></p>
<p><img src=""https://i.imgur.com/UuYpUwC.png"" alt=""enter image description here"" /></p>
"
"74606698","ADF Copy data from Azure Data Bricks Delta Lake to Azure Sql Server","<p>I'm trying to use the data copy activity to extract information from azure databricks delta lake, but I've noticed that it doesn't pass the information directly from the delta lake to the SQL server I need, but must pass it to an azure blob storage, when running it, it throws the following error</p>
<pre><code>ErrorCode=AzureDatabricksCommandError,Hit an error when running the command in Azure Databricks. Error details: Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key Caused by: Invalid configuration value detected for fs.azure.account.key
</code></pre>
<p>Looking for information I found a possible solution but it didn't work.</p>
<p><a href=""https://stackoverflow.com/questions/72724911/invalid-configuration-value-detected-for-fs-azure-account-key-copy-activity-fail"">Invalid configuration value detected for fs.azure.account.key copy activity fails</a></p>
<p>Does anyone have any idea how the hell to pass information from an azure databricks delta lake table to a table in Sql Server??</p>
<p>These are some images of the structure that I have in ADF:</p>
<p><a href=""https://i.stack.imgur.com/JuZgl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JuZgl.jpg"" alt=""enter image description here"" /></a></p>
<p>In the image I get a message that tells me that I must have a Storage Account to continue</p>
<p>These are the configuration images, and execution failed:</p>
<p>Conf:
<a href=""https://i.stack.imgur.com/6tFgB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6tFgB.jpg"" alt=""enter image description here"" /></a></p>
<p>Fail:
<a href=""https://i.stack.imgur.com/aq0Kp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aq0Kp.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ZQUnl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZQUnl.jpg"" alt=""enter image description here"" /></a></p>
<p>Thank you very much</p>
","<azure-databricks><azure-data-factory>","2022-11-28 21:35:04","936","0","2","74632597","<p>The solution for this problem was the following:</p>
<p>Correct the way the Storage Access Key configuration was being defined:</p>
<p>in the instruction: spark.hadoop.fs.azure.account.key..blob.core.windows.net </p>
<p>The following change must be made:
spark.hadoop.fs.azure.account.key.
storageaccountname.<strong>dfs</strong>.core.windows.net</p>
"
"74605028","Azure Data Factory Copy Pipeline with Geography Data Type","<p>I am trying to get a geography data type from a production DB to another DB on a nightly occurrence. I really wanted to leverage upsert as the write activity, but it seems that geography is not supported with this method. I was reading a similar post about bringing the data through ADF as a well known text data type and then changing it, but I keep getting confused on what to do with the data once it is brought over as a well known data type. I would appreciate any advice, thank you.</p>
<p>Tried to utilize ADF pipelines and data flows. Tried to convert the data type once it was in the destination, but then I was not able to run the pipeline again.</p>
","<azure-data-factory><upsert><sqlgeography>","2022-11-28 18:30:59","105","0","1","74614216","<ul>
<li>I tried to upsert the data with geography datatype from one Azure SQL database to another using copy activity and got error message.</li>
</ul>
<p><img src=""https://i.imgur.com/dafO40u.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Then, I did the upsert using dataflow activity. Below are the steps.</p>
</li>
<li><p>A source table is taken in dataflow as in below image.</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>CREATE  TABLE SpatialTable
( id int ,
GeogCol1 geography,
GeogCol2 AS GeogCol1.STAsText() );

INSERT  INTO SpatialTable (id,GeogCol1)
VALUES (1,geography::STGeomFromText('LINESTRING(-122.360 46.656, -122.343 46.656 )', 4326));

INSERT  INTO SpatialTable (id,GeogCol1)
VALUES (2,geography::STGeomFromText('POLYGON((-122.357 47.653 , -122.348 47.649, -122.348 47.658, -122.358 47.658, -122.358 47.653))', 4326));
</code></pre>
<p><img src=""https://i.imgur.com/USQ1lPc.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Then <strong>Alter Row</strong> transformation is taken and in Alter Row Conditions, <strong>Upsert if</strong> <code>isNull(id)==false()</code>is given. (Based on the column id, sink table upserted)
<img src=""https://i.imgur.com/UGxo5HP.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then, in Sink dataset for target table is given. In sink settings, Update method is selected as <strong>Allow Upsert</strong> and required Key column is given. (Here column <strong>id</strong> is selected)
<img src=""https://i.imgur.com/4gmcxda.png"" alt=""enter image description here"" /></p>
</li>
<li><p>When pipeline is run for the first time, data is inserted into target table.</p>
</li>
<li><p>When pipeline is run for the second time by updating the existing data and inserting new records to source, data is upserted correctly.</p>
</li>
</ul>
<p><strong>Source</strong> Data is changed for id=1 and new row is inserted with id=3
<img src=""https://i.imgur.com/rW0IbHN.png"" alt=""enter image description here"" /></p>
<p><strong>Sink</strong> data is reflecting the changes done in source.
<img src=""https://i.imgur.com/CuigKVi.png"" alt=""enter image description here"" /></p>
"
"74604341","How can you only an ADF execute task based on a variable value","<p>I have only been able to find answers to this based on SSIS.
I have a chain of tasks within ADF that I'd only like to execute based on the evaluated value of a parameter within the container.</p>
<p>I've explored the if condition task but it seems to require an activity assigned per condition (e.g. true or false). I am thinking there has to be a way to simply evaluate a parameter value and then based on the value of the parameter determine whether to abort or execute the subsequent chain of tasks.</p>
<p>I am very familiar with SSIS but ADF seems to lack some of the simplicity especially for this particular task.</p>
<p>So what is the simplest way to proceed to another ADF execute task based on a variable value?</p>
","<azure-data-factory>","2022-11-28 17:26:58","153","1","1","74610323","<p>ADF supports only if activity for the above kind of True or false condition checking.</p>
<pre><code>if(True)
{
    True activities
}
else
{
    False activities
}
</code></pre>
<p>In ADF also if activity works like above.</p>
<p><img src=""https://i.imgur.com/6mMBu5L.png"" alt=""enter image description here"" /></p>
<p>If you don't want to use if activity, you can use a <strong>set variable for condition checking like below</strong>.</p>
<p>Use <strong>success of set variable for True activities and failure for False activities</strong>. Here I have used two set variables for True and False activities.</p>
<p><img src=""https://i.imgur.com/MEr8paS.png"" alt=""enter image description here"" /></p>
<p>You can see for T<strong>rue of dynamic content if</strong>, I have given a string value to string variable by <strong>which activity will succeed(True activities)</strong>. For <strong>false of dynamic content if</strong>, I am assigning <strong>integer value to string variable which will generate error(False activities).</strong></p>
<p><strong>Boolean variable var is <code>True</code>:</strong></p>
<p><img src=""https://i.imgur.com/l1ObMw1.png"" alt=""enter image description here"" /></p>
<p><strong>Boolean variable var is <code>False</code>:</strong></p>
<p><img src=""https://i.imgur.com/iO5iBHu.png"" alt=""enter image description here"" /></p>
"
"74601067","is it possible to restrict access to azure portal from certain locations?","<p>I am working with an EU based client and we are using Azure environment</p>
<p>Now, since tools like ADF and Databricks are browser based  and can be used to display/download data outside the EU -- we need to find a way to control such access.</p>
<p>Suggestions are welcome.</p>
<hr />
<p>If there are any approaches using VPN/client provided VMs, those are welcome too.</p>
","<azure-databricks><azure-data-factory><azureportal>","2022-11-28 13:09:02","116","0","1","74601178","<p>Unfortunately , with the current scope it is not possible to restrict the ADF access via Vnet. It can be accessed from anywhere but this request is currently in MSFT backlog with many employees requesting the same feature</p>
<p>I am not sure of ADB though</p>
<p>a similar thread : <a href=""https://learn.microsoft.com/en-us/answers/questions/1034267/azure-data-factory-portal-is-accesible-over-intern.html"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/1034267/azure-data-factory-portal-is-accesible-over-intern.html</a></p>
<p><a href=""https://stackoverflow.com/questions/74349831/even-after-setting-up-the-connect-via-private-endpoint-azure-data-factory-remai/74355332#74355332"">Even after setting up the connect via private endpoint, Azure Data Factory remains accessible over the Internet?</a></p>
"
"74600099","Reading folders with Date format (YYYY-MM) using Azure Data Factory","<p>I have few folders inside the Data lake (Example:Test1 container) that are created every month in this format YYYY-MM (Example:2022-11) and inside this folder I have few set of data files, I want to copy this data files to different folders in the data lake.</p>
<p>And again in the next month new folder is created in the same data lake (Example:Test1 container) with 2022-12 and list goes on, 2023-01.....etc., I want to copy files inside these folders every month to different data lake folder.</p>
<p>How to achieve this?</p>
","<azure><dataset><azure-data-factory>","2022-11-28 11:45:08","84","0","2","74609580","<p>Solution is mentioned in this thread, <a href=""https://stackoverflow.com/questions/74545918/create-a-folder-based-on-date-yyyy-mm-using-data-factory"">Create a folder based on date (YYYY-MM) using Data Factory?</a></p>
<p>Follow the Sink Dataset section and Copy Sink section....remove the parameter sinkfilename from the dataset, and use this dataset as source in the copy activity.</p>
<p>It worked for me.</p>
"
"74600099","Reading folders with Date format (YYYY-MM) using Azure Data Factory","<p>I have few folders inside the Data lake (Example:Test1 container) that are created every month in this format YYYY-MM (Example:2022-11) and inside this folder I have few set of data files, I want to copy this data files to different folders in the data lake.</p>
<p>And again in the next month new folder is created in the same data lake (Example:Test1 container) with 2022-12 and list goes on, 2023-01.....etc., I want to copy files inside these folders every month to different data lake folder.</p>
<p>How to achieve this?</p>
","<azure><dataset><azure-data-factory>","2022-11-28 11:45:08","84","0","2","74609816","<p>Alternative approach. For reading folders with Date format as (YYYY-MM)</p>
<p>I reproduce the same in my environment with copy activity.</p>
<ul>
<li>Open sink dataset and create a parameter with <strong>Name: <code>Folder</code></strong>.</li>
</ul>
<p><img src=""https://i.imgur.com/EgcYX64.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/WNaVujk.png"" alt=""enter image description here"" /></p>
<p>Go to Connection and Add this dynamic content: <code>@dataset().folder</code></p>
<p><img src=""https://i.imgur.com/S4y4OxX.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>You can Add this dynamic content:</strong></p>
</blockquote>
<p><code>@concat(formatDateTime(utcnow(), 'yyyy/MM'))</code></p>
<p><strong>Or</strong></p>
<pre><code>@concat(formatDateTime(utcnow(), 'yyyy'), '/',formatDateTime(utcnow(),'MM')
</code></pre>
<p><img src=""https://i.imgur.com/Us8IE3P.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Pipeline successfully executed and got the output:</strong></p>
</blockquote>
<p><img src=""https://i.imgur.com/oTCZkI2.png"" alt=""enter image description here"" /></p>
"
"74599091","Possible to include pipeline name when Azure Alert trigger alert for Data Factory?","<p>I have Azure Alert to Azure Data Factory. It gives hyperlink to Azure Monitor when pipeline run fails.</p>
<p>Is it possible to include name of Data Factory Pipeline or any other description like activity error message in Alert message?</p>
<p>It would help to speed troubleshooting.</p>
","<azure-data-factory><azure-monitor>","2022-11-28 10:17:52","130","1","1","74599792","<p>I create Alert for pipeline failed metrics and followed below steps to get the pipeline name in Alert message:</p>
<p>In dimension &gt; name I selected the name as <strong>all pipelines</strong></p>
<p><img src=""https://i.imgur.com/V2wnKjI.png"" alt=""enter image description here"" /></p>
<p><strong>Output</strong></p>
<p>You will get all FailuerType and name of pipeline in the Alert mail.</p>
<p><img src=""https://i.imgur.com/xXZbSMV.png"" alt=""enter image description here"" /></p>
"
"74598386","Azure datafactory - Get current branch as parameter","<p>Am using Azure datafactory in combination with git:</p>
<p><a href=""https://i.stack.imgur.com/EncE4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EncE4.png"" alt=""enter image description here"" /></a></p>
<p>During execution of a pipeline, can i get the current branch name as pipeline parameter, maybe like this:?
<a href=""https://i.stack.imgur.com/JJECq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JJECq.png"" alt=""enter image description here"" /></a></p>
<p>Wished output (additional column with branch name):
<a href=""https://i.stack.imgur.com/u9san.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u9san.png"" alt=""enter image description here"" /></a></p>
<p>Sources: <a href=""https://stackoverflow.com/questions/74588596/etl-elt-pipelines-metainformation-about-the-pipeline/74597157#74597157"">ETL / ELT pipelines - Metainformation about the pipeline</a></p>
","<azure-devops><azure-data-factory>","2022-11-28 09:18:50","134","0","1","74603425","<p>The workaround to achieve your scenario is follow below steps:</p>
<p><strong>create pipeline parameter with type string and default value as your branch name</strong>.</p>
<p><img src=""https://i.imgur.com/xdm0nSk.png"" alt=""enter image description here"" /></p>
<p>In source of copy activity use additional column and value as parameter you created for branch name.</p>
<p><img src=""https://i.imgur.com/KR4WpFY.png"" alt=""enter image description here"" /></p>
<p><strong>Output</strong></p>
<p><img src=""https://i.imgur.com/c1UmfyP.png"" alt=""enter image description here"" /></p>
<p>Only possible system variables are as mentioned in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">documents</a> are as below:</p>
<p><img src=""https://i.imgur.com/CfUTQF6.png"" alt=""enter image description here"" /></p>
"
"74597997","How to copy the records which are not in the target datastore using azure data factory","<p>I have a table in sql and it is copied to ADLS. After copying, sql table got inserted with new rows. I wanted to get the new rows.</p>
<p>I tried to use join transformation. But I couldn't get the output. What is the way to achieve this.</p>
","<azure><azure-data-factory>","2022-11-28 08:41:26","71","0","1","74616719","<p>Refer this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal#create-a-pipeline"" rel=""nofollow noreferrer"">link</a>. Using this you can get newly added rows from sql to data lake storage. Reproduced issue from my side and able to get newly added records from pipeline.</p>
<ol>
<li>Created two tables in sql storage with names data_source_table and watermarktable.</li>
<li>data_source_table  is the one which is having data in table and watermarktable used for tracking new records based date.</li>
<li>Created pipeline as shown below,
<img src=""https://i.imgur.com/EicH3eg.png"" alt=""enter image description here"" /></li>
</ol>
<p>In lookup1 selecting the datasource table</p>
<p>In lookup2 select Query as follows</p>
<pre><code>     MAX(LastModifytime) as NewWatermarkvalue from data_source_table;
</code></pre>
<p><img src=""https://i.imgur.com/hl8IyJB.png"" alt=""enter image description here"" /></p>
<p>Then in copy activity source and sink taken as shown below images</p>
<p>SOURCE:</p>
<p><img src=""https://i.imgur.com/3hZRaJq.png"" alt=""enter image description here"" />
Query in Source:</p>
<pre><code> select `* from data_source_table where LastModifytime &gt; '@{activity('Lookup1').output.firstRow.WatermarkValue}' and LastModifytime &lt;= '@{activity('Lookup1').output.firstRow.Watermarkvalue}'
</code></pre>
<p>SINK:
<img src=""https://i.imgur.com/rHL1yz8.png"" alt=""enter image description here"" /></p>
<p>Pipeline ran successfully and data in sql table is loaded into data lake storage file.
<img src=""https://i.imgur.com/atCc3dl.png"" alt=""enter image description here"" /></p>
<p>Inserted new rows inserted in data_source_table and able to get those records from Lookup activity
<img src=""https://i.imgur.com/pqEDVDQ.png"" alt=""enter image description here"" /></p>
"
"74588596","ETL / ELT pipelines - Metainformation about the pipeline","<p>how do you add metainformation about the used ETL / ELT code (and version of this ELT code) to the produced sink files / tables?</p>
<p>Do u consider it as required to have information like &quot;PipelineID&quot; or &quot;DataProductionTime&quot; in the targetfolder?</p>
","<azure><azure-data-factory><etl><data-warehouse><data-lakehouse>","2022-11-27 08:35:58","70","0","1","74597157","<blockquote>
<p>how do you add metainformation about the used ETL / ELT code (and version of this ELT code) to the produced sink files / tables?</p>
</blockquote>
<p>You can do it using the <strong>pipeline dynamic content</strong> and <strong>additional column in copy activity</strong>.</p>
<p><strong>This is my source file:</strong></p>
<p><img src=""https://i.imgur.com/y3rKdpp.png"" alt=""enter image description here"" /></p>
<p>In source of copy activity use additional column.</p>
<p><img src=""https://i.imgur.com/enC7LBY.png"" alt=""enter image description here"" /></p>
<p>In ADF you can find meta information about the pipeline in <strong>System variables</strong> of dynamic content.</p>
<p><img src=""https://i.imgur.com/1GZsH11.png"" alt=""enter image description here"" /></p>
<p><strong>Target file in sink folder:</strong></p>
<p><img src=""https://i.imgur.com/stac0Pe.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>Do u consider it as required to have information like &quot;PipelineID&quot; or &quot;DataProductionTime&quot; in the targetfolder?</p>
</blockquote>
<p>All this depends on your requirement like if you want to know by which pipeline you are getting data you can use this.</p>
"
"74574244","Azure DataFactory replace espcial characters in Dynamic Content","<p>I created a dynamic SQL script that is created with line breaks to make it more readable, but these line breaks are not accepted by Azure Delta Breaks which throws a sitanxis error, is there a way to remove these line breaks? Is it possible to make a replacement?</p>
<p>Dynamic content created in azure data factory:</p>
<pre><code>@CONCAT('SELECT ',activity('Get Parameters').output.firstRow.SourceSelectFields,'
 FROM ',activity('Get Parameters').output.firstRow.SourceTable,' ',activity('Get Parameters').output.firstRow.CrossingTables,'
WHERE ',activity('Get Parameters').output.firstRow.Conditions,
' AND doc.status in (''Complete'',''Draft'')',
' AND PRJT.taskname IN ',variables('ListadoTareas'),
' AND (COALESCE (CAST(DATE_FORMAT(cw.begindate,''yyyy-MM-dd'') AS DATE )))&gt; ''',activity('Get Parameters').output.firstRow.BeginDate,'''',
' AND ''',utcnow('yyyy-MM-dd'),''' &lt;= 
CASE WHEN CAST(DATE_FORMAT(cw.expirationdate,''yyyy-MM-dd'') AS DATE) !=''1970-01-01'' THEN (CAST(ADD_MONTHS(DATE_FORMAT(cw.expirationdate,''yyyy-MM-dd''),4) AS DATE))
     ELSE CAST(ADD_MONTHS(DATE_FORMAT(DATE_ADD(cw.effectivedate,CAST(cw.cus_plazodias_wjcc2 AS INT)),''yyyy-MM-dd''),4)AS DATE)
END',
' AND SUBSTRING(doc.title,1,3) IN ',variables('ListadoDocumentos'),
' AND dl.projectid is null')
</code></pre>
<p>Line breaks are found in SELECT, FROM and WHERE statements and AND conditions.</p>
","<azure-data-factory>","2022-11-25 14:27:28","69","0","1","74600345","<ul>
<li>I tried to pass the below script with line breaks as a dynamic content.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>@concat('select ',activity('Lookup1').output.firstRow.SourceSelectFields, '
from
',
activity('Lookup1').output.firstRow.SourceTable)
</code></pre>
<ul>
<li>When the pipeline is run, Input of the script activity is as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/vnVx6vG.png"" alt=""enter image description here"" /></p>
<ul>
<li>The pipeline run successfully without error.</li>
</ul>
<p><img src=""https://i.imgur.com/cRlrJbd.png"" alt=""enter image description here"" /></p>
<p>But if there is error in the pipeline, try to replace the line breaks with empty string. Below is the approach.</p>
<ul>
<li><p>Variable V1 is taken, and the same script is given.</p>
</li>
<li><p>In Variable V2, the below expression is given to replace the line break with empty string.</p>
</li>
</ul>
<pre><code>@replace(variables('v1'),'
',' ')
</code></pre>
<p><img src=""https://i.imgur.com/mZl3xwb.png"" alt=""enter image description here"" /></p>
<p><strong>Variable V1 Output</strong>
<img src=""https://i.imgur.com/cNbHitN.png"" alt=""enter image description here"" /></p>
<p><strong>Variable V2 Output</strong>
<img src=""https://i.imgur.com/cNbHitN.png"" alt=""enter image description here"" /></p>
<ul>
<li>This way, special character line break can be replaced.</li>
</ul>
"
"74572417","Can we create pipelines to run DAX queries on Azure Analysis Service Tabular Models from ADF or Synapse Analytics?","<p>How can we create ADF pipeline to run DAX query from ADF(or Synapse Analytics) to AAS Tabular Models and get the data stored into tables in Azure Data Warehouse Tables or in a .csv file?</p>
<p>I've read about creating a .Net library for connecting to Analysis Services servers and querying data from .NET code. Is there any other approach?</p>
","<powerbi><dax><azure-data-factory><azure-synapse><azure-analysis-services>","2022-11-25 11:53:17","286","0","1","74574406","<p>You can create a linked server mapping to aas on the sql server.
Create a linked service in adf to the sql database and query the aas via the sql database.
<a href=""https://datasharkx.wordpress.com/2021/03/16/copy-data-from-ssas-aas-through-azure-data-factory"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2021/03/16/copy-data-from-ssas-aas-through-azure-data-factory</a></p>
"
"74571092","Merge multiple Azure databases into one Azure SQL database and sync them weekly","<p>I have 100-150 Azure databases with same table schema. There are 300-400 tables in each database. Separate reports are enabled on all these databases.</p>
<p>Now I want to merge these database into a centralized database and generate some different Power BI reports from this centralized database.</p>
<p>The approach I am thinking is -</p>
<ul>
<li><p>There will be Master table on target database which will have
DatabaseID and Name.</p>
</li>
<li><p>All the tables on target database will have the composite primary key
created with the Source Primary key and Database ID.</p>
</li>
<li><p>There will be multiple (30-35) instances of Azure data factory
pipeline and each instance will be responsible to merge data from
10-15 databases.</p>
</li>
<li><p>These ADF pipelines will be scheduled to run weekly.</p>
</li>
</ul>
<p>Can anyone please guide me that the above approach will be feasible in this scenario? Or there could any other option we can go for.</p>
<p>Thanks in Advance.</p>
","<azure><azure-sql-database><azure-data-factory><azure-synapse>","2022-11-25 10:04:32","121","0","1","74593599","<p>You trying to create a Data Warehouse.</p>
<p>I hope you will never archive to merge 150 Azure SQL Databases because is soon as you try to query that beefy archive what you will see is this:</p>
<p><a href=""https://i.stack.imgur.com/D1rGo.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D1rGo.jpg"" alt=""enter image description here"" /></a></p>
<p>This <a href=""https://community.powerbi.com/t5/Desktop/Power-BI-Size-Limitation-Maximum/td-p/1210063"" rel=""nofollow noreferrer"">because</a> Power BI, as any other tool, comes with limitations:</p>
<ul>
<li><strong>Limitation of Distinct values in a column:</strong> there is a 1,999,999,997 limit on the number of distinct values that can be stored in a column.</li>
<li><strong>Row limitation:</strong> If the query sent to the data source returns more than one million rows, you see an error and the query fails.</li>
<li><strong>Column limitation:</strong> The maximum number of columns allowed in a dataset, across all tables in the dataset, is 16,000 columns.</li>
</ul>
<p>A data warehouse is not just the merge of ALL of your data. You need to clean them and import only the most useful ones.</p>
<p>So the approach you are proposing is overall OK, but just import what you need.</p>
"
"74563667","Azure Data Factory SQL trigger events","<p>I wanted to comment on this post: <a href=""https://stackoverflow.com/questions/72121038/i-want-to-trigger-azure-datafactory-pipeline-whenever-there-is-a-change-in-azure"">I want to trigger Azure datafactory pipeline whenever there is a change in Azure SQL database</a></p>
<p>but I don't have enough reputation...</p>
<p>The solution that Skin comes up with (SQL DB trigger events) looks exactly like what I'm after but I can't find any further documentation on it - in fact the only references I've found say that this functionality <em><strong>doesn't</strong></em> exist?</p>
<p>Can anyone point me to anything online - or a book - that could help?</p>
<p>Cheers</p>
","<azure><azure-data-factory>","2022-11-24 16:19:08","328","0","2","74571519","<p><strong>AFAIK</strong>, In ADF there are no such triggers for SQL changes. ADF supports only Schedule,Tumbling window and Storage event and custom event triggers.</p>
<p>But You can use the logic app triggers (<strong>item created and item modified</strong>) to triggers ADF pipeline.</p>
<p>For this we the SQL table should have an auto increment column.</p>
<p><strong>Here is a demo I have built for item created trigger:</strong></p>
<p>First search for SQL in logic app and click on item created trigger. Then create a connection with your details.</p>
<p>After that give your table details.</p>
<p>After trigger create Action for ADF pipeline run.</p>
<p><img src=""https://i.imgur.com/3Q332mn.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/MiSP4Ua.png"" alt=""enter image description here"" /></p>
<p>Make sure you publish your ADF pipeline to reflect its name in the above drop down. You can assign SQL columns to ADF pipeline parameter like above.</p>
<p>You can set the trigger for one every one minute or one hour as per your requirement. If any new item inserted into SQL table in that period of time it will trigger ADF pipeline.</p>
<p>I have inserted a new record like this <code>insert  into practice values('Six');</code></p>
<p><strong>Flow Suceeded:</strong></p>
<p><img src=""https://i.imgur.com/gM32UOG.png"" alt=""enter image description here"" /></p>
<p><strong>My ADF pipeline:</strong></p>
<p><img src=""https://i.imgur.com/uHag1kq.png"" alt=""enter image description here"" /></p>
<p><strong>Pipeline Triggered:</strong></p>
<p><img src=""https://i.imgur.com/kvOaAVT.png"" alt=""enter image description here"" /></p>
<p><strong>Pipeline successful and you can see variable value:</strong></p>
<p><img src=""https://i.imgur.com/LfRQMNU.png"" alt=""enter image description here"" /></p>
<p>You can use another flow for <strong>item modified</strong> trigger as same above and trigger ADF pipeline from that as well.</p>
"
"74563667","Azure Data Factory SQL trigger events","<p>I wanted to comment on this post: <a href=""https://stackoverflow.com/questions/72121038/i-want-to-trigger-azure-datafactory-pipeline-whenever-there-is-a-change-in-azure"">I want to trigger Azure datafactory pipeline whenever there is a change in Azure SQL database</a></p>
<p>but I don't have enough reputation...</p>
<p>The solution that Skin comes up with (SQL DB trigger events) looks exactly like what I'm after but I can't find any further documentation on it - in fact the only references I've found say that this functionality <em><strong>doesn't</strong></em> exist?</p>
<p>Can anyone point me to anything online - or a book - that could help?</p>
<p>Cheers</p>
","<azure><azure-data-factory>","2022-11-24 16:19:08","328","0","2","74664072","<p>with the new latest feature A new feature that allows invocation of any REST endpoints is now in public preview in Azure SQL databases
, I guess it is possible :</p>
<p><a href=""https://devblogs.microsoft.com/azure-sql/azure-sql-database-external-rest-endpoints-integration-public-preview/"" rel=""nofollow noreferrer"">https://devblogs.microsoft.com/azure-sql/azure-sql-database-external-rest-endpoints-integration-public-preview/</a></p>
<p>Blog:
<a href=""https://datasharkx.wordpress.com/2022/12/02/event-trigger-azure-data-factory-synapse-pipeline-via-azure-sql-database/"" rel=""nofollow noreferrer"">https://datasharkx.wordpress.com/2022/12/02/event-trigger-azure-data-factory-synapse-pipeline-via-azure-sql-database/</a></p>
"
"74563215","Azure DataFactory: Deployment Failed: At least one resource deployment operation failed","<p>I am trying to deploy Azure Data Factory from development to Test and Production instances using Azure DevOps. The pipeline steps include:</p>
<ol>
<li>Copying the Linked Templates to a storage account (Azure PowerShell Task).</li>
<li>Disabling the triggers (Azure PowerShell Task).</li>
<li>ARM Template Deployment (Azure Resource Group Deployment Task).</li>
<li>Enabling the Triggers (Azure PowerShell Task).</li>
</ol>
<p>I have set the override parameters for Test and Production Tasks.
The deployment to the Test Data Factory completed successfully. However, when I tried deploying it to the Production Data Factory, it failed giving the following error :</p>
<pre><code>There were errors in your deployment. Error code: DeploymentFailed.
2022-11-24T14:20:51.2337688Z ##[error]At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details.
2022-11-24T14:20:51.2339819Z ##[debug]Processed: ##vso[task.issue type=error;]At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details.
2022-11-24T14:20:51.2340752Z ##[error]Details:
2022-11-24T14:20:51.2341700Z ##[debug]Processed: ##vso[task.issue type=error;]Details:
2022-11-24T14:20:51.2343511Z ##[error]DeploymentFailed: At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details.
2022-11-24T14:20:51.2351532Z ##[error]Task failed while creating or updating the template deployment.
2022-11-24T14:20:51.2352528Z ##[debug]Processed: ##vso[task.issue type=error;]Task failed while creating or updating the template deployment.
2022-11-24T14:20:51.2361768Z ##[debug]Processed: ##vso[task.complete result=Failed;]Task failed while creating or updating the template deployment.
</code></pre>
<p>I checked the steps for Test and Production tasks, and both seems correct to me. The Resource Group, Data Factory name, template parameters all have been set up.
How do I resolve this, since the error doesn't point to anything specific?</p>
","<azure-devops><azure-rm-template><azure-data-factory>","2022-11-24 15:41:27","669","1","1","74653335","<p>The issue was with my connection to the Shared Integration Runtime for Data Factory.
So, the steps I followed were as follows :</p>
<ol>
<li>In the Azure Portal, go to your Resource Group -&gt; Activity Log
<a href=""https://i.stack.imgur.com/wk7Sv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wk7Sv.png"" alt=""enter image description here"" /></a></li>
</ol>
<ol start=""2"">
<li>Here, I found the exact cause of my Data Factory deployment failing.</li>
</ol>
<pre><code>Access denied. Unable to access shared integration runtime 'integrationRuntimeSelfHosted'. Please check whether this resource has been granted permission by the shared integration runtime.
</code></pre>
<ol start=""3"">
<li><p>Then I went to the Data Factory, where I had created the integration runtime and granted access to the new Data Factory which I was trying to deploy.</p>
</li>
<li><p>Go to Azure DevOps -&gt; Deploy the Pipeline again.</p>
</li>
</ol>
<p>Following the above steps helped me resolve my issue.</p>
"
"74561009","How to connect on-preemies Kafka through Azure data factory","<p>Want to connect on-premise Kafka through Azure data factory to access messages and stored to adls.</p>
<p>Connect to kafka by using ADF</p>
<p>My data is available in on-premise kafka , want to access and load to adls.</p>
","<azure><azure-data-factory><azure-databricks>","2022-11-24 12:43:55","152","0","1","75112167","<p>Assuming your on-prem Kafka is not accessible via the public internet, you have two options. You can either connect via a Managed Virtual Network (also referred to as a Managed Private Endpoint) or a Self-Hosted Integration Runtime.</p>
<p>Here's some explanations I copied from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/choose-the-right-integration-runtime-configuration"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/choose-the-right-integration-runtime-configuration</a>.</p>
<p><strong>Azure integration runtime with managed virtual network</strong></p>
<p>When using the Azure integration runtime with a managed virtual network, you should use managed private endpoints to connect your data sources to ensure data security during transmission. With some additional settings such as Private Link Service and Load Balancer, managed private endpoints can also be used to access on-premises data sources.</p>
<p><strong>Self-hosted integration runtime</strong></p>
<p>To prevent data from different environments from interfering with each other and ensure the security of the production environment, we need to create a corresponding self-hosted integration runtime for each environment. This ensures sufficient isolation between different environments.</p>
<p>Since the self-hosted integration runtime runs on a customer managed machine, in order to reduce the cost, maintenance, and upgrade efforts as much as possible, we can make use of the shared functions of the self-hosted integration runtime for different projects in the same environment. For details on self-hosted integration runtime sharing, refer to the article Create a shared self-hosted integration runtime in Azure Data Factory. At the same time, to make the data more secure during transmission, we can choose to use a private link to connect the data sources and key vault, and connect the communication between the self-hosted integration runtime and the Azure Data Factory service.</p>
"
"74559484","Azure Data Factory - Copy Data Upsert only updating a single row at a time","<p>I'm using Data Factory (well synapse pipelines) to ingest data from sources into a staging layer.  I am using the Copy Data activity with UPSERT.  However i found the performance of incrementally loading large tables particularly slow so i did some digging.</p>
<p>So my incremental load brought in 193k new/modified records from the source. These get stored in the transient staging/landing table that the copy data activity creates in the database in the background.  In this table it adds a  column called BatchIdentifier, however the batch identifier value is different for every row.</p>
<p><a href=""https://i.stack.imgur.com/guIkJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/guIkJ.png"" alt=""enter image description here"" /></a></p>
<p>Profiling the load i can see individual statements issued for each batchidentifier so effectively its processing the incoming data row by row rather than using a batch process to do the same thing.
I tried setting the sink writebatchsize property on copy data activity to 10k but that doesn't make any difference.</p>
<p>Has anyone else come across this, or a better way to perform a dynamic upsert without having to specify all the columns in advance (which i'm really hoping to avoid)</p>
<p>This is the SQL statement issued 193k times on my load as an example.</p>
<p>Does a check to see if the record exists in the target table, if so performs an update otherwise performs an insert. logic makes sense but its performing this on a row by row basis when this could just be done in bulk.</p>
<p><a href=""https://i.stack.imgur.com/Zb1xP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zb1xP.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><upsert>","2022-11-24 10:46:02","517","0","1","74902947","<p>Is your primary key definition in the source the same as in the sink?</p>
<p>I just ran into this same behavior when the columns in the source and destination tables used different columns.</p>
<p>It also appears ADF/Synapse does not use MERGE for upserts, but its own IF EXISTS THEN UPDATE ELSE INSERT logic so there may be something behind the scenes making it select single rows for those BatchId executions.</p>
"
"74559162","Still not working: Using XML with REST API through Azure Datafactory V2?","<p>This post <a href=""https://stackoverflow.com/questions/60964612/using-xml-with-rest-api-through-azure-datafactory-v2"">Using XML with REST API through Azure Datafactory V2</a> says ADF does not support REST API with XML but since the post is 2 years old, I was wondering if Microsoft has enable this feature now?</p>
<p><strong>EDIT</strong></p>
<p>Currently I am doing like this where I am adding an addtional headers where I define it should be XML. However, the format still looks json when I preview the data.
<a href=""https://i.stack.imgur.com/h4Wvi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h4Wvi.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-11-24 10:23:23","141","0","1","74560251","<p>We get this warning message while giving <code>Accept</code> header in copy activity saying <code>application/json</code> value will be automatically give as the value for this header.</p>
<p><img src=""https://i.imgur.com/runEzHV.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Therefore, in copy data activity, the accepted API format is <code>JSON</code> only. As an alternate, you can use web activity and copy data to get the required response in the following way.</p>
</li>
<li><p>I used a sample API which returns XML response. I used this in the web activity and the debug output would be as follows:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/miVNdcA.png"" alt=""enter image description here"" /></p>
<ul>
<li>The entire XML response is stored as a string. Now I have taken a copy data activity where the source is a CSV file (contents of this file don't matter. Try to take a file similar to below file).</li>
</ul>
<p><img src=""https://i.imgur.com/LTe4BKq.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, create an additional column for this source with any column name and value as shown below:</li>
</ul>
<pre><code>@activity('Web1').output.Response
</code></pre>
<p><img src=""https://i.imgur.com/qDfwAp0.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, choose <code>delimited text</code> as the sink, with file name as <code>op.xml</code> and <strong>do not choose the first row as header option</strong>. Also select quote character as <code>No quote character</code>. The following is how I configured the sink dataset.</li>
</ul>
<p><img src=""https://i.imgur.com/nDvckg9.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now in mapping import schemas and keep only the above created additional column.</li>
</ul>
<p><img src=""https://i.imgur.com/MyNAHRw.png"" alt=""enter image description here"" /></p>
<ul>
<li>This will create a file <code>op.xml</code> at the destination storage account. The contents of the file would be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/Tck1El4.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now you can use another copy data activity, choose XML as a the source file format and select the above generated <code>op.xml</code> file. When I preview the data, it is showing the data as expected.</li>
</ul>
<p><img src=""https://i.imgur.com/gQNNr4m.png"" alt=""enter image description here"" /></p>
<br>
<p>Now, you can write this file to any required file format as per your requirement.</p>
"
"74557537","How to format the negative values in dataflow?","<p>I have below column in my table</p>
<p><a href=""https://i.stack.imgur.com/5klUK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5klUK.png"" alt=""enter image description here"" /></a></p>
<p>I need an output as below</p>
<p><a href=""https://i.stack.imgur.com/RDjxv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RDjxv.png"" alt=""enter image description here"" /></a></p>
<p>I am using Dataflow in the Azure data factory and unable to get the above output. I used derived column but no success. I used replace function, but it's not coming correct. Can anyone advise how to format this in dataflow?</p>
","<azure-data-factory>","2022-11-24 08:13:06","89","0","1","74558407","<ul>
<li>Source is taken in data flow with data as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/Qmmkyx2.png"" alt=""enter image description here"" /></p>
<ul>
<li>Derived column transformation is added next to source.</li>
<li>New column is added and the expression is given as
<code>iif(left(id,1)=='-', replace(replace(id,&quot;USD&quot;,&quot;&quot;),&quot;-&quot;,&quot;-$&quot;), concat(&quot;$&quot;, replace(id,&quot;USD&quot;,&quot;&quot;)))</code>
<img src=""https://i.imgur.com/nAAnpfq.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Output of Derived Column activity</strong></p>
<p><img src=""https://i.imgur.com/Tk0hP4Q.png"" alt=""enter image description here"" /></p>
"
"74557524","copy on premise sql server hosted on linux server to Azure datalake using azure data factory","<p>I have a requirement to copy the table from on premise sql database hosted on linux server to Azure datalake using azure data factory. Self hosted integration works natively for windows system. Can someone share your thoughts or work around to achieve this requirement?</p>
<p>Regards
Aravindan</p>
","<azure-data-factory>","2022-11-24 08:11:54","63","0","1","74660654","<p>Unfortunately, this cannot be achieved as the SHIR has below system requirements and in order to connect to onPrem data sources ADF requires SHIR.</p>
<p><a href=""https://i.stack.imgur.com/T7GCP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T7GCP.png"" alt=""enter image description here"" /></a></p>
<p>A workaround could be using SSIS packages in Linux to extract the data. For more information, please refer to this documentation: <a href=""https://learn.microsoft.com/en-us/sql/linux/sql-server-linux-migrate-ssis?view=sql-server-ver16"" rel=""nofollow noreferrer"">Extract, transform, and load data on Linux with SSIS</a></p>
"
"74555343","Azure Data Factory script indexing for pipeline().parameters that are used multiple times in the query","<p>I am using Azure Data Factory script to create parameterized SQL query. I understand that the Index specifies the position in which the parameter's value should go in the SQL command. However, I don't know how to handle the situation where pipeline().parameters are used multiple times in the SQL query. In my example below, the RUN_DATE parameter is used twice. When I simply add the same parameters multiple times, it will show &quot;Parameters with duplicate name will be overwritten.&quot; Any idea how to solve this?</p>
<p>Here is the query:</p>
<pre><code>@concat('
UPDATE s
SET INSERT_DATE = ''', pipeline().parameters.RUN_DATE,'''',
',UPDATE_DATE = ''', pipeline().parameters.RUN_DATE,'''',
'FROM p 
WHERE p.ID = s.ID
AND s.ID = ''', pipeline().parameters.ID,'''',
';')
</code></pre>
<p>See the screenshot:
<a href=""https://i.stack.imgur.com/diSSv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/diSSv.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2022-11-24 03:38:29","97","0","2","74555540","<p>Not sure if I understand the ask 100% here . The scripts which you shared does use the parameter ID and Run_date and since the parameters are accessible through out the lifetime of the pipeline , so even if you do not pass as script parameter , you script will still work fine .</p>
"
"74555343","Azure Data Factory script indexing for pipeline().parameters that are used multiple times in the query","<p>I am using Azure Data Factory script to create parameterized SQL query. I understand that the Index specifies the position in which the parameter's value should go in the SQL command. However, I don't know how to handle the situation where pipeline().parameters are used multiple times in the SQL query. In my example below, the RUN_DATE parameter is used twice. When I simply add the same parameters multiple times, it will show &quot;Parameters with duplicate name will be overwritten.&quot; Any idea how to solve this?</p>
<p>Here is the query:</p>
<pre><code>@concat('
UPDATE s
SET INSERT_DATE = ''', pipeline().parameters.RUN_DATE,'''',
',UPDATE_DATE = ''', pipeline().parameters.RUN_DATE,'''',
'FROM p 
WHERE p.ID = s.ID
AND s.ID = ''', pipeline().parameters.ID,'''',
';')
</code></pre>
<p>See the screenshot:
<a href=""https://i.stack.imgur.com/diSSv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/diSSv.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2022-11-24 03:38:29","97","0","2","74555890","<ul>
<li><p>As @Himanshu said, the pipeline parameters can be referred to script directly. I repro'd it with few changes in the script. below is the approach.</p>
</li>
<li><p>Pipeline parameters <code>RUN_DATE</code> and <code>ID</code> are taken.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/30Oh7Fu.png"" alt=""enter image description here"" /></p>
<ul>
<li>Script activity is taken, and same script is given in the query text box.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>@concat('
UPDATE s
SET INSERT_DATE = ''', pipeline().parameters.RUN_DATE,'''',
',UPDATE_DATE = ''', pipeline().parameters.RUN_DATE,'''',
'FROM p
WHERE p.ID = s.ID
AND s.ID = ''', pipeline().parameters.ID,'''',
';')
</code></pre>
<ul>
<li>When pipeline is run in this way, error will not be as said in the question post. But error produced for this script is,</li>
</ul>
<p><img src=""https://i.imgur.com/zQfwi9Z.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Concat function in adf gives the output value as wrapped within double quotes.
<img src=""https://i.imgur.com/fBom9L9.png"" alt=""enter image description here"" /></p>
</li>
<li><p>To overcome this, script is modified and given as</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>update s
set INSERT_DATE = @{pipeline().parameters.Run_date},
UPDATE_DATE= @{pipeline().parameters.Run_date}
from p
where p.id=s.id and s.id= @{pipeline().parameters.ID}
</code></pre>
<p><img src=""https://i.imgur.com/oDPYUBW.png"" alt=""enter image description here"" /></p>
<ul>
<li>When pipeline is run, it is executed successfully.
<img src=""https://i.imgur.com/GrGtiGe.png"" alt=""enter image description here"" /></li>
</ul>
<p><img src=""https://i.imgur.com/JfEsO8L.png"" alt=""enter image description here"" /></p>
"
"74551756","How to Convert Datetime value from lookup activity output to only a Date value into Set variable activity using AzureDataFactory","<p>Can you please help me in formatting the Lookupactivity Output value from Datetime to Date type and pass into set_variable activity.</p>
<p>Step 1:
I am using a query in Lookup activity as SELECT CAST(MAX([DWHModifiedDate]) AS DATE) AS DWHModifiedDate FROM [Schema].[TableName]</p>
<p>The output from lookup activity is like  &quot;DWHModifiedDate&quot;: &quot;2022-11-18T00:00:00Z&quot;</p>
<p>Step 2: Now i added a Set_variable activity and i want to store only the date from Lookup activity output for example the variable value should be only &quot;2022-11-18&quot;.</p>
<p>Can you please help how to achieve this.</p>
","<azure-data-factory>","2022-11-23 18:53:32","191","0","2","74552067","<p>@formatDateTime(activity('lookupactivity').output.firstRow.DWHModifiedDate,'yyyy-MM-dd')</p>
"
"74551756","How to Convert Datetime value from lookup activity output to only a Date value into Set variable activity using AzureDataFactory","<p>Can you please help me in formatting the Lookupactivity Output value from Datetime to Date type and pass into set_variable activity.</p>
<p>Step 1:
I am using a query in Lookup activity as SELECT CAST(MAX([DWHModifiedDate]) AS DATE) AS DWHModifiedDate FROM [Schema].[TableName]</p>
<p>The output from lookup activity is like  &quot;DWHModifiedDate&quot;: &quot;2022-11-18T00:00:00Z&quot;</p>
<p>Step 2: Now i added a Set_variable activity and i want to store only the date from Lookup activity output for example the variable value should be only &quot;2022-11-18&quot;.</p>
<p>Can you please help how to achieve this.</p>
","<azure-data-factory>","2022-11-23 18:53:32","191","0","2","74674138","<p>You can also just try to extract the required date directly using <code>@split()</code> function. The following is a sample and the output of look up activity looks as shown below:</p>
<p><img src=""https://i.imgur.com/JpwuV0G.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can split on <code>T</code> and extract the 0th index to get the required output as well (since the date is already in the format of <code>yyyy-MM-dd</code>).</li>
</ul>
<pre><code>@split(activity('Lookup1').output.firstRow.dt,'T')[0]
</code></pre>
<p><img src=""https://i.imgur.com/JS6X7z9.png"" alt=""enter image description here"" /></p>
<ul>
<li>This would give the required output like <code>yyyy-MM-dd</code> format.</li>
</ul>
<p><img src=""https://i.imgur.com/lQitkmV.png"" alt=""enter image description here"" /></p>
<ul>
<li>Using <code>formatDateTime</code> also gives desired output.</li>
</ul>
"
"74551005","Data Factory Create Nested Arrays from Flat File","<pre><code>Does any one know how to create an array within an array in Data Factory. I need to make something like this. One Employee has multiple Customers, each Customer buys multiple products.  Its coming from a flat file where Employee repeats multiple times once for each Customer then Customer repeats multiple times per Product. 
{
  &quot;employeeNumber&quot;: &quot;00001&quot;,
  &quot;employeeName&quot;: &quot;John Doe&quot;,
  &quot;customers&quot;: [
    {
      &quot;id&quot;: &quot;99999&quot;,
      &quot;name&quot;: &quot;Jane Doe&quot;,
      &quot;products&quot;: [
        {
          &quot;name&quot;: &quot;XYZ&quot;,
          &quot;price&quot;: 2.00
        },
        {
          &quot;name&quot;: &quot;ABC&quot;,
          &quot;price&quot;: 3.00
        }
      ]
    },
    {
      &quot;id&quot;: &quot;1111&quot;,
      &quot;name&quot;: &quot;John Smith&quot;,
      &quot;products&quot;: [
        {
          &quot;name&quot;: &quot;RVS&quot;,
          &quot;price&quot;: 2.00
        },
        {
          &quot;name&quot;: &quot;GHI&quot;,
          &quot;price&quot;: 3.00
        },
        {
          &quot;name&quot;: &quot;QRS&quot;,
          &quot;price&quot;: 4.00
        }
      ]
    }
  ]
}
</code></pre>
<p>How to create a double nested Array -- Array within and Array from Flat Data in a Data Factory Dataflow.  So much on how to &quot;flatten&quot; JSON to columns.. nothing on how to aggregate flat to &quot;Nested&quot; JSON in nested Arrays.</p>
<p>I was able to get a Struct in a Derived Column to create an Array but I am struggling with how to create another array under the first Array.</p>
","<azure-data-factory>","2022-11-23 17:42:26","145","0","2","74556088","<p>To create arrays, use an Aggregate transformation and the &quot;collect()&quot; function. <a href=""https://youtu.be/zneE18EHJSE"" rel=""nofollow noreferrer"">https://youtu.be/zneE18EHJSE</a></p>
"
"74551005","Data Factory Create Nested Arrays from Flat File","<pre><code>Does any one know how to create an array within an array in Data Factory. I need to make something like this. One Employee has multiple Customers, each Customer buys multiple products.  Its coming from a flat file where Employee repeats multiple times once for each Customer then Customer repeats multiple times per Product. 
{
  &quot;employeeNumber&quot;: &quot;00001&quot;,
  &quot;employeeName&quot;: &quot;John Doe&quot;,
  &quot;customers&quot;: [
    {
      &quot;id&quot;: &quot;99999&quot;,
      &quot;name&quot;: &quot;Jane Doe&quot;,
      &quot;products&quot;: [
        {
          &quot;name&quot;: &quot;XYZ&quot;,
          &quot;price&quot;: 2.00
        },
        {
          &quot;name&quot;: &quot;ABC&quot;,
          &quot;price&quot;: 3.00
        }
      ]
    },
    {
      &quot;id&quot;: &quot;1111&quot;,
      &quot;name&quot;: &quot;John Smith&quot;,
      &quot;products&quot;: [
        {
          &quot;name&quot;: &quot;RVS&quot;,
          &quot;price&quot;: 2.00
        },
        {
          &quot;name&quot;: &quot;GHI&quot;,
          &quot;price&quot;: 3.00
        },
        {
          &quot;name&quot;: &quot;QRS&quot;,
          &quot;price&quot;: 4.00
        }
      ]
    }
  ]
}
</code></pre>
<p>How to create a double nested Array -- Array within and Array from Flat Data in a Data Factory Dataflow.  So much on how to &quot;flatten&quot; JSON to columns.. nothing on how to aggregate flat to &quot;Nested&quot; JSON in nested Arrays.</p>
<p>I was able to get a Struct in a Derived Column to create an Array but I am struggling with how to create another array under the first Array.</p>
","<azure-data-factory>","2022-11-23 17:42:26","145","0","2","74570420","<p>I take flat file from Azure blob storage and converted into nested array in Azure data factory.</p>
<p>Flat file:</p>
<p><img src=""https://i.imgur.com/jLqg16M.png"" alt=""enter image description here"" /></p>
<p>I created pipeline and implemented dataflow in pipeline.
Dataflow:</p>
<p><img src=""https://i.imgur.com/HfOrafw.png"" alt=""enter image description here"" /></p>
<p>Source of dataflow:</p>
<p><img src=""https://i.imgur.com/wpepoYn.png"" alt=""enter image description here"" /></p>
<p>I have taken derived column and created products column with two sub columns name and price with below expression</p>
<pre><code>@(name=Pname,       price=price)
</code></pre>
<p><img src=""https://i.imgur.com/O5DUoFG.png"" alt=""enter image description here"" /></p>
<p>Sub column name:</p>
<p><img src=""https://i.imgur.com/pF0dwXf.png"" alt=""enter image description here"" /></p>
<p>Sub column name:</p>
<p><img src=""https://i.imgur.com/u86RXiI.png"" alt=""enter image description here"" /></p>
<p>I implemented Aggregate activity and applied group by on employeeNumber,employeeName,id,Cname columns.
Image for reference</p>
<p><img src=""https://i.imgur.com/C0ChRAJ.png"" alt=""enter image description here"" /></p>
<p>I applied aggregate on products column applied collect operation to convert products to array with below expression:</p>
<pre><code>collect(products)
</code></pre>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/DHWE9xQ.png"" alt=""enter image description here"" /></p>
<p>Data preview of Aggregate:</p>
<p><img src=""https://i.imgur.com/KWMCER1.png"" alt=""enter image description here"" /></p>
<p>I implemented derived column on the aggregate and created customers column with sub columns id, name, products with below expression:</p>
<pre><code>@(id=id,        name=Cname,     products=products)
</code></pre>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/pLtZXBS.png"" alt=""enter image description here"" /></p>
<p>Sub column Id:</p>
<p><img src=""https://i.imgur.com/DdTiDgK.png"" alt=""enter image description here"" /></p>
<p>Sub column name:</p>
<p><img src=""https://i.imgur.com/TQpuUP0.png"" alt=""enter image description here"" /></p>
<p>Sub column products:</p>
<p><img src=""https://i.imgur.com/BDlXJjV.png"" alt=""enter image description here"" /></p>
<p>I applied select function on derive d column to retrieve employeeNumber, employeeName, customers column
Image for reference:</p>
<p><img src=""https://i.imgur.com/tsH5iQD.png"" alt=""enter image description here"" /></p>
<p>Data preview of select:</p>
<p><img src=""https://i.imgur.com/ppPaYn4.png"" alt=""enter image description here"" /></p>
<p>I applied aggregate on customers column applied collect operation to convert products to array with below expression:</p>
<pre><code>collect(customers)
</code></pre>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/PbMQZc5.png"" alt=""enter image description here"" /></p>
<p>Data preview of Aggregate:</p>
<p><img src=""https://i.imgur.com/gbd767r.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/mICxQlR.png"" alt=""enter image description here"" /></p>
<p>I connected it to sink to store the nested array Json file in blob storage.</p>
<p>Sink:</p>
<p><img src=""https://i.imgur.com/JILN7pm.png"" alt=""enter image description here"" /></p>
<p>I run the pipeline It run successfully.</p>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/QZxsJoM.png"" alt=""enter image description here"" /></p>
<p>The output file stored in my blob container</p>
<p>Output:</p>
<pre><code>{&quot;employeeNumber&quot;:&quot;00001&quot;,
 &quot;employeeName&quot;:&quot;John Doe&quot;,
 &quot;customers&quot;:[
 {     &quot;id&quot;:&quot;1111&quot;,
       &quot;name&quot;:&quot;John Smith&quot;,
       &quot;products&quot;:[
          { &quot;name&quot;:&quot;RVS&quot;,
            &quot;price&quot;:&quot;2.0&quot;
          }, 
          {&quot;name&quot;:&quot;GHI&quot;,
           &quot;price&quot;:&quot;3.0&quot;
          },
          {&quot;name&quot;:&quot;QRS&quot;,
           &quot;price&quot;:&quot;4.0&quot;
          }
        ]
  },
  {     &quot;id&quot;:&quot;99999&quot;,
        &quot;name&quot;:&quot;Jane Doe&quot;,
        &quot;products&quot;:  [
           {&quot;name&quot;:&quot;XYZ&quot;,
            &quot;price&quot;:&quot;2.0&quot;
           },
           {&quot;name&quot;:&quot;ABC&quot;,
            &quot;price&quot;:&quot;3.0&quot;
           }
         ]
  }]
}
</code></pre>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/5yXm0vh.png"" alt=""enter image description here"" /></p>
"
"74550833","ADF Update the record if column not matched (in 2nd condition)","<p>I am beginner in adf and trying to update SQL table through adf in Dataflw activity,</p>
<p>Source - Excel file
Sink - SQL table</p>
<p>Source and SinkColumns - AccountID, LegacyAccID, AccountGroupCD</p>
<p>Now I only want update the record in sink if below condition matched</p>
<p><strong>if(FileAccountID == DBLegacyID &amp;&amp; FileAccountID != DBAccountID)</strong></p>
<p>I can map the FileAccountID == DBLegacyID in sink mapping , How can I add 2nd condition,
really appreciate any help</p>
","<azure-data-factory>","2022-11-23 17:27:32","123","0","2","74551569","<p>One quick thing which I can think , is to use the stored proc actvity on the sink side ( Since you have only few incoming columns ) and use the Update logic in the stored procedure .
HTH</p>
"
"74550833","ADF Update the record if column not matched (in 2nd condition)","<p>I am beginner in adf and trying to update SQL table through adf in Dataflw activity,</p>
<p>Source - Excel file
Sink - SQL table</p>
<p>Source and SinkColumns - AccountID, LegacyAccID, AccountGroupCD</p>
<p>Now I only want update the record in sink if below condition matched</p>
<p><strong>if(FileAccountID == DBLegacyID &amp;&amp; FileAccountID != DBAccountID)</strong></p>
<p>I can map the FileAccountID == DBLegacyID in sink mapping , How can I add 2nd condition,
really appreciate any help</p>
","<azure-data-factory>","2022-11-23 17:27:32","123","0","2","74556097","<p>Add an Alter Row transformation and set that conditional expression in the rule for the Update property.</p>
"
"74548424","Copy activity fails while adding additional columns in parquet file","<p>I have been trying to add additional column in parquet file through copy activity pipeline to copy from csv file to parquet
but it is giving me error the column name is invalid. Column name cannot contain these character:[,;{}()\n\t=]</p>
<p>I am adding only filename as column name in additional column at source and taking $$filepath as value.</p>
","<azure><azure-data-factory>","2022-11-23 14:28:42","170","0","1","74556118","<p>I reproduced the above and got the same error.</p>
<p><img src=""https://i.imgur.com/X9vn933.png"" alt=""enter image description here"" /></p>
<p>The above error occurs when we give special characters (<code>,;{}()\n\t=</code>) in the column name of the additional column. For Parquet file special characters not allowed for columns names.</p>
<p><img src=""https://i.imgur.com/5jb0C6I.png"" alt=""enter image description here"" /></p>
<p>When we avoid the above special characters in the column name, we can get desired result.</p>
<p><img src=""https://i.imgur.com/nVah4vY.png"" alt=""enter image description here"" /></p>
<p><strong>Parquet file:</strong></p>
<p><img src=""https://i.imgur.com/ewfi3Kg.png"" alt=""enter image description here"" /></p>
"
"74548062","Azure Data Factory -> Tumbling Window","<p>I have a <strong>schedule trigger</strong> that executes a pipeline <strong>every day once at 6:15 AM</strong>. And now my requirement is I have multiple pipelines which I want to run post-completion of my pipeline part of the said <strong>schedule trigger</strong>. How can I achieve this using the <strong>Tumbling window?</strong> I don't find a way.</p>
<p>Thanks,
Vivek</p>
","<azure><azure-data-factory>","2022-11-23 14:03:55","65","0","1","74548485","<p>You can add a dependency on the scheduling trigger in the Advanced section of the tumbling window trigger settings.</p>
<p><a href=""https://i.stack.imgur.com/pcOZ0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pcOZ0.png"" alt=""enter image description here"" /></a></p>
"
"74545983","Synapse Pipeline : DF-Executor-OutOfMemoryError","<p>I am having nested json as source in gzip format. In Synapse pipeline I am using the dataflow activity where I have mentioned the compression type as gzip in the source dataset. The pipeline was executing fine for small size files under 10MB. When I tried to execute pipeline for a large gzip file about 89MB.</p>
<p>The dataflow activity failed with below error:</p>
<pre><code>Error1 {&quot;message&quot;:&quot;Job failed due to reason: Cluster ran into out of memory issue during execution,
 please retry using an integration runtime with bigger core count and/or memory optimized compute type.
 Details:null&quot;,&quot;failureType&quot;:&quot;UserError&quot;,&quot;target&quot;:&quot;df_flatten_inf_provider_references_gz&quot;,&quot;errorCode&quot;:&quot;DF-Executor-OutOfMemoryError&quot;}
</code></pre>
<p><a href=""https://i.stack.imgur.com/g6Ut2.png"" rel=""nofollow noreferrer"">Error1</a></p>
<p>Requesting for your help and guidance.</p>
<p>To resolve Error1, I tried Azure integration runtime with bigger core count (128+16 cores) and memory optimized compute type but still the same error.
I thought it could be too intensive to read json directly from gzip so I tried a basic copy data activity to decompress the gzip file first but still its failing with the same error.</p>
","<azure><azure-data-factory><azure-synapse><azure-data-lake-gen2><azure-adf>","2022-11-23 11:22:22","85","0","1","74642082","<p>As per your scenario I would recommend Instead of pulling all the data from Json file, pulled from small Json files. You first partitioned your big Json file in few parts with the dataflow using <strong>Round robin partition technique</strong>. and store this files into a folder in blob storage</p>
<p><img src=""https://i.imgur.com/XtrLw4L.png"" alt=""enter image description here"" /></p>
<p>Data is evenly distributed among divisions while using round robin. When you don't have excellent key candidates, use round-robin to put a decent, clever partitioning scheme into place. The number of physical divisions is programmable.</p>
<blockquote>
<p>You need to evaluate the data size or the partition number of input data, then set reasonable partition number under &quot;Optimize&quot;. For example, the cluster that you use in the data flow pipeline execution is 8 cores and the memory of each core is 20GB, but the input data is 1000GB with 10 partitions. If you directly run the data flow, it will meet the OOM issue because 1000GB/10 &gt; 20GB, so it is better to set repartition number to 100 (1000GB/100 &lt; 20GB).</p>
</blockquote>
<p>And after above process use these partitioned files to perform dataflow operations with for each activity and in last merge them in a single file.</p>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance-sources"" rel=""nofollow noreferrer"">Partition in dataflow.</a></p>
"
"74545944","Azure Data Factory Script Activity does not like the keyword GO","<p>If I create a script, e.g.</p>
<pre><code>print 'hello'
GO
print 'cats'
GO
</code></pre>
<p>Then the script errors when I try to run my ADF pipeline:</p>
<blockquote>
<p>Operation on target GreetCatsActivity failed: Incorrect syntax near 'GO'.</p>
</blockquote>
<p>Is GO not allowed in scripts? The issue is I need it to run a gigantic script that is autogenerated and has tons of GO statements in it. Part of the script might reference things created earlier in the script so I suspect the GO statements are important to ensure items are created to be used later on.</p>
<p>Could I be doing something wrong or is there another way to handle this?</p>
","<azure-sql-database><azure-data-factory>","2022-11-23 11:18:06","131","0","1","74555430","<p>Just throught of sharing this as a think this may be helpful .
You can use the a Mapping data flow pipeline to replace the &quot;GO&quot; .
What I tried is on the source side I added the scripts ( a files with the extension .sql , I am assuming you must be gaving something similar  )  which is shared with &quot;GO&quot; above &amp;  I used the FIlter to ger rid of the all the GO's and on the sink I did write back the scripts ( without GO to a blob .</p>
<p><a href=""https://i.stack.imgur.com/nbEH9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nbEH9.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/q7hHe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q7hHe.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/Ueg4I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ueg4I.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/uaph9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uaph9.png"" alt=""enter image description here"" /></a></p>
<p>Now I wanted to automate the execution of the command using the Script activity in ADF .</p>
<p>For executing the queries we will use the a Lookup which will read the file which we created in the last step and a for each loop and a Script activity inside that .</p>
<p>My lookup output looks something like</p>
<p><a href=""https://i.stack.imgur.com/jmkWQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jmkWQ.png"" alt=""enter image description here"" /></a></p>
<p>and so wjem you are setting the scripts activity , please pass dynamic value as i did
<a href=""https://i.stack.imgur.com/VoSTt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VoSTt.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/SkPfR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SkPfR.png"" alt=""enter image description here"" /></a></p>
<p>HTH</p>
"
"74545918","Create a folder based on date (YYYY-MM) using Data Factory?","<p>I have few set of monthly files dropping in my data lake folder and I want to copy them to a different folder in the data lake and while copying the data to the target data lake folder, I want to create a folder in the format YYYY-MM (Ex: 2022-11) and I want to copy the files inside this folder.</p>
<p>And again in the next month I will get new set of data and I want to copy them to (2022-12) folder and so on.</p>
<p>I want to run the pipeline every month because we will get monthly load of data.</p>
","<azure><triggers><azure-data-factory>","2022-11-23 11:16:05","378","0","1","74548553","<p>As you want to copy only the new files using the ADF every month,
This can be done in two ways.</p>
<p><strong>First will be using a Storage event trigger.</strong></p>
<p><strong>Demo:</strong></p>
<p>I have created pipeline parameters like below for new file names.</p>
<p><img src=""https://i.imgur.com/hrsZMWi.png"" alt=""enter image description here"" /></p>
<p>Next create a storage event trigger and give the <code>@triggerBody().fileName</code> for the pipeline parameters.</p>
<p><img src=""https://i.imgur.com/k5BafTD.png"" alt=""enter image description here"" /></p>
<p>Parameters:</p>
<p><img src=""https://i.imgur.com/ab9z9b8.png"" alt=""enter image description here"" /></p>
<p>Here I have used two parameters for better understanding. If you want, you can do it with single pipeline parameter also.</p>
<p><strong>Source dataset with dataset parameter for filename:</strong></p>
<p><img src=""https://i.imgur.com/DneNLXD.png"" alt=""enter image description here"" /></p>
<p><strong>Sink dataset with dataset parameter for Folder name and filename:</strong></p>
<p><img src=""https://i.imgur.com/RQR9LmU.png"" alt=""enter image description here"" /></p>
<p><strong>Copy source:</strong></p>
<p><img src=""https://i.imgur.com/CUHke3U.png"" alt=""enter image description here"" /></p>
<p><strong>Copy sink:</strong></p>
<p>Expression for foldername: <code>@formatDateTime(utcnow(),'yyyy-MM')</code></p>
<p><img src=""https://i.imgur.com/jDO0wJC.png"" alt=""enter image description here"" /></p>
<p>File copied to required folder successfully when I uploaded to source folder.</p>
<p><img src=""https://i.imgur.com/bB23kLN.png"" alt=""enter image description here"" /></p>
<p>So, every time a new file uploaded to your folder it gets copied to the required folder. If you don't want the file to be exist after copy, use delete activity to delete source file after copy activity.</p>
<p><strong>NOTE:</strong> Make sure you publish all the changes before triggering the pipeline.</p>
<p><strong>Second method</strong> can be using Get Meta data activity and ForEach and copy activity inside ForEach.</p>
<p>Use schedule trigger for this for every month.</p>
<p>First use <strong>Get Meta data</strong>(use another source dataset and give the path only till folder) to get the child Items and in the <strong>filter by Last modified of Meta data activity give your month starting date in UTC(use dynamic content <code>utcnow()</code> and <code>FormatDatetime()</code> for correct format)</strong>.</p>
<p>Now you will get all the list of child Items array which have last modified date as this month. Give this <strong>array to ForEach and inside ForEach use copy activity</strong>.</p>
<p>In copy activity source, use dataset parameter for file name (same as above) and give <code>@item().name</code>.</p>
<p>In copy activity sink, use two dataset parameters, one for Folder name and another for file name.
In Folder name give the same dynamic content for <code>yyyy-MM</code> format as above and for file name give as <code>@item().name</code>.</p>
"
"74543358","Data conversion during copy in Azure Data Factory","<p>I'm new to Data Factory ecosystem. I'm trying to copy data from source MySQL database to sink CosmosDB for MongoDB. An example source schema that I have is something like this:</p>
<pre><code>*inventory_warehouse_table*

--------------------------------------------------------
| id | warehouse_name | warehouse_address | lat | long |
--------------------------------------------------------
</code></pre>
<p>The sink schema is of the form like:</p>
<pre><code>*inventory_warehouse_collection*

{
  id: &lt;int&gt;
  warehouse_name: &lt;string&gt;
  warehouse_address: &lt;string&gt;
  geo_coordinates: {
   type: &quot;Point&quot;,
   coordinates: [lat, long]
  }
}
</code></pre>
<p>I don't see any schema mapping in copy data activity for achieving the same. How can I do this in Azure Data Factory? Is there any other pipeline I need to create for doing so?</p>
","<mysql><azure><azure-data-factory><azure-cosmosdb-mongoapi>","2022-11-23 07:54:06","128","0","1","74546191","<ul>
<li>Source and sink are taken as in below images.
<img src=""https://i.imgur.com/Q77aOJp.png"" alt=""enter image description here"" />
Img:1 Source dataset</li>
</ul>
<p><img src=""https://i.imgur.com/jlmoYaT.png"" alt=""enter image description here"" /></p>
<p>Img:2 Sink Dataset</p>
<ul>
<li>In mapping activity, when import schemas is clicked, Mapping between source and sink is seen.
<img src=""https://i.imgur.com/XV9koV4.png"" alt=""enter image description here"" />
img-3 mapping setting</li>
</ul>
<p>Except the last column, all other columns are mapped. Since last column has concat of lat, long columns, Dataflow is used to combine the columns. Below is the approach.</p>
<ul>
<li>Source is taken in dataflow. Then Derived column transformation is added and expression for new column is given as <code>concat(&quot;[&quot;,toString(lat),&quot;,&quot;,toString(long),&quot;]&quot;)</code></li>
</ul>
<p><img src=""https://i.imgur.com/AEwO6iF.png"" alt=""enter image description here"" /></p>
<ul>
<li>Since cosmos dB - mongo API is not supported in dataflow, data is staged in blob storage.</li>
</ul>
<p><img src=""https://i.imgur.com/lduy2iW.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then copy activity is used to copy data from blob to cosmos db-mongo api</li>
</ul>
<p><img src=""https://i.imgur.com/lduy2iW.png"" alt=""enter image description here"" /></p>
<ul>
<li>New row is inserted into cosmos db.
<img src=""https://i.imgur.com/Rxr3nz7.png"" alt=""enter image description here"" /></li>
</ul>
"
"74543329","Truncate a table in Oracle Database before Ingestion in Data Factory V2","<p>I'm working on a Ingestion Flow where we ingest the data from csv file to Oracle database. This is a truncate and load. So we should truncate the table before it is loaded. Trying to execute the below SP in the Lookup activity of ADF</p>
<pre><code>BEGIN
execute oracle.cml_trunc_table('SCHEMA','TABLE_NAME')&quot;;
END;
</code></pre>
<p>Throws the below Error</p>
<pre><code>PLS-00103: Encountered the symbol &quot;ORACLE&quot; when expecting one of the following:

:= . ( @ % ; immediate
The symbol &quot;:=&quot; was substituted for &quot;ORACLE&quot; to continue.
</code></pre>
<p>Also tried removing the &quot;ORACLE&quot; keyword and it failed with Invalid SQL/
Is there any other approach to execute the truncate table statement</p>
<p>Thanks in advance</p>
","<oracle><azure-data-factory>","2022-11-23 07:51:19","50","0","2","74543710","<p>I don't know anything about Azure, but - as error you got is related to <strong>Oracle</strong>, it says that you've used invalid syntax.</p>
<p>It looks like that you're trying to call a procedure named <code>cml_trunc_table</code> which is <strong>a)</strong> owned by Oracle user named <code>oracle</code>, or <strong>b)</strong> part of a package named <code>oracle</code>.</p>
<p><code>execute</code>, on the other hand, looks as if you tried to <strong>a)</strong> execute that procedure at SQL*Plus prompt, or <strong>b)</strong> run dynamic SQL (but then you have to use <code>execute immediate</code> - that's why error says that &quot;immediate&quot; might be missing).</p>
<p>From my point of view, you should try one of these:</p>
<p>Remove <code>execute</code>, entirely:</p>
<pre><code>begin
  oracle.cms_trunc_table('SCHEMA', 'TABLE_NAME');
end;
/
</code></pre>
<p>or (if you're running it at SQL*Plus prompt - I doubt you are):</p>
<pre><code>exec oracle.cms_trunc_table('SCHEMA', 'TABLE_NAME');
</code></pre>
<p>or use dynamic SQL, but then you'd just <em>truncate</em> the table, not call the procedure:</p>
<pre><code>begin
  execute immediate 'truncate table schema.table_name';
end;
/
</code></pre>
<p>If I had to bet, I'd put my money on the 1st option I posted.</p>
"
"74543329","Truncate a table in Oracle Database before Ingestion in Data Factory V2","<p>I'm working on a Ingestion Flow where we ingest the data from csv file to Oracle database. This is a truncate and load. So we should truncate the table before it is loaded. Trying to execute the below SP in the Lookup activity of ADF</p>
<pre><code>BEGIN
execute oracle.cml_trunc_table('SCHEMA','TABLE_NAME')&quot;;
END;
</code></pre>
<p>Throws the below Error</p>
<pre><code>PLS-00103: Encountered the symbol &quot;ORACLE&quot; when expecting one of the following:

:= . ( @ % ; immediate
The symbol &quot;:=&quot; was substituted for &quot;ORACLE&quot; to continue.
</code></pre>
<p>Also tried removing the &quot;ORACLE&quot; keyword and it failed with Invalid SQL/
Is there any other approach to execute the truncate table statement</p>
<p>Thanks in advance</p>
","<oracle><azure-data-factory>","2022-11-23 07:51:19","50","0","2","74551671","<p>One more thing is that the lookup activity expects some values to be returned  back . Pardon my zero knowldge on PL/SQL , but in the tSQL works , I will return some values , so the last line of stored procedure should hhave somelike</p>
<p>SELECT somecolumn .</p>
"
"74542548","How can I create the current Year/Month/Day folder dynamically in the Azure Data Factory pipeline?","<p>I'm using copy activity to send data to Azure Data Lake Gen2. I need to create a Year/Month/Day folder dynamically.</p>
<pre><code>file_1.csv
file_2.csv
file_3.csv
.
-
-
file_9.csv
</code></pre>
<p>My question: how can I Create a Year/Month/Day folder dynamically transferring from one container to another container?</p>
","<azure><azure-data-factory>","2022-11-23 06:22:09","195","0","1","74544163","<p>You can use the following procedure for getting <strong>Year/Month/Day</strong> folder dynamically.</p>
<p>In my storage account, these are the files.</p>
<p><img src=""https://i.imgur.com/UIX6pPy.png"" alt=""enter image description here"" /></p>
<ul>
<li>Create a copy activity with wild card path.</li>
</ul>
<p><img src=""https://i.imgur.com/QYbQC7b.png"" alt=""enter image description here"" /></p>
<p>Then, go to sink -&gt; Open sink dataset and Create dataset parameter with the name <code>folder</code>.</p>
<p><img src=""https://i.imgur.com/kqAs4Hb.png"" alt=""enter image description here"" /></p>
<p>Go to connection, and added this dynamic content: <code>@dataset().folder</code></p>
<p><img src=""https://i.imgur.com/8ftZgiN.png"" alt=""enter image description here"" /></p>
<p><em>Now, add this dynamic content to the dataset properties value:</em></p>
<pre><code>@concat(formatDateTime(utcnow(), 'yyyy'), '/',formatDateTime(utcnow(),'MM'),'/',formatDateTime(utcnow(),'dd'),'/')
</code></pre>
<p><img src=""https://i.imgur.com/xPzne4q.png"" alt=""enter image description here"" /></p>
<p><em><strong>Pipeline successfully executed and got this output :</strong></em></p>
<p><img src=""https://i.imgur.com/TFx81aT.png"" alt=""enter image description here"" /></p>
"
"74536245","Overwrite sql table with new data in Azure Dataflow","<p>Here is my situation. Iam using Alteryx ETL tool where in basically we are appending new records to tableau by using option provided like 'Overwrite the file'.</p>
<p>What it does is any data incoming is captured to the target and delete the old data--&gt; publish results in Tableau visualisation tool.</p>
<p>So whatever data coming in source must overwrite the existing data in Sink table.</p>
<p>How can we achieve this in Azure data Flow?</p>
","<azure><azure-data-factory>","2022-11-22 16:46:59","107","0","2","74538820","<p>If your requirement is just to copy data from your source to target and truncate the table data before the latest data is copied, then you can just use a copy activity in Azure Data factory. In copy activity you have an option called Pre-copy script, in which you can specify a query to truncate the table data and then proceed with copying the latest data.</p>
<p><a href=""https://i.stack.imgur.com/nHIVy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nHIVy.png"" alt=""enter image description here"" /></a></p>
<p>Here is an article by a community volunteer where a similar requirement has been discussed with various approaches - <a href=""https://azurelib.com/truncate-table-before-copy/"" rel=""nofollow noreferrer"">How to truncate table in Azure Data Factory</a></p>
<p>In case if your requirement is to do data transformation first and then copy the data to your target sql table and truncate table before your copy the latest transformed data then, you will have to use mapping data flow activity.
<a href=""https://i.stack.imgur.com/dQMlp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dQMlp.png"" alt=""enter image description here"" /></a></p>
"
"74536245","Overwrite sql table with new data in Azure Dataflow","<p>Here is my situation. Iam using Alteryx ETL tool where in basically we are appending new records to tableau by using option provided like 'Overwrite the file'.</p>
<p>What it does is any data incoming is captured to the target and delete the old data--&gt; publish results in Tableau visualisation tool.</p>
<p>So whatever data coming in source must overwrite the existing data in Sink table.</p>
<p>How can we achieve this in Azure data Flow?</p>
","<azure><azure-data-factory>","2022-11-22 16:46:59","107","0","2","74556294","<p>If you are writing to a database table, you'll see a sink setting for &quot;truncate table&quot; which will remove all previous rows and replace them with your new rows. Or if you are trying to overwrite just specific rows based on a key, then use an Alter Row transformation and use the &quot;Update&quot; option.</p>
"
"74535404","Azure Data Factory Cosmos DB Copy Data: Write to Target Sink Container only if there are no records exists using Sink's Dynamic content condition","<p>I have created Azure Data Factory where it Copies data from Once Cosmos DB container (Source) into Sink Target Container. I have requirement that Copy Data should happen only and only if there are no Records in Target Sink container.</p>
<p><a href=""https://i.stack.imgur.com/7nNVq.png"" rel=""nofollow noreferrer"">This is the screen shot for reference</a></p>
<p>Could you please help to write any conditional check in Sink Dynamic Content to copy the data if only if there are no records in Target Sink Container.</p>
<p>Your help will be much appreciated</p>
","<azure-data-factory><azure-data-explorer>","2022-11-22 15:49:24","142","0","1","74573286","<p>You cannot directly create Sink <strong>Dynamic Content</strong> to copy the data as per the above scenario. You can use the following procedure for getting Sink to copy the data if only there are no records in Target Sink Container.</p>
<p>I reproduce the same in my environment using <strong>lookup</strong> and <strong>If Condition</strong>.</p>
<ul>
<li><em>First, create a lookup activity with the source dataset.</em></li>
</ul>
<p><img src=""https://i.imgur.com/iPCuMLB.png"" alt=""enter image description here"" /></p>
<ul>
<li><em>Then, If I executed my lookup activity this is the output.</em></li>
</ul>
<p><img src=""https://i.imgur.com/KYiZWYS.png"" alt=""enter image description here"" /></p>
<ul>
<li><p><em>As you can check the output of the lookup activity has <strong>count: 0</strong> because Inside the cosmos DB container there are no files.</em></p>
</li>
<li><p><em>So, connect <strong>lookup</strong> activity with <strong>If Condition</strong>. Use this Dynamic expression</em>: <code>@greater(activity('Lookup1').output.count,0)</code></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/d4yW2Ej.png"" alt=""enter image description here"" /></p>
<ul>
<li><em>As per the above scenario, Choose If Condition activity is False -&gt; Use copy activity. Create another cosmos dataset with data. This is the sample data. I have in the cosmos DB dataset.</em></li>
<li><em>Note:  I created two datasets with and without data of cosmos DB.</em></li>
</ul>
<p><img src=""https://i.imgur.com/R47tCUR.png"" alt=""enter image description here"" /></p>
<ul>
<li><em>Use the same lookup activity cosmos DB dataset in the copy activity sink.</em></li>
</ul>
<blockquote>
<p><strong>Pipeline successfully executed and got the output:</strong></p>
</blockquote>
<p><img src=""https://i.imgur.com/6yhEEYL.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/WMCc4Ke.png"" alt=""enter image description here"" /></p>
"
"74534373","Azure Data Factory Event Based Trigger File to Execute the two different environment ADF pipelines","<p>I would like to take your advise and approach, how to we implement if we have two different ADF with different subscription or environment, we decided to implement to use trigger file approach to identify the first ADF-A environment pipeline has been completed, so that we would have to automatically start/trigger the ADF-B environment pipeline. this is our goal to implement the solution for our project. kindly someone, please provide the best practice approach and component to implement above that solution.</p>
<p>I would be really appreciating your help for me to lean further on this platform.</p>
<p>Advance Thanks</p>
","<azure-data-factory>","2022-11-22 14:37:51","157","0","1","74538959","<p>If your requirement is to trigger a <code>PipelineA</code> when a new file arrives using Event Triggers and then once the <code>PipelineA</code> run is completed successfully then initiate a <code>PipelineB</code> run then you can utilize the REST API using a web activity and initiate a pipeline run.</p>
<p><strong>Approach 1</strong>:
To do this in in your <code>PipelineA</code> at the end of all activity have a web activity  to make a call to your <code>pipelineB</code> using the REST API - <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run?tabs=HTTP"" rel=""nofollow noreferrer"">Pipelines - Create Run</a>.</p>
<p>Here is an article by a community volunteer on how to use REST API to trigger a pipeline run - <a href=""https://medium.com/@rajbca00/execute-azure-adf-pipeline-using-rest-api-76d6cee5c473"" rel=""nofollow noreferrer"">Execute Azure ADF Pipeline using REST API</a></p>
<p><strong>Approach 2</strong>:
The second approach could be, before end of your <code>PipelineA</code> write a dummy file to a blob location and create an event trigger for your <code>pipelineB</code> so that the event trigger looks for file creation in the same location where <code>pipelineA</code> writes a file before completion and as soon as it is created then the second <code>pipelineB</code> starts executing.</p>
"
"74534020","How to create a trigger to ( to data factory or azure function or databricks ) when there are 500 new files land in a azure storage blob","<p>I have a azure storage container where I will be getting many files on daily basis.My requirement is that I need a trigger in azure data factory or databricks when each time 500 new files are arrived so I can process them.
In Datafatory we have event trigger which will trigger for each new files(with filename and path). but is it possible to get multiple new files and their details at same time?
What services in azure I can use for this scenario.. event hub? azure function.. queues?</p>
","<azure><azure-functions><azure-data-factory><azure-databricks><azure-eventhub>","2022-11-22 14:11:48","132","0","1","74534951","<p>One of the characteristics of a serverless architecture is to execute something whenever a new event occurs. Based on that, you can't use those services alone.</p>
<p>Here's what I would do:</p>
<p>#1 Azure Functions with Blob Trigger, to execute whenever a new file arrives. This would not start the processing of the file, but just 'increment' the count of files that would be stored on Cosmos DB.</p>
<p>#2 Azure Cosmos DB, also offers a Change Feed, which is like an event sourcing whenever something changes in a collection. As the document  created / modified on #1 will hold the count, you can use another Azure Function #3 to consume the  change feed.</p>
<p>#3 This function will just contain an if statement which will &quot;monitor&quot; the current count, and if it's above the threshold, start the processing.</p>
<p>After that, you just need to update the document and reset the count.</p>
"
"74533386","Azure Data Factory: Upload filesystem binary files (jpg or png) to varbinary column in on-premises SQL Database using self-hosted Integration Runtime","<p>I am trying to upload a folder (about 7300 items) of picture files (jpg or png) from the filesystem to an on-premises SQL database table called DocumentBinary:</p>
<p><a href=""https://i.stack.imgur.com/ruDWF.png"" rel=""nofollow noreferrer"">DocumentBinary table</a></p>
<p>I want to use a Data Factory pipeline for this.
I am using a self-hosted Integration Runtime to connect to the SQL Server database.</p>
<p>I have searched all over but cannot find an example of how to do this.</p>
<p>I have tried using a 'Copy Data' activity with a binary source dataset for the files in the filesystem (pointing at the file folder), and a SQL Server sink dataset for the database table, but Data Factory shows the following error message:</p>
<blockquote>
<p>Sink must be binary when source is binary dataset.</p>
</blockquote>
<p>(both source and sink need to be binary).</p>
<p>I have tried using a DelimitedText dataset with no delimiters, as the source, but I get the following error:</p>
<blockquote>
<p>Row/Column delimiter cannot be empty string ... when dataset is referenced in Copy Data Activity Name.</p>
</blockquote>
","<sql-server><azure-data-factory>","2022-11-22 13:26:49","310","0","1","74542985","<blockquote>
<p>Sink must be binary when source is binary dataset.</p>
</blockquote>
<p>Cause of error is <strong>In Azure Data Factory If you are trying to copy Binary file then your Source and Sink both should be binary dataset</strong></p>
<p>As per your requirement I want to suggest you a Workaround or alternative to overcome this issue.</p>
<ul>
<li>First, I took <code>Get Metadata activity</code> and set Field list as <code>Childitems</code> <strong>to get the all picture files (jpg or Png) from the filesystem</strong>.</li>
</ul>
<p>Dataset for Get Metadata:
<img src=""https://i.imgur.com/kUS0A61.png"" alt=""enter image description here"" /></p>
<p>Get Metadata setting:
<img src=""https://i.imgur.com/I9974xO.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then I passed the output of Get Metadata to ForEach with <code>@activity('Get Metadata1').output.childItems</code></li>
</ul>
<p><img src=""https://i.imgur.com/qu8EAdp.png"" alt=""enter image description here"" /></p>
<ul>
<li>After that in copy activity I took one <code>Set Variable activity</code> where <strong>I created a Demo variable with string datatype</strong> and in value I am concating the path with file name like <code>@concat('E:\images\',item().name)</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/AfsC9wE.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I am passing the above variable to script activity to insert that file into table with insert query as below</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>INSERT INTO ImageTable (Name, Photo)
SELECT '@{item().name}', BulkColumn
FROM Openrowset( Bulk '@{variables('demo')}', Single_Blob) as image
</code></pre>
<p><img src=""https://i.imgur.com/OYe3Rit.png"" alt=""enter image description here"" /></p>
<p><strong>OUTPUT</strong></p>
<p><img src=""https://i.imgur.com/O6ra3uU.png"" alt=""enter image description here"" /></p>
"
"74532114","Azure data factory - pass multiple values from lookup into dynamic query?","<p>I have a lookup function that returns a list of valid GUID IDs in ADF. I then have a foreach process which runs a stored procedure for each GUID ID and assigns an ID column to it.</p>
<p>What I want to do is then have another lookup run which will run the below query to bring me the GUID and also the newly assigned ID. It is very simple to write in SQL</p>
<pre><code>SELECT GUID, Identifier from DBO.GuidLOAD
             WHERE GUID in ('GUIDID','GUIDID','GUIDID')
</code></pre>
<p>However I am struggling to translate this in ADF.. I have got as far as the @Concat part and most of the help I find online only refers to dynamic queries with single values as input parameters.. where mine is a list of GUIDs where they may be 1, more or none at all..</p>
<p>Can someone advise the best way of writing this dynamic query?</p>
<p><a href=""https://i.stack.imgur.com/HIETt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HIETt.png"" alt=""enter image description here"" /></a></p>
<p>first 2 run fine, I just need the third lookup to run the query based on the output of the first lookup</p>
","<azure-sql-database><azure-data-factory>","2022-11-22 11:45:36","628","0","1","74533971","<ul>
<li>You can use string interpolation (@{...}) instead of <code>concat()</code>. I have a sample table with with 2 records in my demo table as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/RqiauvM.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have sample look up which returns 3 guid records. The following is debug output of lookup activity.</li>
</ul>
<p><img src=""https://i.imgur.com/TH1pUf7.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have used a for loop to create an array of these guid's returned by lookup activity using append variable activity. The <code>items</code> value for each activity is <code>@activity('get guid').output.value</code>. The following is the configuration of append variable inside for each.</li>
</ul>
<pre><code>@item().guids
</code></pre>
<p><img src=""https://i.imgur.com/v1B9UNN.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have used <code>join</code> function on the above array variable to create a string which can be used in the required query.</li>
</ul>
<pre><code>&quot;@{join(variables('req'),'&quot;,&quot;')}&quot;
</code></pre>
<p><img src=""https://i.imgur.com/B08aw6b.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, the query accepts guid's wrapped inside single quotes i.e., <code> WHERE GUID in ('GUIDID','GUIDID','GUIDID')</code>. So, I created 2 parameters with following values. I used them in order to replace double quotes from the above final variable with single quotes.</li>
</ul>
<pre><code>singlequote: '
doublequote: &quot;
</code></pre>
<p><img src=""https://i.imgur.com/MyAWgTv.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now in the look up where you want to use your query, you can build it using the below dynamic content:</li>
</ul>
<pre><code>SELECT guid, identifier from dbo.demo WHERE GUID in (@{replace(variables('final'),pipeline().parameters.doublequote,pipeline().parameters.singlequote)})
</code></pre>
<p><img src=""https://i.imgur.com/L0U1JZi.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, when I debug the pipeline, the following query would be executed which can be seen in the debug input of the final lookup.</li>
</ul>
<p><img src=""https://i.imgur.com/bQ8tEmE.png"" alt=""enter image description here"" /></p>
<ul>
<li>The output would be as below. Only one row should be returned from the sample I have taken and output is as expected:</li>
</ul>
<p><img src=""https://i.imgur.com/tAppIcX.png"" alt=""enter image description here"" /></p>
"
"74531579","Data factory/Synapse copy task fails with staging enabled when no files found","<p>We have a pipeline that copies data from parquet files stored in our ADLS gen 2 data lake to a Synapse dedicated SQL pool. The pipeline is metadata driven and will loop through a list of source containers and process new files in each of them by filtering on the last modified start time. Staging is enabled because we add a number of columns for logging purposes before the data is inserted into the dedicated pool.</p>
<p><a href=""https://i.stack.imgur.com/oY39B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oY39B.png"" alt=""Configuration of copy task"" /></a></p>
<p>The pipeline runs great, except when no new files are found. The staging step of the copy activity is still executed, but the step from staging to Synapse fails because the temporary staging location cannot be found as no files have been placed there:</p>
<p><a href=""https://i.stack.imgur.com/pfj0W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pfj0W.png"" alt=""Second step fails"" /></a></p>
<p>I've been searching for a way to skip the copy task altogether if no files are found, but tricky part is that the copy task searches recursively through the raw/data folder for any new files (final struncture is raw/data/yyyy/mm/dd/filename.parquet). The get metadata activity which would usually be the likely solution is not able to do this so I have no way of checking first if new files exist before the copy activity is executed. I have seen posts where you can use dynamic variables to create a recursive list for the get metadata activity but that creates too much overhead and will not be a workable solution.
Does anyone here have any idea as to how I can prevent the copy activity from failing in the above scenario, or provide an easy way to skip it?</p>
","<azure-data-factory><azure-synapse>","2022-11-22 11:04:04","222","1","1","74545887","<p>As your file structure is same for every parquet file, you can use Get Meta data activity to list out all the files and add filter condition to get the all the latest files.</p>
<p>These are my sample files inside the folders.</p>
<p><img src=""https://i.imgur.com/JI6Xvho.png"" alt=""enter image description here"" /></p>
<p>I have used a dataset parameter for wildcard placeholder.</p>
<p><img src=""https://i.imgur.com/eBjXsP7.png"" alt=""enter image description here"" /></p>
<p>You can see I got only one file output as <code>ChildItems</code>.</p>
<p><img src=""https://i.imgur.com/Zqq1QM5.png"" alt=""enter image description here"" /></p>
<p>If the above <strong>array length is 0 then it means, there are no latest files</strong>. So check whether the length of the <code>ChildItems</code> array is equals to Zero or not.</p>
<p>Use if condition and below expression.</p>
<pre><code>@not(equals(length(activity('Get Metadata1').output.childItems),0))
</code></pre>
<p>In <strong>True activities use your copy activity</strong> by which you can execute copy activity only if there are latest files in the storage.</p>
<p><img src=""https://i.imgur.com/7Qodyuz.png"" alt=""enter image description here"" /></p>
"
"74531230","(Azure Data Factory Pipeline) For each Notebook in a specific folder","<p>How can I create a ForEach activity that:</p>
<ol>
<li>Get the list of all Notebooks existing in a specific folder workspace in Databricks.</li>
<li>Execute each Notebook</li>
</ol>
<p>Because currently, I'm doing it by adding a Notebook activity for each Notebook, and connecting them one after another.
But this kind of working is not efficient, because when a new Notebook is created in Databricks, I must remember to update my Pipeline execution in Azure Synapse Data Factory.</p>
<p>Thanks.</p>
","<jupyter-notebook><azure-data-factory><azure-synapse>","2022-11-22 10:37:36","221","1","1","74533140","<p>You can use this <a href=""https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/workspace#example-4"" rel=""nofollow noreferrer"">REST API</a> to get the Notebooks list in a cluster.
First, I tried to get the list using web activity but not able to do it. That's why I have used Databricks notebook(<code>start_notebook</code>) to get the list of notebooks and then filtered required notebooks.</p>
<p><strong><code>start_notebook</code> code:</strong></p>
<pre><code>import requests
import json

my_json = {&quot;path&quot;: &quot;/Users/&lt; yours@mail.com &gt;/Folder/&quot;}    

auth = {&quot;Authorization&quot;: &quot;Bearer &lt;Access token&gt;&quot;}

response = requests.get('https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/workspace/list', json = my_json, headers=auth).json()
dbutils.notebook.exit(response)
</code></pre>
<p>Then In ADF Notebook activity output you can get the list of notebook as JSON array like below.</p>
<p><img src=""https://i.imgur.com/u5GrQTS.png"" alt=""enter image description here"" /></p>
<p>Now use filter activity to filter the <code>start_notebook</code> from the above array.</p>
<p>I have used a parameter for the name.</p>
<p><img src=""https://i.imgur.com/NziMrr0.png"" alt=""enter image description here"" /></p>
<p><strong>Filter activity:</strong></p>
<p><img src=""https://i.imgur.com/pmZPD8e.png"" alt=""enter image description here"" /></p>
<p><strong>Items:</strong> <code>@activity('Notebook1').output.runOutput.objects</code>
<strong>Condition:</strong><code>@not(equals(last(split(string(item().path),'/')), pipeline().parameters.start))</code></p>
<p><strong>Filter output array:</strong></p>
<p><img src=""https://i.imgur.com/kMmeBxQ.png"" alt=""enter image description here"" /></p>
<p>Give this output array to a ForEach as <code>@activity(&quot;Filter1&quot;).output.Value</code> and inside forEach use Notebook activity(give <code>@item().path</code> for Notebook path).</p>
<p><img src=""https://i.imgur.com/aDT6HLF.png"" alt=""enter image description here"" /></p>
"
"74528749","How to downgrade ADF Self-hosted Integration Runtime version?","<p>It has been very unstable since latest Self-hosted IR version was installed to VM.
There has been activity timeouts and even temp offlines.</p>
<p>How to downgrade to previous version? Need to uninstall IR from VM first?</p>
","<azure-data-factory>","2022-11-22 07:18:04","370","1","1","74654541","<p>I created virtual machine in Azure portal.
I created azure data factory and created selfhosted integration run time.</p>
<p>image for reference:</p>
<p><img src=""https://i.imgur.com/rvdKgkU.png"" alt=""enter image description here"" /></p>
<p>I install the Microsoft integration run time 5.22.8312.1 in virtual machine and connected to azure data factory using keys.
Image for reference:</p>
<p><img src=""https://i.imgur.com/uKEWTqV.png"" alt=""enter image description here"" /></p>
<p>It is successfully connected to Azure data factory.
Image for reference:</p>
<p><img src=""https://i.imgur.com/cy8aXja.png"" alt=""enter image description here"" /></p>
<p>I install the Microsoft integration run time 5.23.8324.1 in virtual machine without stopping the service of Microsoft integration run time 5.22.8312.1. upgraded version automatically connected to the integration runtime without entering key of runtime.
Image for reference:</p>
<p><img src=""https://i.imgur.com/uKEWTqV.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/L6OYMHw.png"" alt=""enter image description here"" /></p>
<p>I want to downgrade Integration runtime from 5.23.8324.1 version to 5.22.8312.1. for that I uninstall the 5.23.8324.1 uninstall the IR from VM without deleting the details of IR which is in Azure data factory.
Image for reference:</p>
<p><img src=""https://i.imgur.com/ulbouVp.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/OHBhaBQ.png"" alt=""enter image description here"" /></p>
<p>I reinstall the Microsoft integration runtime 5.22.8312.1 again in VM, its automatically connected to the data factory. IR is downgraded successfully.
Image for reference:</p>
<p><img src=""https://i.imgur.com/cy8aXja.png"" alt=""enter image description here"" /></p>
"
"74528215","Databricks jobs support file based trigger","<p>We would like know if we using Databricks jobs instead ADF for orchestration, we might have to check if databricks jobs support file based trigger. kindly advise.</p>
<p>ultimately goal is, we have different ADF  environment and subscription, we know that the subscription and environment does not a issues to stop our goal.</p>
<p>Kindly help.</p>
","<azure-databricks><azure-data-factory>","2022-11-22 06:14:55","165","1","2","74528730","<p>I doubt that . But in ADF you do have the support for file based trigger and also that ADF has a notebook activity . You can stich these together .
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook</a></p>
"
"74528215","Databricks jobs support file based trigger","<p>We would like know if we using Databricks jobs instead ADF for orchestration, we might have to check if databricks jobs support file based trigger. kindly advise.</p>
<p>ultimately goal is, we have different ADF  environment and subscription, we know that the subscription and environment does not a issues to stop our goal.</p>
<p>Kindly help.</p>
","<azure-databricks><azure-data-factory>","2022-11-22 06:14:55","165","1","2","74630205","<p>There will be upcoming feature to trigger jobs based on the file events.  It was mentioned in the latest <a href=""https://www.databricks.com/p/webinar/productroadmapwebinar"" rel=""nofollow noreferrer"">Databricks quarterly roadmap webinar</a> that you can watch.</p>
"
"74521279","Use secrets from Azure Key Vault in ADF","<p>I have few azure data factory activities which require username and password to be passed to execute. I am trying to keep this username and password in azure key vault and want to use this in the ADF activity.</p>
<p>I tried an option as shown below.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-use-azure-key-vault-secrets-pipeline-activities"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-use-azure-key-vault-secrets-pipeline-activities</a></p>
<p>But here in the web activity we have to provide the keyvault url, which will be different in different environments. That will be again difficult for ci/cd deployment.</p>
<p>Any other ways we could get azure KV secrets in ADF pipeline task ?</p>
","<azure><azure-data-factory><azure-keyvault>","2022-11-21 15:32:07","350","0","1","74526121","<p>First of all, to simplify the solution, define the url to Azure Key Vault as a global parameter.<br />
Then, update that global parameter from Release pipeline (Azure DevOps) with appropriate URL per environment.
You can deploy ADF with one of two options, depends on your preferences.<br />
For deployment check these (mine) resources:<br />
<a href=""https://sqlplayer.net/adftools/"" rel=""nofollow noreferrer"">#adftools</a><br />
<a href=""https://sqlplayer.net/2021/01/two-methods-of-deployment-azure-data-factory/"" rel=""nofollow noreferrer"">Two methods of deployment Azure Data Factory</a></p>
"
"74519320","Dynamic variable in Auth Headers - Azure Data Factory","<p>I have a ADF flow where I first retrieve an Oauth 2.0 token which then should be used in an Odata request.
This works perfect if I first extract the token and then hard code it in the auth headers(Bearer xxxxxxxx).</p>
<p>However I want to use a variable which I set earlier in the flow in this value. Like &quot;Bearer [variable]&quot;. I have tried with the following: &quot;Bearer @variables('Token')&quot; but it doesnt work. I have double checked and the variable 'Token' is getting the correct token value so this should not be the problem.</p>
<p><a href=""https://i.stack.imgur.com/P0Q4c.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>So my question is whether it is possible to use variables in the Auth Header section?</p>
<p>I have tried to use &quot;Bearer @variables('Token')&quot; in the Value field for Auth Headers. This doesnt work unfortunately.</p>
","<odata><azure-data-factory>","2022-11-21 12:57:13","301","0","1","74527789","<p><em><strong>I reproduced the same thing in my environment and got this output</strong></em></p>
<ul>
<li><em>Create a web activity and generated a bearer token with a sample URL</em> and <em>Create a parameter  variable with the name <code>Token</code></em></li>
</ul>
<p><img src=""https://i.imgur.com/lieRTrV.png"" alt=""enter image description here"" /></p>
<ul>
<li><em>Create a set variable and use this dynamic content value <code>@activity('Web1').output.data.Token</code></em></li>
</ul>
<p><img src=""https://i.imgur.com/PwW5L0V.png"" alt=""enter image description here"" /></p>
<p>*Then, I created a copy activity with <strong><code>Additional headers</code></strong>.</p>
<p>As suggested by @<strong>Scott Mildenberger</strong>*</p>
<pre><code>Authorization: @concat('Bearer ', variables('token'))
</code></pre>
<p><img src=""https://i.imgur.com/65jVVGP.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><em><strong>Pipeline successfully executed and got the output:</strong></em></p>
</blockquote>
<p><img src=""https://i.imgur.com/K8FjC6n.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/cNCAlcG.png"" alt=""enter image description here"" /></p>
"
"74517414","Access denied to storage account from Azure Data Factory","<p>My goal is to run an exe file stored in a private Azure Blob container.</p>
<p>The exe is simple : it creates a text file, write the current datetime in it, and then push it to the private Azure Blob container.</p>
<p>This has to be sent from Azure Data Factory. To do this, here is my environment :</p>
<ul>
<li><p>Azure Data Factory running with the simple pipeline :
<a href=""https://i.stack.imgur.com/txQ9r.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/txQ9r.png</a></p>
</li>
<li><p>Private storage account with the following configuration :
<a href=""https://i.stack.imgur.com/SJrGX.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/SJrGX.png</a></p>
</li>
<li><p>A linked service connected to the storage account :
<a href=""https://i.stack.imgur.com/8xW5l.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/8xW5l.png</a></p>
</li>
<li><p>A private managed virtual network approved :
<a href=""https://i.stack.imgur.com/G2DH3.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/G2DH3.png</a></p>
</li>
<li><p>A linked service connected to an Azure Batch :
<a href=""https://i.stack.imgur.com/Yaq6C.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/Yaq6C.png</a></p>
</li>
<li><p>A batch account linked to the right storage account</p>
</li>
<li><p>A pool running on this batch account</p>
</li>
</ul>
<p>Two things that I need to add in context :</p>
<ul>
<li>When I set the storage account to public, it works and I find the text file in my blob storage. So the process works well, but there is a security issue somewhere I can't find.</li>
<li>All the resources (ADF, Blob storage, Batch account) used have a role has contributor/owner of the blob with a managed identity.</li>
</ul>
<p>Here is the error I get when I set the storage account to private :</p>
<pre><code>{
   &quot;errorCategory&quot;:0,
   &quot;code&quot;:&quot;BlobAccessDenied&quot;,
   &quot;message&quot;:&quot;Access for one of the specified Azure Blob(s) is denied&quot;,
   &quot;details&quot;:[
      {
         &quot;Name&quot;:&quot;BlobSource&quot;,
         &quot;Value&quot;:&quot;https://XXXXXXXXXXXXXXXXX/testv2.exe?sv=2018-03-28&amp;sr=b&amp;sig=XXXXXXXXXXXXXXXXXX&amp;sp=r&quot;
      },
      {
         &quot;Name&quot;:&quot;FilePath&quot;,
         &quot;Value&quot;:&quot;D:\\batch\\tasks\\workitems\\XXXXXXXXXXX\\job-1\\XXXXXXXXXXXXXXXXXXXXXXXX\\testv2.exe&quot;
      }
   ]
}
</code></pre>
<p>Thank you for your help!</p>
","<azure><azure-data-factory><azure-batch><azure-security><custom-activity>","2022-11-21 10:25:21","107","0","1","74536210","<p>Solution found Azure community support :</p>
<p>Check Subnet information under Network Configuration from the Azure portal &gt; Batch Account &gt; Pool &gt; Properties. Take note and write the information down.</p>
<p>Navigate to the storage account, and select Networking. In the Firewalls and virtual networks setting, select Enable from selected virtual networks and IP addresses for Public network access. Add the Batch pool's subnet in the firewall allowlist.</p>
<p>If the subnet doesn't enable the service endpoint, when you select it, a notification will be displayed as follows:</p>
<p>The following networks don't have service endpoints enabled for 'Microsoft.Storage'. Enabling access will take up to 15 minutes to complete. After starting this operation, it is safe to leave and return later if you don't wish to wait.</p>
<p>Therefore, before you add the subnet, check it in the Batch virtual network to see if the service endpoint for the storage account is enabled.</p>
<p>After you complete the configurations above, the Batch nodes in the pool can access the storage account successfully.</p>
"
"74515475","byNames() and byName() in Azure DataFlow,How to use them in which scenarios?","<p>I want to use byName() and byNames() in my flow.But I am not getting exact usage of it like am not finding any examples / scenarios . The Examples given by Azure are not clear. Please help .</p>
","<azure><azure-data-factory>","2022-11-21 07:24:57","236","0","1","74517743","<ul>
<li><code>byName()</code> searches for a column in a given stream name (stream name is optional). Look at the following example. Let's say you have source data as in the following image:</li>
</ul>
<p><img src=""https://i.imgur.com/ENugtqQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have added another column called team with value as <code>og</code> using derived column transformation.</li>
</ul>
<p><img src=""https://i.imgur.com/Qjt6OZj.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now consider you want to populate a column based on whether it exists or not. If there is no column with given name, <code>null</code> values are returned. I used the following dynamic content in <code>derivedColumn1</code> transformation,</li>
</ul>
<pre><code> toString(byName('team'))
</code></pre>
<p><img src=""https://i.imgur.com/dG67XJv.png"" alt=""enter image description here"" /></p>
<ul>
<li>I get the following output:</li>
</ul>
<p><img src=""https://i.imgur.com/uGsOHU1.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Since we have given <code>team</code> as a column, it populated each row of <code>new</code> column with corresponding <code>team</code> column value. Instead of giving <code>team</code>, if we give any other column name that does not exist, then the entire <code>new</code> column will be populated with NULL values.</p>
</li>
<li><p><code>byNames()</code> is similar to <code>byName()</code>. Instead of giving a single column, you give an array of column names. The following dynamic content is an example where I have given array of column names, along with stream (optional argument).</p>
</li>
</ul>
<pre><code>array(byNames(['gname','team'],'source1'))
</code></pre>
<p><img src=""https://i.imgur.com/mk8V52n.png"" alt=""enter image description here"" /></p>
<ul>
<li>I got the following output:</li>
</ul>
<p><img src=""https://i.imgur.com/cfvcz5P.png"" alt=""enter image description here"" /></p>
<ul>
<li>Here I am searching for columns <code>gname and team</code> in the stream <code>source1</code> (which has only id and gname columns). Since we are looking to get a column where it does not exist, it has populated the <code>new</code> column with <code>NULL</code>. If all the given column names exist, the values are populated accordingly.</li>
</ul>
"
"74509365","How to copy file based on date in Azure Data Factory","<p>I have a list of files in a adls container which contain date in the name as given below:</p>
<pre><code>TestFile-Name-20221120. csv
TestFile-Name-20221119. csv
TestFile-Name-20221118. csv
</code></pre>
<p>and i want to copy files which contain today date only like TestFile-Name-20221120. csv on today and so on.
I've used get metedata activity to get list of files and then for each to iterate over each file and then used set variable to extract name from the file like 20221120 but not sure how to proceed further.</p>
","<azure-data-factory>","2022-11-20 15:19:48","126","0","2","74509800","<p>We have something similar running. We check an SFTP folder for the existanc e of files, using the <code>Get Metadata</code> activity. In our case, there can be folders or files. We only want to process files, and very specific ones for that matter (I.e. we have 1 pipeline per filename we can process, as the different filenames would contain different columns/datatypes etc).</p>
<p>Our pipeline looks like this:</p>
<p><a href=""https://i.stack.imgur.com/TBq1U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TBq1U.png"" alt=""enter image description here"" /></a></p>
<p>Within our <code>Get Metadata</code> component, we basically just filter for the name of the object we want, and we only want files ending in .zip, meaning we added a Filename filter:</p>
<p><a href=""https://i.stack.imgur.com/iWFGt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iWFGt.png"" alt=""enter image description here"" /></a>:</p>
<p>In your case, the first part would be 'TestFile-Name-', and the second part would be *.csv'.</p>
<p>We then have a <code>For Each</code> loop set up, to process anything (the child items) we retrieved in the <code>Get Metadata</code> step. Within the <code>For Each</code> we defined an <code>If Condition</code> to only process files, and not folders.</p>
<p>In our cases, we use the following expression:</p>
<pre><code>@equals(item().type, 'File')
</code></pre>
<p>In your case, you could use something like:</p>
<pre><code>@endsWith(item().name, concat(&lt;variable containing your date&gt;, '.csv'))
</code></pre>
"
"74509365","How to copy file based on date in Azure Data Factory","<p>I have a list of files in a adls container which contain date in the name as given below:</p>
<pre><code>TestFile-Name-20221120. csv
TestFile-Name-20221119. csv
TestFile-Name-20221118. csv
</code></pre>
<p>and i want to copy files which contain today date only like TestFile-Name-20221120. csv on today and so on.
I've used get metedata activity to get list of files and then for each to iterate over each file and then used set variable to extract name from the file like 20221120 but not sure how to proceed further.</p>
","<azure-data-factory>","2022-11-20 15:19:48","126","0","2","74513838","<p>Assuming all the file names start with TestFile-Name-,
and you want to copy the data of file with todays date,
use get metadata activity to check if the file exists and the file name can be dynamic like</p>
<p>@concat('TestFile-Name-',utcnow(),'.csv')</p>
<p>Note: you need to fromat utcnow as per the needed format</p>
<p>and if file exists, then proceed for copy else ignore</p>
"
"74506346","Rename the folder dynamically in Azure Data Factory pipeline","<p>how to rename folders when transferring from one container to anther container by using  dynamically in azure data factory</p>
<p>I want to modify the folder name by dynamically</p>
","<azure-data-factory>","2022-11-20 07:30:48","90","0","1","74568970","<p>I tried to dynamically rename the folder in ADF pipeline while copying. Below is the process.</p>
<ul>
<li><p>Lookup activity is taken with the dataset containing old_name and new_name of the folder.
<img src=""https://i.imgur.com/HbPAbk5.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In For-each, Copy data is taken. In source dataset, folder name is given dynamically.
<strong>Wildcard File Path:</strong> container2/<code>@{item().old_name}</code>/*
<img src=""https://i.imgur.com/jtxofG9.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In sink dataset, new folder name is given dynamically in another container.
<strong>File Path:</strong> container1/<code>@{item().new_name}</code>/
<img src=""https://i.imgur.com/PhyaJAr.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Files are copied to new container with new folder names.</p>
</li>
</ul>
"
"74503680","Insert JSON data into SQL DB using Airflow/python","<p>I extracted data from an API using Airflow.
The data is extracted from the API and saved on cloud storage in JSON format.</p>
<p>The next step is to insert the data into an SQL DB.
I have a few questions:</p>
<ul>
<li>Should I do it on Airflow or using another ETL like AWS Glue/Azure Data factory?</li>
<li>How to insert the data into the SQL DB? I google &quot;how to insert data into SQL DB using python&quot;?. I found a solution that loops all over JSON records and inserts the data 1 record at a time.
It is not very efficient. Any other way I can do it?</li>
<li>Any other recommendations and best practices on how to insert the JSON data into the SQL server?</li>
</ul>
<p>I haven't decided on a specific DB so far, so feel free to pick the one you think fits best.</p>
<p>thank you!</p>
","<json><azure><airflow><azure-data-factory><json-api-response-converter>","2022-11-19 20:56:15","239","0","2","74525807","<p>You can use Airflow just as a scheduler to run some python/bash scripts in defined time with some dependencies rules, but you can also take advantage of the operators and the hooks provided by Airflow community.</p>
<p>For the ETL part, Airflow isn't an ETL tool. If you need some ETL pipelines, you can run and manage them using Airlfow, but you need an ETL service/tool to create them (Spark, Athena, Glue, ...).</p>
<p>To insert data in the DB, you can create your own python/bash script and run it, or use the existing operators. You have some generic operators and hooks for postgress, MySQL and the different databases (<a href=""https://github.com/apache/airflow/tree/main/airflow/providers/mysql"" rel=""nofollow noreferrer"">MySQL</a>, <a href=""https://github.com/apache/airflow/tree/main/airflow/providers/postgres"" rel=""nofollow noreferrer"">postgres</a>, <a href=""https://github.com/apache/airflow/tree/main/airflow/providers/oracle"" rel=""nofollow noreferrer"">oracle</a>, <a href=""https://github.com/apache/airflow/tree/main/airflow/providers/microsoft/mssql"" rel=""nofollow noreferrer"">mssql</a>), and there are some other optimized operators and hooks for each cloud service (<a href=""https://github.com/apache/airflow/blob/main/airflow/providers/amazon/aws/operators/rds.py"" rel=""nofollow noreferrer"">AWS RDS</a>, <a href=""https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/cloud_sql.py"" rel=""nofollow noreferrer"">GCP Cloud SQL</a>, <a href=""https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/spanner.py"" rel=""nofollow noreferrer"">GCP Spanner</a>...), if you want to use one of the managed/serverless services, I recommend using its operators, and if you want to deploy your service on a VM or K8S cluster, you need to use the generic ones.</p>
<p>Airflow supports almost all the popular cloud services, so try to choose your cloud provider based on cost, performance, team knowledge and the other needs of your project, and you will surly find a good way to achieve your goal with Airlfow.</p>
"
"74503680","Insert JSON data into SQL DB using Airflow/python","<p>I extracted data from an API using Airflow.
The data is extracted from the API and saved on cloud storage in JSON format.</p>
<p>The next step is to insert the data into an SQL DB.
I have a few questions:</p>
<ul>
<li>Should I do it on Airflow or using another ETL like AWS Glue/Azure Data factory?</li>
<li>How to insert the data into the SQL DB? I google &quot;how to insert data into SQL DB using python&quot;?. I found a solution that loops all over JSON records and inserts the data 1 record at a time.
It is not very efficient. Any other way I can do it?</li>
<li>Any other recommendations and best practices on how to insert the JSON data into the SQL server?</li>
</ul>
<p>I haven't decided on a specific DB so far, so feel free to pick the one you think fits best.</p>
<p>thank you!</p>
","<json><azure><airflow><azure-data-factory><json-api-response-converter>","2022-11-19 20:56:15","239","0","2","74532606","<ul>
<li>You can use Azure Data Factory or Azure Synapse Analytics to move data in Json file to SQL server. Azure Data Factory supports 90+ connectors as of now. (Refer <strong>MS doc</strong> on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Connector overview - Azure Data Factory &amp; Azure Synapse</a> for more details about connectors that are supported by Data Factory).</li>
</ul>
<p><img src=""https://i.imgur.com/UNmbOO6.png"" alt=""img"" /></p>
<p>Img:1 Some connectors which are supported by ADF.</p>
<ul>
<li><p>Refer MS docs on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-cloud-storage?tabs=data-factory#prerequisites"" rel=""nofollow noreferrer"">pre-requisites</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-cloud-storage?tabs=data-factory#required-permissions"" rel=""nofollow noreferrer"">Required Permissions</a> to connect Google cloud storage with ADF</p>
</li>
<li><p>Take source connector as Google Cloud storage in copy activity. Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-cloud-storage?tabs=data-factory#prerequisites"" rel=""nofollow noreferrer"">Copy data from Google Cloud Storage - Azure Data Factory &amp; Azure Synapse | Microsoft Learn</a></p>
</li>
<li><p>Take SQL DB connector for sink.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/CyJDCBV.png"" alt=""enter image description here"" /></p>
<ul>
<li>ADF supports Auto create table option when there is no table created in Azure SQL database. Also, you can map the source and sink columns in mapping settings.</li>
</ul>
"
"74500181","Azure | ADF | Change value from dictionary saved in parameter (ADF)","<p>I have a global parameter in Azure Data Factory of object type.</p>
<p>The value could be something like</p>
<pre><code>{&quot;values&quot;:{&quot;key1&quot;:&quot;value1&quot;,&quot;key2&quot;:&quot;value2&quot;}}
</code></pre>
<p>I know that you can set the value of a global parameter using pipeline expressions.</p>
<p>Is there a way to change the value of one of the keys of the object and save changes back to the parameter using a pipeline?</p>
","<json><azure-data-factory>","2022-11-19 12:55:13","78","0","1","74514759","<ul>
<li>A global parameter is created with</li>
</ul>
<blockquote>
<p>Name: globalParameter<br />
Type: Object<br />
Value: {&quot;values&quot;:{&quot;key1&quot;:&quot;value1&quot;,&quot;key2&quot;:&quot;value2&quot;}}</p>
</blockquote>
<p><img src=""https://i.imgur.com/I0YmynY.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Here, I tried to change the value of key1 to <strong>value3</strong>.</p>
</li>
<li><p>In global parameters, <strong>Edit all</strong> is clicked.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/Y3Y2PTb.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Value of key1 is changed and then saved.
<img src=""https://i.imgur.com/Hub4RVF.png"" alt=""enter image description here"" /></p>
</li>
<li><p>By this way, values of the global parameter can be changed.</p>
</li>
</ul>
"
"74496893","Pipeline run check from different ADF","<p>We need to make sure that below scenarios should be working.</p>
<ul>
<li>same subscription with different Azure Data Factory</li>
<li>different subscription with different Azure Data Factory</li>
</ul>
<p>Please provide your pros and cons each statement.</p>
","<azure-data-factory>","2022-11-19 01:23:11","59","-1","1","74497468","<p>As long as all the subscription are within the same tenant, it won't matter whether the adf pipeline is in same subscription or not. The process to get the status would remain the same</p>
"
"74495075","Best way to store a ""Last execution date"" in Azure Data Factory","<p>Every day I run a pipeline that runs a Copy Data activity (DB2 =&gt; Parquet file).</p>
<p>How can I store the &quot;Last execution date&quot; of this activity?</p>
<p>Is there a best practice solution?</p>
<p>Because it would be unfortunate if we must do this in the old school way (store the date in a text file, or SQL Table, ...)</p>
<p>Thanks.</p>
","<azure-data-factory>","2022-11-18 20:37:34","368","0","2","74513903","<p>If you want to know as to when a ADF was triggered by a scheduled  trigger . You can use this API .</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/trigger-runs/query-by-factory?tabs=HTTP"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/trigger-runs/query-by-factory?tabs=HTTP</a></p>
<pre><code>{
  &quot;value&quot;: [
    {
      &quot;triggerName&quot;: &quot;exampleTrigger&quot;,
      &quot;triggerRunId&quot;: &quot;08586724970898148904457116912CU27&quot;,
      &quot;triggerType&quot;: &quot;ScheduleTrigger&quot;,
      &quot;triggerRunTimestamp&quot;: &quot;2018-06-16T00:43:15.660141Z&quot;,
      &quot;status&quot;: &quot;Succeeded&quot;,
      &quot;message&quot;: &quot;&quot;,
      &quot;properties&quot;: {
        &quot;TriggerTime&quot;: &quot;6/16/2018 12:43:15 AM&quot;,
        &quot;ScheduleTime&quot;: &quot;6/16/2018 12:43:14 AM&quot;
      },
      &quot;triggeredPipelines&quot;: {
        &quot;examplePipeline&quot;: &quot;9f3ce8b3-37d7-43eb-96ac-a656c0476283&quot;
      }
    }
  ]
</code></pre>
<p>}</p>
<p>and for Pipelines runs you can use the API</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipeline-runs/query-by-factory?tabs=HTTP"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipeline-runs/query-by-factory?tabs=HTTP</a></p>
<pre><code>{
  &quot;value&quot;: [
    {
      &quot;runId&quot;: &quot;2f7fdb90-5df1-4b8e-ac2f-064cfa58202b&quot;,
      &quot;pipelineName&quot;: &quot;examplePipeline&quot;,
      &quot;parameters&quot;: {
        &quot;OutputBlobNameList&quot;: &quot;[\&quot;exampleoutput.csv\&quot;]&quot;
      },
      &quot;invokedBy&quot;: {
        &quot;id&quot;: &quot;80a01654a9d34ad18b3fcac5d5d76b67&quot;,
        &quot;name&quot;: &quot;Manual&quot;
      },
      &quot;runStart&quot;: &quot;2018-06-16T00:37:44.6257014Z&quot;,
      &quot;runEnd&quot;: &quot;2018-06-16T00:38:12.7314495Z&quot;,
      &quot;durationInMs&quot;: 28105,
      &quot;status&quot;: &quot;Succeeded&quot;,
      &quot;message&quot;: &quot;&quot;,
      &quot;lastUpdated&quot;: &quot;2018-06-16T00:38:12.7314495Z&quot;,
      &quot;annotations&quot;: [],
      &quot;runDimension&quot;: {
        &quot;JobId&quot;: &quot;79c1cc52-265f-41a5-9553-be65e736fbd3&quot;
      }
    },
    {
      &quot;runId&quot;: &quot;16ac5348-ff82-4f95-a80d-638c1d47b721&quot;,
      &quot;pipelineName&quot;: &quot;examplePipeline&quot;,
      &quot;parameters&quot;: {
        &quot;OutputBlobNameList&quot;: &quot;[\&quot;exampleoutput.csv\&quot;]&quot;
      },
      &quot;invokedBy&quot;: {
        &quot;id&quot;: &quot;7c5fd7ef7e8a464b98b931cf15fcac66&quot;,
        &quot;name&quot;: &quot;Manual&quot;
      },
      &quot;runStart&quot;: &quot;2018-06-16T00:39:49.2745128Z&quot;,
      &quot;runEnd&quot;: null,
      &quot;durationInMs&quot;: null,
      &quot;status&quot;: &quot;Cancelled&quot;,
      &quot;message&quot;: &quot;&quot;,
      &quot;lastUpdated&quot;: &quot;2018-06-16T00:39:51.216097Z&quot;,
      &quot;annotations&quot;: [],
      &quot;runDimension&quot;: {
        &quot;JobId&quot;: &quot;84a3c493-0628-4b44-852f-ef5b3a11bdab&quot;
      }
    }
  ]
}
</code></pre>
"
"74495075","Best way to store a ""Last execution date"" in Azure Data Factory","<p>Every day I run a pipeline that runs a Copy Data activity (DB2 =&gt; Parquet file).</p>
<p>How can I store the &quot;Last execution date&quot; of this activity?</p>
<p>Is there a best practice solution?</p>
<p>Because it would be unfortunate if we must do this in the old school way (store the date in a text file, or SQL Table, ...)</p>
<p>Thanks.</p>
","<azure-data-factory>","2022-11-18 20:37:34","368","0","2","74515534","<blockquote>
<p>Best way to store a &quot;Last execution date&quot; in Azure Data Factory</p>
</blockquote>
<p>The SQL table Is the best way to store the <code>Last Execution Date</code> of activity or pipeline because <strong>In SQL table you can update the value as well as append the value</strong>.  But <strong>in csv file, text file you cannot append the value in it. It will just overwrite that value</strong>.</p>
<p>To store the value in SQL Table you can follow below procedure:
Simply you can use <code>script activity</code> after that <code>copy activity</code> with <strong>Insert query</strong> and <code>@utcnow()</code> function.</p>
<p><img src=""https://i.imgur.com/EyoBVc4.png"" alt=""enter image description here"" /></p>
<p><strong>OUTPUT</strong></p>
<p><img src=""https://i.imgur.com/rIcyiSO.png"" alt=""enter image description here"" /></p>
"
"74494102","ADF linked service connection string update","<p>I'm using ADF dataflow to process data from CosmosDB. Is there a way to auto update the connection key in linked service when the cosmosDB keys are rotated ?</p>
","<azure><azure-data-factory>","2022-11-18 18:44:36","57","0","2","74497487","<p>based on my understanding unfortunately it is not possible directly. What you can do is have a custom logic via azure automation or any tool to rotate the keys and push the keys to the key vault when rotated. And you can map the linked service in adf ti fetch from key vault rather than manually updating it</p>
"
"74494102","ADF linked service connection string update","<p>I'm using ADF dataflow to process data from CosmosDB. Is there a way to auto update the connection key in linked service when the cosmosDB keys are rotated ?</p>
","<azure><azure-data-factory>","2022-11-18 18:44:36","57","0","2","74500713","<p>I would recommend you consider authenticating using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db?tabs=data-factory#managed-identity"" rel=""nofollow noreferrer"">Data Factory Managed Service Identity</a>. This approach avoids having to manage any passwords or keys and is the preferred approach from a security best practice.</p>
"
"74491755","Maximum 5000 rows returned using OPENJSON in SQL Server?","<p>Using Azure Data Factory I have created a pipeline to upload any number of JSON files from Azure blob storage. I am loading the JSON data into a stage table with the following fields</p>
<ul>
<li>FileName varchar(200)</li>
<li>JSONData nvarchar(max)</li>
</ul>
<p>Using the SQL Server <code>OPENJSON</code> command, only 5,000 rows are returned, when there should be over 400,000 rows returned.</p>
<p>I used this query to parse the JSON data column into a final table (some info redacted):</p>
<pre><code> insert into jsonTable
     select * 
     from OPENJSON((select jsondata from rawJSONupload 
                    where filename = 'filename'))
              with (
                    field1 nvarchar(5),
                    field2 real,
                    field3 real,
                    EnteredDate datetime,
                    FilePath nvarchar(500)
                   )
</code></pre>
<p>My issue is that I am only getting 5,000 rows returned from the <code>SELECT * FROM OPENJSON(...)</code> query, where I know that I should have 400,000+ rows returned. Is there a max of 5,000 rows returned using <code>OPENJSON</code>, and if so is there a way around that, or do I need to find another way to load JSON data into a table (not one at a time).</p>
","<json><sql-server><import><azure-data-factory><open-json>","2022-11-18 15:14:37","186","0","1","74495888","<p>This is a limitation of Azure Data Factory, not SQL Server (OPENJSON work fine with big json). The upload is limited to 5000 thousand lines.
The limit is specified, for example, in this documentation - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#limitations-and-workarounds"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#limitations-and-workarounds</a> (section &quot;Limitations and workarounds&quot;).</p>
<p>You need to look for a solution in the corresponding tags, e.g:</p>
<p><a href=""https://stackoverflow.com/questions/55828405/handle-5000-rows-in-lookup-azure-data-factory"">Handle &gt;5000 rows in Lookup @ Azure Data Factory</a></p>
<p><a href=""https://stackoverflow.com/questions/60283222/handling-big-jsons-in-azure-data-factory"">Handling big JSONs in Azure Data Factory</a></p>
"
"74491059","Deploy ADF from prod to dev","<p>I currently have Azure Data Factory on Prod. It is already functional and has pipelines, Linked Services etc.</p>
<p>Now I want to deploy Develop, have built Deplyoment pipelines and am able to move Prod to Dev.</p>
<p>However, I need to make a few changes to make Dev work, e.g. I need to adjust parameters of Linked Service so that it accesses storage/KV on Dev and not on Prod.</p>
<p>But if I have developed later on Dev (e.g. new pipeline) and want to move these changes to Prod, of course the parameters of Linked Service are also deployed with PR on prod.</p>
<p>This means that I then have Linked Service on both Dev and Prod and both access Dev.</p>
<p>Example:</p>
<pre><code>PR Dev to Prod, wants to change - https://prdtestkv.vault.azure.net --&gt; https://devtestkv.vault.azure.net

Then Prod linked service KV is the same as on Dev --&gt; https://devtestkv.vault.azure.net
</code></pre>
<p>is it possible to exclude these changes during PR from Dev to Prod? So that I can only merge other changes? Thanks!</p>
","<git><azure-devops><azure-data-factory>","2022-11-18 14:20:51","95","0","2","74516392","<p>If you want to connect to different databases on the same logical SQL server, you can parameterize the database name in the linked service definition. You can parameterize a linked service and pass dynamic values at run time. Refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services?tabs=data-factory"" rel=""nofollow noreferrer"">document</a> for more information. Here are the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services?tabs=data-factory#supported-linked-service-types"" rel=""nofollow noreferrer"">Supported linked service types</a></p>
"
"74491059","Deploy ADF from prod to dev","<p>I currently have Azure Data Factory on Prod. It is already functional and has pipelines, Linked Services etc.</p>
<p>Now I want to deploy Develop, have built Deplyoment pipelines and am able to move Prod to Dev.</p>
<p>However, I need to make a few changes to make Dev work, e.g. I need to adjust parameters of Linked Service so that it accesses storage/KV on Dev and not on Prod.</p>
<p>But if I have developed later on Dev (e.g. new pipeline) and want to move these changes to Prod, of course the parameters of Linked Service are also deployed with PR on prod.</p>
<p>This means that I then have Linked Service on both Dev and Prod and both access Dev.</p>
<p>Example:</p>
<pre><code>PR Dev to Prod, wants to change - https://prdtestkv.vault.azure.net --&gt; https://devtestkv.vault.azure.net

Then Prod linked service KV is the same as on Dev --&gt; https://devtestkv.vault.azure.net
</code></pre>
<p>is it possible to exclude these changes during PR from Dev to Prod? So that I can only merge other changes? Thanks!</p>
","<git><azure-devops><azure-data-factory>","2022-11-18 14:20:51","95","0","2","74626455","<p>You can use powershell scripts to replace the strings while deploying to or from one env to another :</p>
<pre><code>(Get-Content -path $(System.DefaultWorkingDirectory)/_adfrepo/annu-analytics/TemplateForWorkspace.json) -replace 'https://prdtestkv.vault.azure.net','https://devtestkv.vault.azure.net'| Set-Content -Path $(System.DefaultWorkingDirectory)/_adfrepo/annu-analytics/TemplateForWorkspace.json 
</code></pre>
<p>Just change the order of prod and dev kv connection string while deploying from  dev to prod.</p>
"
"74490286","Azure Data Factory - Retry for an insert Copy Activity (AzureSQL DB)","<p>We’ve had twice intermittent issue of the copy activities running into</p>
<blockquote>
<p>A transport-level error has occurred when receiving results from the
server. (provider: TCP Provider, error: 0 - An existing connection was
forcibly closed by the remote host.) And on the next run, the issue is
not there anymore.</p>
</blockquote>
<p>For SQL, say if 100k records get batched into 10k records, will we end up with duplicate records if something happens in the middle of the copy activity? I believe the copy activity is not treated as a single DB transaction.</p>
<p>For UPSERT (copy activities) in SQL, <strong>we do have retry enabled</strong>, as the key columns will ensure no duplicates will be created. We’re wondering if we can also enable Retry for INSERT (copy activities).</p>
<p>In our other projects, we do have retry enabled for the copy activities for those involving Files (since as per link, files will just be picked up on the one that failed).
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#resume-from-last-failed-run"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#resume-from-last-failed-run</a></p>
<blockquote>
<p>Resume happens at file level. If copy activity fails when copying a
file, in next run, this specific file will be re-copied.</p>
</blockquote>
<p>Question is - will it be safe to enable RETRY for Copy Activites doing SQL Inserts (Azure SQL to another Azure SQL table)? Will it cause us to run into duplicate records when a transient error happens in the middle of the operation?</p>
","<azure><azure-sql-database><retry-logic><azure-data-factory>","2022-11-18 13:19:12","147","0","1","74493820","<p>Unfortunately copy activities in adf are not transaction bound and unless there is a pre script involved, copy activity would only append the data thereby creating duplicates. So ideally best way would be to copy it within a staging table and then leverage stored procedure activity to move into final table that would be bound within transaction</p>
"
"74489330","What is the industry standard Deduping method in Dataflows?","<p>So Deduping is one of the basic and imp Datacleaning technique.</p>
<p>There are a number of ways to do that in dataflow.</p>
<p>Like myself doing deduping with help of aggregate transformation where i put key columns(Consider &quot;Firstname&quot; and &quot;LastName&quot; as cols) which are need to be unique in Group by and a column pattern like name != 'Firstname' &amp;&amp; name!='LastName'
$$ _____first($$) in aggregate tab.</p>
<p>The problem with this method is ,if we have a total of 200 cols among 300 cols to be considered as Unique cols, Its a very tedious to do include 200 cols in my column Pattern.</p>
<p>Can anyone suggest a better and optimised Deduping process in Dataflow acc to the above situation?</p>
","<azure><azure-data-factory>","2022-11-18 11:57:48","82","0","1","74498058","<p>I tried to repro the deduplication process using dataflow. Below is the approach.</p>
<ul>
<li>List of columns that needs to be grouped by are given in dataflow parameters.
In this repro, three columns are given. This can be extended as per requirements.</li>
</ul>
<blockquote>
<pre><code>Parameter Name: Par1
Type: String
Default value: 'col1,col2,col3'
</code></pre>
</blockquote>
<p><img src=""https://i.imgur.com/3MZIAHg.png"" alt=""enter image description here"" /></p>
<ul>
<li>Source is taken as in below image.
(Group By columns: col1, col2, col3;
Aggregate column: col4)</li>
</ul>
<p><img src=""https://i.imgur.com/aDk4udx.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Then Aggregate transform is taken and in <strong>group by</strong>,
<code>sha2(256,byNames(split($Par1,',')))</code> is given in columns and it is named as <code>groupbycolumn</code>
<img src=""https://i.imgur.com/7hwWKCn.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In Aggregates, <strong>+ Add column pattern</strong> near column1 and then delete Column1. Then Enter <strong>true()</strong> in matching condition. Then click on undefined column expression and enter <code>$$</code> in <strong>column name expression</strong> and <code>first($$)</code> in <strong>value expression</strong>.
<img src=""https://user-images.githubusercontent.com/113445679/202837323-d4fd6f73-71b9-4ec0-8338-d45eb02eb1df.gif"" alt=""gif31"" /></p>
</li>
</ul>
<p><strong>Output of aggregation function</strong></p>
<p><img src=""https://i.imgur.com/MVQpsjc.png"" alt=""enter image description here"" /></p>
<p>Data is grouped by col1,col2 and col3 and first value of col4 is taken for every col1,col2 and col3 combination.</p>
<ul>
<li>Then using select transformation, <strong>groupbycolumn</strong> from above output can be removed before copying to sink.</li>
</ul>
<p>Reference:  ** MS document** on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-script#distinct-row-using-all-columns"" rel=""nofollow noreferrer"">Mapping data flow script - Azure Data Factory | Microsoft Learn</a></p>
"
"74487447","How to use the newly created column in Derived Column Transformation again in the same Derived column transformation?","<p>Suppose I created a column named 'Studata' in derived column where in expression is</p>
<p>right({someothercolname},4)</p>
<p>And in the same derived column , I created other column 'studata2' where in expression I wanted to give as left({Studata},1).
So this is throwing error because I cannot use the same column created in same derived column transformation.</p>
<p>So without an extra derived column ,can we achieve this within same derived column transformation?</p>
","<azure><azure-data-factory>","2022-11-18 09:25:59","275","0","1","74488169","<ul>
<li><code>Derived column transformation</code> fetches the schema on the incoming stream and only perform the expressions based on that column only.</li>
<li>The columns which you create in <code>Derived column transformation</code> are accessible in next transformation only not in same <code>Derived column transformation</code>.</li>
<li>I as it fetches the schema of previous stream only so it will throw <strong>error as column not found</strong>.</li>
</ul>
<p><img src=""https://i.imgur.com/1pCdOpQ.png"" alt=""enter image description here"" /></p>
<p>Work around is to use two <code>Derived column transformations</code> one after another as below:</p>
<p><img src=""https://i.imgur.com/Akj6N7v.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/6TPhP4t.png"" alt=""enter image description here"" /></p>
<p><strong>Output</strong></p>
<p><img src=""https://i.imgur.com/T1ZyITZ.png"" alt=""enter image description here"" /></p>
"
"74486349","how to check and compare the file names that are inside a folder (Datalake) using ADF","<p>My requirement is to compare the files names in the Datalake folder with the filenames in the .CSV File and if the filenames are matching then I want to copy these files and if filenames are not matching then I want to store these file names in a .CSV file in the datalake.</p>
<p>Kindly Help.</p>
","<azure><file><azure-data-factory>","2022-11-18 07:41:18","265","1","1","74498233","<p>You can achieve the requirement in the following way using 3 steps i.e., get filenames from csv file and ADLS folder, filter the matching and unmatched file names (from folder) and finally do respective copy operations.</p>
<p><strong>Step-1</strong>:</p>
<ul>
<li>I used <code>get metadata</code> activity to get the list of filenames from ADLS folder (<em>sample1.csv, sample2.csv, sample3.csv, sample4.csv</em>). Create dataset pointing to your folder and use <code>child items</code> as field list.</li>
</ul>
<p><img src=""https://i.imgur.com/mlX9dbZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>And <code>look up</code> to get the filenames (<em>sample1.csv, sample2.csv, sample5.csv, sample6.csv</em>) from the csv file.</li>
</ul>
<p><img src=""https://i.imgur.com/TApwwpk.png"" alt=""enter image description here"" /></p>
<br>
<p><strong>Step-2</strong></p>
<ul>
<li>Now using filter activity, get the matching file names. I used the following as my items and filter condition to get matching filenames:</li>
</ul>
<pre><code>items- @activity('list of files in folder').output.childItems
condition- @contains(string(activity('filenames present in csv').output.value),item().name)
</code></pre>
<p><img src=""https://i.imgur.com/zS3azkC.png"" alt=""enter image description here"" /></p>
<ul>
<li>To get the unmatched filenames from the ADLS folder, I used the following items and filter condition:</li>
</ul>
<pre><code>items- @activity('list of files in folder').output.childItems
condition- @not(contains(string(activity('filenames present in csv').output.value),item().name))
</code></pre>
<p><img src=""https://i.imgur.com/yx4zynF.png"" alt=""enter image description here"" /></p>
<br>
<p><strong>Step-3:</strong></p>
<ul>
<li><p>Now, use for each activity to copy each file to another location. I used the items value in 1st for each as <code>@activity('getting matching files').output.Value</code>. Inside this, I have configured a copy activity to copy the current for each item (i.e., filename).</p>
</li>
<li><p>I have created a parameter in <a href=""https://i.imgur.com/jDNbMx6.png"" rel=""nofollow noreferrer"">dataset</a> called <code>filename</code>. I passed its value (@item().name) from copy data source settings as shown below.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/k3sD9k3.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now for unmatched filenames from folder, I used for each and append variable to create an array of filenames like <code>[&quot;sample3.csv&quot;, &quot;sample4.csv&quot;]</code>. The items value in for each is <code>@activity('getting unmatched files').output.Value</code>.</li>
<li>Inside for each, I used <code>append variable</code> with value as <code>@item().name</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/g9jj2VH.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, we have to create a new csv file with all the unmatched filenames from folder. Use copy data activity, take a <a href=""https://i.imgur.com/9BM87wX.png"" rel=""nofollow noreferrer"">sample csv file</a> (with some content. This content doesnot matter, we just need a file to use as source).</li>
<li>Now add an additional column, called filenames with dynamic content value as below. (Make sure in pipeline JSON the filenames value is same as in <a href=""https://i.imgur.com/D0DQLWo.png"" rel=""nofollow noreferrer"">this reference image</a> )</li>
</ul>
<pre><code>@join(variables('filenames'),'
')

#the values will be joined using newline(\n). 
#Using \n directly in dynamic content would not work as it will be taken as \\n. 
#So change it in pipeline json as in above reference image.
</code></pre>
<p><img src=""https://i.imgur.com/ObH2fHO.png"" alt=""enter image description here"" /></p>
<ul>
<li>Configure the sink as following. Select the path, filename and make sure to select quote character as <code>no quote character</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/SBfeAr7.png"" alt=""enter image description here"" /></p>
<br>
<p><strong>Output:</strong></p>
<ul>
<li>When I run the pipeline, it would generate the required output. The matched and copied files are generated as below:</li>
</ul>
<p><img src=""https://i.imgur.com/n0m57kz.png"" alt=""enter image description here"" /></p>
<ul>
<li>The unmatched filenames written to csv files will be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/zEk2f3J.png"" alt=""enter image description here"" /></p>
"
"74485662","set up dependencies between 2 separate ADF environments","<p>I want to know, in order to set up dependencies between 2 separate ADF environments.
I know this is possible through setting a trigger/web activity.</p>
<p>Is there is any point of failures in the scheduling the inter ADF pipelines? We need to be 100% sure for this solution.</p>
","<azure-data-factory>","2022-11-18 06:27:30","74","-1","1","74487146","<p>There are multiple ways to set up dependencies:</p>
<ol>
<li>You can trigger an ADF pipeline via web activity thereby linking those 2 ADFs</li>
<li>You can generate a file in blob at the end of 1st pipeline and configure event trigger for another pipeline</li>
</ol>
"
"74485330","How to check if files present under a data lake directory are empty using Azure Data Factory?","<p>How to check if files present under a data lake directory are empty using Azure Data Factory?</p>
<p>There are multiple files present in the data lake directory and I want to check if these files are empty or not, If files are empty then I want to store the filenames of these empty files in a CSV File.</p>
","<azure><filesize><azure-data-factory>","2022-11-18 05:36:15","132","0","2","74487207","<p>If you want to check whether the file is empty or not Please follow the below Steps. I tried to reproduce the same in my environment and I got the below results:</p>
<p>In my storage account, I have two files one is <code>demo123.csv</code> empty and another one is a <code>vm_name3.csv</code> non-empty file.</p>
<p><img src=""https://i.imgur.com/ThbdWoh.png"" alt=""enter image description here"" /></p>
<p><strong>Please follow these Steps:</strong></p>
<p><strong>Step1:</strong> First Create Get metadata with child items</p>
<p><img src=""https://i.imgur.com/GSoIjhF.png"" alt=""enter image description here"" /></p>
<p><strong>Step 2:</strong> Add dynamic expression <code>@activity('Get Metadata1').output.childItems</code> on the <strong>forEach activity</strong>.</p>
<p><img src=""https://i.imgur.com/tJJQr2W.png"" alt=""enter image description here"" /></p>
<ul>
<li>Create a dataset parameter<br />
<img src=""https://i.imgur.com/8MYpvAQ.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Step3 :</strong> Inside <strong>forEach activity</strong> -&gt;use lookup and If condition. Add this dynamic expression on <strong>lookup activity</strong> : <code>@item().name</code></p>
<blockquote>
<p><strong>Lookup</strong></p>
</blockquote>
<p><img src=""https://i.imgur.com/Ipgsfyt.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>If condition:</strong></p>
</blockquote>
<p>Using this dynamic expression <code>@equals(activity('Lookup1').output. count,0)</code>.You will know whether a file is empty or not.</p>
<p><img src=""https://i.imgur.com/oaibttS.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Then add append variable inside True conduction.<br />
<img src=""https://i.imgur.com/lwIPEcY.png"" alt=""enter image description here"" /></p>
</li>
<li><p>I created two variables with array type.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/bMBZh7l.png"" alt=""enter image description here"" /></p>
<ul>
<li>Add this dynamic expression: <code>@item().name</code> on append variable<br />
<img src=""https://i.imgur.com/PHCILCE.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Step4:</strong></p>
<p>Add set variable to the forEach activity:<br />
<img src=""https://i.imgur.com/v3v15v8.png"" alt=""enter image description here"" /></p>
<p>After successful execution of pipeline. I got empty file name:</p>
<p><img src=""https://i.imgur.com/sVtFy0z.png"" alt=""enter image description here"" /></p>
<p>If you want to store the filenames of these empty files in a CSV File .Then follow this <a href=""https://stackoverflow.com/questions/74471581/how-to-copy-the-data-from-append-variable-activity-to-a-csv-file-using-azure-dat/74475906#74475906"">SO</a> thread by Aswin</p>
"
"74485330","How to check if files present under a data lake directory are empty using Azure Data Factory?","<p>How to check if files present under a data lake directory are empty using Azure Data Factory?</p>
<p>There are multiple files present in the data lake directory and I want to check if these files are empty or not, If files are empty then I want to store the filenames of these empty files in a CSV File.</p>
","<azure><filesize><azure-data-factory>","2022-11-18 05:36:15","132","0","2","74528892","<p>I have followed this thread <a href=""https://stackoverflow.com/questions/65152580/azure-data-factory-v2-check-file-size-for-copy-activity"">Azure Data Factory V2 Check file size for Copy Activity</a> and applied if condition to judge the file size and append variable and set variable to store the file names, Worked for me.</p>
"
"74484813","Azure Databricks: Unexpected failure while waiting for the cluster to be ready. Cause Cluster is unusable since the driver is unhealthy","<p>I have some scheduled data pipelines that are orchestrated via Azure Data Factory, each with a Databricks activity that runs on a job cluster.</p>
<p>All my Databricks activities are stuck in retry loops and failing with the following error,</p>
<pre><code>Databricks execution failed with error state: InternalError, error message: Unexpected failure while waiting for the cluster &lt;cluster-id&gt; to be ready.Cause Cluster &lt;cluster-id&gt; is unusable since the driver is unhealthy.
</code></pre>
<p>My Databricks cluster is not even starting up.</p>
<p>This issue is quite similar to what has been posted here,
<br>
<a href=""https://stackoverflow.com/questions/71941989/aws-databricks-cluster-start-failure"">AWS Databricks cluster start failure</a></p>
<p>However, there are a few differences,</p>
<ol>
<li>My pipelines are running on Azure: Azure Data Factory and Azure
Databricks</li>
<li>I can spin up my interactive clusters (in the same workspace)
without any problem</li>
<li>I have checked with my colleagues who are running similar pipelines
on different subscriptions (in the same region), but they are not
facing any issue</li>
</ol>
<p>Any idea what is going on here? Is it just a service interruption of sorts or is there something I can do resolve this?</p>
","<azure><azure-data-factory><databricks><azure-databricks>","2022-11-18 04:02:55","992","0","1","74497288","<p>It turns out that my pipelines were failing because the init script that has been configured for our clusters is not executing correctly.</p>
<p>We have a in-built Python package that we maintain in Azure Artifacts. To install this package, we need to use a DevOps token. To install the package in our clusters, a command is available in the init script and because the token has expired, the init script was failing.</p>
<p>As a result, the cluster could not start up properly. The error message is quite cryptic though. &quot;Cause Cluster is unusable since the driver is unhealthy&quot; could literally mean anything.</p>
<p>However, if you come across this yourselves, check your init script.</p>
<p>Note: Another hint here was that when we looked through the Event log, we noticed that the time between the events <code>INIT_SCRIPTS_STARTED</code> and <code>INIT_SCRIPTS_FINISHED</code> was very long. More so than it should actually take.</p>
"
"74479675","Azure synapse analytics- synapse link authentication","<p>The synapse link for Dataverse is running fine when the storage account access key is disabled. We can able to create new records, there is no problem here.
But it fails to set up a new synapse link for Dataverse when the storage account key is disabled. Has anyone seen this issue before?</p>
<p>expecting synapse link to work when the storage account access key is disabled</p>
","<azure><azure-data-factory><azure-synapse>","2022-11-17 17:27:13","88","1","1","74481340","<p>As per my analysis looks like the Storage account key access should be enabled at the time of Synapse link creation and once it is created successfully, then you can disable the storage access key and the behavior should be similar to that your existing Synapse link service.</p>
"
"74473382","Azure Data factory expression","<p>i saw this expression in one of the Pipeine , in the filter activity condition. can anyone help me understad this expresson used( you can refrase inorder to make it understandable). looks difficult to understand.</p>
<pre><code>@if(equals(pipeline().parameters.FileName,'default'),endswith(toUpper(item().name),'.PDF'),
and(startswith(item().name,replace(string(pipeline().parameters.Filemane),'*.txt','')),
endswith(toUpper(item().name),'.PGP')))
</code></pre>
<p>Thanks</p>
<p>I  dont have blocker , but i cannot understand the Expression. Just wanted to get some clarity what is the purpose of that code , what are they trying to achieve in that particualar filter condition in the ADF</p>
","<azure-data-factory><etl><common-expression-language>","2022-11-17 09:54:28","356","0","2","74473740","<p>Well I am not sure if there is a difference between the first code before '<strong>Thanks</strong>' and the code after it.</p>
<p>But it seems that the code will return true or false.
I can reformulate to this in c# like format:</p>
<pre class=""lang-java prettyprint-override""><code>
if(pipeline().parameters.FileName == 'default')
{
    // if the name of item ends with .pdf then return true, else return false
    return endswith(toUpper(item().name),'.PDF'), 
}
else
{
   // replace the *.txt with an empty string. 
   // I think it means if the file ends with .txt then replace it with an empty string
   string replacedText = replace(pipeline().parameters.Filemane,'*.txt','')
   // check if the itemName is ends with .txt (in this case this condition will fail)
   boolean cond1 = startswith(item().name, replacedText)
   
   // if the name of item ends with .PGP then cond2 = true
   boolean cond2 = endswith(toUpper(item().name),'.PGP')
   
   return cond1 &amp;&amp; cond2 
}
</code></pre>
"
"74473382","Azure Data factory expression","<p>i saw this expression in one of the Pipeine , in the filter activity condition. can anyone help me understad this expresson used( you can refrase inorder to make it understandable). looks difficult to understand.</p>
<pre><code>@if(equals(pipeline().parameters.FileName,'default'),endswith(toUpper(item().name),'.PDF'),
and(startswith(item().name,replace(string(pipeline().parameters.Filemane),'*.txt','')),
endswith(toUpper(item().name),'.PGP')))
</code></pre>
<p>Thanks</p>
<p>I  dont have blocker , but i cannot understand the Expression. Just wanted to get some clarity what is the purpose of that code , what are they trying to achieve in that particualar filter condition in the ADF</p>
","<azure-data-factory><etl><common-expression-language>","2022-11-17 09:54:28","356","0","2","74475958","<p>If you use the above expression in the filter activity. It will filter the values of the array based on the <code>FileName</code> parameter.</p>
<p>This is my sample array of file names:</p>
<pre><code>['rakesh.pdf','correct1.pgp','wrong1.pgp','laddu.pdf','correct2.pgp','wrong2.pgp','virat.pdf','wrong3.pgp','correct3.txt']
</code></pre>
<p>For you the array will be an array of objects. that's why in the above expression it's there as <code>item().name</code>. For me it's only <code>item()</code>.</p>
<p>Filter activity filters based on the condition. <strong>If the particular array item satisfies the condition(true/false) then it filters it.</strong></p>
<pre><code> @if(equals(pipeline().parameters.FileName,'default'),endswith(toUpper(item().name),'.PDF'), and(startswith(item().name,replace(string(pipeline().parameters.Filemane),'*.txt','')), endswith(toUpper(item().name),'.PGP')))
</code></pre>
<p>In the above expression, If the <code>FileName</code> parameter value is <code>'default'</code>, it filters the items which are ending with <code>.PDF</code>. <code>if(condition,True case,else case)</code> so in <strong>true case</strong> we are checking <code>endswith()</code> <code>.PDF</code> or not. If it is true, then filter condition is true and that particular array value will be filtered.</p>
<p><strong>else case</strong></p>
<pre><code>and(startswith(item().name,replace(string(pipeline().parameters.Filemane),'*.txt','')), endswith(toUpper(item().name),'.PGP'))
</code></pre>
<p>If the parameter value is <code>correct*.txt</code>(other than default).
In this case it replaces the <code>'*.txt'</code> with empty strings('') and returns <code>'correct'</code>. Then it checks(<strong>first condition</strong>) the whether the array value starts with <code>'correct'</code> and endswith <code>.PGP</code>(<strong>second condition</strong>). If both conditions are true, then filter condition is true and array value is filtered.</p>
<p><strong>This is a sample demo:</strong></p>
<p><strong>Array:</strong></p>
<p><img src=""https://i.imgur.com/NH3yVOB.png"" alt=""enter image description here"" /></p>
<p><strong>Filter with same condition:</strong></p>
<p>for me it's only <code>item()</code> not <code>item().name</code>.</p>
<p><img src=""https://i.imgur.com/7UVc6FX.png"" alt=""enter image description here"" /></p>
<p>If I take the parameter value as <code>'default'</code>.</p>
<p><img src=""https://i.imgur.com/glfhkJI.png"" alt=""enter image description here"" /></p>
<p><strong>Filter output array (<code>all .PDF files</code>):</strong></p>
<p><img src=""https://i.imgur.com/z3HR6jI.png"" alt=""enter image description here"" /></p>
<p>If I take the parameter value as <code>'correct*.txt'</code>.</p>
<p><img src=""https://i.imgur.com/JbVbcxa.png"" alt=""enter image description here"" /></p>
<p><strong>Filter output array(<code>.PGP files starts with 'correct'</code>):</strong></p>
<p><img src=""https://i.imgur.com/zD6kaS4.png"" alt=""enter image description here"" /></p>
<p><strong>NOTE:</strong> If your parameter value is only like <code>'correct.txt'</code> then use <code>'.txt'</code> in the expression.</p>
"
"74471963","Azure Data Factory Blob Event triggering multiple times for a single small file","<p>I have created an ADF blob event based trigger for a pipeline. The file that I am testing with is around 931 kb. When I upload the file, ADF triggers the pipeline three times. Is there some configuration that I am missing here? Any help would be great. Thanks.</p>
","<azure><azure-data-factory>","2022-11-17 07:59:34","218","1","1","74572925","<p>I tried to reproduce the scenario, but it is working fine for me. <strong>For 4 File uploads it is triggering pipeline 4 times</strong></p>
<p><img src=""https://i.imgur.com/PRBH7Xf.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/Jf9wj3s.png"" alt=""enter image description here"" /></p>
<p>Possibly, the way the file is written to storage causes this in your situation. After the file is created with a size of 0, it is updated with its real size. If you choose to disregard the size zero updates.</p>
<p><img src=""https://i.imgur.com/1tjjHLO.png"" alt=""enter image description here"" /></p>
<p>Please choose yes for the &quot;Ignore empty blobs&quot; function. The user may regulate how files are uploaded to blogs; ADF does not have any restrictions over this.</p>
"
"74471581","How to copy the data from Append variable activity to a csv file using Azure Data Factory","<p>How to copy the data from append variable activity to a csv file using Azure Data Factory</p>
<p>I have array of file names stored in append variable activity. I want to store all these files names inside a .CSV file in the data lake location.</p>
<p>For more info refer this
<a href=""https://stackoverflow.com/questions/74432836/how-to-compare-the-file-names-that-are-inside-a-folder-datalake-using-adf"">how to compare the file names that are inside a folder (Datalake) using ADF</a></p>
","<azure><etl><azure-data-factory>","2022-11-17 07:23:12","465","0","1","74475906","<ul>
<li>In this repro, Variable V1 (array type) is taken with values as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/ExhuSWh.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>New variable v2 of string type is taken and value is given as <code>@join(variables('V1'),decodeUriComponent('%0A'))</code>
This step is done to join all the strings of the array using <code>\n (line feed)</code>.
<img src=""https://i.imgur.com/MGZRIiJ.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then Copy activity is taken and dummy source dataset with one row is taken.
<img src=""https://i.imgur.com/ELnLT2Q.png"" alt=""enter image description here"" /></p>
</li>
<li><p>In Source, <strong>+New</strong>  is selected and value is given as dynamic content <code>@varaiables('v2')</code>.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/m2SKJRL.png"" alt=""enter image description here"" /></p>
<ul>
<li>Sink dataset is created for CSV file.</li>
<li>In Mapping, import schemas is clicked and other than <code>col1</code> , all other columns are deleted.</li>
</ul>
<p><img src=""https://i.imgur.com/aZPtdQp.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then pipeline is debugged, and values got loaded in csv file.
<img src=""https://i.imgur.com/4GWtOOe.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Edited</strong></p>
<ul>
<li>Variable v2 is storing all the missing file names. (False activity of IF condition)</li>
<li>After for-each, Set variable is added and variable <strong>v3</strong> (string type) is set as
<code>@join(variables('v2'),decodeUriComponent('%0A'))</code></li>
</ul>
<p><img src=""https://i.imgur.com/ix1YUOW.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then, in  copy activity, +New column is added in source
<img src=""https://i.imgur.com/4hThrAB.png"" alt=""enter image description here"" /></li>
</ul>
"
"74471522","Add created on while copying data from SQL to azure data lake gen 2","<p>I want to copy the data to SQL from csv file in ADLS gen2. In Sql table, there is a column called <code>created on</code>. But csv file doesn't have that column. How can I copy the current date in created on along with other columns?</p>
","<sql><azure><azure-data-factory>","2022-11-17 07:18:47","60","0","1","74472873","<ol>
<li>You can add a column in source settings of copy activity and give the dynamic value as <code>@utcnow()</code>
or</li>
<li>Add a derived column transformation in dataflow and add the new column and give the data as <code>currentUTC()</code></li>
</ol>
<p><strong>Method:1</strong> [Using Copy Activity]</p>
<ul>
<li>Copy activity is taken and in source settings, source dataset is taken.</li>
</ul>
<p><img src=""https://i.imgur.com/YNLD9Wj.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then Additional columns <strong>+New</strong> is clicked. Created_on is given in Name and <code>@utcnow()</code> is given as dynamic content.</li>
</ul>
<p><img src=""https://i.imgur.com/3d2yLOP.png"" alt=""enter image description here"" /></p>
<ul>
<li>After adding new column, <strong>preview data of source dataset</strong> looks as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/n8aWCME.png"" alt=""enter image description here"" /></p>
<ul>
<li>After this, file can be copied to sink.</li>
</ul>
<p><strong>Method:2</strong> [Using Dataflow]</p>
<ul>
<li><p>Source data is taken as in below image in dataflow.
<img src=""https://i.imgur.com/kEpxgal.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Derived column transformation is added and <strong>+ADD</strong> is selected in columns.  <code>currentUTC()</code> is given in the expression.
<img src=""https://i.imgur.com/Tt56rFF.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><img src=""https://i.imgur.com/7Q4xND0.png"" alt=""enter image description here"" /></p>
<ul>
<li>By this way, you can add the column dynamically whenever the data is copied to SQL.</li>
</ul>
"
"74470999","Exception Handling for Copy Activity in Azure Data Factory","<p>I was using the copy activity for updating rows to azure table storage. Currently the pipeline fails if there are any errors in updating any of the rows/batches.</p>
<p>Is there a way to gracefully handle the failed rows and continue with the copy activity for the rest of the data?</p>
<p>I already tried the Fault Tolerance option which the copy activity provides but that does not solve this case.</p>
<p><a href=""https://i.stack.imgur.com/kvQnz.png"" rel=""nofollow noreferrer"">FaultTolerance Page</a></p>
","<azure><exception><azure-data-factory>","2022-11-17 06:22:37","301","0","1","74486097","<ul>
<li>I have repro'd the same and got the same error when mapping the column containing special character data to RowKey column in Table Storage.</li>
</ul>
<p><strong>Source dataset</strong>
<img src=""https://i.imgur.com/CYRUqGc.png"" alt=""enter image description here"" /></p>
<p><strong>Fault tolerance settings</strong>
<img src=""https://i.imgur.com/NOMsfmI.png"" alt=""enter image description here"" /></p>
<p><strong>Error Message:</strong></p>
<p><img src=""https://i.imgur.com/p4QNnUZ.png"" alt=""enter image description here"" /></p>
<p>In copy activity, it is not possible to skip the incompatible rows other than using fault tolerance. Workaround is to use <strong>dataflow activity</strong> and separate the compatible rows and incompatible rows and then copy the compatible data using copy activity. Below is the approach.</p>
<ul>
<li>Source is taken as in below image.</li>
</ul>
<p><img src=""https://i.imgur.com/meD2bWO.png"" alt=""enter image description here"" /></p>
<ul>
<li>Since <code>col4</code> needs to be checked before loading to Table storage, Condition is given on col4 data using condition activity. <strong>Conditional split Transformation</strong> is added after source transformation. Condition is given as,
<strong>FalseStream</strong> :
<code>like(col4,'%#%')||like(col4,'%$%')||like(col4,'%/%')||like(col4,'%\\%')</code></li>
</ul>
<p>**Sample characters are given in the above condition. **</p>
<p><strong>True Stream</strong> will be the rows which do not match the above condition.
<img src=""https://i.imgur.com/YbiPPKO.png"" alt=""enter image description here"" /></p>
<ul>
<li>Both False and true Streams are added to Sink1 and sink2 respectively to copy the data to blob storage.
<strong>Output</strong>:
False Stream:
<img src=""https://i.imgur.com/IBsMh3Y.png"" alt=""enter image description here"" /></li>
</ul>
<p>True Stream Data:</p>
<p><img src=""https://i.imgur.com/LPEfREy.png"" alt=""enter image description here"" /></p>
<ul>
<li>Once compatible data are copied to Blob, they can be copied to Table Storage using copy activity.</li>
</ul>
"
"74470345","How to increase the Until Activity timeout to more than 7 days in Azure Data factory?","<p>How to increase the Until Activity time to more than 7 days in Azure Data factory<strong>strong text</strong></p>
","<azure><azure-data-factory>","2022-11-17 04:49:26","88","0","2","74470381","<p><a href=""https://i.stack.imgur.com/iKR9D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iKR9D.png"" alt=""enter image description here"" /></a></p>
<p>you cannot make it more than 7 days.</p>
"
"74470345","How to increase the Until Activity timeout to more than 7 days in Azure Data factory?","<p>How to increase the Until Activity time to more than 7 days in Azure Data factory<strong>strong text</strong></p>
","<azure><azure-data-factory>","2022-11-17 04:49:26","88","0","2","74485059","<p>I will be very interersted to know the use case that you need an untill with 7 days timeout , please elaborate .
I could have explored a foreach loop with a wait command .I suggest using the range function .</p>
<pre><code>@range(1, maxNum)
</code></pre>
"
"74467972","Parameterize Managed Private Endpoint in Blob Linked Service ADF","<p>I need to use multiple Blob Storage Accounts in ADF. I am trying to create a single linked service for all storages with parameters. I unable to parameterized managed private endpoint. When I hardcode storage name then managed private endpoint (which has been created in ADF) gets selected automatically. Is there a way to parameterize it through Advance-&gt;JSON OR by any other way? ]<a href=""https://i.stack.imgur.com/yf0Uj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yf0Uj.jpg"" alt=""Blob Linked Service"" /></a></p>
<p>Unable to parameterize managed private endpoint. Did not find any Microsoft documentation.</p>
","<azure><azure-blob-storage><azure-data-factory><linked-service>","2022-11-16 22:22:48","211","0","1","74497458","<p>I created Azure data factory and storage account in azure portal. I setup the Integration Runtime as mention below</p>
<p><img src=""https://i.imgur.com/PZ7pr3A.png"" alt=""enter image description here"" /></p>
<p>I created managed private endpoint in Azure Data factory.
Manage-&gt;Security-&gt;managed private endpoint
Image for reference:</p>
<p><img src=""https://i.imgur.com/sseixyD.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/3NX1oUs.png"" alt=""enter image description here"" /></p>
<p>After creation of managed private endpoint we need to approve in storage account settings. for that I click on
Manage approval in azure portal which mentioned above. It takes me to below page</p>
<p><img src=""https://i.imgur.com/VrBH3qb.png"" alt=""enter image description here"" /></p>
<p>select private end point and click on approve which mentioned below.
It approves the private endpoint.</p>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/APpXLSb.png"" alt=""enter image description here"" /></p>
<p>Managed private endpoint is created and approved successfully.</p>
<p>we can achieve Parameterize Managed Private Endpoint in Blob Linked Service ADF using below Json script</p>
<pre><code>{
    &quot;name&quot;: &quot;DataLakeBlob&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;parameters&quot;: {
            &quot;StorageAccountEndpoint&quot;: {
                &quot;type&quot;: &quot;String&quot;,
                &quot;defaultValue&quot;: &quot;https://&lt;storage AccountName&gt;.blob.core.windows.net&quot;
            }
        },
        &quot;type&quot;: &quot;AzureBlobStorage&quot;,
        &quot;typeProperties&quot;: {
            &quot;sasUri&quot;: &quot;@{linkedService().StorageAccountEndpoint}&quot;
        },
        &quot;description&quot;: &quot;Test Description&quot;
    }
}
</code></pre>
<p>Mark the Specify dynamic contents in JSON format when test connection it connected successfully.</p>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/LNNYhRK.png"" alt=""enter image description here"" /></p>
<p>This works from my end please check from your end.</p>
"
"74466093","How to create a key-value map with pipeline expression (not data flow expression) in Azure Data Factory (Synapse Studio)","<p>I have two arrays that have respectively contain keys and values:
<strong>array 1</strong> <code>[&quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;]</code> and <strong>array 2</strong> <code>[&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;]</code></p>
<p>With ADF data flow expression, I can construct a key-value map with these two arrays using <code>keyValues</code> function:</p>
<pre><code>keyValues([&quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;],[&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;])
</code></pre>
<p>And this will return a result like this:</p>
<pre><code>[&quot;key1&quot; -&gt; &quot;value1&quot;, &quot;key2&quot; -&gt; &quot;value2&quot;, &quot;key3&quot; -&gt; &quot;value3&quot;]
</code></pre>
<p>I want to construct the same key-value map within a pipeline, not data flow, but the <code>keyValues</code> function is not available in pipeline expression. How can I construct a key-value map with pipeline expression on Azure Data Factory / Synapse Studio?</p>
","<azure><azure-synapse><azure-data-factory>","2022-11-16 19:24:06","647","1","1","74470959","<p>There is no direct way to create a key value pair using pipeline expression builder as in dataflows. The following is one to build the required key value pairs in ADF pipelines:</p>
<ul>
<li>I have taken 2 parameters with the following values:</li>
</ul>
<p><img src=""https://i.imgur.com/ZjecqJY.png"" alt=""enter image description here"" /></p>
<ul>
<li>Let's say <code>req</code> is the variable in which we would like to store our final key value pair. I have initialized it with the value <code>{}</code></li>
</ul>
<p><img src=""https://i.imgur.com/dYWJRKx.png"" alt=""enter image description here"" /></p>
<ul>
<li>In <code>for each</code>, I have used items value as <code>@range(0,length(pipeline().parameters.keys))</code> to generate index.</li>
</ul>
<p><img src=""https://i.imgur.com/kjPjTwQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have taken a variable called <code>temp</code> to apply union on the current value of <code>req</code> and the dynamically built key-value pair for current iteration.</li>
</ul>
<pre><code>@string(union(json(variables('req')),json(concat('{&quot;',pipeline().parameters.keys[item()],'&quot;:&quot;',pipeline().parameters.values[item()],'&quot;}'))))
</code></pre>
<p><img src=""https://i.imgur.com/KxPqDvR.png"" alt=""enter image description here"" /></p>
<ul>
<li>Finally, I am updating the value of <code>req</code> variable for each iteration with the current <code>temp</code> value i.e., <code>@variables('temp')</code>:</li>
</ul>
<p><img src=""https://i.imgur.com/J8IFXRs.png"" alt=""enter image description here"" /></p>
<ul>
<li>After running the pipeline, it would generate the following output in req variable:
<img src=""https://i.imgur.com/3Zp7UjF.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>NOTE:</strong> Object type variables are not supported in ADF pipelines. Whenever you want to use this object (stored as string), you can convert it to an object type using <code>@json()</code> function</p>
"
"74463268","How to convert from .NET Framework datetime type in synapse pipeline?","<p>I am developing a pipeline in Azure Synapse to convert a <code>jason</code> file downloaded via api call into a parquet.
The column names are <code>lastModDate</code> and <code>lastModifiedDate</code> and I want to store these in the sink as Datetime or <code>DatetimeOffset</code> type, but I got an error. I think the datetime format is type of .NET Framework, and want to convert it to format like <code>yyyy-MM-ddTHH:mm:dd</code>. The data is converted correctly in the data preview, but what should I do in the copy activity?</p>
","<azure><azure-data-factory>","2022-11-16 15:37:27","55","0","1","74468083","<p>If your requirement is to transform the data before copying to destination data store, the ideal way is to use a mapping data flow activity and use derive column transformation to convert the source data as per your sink data store requirement.</p>
<p>Copy activity is only for data movement between data stores and not to transform the data.</p>
<p>For more info, please refer to this document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived column transformation in mapping data flow</a></p>
"
"74461886","How do I send the output of ""Lookup Activity"" by email","<p>I'm trying to send the output of lookup activity by email with a web activity but I'm not entirely sure how exactly I should create the pipeline.</p>
<p><a href=""https://i.stack.imgur.com/Via3D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Via3D.png"" alt=""copy data - Azure SQL Database -&gt; Azure SQL Database
Lookup - select count(*) NULL values from specific column
ForEach - Settings -&gt; Items -&gt; @activity('Lookup1').output.value
Inside ForEach - Set variable and Web activity"" /></a></p>
<p>The idea of ​​the pipeline is to check how many rows with null values ​​are in the concrete column and send the result by email.</p>
<p>The error I'm getting is:</p>
<p><a href=""https://i.stack.imgur.com/bTpON.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bTpON.png"" alt=""Web1 error"" /></a></p>
<p>And this is how I'm trying to get the output message from the Lookup activity:</p>
<p><a href=""https://i.stack.imgur.com/dacBU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dacBU.png"" alt=""output"" /></a></p>
","<azure><azure-data-factory>","2022-11-16 14:08:24","192","0","1","74464699","<p>I tried to reproduce and ended up with similar error.</p>
<p><img src=""https://i.imgur.com/Lvt6UNH.png"" alt=""enter image description here"" /></p>
<p>Simple solution is that in web activity invoking Logic App, just <strong>copy your body content into the body text field box WITHOUT selecting &quot;Add Dynamic Content&quot;</strong></p>
<p>Web Activity (Not using Add Dynamic Content):
<img src=""https://i.imgur.com/oLotfNX.png"" alt=""enter image description here"" /></p>
<p>Pipeline runs successfully:
<img src=""https://i.imgur.com/twYLOfx.png"" alt=""enter image description here"" /></p>
<p>Output:
<img src=""https://i.imgur.com/9JqPch2.png"" alt=""enter image description here"" /></p>
"
"74461249","DnsResolutionFailure when Azure Data Factory try to access File Server after update of Self-Hosted Integration Runtime","<p>I have ADF Linked Services to File Server(on-prem) It is accessing X drive.
Linked Service is using Self-hosted integration Runtime (VM) to access File Server.
I have pipeline which copy files from Azure Blob storage to File Server.
This has been working fine for more than years.</p>
<p>However this connection got broke after Integration Runtime got software update last week.</p>
<pre><code>Operation on target ForEach1 failed: Activity failed because an inner activity failed; Inner 
activity name: Copy data, Error: ErrorCode= DnsResolutionFailure, 
'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException, Message=x could not be 
resolved., Source=Microsoft.DataTransfer.Common,'
</code></pre>
<p>What could be issue?</p>
","<azure-data-factory>","2022-11-16 13:24:03","337","0","1","74468664","<p>As per my analysis below are findings:</p>
<p><strong>Root cause</strong>:</p>
<p>As of version <code>5.22.8297.1</code>, to improve security, the File System Connector will no longer support connecting to local disk, for example, the C: drive, as well as \localhost.</p>
<p><strong>Action Required</strong>:</p>
<p>For long term solution, ADF product team recommends, customers should serve files over a remote network share instead of from the local disk of the same machine SHIR is running on.</p>
<p><strong>Temporary workaround</strong>:</p>
<p>Downgrade to a previous version of SHIR, until you've made the required change listed above (Long Term solution). Ensure to disable auto-update until the action is taken.</p>
<p>The latest version that supports the above scenario is <code>5.22.8285.1</code>:</p>
<p><a href=""https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_5.22.8285.1.msi"" rel=""nofollow noreferrer"">https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_5.22.8285.1.msi</a>.</p>
<p>Once the action is completed, please re-enable auto-update, or manually update to the latest version as soon as possible.</p>
<p>Here is a GitHub issue where ADF product team is actively engaging with users in regard to this issue. Feel free to add your comments/feedbacks if you have any: <a href=""https://github.com/Azure/Azure-DataFactory/issues/472"" rel=""nofollow noreferrer"">Integration Runtime Upgrade Breaks Sink Connections</a></p>
"
"74461194","Synapse/ADF pipeline error 2200 on upserting CSV into on-prem SQL-server table","<p>In my pipeline I perform an upsert from a flat CSV file (semicolon separated) into a table in our on-premise SQL-server. At first all the data types from the CSV file are read as STRING and I did the same with the SQL table, which means that I set all the fields to NVARCHAR(500). This worked fine, the upsert is based on an id field (EMPLOYEE_ID).</p>
<p>This CSV file is coming from another application (also MS-SQL based). So that why I changed all the NVARCHAR(500) fields to the correct ones coming from the source database.</p>
<p>Now I've done that I'm getting this error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=SqlOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed. Please search error to get more details.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=The data type text cannot be used as an operand to the UNION, INTERSECT or EXCEPT operators because it is not comparable.,Source=.Net SqlClient Data Provider,SqlErrorNumber=5335,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=5335,State=1,Message=The data type text cannot be used as an operand to the UNION, INTERSECT or EXCEPT operators because it is not comparable.,},],'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;EMP upsert&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>I'd see that it's trying to upsert a text field to do the UNION. But I'm not using a text field to match on. Can somebody tell me what I'm doing wrong and how to solve it?</p>
<p>I've refreshed all the schema definitions in Azure to make sure that it sees all the datatypes correctly.</p>
","<azure><merge><azure-data-factory><azure-synapse><upsert>","2022-11-16 13:20:04","174","0","1","74474161","<p><strong>I tried to reproduce the issue in my environment and ended up with same error.</strong></p>
<p><img src=""https://i.imgur.com/zAZe0yT.png"" alt=""enter image description here"" /></p>
<ul>
<li>The cause of error can the logic of upsert it takes the column as a key and compare the value with both source and destination and based on this it updates or insert records.</li>
<li>Text, xml, ntext these datatypes are not comparable</li>
<li>The workaround for this issue is to cast the column with <code>TEXT</code> data type with Use <code>nvarchar(max)</code>, <code>varchar(max)</code> and <code>varbinary(max)</code> instead.</li>
</ul>
<p>Form <a href=""https://learn.microsoft.com/en-us/sql/t-sql/data-types/ntext-text-and-image-transact-sql?view=sql-server-ver16"" rel=""nofollow noreferrer"">Microsoft Document</a>,</p>
<blockquote>
<p>The <strong>ntext</strong>, <strong>text</strong>, and <strong>image</strong> data types will be removed in a future version of SQL Server. Avoid using these data types in new development work, and plan to modify applications that currently use them. Use <a href=""https://learn.microsoft.com/en-us/sql/t-sql/data-types/nchar-and-nvarchar-transact-sql?view=sql-server-ver16"" rel=""nofollow noreferrer"">nvarchar(max)</a>, <a href=""https://learn.microsoft.com/en-us/sql/t-sql/data-types/char-and-varchar-transact-sql?view=sql-server-ver16"" rel=""nofollow noreferrer"">varchar(max)</a>, and <a href=""https://learn.microsoft.com/en-us/sql/t-sql/data-types/binary-and-varbinary-transact-sql?view=sql-server-ver16"" rel=""nofollow noreferrer"">varbinary(max)</a> instead.</p>
</blockquote>
<p>I casted the column with Text datatype to varchar(max) in Pre-copy script so it will alter the column datatype before the upsert operation.</p>
<p>Query:</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE dbo.yourtablename
ALTER COLUMN yourTextcolumnname VARCHAR(MAX)
</code></pre>
<p><img src=""https://i.imgur.com/Hf74qRv.png"" alt=""enter image description here"" /></p>
<p>Pipeline executing successfully:</p>
<p><img src=""https://i.imgur.com/3qnuvBX.png"" alt=""enter image description here"" /></p>
"
"74460029","How Paramterize Copy Activity to SQL DB with Azure Data Factory","<p>I'm trying to automatically update tables in  Azure SQL Database from another SQLDB with Azure Data Factory. At the moment, the only way to update the table Azure SQL Database is to physically select the table you want to update in Azure SQL Database, as shown here:</p>
<p><a href=""https://i.stack.imgur.com/I6Chn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I6Chn.png"" alt=""enter image description here"" /></a></p>
<p>My configuration to automatically select a table the SQLDB that I want to copy to Azure SQL Database is as follows:</p>
<p><a href=""https://i.stack.imgur.com/GwdWt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GwdWt.png"" alt=""enter image description here"" /></a></p>
<p>The parameters are as follows:</p>
<pre><code>@concat('SELECT * FROM ',pipeline().parameters.Domain,'.',pipeline().parameters.TableName)
</code></pre>
<p>Can someone let me know how to configure my SINK and/or connection to automatically insert the table selected from SOURCE.</p>
<p>My SINK looks like the following:
<a href=""https://i.stack.imgur.com/P0otD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P0otD.png"" alt=""enter image description here"" /></a></p>
<p>And my connection looks like the following:
<a href=""https://i.stack.imgur.com/VFTtZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VFTtZ.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-11-16 11:55:42","171","0","1","74461381","<blockquote>
<p>Can someone let me know how to configure my SINK and/or connection to
automatically insert the table selected from SOURCE.</p>
</blockquote>
<p>You can use <strong>Edit option</strong> in the SQL dataset.</p>
<p><strong>Create a dataset parameter for the sink table name</strong>. In the SQL sink dataset <strong>check the <code>Edit</code> checkbox in it and use the dataset parameter</strong>. If you want, you can use dataset parameter for the database name also. Here I have given directly (dbo).</p>
<p><img src=""https://i.imgur.com/ZDW2L6Y.png"" alt=""enter image description here"" /></p>
<p>Now in the copy activity sink, <strong>you can give the table name dynamically</strong> from any pipeline parameter (give your parameter in this case) or any variable using the <strong>dynamic content</strong>.</p>
<p><img src=""https://i.imgur.com/r4dhLwV.png"" alt=""enter image description here"" /></p>
<p>Also, enable the <strong>Auto create table</strong> which will <strong>create new table if the table with the given name not exists and if it exists it ignores creation and copies data to it</strong>.</p>
<p><strong>My sample result:</strong></p>
<p><img src=""https://i.imgur.com/fOeSDxo.png"" alt=""enter image description here"" /></p>
"
"74459943","how to using web activity in data factory","<p>I have to implement one report using Lookup activity and Web activity.</p>
<p>Lookup activity having output from store procedure with multiple records like below</p>
<p>Name ActiveRecords Active
Abc  500             0
XYZ  300             200</p>
<p>Something like the above I have output from the procedure and then I have to use this on web activity.
Also, I am having exciting web activity and I have appended it.</p>
<p>Thanks in advance</p>
","<azure><azure-data-factory>","2022-11-16 11:49:05","353","0","1","74475485","<ol>
<li>Created ADF pipeline as shown below,
<img src=""https://i.imgur.com/d5IjqwA.png"" alt=""enter image description here"" /></li>
<li>In Lookup activity calling sql stored procedure as shown below,
<img src=""https://i.imgur.com/qRsZpoz.png"" alt=""enter image description here"" /></li>
<li>Output of lookup activity as shown below,
<img src=""https://i.imgur.com/fjjUug0.png"" alt=""enter image description here"" /></li>
<li>The output of Lookup activity is using in web as</li>
</ol>
<pre><code>@string(activity('Lookup').output)
</code></pre>
<p><img src=""https://i.imgur.com/5bSafgP.png"" alt=""enter image description here"" />
5. Debug the pipeline and it is taking output of lookup as shown below,
<img src=""https://i.imgur.com/xhboQEc.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/02csCy3.png"" alt=""enter image description here"" /></p>
"
"74449644","Upon CICD Pipeline deployment to another environment I get a bad resource error for a Linked Service in ADF. I can't update it or delete it","<p>I am pushing an ADF factory to another environment via a CICD Pipeline and YAML Config file in Azure Devops. I can successfully deploy but one of my linked services becomes a &quot;bad resource&quot; although it works in the master branch when I published it.</p>
<p>Furthermore I cannot delete this in the target data factory nor can I edit it. Getting the bad resource error. I suspect I need to edit something in the ARM file but I don't really understand this error nor can I find much information on similar.</p>
<p><a href=""https://i.stack.imgur.com/TgeAy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TgeAy.png"" alt=""enter image description here"" /></a></p>
<p><code>{&quot;stack&quot;:&quot;Error: Error: Unable to save [SERVICENAME]. Bad resource\n at Rl.&lt;anonymous&gt; (https://adf.azure.com/app.06b0e174dd8e6fa8.js:1:11274843)\n at Generator.next (&lt;anonymous&gt;)\n at https://adf.azure.com/main.d1fe4ec6f69aa72f.js:1:66326\n at new c</code></p>
<p>That when I deploy my ADF to a new environment it succeeds with connections intact or at least that I can fix/edit.</p>
<p>EDIT: Even when I recreate the Linked Service I get the same error.</p>
","<azure-devops><azure-data-factory><cicd>","2022-11-15 17:10:44","93","0","1","74461812","<p>The answer to this is to store all of your connection credentials as secrets in Azure Keyvault then reference that. I am unclear why using the parameters in a linked service do not transfer into the ARM template and this cause it to be a &quot;bad resource&quot; but the Keyvault method translates into ARM correctly and the problem doesn't persist.</p>
"
"74445617","Azure Data Factory JSON Merge Operation Only Merges First Two Rows","<p>I am attempting to merge multiple JSON files. For testing purposes, I am just attempting to merge just two JSON files. However, the results of the Merge only includes the first row of each JSON.</p>
<p>For example, if I were to merge the following JSON tables, see image(I know the image doesn't show JSON tables, but the actual JSON is too big to show here) the output would just show the first row of each table.</p>
<p>First Table</p>
<p><a href=""https://i.stack.imgur.com/AbkZD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AbkZD.png"" alt=""JSON Table One"" /></a></p>
<p>Second Table</p>
<p><a href=""https://i.stack.imgur.com/1aZhG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1aZhG.png"" alt=""enter image description here"" /></a></p>
<p>Output
<a href=""https://i.stack.imgur.com/AOAby.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AOAby.png"" alt=""enter image description here"" /></a></p>
<p>You can see the output just shows the first row of each table.</p>
<p>My configuration is as follows:</p>
<p><a href=""https://i.stack.imgur.com/5fU6n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5fU6n.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/3bmpN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3bmpN.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/QXafK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QXafK.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/hvWM8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hvWM8.png"" alt=""enter image description here"" /></a></p>
<p>I think this could an easy solution.</p>
<p>Your thoughts are welcomed</p>
<p>So, I changed my collection reference to the following:</p>
<p><a href=""https://i.stack.imgur.com/CBe5B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CBe5B.png"" alt=""enter image description here"" /></a></p>
<p>I updated the Copy Activity as follows, but still getting the the first and second rows 4000 times:</p>
<p><a href=""https://i.stack.imgur.com/lOau7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lOau7.png"" alt=""enter image description here"" /></a></p>
<p>I don't know why this keeps on failing for me.</p>
<p>The above is a preview of my json
<a href=""https://i.stack.imgur.com/1CmoK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1CmoK.png"" alt=""enter image description here"" /></a></p>
<p>I have updated my mapping as follows:</p>
<p><a href=""https://i.stack.imgur.com/fYMo4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fYMo4.png"" alt=""enter image description here"" /></a></p>
<p>But I still keep only getting two rows:</p>
<p><a href=""https://i.stack.imgur.com/FM3Z8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FM3Z8.png"" alt=""enter image description here"" /></a></p>
<p>There must be something simple that I'm missing....</p>
","<azure-data-factory>","2022-11-15 12:27:46","68","0","1","74450045","<p>I reproduced the above and got the below result.</p>
<p>These are my sample json files.</p>
<p><strong>Sample1.json</strong></p>
<pre><code>{
&quot;data&quot;:[
{
  &quot;name&quot;: &quot;Rakesh&quot;,
  &quot;id&quot;: &quot;1&quot;
},
{
  &quot;name&quot;: &quot;Kohli&quot;,
  &quot;id&quot;: &quot;2&quot;
}
]
}
</code></pre>
<p><strong>Sample2.json</strong></p>
<pre><code>{
&quot;data&quot;:[
{
  &quot;name&quot;: &quot;Laddu&quot;,
  &quot;id&quot;: &quot;1&quot;
},
{
  &quot;name&quot;: &quot;Virat&quot;,
  &quot;id&quot;: &quot;2&quot;
}
]
}
</code></pre>
<p>I got the same output when I did the mapping like below.</p>
<p><img src=""https://i.imgur.com/OYVxqBq.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/gq7QGoI.png"" alt=""enter image description here"" /></p>
<p>Give the mapping like below <code>import schemas-&gt; advanced editor -&gt; collection reference</code> and you can get the desired merging.</p>
<p><img src=""https://i.imgur.com/VQr4JTm.png"" alt=""enter image description here"" /></p>
<p><strong>Output json:</strong></p>
<p><img src=""https://i.imgur.com/jZ8StGE.png"" alt=""enter image description here"" /></p>
"
"74444899","how to store web activity result in a variable?","<p>i have a web activity through which i am executing a rest API(execute queries)
this is a sample output of that:</p>
<pre><code>        {

         &quot;results&quot;: [
             {
             &quot;tables&quot;: [
                 {
                 &quot;rows&quot;: [
                       {
                       &quot;[Value]&quot;: &quot;2022-10-25T00:00:00Z&quot;
                       }
                      ]
                  }
             ]
         }
]
</code></pre>
<p>i want to store the date value inside [Value] in a variable in adf</p>
<p>(variable value should be:2022-10-25T00:00:00Z)</p>
<p>but i am not able to do that because of square brackets
this is what i have tried</p>
<pre><code>&quot;@activity('SQl validation').output.results[0].tables[0].rows[0].[value]&quot;
</code></pre>
<p>but it give me error</p>
<pre><code>Position 73 Unrecognized expression: value
</code></pre>
<p>please suggest how i can fix this</p>
<p><a href=""https://i.stack.imgur.com/Jko5V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jko5V.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><azure-devops-rest-api><powerbi-rest-api>","2022-11-15 11:30:43","125","0","1","74445468","<ul>
<li>Look at the following demonstration. I have the lookup which returns the following values.</li>
</ul>
<p><img src=""https://i.imgur.com/NsRqUYE.png"" alt=""enter image description here"" /></p>
<ul>
<li>To access the <code>[companyId]</code> attribute from this output array, I have used the following dynamic content (Using for loop just for demonstration):</li>
</ul>
<pre><code>@string(activity('myLookUp').output.value[item()]['[companyId]'])
</code></pre>
<p><img src=""https://i.imgur.com/oeIQlZS.png"" alt=""enter image description here"" /></p>
<ul>
<li>So, you can use the following dynamic content instead:</li>
</ul>
<pre><code>@activity('SQl validation').output.results[0].tables[0].rows[0]['[value]']
</code></pre>
"
"74444386","How to Perform left Anti and Right Antijoins in Dataflow?","<p>I can see i can do only</p>
<p>Left right inner,Full ,Cross joins in Dataflow.
I can t see a left Anti or Right Anti joins in Dataflow. So how to perform those joins like that of in Sql in Azure data factory</p>
","<azure><azure-data-factory>","2022-11-15 10:53:11","233","0","1","74456503","<p>As mentioned in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join"" rel=""nofollow noreferrer"">document</a> you can able to see built in joins and perform Full outer, inner, Left outer, Right outer and Cross joins in Data flow as shown in below image.
<img src=""https://i.imgur.com/k7QWqhj.png"" alt=""enter image description here"" />
But as per your requirement you can't see built in anti joins like left anti join, right anti join in data flow like SQL.
For anti joins in Data flow we can perform join conditions as shown in below image.
<img src=""https://i.imgur.com/DGtp8Op.png"" alt=""enter image description here"" />
In join conditions what you have to do is as shown in below image from left column select source1 and from  right column select souce2 and in between  there is filter operation like [= =,! =,&lt;,&gt;, &lt;=,&gt;=, = = =, &lt; = &gt; ].As per our requirement we can perform any of these operations in left join.
<img src=""https://i.imgur.com/svw4Uw1.png"" alt=""enter image description here"" />
I created data flow as shown below by taking source1 as employee data and source2 as depart and combined these sources using left join.
<img src=""https://i.imgur.com/eaigxNP.png"" alt=""enter image description here"" />
After choosing left join in join conditions column1 as employee data and column2 as depart and in filter i am using === operator
<img src=""https://i.imgur.com/LmrVVem.png"" alt=""enter image description here"" />
After performing left join and join condition below is the output got.
<img src=""https://i.imgur.com/8yiy6Ak.png"" alt=""enter image description here"" /></p>
<p>Here is the Source1= employee data input:</p>
<p><img src=""https://i.imgur.com/bGmaXvw.png"" alt=""enter image description here"" /></p>
<p>Sorce 2= depart input:
<img src=""https://i.imgur.com/ARFV79H.png"" alt=""enter image description here"" /></p>
<p>Alternative method:</p>
<p>As of now it is not possible in joins but you can try it by using Exists.</p>
<p>Source1 in dataflow</p>
<p><img src=""https://i.imgur.com/sEpY89V.png"" alt=""enter image description here"" /></p>
<p>Source2 data in dataflow</p>
<p><img src=""https://i.imgur.com/FiWdG0c.png"" alt=""enter image description here"" /></p>
<p>Next both sources are joined by EXIST activity. In below image you can find exist type &amp; exists conditions. In Exist Activity Exist type is Doesn't exist
<img src=""https://i.imgur.com/QNhzk2k.png"" alt=""enter image description here"" /></p>
<p>After validation you can see required left anti join output in Data preview as shown below
<img src=""https://i.imgur.com/EwAL1cO.png"" alt=""enter image description here"" /></p>
"
"74443889","Data Factory | Copy recursively from multiple subfolders into one folder wit same name","<p>Objective: Copy all files from multiple subfolders into one folder with same filenames.
E.g.</p>
<pre><code>Source Root Folder
20221110/
  AppID1
    File1.csv
    File2.csv
  /AppID2
     File3.csv
     File4.csv
20221114
   AppID3
     File5.csv
     File6.csv
and so on
Destination Root Folder
    File1.csv
    File2.csv
    File3.csv
    File4.csv
    File5.csv
    File6.csv
</code></pre>
<p>Approach 1 Azure Data Factory V2 All datasets selected as binary</p>
<ol>
<li>GET METADATA - CHILDITEMS</li>
<li>FOR EACH - Childitem</li>
<li>COPY ACTIVITY(RECURSIVE : TRUE, COPY BEHAVIOUR: FLATTEN)</li>
</ol>
<p>This config renames the files with autogenerated names.
If I change the copy behaviour to preserve hierarchy, Both file name and folder structure remains intact.</p>
<p>Approach 2</p>
<ol>
<li>GET METADATA - CHILDITEMS</li>
<li>FOR EACH - Childitems</li>
<li>Execute PL2 (Pipeline level parameter: @item.name)</li>
<li>Get Metadata2 (Parameterised from dataset, invoked at pipeline level)</li>
<li>For EACH2- Childitems</li>
<li>Copy (Source: FolderName - Pipeline level, File name - ForEach2)</li>
</ol>
<p>Both approaches not giving the desired output. Any help/Workaround would be appreciated.</p>
","<azure-blob-storage><file-storage><azure-data-factory>","2022-11-15 10:13:47","306","0","2","74469545","<p>My understanding is in Option 2
Step 3 &amp; 5 :is done as to iterate through the folder and subfolder correct ?</p>
<p>6 . Copy (Source: FolderName - Pipeline level, File name - ForEach2)</p>
<p>I think since in step 6 you already have the filename . On the SINK side , add an dynamic expression and add @Filename and that should do the trick .</p>
"
"74443889","Data Factory | Copy recursively from multiple subfolders into one folder wit same name","<p>Objective: Copy all files from multiple subfolders into one folder with same filenames.
E.g.</p>
<pre><code>Source Root Folder
20221110/
  AppID1
    File1.csv
    File2.csv
  /AppID2
     File3.csv
     File4.csv
20221114
   AppID3
     File5.csv
     File6.csv
and so on
Destination Root Folder
    File1.csv
    File2.csv
    File3.csv
    File4.csv
    File5.csv
    File6.csv
</code></pre>
<p>Approach 1 Azure Data Factory V2 All datasets selected as binary</p>
<ol>
<li>GET METADATA - CHILDITEMS</li>
<li>FOR EACH - Childitem</li>
<li>COPY ACTIVITY(RECURSIVE : TRUE, COPY BEHAVIOUR: FLATTEN)</li>
</ol>
<p>This config renames the files with autogenerated names.
If I change the copy behaviour to preserve hierarchy, Both file name and folder structure remains intact.</p>
<p>Approach 2</p>
<ol>
<li>GET METADATA - CHILDITEMS</li>
<li>FOR EACH - Childitems</li>
<li>Execute PL2 (Pipeline level parameter: @item.name)</li>
<li>Get Metadata2 (Parameterised from dataset, invoked at pipeline level)</li>
<li>For EACH2- Childitems</li>
<li>Copy (Source: FolderName - Pipeline level, File name - ForEach2)</li>
</ol>
<p>Both approaches not giving the desired output. Any help/Workaround would be appreciated.</p>
","<azure-blob-storage><file-storage><azure-data-factory>","2022-11-15 10:13:47","306","0","2","74470697","<p>If <strong>all of your files are in the same directory level</strong>, you can try the below approach.</p>
<p>First use Get Meta data activity to get all files list and then use copy inside ForEach to copy to a target folder.</p>
<p><strong>These are my source files with directory structure:</strong></p>
<p><img src=""https://i.imgur.com/F8kH6wj.png"" alt=""enter image description here"" /></p>
<p><strong>Source dataset:</strong></p>
<p><strong>Based on your directory level</strong> use the wildcard placeholder(<code>*/*</code>) in the source dataset.</p>
<p><img src=""https://i.imgur.com/wMj5e9Q.png"" alt=""enter image description here"" /></p>
<p>The above error is only a warning, and we can ignore it while debug.</p>
<p><strong>Get meta data activity:</strong></p>
<p><img src=""https://i.imgur.com/dSyqcNR.png"" alt=""enter image description here"" /></p>
<p>This will give all the files list inside subfolders.</p>
<p><img src=""https://i.imgur.com/KpI6OL3.png"" alt=""enter image description here"" /></p>
<p>Give this array to a ForEach activity and inside ForEach use copy activity.</p>
<p><strong>Copy activity source:</strong></p>
<p><img src=""https://i.imgur.com/tGfRCW8.png"" alt=""enter image description here"" /></p>
<p>In the above also, the <code>*/*</code> should be same as we gave in Get Meta data.</p>
<p>For sink dataset create a dataset parameter and use in the file path of dataset.</p>
<p><img src=""https://i.imgur.com/QnmwkPG.png"" alt=""enter image description here"" /></p>
<p><strong>Copy activity sink:</strong></p>
<p><img src=""https://i.imgur.com/aIi4Hsa.png"" alt=""enter image description here"" /></p>
<p><strong>Files copied to target folder:</strong></p>
<p><img src=""https://i.imgur.com/rOkti19.png"" alt=""enter image description here"" /></p>
<p><strong>If your source files are not in same directory level</strong> then you can try the <strong>recursive approach</strong> mentioned in this <a href=""https://richardswinbank.net/adf/get_metadata_recursively_in_azure_data_factory"" rel=""nofollow noreferrer"">article</a> by <strong>@Richard Swinbank</strong>.</p>
"
"74441852","Dict Union in Data Factory Expression","<p>I am trying to combine some dictionaries within Data Factory's expressions, but so far have found it nearly impossible to do this in an elegant way, and my solutions so far are quite difficult to manage. So is the workaround below the only way of merging dictionaries in Data Factory? Or is there a simpler way of doing this that I am missing?</p>
<h2>Working Baseline</h2>
<p>Let's start with a basic pipeline with two input parameters:</p>
<ul>
<li><code>a</code> (Object) with default <code>{&quot;a&quot;: 1}</code></li>
<li><code>b</code> (Object) with default <code>{&quot;b&quot;: 2}</code></li>
</ul>
<p>Add a &quot;Set Variable&quot; activity which creates a union of <code>a</code> and <code>b</code> and converts it to a string:</p>
<pre><code>@string(union(
    pipeline().parameters.a,
    pipeline().parameters.b
))
</code></pre>
<p>This works and produces the expected out:</p>
<pre><code>{
    &quot;name&quot;: &quot;output&quot;,
    &quot;value&quot;: &quot;{\&quot;a\&quot;:1,\&quot;b\&quot;:2}&quot;
}
</code></pre>
<h2>Non-working Extension</h2>
<p>Let's suppose I want to add some metadata to the output dict. I would have expected that I can do this:</p>
<pre><code>@string(union(
    pipeline().parameters.a,
    pipeline().parameters.b,
    {'datetime': utcNow()}
))
</code></pre>
<p>but this does not work as it is apparently invalid syntax.</p>
<h3>Workaround</h3>
<p>I have found that it is possible to wrap a JSON string in the <code>json</code> function:</p>
<pre><code>@string(union(
    pipeline().parameters.a,
    pipeline().parameters.b,
    json('{&quot;c&quot;: 3}')
))
</code></pre>
<p>But this seems rather cumbersome. In particular, it does not seem to be possible to use <code>@{...}</code> on the internal strings and instead one must resort to clunky uses of <code>concat</code>:</p>
<pre><code>@string(union(
    pipeline().parameters.a,
    pipeline().parameters.b,
    json(concat(
        '{&quot;datetime&quot;: &quot;',
        utcNow(),
        '&quot;}'
    ))
))
</code></pre>
","<azure-data-factory>","2022-11-15 07:16:35","70","0","1","74456857","<ul>
<li><p>To build an object, you are using union on the parameters. But since the datetime object is not available, we have to build it using <code>utcNow()</code> function.</p>
</li>
<li><p>The method that you are using is not a workaround, it is the way to build the JSON object.</p>
</li>
<li><p>String interpolation<code>@{...}</code> is helps us to use dynamic content within expression builder, but not inside function arguments.</p>
</li>
<li><p>One way so that it is easily understandable is to split the activity. Create the datetime object in one set variable and then apply <code>union()</code> in another.</p>
</li>
<li><p>To build the datetime object, use the following dynamic content:</p>
</li>
</ul>
<pre><code>{&quot;datetime&quot;:&quot;@{utcnow()}&quot;}
</code></pre>
<p><img src=""https://i.imgur.com/ppy7tWJ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now you can apply union in the following way:</li>
</ul>
<pre><code>@string(union(
    pipeline().parameters.a,
    pipeline().parameters.b,
    json(variables('demo'))
))
</code></pre>
<p><img src=""https://i.imgur.com/TrMSvfv.png"" alt=""enter image description here"" /></p>
"
"74440860","How the Script activity output be pushed in form csv /structured format as a sink","<p>If i run the script activity ,it will give an output in json , how to push that data in csv /anyother format to blob/database ???</p>
<p>Consider i have 1 million rows outputed from Script activity(Single Query) ??</p>
","<azure><azure-data-factory>","2022-11-15 05:09:45","280","1","1","74448755","<p>We can take the json output of script actvity and copy it to SQL database. Below is the approach.</p>
<ul>
<li>In Script actvity,  Query is given in script settings.
<code>select * from sampletb</code></li>
</ul>
<p><img src=""https://i.imgur.com/QGZJPSE.png"" alt=""enter image description here"" /></p>
<p>**Output of the script activity: **</p>
<p><img src=""https://i.imgur.com/VosoR58.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then Output of this activity is stored in a variable of string type.
<code>@string(activity('Script1').output.resultSets[0].rows)</code></li>
</ul>
<p><img src=""https://i.imgur.com/e7oxMhR.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>In SQL database, target table named <code>test_tgt</code> is created.</p>
</li>
<li><p>Another script activity is taken and below script is written to copy to SQL database.</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>declare @json nvarchar(4000)=N'@{variables('v1')}';
INSERT INTO test_tgt
SELECT * FROM OPENJSON(@json)
WITH (
Col varchar(60) ,
col1 varchar(50)
);
</code></pre>
<p><img src=""https://i.imgur.com/7O0Vl9H.png"" alt=""enter image description here"" /></p>
"
"74438864","Azure Function versus Data Factory pipeline, which do you recommend?","<p>We need to construct an application that runs once a month.</p>
<p>Its purpose is to pull data from a database and feed it to another database with the same schema, while doing some minor transformations to that data (nothing too intensive, though, mostly obfuscation).</p>
<p>We would like it to be as cost-effective as possible, since it's meant to help development, and doesn't bring any value to production. We would also require it run entirely on the cloud, since the values coming from the source database could be sensitive and should not go through any local machine.</p>
<p>Would you recommend we created an ADF pipeline for this, or an Azure function? Or something entirely different (as long as it is still an Azure service)? It wouldn't ever need to scale out or up, and performance doesn't really matter.</p>
<p>Also, we have developers who can handle the code, so the no-code or low-code options can be ruled out if they're not exactly better.</p>
","<azure><azure-functions><azure-data-factory>","2022-11-14 23:03:12","138","0","1","74440744","<p>ADF pipeline has in built connectors to access the databases and a copy activity to copy the data.
In case if there are transformations involved, you can either copy the data as is in staging table and then leverage stored procedure to do transformations which would be cheaper as compared to leveraging a dataflow within ADF to incorporate minor transformations.</p>
"
"74437574","Find and replace function in Alteryx -How it can be done in Azure Data Flow","<p>I have a &quot;Find and replace &quot; tool in Alteryx which finds the Col value of csv file1 and replaces it with the look up csv file2 which has 2 columns like
Word and  ReplacementWord.</p>
<p>Example :
Address is a col in Csv file1 which has value like St.Xyz,NY,100067</p>
<p>And Csv file 2 has
Word    ReplacementWord
NY        NewYork
ZBW        Zimbawe  etc....</p>
<p>Now the final Output should be
Address
St.Xyz,NewYork,100067</p>
<p>Please help guys .</p>
<p>Hey here's the problem .I have a &quot;Find and replace &quot; tool in Alteryx which finds the Col value of csv file1 and replaces it with the look up csv file2 which has 2 columns like</p>
<p>Word and  ReplacementWord.</p>
<p>Example :</p>
<p>Address is a col in Csv file1 which has value like St.Xyz,NY,100067</p>
<p>And Csv file 2 has</p>
<p>Word    ReplacementWord</p>
<p>NY        NewYork</p>
<p>ZBW        Zimbawe  etc....</p>
<p>Now the final Output should be</p>
<p>Address</p>
<p>St.Xyz,NewYork,100067</p>
<p>Please help guys .</p>
","<azure><azure-data-lake-gen2><azure-data-factory>","2022-11-14 20:39:48","650","0","1","74442702","<p>I tried to reproduce your scenario in my environment to achieve the desired output I Followed below steps:</p>
<ul>
<li><p>In dataflow activity I took 2 Sources
<strong>Source 1 is the file which contain the actual address.</strong>
<img src=""https://i.imgur.com/kITHuZF.png"" alt=""enter image description here"" />
<strong>Source 2 is the file which contain the country codes with names.</strong>
<img src=""https://i.imgur.com/HF6E5A4.png"" alt=""enter image description here"" /></p>
</li>
<li><p>After that I took <code>lookup</code> to merge files based on the country code. In lookup condition I provided <code>split(Address,',')[2]</code> to split the address string with comma and get the 2nd value from it Which will be the country code based on this : Xyz,<strong>NY</strong>,100067 and column_1 of 2nd source.
<img src=""https://i.imgur.com/Mwv3T4p.png"" alt=""enter image description here"" />
Lookup data preview:
<img src=""https://i.imgur.com/nwCI8GN.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Now took <code>Derived Column</code> and gave column name as Address with the expression <code>replace(Address, split(Address,',')[2], Column_2)</code>  It will replace the What we split in lookup from Address string to value of Column_2
<img src=""https://i.imgur.com/PrKaU4J.png"" alt=""enter image description here"" />
Derived column preview:
<img src=""https://i.imgur.com/mGQ8u7R.png"" alt=""enter image description here"" /></p>
</li>
<li><p>then took select and deleted the unwanted columns
<img src=""https://i.imgur.com/vvAxlgt.png"" alt=""enter image description here"" />
Select Preview:
<img src=""https://i.imgur.com/O5mRONh.png"" alt=""enter image description here"" /></p>
</li>
<li><p>now providing this to sink dataset
<img src=""https://i.imgur.com/K2JIVxe.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p><strong>Output</strong></p>
<p><img src=""https://i.imgur.com/mNNrsHi.png"" alt=""enter image description here"" /></p>
"
"74436237","Azure Data Factory Variable Not Accepting Arrays","<p>I am trying to pass a number of variables into a copy activity. However, I keep on getting the error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;InvalidTemplate&quot;,
    &quot;message&quot;: &quot;The expression 'string(activity('MyLookup').output.value[0].companyId[item()])' cannot be evaluated because property '0' cannot be selected. Property selection is not supported on values of type 'String'.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Set variable1&quot;,
    &quot;details&quot;: &quot;&quot;
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/X4PqQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X4PqQ.png"" alt=""enter image description here"" /></a></p>
<p>My Variables are as follows:</p>
<p><a href=""https://i.stack.imgur.com/H5ORh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H5ORh.png"" alt=""enter image description here"" /></a></p>
<p>The actual configuration for the Set variable 1 is:
<a href=""https://i.stack.imgur.com/F22Vq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F22Vq.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Qg1j7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qg1j7.png"" alt=""enter image description here"" /></a></p>
<p>Any thoughts on why I'm getting the error?</p>
<p>The settings of the Lookup activity 'MyLookup' is as follows:</p>
<p><a href=""https://i.stack.imgur.com/IQUz5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IQUz5.png"" alt=""enter image description here"" /></a></p>
<p>The output of Mylookup activity is as follows:</p>
<p><a href=""https://i.stack.imgur.com/OH9w9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OH9w9.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-11-14 18:25:57","102","0","1","74469453","<p>Foreach Items property should be set as  :</p>
<p><code>@activity('MyLookup').output.Value</code></p>
<p>and the SetVariable which is inside the FE loop should</p>
<p>@item().companyid</p>
"
"74435959","Azure Data Factory -> Using Metadata activity + Filter","<p>I have a pipeline as seen in the screenshot below, the Filter activity below looks for <strong>CSV files</strong> + <strong>starting with a pattern</strong> as I specified + <strong>filters out the file if it contains the current date</strong>.</p>
<p><a href=""https://i.stack.imgur.com/geKqK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/geKqK.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/iEU60.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iEU60.png"" alt=""enter image description here"" /></a></p>
<p><strong>Condition below:</strong>
@and(and(and(equals(item().type,'File'),not(equals(item().name,concat('BBN_TX1_',formatDateTime(convertTimeZone(utcNow(),'UTC','GMT Standard Time'),'yyyy-MM-dd'),'.csv')))),startswith(item().name,'BBN_TX1_')),endswith(item().name,'.csv'))</p>
<p><strong>My requirement:</strong>
I want to <strong>Filter out/Ignore</strong> any files which are of the current date or greater. I was able to achieve my requirement for the current date but for files greater than the current date I need help, please can anyone advise me on what can be done?</p>
","<azure-data-factory>","2022-11-14 17:59:00","139","0","1","74438696","<p>Thanks for your input! Here is the condition I used and it works -</p>
<p>@and(and(and(equals(item().type,'File'),startswith(item().name,'BBN_PLN_')),endswith(item().name,'.xlsx')),less(formatDateTime(substring(item().name,8,10),'yyyy-MM-dd'),formatDateTime(convertTimeZone(utcNow(),'UTC','GMT Standard Time'),'yyyy-MM-dd')))</p>
"
"74433769","Error while accessing SAP data using Azure data factory CDC connector","<p>We are trying to read data from SAP using Azure data factory change data capture(CDC) connector. We get the below error when tried to access the data. The connector works fine for full load and it fails for delta load.</p>
<p><em>Error Message: DF-SAPODP-012 - SapOdp copy activity failure with run id: XXXXXXXX-XXXX-4444-826e-XXXXX, error code: 2200 and error message: ErrorCode=SapOdpOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Sap Odp operation 'OpenOdpRead' failed. Error Number: '013', error message: 'Error while accessing',Source=Microsoft.DataTransfer.Runtime.SapRfcHelper,', Exception: com.microsoft.dataflow.Utils$.failure(Utils.scala:76)
com.microsoft.dataflow.store.sapodp.SapOdpAdmsRequestConstructor$.executeAndMonitorCopyActivity(SapOdpAdmsRequestConstructor.scala:206)
com.microsoft.dataflow.store.sapodp.SapOdpAdmsRequestConstructor$.com$microsoft$dataflow$store$sapodp$SapOdpAdmsRequestConstructor$$executeSapCDCCopyInternal(SapOdpAdmsReque</em></p>
","<azure-data-factory>","2022-11-14 15:06:28","481","1","1","74433770","<p>The issue was due to the additional privileges needed for the user to read data from SAP Operational Data Provisioning (ODP) framework. The full load works as there is not need to track the changes. To solve this issue, we added authorization objects S_DHCDCACT, S_DHCDCCDS, S_DHCDCSTP to the user profile which read data from SAP.</p>
"
"74433225","How to add an extension to a file copy activity with Azure Data Factory","<p>The datasets that I ingest from a REST API endpoint do not include the .JSON extension to the files (even though they're JSON files). Therefore, can someone let me know where I can add a .json extension from the following scenarios</p>
<p>Scenario 1.
Adding .JSON to the relativeURL
<a href=""https://i.stack.imgur.com/2cL2K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2cL2K.png"" alt=""enter image description here"" /></a></p>
<p>Scenario 2
Adding .JSON to the SINK
<a href=""https://i.stack.imgur.com/AHAZo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AHAZo.png"" alt=""enter image description here"" /></a></p>
<p>Scenario 3
Adding .JSON to SOURCE - However, I don't think this is possible
<a href=""https://i.stack.imgur.com/9nfP8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9nfP8.png"" alt=""enter image description here"" /></a></p>
<p>Can someone please take a look at the three scenarios and let me know if I can add .JSON extension to any of those methods?</p>
","<azure-data-factory>","2022-11-14 14:28:27","103","0","1","74441042","<p>Thanks to <strong>@Scott Mildenberger</strong>, we can provide the name of the file and its extension from the sink dataset.</p>
<ul>
<li>The following is a demonstration of the same. I have a file called <code>sample</code> without extension.</li>
<li>In the sink dataset, you can simply <code>concat</code> the extension to your filename (If it is just a single file, you can directly give the required name with extension directly). I have used the following dynamic content (fileName parameter value is <code>req_filename</code>).</li>
</ul>
<pre><code>@concat(dataset().fileName,'.json')
</code></pre>
<p><img src=""https://i.imgur.com/yQh9Cyt.png"" alt=""enter image description here"" /></p>
<ul>
<li>The following file would be generated in the sink.</li>
</ul>
<p><img src=""https://i.imgur.com/LABUFGT.png"" alt=""enter image description here"" /></p>
"
"74432961","What Protocol is Azure Data Factory Copy Activity using?","<p>I'm trying to copy data through a self hosted Integration Runtime using Azure Data Factory. Since we hav databases on prem (sql, postgress, oracle) with several firewalls in between. I'm wondering what Protocol the Linked Service is using for the copy activity? Is it TCP?</p>
<p>Thanks in advance!</p>
","<azure><azure-sql-database><azure-data-factory>","2022-11-14 14:08:59","198","0","1","74441649","<p>It is mostly https. From <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations#frequently-asked-questions"" rel=""nofollow noreferrer"">the docs</a>:</p>
<p><strong>What are the port requirements for the self-hosted integration runtime to work?</strong></p>
<blockquote>
<p>The self-hosted integration runtime makes HTTP-based connections to access the internet. The outbound ports 443 must be opened for the self-hosted integration runtime to make this connection. Open inbound port 8060 only at the machine level (not the corporate firewall level) for credential manager application. If Azure SQL Database or Azure Synapse Analytics is used as the source or the destination, you need to open port 1433 as well.</p>
</blockquote>
<p>Also, in addition to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations#firewall-requirements-for-on-premisesprivate-network"" rel=""nofollow noreferrer"">default ports</a>, port 1433 needs to be opened in the following scenario.</p>
<blockquote>
<p>Required only when you copy from or to Azure SQL Database or Azure Synapse Analytics and optional otherwise. Use the staged-copy feature to copy data to SQL Database or Azure Synapse Analytics without opening port 1433.</p>
</blockquote>
"
"74432836","how to compare the file names that are inside a folder (Datalake) using ADF","<p>I have list of files inside a datalake folder and I have list of files names stored in the .CSV File..</p>
<p>My requirement is to compare the files names in the Datalake folder with the filenames in the .CSV File and if the filenames are matching then I want to copy these files and if filenames are not matching then I want to send an Email with missing files in the datalake.</p>
<p>I have used GetMetaData  activity(child items) to get the list of files in the datalake folder and I'm stuck here. Now I want to compare these filenames with the filenames stored in the .CSV File and do the further operations.</p>
<p>Kindly Help</p>
<p>My requirement is to compare the files names in the Datalake folder with the filenames in the .CSV File and if the filenames are matching then I want to copy these files and if filenames are not matching then I want to send an Email with missing files in the datalake.</p>
","<azure><azure-data-factory><azure-data-lake-gen2>","2022-11-14 14:00:33","311","0","1","74443185","<ul>
<li>Get Metadata activity is taken, and dataset is created for datalake. ChildItems is taken as argument for output.</li>
</ul>
<p><img src=""https://i.imgur.com/ejJEm4d.png"" alt=""enter image description here"" /></p>
<ul>
<li>Output of the metadata activity is passed in for each activity.
<code>@activity('Get Metadata1').output.childItems</code></li>
</ul>
<p><img src=""https://i.imgur.com/piCjWL5.png"" alt=""enter image description here"" /></p>
<ul>
<li>Inside for-each, lookup is taken and csv file which contains list of file names is referred.</li>
</ul>
<p><img src=""https://i.imgur.com/ERoVtq3.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>If condition is taken and expression is given as
<code>@contains(string(activity('Lookup1').output.value),item().name)</code></p>
</li>
<li><p>In true case, copy activity is added to copy the matched file name into SQL database.</p>
</li>
</ul>
<p><strong>Edited</strong>- To copy from one location to other location in datalake, follow below steps 1 and 2</p>
<ol>
<li>Source dataset is taken and in file path , file name is given as <code>@{item().name}</code>
<img src=""https://i.imgur.com/dEWc1t8.png"" alt=""enter image description here"" /></li>
</ol>
<p><img src=""https://i.imgur.com/Gn2ZyiP.png"" alt=""enter image description here"" /></p>
<ol start=""2"">
<li>In Sink dataset also, file path is given similarly. This will dynamically create filename as in source.
<img src=""https://i.imgur.com/FtkUxOO.png"" alt=""enter image description here"" /></li>
</ol>
<ul>
<li>In false case, append variable is added and all the values which do not match with lookup, got appended to variable of type <strong>array</strong>.</li>
</ul>
<p><img src=""https://i.imgur.com/teOuO6L.png"" alt=""enter image description here"" /></p>
<ul>
<li>Refer the <strong>MS document</strong> on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-send-email"" rel=""nofollow noreferrer"">How to send email - Azure Data Factory &amp; Azure Synapse | Microsoft Learn</a> for sending email.</li>
</ul>
"
"74432610","Azure Data Factory - convert Json Array to Json Object","<p>I retrieve data using Azure Data Factory from an OnPremise database and the output I get is as follows:</p>
<pre><code>{
  &quot;value&quot;:[
    {
     &quot;JSON_F52E2B61-18A1-11d1-B105-XXXXXXX&quot;:&quot;{\&quot;productUsages\&quot;:[{\&quot;customerId\&quot;:3552,\&quot;productId\&quot;:120,\&quot;productionDate\&quot;:\&quot;2015-02-10\&quot;,\&quot;quantity\&quot;:1,\&quot;userName\&quot;:\&quot;XXXXXXXX\&quot;,\&quot;productUsageId\&quot;:XXXXXX},{\&quot;customerId\&quot;:5098,\&quot;productId\&quot;:120,\&quot;productionDate\&quot;:\&quot;2015-04-07\&quot;,\&quot;quantity\&quot;:1,\&quot;userName\&quot;:\&quot;ZZZZZZZ\&quot;,\&quot;productUsageId\&quot;:ZZZZZZ}]}&quot;
  }
 ]
}
</code></pre>
<p>The entire value array is being serialized into a JSON and I end up with:</p>
<pre><code>[{
&quot;productUsages&quot;:
[
    {
        &quot;customerId&quot;: 3552,
        &quot;productId&quot;: 120,
        &quot;productionDate&quot;: &quot;2015-02-10&quot;,
        &quot;quantity&quot;: 1,
        &quot;userName&quot;: &quot;XXXXXXXX&quot;,
        &quot;productUsageId&quot;: XXXXXX
    },
    {
        &quot;customerId&quot;: 5098,
        &quot;productId&quot;: 120,
        &quot;productionDate&quot;: &quot;2015-04-07&quot;,
        &quot;quantity&quot;: 1,
        &quot;userName&quot;: &quot;ZZZZZZZ&quot;,
        &quot;productUsageId&quot;: ZZZZZZZ
    }
]
}]
</code></pre>
<p>I need to have a Json Object at a root level, not Json Array ([] replaced with {}). What's the easiest way to achieve that in Azure Data Factory?</p>
<p>Thanks</p>
","<json><azure-data-factory>","2022-11-14 13:43:00","122","0","1","74445134","<p>In ADF When you read any Json file it will read as array of Objects by default :</p>
<p><strong>Sample data While reading Json data:</strong></p>
<p><img src=""https://i.imgur.com/W4at30p.png"" alt=""enter image description here"" /></p>
<p>Data preview:</p>
<p><img src=""https://i.imgur.com/2tdCvpG.png"" alt=""enter image description here"" /></p>
<p>But when you want to move data to sink in Json format you have option called <strong>Set of objects</strong> you need to select that:</p>
<p><img src=""https://i.imgur.com/w5ZkWR4.png"" alt=""enter image description here"" /></p>
<p><strong>Sample data While storing in sink in form of Json data:</strong></p>
<p><img src=""https://i.imgur.com/3jj1K6j.png"" alt=""enter image description here"" /></p>
<p><strong>Output</strong></p>
<p><img src=""https://i.imgur.com/FPXOJKw.png"" alt=""enter image description here"" /></p>
"
"74431834","I have problem in getting data perfectly in adf","<p>Cconsider one CSV file with employee details and attendance marking attendance with 0 and 1. For example, 1 indicates the employee is present, 0 indicates employee is absent. My problem is to get the working date of employee if they are present (1). It should be the same day where the employee is absent (0). It should be next working day by reading the previous row.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>emp id</th>
<th>working</th>
<th>working day</th>
</tr>
</thead>
<tbody>
<tr>
<td>123</td>
<td>1</td>
<td>11/14/2022</td>
</tr>
<tr>
<td>123</td>
<td>0</td>
<td>11/15/2022</td>
</tr>
<tr>
<td>123</td>
<td>1</td>
<td>11/14/2022</td>
</tr>
</tbody>
</table>
</div>
<p>I have tried using data flow in ADF, but it is not getting. Please provide solution for me in Azure Data Factory.</p>
","<azure-data-factory>","2022-11-14 12:42:21","65","0","1","74530798","<p>To manipulate the csv data in data factory we have to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data Flow activity</a> in <strong>Azure data factory</strong></p>
<p>I reproduce your scenario in my environment please follow below steps to get issue resolved:</p>
<p>I took one <strong>sample file as source</strong> in <code>Data flow activity</code> similar to data you provided as below (assuming you don't have date values when employee is absent):</p>
<p><img src=""https://i.imgur.com/YoDOfP8.png"" alt=""enter image description here"" /></p>
<p>Then I took windows transformation activity <strong>over emp id and sort by emp id</strong> and created windows column <strong>working date</strong> with updating Date based on previous row value with expression:</p>
<pre><code>lag(addDays({working day}, 1),1)
</code></pre>
<p><img src=""https://i.imgur.com/uhWRshZ.png"" alt=""enter image description here"" /></p>
<p>Windows transformation data preview:</p>
<p><img src=""https://i.imgur.com/KW7SGfZ.png"" alt=""enter image description here"" /></p>
<p>Now I took derived column transformation <strong>to get date working date of employee if they are present or absent</strong>. I am updating the column working day if working is 0 then value should be working date else value will working day.</p>
<pre><code>iif(working==0,{working date},{working day})
</code></pre>
<p><img src=""https://i.imgur.com/E5rXCFv.png"" alt=""enter image description here"" /></p>
<p>Derived column transformation data preview:</p>
<p><img src=""https://i.imgur.com/1lQ6WX5.png"" alt=""enter image description here"" /></p>
<p>Now with select activity delete unnecessary columns and store the data in sink.</p>
<p>Select transformation data preview:
<img src=""https://i.imgur.com/PCeb1G5.png"" alt=""enter image description here"" /></p>
"
"74431032","Azure Data Factory Copy Files","<p>I want to copy files from one folder to another folder in data lake using ADF pipelines.
Ex : a/b/c/d.  TO a/b
Here a,b,c,d are folders here I don't want to copy c,d folders .I have to copy the files inside those folders to 'b' folder.</p>
<p>I created a pipeline using Get Metadata , For each  and in For Each I used copy activity.But here I am able to copy files with folder itself .I'm failing to remove folders.</p>
","<azure><azure-data-factory><azure-file-copy>","2022-11-14 11:39:06","219","-1","1","74434637","<p><strong>I reproduced your scenario follow the below steps:</strong></p>
<ul>
<li><p>In my demo container I have nested folders like a/b/c/d under d folder I have 3 files as below.
<img src=""https://i.imgur.com/asRPoUC.png"" alt=""enter image description here"" /></p>
</li>
<li><p>To copy files from folder to folder I took <code>Get metadata</code> activity to get list of files from folder.
<strong>Dataset for Get Metadata:</strong>
<img src=""https://i.imgur.com/Bz1vKyU.png"" alt=""enter image description here"" />
<strong>Get Metadata settings:</strong>
<img src=""https://i.imgur.com/4rkmuS5.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then I took <code>for-each</code> activity and passed the output of <code>Get Metadata</code> activities output to it.</p>
</li>
</ul>
<pre><code>    @activity('Get Metadata1').output.childItems
</code></pre>
<p><img src=""https://i.imgur.com/2FT1yNE.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then created <code>copy activity</code> inside for each activity and
<strong>created source dataset with filename parameter</strong>
<img src=""https://i.imgur.com/jfzsQ73.png"" alt=""enter image description here"" />
In file name gave dynamic value as <code>@dataset().filename</code>
<img src=""https://i.imgur.com/SJg5tH3.png"" alt=""enter image description here"" />
In copy activity source gave dynamic value for dataset property filename as <code>@item().name</code>
<img src=""https://i.imgur.com/p9CawLZ.png"" alt=""enter image description here"" />
Now created sink dataset with <code>a/b</code> directories only
<img src=""https://i.imgur.com/Nmn0K7h.png"" alt=""enter image description here"" />
and passed it to sink
<img src=""https://i.imgur.com/W5RcMn8.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Output</strong></p>
<ul>
<li>files copied under b folder without coping c and d folder
<img src=""https://i.imgur.com/a1vee5U.png"" alt=""enter image description here"" /></li>
</ul>
"
"74428479","Map Azure Blob Storage files to a custom activity in Azure Data Factory","<p>I have a container with 100 binary files. Is there a way to run a custom activity (can be a .net program or, ideally, a container) for each one of these files using Azure Data Factory?</p>
","<azure><azure-data-factory>","2022-11-14 07:58:17","80","0","1","74519231","<p>I created batch account in Azure portal
and created pool in batch account.
Image for reference:</p>
<p>I created pipeline in ADF and I created custom Activity using following details:</p>
<p><img src=""https://i.imgur.com/WMtMsmn.png"" alt=""enter image description here"" /></p>
<p>AzureBatch2:</p>
<p><img src=""https://i.imgur.com/lOJClXw.png"" alt=""enter image description here"" /></p>
<p>Here AzureBlobStorage1 is blob storage linked service where bin files are there.
AzureBlobStorage1:</p>
<p><img src=""https://i.imgur.com/kjOuXGf.png"" alt=""enter image description here"" /></p>
<p>binaryfile:</p>
<p><img src=""https://i.imgur.com/W6Y2Vrb.png"" alt=""enter image description here"" /></p>
<p>Custom1 Settings:</p>
<p><img src=""https://i.imgur.com/RP433ha.png"" alt=""enter image description here"" /></p>
<p>I set command as</p>
<pre><code>cmd
</code></pre>
<p>I started debug and it is running successfully.</p>
<p><img src=""https://i.imgur.com/nrgzqRt.png"" alt=""enter image description here"" /></p>
<p>Custom Activity created adfjobs folder in binaryfile storage account.
Image for reference:</p>
<p><img src=""https://i.imgur.com/sPVVkuq.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/u7gEWBN.png"" alt=""enter image description here"" /></p>
<p>It is working fine from my end kindly check from your end.</p>
"
"74419762","Can we fetch a single row data from csv file in ADLS using Azure data factory","<p>I need to pick a time stamp data from a column ‘created on’ from a csv file in ADLS. Later I want to query Azure SQL DB like delete from table where created on = ‘time stamp’ in ADF. Please help on how could this be achieved.</p>
","<azure><azure-sql-database><azure-storage><azure-data-factory>","2022-11-13 09:20:08","260","0","1","74428701","<p>Here I repro'd to fetch a selected row from the CSV in ADLS.</p>
<p>Create a Linked service and Dataset of the source file.
<img src=""https://i.imgur.com/AxBWEhA.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/TI3wLzI.png"" alt=""enter image description here"" /></p>
<p>Read the Data by the Lookup Activity from the Source path.
<img src=""https://i.imgur.com/5xW5ZzG.png"" alt=""enter image description here"" />
For each activity iterates the values from the output of <code>Lookup.@activity('Lookup1').output.value</code>
<img src=""https://i.imgur.com/vKv9qrZ.png"" alt=""enter image description here"" /></p>
<p>Inside of <code>For Each</code> activity use <code>Append Variable</code> and <code>set Variable</code> Use value for append variable from the For each item records.
<img src=""https://i.imgur.com/JTjjZ4Z.png"" alt=""enter image description here"" />
Using it as Index variable.
<img src=""https://i.imgur.com/EsxcEep.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/45vFmzi.png"" alt=""enter image description here"" /></p>
<p>Use script activity to run query and reflect the script on the data.
<img src=""https://i.imgur.com/ETy3BrK.png"" alt=""enter image description here"" /></p>
<pre><code>Delete FROM dbo.test_table where Created_on = @{variables('Date_COL3')[4]}
</code></pre>
<p><img src=""https://i.imgur.com/DiwrhQJ.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/AZ1FYBU.png"" alt=""enter image description here"" /></p>
"
"74419214","How to Add a conditional expression in Azure Dataflow Window Transformation","<p>How do I add an iif conditional expression in a window function under window Columns. I want to do the following:</p>
<pre><code>iif(ID&gt;5,(lag(op_hours,0)+lag(op_hours,1)+lag(op_hours,2)+lag(op_hours,3)+lag(op_hours,4)+lag(op_hours,5))/6,toDouble(4455))
</code></pre>
<p>under WindowColumns. However, the expression builder is throwing an error: &quot;Columns should be wrapped within aggregate/Window functions&quot; at <code>ID&gt;5</code>.</p>
<p>Any help would be appreciated.</p>
","<azure><azure-data-factory>","2022-11-13 07:32:26","72","0","1","74428352","<blockquote>
<p>How to Add a conditional expression in Azure Dataflow Window Transformation</p>
</blockquote>
<p>In windows transformation we cannot give column name as single entity We have to pass it with the <strong>Aggregate function</strong> or <strong>Windows function</strong></p>
<p>I tried to reproduce your scenario and getting similar error:</p>
<p><img src=""https://i.imgur.com/R5K0iAN.png"" alt=""enter image description here"" /></p>
<p><strong>To work around that I performed below steps:</strong></p>
<p>First, I created column as <code>Net Sales</code>  and gave the Lag expression to the windows Transformation To get the exact value:</p>
<p><img src=""https://i.imgur.com/8CiCDUh.png"" alt=""enter image description here"" /></p>
<p>It gave the output with Lag expression to all the values for particular window.</p>
<p><img src=""https://i.imgur.com/d2F7pN9.png"" alt=""enter image description here"" /></p>
<p>After I created a derived column with the same name and passed the conditional statement as</p>
<pre><code>iif(Id&gt;5,{ Net sales },toDouble(4455))
</code></pre>
<p>It is updating the <code>Net Sales</code> column values which have Id greater than 5 to decimal of 4455.</p>
<p><img src=""https://i.imgur.com/NUtPC2b.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/ulmnUiP.png"" alt=""enter image description here"" /></p>
"
"74418976","How to convert multiple json files to single csv in Pyspark","<p>I have multiple files in ADLS I want to convert them into single csv but without using Pandas. Is it possible to convert them using Pyspark?</p>
<p><a href=""https://i.stack.imgur.com/P108Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P108Q.png"" alt=""enter image description here"" /></a></p>
<p>These files are coming from API which has 225 000 records. I am using this script to convert it to csv</p>
<p><a href=""https://i.stack.imgur.com/sBbi3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sBbi3.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/qhBw1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qhBw1.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/0mA8L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0mA8L.png"" alt=""enter image description here"" /></a></p>
","<pyspark><azure-data-factory><azure-databricks><azure-synapse>","2022-11-13 06:36:35","179","1","1","74419320","<p>You can place all JSON files in a folder and import all using: <code>spark.read.option(&quot;multiLine&quot;, True).json(&quot;/path/to/folder&quot;)</code>.</p>
<p>However, importing JSON files into dataframe is little trickier as you may not get desired format and you may have to preprocess JSON file before import or Spark dataframe after import.</p>
<p>For example, assume JSON files per continent:</p>
<pre><code>NA.json
{
    &quot;US&quot; : {
        &quot;capital&quot;: &quot;Washington, D.C.&quot;,
        &quot;population in million&quot;: 330
    },
    &quot;Canada&quot; : {
        &quot;capital&quot;: &quot;Ottawa&quot;,
        &quot;population in million&quot;: 38
    }
}

EU.json
{
    &quot;England&quot; : {
        &quot;capital&quot;: &quot;London&quot;,
        &quot;population in million&quot;: 56
    },
    &quot;France&quot; : {
        &quot;capital&quot;: &quot;Paris&quot;,
        &quot;population in million&quot;: 67
    }
}

AUS.json
{
    &quot;Australia&quot; : {
        &quot;capital&quot;: &quot;Canberra&quot;,
        &quot;population in million&quot;: 25
    },
    &quot;New Zealand&quot; : {
        &quot;capital&quot;: &quot;Wellington&quot;,
        &quot;population in million&quot;: 5
    }
}
</code></pre>
<p>These files get imported with root JSON objects mapped to each column and nested JSON data mapped as nested map:</p>
<pre class=""lang-py prettyprint-override""><code>df = spark.read.option(&quot;multiLine&quot;, True).json(&quot;/content/sample_data/json&quot;)

+--------------+------------+------------+-----------+---------------+--------------------+
|     Australia|      Canada|     England|     France|    New Zealand|                  US|
+--------------+------------+------------+-----------+---------------+--------------------+
|{Canberra, 25}|        null|        null|       null|{Wellington, 5}|                null|
|          null|{Ottawa, 38}|        null|       null|           null|{Washington, D.C....|
|          null|        null|{London, 56}|{Paris, 67}|           null|                null|
+--------------+------------+------------+-----------+---------------+--------------------+
</code></pre>
<p>Depending on structure of your JSON files, you will have to deal with 2 things:</p>
<ul>
<li>Mapping JSON object to a generalised schema to prevent fragmented column names as above.</li>
<li>Preventing nested data to be mapped as nested map type as shown with <code>{Canberra, 25}</code> above. You may want to transform this to bring all data to tabular form.</li>
</ul>
"
"74417333","Azure Data Factory, If Activity expression with array element","<p><a href=""https://i.stack.imgur.com/SaP19.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SaP19.png"" alt=""enter image description here"" /></a></p>
<p>I have an array HeaderList with a list of names.  I have a look up activity to look at a CSV file header.  Then, I have a IF activity to compare the first element. the expression in If activity is like this:</p>
<pre><code>@equals(activity('Lookup2').output.firstRow.Prop_0,variables('HeaderList')[0])
</code></pre>
<p>That does not work.  If I change it to this:
@equals(activity('Lookup2').output.firstRow.Prop_0,'XYZ'), then it works.  How do I reference an array element in expression?</p>
<p>Thanks</p>
<pre><code>@equals(activity('Lookup2').output.firstRow.Prop_0,variables('HeaderList')[0])
</code></pre>
<p><a href=""https://i.stack.imgur.com/w86k5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w86k5.png"" alt=""enter image description here"" /></a></p>
<p>What does it mean?</p>
","<azure-data-factory>","2022-11-12 23:24:44","426","0","1","74426748","<p>I have got the same error in the if condition activity. But when the pipeline is debugged, it did not throw any error. I have repro'd the same in my ADF environment. Below are the steps.</p>
<ul>
<li><p>Lookup activity is taken, and it refers to a csv file.
<img src=""https://i.imgur.com/ckVQWHM.png"" alt=""enter image description here"" /></p>
</li>
<li><p>An array variable 'HeaderList' is taken and values for the variable is set using <strong>set variables</strong> activity.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/X4CSdW0.png"" alt=""enter image description here"" /></p>
<ul>
<li>Then <strong>If Condition</strong> activity is taken and below expression is given as a dynamic content.</li>
</ul>
<p><code>@equals(activity('Lookup1').output.firstRow.prop_0,variables('HeaderList')[0])</code></p>
<ul>
<li><p>The same error is produced.
Error: <strong>Cannot fit unknown into function parameter any</strong>.
<img src=""https://i.imgur.com/E8bkvNB.png"" alt=""enter image description here"" /></p>
</li>
<li><p>When pipeline is debugged, it did not throw any error. It is successful.
<img src=""https://i.imgur.com/Kc5EYlq.png"" alt=""enter image description here"" /></p>
</li>
</ul>
"
"74415137","How to merge multiple csv files into a single parquet file in azure data flow?","<p>I would like to merge multiple CSV files in AzureDataFlow and output them to a single Parquet file, but I am having trouble with the data source file.</p>
<p>There are two types of data source files.
One has a header and the other does not.
These files have the same column names and I would like to merge them, but I don't know how.
I want to merge the files with headers by aligning the headers to the files with headers.</p>
<p><strong>In other words,I want to combine files with headers with files without headers.</strong></p>
<p>I have already tried one method.
That is to skip the first line of a file that has a header, merge it with the header removed, and then add the header in the mapping.
However, that method requires me to manually change the mapping every time the header of the data source is changed.
This is not a smart approach.
Any answers would be appreciated.
Thank you.</p>
","<azure><azure-data-factory>","2022-11-12 17:37:31","317","0","1","74418211","<p>If you simply want to combine the 2 files, use Union with the By Position method: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-union"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-union</a>.</p>
<p>If the columns do not line-up positionally, you can rearrange the column ordering with a Select transformation.</p>
"
"74414506","Azure DataFactory Linked Service FileSystem Errornr","<p>Azure DataFactory File System Linked Service is not working with this error:</p>
<p>Error details
Error code
28051
Details
c could not be resolved.</p>
<p>I tried to connect file excel in onpremise machine using the self hosted integration runtimeg</p>
","<azure><filesystems><azure-data-factory>","2022-11-12 16:07:25","754","2","3","74425415","<p>Appears that ours auto-updated on 11/11 and it hasn't worked since then for connecting to C drive like it has.</p>
<p>But I've fixed this for myself. have change the Host from <code>c:\</code> to <code>\\servername\c$</code> and now testing the connection is successful.</p>
"
"74414506","Azure DataFactory Linked Service FileSystem Errornr","<p>Azure DataFactory File System Linked Service is not working with this error:</p>
<p>Error details
Error code
28051
Details
c could not be resolved.</p>
<p>I tried to connect file excel in onpremise machine using the self hosted integration runtimeg</p>
","<azure><filesystems><azure-data-factory>","2022-11-12 16:07:25","754","2","3","74440054","<p>I started experimenting this same issue in two different Integration Runtimes after they updated to version <code>5.22.8312.1</code>. For some reason I was unable to reach any path under <code>C:</code> from Azure Data Factory, but I was able to reach network paths.</p>
<p>I solved the issue mapping the desired local path to a network location, and making sure that the user I'm using to connect has access to that path. After that, I changed the path from <code>C:\path\to\file.csv</code> to <code>\\localhost\path\to\file.csv</code>. Make sure that this new path works on the local machine before trying in Azure Data Factory.</p>
"
"74414506","Azure DataFactory Linked Service FileSystem Errornr","<p>Azure DataFactory File System Linked Service is not working with this error:</p>
<p>Error details
Error code
28051
Details
c could not be resolved.</p>
<p>I tried to connect file excel in onpremise machine using the self hosted integration runtimeg</p>
","<azure><filesystems><azure-data-factory>","2022-11-12 16:07:25","754","2","3","74452035","<p>For those having problems with UNC share paths, you can use the relative path &quot;.\..\..\..\..\&quot; instead of &quot;C:\&quot; as well, as I wrote in this related question: <a href=""https://stackoverflow.com/questions/74394857/integration-runtime-automatically-updated-and-now-i-cant-copy-data-from-onprem/74451926#74451926"">Integration Runtime automatically updated, and now I can&#39;t copy data from onprem location</a></p>
"
"74408956","Azure Data Factory: Delete an Empty Directory Issue","<p>Within an Azure Data Factory Pipeline I am attempting to remove an empty directory.</p>
<p>The files within the directory were removed by a previous pipelines' iterative operation thus leaving an empty directory to be removed.</p>
<p>The directory is a sub-folder: The hierarchy being:</p>
<p>container / top-level-folder (always present) / directory - dynamically created - the result of an unzip operation.</p>
<p>I have defined a specific dataset that points to</p>
<p>container / @concat('top-level-folder/',dataset().dataset_folder)</p>
<p>where 'dataset_folder' is the only parameter.</p>
<p>The Delete Activity is configured like this:</p>
<p><a href=""https://i.stack.imgur.com/vbhcQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vbhcQ.png"" alt=""enter image description here"" /></a></p>
<p>On running the pipeline it errors with this error:</p>
<p><strong>Failed to execute delete activity with data source 'AzureBlobStorage' and error 'The required Blob is missing. Folder path: container/top level directory/Directory to be removed/.'. For details, please reference log file here:</strong></p>
<p>The log is an empty spreadsheet.</p>
<p>What am I missing from either the dataset or delete activity?</p>
","<azure><azure-blob-storage><azure-data-factory>","2022-11-11 22:55:31","199","0","2","74442651","<p>In azure blob storage, when all the contents inside a folder are deleted, then the folder would automatically get deleted.</p>
<ul>
<li>When I deleted each file and tried to delete the folder in the end, then I got the same error.</li>
</ul>
<p><img src=""https://i.imgur.com/EHevfIV.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>This is because the folder is deleted as soon as the files in the folders are deleted. Only when using azure data lake storage, you will have to specifically delete the folder as well.</p>
</li>
<li><p>Since the requirement is to delete the folder in azure blob storage, you can simply remove the delete activity that you are using to remove the folder.</p>
</li>
</ul>
"
"74408956","Azure Data Factory: Delete an Empty Directory Issue","<p>Within an Azure Data Factory Pipeline I am attempting to remove an empty directory.</p>
<p>The files within the directory were removed by a previous pipelines' iterative operation thus leaving an empty directory to be removed.</p>
<p>The directory is a sub-folder: The hierarchy being:</p>
<p>container / top-level-folder (always present) / directory - dynamically created - the result of an unzip operation.</p>
<p>I have defined a specific dataset that points to</p>
<p>container / @concat('top-level-folder/',dataset().dataset_folder)</p>
<p>where 'dataset_folder' is the only parameter.</p>
<p>The Delete Activity is configured like this:</p>
<p><a href=""https://i.stack.imgur.com/vbhcQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vbhcQ.png"" alt=""enter image description here"" /></a></p>
<p>On running the pipeline it errors with this error:</p>
<p><strong>Failed to execute delete activity with data source 'AzureBlobStorage' and error 'The required Blob is missing. Folder path: container/top level directory/Directory to be removed/.'. For details, please reference log file here:</strong></p>
<p>The log is an empty spreadsheet.</p>
<p>What am I missing from either the dataset or delete activity?</p>
","<azure><azure-blob-storage><azure-data-factory>","2022-11-11 22:55:31","199","0","2","74452820","<p>I used an Azure function suggested here:
<a href=""https://stackoverflow.com/questions/74447381/delete-folder-using-microsoft-windowsazure-storage-blob-blockblob-deleteasync/74449166#74449166"">Delete folder using Microsoft.WindowsAzure.Storage.Blob : blockBlob.DeleteAsync();</a></p>
<p>to perform the action.</p>
"
"74406028","Can we use different run-time in Copy Activity of Azure Data Factory?","<p>I am using Copy Activity for migrating the Data from On-premises Database to the on-cloud Database. Here I am using Self-Hosted Integration Runtime for both on-premises and on-cloud databases.</p>
<p>The Integration run-time is different for on-premises and on-cloud Databases.</p>
<p>When I execute the pipeline, it shows that both the source and target need to be in the same self-hosted integration runtime.</p>
<p>Is it possible to execute the pipeline having 2 self-hosted integration runtimes?</p>
<p>If it is possible, Please let me know how we can execute the pipeline of having different 2 self-hosted integration runtimes.</p>
","<azure><azure-data-factory><linked-service>","2022-11-11 17:15:03","140","0","2","74406528","<p>I think at this time I think this is what you have .</p>
<p>In-premise -&gt; SH IR 1</p>
<p>On-cloud Databases. -&gt; SH IR 2</p>
<p>Can you please let me know if what do you mean by &quot;on-cloud Databases&quot; ?
Anyways , you can try the two step logic , To write the data to a blob and then read from blob and write to the cloud databses .</p>
"
"74406028","Can we use different run-time in Copy Activity of Azure Data Factory?","<p>I am using Copy Activity for migrating the Data from On-premises Database to the on-cloud Database. Here I am using Self-Hosted Integration Runtime for both on-premises and on-cloud databases.</p>
<p>The Integration run-time is different for on-premises and on-cloud Databases.</p>
<p>When I execute the pipeline, it shows that both the source and target need to be in the same self-hosted integration runtime.</p>
<p>Is it possible to execute the pipeline having 2 self-hosted integration runtimes?</p>
<p>If it is possible, Please let me know how we can execute the pipeline of having different 2 self-hosted integration runtimes.</p>
","<azure><azure-data-factory><linked-service>","2022-11-11 17:15:03","140","0","2","74410849","<p>Check this article
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime</a>
You don't need a Self hosted IR to move data from on Prem to Cloud
If the source is in Onprem you can use -self hosted ir
If the database/target is in cloud -You can either use a Azure IR having a private endpoint/same self hosted ir.
Also please let me know why u want 2 self hosted ir here</p>
"
"74403020","Azure Data Factory querying Mongo DB to filter on Dates","<p>I am querying a mongoDB from ADFv2 and I am trying to filter the data based on dates. However the date appears in a random string of numbers. Just wondering if there there is a way to filter the data dynamically in the source of the copy task as I am trying to load the data in an incremental fashion.</p>
<pre><code>    [
  {
    &quot;_id&quot;: {
      &quot;$oid&quot;: &quot;5d5123dc8cf0b4453ceb2088&quot;
    },
    &quot;TemplateId&quot;: &quot;5d3ac5c77eb20a2cf4bdf46a&quot;,
    &quot;LastUpdate&quot;: {
      &quot;$date&quot;: 1565603495299
    },
    &quot;Answers&quot;: [
      {
        &quot;Question&quot;: &quot;Q001c&quot;,
        &quot;Answer&quot;: {
          &quot;_t&quot;: &quot;System.String[]&quot;,
          &quot;_v&quot;: [
            &quot;0,In person&quot;
          ]
        }
      },
      {
        &quot;Question&quot;: &quot;Q001a&quot;,
        &quot;Answer&quot;: {
          &quot;_t&quot;: &quot;System.String[]&quot;,
          &quot;_v&quot;: [
            &quot;0,Yes&quot;
          ]
        }
      },
      {
        &quot;Question&quot;: &quot;Q003a&quot;,
        &quot;Answer&quot;: {
          &quot;_t&quot;: &quot;System.String[]&quot;,
          &quot;_v&quot;: [
            &quot;0,Yes&quot;
          ]
        }
      },
      {
        &quot;Question&quot;: &quot;Q006a&quot;,
        &quot;Answer&quot;: {
          &quot;_t&quot;: &quot;System.String[]&quot;,
          &quot;_v&quot;: [
            &quot;1,No&quot;
          ]
</code></pre>
<p>The SInk/Destination is into a SQL database. Any help will be greatly appreciated.</p>
","<sql><azure><azure-data-factory>","2022-11-11 13:12:05","224","0","1","74428748","<p>I repro'd the incremental loading of data from MongoDb to SQL DB using ADF. As @Nick.Mc.Dermaid suggested, max of timestamp value should be given as watermark value for incremental data load.  Approach that followed here is to copy all the data with date greater than watermark value and to update the watermark value with max of date.  So that in next pipeline run, delta data will be copied. Below are the detailed steps.</p>
<ul>
<li><p>In MongoDB API, two documents are inserted into a container.
<img src=""https://i.imgur.com/2JjbwYm.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Then In SQL database, Watermark table is created as in below image with <code>watermark value =1000000000000</code>.
<img src=""https://i.imgur.com/MuKBX5r.png"" alt=""enter image description here"" />
The value for watermark is set as above, so that in first run all data from source gets loaded into sink.</p>
</li>
<li><p>A stored procedure is written in SQL database to update the watermark table with latest date value.</p>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>create proc usp_update_watermark_table as
begin
update watermark
set watermarkvalue=(select max(LastUpdate) from tgt_table)
end
</code></pre>
<ul>
<li>In ADF, Lookup activity is taken and watermark table is referred in that.
<code>select WatermarkValue from watermark</code></li>
</ul>
<p><img src=""https://i.imgur.com/EEI3tn5.png"" alt=""enter image description here"" /></p>
<ul>
<li>Copy activity is taken next to lookup activity. In Source dataset, MongoDB API is taken and in filter , below expression is given to copy the data that are greater than the value from lookup activity.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>{&quot;LastUpdate&quot;:{$gt:@{activity('LookupLastWaterMark').output.firstRow.WatermarkValue}}}
</code></pre>
<p><img src=""https://i.imgur.com/pbpy52Q.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>Stored procedure activity is added next to copy activity. So that new value gets updated in watermark table.
<img src=""https://i.imgur.com/pqTYuj8.png"" alt=""enter image description here"" /></p>
</li>
<li><p>After pipeline run, the output table and watermark table is updated.
<img src=""https://i.imgur.com/wga40Cp.png"" alt=""enter image description here"" /></p>
</li>
<li><p>New Document is added in source
<img src=""https://i.imgur.com/fzgtodK.png"" alt=""enter image description here"" /></p>
</li>
<li><p>When pipeline is triggered again, only delta record got loaded to sink.</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/CnlF4ac.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/zOxIFYS.png"" alt=""enter image description here"" /></p>
"
"74398812","Need to do an incremental load using ADF. Source is a csv from ADLS and Sink is Azure SQL","<p>I am trying to do an incremental data load to Azure sql from csv files in ADLS through ADF. The problem I am facing is Azure SQL would generate the primary key column (ID) and the data would be inserted to Azure SQl. But when the pipeline is re triggered the data would be duplicated again. So how do I handle these duplicates ? Because only incremental load should be updated everytime but since primary key column is generated by SQL there would be duplicates every run. Please help !!</p>
","<azure><azure-sql-database><azure-data-factory>","2022-11-11 06:33:50","205","0","1","74442386","<blockquote>
<p>You can consider comparing source and sink data first by excluding
Primary key column and then filter that rows which modified and take
it to sink table.</p>
</blockquote>
<p>In below video I created a hash on top of few columns from source and sink and compared them to identify changed data. Same way you can consider checking the changed data first and then load it to sink table.
<a href=""https://www.youtube.com/watch?v=i2PkwNqxj1E"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=i2PkwNqxj1E</a></p>
"
"74397171","How to add Broker Properties header to External Call Transformation on Data Factory","<p>I have an issue in Data Factory's external call.  I need to send data to a service bus, which requires a SessionId to be specified in the headers.  However it is in the BrokerProperties as specified here: <a href=""https://learn.microsoft.com/en-us/rest/api/servicebus/Message-Headers-and-Properties?redirectedfrom=MSDN"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/servicebus/Message-Headers-and-Properties?redirectedfrom=MSDN</a></p>
<p>Using the mapping data flow I created an external call transformation with payload already tested and working fine.  However in the Additional Headers parameters I am required to give what looks like:</p>
<pre><code>BrokerProperties:  { &quot;SessionId&quot;: &quot;&lt;some value&gt;&quot; }
</code></pre>
<p>I tried to assemble this in the Additional Headers in the External Call Transformation:</p>
<p><a href=""https://i.stack.imgur.com/4hiYg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4hiYg.png"" alt=""enter image description here"" /></a></p>
<p>The code of these is (NAITNumber is a column coming through, simple number):</p>
<pre><code>Name: 'BrokerProperties:{SessionId:' 
Value: concat(toString(NAITNumber),'}')
</code></pre>
<p>Which gives me the error: <em>The value '{SessionId: 116}' of the HTTP header 'BrokerProperties' is invalid.</em></p>
<p>So it seems I need the double quotes around the key value pairs, so tried it with:</p>
<pre><code>Name: '&quot;BrokerProperties&quot;:{&quot;SessionId&quot;'
Value: concat('&quot;',toString(NAITNumber),'&quot;}')
</code></pre>
<p>And I get the error: <em>Bad Request - Invalid Header
HTTP Error 400. The request has an invalid header name.</em></p>
<p>Anyone else know how to specify the sessionId so Service Bus is happy?</p>
","<azure-data-factory><azureservicebus>","2022-11-11 01:37:08","110","0","1","74397373","<p>OK, this will be useful for others using the mapping data flows External Call to post to Azure Service Bus.  Actually its also useful for passing any additional headers to anything else, that also requires a Key Value pair within a Header value:</p>
<p>The initial header value doesn't need to be in double quotes. So to provide an additional Key Value paired Header you do this in the &quot;Additional Headers&quot; section under the Advanced drop down of the External Call Transformation (the &quot;Name:&quot; and &quot;Value:&quot; denote which box you are entering data into):</p>
<pre><code>Name: 'AdditionalHeader:{&quot;ActualName&quot;'
Value: concat('&quot;',toString(ActualValue),'&quot;}')
</code></pre>
<p>I figured this out after a lot of trail and error, in my case it was:</p>
<pre><code>Name: 'BrokerProperties:{&quot;SessionId&quot;'
Value: concat('&quot;',toString(NAITNumber),'&quot;}')
</code></pre>
"
"74396387","Azure Data Factory Counting number of Files in Folder","<p>I am attempting to determine if a folder is empty.</p>
<p>My current method involves using a GetMeta shape and running the following to set a Boolean.</p>
<p><strong>@greater(length(activity('Is Staging Folder Empty').output.childItems), 0)</strong></p>
<p>This works great when files are present.</p>
<p>When the folder is empty (a state I want to test for) I get</p>
<p>&quot;The required Blob is missing&quot;.</p>
<p>Can I trap this condition?</p>
<p>What alternatives are there to determine if a folder is empty?</p>
","<azure><azure-data-factory>","2022-11-10 23:18:04","338","0","2","74397748","<p>I have reproduced the above and got same error.</p>
<p><img src=""https://i.imgur.com/zDWFzwg.png"" alt=""enter image description here"" /></p>
<p>This error occurs when the folder is empty, and the <strong>source is a Blob storage</strong>. You can see it is working fine for me when the Source is ADLS.</p>
<p><img src=""https://i.imgur.com/dMmPxQd.png"" alt=""enter image description here"" /></p>
<p>for sample I have used set variable.</p>
<p><img src=""https://i.imgur.com/EiwpW66.png"" alt=""enter image description here"" /></p>
<p>inside false of if.</p>
<p><img src=""https://i.imgur.com/Pkxx5ZM.png"" alt=""enter image description here"" /></p>
<p><strong>if folder is empty:</strong></p>
<p><img src=""https://i.imgur.com/9lFTb1E.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>Can I trap this condition?</p>
<p>What alternatives are there to determine if a folder is empty?</p>
</blockquote>
<p>One alternative can be to use ADLS instead of Blob storage as source.</p>
<p><strong>(or)</strong></p>
<p>You can do like below, if you want to avoid this error with Blob storage as source. Give an <strong>if activity</strong> for the <strong>failure of Get Meta</strong> and check the error in the expression.</p>
<pre><code>@startswith(string(activity('Get Metadata1').error.message), 'The required Blob is missing')
</code></pre>
<p><img src=""https://i.imgur.com/fSD4OIh.png"" alt=""enter image description here"" /></p>
<p><strong>In True activities (required error and folder is empty)</strong> I have used set variable for demo.</p>
<p><img src=""https://i.imgur.com/HpV9JG1.png"" alt=""enter image description here"" /></p>
<p><strong>In False activities (If any other error apart from the above occurs) use Fail activity</strong> to fail the pipeline.</p>
<p><strong>Fail Message:</strong>    <code>@string(activity('Get Metadata1').error.message)</code></p>
<p><img src=""https://i.imgur.com/bEqlqQA.png"" alt=""enter image description here"" /></p>
<p><strong>For success of Get Meta activity, there is no need to check the count of child Items</strong> because Get Meta data fails if the folder is empty. So, on success Go with your activities flow.</p>
"
"74396387","Azure Data Factory Counting number of Files in Folder","<p>I am attempting to determine if a folder is empty.</p>
<p>My current method involves using a GetMeta shape and running the following to set a Boolean.</p>
<p><strong>@greater(length(activity('Is Staging Folder Empty').output.childItems), 0)</strong></p>
<p>This works great when files are present.</p>
<p>When the folder is empty (a state I want to test for) I get</p>
<p>&quot;The required Blob is missing&quot;.</p>
<p>Can I trap this condition?</p>
<p>What alternatives are there to determine if a folder is empty?</p>
","<azure><azure-data-factory>","2022-11-10 23:18:04","338","0","2","74397790","<p>An alternative would be</p>
<p>Blob:
<a href=""https://i.stack.imgur.com/wuEcM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wuEcM.png"" alt=""enter image description here"" /></a></p>
<p>Dataset :
<a href=""https://i.stack.imgur.com/Pn4bq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pn4bq.png"" alt=""enter image description here"" /></a></p>
<p>where test is the container and test is the folder inside the container which I am trying to scan (Which ideally doesnt exist as seen above)</p>
<p>Use get meta data activity to check if the folder exists :
<a href=""https://i.stack.imgur.com/l0yMz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l0yMz.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ea8Nx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ea8Nx.png"" alt=""enter image description here"" /></a></p>
<p>If false, exit else count for the files</p>
"
"74394857","Integration Runtime automatically updated, and now I can't copy data from onprem location","<p>recently my Microsoft self hosted integration runtime automatically updated and now I can't pull data from my onprem folder and transfer it to a blob storage. The error code I receive is</p>
<p>Error Code  28051</p>
<p>Details     d could not be resolved.
Activity ID: d999e0c0-cb2c-4161-aad5-e01510ca7e8f</p>
<p>Has this happened to any else before?</p>
<p><a href=""https://i.stack.imgur.com/31TSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31TSu.png"" alt=""Error Code Image"" /></a></p>
","<azure-data-factory>","2022-11-10 20:16:51","1207","1","7","74395264","<p>Update the selfhosted IR from here : <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=39717"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/download/details.aspx?id=39717</a> and see if that mitigate the issues .</p>
<p>if you are on Windows like me . event logs should give you more info .
<a href=""https://i.stack.imgur.com/3AYe9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3AYe9.png"" alt=""enter image description here"" /></a></p>
"
"74394857","Integration Runtime automatically updated, and now I can't copy data from onprem location","<p>recently my Microsoft self hosted integration runtime automatically updated and now I can't pull data from my onprem folder and transfer it to a blob storage. The error code I receive is</p>
<p>Error Code  28051</p>
<p>Details     d could not be resolved.
Activity ID: d999e0c0-cb2c-4161-aad5-e01510ca7e8f</p>
<p>Has this happened to any else before?</p>
<p><a href=""https://i.stack.imgur.com/31TSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31TSu.png"" alt=""Error Code Image"" /></a></p>
","<azure-data-factory>","2022-11-10 20:16:51","1207","1","7","74400176","<p>We had the same issue. We downgraded to 5.21; this solved it for us. (also disabled auto-update for now.)</p>
<p>download at:
<a href=""https://www.microsoft.com/en-US/download/details.aspx?id=39717"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-US/download/details.aspx?id=39717</a></p>
"
"74394857","Integration Runtime automatically updated, and now I can't copy data from onprem location","<p>recently my Microsoft self hosted integration runtime automatically updated and now I can't pull data from my onprem folder and transfer it to a blob storage. The error code I receive is</p>
<p>Error Code  28051</p>
<p>Details     d could not be resolved.
Activity ID: d999e0c0-cb2c-4161-aad5-e01510ca7e8f</p>
<p>Has this happened to any else before?</p>
<p><a href=""https://i.stack.imgur.com/31TSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31TSu.png"" alt=""Error Code Image"" /></a></p>
","<azure-data-factory>","2022-11-10 20:16:51","1207","1","7","74420933","<p>Same thing here on 5.22 - upgraded to 5.23 and still a problem.
Does your northernmeats connection use a linked service pointing explicitly to the d:\ drive on the machine hosting your IR? We had the same setup (using our c:\ drive) and started getting the same error message - 'c could not be resolved'.</p>
<p>I switched it to using a UNC path like \\MACHINENAME\c$ and it immediately sprang back to life - looks like its specifically related to using the local drive name rather than a share name, so you can workaround it (relatively) easily.</p>
<p>In our case, I wanted to make it a share anyway, so I could throw another IR on another machine on the same net for resilience, a second node, and have it hit the one central folder, rather than the c drive of whichever node happened to fire.
Hopefully this helps anyone in the same boat, and means you're not stuck on 5.21</p>
"
"74394857","Integration Runtime automatically updated, and now I can't copy data from onprem location","<p>recently my Microsoft self hosted integration runtime automatically updated and now I can't pull data from my onprem folder and transfer it to a blob storage. The error code I receive is</p>
<p>Error Code  28051</p>
<p>Details     d could not be resolved.
Activity ID: d999e0c0-cb2c-4161-aad5-e01510ca7e8f</p>
<p>Has this happened to any else before?</p>
<p><a href=""https://i.stack.imgur.com/31TSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31TSu.png"" alt=""Error Code Image"" /></a></p>
","<azure-data-factory>","2022-11-10 20:16:51","1207","1","7","74429445","<p>We had the &quot;d could not be resolved&quot; issue. After being in contact with MS support about the problem, which started when the Integration Runtime was automatically updated to 5.22, they recommended upgrading to 5.23. This did not solve the problem.</p>
<p>However, they also recommended using <strong>fully qualified name</strong> in the linked service that points towards the drive location (host) instead of using a locally mapped name. In other words: <strong>don't use &quot;D:\yourfolder&quot; but &quot;\\servername\D\yourfolder&quot;</strong>. Kind of what &quot;PaulE&quot; also pointed out, I just wanted to highlight that this is also what MS recommends in this case.</p>
<p>Apparently, version 5.22 was the first version where using colon in the path was a problem. Next time, please tell us ahead of such a change, MS ;-)</p>
"
"74394857","Integration Runtime automatically updated, and now I can't copy data from onprem location","<p>recently my Microsoft self hosted integration runtime automatically updated and now I can't pull data from my onprem folder and transfer it to a blob storage. The error code I receive is</p>
<p>Error Code  28051</p>
<p>Details     d could not be resolved.
Activity ID: d999e0c0-cb2c-4161-aad5-e01510ca7e8f</p>
<p>Has this happened to any else before?</p>
<p><a href=""https://i.stack.imgur.com/31TSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31TSu.png"" alt=""Error Code Image"" /></a></p>
","<azure-data-factory>","2022-11-10 20:16:51","1207","1","7","74451926","<p>We are experiencing the same issue. Accessing local drives C: and D: is not possible anymore since the update to 5.22.</p>
<p>Using the network share path, e.g. \\localhost\c$\, wasn't a solution either. We were accessing many files within a for-loop in parallel and then got errors regarding &quot;max connection limit reached&quot;.</p>
<p>Our current workaround is using a relative path.
The SHIR process has its working directory in &quot;C:\Program Files\Microsoft Integration Runtime\5.0\Shared\&quot;. Therefore replacing &quot;C:\&quot; with &quot;.\..\..\..\..\&quot; was the solution for us.</p>
<p>This won't work for other drive letters than C:, though.</p>
"
"74394857","Integration Runtime automatically updated, and now I can't copy data from onprem location","<p>recently my Microsoft self hosted integration runtime automatically updated and now I can't pull data from my onprem folder and transfer it to a blob storage. The error code I receive is</p>
<p>Error Code  28051</p>
<p>Details     d could not be resolved.
Activity ID: d999e0c0-cb2c-4161-aad5-e01510ca7e8f</p>
<p>Has this happened to any else before?</p>
<p><a href=""https://i.stack.imgur.com/31TSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31TSu.png"" alt=""Error Code Image"" /></a></p>
","<azure-data-factory>","2022-11-10 20:16:51","1207","1","7","74601778","<p>Microsoft has now(2022-11-25) released documentation update and posted health event to affected users.</p>
<blockquote>
<p>Copying files from local machine is not supported under Azure Integration Runtime.
Refer to the command line from here to enable the access to the local machine under Self-hosted integration runtime. By default, it's disabled. (1)</p>
</blockquote>
<p>To enable local disk access there is two solution:</p>
<ul>
<li>Use UNC and Windows' builtin file shares. (@PaulE already gave this answer)</li>
<li>To enable old behaviour running <code>dmgcmd.exe -DisableLocalFolderPathValidation</code> should enable old behaviour (2)(3)</li>
</ul>
<hr />
<p>(1) <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#sample-linked-service-and-dataset-definitions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#sample-linked-service-and-dataset-definitions</a></p>
<p>(2) <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#set-up-an-existing-self-hosted-ir-via-local-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#set-up-an-existing-self-hosted-ir-via-local-powershell</a></p>
<p>(3) Health event:</p>
<blockquote>
<p>We have important information regarding your Azure Data Factory V2 resources.</p>
</blockquote>
<blockquote>
<p>We have introduced a new security control for the Self-hosted Integration Runtime (SHIR) admin that lets them allow or disallow local SHIR file system access through File system connector. SHIR admins can use the local command line (dmgcmd.exe -DisableLocalFolderPathValidation/-EnableLocalFolderPathValidation) to allow or disallow.</p>
</blockquote>
<blockquote>
<p>The latest SHIR version for download can be found here: <a href=""https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_5.23.8355.1.msi"" rel=""nofollow noreferrer"">https://download.microsoft.com/download/E/4/7/E4771905-1079-445B-8BF9-8A1A075D8A10/IntegrationRuntime_5.23.8355.1.msi</a></p>
</blockquote>
<blockquote>
<p>Note: We have changed the default setting to disallow local SHIR file system access from SHIR versions (&gt;= 5.22.8297.1). Using the above command line, you should explicitly opt-out the security control and allow local SHIR file system if needed.</p>
</blockquote>
"
"74394857","Integration Runtime automatically updated, and now I can't copy data from onprem location","<p>recently my Microsoft self hosted integration runtime automatically updated and now I can't pull data from my onprem folder and transfer it to a blob storage. The error code I receive is</p>
<p>Error Code  28051</p>
<p>Details     d could not be resolved.
Activity ID: d999e0c0-cb2c-4161-aad5-e01510ca7e8f</p>
<p>Has this happened to any else before?</p>
<p><a href=""https://i.stack.imgur.com/31TSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31TSu.png"" alt=""Error Code Image"" /></a></p>
","<azure-data-factory>","2022-11-10 20:16:51","1207","1","7","74646611","<p>I don't work much with ADF, but I know it's similar to Synapse.  Here's what I did in Synapse specifically that works without making any changes to the Integration Runtime or running any commands:</p>
<p>In the Linked Service configuration, set your host to <code>\\?\</code></p>
<p><a href=""https://i.stack.imgur.com/TUt9k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TUt9k.png"" alt=""Edit Linked Service"" /></a></p>
<p>Then, in your dataset, you can specify the full path to the folder including C:, D:, etc... I parameterize everything as shown below:</p>
<p><a href=""https://i.stack.imgur.com/zEVLT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zEVLT.png"" alt=""Dataset Connection"" /></a></p>
<p><a href=""https://i.stack.imgur.com/MJWlK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MJWlK.png"" alt=""Dataset Parameters"" /></a></p>
"
"74392642","Move Entire Azure Data Lake Folders using Data Factory?","<p>I'm currently using Azure Data Factory to load flat file data from our Gen 2 data lake into Synapse database tables. Unfortunately, we receive (many) thousands of files into timestamped folders for each feed. I'm currently using Synapse external tables to copy this data into standard heap tables.</p>
<p>Since each folder contains so many files, I'd like to move (or Copy/Delete) the entire folder (after processing) somewhere else in the lake. Is there some practical way to do that with Azure Data Factory?</p>
","<azure-data-factory><azure-synapse><azure-data-lake-gen2>","2022-11-10 16:53:36","86","0","1","74398787","<p>Yes, you can use copy activity with a wild card. I tried to reproduce the same in my environment and I got the below results:</p>
<p>First, add source dataset and select wildcard with folder name. In my scenario, I have a folder name <strong>pool</strong>.</p>
<p><img src=""https://i.imgur.com/QfEGAwk.png"" alt=""enter image description here"" /></p>
<p>Then select sink dataset with file path</p>
<p><a href=""https://i.stack.imgur.com/4veJ7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4veJ7.png"" alt=""enter image description here"" /></a></p>
<p>The pipeline run is successful. It transferred the file from one location to another location with the required name. Look at the following image for reference.</p>
<p><img src=""https://i.imgur.com/wiJd9LL.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/3q29Mpg.png"" alt=""enter image description here"" /></p>
"
"74392280","How can I set the timezone in Azure Data Factory for Oracle connection?","<p>We have an issue when we copy data from oracle to ADLS using ADF(Azure Data Factory).
The oracle DB has tables with timestamp values at European timezone. We use azure data factory to copy the data in to ADLS. The Data Factory IR (Integration Runtime) is on an on-prem VM that is in US Eastern time zone.</p>
<p>The issue is - When we copy oracle table that has timestamp (but no timezone), the ADF copy activity automatically converts the timestamp value to US Eastern Timezone. But we don’t want this to happen, we want to ingest the data as it is in the source table.</p>
<p>Example:
Data in Oracle Table - 2020-03-04T00:00:00 ( this is in CET )
Data in ADLS - 2020-03-03T19:00:00.000+0000 ( above date got converted to US EST, since there is no timezon info in Oracle table, and its being interpreted as UTC by Spark (+0000))</p>
<p><strong>Expected in ADLS -  2020-03-04T00:00:00.000+0000 (don't want timezone conversion)</strong></p>
<p>Is there a way to enforce a timezone at oracle connection level in Azure Data Factory ?</p>
<p>We tried to set property in Oracle Linked service - connection parameters ( PFA) but this had no effect on the timezone, we still got it converted to EST.
TIME_ZONE='Europe\Madrid'
TIME_ZONE='CET'</p>
<p><a href=""https://i.stack.imgur.com/3gvaN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3gvaN.png"" alt=""enter image description here"" /></a></p>
","<oracle><timezone><azure-data-factory><linked-service>","2022-11-10 16:27:38","157","0","1","74653022","<p>Timestamp is internally converted to Datetime in ADF</p>
<p><img src=""https://i.imgur.com/dDDH9RA.png"" alt=""enter image description here"" />
Image source: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle?tabs=data-factory#data-type-mapping-for-oracle"" rel=""nofollow noreferrer"">MS document</a></p>
<ul>
<li><p>Thus, In Mapping tab of copy activity, Change the datatype of source column and copy the data. Below is the approach to change type.</p>
</li>
<li><p>Click the JSON representation of the pipeline.
<img src=""https://i.imgur.com/ba0896X.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Edit the datatype in Json for column with timestamp to String (both in Source and sink).</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/RcrIEMp.png"" alt=""enter image description here"" /></p>
<ul>
<li>Once pipeline is run, data is copied into sink as in source format.</li>
</ul>
<p>Source:</p>
<p><img src=""https://i.imgur.com/fj7nsLw.png"" alt=""enter image description here"" /></p>
<p>Sink:</p>
<p><img src=""https://i.imgur.com/RsbeQUG.png"" alt=""enter image description here"" /></p>
"
"74390237","Stop ADF pipeline execution if there is no data","<p>I must stop the execution of an adf pipeline if there is no data in a table, but this should not generate an error, it should only stop the execution, is this possible?</p>
","<azure-data-factory>","2022-11-10 13:58:14","439","0","1","74390358","<p>You can use if activity wherein 1st validate whether there is any data in table, if yes then use other activities within True case else do nothing.
It would exit without any issues</p>
"
"74389338","Azure Data Factory - REST Pagination rules","<p>I'm trying to pull data from Hubspot to my SQL Server Database through an Azure Data Factory pipeline with the usage of a REST dataset. I have problems setting up the right pagination rules. I've already spent a day on Google and MS guides, but I find it hard to get it working properly.</p>
<p><a href=""https://legacydocs.hubspot.com/docs/methods/contacts/get_contacts"" rel=""nofollow noreferrer"">This</a> is the source API. I am able to connect and pull the first set of 20 rows. It gives an offset which is usable with <code>vidoffset=</code> which is returned in the body.</p>
<p>I need to return the result of vid-offset from the body to the HTTP request. Also the process needs to stop when <code>has-more</code> results in 'false'.</p>
<p><a href=""https://i.stack.imgur.com/04J1D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/04J1D.png"" alt=""These are my dataset connection details"" /></a></p>
<p><a href=""https://i.stack.imgur.com/zG6PA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zG6PA.png"" alt=""These are my copy source settings"" /></a></p>
","<rest><azure-data-factory>","2022-11-10 12:55:30","154","0","1","74400601","<p><strong>I tried to reproduce the same in my environment and I got the below results:</strong></p>
<p>First I create a linked service with this URL: <code>https://api.hubapi.com/contacts/v1/lists/all/contacts/all?hapikey=demo&amp;vidOffset</code></p>
<p><img src=""https://i.imgur.com/EqYNhHs.png"" alt=""enter image description here"" /></p>
<p>Then after I created the pagination <code>end condition rule</code> with <code>$.has-more</code> and <code>absolute URL.</code>
For demo purpose, I took sink as a storage account.</p>
<p><img src=""https://i.imgur.com/kDJQHQG.png"" alt=""enter image description here"" /></p>
<p>The pipeline run success full look at the below image for reference.</p>
<p><img src=""https://i.imgur.com/MJEDv4W.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/ipFIRT4.png"" alt=""enter image description here"" /></p>
<p>For more information refer this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest?tabs=data-factory#pagination-support"" rel=""nofollow noreferrer"">Ms Document</a></p>
"
"74385747","Single Dataflow PipeLine but Different parameter passing value getting failed","<p>I am trying to schedule the single dataflow will aggregate source to destination environment but our requirement is single data flow pipeline if we pass the parameter while trigger schedule. which means if I schedule A, i can pass the source raw folder details and other necessary information. similarly i can pass another schedule trigger B, i can pass the different parameter value. is it possible but i can see the failure. please advise.</p>
<p>Trigger (A) source ---&gt; Dataflow ----&gt; stage folder ---&gt; copy activity(dynamic) ---&gt; sql(on premise)</p>
<p>Trigger (B) source ---&gt; Dataflow ----&gt; stage folder ---&gt; copy activity(dynamic) ---&gt; sql(on premise)</p>
","<azure-data-factory>","2022-11-10 07:56:05","54","0","1","74395537","<p>When you debug for say stage folder 1 as parameter  , are you setting the mapping ? Since its SQL as sink , I am assuming that you are  . Now in the debug mode pass parameter for stage folder 2 details , unless schema is same in the both the case , it will fail with the same error which you mentioned .</p>
"
"74382303","ADF_PUBLISH Branch never seems to sync with my Collab (Master branch) after publishing from Master","<p>I am looking to understand some behavior as I am setting up a CI-CD Azure Data Factory for the first time.</p>
<p>My expectation is this so far:</p>
<p>I have an Pipeline YML file sitting in Master branch that triggers upon detecting changes to ADF_PUBLISH when I hit the publish button in the UI. It succeeds and then loads those changes to the next (UAT) environment as another ADF factory.</p>
<p>In the added image though my ADF_PUBLISH branch is always both ahead and behind but the code within doesn't seem to represent that. These differences are ARM file changes that don't exist in Master... So I guess I just don't understand what is saying or if I should be concerned about it or fix it.</p>
<p><a href=""https://i.stack.imgur.com/NlVbW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NlVbW.png"" alt=""enter image description here"" /></a></p>
<p>I've synced adf_publish to master to resolve this but it just happens anyway so I stopped. I don't get errors or anything from it.</p>
","<azure-data-factory>","2022-11-09 22:57:14","142","1","1","74390278","<p>The Publish branch is not a regular branch in the sense that it does not contain the same files as your main branch. Instead, it is a JSON representation of the assets. As such, the file and folder structure is very different and a direct comparison will not be very useful. You may find this <a href=""https://stackoverflow.com/questions/69904715/azure-synapse-workspace-where-the-scripts-are-published/69905368#69905368"">answer</a> to be useful.</p>
"
"74382176","Data Factory GIT error: Git file and resource name are different","<p>Complete newbie here...  I am working in Azure Data Factory and I clicked Validate All and this error comes up:  Arm-template-parameters-definition
Git file and resource name are different.</p>
<p>I have no idea where to look or what this means.  Can anyone explain what this means and/or where to look to resolve it?  I have googled and read, but nothing is clicking right now.  There does not seem to be anything documentation relating to that error.</p>
","<git><azure-data-factory>","2022-11-09 22:38:15","125","0","1","75214531","<p>I had the same issue. The problem was that the trigger file name was not the same as the value recorded in the &quot;name&quot; parameter within the trigger.</p>
<p>e.g. If your trigger is called my_trigger.json, you need to make sure that the trigger reads:</p>
<pre><code>{&quot;name&quot;: my_trigger ...}
</code></pre>
<p>and not</p>
<pre><code>{&quot;name&quot;: something_else ...}
</code></pre>
"
"74379991","ADF Error code 2011: Required property 'connectionString' is not provided in connection properties","<p>I am trying to connect to snowflake using linked service and copy data from adls to SF using adf pipeline. I created the linked service and tested the connection. It works fine. Even the debug over the pipeline works fine. althogh when I manually try to trigger the pipeline I get &quot;Required property 'connectionString' is not provided in connection properties&quot;</p>
<p><a href=""https://i.stack.imgur.com/2jOrO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2jOrO.png"" alt=""enter image description here"" /></a></p>
<p>Thanks in advance.</p>
","<azure-data-factory><azure-adf><linked-service>","2022-11-09 18:53:29","349","0","1","74468420","<p>This looks like a configuration issue and this behavior is noticed when the linked service is configured to use parameters (linked service parameterization), or a key vault is used, and if the connection string value isn't passed to the linked serviced during runtime.</p>
<p>Since it is working in debug mode, I would recommend publishing your linked service and in case if it is parameterized or using AKV, then please make sure that the connection string value is being passed and evaluated at runtime.</p>
"
"74374933","Azure Data Factory LOOKUP possibilities","<p>I'm trying to add a lookup activity that will look up a series of values(companyIds) and insert the values into here</p>
<p>[&quot;/apiCore/api/countries&quot;,&quot;/apiCore/api/Metrics/<strong>MyLookup</strong>&quot;]</p>
<p>At present my configuration looks like the following:</p>
<p><a href=""https://i.stack.imgur.com/mdTl8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mdTl8.png"" alt=""enter image description here"" /></a></p>
<p>I was wondering if it was possible to add Lookup activity to insert the values as follows:</p>
<p><a href=""https://i.stack.imgur.com/1wXm9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1wXm9.png"" alt=""enter image description here"" /></a></p>
<p>And then enter a parameter like @activity('MyLookup').output.value to:</p>
<p>[&quot;/apiCore/api/countries&quot;,&quot;/apiCore/api/Metrics/<strong>MyLookup</strong>&quot;] so it change to:</p>
<p>[&quot;/apiCore/api/countries&quot;,&quot;/apiCore/api/Metrics/** @activity('MyLookup').output.value**&quot;]</p>
<p>Can someone let me know if the above would work? If not, do you have any suggestions?</p>
<p>I got the answer to my suggestion with the following error:</p>
<pre><code>{
    &quot;errorCode&quot;: &quot;2200&quot;,
    &quot;message&quot;: &quot;ErrorCode=HttpRequestFailedWithClientError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Http request failed with client error, status code 400 BadRequest, please check your activity settings. If you configured a baseUrl that includes path, please make sure it ends with '/'.\nRequest URL: https://pm2.preqinsolutions.com/apiCore/api/Metrics/@activity('MyLookup').output.value.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=System,'&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;dynamoCompanies&quot;,
    &quot;details&quot;: []
}
</code></pre>
<p>You will notice the error is with:</p>
<p>@activity('MyLookup').output.value</p>
<p>here:</p>
<pre><code>https://pm2.preqinsolutions.com/apiCore/api/Metrics/@activity('MyLookup').output.value.,
</code></pre>
<p>Any suggestions will be very much welcomed</p>
<p>Updated question
<a href=""https://i.stack.imgur.com/I2OSK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I2OSK.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2022-11-09 12:32:31","101","0","1","74384759","<ul>
<li><p>Using string interpolation, you can build the URL by adding the output of look up activity. Instead of using the URL as <code>https://pm2.preqinsolutions.com/apiCore/api/Metrics/@activity('MyLookup').output.value</code>, you can try the following:</p>
</li>
<li><p>I have taken a sample lookup which gives the following output:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/T6TbbsU.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have used a variable to build the URL value. I have taken the value <code>/apiCore/api/Metrics/</code> in a parameter called <code>relativeURL</code>.</li>
</ul>
<pre><code>https://pm2.preqinsolutions.com@{pipeline().parameters.relativeUrl}@{activity('Lookup1').output.value}
</code></pre>
<ul>
<li>This generates the required URL. When I call it in web activity, you can see the URL that is being passed in debug input (I don't have access to the activity fails).</li>
</ul>
<p><img src=""https://i.imgur.com/QqmkV4M.png"" alt=""enter image description here"" /></p>
"
"74371116","Extract data from ERP to SQl database in azure data factory","<p>I need to extract data from Sage ERP (which will be updated automatically every month) and transfer it to the SQL database using azure data factory. But I don't know much about ERP. Also, I can't share any data. Anyone can please help me or give me an idea about it?</p>
<p>I can't try to do anything because now I don't have any credentials If I send my client documentation for this process. They will ask me to work on their system</p>
","<azure><azure-data-factory>","2022-11-09 07:12:27","59","0","1","74468302","<p>Unfortunately, there is no out of box ADF connector available to copy data from Sage ERP.</p>
<p>As an alternative, you may explore <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-custom-activity"" rel=""nofollow noreferrer"">Custom Activity</a> or Azure function activity in ADF. To move data to/from a data store that the service does not support, or to transform/process data in a way that isn't supported by the service, you can create a Custom activity with your own data movement or transformation logic and use the activity in a pipeline. The custom activity runs your customized code logic on an Azure Batch pool of virtual machines.</p>
<p>I would also recommend you to please submit a feature request for the same in IDEAS forum: <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c</a></p>
"
"74370693","How to rename column names from lookup in ADF?","<p>I have metadata in my Azure SQL db /csv file as below which has old column name and datatypes and new column names.
<a href=""https://i.stack.imgur.com/OdIFU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OdIFU.png"" alt=""enter image description here"" /></a></p>
<p>I want to rename and change the data type of oldfieldname based on those metadata in ADF.</p>
<p>The idea is to store the metadata file in cache and use this in lookup but I am not able to do it in data flow expression builder. Any idea which transform or how I should do it?</p>
","<azure><azure-data-factory>","2022-11-09 06:25:17","509","0","1","74385261","<p>I have reproduced the above and able to change the column names and datatypes like below.</p>
<p>This is the sample csv file I have taken from blob storage which has meta data of table.</p>
<p>In your case, take care of new Data types because if we don't give correct types, it will generate error because of the data inside table.</p>
<p><img src=""https://i.imgur.com/Zon72o4.png"" alt=""enter image description here"" /></p>
<p>Create dataset and give this to lookup and don't check first row option.</p>
<p><img src=""https://i.imgur.com/TBIqnIf.png"" alt=""enter image description here"" /></p>
<p>This is my sample SQL table:</p>
<p><img src=""https://i.imgur.com/ZHNzYZA.png"" alt=""enter image description here"" /></p>
<p>Give the lookup output array to ForEach.</p>
<p><img src=""https://i.imgur.com/Yr4kF57.png"" alt=""enter image description here"" /></p>
<p>Inside ForEach use script activity to execute the script for changing column name and Datatype.</p>
<p><img src=""https://i.imgur.com/ufKT9ik.png"" alt=""enter image description here"" /></p>
<p><strong>Script:</strong></p>
<pre><code>EXEC SP_RENAME 'mytable2.@{item().OldName}', '@{item().NewName}', 'COLUMN';

ALTER TABLE mytable2
ALTER COLUMN @{item().NewName} @{item().Newtype};
</code></pre>
<p>Execute this and below is my SQL table with changes.</p>
<p><img src=""https://i.imgur.com/DtYRA8r.png"" alt=""enter image description here"" /></p>
"
"74367450","Azure Data Factory DataFlow Sink to Delta fails when schema changes (column data types and similar)","<p>We have an Azure Data Factory dataflow, it will sink into Delta. We have Owerwrite, Allow Insert options set and Vacuum = 1.
When we run the pipeline over and over with no change in the table structure pipeline is successfull.
But when the table structure being sinked changed, ex data types changed and such the pipeline fails with below error.</p>
<p>Error code: DFExecutorUserError
Failure type: User configuration issue</p>
<p>Details: Job failed due to reason: at Sink 'ConvertToDelta': Job aborted.</p>
<p><a href=""https://i.stack.imgur.com/NW0WV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NW0WV.png"" alt=""enter image description here"" /></a></p>
<p>We tried setting Vacuum to 0 and back, Merge Schema set and now, instead of Overwrite Truncate and back and forth, pipeline still failed.</p>
","<delta-lake><azure-data-factory>","2022-11-08 21:40:00","484","0","3","74370037","<p>Can you try enabling Delta Lake's schema evolution (<a href=""https://www.databricks.com/discover/diving-into-delta-lake-talks/schema-enforcement-evolution"" rel=""nofollow noreferrer"">more information</a>)?  By default, Delta Lake has schema enforcement enabled which means that the change to the source table is not allowed which would result in an error.</p>
<p>Even with overwrite enabled, unless you specify schema evolution, overwrite will fail because by default the schema cannot be changed.</p>
"
"74367450","Azure Data Factory DataFlow Sink to Delta fails when schema changes (column data types and similar)","<p>We have an Azure Data Factory dataflow, it will sink into Delta. We have Owerwrite, Allow Insert options set and Vacuum = 1.
When we run the pipeline over and over with no change in the table structure pipeline is successfull.
But when the table structure being sinked changed, ex data types changed and such the pipeline fails with below error.</p>
<p>Error code: DFExecutorUserError
Failure type: User configuration issue</p>
<p>Details: Job failed due to reason: at Sink 'ConvertToDelta': Job aborted.</p>
<p><a href=""https://i.stack.imgur.com/NW0WV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NW0WV.png"" alt=""enter image description here"" /></a></p>
<p>We tried setting Vacuum to 0 and back, Merge Schema set and now, instead of Overwrite Truncate and back and forth, pipeline still failed.</p>
","<delta-lake><azure-data-factory>","2022-11-08 21:40:00","484","0","3","74374332","<p>I created ADLS Gen2 storage account and created input and output folders and uploaded parquet file into input folder.
I created pipeline and created dataflow as below:</p>
<p><img src=""https://i.imgur.com/b0njc9t.png"" alt=""enter image description here"" /></p>
<p>I have taken Parquet file as source.
Dataflow Source:</p>
<p><img src=""https://i.imgur.com/la3Xl0P.png"" alt=""enter image description here"" /></p>
<p>Dataset of Source:</p>
<p><img src=""https://i.imgur.com/KHq4Ir4.png"" alt=""enter image description here"" /></p>
<p>Data preview of Source:</p>
<p><img src=""https://i.imgur.com/eaYyAXc.png"" alt=""enter image description here"" /></p>
<p>I created derived column to change the structure of the table.
Derived column:</p>
<p><img src=""https://i.imgur.com/yh4ssXH.png"" alt=""enter image description here"" /></p>
<p>I updated 'difficulty' column of parquet file. I changed the datatype of 'difficulty' column from long to double using below code:</p>
<pre><code>difficulty : toDouble(difficulty)
</code></pre>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/uVbCvWP.png"" alt=""enter image description here"" /></p>
<p>I updated 'transactions_len' column of parquet file. I changed the datatype of 'transactions_len' column from Integer to float using below code:</p>
<pre><code>transactions_len : toFloat(transactions_len)
</code></pre>
<p>I updated 'number' column of parquet file. I changed the datatype of 'number' column from long to string using below code:</p>
<pre><code>number : toString(number)
</code></pre>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/JGc3KZY.png"" alt=""enter image description here"" /></p>
<p>Data preview of Derived column:</p>
<p><img src=""https://i.imgur.com/HxTujKO.png"" alt=""enter image description here"" /></p>
<p>I have taken delta as sink.
Dataflow sink:</p>
<p><img src=""https://i.imgur.com/sFntMEF.png"" alt=""enter image description here"" /></p>
<p>Sink settings:</p>
<p><img src=""https://i.imgur.com/enKI6GX.png"" alt=""enter image description here"" /></p>
<p>Data preview of Sink:</p>
<p><img src=""https://i.imgur.com/YPFSAXk.png"" alt=""enter image description here"" /></p>
<p>I run the pipeline It executed successfully.</p>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/RWZljAv.png"" alt=""enter image description here"" /></p>
<p>I t successfully stored in my storage account output folder.</p>
<p>Image for reference:</p>
<p><img src=""https://i.imgur.com/Nbq022w.png"" alt=""enter image description here"" /></p>
<p>The procedure worked in my machine please recheck from your end.</p>
"
"74367450","Azure Data Factory DataFlow Sink to Delta fails when schema changes (column data types and similar)","<p>We have an Azure Data Factory dataflow, it will sink into Delta. We have Owerwrite, Allow Insert options set and Vacuum = 1.
When we run the pipeline over and over with no change in the table structure pipeline is successfull.
But when the table structure being sinked changed, ex data types changed and such the pipeline fails with below error.</p>
<p>Error code: DFExecutorUserError
Failure type: User configuration issue</p>
<p>Details: Job failed due to reason: at Sink 'ConvertToDelta': Job aborted.</p>
<p><a href=""https://i.stack.imgur.com/NW0WV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NW0WV.png"" alt=""enter image description here"" /></a></p>
<p>We tried setting Vacuum to 0 and back, Merge Schema set and now, instead of Overwrite Truncate and back and forth, pipeline still failed.</p>
","<delta-lake><azure-data-factory>","2022-11-08 21:40:00","484","0","3","74391623","<p>The source (Ingestion) was generated to azure blob with given a specific filename. Whenever we generated to source parquet files without specifying a specific filename but only a directory the sink worked</p>
"
"74363693","Azure Data Factory - Lookup Activity","<p>I'm calling a procedure using a lookup activity in Azure Data Factory.
<em><strong>NOTE:</strong></em> The reason to use Lookup here is, I wanted to store the OUTPUT parameter value from procedure into a variable in ADF for future use.</p>
<p><strong>Below works,</strong></p>
<pre><code>DECLARE @ADFOutputMsg [VARCHAR](500);
EXEC Test.spAsRunTVA @ReportDate = '2022-06-01', @OutputMsg = @ADFOutputMsg OUTPUT;
SELECT @ADFOutputMsg As OutputMsg;
</code></pre>
<p><a href=""https://i.stack.imgur.com/Jcqdz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jcqdz.png"" alt=""Works"" /></a></p>
<p><strong>But when I want to pass dynamic parameters, it doesn't like,</strong></p>
<pre><code>DECLARE @ADFOutputMsg [VARCHAR](500);
EXEC @{pipeline().parameters.SchemaName}.spAsRunTVA @ReportDate = @{substring(pipeline().parameters.FileName,8,10)}, @OutputMsg = ADFOutputMsg OUTPUT;
SELECT @ADFOutputMsg As OutputMsg;
</code></pre>
<p><a href=""https://i.stack.imgur.com/0Ejdx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Ejdx.png"" alt=""Doesn't Work"" /></a></p>
<p><strong>I also tried to keep the date As-Is and just updated SchemaName to be dynamic but I still get the error.</strong></p>
<p><a href=""https://i.stack.imgur.com/Lah1s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lah1s.png"" alt=""Change"" /></a>
<a href=""https://i.stack.imgur.com/k1jMT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k1jMT.png"" alt=""Error Message"" /></a></p>
","<azure-data-factory>","2022-11-08 15:58:22","150","0","1","74391942","<p>Please Provide single quote <code>' '</code> at your dynamic content</p>
<p><code>'@{substring(pipeline().parameters.FileName,8,10)}'</code></p>
<p><strong>I tried to reproduce similar kind of approach in my environment and I got below results:</strong></p>
<p>Use the below dynamic content in the query with lookup activity.  and also Added dynamic content with single quotes <code>' '</code></p>
<pre><code>select * from for_date where date1='@{string(pipeline().parameters.Date)}'
</code></pre>
<p><img src=""https://i.imgur.com/lUvRB7c.png"" alt=""enter image description here"" /></p>
<p>Added <strong><code>Date</code></strong> Parameter</p>
<p><img src=""https://i.imgur.com/QztxRPo.png"" alt=""enter image description here"" /></p>
<p><strong>Got this Output:</strong></p>
<p><img src=""https://i.imgur.com/pIYtmAb.png"" alt=""enter image description here"" /></p>
"
"74360838","Using contains function in Azure Data Factory Dataflow expression builder","<p>I am using Azure Data Factory in which a data flow is used, I want to split my file in to two based on a condition. I am attaching an image with 2 lines, the first one is working but I want to use more programatic approach to achieve the same output:</p>
<p><a href=""https://i.stack.imgur.com/9Weuu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Weuu.png"" alt=""enter image description here"" /></a></p>
<p>I have a column named <code>indicator</code> inside my dataset, I want to use <code>contains</code> functionality to split the data, basically having 1 file where a string value inside <code>indicator</code> column has substring <code>Weekly</code> or does not.</p>
<p>Similar to what I would use in pandas:</p>
<pre><code>df1 = df[df.indicator.str.contains('Weekly')]
df2 = df[~df.indicator.str.contains('Weekly')]
</code></pre>
<p><a href=""https://i.stack.imgur.com/O7n4D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O7n4D.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2022-11-08 12:32:55","857","0","2","74368752","<p>If you are looking for the existing of a value inside of a string scalar column, use instr().</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#instr"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#instr</a></p>
"
"74360838","Using contains function in Azure Data Factory Dataflow expression builder","<p>I am using Azure Data Factory in which a data flow is used, I want to split my file in to two based on a condition. I am attaching an image with 2 lines, the first one is working but I want to use more programatic approach to achieve the same output:</p>
<p><a href=""https://i.stack.imgur.com/9Weuu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Weuu.png"" alt=""enter image description here"" /></a></p>
<p>I have a column named <code>indicator</code> inside my dataset, I want to use <code>contains</code> functionality to split the data, basically having 1 file where a string value inside <code>indicator</code> column has substring <code>Weekly</code> or does not.</p>
<p>Similar to what I would use in pandas:</p>
<pre><code>df1 = df[df.indicator.str.contains('Weekly')]
df2 = df[~df.indicator.str.contains('Weekly')]
</code></pre>
<p><a href=""https://i.stack.imgur.com/O7n4D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O7n4D.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2022-11-08 12:32:55","857","0","2","74370993","<p>You can try the below expression as well in the Conditional split.</p>
<p><code>contains()</code> expects an array. So first split the column content to create the array and give this to contains function.</p>
<pre><code>contains(split(indicator, ' '),#item=='weekly')
</code></pre>
<p>This is my sample data.</p>
<p><img src=""https://i.imgur.com/tW8EoGB.png"" alt=""enter image description here"" /></p>
<p><strong>Conditional split:</strong></p>
<p><img src=""https://i.imgur.com/viUypOK.png"" alt=""enter image description here"" /></p>
<p><strong>Weekly data in the output:</strong></p>
<p><img src=""https://i.imgur.com/cEigdIb.png"" alt=""enter image description here"" /></p>
<p><strong>Remaining data:</strong></p>
<p><img src=""https://i.imgur.com/tpqwrws.png"" alt=""enter image description here"" /></p>
"
"74358199","ADF: Using ForEach and Execute Pipeline with Pipeline Folder","<p>I have a folder of pipelines, and I want to execute the pipelines inside the folder using a single pipeline. There will be times when there will be another pipeline added to the folder, so creating a pipeline filled with Execute Pipelines is not an option (well, it is the current method, but it's not very &quot;automate-y&quot; and adding another Execute Pipeline whenever a new pipeline is added is, as you can imagine, a pain). I thought of the ForEach Activity, but I don't know what the approach is.</p>
<p><a href=""https://i.stack.imgur.com/QeJyJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QeJyJ.png"" alt=""ADF Pipeline Folder"" /></a></p>
","<automation><azure-data-factory>","2022-11-08 09:12:14","219","1","2","74363432","<p>Folders are really just organizational structures for the code assets that describe pipelines (same for Datasets and Data Flows), they have no real substance or purpose inside the executing environment. This is why pipeline names have to be globally unique rather than unique to their containing folder.</p>
<p>Another problem you are going to face is that the &quot;Execute Pipeline&quot; activity is not very dynamic. The pipeline name has to be known as design time, and while parameter values are dynamic, the parameter names are not. For these reasons, you can't have a foreach loop that dynamically executes child pipelines.</p>
<p>If I were tackling this problem, it would be through an <a href=""https://stackoverflow.com/questions/59085000/method-to-put-alerts-on-long-running-azure-data-factory-pipeline/59290603#59290603"">external pipeline management system</a> that you would have to <a href=""https://www.youtube.com/watch?v=V8dLIIb6qGY"" rel=""nofollow noreferrer"">build yourself</a>. This is not trivial, and in your case would have additional challenges because of the folder level focus.</p>
"
"74358199","ADF: Using ForEach and Execute Pipeline with Pipeline Folder","<p>I have a folder of pipelines, and I want to execute the pipelines inside the folder using a single pipeline. There will be times when there will be another pipeline added to the folder, so creating a pipeline filled with Execute Pipelines is not an option (well, it is the current method, but it's not very &quot;automate-y&quot; and adding another Execute Pipeline whenever a new pipeline is added is, as you can imagine, a pain). I thought of the ForEach Activity, but I don't know what the approach is.</p>
<p><a href=""https://i.stack.imgur.com/QeJyJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QeJyJ.png"" alt=""ADF Pipeline Folder"" /></a></p>
","<automation><azure-data-factory>","2022-11-08 09:12:14","219","1","2","74368008","<p>I have not tried this approach but I think we can use the</p>
<ol>
<li>ADF RestAPI to get all the details of the pipelines which needs to be executed. Since the response is in JSON you can write it back to temp blob and add filter and focus on what you need .</li>
</ol>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/list-by-factory?tabs=HTTP"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/list-by-factory?tabs=HTTP</a></p>
<ol start=""2"">
<li>You can use the Create RUN API to trigger the pipeline .</li>
</ol>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run?tabs=HTTP"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run?tabs=HTTP</a></p>
<p>As Joel called out , if different pipeline has different count of paramter , it will be little messy to maintain .</p>
"
"74350357","Azure Data Factory - Triggers","<p>I want to achieve below requirement -
Say I have a trigger named <strong>RunBatchJob</strong></p>
<p><strong>Weekdays: Monday - Friday</strong>
I want to schedule this trigger to run on below times:
Recurrence -&gt; Every 5 Minutes between 06:00AM until 10:00 PM
Recurrence -&gt; Every 30 Minutes between 10:01 PM until 05:59 AM</p>
<p><strong>Weekends: Saturday, Sunday</strong>
I want to schedule this trigger to run on below times:
Recurrence -&gt; Every 10 Minutes between 06:00AM until 10:00 PM
Recurrence -&gt; Every 1 Hour between 10:01 PM until 05:59 AM</p>
<p>This used to be particularly easy on SQL server jobs, can anyone please advise me how to do it on ADF?</p>
","<azure-data-factory>","2022-11-07 17:11:12","168","0","1","74351825","<p>You can separate the triggers into 4 cases. Use a scheduled trigger for 1 week occurrence. Then you can enumerate all the combinations of trigger times in each trigger.</p>
<p>Here is the example for the 1st case(Mon-Fri Every 5 Minutes between 06:00 AM until 10:00 PM)
<a href=""https://i.stack.imgur.com/hLnKy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hLnKy.png"" alt=""enter image description here"" /></a></p>
<p>You can just repeat the idea for 2nd case(Mon-Fri Every 30 Minutes between 10:01 PM until 05:59 PM)
<a href=""https://i.stack.imgur.com/UqgE3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UqgE3.png"" alt=""enter image description here"" /></a></p>
"
"74349938","Azure Data Factory $Skiptoken (for Rest API POST Method) - Workaround?","<pre><code>I am using a Copy Data activity with a Source that is a REST API POST Method using body as a Kusto query. But while using POST method  collection only 1000 record showing as a web page limourceitation.
 
[Source][1]
[Source Dataset][1]

And Sink dataset is SQL database
The REST API endpoint looks something like this: 

{
  &quot;totalRecords&quot;: 2145,
  &quot;count&quot;: 1000,
  &quot;data&quot;: [
    {

     }
  ],
  &quot;facets&quot;: [],
  &quot;resultTruncated&quot;: &quot;false&quot;,
  &quot;$skipToken&quot;: &quot;ew0KICAiJGlkIjogIjEiLA0KICAiTWF4Um93cyI6IDEwMDAsDQogICJSb3dzVG9Ta2lwIjogMTAwMCwNCiAgIkt1c3RvQ2x1c3RlclVybCI6ICJodHRwczovL2FyZy13ZXUtdGhyZWUtc2YuYXJnLmNvcmUud2luZG93cy5uZXQiDQp9&quot;
}
</code></pre>
<p>I know through powershell this is possible Like a Do-while loop until $skipToken is null, but in ADF not sure how to perform.
Can anyone suggest a solution?</p>
","<azure-data-factory><kusto-explorer><azure-resource-graph>","2022-11-07 16:38:29","177","1","1","74385086","<blockquote>
<p>Azure Data Factory $Skiptoken (for Rest API POST Method) - Workaround?</p>
</blockquote>
<p>From the information you provided we can conclude that you want to perform pagination on Rest API Till the the <code>$Skiptoken</code> is null
to perform this Azure Data Factory, provides <code>Pagination rules</code> Where you can set the end condition for the pagination of Rest API</p>
<p><strong>Reference Images:</strong></p>
<ul>
<li><p>My sample data
<img src=""https://i.imgur.com/W97oOok.png"" alt=""enter image description here"" /></p>
</li>
<li><p>EndCondition
<img src=""https://i.imgur.com/yavoeWM.png"" alt=""enter image description here"" /></p>
</li>
</ul>
<p>Here I am providing the <strong>EndCondition</strong> like when the  <code>$.page.number</code> From the Output of Rest API Is <code>Empty</code> It Will stop that pagination.</p>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest?tabs=data-factory#pagination-support"" rel=""nofollow noreferrer"">Set end condition </a></p>
"
"74349831","Even after setting up the connect via private endpoint, Azure Data Factory remains accessible over the Internet?","<p>Even after setting up the connect via private endpoint, Azure Data Factory remains accessible over the Internet?</p>
<p><a href=""https://i.stack.imgur.com/csZIU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/csZIU.png"" alt=""enter image description here"" /></a></p>
<p>Private Connections</p>
<p><a href=""https://i.stack.imgur.com/2HCHQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2HCHQ.png"" alt=""enter image description here"" /></a></p>
<p>Private Zone Entries</p>
<p><a href=""https://i.stack.imgur.com/TBEki.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TBEki.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/wpHwW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wpHwW.png"" alt=""enter image description here"" /></a></p>
<p>It is accessible over the Internet</p>
<p><a href=""https://i.stack.imgur.com/jJl9a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jJl9a.png"" alt=""enter image description here"" /></a></p>
<p>How to restrict the Data Factory access only within the VNET?</p>
","<azure><azure-data-factory>","2022-11-07 16:29:23","308","0","2","74349923","<p>The Private Endpoint you have created is only valid for use with Self-Hosted Integration Runtimes and does not apply to the Data Factory Studio portal.</p>
<blockquote>
<p>&quot;Choose whether to connect your self-hosted integration runtime to &gt;Azure Data Factory via public endpoint or private endpoint. This &gt;applies to self-hosted integration runtime running either on premises &gt;or inside customer managed Azure virtual network&quot;</p>
</blockquote>
"
"74349831","Even after setting up the connect via private endpoint, Azure Data Factory remains accessible over the Internet?","<p>Even after setting up the connect via private endpoint, Azure Data Factory remains accessible over the Internet?</p>
<p><a href=""https://i.stack.imgur.com/csZIU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/csZIU.png"" alt=""enter image description here"" /></a></p>
<p>Private Connections</p>
<p><a href=""https://i.stack.imgur.com/2HCHQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2HCHQ.png"" alt=""enter image description here"" /></a></p>
<p>Private Zone Entries</p>
<p><a href=""https://i.stack.imgur.com/TBEki.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TBEki.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/wpHwW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wpHwW.png"" alt=""enter image description here"" /></a></p>
<p>It is accessible over the Internet</p>
<p><a href=""https://i.stack.imgur.com/jJl9a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jJl9a.png"" alt=""enter image description here"" /></a></p>
<p>How to restrict the Data Factory access only within the VNET?</p>
","<azure><azure-data-factory>","2022-11-07 16:29:23","308","0","2","74355332","<p>Unfortunately , with the current scope it is not possible to restrict the ADF access via  Vnet.
It can be accessed from anywhere but this request is currently in MSFT backlog with many employees requesting the same feature</p>
<p>a similar thread :
<a href=""https://learn.microsoft.com/en-us/answers/questions/1034267/azure-data-factory-portal-is-accesible-over-intern.html"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/1034267/azure-data-factory-portal-is-accesible-over-intern.html</a></p>
"
"74347444","Azure Data Factory Rest API - With AccountCode, APIKey & Token","<p>I have a use case wherein, I need to connect to an API for data request.</p>
<ul>
<li>The API requires a valid token to process the requests.</li>
<li>To generate the token, I have a accountCode &amp; secret key</li>
</ul>
<p><strong>Assume BaseURL as</strong>
BaseURL - <a href=""http://api.xxxxx.com/%7BaccountCode%7D/data"" rel=""nofollow noreferrer"">http://api.xxxxx.com/{accountCode}/data</a> (Value of account needs to be passed)</p>
<p>**Below script in Python/Java needs to be run to fetch the dateToken &amp; token</p>
<ul>
<li>If we use Python 3.6 or above. Below is the code -**</li>
</ul>
<p>--START-- {</p>
<pre><code>import time

import requests

from hashlib import md5

account_code = &quot;&lt;account_name&gt;&quot;
key = &quot;&lt;api_key&gt;&quot;
actual_unix_time = int(time.time_ns() / 1000) # in milliseconds
TTL = 31536000000 # for 1 year
expiration_time = actual_unix_time + TTL
base_url = &quot;https://api.xxxxx.com&quot;
url = f&quot;/{account_code}/data?fromDate=last6Hours&amp;granularity=minute&amp;type=ALL%2CVOD%2CLIVE&amp;operation=reduceJoin&amp;metrics=bufferratio&quot;
pre_url = f&quot;{url}&amp;dateToken={expiration_time}&quot;
token_generated = md5(f&quot;{pre_url}{key}&quot;.encode('utf-8'))
token_value = token_generated.hexdigest()
request_url = f&quot;{base_url}{pre_url}&amp;token={token_value}&quot;
response = requests.get(request_url)

print(response)
print(response.text)
</code></pre>
<p>} --END--</p>
<p><strong>- If we use Java. Below is the code -</strong></p>
<p>--START-- {</p>
<pre><code>var key = pm.environment.get(&quot;NPAW-API-KEY&quot;); 
var base_url = &quot;https://api.xxxxx.com&quot;;

var url = pm.request.url.toString(); 
var path = url.replace(base_url, ''); 
var pre_url = pm.variables.replaceIn(path);

var moment = require('moment'); 
var actual_unix_time = moment().unix()*1000;

var TTL = 31536000000

var expiration_time = (actual_unix_time + TTL); 
var pre_url = pre_url+&quot;&amp;dateToken=&quot;+expiration_time;

var token_generated = CryptoJS.MD5(pre_url + key).toString();

var token_value = token_generated;

var request_url = (base_url+pre_url+'&amp;token='+token_value).toString();
</code></pre>
<p>}--END--</p>
<p>Example of how the final URL - <a href=""https://api.xxxxx.com/kb-vivek/data?fromDate=today&amp;granularity=hour&amp;type=ALL,VOD,LIVE&amp;operation=reduceJoin&amp;metrics=views,playtime&amp;dateToken=1699016056000&amp;token=7a9c97a4d4f108d1d32be2f7f8d00731"" rel=""nofollow noreferrer"">https://api.xxxxx.com/kb-vivek/data?fromDate=today&amp;granularity=hour&amp;type=ALL,VOD,LIVE&amp;operation=reduceJoin&amp;metrics=views,playtime&amp;dateToken=1699016056000&amp;token=7a9c97a4d4f108d1d32be2f7f8d00731</a></p>
<p>I tried to use Postman, wherein, I could pass the above script in the Pre-Request script and set environment variables for accountCode &amp; Secret Key and I was able to achieve the result as desired.</p>
<p><strong>Question: How can I achieve this in Azure Data Factory?</strong></p>
","<azure-data-factory>","2022-11-07 13:34:43","237","0","1","74373101","<ul>
<li>To achieve the requirement, we need to use a combination of set variables and dataflows (to generate md5 hex string and store final url in a file).</li>
<li>First, I have created 4 parameters with values as shown below:</li>
</ul>
<pre><code>base_url: https://api.xxxxx.com
account_code: &lt;account_name&gt;
key: &lt;api_key&gt;
TTL: 31536000000
</code></pre>
<p><img src=""https://i.imgur.com/L8kvOUv.png"" alt=""enter image description here"" /></p>
<ul>
<li>First, I have created a variable to build <code>url</code>. I used the following dynamic content:</li>
</ul>
<pre><code>/@{pipeline().parameters.account_code}/data?fromDate=last6Hours&amp;granularity=minute&amp;type=ALL%2CVOD%2CLIVE&amp;operation=reduceJoin&amp;metrics=bufferratio
</code></pre>
<p><img src=""https://i.imgur.com/88rXYLS.png"" alt=""enter image description here"" /></p>
<ul>
<li>Next, I have built the <code>pre_url</code> with the following dynamic content:</li>
</ul>
<pre><code>@{variables('url')}&amp;dateToken=@{add(div(sub(ticks(utcNow()), ticks('1970-01-01')),10),31536000000)}
</code></pre>
<p><img src=""https://i.imgur.com/wnCza67.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, to encode the string and convert it to md5 hex string, I have used dataflow. I have passed <code>base_url, pre_url and key</code> to the dataflow from the pipeline.</li>
</ul>
<p><img src=""https://i.imgur.com/xEcnbud.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have taken a sample csv file with only one row from blob storage (the data in this file does not matter but make sure it has 1 row only).</li>
</ul>
<p><img src=""https://i.imgur.com/UOaAX7Z.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have created a derived column to create final URL by concatenating <code>base_url, pre_url and encoded md5 hex string</code>. Use the following content:</li>
</ul>
<pre><code>$base_url+$pre_url+'&amp;token='+md5(encode(concat($pre_url,$key)))
</code></pre>
<p><img src=""https://i.imgur.com/p0p0B85.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now I am writing this data to a file by using <code>output to single file</code> option in the sink settings.</li>
</ul>
<p><img src=""https://i.imgur.com/FF8E8kW.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I debug the pipeline, the file will be written to my storage account. The contents of the file will be as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/mgFbzz8.png"" alt=""enter image description here"" /></p>
<p><strong>NOTE:</strong></p>
<ul>
<li>Now since you want to generate date token once and use it for a year, I have written the data to a file.</li>
<li>You run this pipeline once and generate a file with required URL (as above). Anytime you want to access this URL, you can use <code>look up</code> activity to access the URL anywhere required.</li>
<li>I have used <code>utcNow()</code> to generate <code>dateToken</code>. But if you have any specific date in mind, you can simply use that in the correct format (in place of utcNow()).</li>
</ul>
"
"74347246","Selecting the last element after split in Azure Data Factory","<p>I am trying to select the last string after splitting it in Azure Data Factory.</p>
<p>My file name looks like this:</p>
<p><code>s = &quot;cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv&quot;</code></p>
<p>With Python I would use <code>s.split('/')[-1]</code> to get the last element, according to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#last"" rel=""nofollow noreferrer"">Microsoft documentation</a> I can use <code>last</code> to achieve this, so I've tried this in the sink database Pipeline expression builder:</p>
<p><code>@last(split(dataset().fileName, '/'))</code></p>
<p>Which gives me a red underline stating:</p>
<blockquote>
<p>Cannot fit string list item into the function parameter string</p>
</blockquote>
<p>However, after running the pipeline I get what I desire, the file named <code>hospital_admissions.csv</code> placed in the folder I want it to go, so my question is if I am chaining the functions correctly &amp; why am I having the error with a working code?</p>
","<azure><azure-data-factory>","2022-11-07 13:18:17","663","0","2","74347418","<p>you can do the following:
Create 2 Variables:
Variable 1 (Array):</p>
<pre><code>@split('cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv', '/')
</code></pre>
<p>Variable 2 (String):</p>
<pre><code>@last(variables('test'))
</code></pre>
<p><a href=""https://i.stack.imgur.com/tBhR6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tBhR6.png"" alt=""OutputVariable1"" /></a></p>
<p><a href=""https://i.stack.imgur.com/PuafR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PuafR.png"" alt=""OutputVariable2"" /></a></p>
<p><a href=""https://i.stack.imgur.com/1vgCg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1vgCg.png"" alt=""VariableDeclaration"" /></a></p>
"
"74347246","Selecting the last element after split in Azure Data Factory","<p>I am trying to select the last string after splitting it in Azure Data Factory.</p>
<p>My file name looks like this:</p>
<p><code>s = &quot;cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv&quot;</code></p>
<p>With Python I would use <code>s.split('/')[-1]</code> to get the last element, according to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#last"" rel=""nofollow noreferrer"">Microsoft documentation</a> I can use <code>last</code> to achieve this, so I've tried this in the sink database Pipeline expression builder:</p>
<p><code>@last(split(dataset().fileName, '/'))</code></p>
<p>Which gives me a red underline stating:</p>
<blockquote>
<p>Cannot fit string list item into the function parameter string</p>
</blockquote>
<p>However, after running the pipeline I get what I desire, the file named <code>hospital_admissions.csv</code> placed in the folder I want it to go, so my question is if I am chaining the functions correctly &amp; why am I having the error with a working code?</p>
","<azure><azure-data-factory>","2022-11-07 13:18:17","663","0","2","74357235","<p>The pipeline expression builder might be recognizing the value generated by <code>split(dataset().fileName,'/')</code> as an array. Hence the message <strong>Cannot fit string list item into the function parameter string</strong>.</p>
<p><img src=""https://i.imgur.com/OhAYWc1.png"" alt=""enter image description here"" /></p>
<ul>
<li>However, while executing, it is giving the expected output.</li>
<li>To make it so that the warning <code>Cannot fit string list item into the function parameter string</code> is not shown, you can use the result of <code>split</code> along with <code>array</code> function. You can still chain the function using the following dynamic content and get expected result:</li>
</ul>
<pre><code>@last(array(split(dataset().filename,'/')))
</code></pre>
<p><img src=""https://i.imgur.com/MsVCYUS.png"" alt=""enter image description here"" /></p>
"
"74346245","Get runOutput from python activity (not notebook activity) in ADF V2","<p>I need to receive the runOutput from python activity in ADF.
In Notebook activity we can receive it using <code> @ {activity ('databricks notebook activity name').output.runOutput}</code>.
But same thing when I am trying to do using python activuty, it is not working even though I am exiting notebook with <code> dbutils.notebook.exit('my message)</code>.</p>
<p>Any help would be appreciated.
Thanks.</p>
","<python><azure><azure-databricks><azure-data-factory>","2022-11-07 11:59:07","170","0","1","74356462","<p>I have reproduced the above and not able to get the <code>runOutput</code> of Python script like we get in Notebook activity.</p>
<p><code>dbutils.notebook.exit('my message')</code></p>
<p>I have used the above script in my <code>.py</code> file.</p>
<p><img src=""https://i.imgur.com/CbfrSGD.png"" alt=""enter image description here"" /></p>
<p>As per my repro, <code>runOutput</code> might not be there for python activity. You can try this workaround where I am able to get the Output from <a href=""https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/2.0/jobs#request-7"" rel=""nofollow noreferrer"">REST API job</a> runs.</p>
<p>First extract the particular run id of the python activity from the above output.</p>
<pre><code>@string(last(split(activity('Python1').output.runPageUrl,'/')))
</code></pre>
<p><img src=""https://i.imgur.com/zrQ3aIG.png"" alt=""enter image description here"" /></p>
<p>Then use web activity to get the details of that particular run with Bearer token as authorization.</p>
<p><img src=""https://i.imgur.com/2KOM02u.png"" alt=""enter image description here"" /></p>
<p><strong>URL:</strong> <code>https://&lt; databricks-instance &gt;/api/2.0/jobs/runs/get-output?run_id=@{variables('run_id')}</code></p>
<p><strong>For Header:</strong> give name as <code>Authorization</code> and for value give <code>Bearer &lt; your databricks bearer token&gt;</code></p>
<p>In the logs of the above web activity output, you can find the python activity output.</p>
<pre><code>@string(activity('Web1').output.logs)
</code></pre>
<p><img src=""https://i.imgur.com/lpb89Rr.png"" alt=""enter image description here"" /></p>
<p><strong>Output:</strong></p>
<p><img src=""https://i.imgur.com/1JXU1q0.png"" alt=""enter image description here"" /></p>
"
"74346239","Dataflow is failing when the File Path is provided with more than one directory level","<p>I have created a dataflow in ADF and in the File Path variable of the dataset, i am passing the directory structure where my input files are present over Azure containers.</p>
<p>When this directory structure comprises of only one level(For Eg: source ), then everything works fine. But when this directory structure has more than one value(For Eg: source/database_files) then i start getting error. Below is the error that i receive:
{&quot;message&quot;:&quot;Job failed due to reason: at Source 'input': abfss://source/database_files@stgresourcedfs.core.windows.net/ has invalid authority</p>
","<azure><azure-data-factory>","2022-11-07 11:58:46","40","0","1","74359569","<p>Here I repro'd to pass multilevel directory by the parameterization.</p>
<p>Create linked service and dataset with the parameter.
<img src=""https://i.imgur.com/NwQtODR.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/5B5X5fn.png"" alt=""enter image description here"" /></p>
<p>Provide file path dynamically from the dataset parameters.</p>
<p><strong>In the Container path it cannot accept <code>'/'</code> so we need to parameterize the path.</strong>
<img src=""https://i.imgur.com/lbGxBiS.png"" alt=""enter image description here"" /></p>
<p>We are able to preview data of the dataset.
<img src=""https://i.imgur.com/BZ1DVIH.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/7HlKfT3.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/KDM6JCG.png"" alt=""enter image description here"" /></p>
"
"74345205","push data using REST API as source data to Azure Data factory pipeline","<p>azure experts out there,</p>
<p>I have a specific requirement to be able to push data to the Azure Data Factory pipeline as the source. I understand the common pattern is to let ADF Pipeline pull data from the source REST API on schedule and push data to Sink like Blob storage</p>
<p>REST API(Source) &lt;---- ADF ----&gt; Blob (Sink) - Common Pattern where ADF Actively pulls from the source and pushes to sink (Data flow is Triggered based on schedule)</p>
<p>REST API(Source) ----&gt; ADF ----&gt; Blob (Sink) - Required flow where Client API Push data to ADF and ADF write data to sink (Data flow is triggered based on client API instantly)</p>
<p>Any pointers are much appreciated.</p>
","<api><rest><azure-data-factory>","2022-11-07 10:33:59","315","0","1","74350722","<p>There is a REST API for the Data Factory, here is the link to the documentation for the endpoint that creates a Pipeline run</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run?tabs=HTTP"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/create-run?tabs=HTTP</a></p>
<p>There are Data Factory triggers for Storage Events or Event Grid if you prefer to have your REST API use those.</p>
"
"74343091","How to extract date from file and make it as a folder in ADF","<p>I have a file name abcd_050420211200.csv</p>
<p>My requirement is I can to extract date part from filename and make it as a folder structure
ex:2021/04/05/abcd_050420211200.csv.</p>
<p>I have extracted date part from file using split but result is in 050420211200 I was not able to make it in proper date format and making them in a folder format</p>
","<azure-data-factory><azure-databricks><azure-data-lake-gen2>","2022-11-07 07:14:24","240","0","1","74344219","<p>Took set variable activity Splitting filename where it finds underscore,</p>
<pre><code>@split(item().name,'_')[1]
</code></pre>
<p><img src=""https://i.imgur.com/0qrJnS1.png"" alt=""enter image description here"" /></p>
<p>Took another set variable activity and splitting the output of above activity as date with index number.</p>
<pre><code>@substring(variables('infilename'),0,8)
</code></pre>
<p><img src=""https://i.imgur.com/QBK0gJO.png"" alt=""enter image description here"" /></p>
<p>Took another set variable activity and further spllitting the output of previous activity to day, month and year wise as followes:</p>
<pre><code>for Day: @substring(variables('datesplit'),0,2)
for Month: @substring(variables('datesplit'),2,2)
for Year: @substring(variables('datesplit'),4)
</code></pre>
<p><img src=""https://i.imgur.com/xTA1wgq.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/uo4jISK.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/U9aZEkU.png"" alt=""enter image description here"" /></p>
<p>Now concat cat all three values with the <code>/</code> sign <strong>value3 is year variable, value2 is month variable, value1 is day variable.</strong></p>
<pre><code>@concat(variables('value3'),'/',variables('value2'),'/',variables('value1'))
</code></pre>
<p><img src=""https://i.imgur.com/a1gbHl4.png"" alt=""enter image description here"" /></p>
<p>and now pass this value to sin by creating parameter</p>
<p>create parameters to sink dataset.
<img src=""https://i.imgur.com/70zxppg.png"" alt=""enter image description here"" /></p>
<p>Pass dynamic values of parameters created to filename and folder name
<img src=""https://i.imgur.com/URsXYyp.png"" alt=""enter image description here"" /></p>
<p>now pass the date variable to folder name dataset property
<img src=""https://i.imgur.com/x6PfGQy.png"" alt=""enter image description here"" /></p>
<p><strong>OUTPUT</strong>
<img src=""https://i.imgur.com/Ddy1HjD.png"" alt=""enter image description here"" /></p>
"
"74332030","Azure Data Factory Copy Data activity - Use variables/expressions in mapping to dynamically select correct incoming column","<p>I have the below mappings for a Copy activity in ADF:</p>
<pre><code>&quot;translator&quot;: {
            &quot;type&quot;: &quot;TabularTranslator&quot;,
            &quot;mappings&quot;: [
                {
                    &quot;source&quot;: {
                        &quot;path&quot;: &quot;$['id']&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;TicketID&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;path&quot;: &quot;$['summary']&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;TicketSummary&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;path&quot;: &quot;$['status']['name']&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;TicketStatus&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;path&quot;: &quot;$['company']['identifier']&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;CustomerAccountNumber&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;path&quot;: &quot;$['company']['name']&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;CustomerName&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;path&quot;: &quot;$['customFields'][74]['value']&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Landlord&quot;
                    }
                },
                {
                    &quot;source&quot;: {
                        &quot;path&quot;: &quot;$['customFields'][75]['value']&quot;
                    },
                    &quot;sink&quot;: {
                        &quot;name&quot;: &quot;Building&quot;
                    }
                }
            ],
            &quot;collectionReference&quot;: &quot;&quot;,
            &quot;mapComplexValuesToString&quot;: false
        }
</code></pre>
<p>The challenge I need to overcome is that the array indexes of the custom fields of the last two sources might change. So I've created an Azure Function which calculates the correct array index. However I can't work out how to use the Azure Function output value in the source path string - I have tried to refer to it using an expression like <code>@activity('Get Building Field Index').output</code> but as it's expecting a JSON path, this doesn't work and produces an error:</p>
<p>JSON path $['customFields'][@activity('Get Building Field Index').outputS]['value'] is invalid.</p>
<p>Is there a different way to achieve what I am trying to do?</p>
<p>Thanks in advance</p>
","<azure-data-factory>","2022-11-05 22:52:00","335","1","1","74332646","<p>I have a slightly similar scenario that you might be able to work with.</p>
<p>First, I have a JSON file that is emitted that I then access with Synapse/ADF with <code>Lookup</code>.</p>
<p>I next have a <code>For each</code> activity that runs a <code>copy data</code> activity.</p>
<p>The <code>for each</code> activity receives my <code>Lookup</code> and makes my JSON usable, by setting the following in the <code>For each</code>'s <code>Settings</code> like so:</p>
<p><code>@activity('Lookup').output.firstRow.childItems</code></p>
<p>My JSON roughly looks as follows:</p>
<pre><code>{&quot;childItems&quot;: [
    {&quot;subpath&quot;: &quot;path/to/folder&quot;,
     &quot;filename&quot;: &quot;filename.parquet&quot;,
     &quot;subfolder&quot;: &quot;subfolder&quot;,
     &quot;outfolder&quot;: &quot;subfolder&quot;,
     &quot;origin&quot;: &quot;A&quot;}]}
</code></pre>
<p>So this means in my <code>copy data</code> activity within the <code>for each</code> activity, I can access the parameters of my JSON like so:</p>
<pre><code>@item()['subpath']
@item()['filename']
@item()['folder']
.. etc
</code></pre>
<p>Edit:</p>
<p>Adding some screen caps of the parameterization:</p>
<p><a href=""https://i.stack.imgur.com/aHpWk.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/aHpWk.png</a></p>
"
"74321117","Pass Multiple Value to a For Each in Azure Data Factory","<p>Can someone let me know if its possible to pass a parameter and an activity to a For Each in Azure Data Factory.</p>
<p>From the image I want to pass the parmater 'relativeURLs' into a For Each.</p>
<p><a href=""https://i.stack.imgur.com/1xBp4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1xBp4.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/OYALL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OYALL.png"" alt=""enter image description here"" /></a></p>
<p>I would then like to do a For Each on the Lookup activity 'CompanyId Lookup
Is that possible?</p>
","<azure-data-factory>","2022-11-04 17:49:07","198","0","2","74382318","<p>I am not very confident if I get the ask correctly .</p>
<blockquote>
<p>I would then like to do a For Each on the Lookup activity 'CompanyId
Lookup Is that possible?</p>
</blockquote>
<p>This should go in the Foreach</p>
<blockquote>
<p>@activity('your lookup activity name ').output.value</p>
</blockquote>
<p>Since the relative url is a parameter , you can reference that  inside the FE loop</p>
"
"74321117","Pass Multiple Value to a For Each in Azure Data Factory","<p>Can someone let me know if its possible to pass a parameter and an activity to a For Each in Azure Data Factory.</p>
<p>From the image I want to pass the parmater 'relativeURLs' into a For Each.</p>
<p><a href=""https://i.stack.imgur.com/1xBp4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1xBp4.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/OYALL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OYALL.png"" alt=""enter image description here"" /></a></p>
<p>I would then like to do a For Each on the Lookup activity 'CompanyId Lookup
Is that possible?</p>
","<azure-data-factory>","2022-11-04 17:49:07","198","0","2","74383526","<p>Here is the procedure to Pass Multiple Value to a For Each in Azure Data Factory.</p>
<p>create Linked service and dataset.
<img src=""https://i.imgur.com/xhu6US2.png"" alt=""enter image description here"" />
<img src=""https://i.imgur.com/m2qzYEz.png"" alt=""enter image description here"" />
Create parameter of <code>relativeURL</code>  with respective values
<img src=""https://i.imgur.com/cjzEvOz.png"" alt=""enter image description here"" />
Read the data by the Lookup activity.
<img src=""https://i.imgur.com/xEi9OTR.png"" alt=""enter image description here"" /></p>
<pre><code>@range(0,length(pipeline().parameters.relativeURL))
</code></pre>
<p>using two values inside Foreach using their indexes.
In ForEach, <strong>check the Sequential</strong>
<img src=""https://i.imgur.com/ynkKMyn.png"" alt=""enter image description here"" /></p>
<p>Create variables for different values.
The value of the set variable from the lookup activity.</p>
<pre><code>@string(activity('Lookup1').output.value[0].data[item()])
</code></pre>
<p><img src=""https://i.imgur.com/RAsecdo.png"" alt=""enter image description here"" />
Value for this set variable from the pipeline parameters of the relativeURL</p>
<pre><code>@pipeline().parameters.relativeURL[item()]
</code></pre>
<p><img src=""https://i.imgur.com/7MZtkeJ.png"" alt=""enter image description here"" /></p>
"