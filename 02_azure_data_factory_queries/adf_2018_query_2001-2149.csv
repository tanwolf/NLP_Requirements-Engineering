QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"49145184","Not Able to Publish ADF Incremental Package","<p>As Earlier Posted a thread for syncing Data from Premises Mysql  to Azure SQL over <a href=""https://stackoverflow.com/questions/49132994/does-incremental-sync-with-azure-data-factory-v2-support-only-sql-server-to-azur/49142686#49142686"">here</a> referring this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal#overview"" rel=""nofollow noreferrer"">article</a>, and found that lookup component for watermark detection is only available for SQL Server Only.</p>

<p>So tried a work Around, that while using ""Copy"" Data Flow task ,will pick data greater than last watermark stored from Mysql.</p>

<p><strong>Issue</strong>:
Able to validate package successfully but not able to publish same.</p>

<p><strong>Question</strong> :
In Copy Data Flow Task i'm using below query to get data from MySql greater than watermark available.</p>

<p>Can't we use Query like  below on other relational sources  like Mysql</p>

<pre><code>select * from @{item().TABLE_NAME} where @{item().WaterMark_Column} &gt; '@{activity('LookupOldWaterMark').output.firstRow.WatermarkValue}'
</code></pre>

<p>CopyTask SQL Query Preview
<a href=""https://i.stack.imgur.com/UA9qj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UA9qj.png"" alt=""enter image description here""></a></p>

<p>Validate Successfully
<a href=""https://i.stack.imgur.com/b8ytC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b8ytC.png"" alt=""enter image description here""></a></p>

<p>Error With no  Details
<a href=""https://i.stack.imgur.com/ROk9G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ROk9G.png"" alt=""enter image description here""></a></p>

<p>Debug Successfully
<a href=""https://i.stack.imgur.com/x5uMC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x5uMC.png"" alt=""enter image description here""></a></p>

<p>Error After following steps mentioned by Franky
Azure SQL Linked Service Error (Resolved by re configuring connection /edit credentials in connection tab)
<a href=""https://i.stack.imgur.com/faMFM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/faMFM.png"" alt=""enter image description here""></a></p>

<p>Source Query got blank (resolved by re-selection source type and rewriting query)
<a href=""https://i.stack.imgur.com/rIgJJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rIgJJ.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2018-03-07 06:05:20","732","0","2","49163901","<p>Did any of the objects publish into your Data Factory?</p>
"
"49145184","Not Able to Publish ADF Incremental Package","<p>As Earlier Posted a thread for syncing Data from Premises Mysql  to Azure SQL over <a href=""https://stackoverflow.com/questions/49132994/does-incremental-sync-with-azure-data-factory-v2-support-only-sql-server-to-azur/49142686#49142686"">here</a> referring this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal#overview"" rel=""nofollow noreferrer"">article</a>, and found that lookup component for watermark detection is only available for SQL Server Only.</p>

<p>So tried a work Around, that while using ""Copy"" Data Flow task ,will pick data greater than last watermark stored from Mysql.</p>

<p><strong>Issue</strong>:
Able to validate package successfully but not able to publish same.</p>

<p><strong>Question</strong> :
In Copy Data Flow Task i'm using below query to get data from MySql greater than watermark available.</p>

<p>Can't we use Query like  below on other relational sources  like Mysql</p>

<pre><code>select * from @{item().TABLE_NAME} where @{item().WaterMark_Column} &gt; '@{activity('LookupOldWaterMark').output.firstRow.WatermarkValue}'
</code></pre>

<p>CopyTask SQL Query Preview
<a href=""https://i.stack.imgur.com/UA9qj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UA9qj.png"" alt=""enter image description here""></a></p>

<p>Validate Successfully
<a href=""https://i.stack.imgur.com/b8ytC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b8ytC.png"" alt=""enter image description here""></a></p>

<p>Error With no  Details
<a href=""https://i.stack.imgur.com/ROk9G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ROk9G.png"" alt=""enter image description here""></a></p>

<p>Debug Successfully
<a href=""https://i.stack.imgur.com/x5uMC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x5uMC.png"" alt=""enter image description here""></a></p>

<p>Error After following steps mentioned by Franky
Azure SQL Linked Service Error (Resolved by re configuring connection /edit credentials in connection tab)
<a href=""https://i.stack.imgur.com/faMFM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/faMFM.png"" alt=""enter image description here""></a></p>

<p>Source Query got blank (resolved by re-selection source type and rewriting query)
<a href=""https://i.stack.imgur.com/rIgJJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rIgJJ.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2018-03-07 06:05:20","732","0","2","49179811","<p>Could you verify if you have access to create a template deployment in the azure portal?</p>

<p>1) Export the ARM Template: int he top-right of the ADFv2 portal, click on ARM Template -> Export ARM Template, extract the zip file and copy the content of the ""arm_template.json"" file.</p>

<p>2) Create ARM Template deployment: Go to <a href=""https://portal.azure.com/#create/Microsoft.Template"" rel=""nofollow noreferrer"">https://portal.azure.com/#create/Microsoft.Template</a> and log in with the same credentials you use in the ADFv2 portal (you can also get to this page going in the Azure portal, click on ""Create a resource""  and search for ""Template deployment""). Now click on ""Build your own template in editor"" and paste the ARM template from the previous step in the editor and Save. </p>

<p>3) Deploy template: Click on existing resource group and select the same resource group as the one where your Data Factory is. Fill out the parameters that are missing (for this testing it doesn't really matter if the values are valid); Factory name should already be there. Agree the terms and click purchase.</p>

<p>4) Verify the deployment succeeded. If not let me know the error, it might be an access issue which would explain why your publish fails. (ADF team is working on giving a better error for this issue).</p>
"
"49138273","pass multiple dynamic parameters to U-SQL script via data factory","<p>currently I can pass one parameter to u-sql script in data factory workflow.
and with that parameter i can apply some pattern to generate files paths.
is there any way to pass collection of datetimes parameters to u-sql and 
apply pattern to generate file paths?</p>
","<azure-data-factory><azure-data-lake><u-sql>","2018-03-06 19:09:07","372","1","2","49150246","<p>Pass a Json parameter. Then handle it with u-sql.</p>
"
"49138273","pass multiple dynamic parameters to U-SQL script via data factory","<p>currently I can pass one parameter to u-sql script in data factory workflow.
and with that parameter i can apply some pattern to generate files paths.
is there any way to pass collection of datetimes parameters to u-sql and 
apply pattern to generate file paths?</p>
","<azure-data-factory><azure-data-lake><u-sql>","2018-03-06 19:09:07","372","1","2","49158831","<p>You can pass multiple parameters. U-SQL also allows parameters of type <code>SqlArray&lt;&gt;</code>. I am not sure though if ADF supports passing in such typed values. I think the PowerShell APIs do allow it.</p>

<p>I assume that passing the values as a file will not work, since you will not get compile time partition elimination with it.</p>
"
"49132994","Does Incremental Sync with Azure Data Factory V2 support only Sql Server to Azure SQL","<p>I was trying incremental sync to  design incremental sync data between MySQL and Azure Sql referring <a href=""https://learn.microsoft.com/en-in/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal#overview"" rel=""nofollow noreferrer"">article</a> and while designing pipeline for <a href=""https://learn.microsoft.com/en-in/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal#create-a-pipeline"" rel=""nofollow noreferrer"">new watermark</a> found that lookup component only support SQL SERVER.</p>

<p><strong>Question</strong> </p>

<p>Is there a way to sync incrementally on cloud from hosted Mysql to Azure SQl  using ADF v1/v2?</p>

<p>What other component we can use to select data from Mysql i tried Store Procedure but it support Sql server only.</p>
","<azure-data-factory>","2018-03-06 14:18:55","367","0","2","49139235","<p>Currently, the ADF Control Flow activity ""Lookup"" does not support MySQL, so the watermark table must exist in an entity that Lookup can communicate with, ao you can create the watermark table and associated sprocs in Azure SQL DB, while pulling the source data from MySQL tables.</p>
"
"49132994","Does Incremental Sync with Azure Data Factory V2 support only Sql Server to Azure SQL","<p>I was trying incremental sync to  design incremental sync data between MySQL and Azure Sql referring <a href=""https://learn.microsoft.com/en-in/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal#overview"" rel=""nofollow noreferrer"">article</a> and while designing pipeline for <a href=""https://learn.microsoft.com/en-in/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal#create-a-pipeline"" rel=""nofollow noreferrer"">new watermark</a> found that lookup component only support SQL SERVER.</p>

<p><strong>Question</strong> </p>

<p>Is there a way to sync incrementally on cloud from hosted Mysql to Azure SQl  using ADF v1/v2?</p>

<p>What other component we can use to select data from Mysql i tried Store Procedure but it support Sql server only.</p>
","<azure-data-factory>","2018-03-06 14:18:55","367","0","2","49142686","<p>We (ADF team) are actively working on expanding Lookup activity to support more data stores including MySQL.  We will announce when it's ready.  Please stay tuned!</p>
"
"49113153","process multiple files on azure data lake","<p>let's assume there are two file sets A and B on azure data lake store.</p>

<pre><code>/A/Year/
/A/Month/Day/Month/
/A/Year/Month/Day/A_Year_Month_Day_Hour

/B/Year/
/B/Month/Day/Month/
/B/Year/Month/Day/B_Year_Month_Day_Hour
</code></pre>

<p>I want to get some values (let's say DateCreated of A entity) and use these values generate file paths for B set.</p>

<p>how can I achieve that?</p>

<p>some thoughts,but i'm not sure about this.
1.select values from A 
2.store on some storage ( azure data lake or azure sql database).
3. build one comma separated string pStr
4. pass pStr via Data Factory to stored procedure which generates file paths with pattern.</p>

<p><strong>EDIT</strong></p>

<p>according to @mabasile_MSFT answer</p>

<p>Here is what i have right now.
First USQL script that generates json file, which looks following way.</p>

<pre><code>{
FileSet:[""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__12"",
""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__13"",
""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__14"",
""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__15""]
}
</code></pre>

<p>ADF pipeline which contains Lookup and second USQL script.
Lookup reads this json file FileSet property and as i understood i need to somehow pass this json array to second script right?
But usql compiler generates string variable like </p>

<p>DECLARE @fileSet string =  ""[""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__12"",
        ""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__13"",
        ""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__14"",
        ""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__15""]""</p>

<p>and the script even didn't get compile after it.</p>
","<azure-data-factory><azure-data-lake><u-sql>","2018-03-05 14:52:51","1250","0","2","49149990","<p>Try this root link, that can help you start with all about u-sql: 
<a href=""http://usql.io"" rel=""nofollow noreferrer"">http://usql.io</a></p>

<p>Usefull link for your question:
<a href=""https://saveenr.gitbooks.io/usql-tutorial/content/filesets/filesets-with-dates.html"" rel=""nofollow noreferrer"">https://saveenr.gitbooks.io/usql-tutorial/content/filesets/filesets-with-dates.html</a></p>
"
"49113153","process multiple files on azure data lake","<p>let's assume there are two file sets A and B on azure data lake store.</p>

<pre><code>/A/Year/
/A/Month/Day/Month/
/A/Year/Month/Day/A_Year_Month_Day_Hour

/B/Year/
/B/Month/Day/Month/
/B/Year/Month/Day/B_Year_Month_Day_Hour
</code></pre>

<p>I want to get some values (let's say DateCreated of A entity) and use these values generate file paths for B set.</p>

<p>how can I achieve that?</p>

<p>some thoughts,but i'm not sure about this.
1.select values from A 
2.store on some storage ( azure data lake or azure sql database).
3. build one comma separated string pStr
4. pass pStr via Data Factory to stored procedure which generates file paths with pattern.</p>

<p><strong>EDIT</strong></p>

<p>according to @mabasile_MSFT answer</p>

<p>Here is what i have right now.
First USQL script that generates json file, which looks following way.</p>

<pre><code>{
FileSet:[""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__12"",
""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__13"",
""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__14"",
""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__15""]
}
</code></pre>

<p>ADF pipeline which contains Lookup and second USQL script.
Lookup reads this json file FileSet property and as i understood i need to somehow pass this json array to second script right?
But usql compiler generates string variable like </p>

<p>DECLARE @fileSet string =  ""[""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__12"",
        ""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__13"",
        ""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__14"",
        ""/Data/SomeEntity/2018/3/5/SomeEntity_2018_3_5__15""]""</p>

<p>and the script even didn't get compile after it.</p>
","<azure-data-factory><azure-data-lake><u-sql>","2018-03-05 14:52:51","1250","0","2","49157755","<p>You will need two U-SQL jobs, but you can instead use an ADF Lookup activity to read the filesets.   </p>

<p>Your first ADLA job should extract data from A, build the filesets, and <a href=""https://github.com/Azure/usql/tree/master/Examples/DataFormats/Microsoft.Analytics.Samples.Formats"" rel=""nofollow noreferrer"">output to a JSON file</a> in <a href=""https://learn.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-manage-use-dotnet-sdk"" rel=""nofollow noreferrer"">Azure Storage</a>.    </p>

<p>Then use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup activity</a> in ADF to read the fileset names from your JSON file in Azure Storage.  </p>

<p>Then define your second U-SQL activity in ADF.  Set the fileset as a parameter (under Script > Advanced if you're using the online UI) in the U-SQL activity - the value will look something like <code>@{activity('MyLookupActivity').output.firstRow.FileSet}</code> (see Lookup activity docs above).   </p>

<p>ADF will write in the U-SQL parameter as a DECLARE statement at the top of your U-SQL script.  If you want to have a default value encoded into your script as well, use DECLARE EXTERNAL - this will get overwritten by the DECLARE statements ADF writes in so it won't cause errors.   </p>

<p>I hope this helps, and let me know if you have additional questions!</p>
"
"49061150","Azure Data Factory : copying data from CosmosDB to SQL Database fails with ""range-indexed"" error","<p>I need to move data from Azure CosmosDB (previously known as DocumentDB) into Azure SQL database.</p>

<p>I am using the ""Copy Data"" resource where I have setup the source and the destination as well as the mapping.</p>

<p>The schedule has been setup for once every hour. The issues : </p>

<ol>
<li>I am noticing that the no. of instances or ""runs"" are more than once per hour. Is this because it is failing and it keeps retrying for a certain amount of times ? </li>
<li>The main error why its failing seems to be this : </li>
</ol>

<blockquote>
  <p>Activity Copy_dbo_SubscriptionLocator failed: Failure happened on
  'Source' side.
  ErrorCode=UserErrorDocumentDBReadError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=DocumentDb
  operation failed: Message: {""Errors"":[""An invalid query has been
  specified with filters against path(s) that are not range-indexed.
  Consider adding allow scan header in the request.""]}</p>
</blockquote>

<p>What changes do I need to make on CosmosDB end to avoid the error ?</p>

<p>Following is my current indexing policy : </p>

<pre><code>{
    ""indexingMode"": ""consistent"",
    ""automatic"": true,
    ""includedPaths"": [
        {
            ""path"": ""/*"",
            ""indexes"": [
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""Number"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Hash"",
                    ""dataType"": ""String"",
                    ""precision"": 3
                }
            ]
        }
    ],
    ""excludedPaths"": [
        {
            ""path"": ""/OneOfTheColumns/*""
        }
    ]
}
</code></pre>

<p>Looks like the issue is with the datetime field which needs to be range indexed as a string. I could possibly use the _ts field in the document. But how do I change the Copy job to convert the query's datetime to epoch time and use its value for _ts field.</p>
","<azure><azure-sql-database><azure-cosmosdb><azure-data-factory>","2018-03-02 00:56:40","1362","0","2","49062194","<p>You need to update your index policy of the document collection to allow this query.</p>

<p>Say you have a query like ""select * from c where c.property>""something"", this c.property must have a range index.</p>

<p>You can learn more about the indexing policy <a href=""https://azure.microsoft.com/en-us/blog/update-your-documentdb-indexing-policies-online/"" rel=""nofollow noreferrer"">here</a>: and <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/indexing-policies#modify-the-indexing-policy-of-a-collection"" rel=""nofollow noreferrer"">here</a> </p>
"
"49061150","Azure Data Factory : copying data from CosmosDB to SQL Database fails with ""range-indexed"" error","<p>I need to move data from Azure CosmosDB (previously known as DocumentDB) into Azure SQL database.</p>

<p>I am using the ""Copy Data"" resource where I have setup the source and the destination as well as the mapping.</p>

<p>The schedule has been setup for once every hour. The issues : </p>

<ol>
<li>I am noticing that the no. of instances or ""runs"" are more than once per hour. Is this because it is failing and it keeps retrying for a certain amount of times ? </li>
<li>The main error why its failing seems to be this : </li>
</ol>

<blockquote>
  <p>Activity Copy_dbo_SubscriptionLocator failed: Failure happened on
  'Source' side.
  ErrorCode=UserErrorDocumentDBReadError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=DocumentDb
  operation failed: Message: {""Errors"":[""An invalid query has been
  specified with filters against path(s) that are not range-indexed.
  Consider adding allow scan header in the request.""]}</p>
</blockquote>

<p>What changes do I need to make on CosmosDB end to avoid the error ?</p>

<p>Following is my current indexing policy : </p>

<pre><code>{
    ""indexingMode"": ""consistent"",
    ""automatic"": true,
    ""includedPaths"": [
        {
            ""path"": ""/*"",
            ""indexes"": [
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""Number"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Hash"",
                    ""dataType"": ""String"",
                    ""precision"": 3
                }
            ]
        }
    ],
    ""excludedPaths"": [
        {
            ""path"": ""/OneOfTheColumns/*""
        }
    ]
}
</code></pre>

<p>Looks like the issue is with the datetime field which needs to be range indexed as a string. I could possibly use the _ts field in the document. But how do I change the Copy job to convert the query's datetime to epoch time and use its value for _ts field.</p>
","<azure><azure-sql-database><azure-cosmosdb><azure-data-factory>","2018-03-02 00:56:40","1362","0","2","49062296","<p>I'm not sure about which language you are using, so I reproduced your issue on my side, please refer to my steps.</p>

<p>My sample documents:</p>

<pre><code>[
    {
        ""id"": ""1"",
        ""name"": ""Jay"",
        ""course"": ""A"",
        ""score"": 50
    },
    {
        ""id"": ""2"",
        ""name"": ""Peter"",
        ""course"": ""B"",
        ""score"": 20
    }
]
</code></pre>

<p>Index policy:</p>

<pre><code>{
    ""indexingMode"": ""consistent"",
    ""automatic"": true,
    ""includedPaths"": [
        {
            ""path"": ""/name/?"",
            ""indexes"": [
                {
                    ""kind"": ""Hash"",
                    ""dataType"": ""String"",
                    ""precision"": 3
                },
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""Number"",
                    ""precision"": -1
                }
            ]
        }
    ],
    ""excludedPaths"": [
        {
            ""path"": ""/*""
        }
    ]
}
</code></pre>

<p>Query in the source:<code>(SELECT c.id,c.name,c.course,c.score FROM c where c.score&gt;30)</code></p>

<p><a href=""https://i.stack.imgur.com/unJSZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/unJSZ.png"" alt=""enter image description here""></a></p>

<p>Reproduce your issue:</p>

<p><a href=""https://i.stack.imgur.com/zRkWb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zRkWb.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Range supports efficient equality queries, range queries (using >, &lt;,</p>
  
  <blockquote>
    <p>=, &lt;=, !=), and ORDER BY queries. ORDER By queries by default also require maximum index precision (-1). The data type can be String or
    Number.</p>
  </blockquote>
</blockquote>

<p>When your have range query, you need define range index on the field. Please refer to official <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/indexing-policies#index-data-types-kinds-and-precisions"" rel=""nofollow noreferrer"">doc</a>.</p>

<p>So, I modified the above index policy and copied data successfully:</p>

<pre><code>{
    ""indexingMode"": ""consistent"",
    ""automatic"": true,
    ""includedPaths"": [
        {
            ""path"": ""/*"",
            ""indexes"": [
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""Number"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""String"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Spatial"",
                    ""dataType"": ""Point""
                }
            ]
        }
    ],
    ""excludedPaths"": []
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/s0n5v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s0n5v.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/HNcUl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HNcUl.png"" alt=""enter image description here""></a></p>

<p>Hope it helps you.</p>
"
"49041729","Change Data Factory Window Timezone","<p>I have an activity in a pipeline that needs to run at 2am Sydney Time, and select timeslice data from 2am the previous day up until 2am the run date. </p>

<p>02:00 sydney is 15:00 UTC, so I specified in my datasets and pipelines</p>

<pre><code>""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 1,
        ""offset"":""15:00:00""
    },
</code></pre>

<p>And I have a select query in a copy activity</p>

<pre><code>""oracleReaderQuery"": ""`$`$Text.Format('select * FROM table WHERE juliandate  &gt;= TO_NUMBER(TO_CHAR(TO_DATE(\\'{0:yyyy-MM-dd}\\',\\'YYYY-MM-DD\\'), \\'YYYYDDD\\'))-1900000 and juliandate &lt; TO_NUMBER(TO_CHAR(TO_DATE(\\'{1:yyyy-MM-dd}\\',\\'YYYY-MM-DD\\'), \\'YYYYDDD\\'))-1900000', WindowStart, WindowEnd)""
</code></pre>

<p>Now the pipeline seems to run at the correct time of 2am, however, the window date passed to the query seems to still be in UTC - see the run below for the 1st March at 2am</p>

<p><a href=""https://i.stack.imgur.com/2g3Wg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2g3Wg.png"" alt=""enter image description here""></a></p>

<p>I could fix this I guess by adding in the 11 hours in the query itself, but it seems a bit of a workaround - and there must be a better way?</p>
","<azure-data-factory>","2018-03-01 02:29:58","1111","0","1","49049115","<p>What you changed is the time when it will be run, but that doesnt make data factory use another timezone. WindowStart and WindowEnd in your query will always be in UTC, you can vote for the idea to change timezones here, but the idea has been around since 2015 and most people fix it by adding hours: <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/10879920-allow-setting-the-timezone-for-slices"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/10879920-allow-setting-the-timezone-for-slices</a></p>

<p>You can use some of these functions to get what you want here (most likely AddHours(X,Y) ): <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-functions-variables#data-factory-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-functions-variables#data-factory-functions</a></p>

<p>Hope this helped!</p>
"
"49041511","Ms Integration Runtime data factory","<p>Kind of new with the integration runtime.
I had a pipeline running with no issues  but recently we had an AD upgrade and the local on premesis SQL db changed my user from 'bluecompany\joe' to  'redcompany\joe'
This has caused my datafactory to stop working properly . as it can't connect to the SQL onpremesis . 
I can't seem to find the place of where I can update this change? </p>

<p>Error:
Copy activity encountered a user error at Source side: Integration Runtime (Self-hosted) Node Name=ORG200016,ErrorCode=UserErrorFailedToConnectToSqlServer,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot connect to SQL Server: 'org200016.bluecompany.com.au', Database: 'GroupRisk', User: 'bluecompany\joe'.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.ComponentModel.Win32Exception,Message=This user can't sign in because this account is currently disabled,Source=Microsoft.DataTransfer.ClientLibrary,'.</p>

<p>any ideas would be very welcomed. Thank you</p>
","<runtime><integration><azure-data-factory>","2018-03-01 02:03:57","594","2","2","49044222","<p>As your login account has changed, I think you will need to update the account in the corresponding linked service, where you entered your credentials for this database previously.</p>

<p>Be sure the test connection succeeds after you edit the linked serivce. Then the pipeline should be able to connect to your database again.</p>
"
"49041511","Ms Integration Runtime data factory","<p>Kind of new with the integration runtime.
I had a pipeline running with no issues  but recently we had an AD upgrade and the local on premesis SQL db changed my user from 'bluecompany\joe' to  'redcompany\joe'
This has caused my datafactory to stop working properly . as it can't connect to the SQL onpremesis . 
I can't seem to find the place of where I can update this change? </p>

<p>Error:
Copy activity encountered a user error at Source side: Integration Runtime (Self-hosted) Node Name=ORG200016,ErrorCode=UserErrorFailedToConnectToSqlServer,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot connect to SQL Server: 'org200016.bluecompany.com.au', Database: 'GroupRisk', User: 'bluecompany\joe'.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.ComponentModel.Win32Exception,Message=This user can't sign in because this account is currently disabled,Source=Microsoft.DataTransfer.ClientLibrary,'.</p>

<p>any ideas would be very welcomed. Thank you</p>
","<runtime><integration><azure-data-factory>","2018-03-01 02:03:57","594","2","2","49044240","<p>Depending on which version of ADF you're using, there are different ways to update your linked service:</p>

<ol>
<li><p>login to <a href=""https://portal.azure.com/"" rel=""nofollow noreferrer"">https://portal.azure.com/</a> and find you data factory (if you don't have an account to login to portal, you need to find the admin who create this linked service and ask him to update for you)</p></li>
<li><p>if you're using v1 data factory, find the ""Author and Deploy"" where you should be able to find your linked service corresponding to your on premise SQL server.</p></li>
<li><p>if you're using v2 data factory, find the ""Author and Monitor"", click on the pen logo where you should be able to find your linked service from the ""connections"" tab, it will allow you to edit the linked service.</p></li>
</ol>

<p>Thanks,</p>

<p>Eva</p>
"
"49040918","Unable to see data after data migration from SQL Server to Azure Cosmos db","<p>I used Azure Data Factory to migrate data from SQL Server to Azure CosmosDB. In Data Factory, it shows the status is Ready and succeeded (100%). Retry attempt is 1. </p>

<p>But I am not able to see any data in CosmosDB after the migration is done. It also shows an error message from CosmosDB that </p>

<blockquote>
  <p>""code"" : 500, ""body"": ""Detected corrupted documents without _id identifier, verify that you are only storing documents through Mongo protocol and not SQL API or DocumentDB SDK.""}</p>
</blockquote>

<p>A few things I did differently is that</p>

<ul>
<li>I un-selected the ""column name is selected"" option, which I assume the data will migrate from scratch.</li>
<li>I converted from SQL Server to Cosmos DB - mongo db. I am not sure if it will make the data corrupted because it is basically converting from SQL to NO-SQL. </li>
</ul>

<p>Please let me know your thoughts. Thank you!</p>
","<sql><azure><nosql><azure-cosmosdb><azure-data-factory>","2018-03-01 00:37:22","355","0","1","49041526","<p>From this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#supported-capabilities"" rel=""nofollow noreferrer"">official doc</a> you could see Azure Cosmos DB connector supports only Cosmos DB SQL API(document db). And the type in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#dataset-properties"" rel=""nofollow noreferrer"">Dataset properties</a> need to be set to <code>DocumentDbCollection</code>.</p>

<p>Azure Document DB and Azure Mongo DB are both <code>NO-SQL</code> database in the Azure Cosmos DB.However, there are many <a href=""https://medium.com/@thomasweiss_io/azure-documentdb-vs-mongodb-6d5806c16239"" rel=""nofollow noreferrer"">differences</a> between them.</p>

<p>Hope it helps you.</p>
"
"49040627","How to check if Azure datafactory pipeline is running without run id in C#","<p>Is there a way in C# to find if pipeline is running without run ID ?</p>

<p>I have tried looking up, but could on find methods which take RunID and give status of pipeline</p>
","<azure-data-factory>","2018-03-01 00:02:12","1494","0","1","49131976","<p>Through <code>C#</code>, it might not be possible as <code>Pipeline Runs - Get</code> has the <code>runId</code> as the required parameter detailed documentation <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelineruns/get"" rel=""nofollow noreferrer"">here</a>. </p>

<p>You can always try using <code>PowerShell</code> to check the run status by using cmdlet <code>Get-AzureRmDataFactoryV2PipelineRun</code> where you can pass information such as   <code>ResourceGroupName, DataFactoryName, LastUpdatedAfter, LastUpdatedBefore, PipelineName</code>. More information <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactories/get-azurermdatafactoryv2pipelinerun?view=azurermps-5.4.0"" rel=""nofollow noreferrer"">here</a>.  </p>
"
"49027838","How to create a linked service for azure key vault in ADF version2?","<p>I've to create a pipeline which will copy data from one blob to another blob &amp; I would like to use azure data factory version 2 with key vault. How can I do that?</p>
","<azure><azure-data-factory><azure-keyvault>","2018-02-28 10:53:41","2143","0","1","49031534","<p>You need a <code>Azure Key Vault</code> linked service to be created first -</p>

<pre><code>{
    ""name"": ""AzureKeyVaultLinkedService"",
    ""properties"": {
    ""type"": ""AzureKeyVault"",
    ""typeProperties"": {
        ""baseUrl"": ""https://&lt;azureKeyVaultName&gt;.vault.azure.net""
        }
    }
}
</code></pre>

<p>Then when you are creating your <code>Azure Blob Storage</code> linkedservice just refer the <code>Azure Key Vault</code> property for your <code>connectiongString</code> or <code>sasUri</code> to reference the secret stored in your key vault -</p>

<pre><code>{
    ""name"": ""AzureStorageLinkedService"",
    ""properties"": {
        ""type"": ""AzureStorage"",
        ""typeProperties"": {
            ""connectionString"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""secretName"": ""&lt;secret name in AKV&gt;"",
                ""store"":{
                    ""referenceName"": ""&lt;Azure Key Vault linked service&gt;"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        },
        ""connectVia"": {
            ""referenceName"": ""&lt;name of Integration Runtime&gt;"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p>Detailed information to be found <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">here</a>.</p>
"
"48990796","Moving data from Google Big Query to Azure Data Lake Store using Azure Data Factory","<p>I have a scenario where I need to connect the Azure Data Factory (v2) in Google Big Query for to move data to my Azure Data Lake but I don't work.</p>

<p>When I create a Linked Service, I choose Big Query as Source, I put all informations about BigQuery, as project-name, type of connection, etc but when I click in Validade button a message is show for me: (for exemple)...</p>

<blockquote>
  <p>UserError: ERROR [28000] [Microsoft][BigQuery] (80) Authentication
  failed: invalid_grant ERROR [28000] [Microsoft][BigQuery] (80)
  Authentication failed: invalid_grant'Type=,Message=ERROR [28000]
  [Microsoft][BigQuery] (80) Authentication failed: invalid_grant ERROR
  [28000] [Microsoft][BigQuery] (80) Authentication failed:
  invalid_grant,Source=,', activityId:
  05ff5ce1-5262-4794-853c-c9e39b7f4b71</p>
</blockquote>

<p>Any idea? Someone already tested this connector?</p>

<p>Tks.
Peeter Bonomo</p>
","<google-bigquery><azure-data-lake><azure-data-factory>","2018-02-26 14:30:09","3926","2","3","49000979","<p>According to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-bigquery"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-google-bigquery</a>, to connect to Google BigQuery via cloud IR, you need to provide the below information:</p>

<pre><code>{
    ""name"": ""GoogleBigQueryLinkedService"",
    ""properties"": {
        ""type"": ""GoogleBigQuery"",
        ""typeProperties"": {
            ""project"" : ""&lt;project ID&gt;"",
            ""additionalProjects"" : ""&lt;additional project IDs&gt;"",
            ""requestGoogleDriveScope"" : true,
            ""authenticationType"" : ""UserAuthentication"",
            ""clientId"": ""&lt;id of the application used to generate the refresh token&gt;"",
            ""clientSecret"": {
                ""type"": ""SecureString"",
                ""value"":""&lt;secret of the application used to generate the refresh token&gt;""
            },
            ""refreshToken"": {
                 ""type"": ""SecureString"",
                 ""value"": ""&lt;refresh token&gt;""
            }
        }
    }
}
</code></pre>

<p>and that the user you use to grant permission to the application should have access to the project you want to query.</p>

<p>Thanks,
Eva</p>
"
"48990796","Moving data from Google Big Query to Azure Data Lake Store using Azure Data Factory","<p>I have a scenario where I need to connect the Azure Data Factory (v2) in Google Big Query for to move data to my Azure Data Lake but I don't work.</p>

<p>When I create a Linked Service, I choose Big Query as Source, I put all informations about BigQuery, as project-name, type of connection, etc but when I click in Validade button a message is show for me: (for exemple)...</p>

<blockquote>
  <p>UserError: ERROR [28000] [Microsoft][BigQuery] (80) Authentication
  failed: invalid_grant ERROR [28000] [Microsoft][BigQuery] (80)
  Authentication failed: invalid_grant'Type=,Message=ERROR [28000]
  [Microsoft][BigQuery] (80) Authentication failed: invalid_grant ERROR
  [28000] [Microsoft][BigQuery] (80) Authentication failed:
  invalid_grant,Source=,', activityId:
  05ff5ce1-5262-4794-853c-c9e39b7f4b71</p>
</blockquote>

<p>Any idea? Someone already tested this connector?</p>

<p>Tks.
Peeter Bonomo</p>
","<google-bigquery><azure-data-lake><azure-data-factory>","2018-02-26 14:30:09","3926","2","3","50260076","<p>The documentation for the ADF connector to BigQuery explains what the parameters mean, but it doesn't give any guidance on how to obtain the values for those parameters.</p>

<p>I spent a couple of days on this and finally got it to work.  I used ""User Authentication"" because I wanted to use a cloud-based IR.  The ""Client Id"" and ""Client Secret"" can be obtained by creating new credentials in the GCP Console.  But to get the ""Refresh Token"", you have to do the OAuth2 dance with Google and intercept the token.</p>

<p>I put together a PowerShell script to do this for you and wrote up a post on GitHub that walks you through the process of authenticating ADF v2 to Google BigQuery:</p>

<p><a href=""https://github.com/AnalyticJeremy/ADF_BigQuery"" rel=""noreferrer"">https://github.com/AnalyticJeremy/ADF_BigQuery</a></p>
"
"48990796","Moving data from Google Big Query to Azure Data Lake Store using Azure Data Factory","<p>I have a scenario where I need to connect the Azure Data Factory (v2) in Google Big Query for to move data to my Azure Data Lake but I don't work.</p>

<p>When I create a Linked Service, I choose Big Query as Source, I put all informations about BigQuery, as project-name, type of connection, etc but when I click in Validade button a message is show for me: (for exemple)...</p>

<blockquote>
  <p>UserError: ERROR [28000] [Microsoft][BigQuery] (80) Authentication
  failed: invalid_grant ERROR [28000] [Microsoft][BigQuery] (80)
  Authentication failed: invalid_grant'Type=,Message=ERROR [28000]
  [Microsoft][BigQuery] (80) Authentication failed: invalid_grant ERROR
  [28000] [Microsoft][BigQuery] (80) Authentication failed:
  invalid_grant,Source=,', activityId:
  05ff5ce1-5262-4794-853c-c9e39b7f4b71</p>
</blockquote>

<p>Any idea? Someone already tested this connector?</p>

<p>Tks.
Peeter Bonomo</p>
","<google-bigquery><azure-data-lake><azure-data-factory>","2018-02-26 14:30:09","3926","2","3","71042787","<p>This is the error you get for any access issues unfortunately.  It's also the error you get when your refresh token has expired which it always does after 60 minutes.  Which is incredibly curious....this like so many sets of instructions on OAuth2.0 authentication for ADF never mention all this work is to get a code that expires in 60 minutes.  Without some method of refreshing it everytime you connect this is worthless. At least the following link mentions this error and you get it because the token has expired...its the only blog post (or Microsoft documentation) that bothers to mention this super big issue.  <a href=""https://medium.com/@varunck/data-transfer-from-google-bigquery-to-azure-blob-storage-using-azure-data-factory-adf-80659c078f83"" rel=""nofollow noreferrer"">https://medium.com/@varunck/data-transfer-from-google-bigquery-to-azure-blob-storage-using-azure-data-factory-adf-80659c078f83</a></p>
<p>Here's a different method which is what I will be attempting...it uses service account and IR  <a href=""https://www.youtube.com/watch?v=oRqRt7ya_DM"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=oRqRt7ya_DM</a></p>
"
"48953776","Data factory version 2","<p>Is there any ways to paste Json code to pipeline code in azure data factory version 2?
I am able to copy from pipeline code but no ways to paste there.</p>
","<azure><pipeline><azure-data-factory>","2018-02-23 18:01:34","106","1","2","48955613","<p>There is currently no way to do that, but you can paste the pipeline json and save it into a file, and then upload it with powershell as I explained here: <a href=""https://stackoverflow.com/questions/48931495/upload-adf-json-files-to-my-data-factory"">Upload ADF json files to my Data Factory</a></p>

<p>Hope this helped!</p>
"
"48953776","Data factory version 2","<p>Is there any ways to paste Json code to pipeline code in azure data factory version 2?
I am able to copy from pipeline code but no ways to paste there.</p>
","<azure><pipeline><azure-data-factory>","2018-02-23 18:01:34","106","1","2","49009923","<p>I created another answer because I cant format the code in a comment.</p>

<p>If you can get the repository to a folder, you can iterate over a folder and add everything inside with the following code: </p>

<pre><code>$files = Get-ChildItem 'C:\PathToFiles' 
$files | ForEach-Object($_){ 
    Set-AzureRmDataFactoryV2Pipeline -DataFactoryName ""your df name"" -ResourceGroupName ""your RG name"" -Name ""pipelineName"" -DefinitionFile $_.Name 
}
</code></pre>

<p>Hope this helped!</p>
"
"48947475","Debug sql database scoped credentials failure","<p>I created a scoped credential in a Azure SQL Datawarehouse database to create an external table over some files in a Azure Data Lake Store.</p>

<p>When I try creating the external table I get the message.</p>

<blockquote>
  <p>Msg 105061, Level 16, State 1, Line 35 Unable to find any valid
  credential associated with the specified data source. Credential is
  required to connect to Azure Data Lake Store.</p>
</blockquote>

<p>How do I troubleshoot this? My AzureAD application has access to the storage. I use the same AD-application (with a different key) for my Azure Data Factory pipeline that stores the files in the Azure Data Lake Store.</p>

<p>I haven't found any commands that let you test your credentials and see what credentials the database tries to use or why it fails. Any ideas?</p>

<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql</a></p>
","<azure><azure-data-factory><error-log><sql-data-warehouse>","2018-02-23 12:04:10","715","1","1","48948379","<p>So I had missed adding my scoped credential when I created the external data source. So create the scoped credential first, then the external data source.</p>
"
"48947169","Azure Data factory continous deployment using powershell","<p>I need to do Continous Integration and Deployment for my Azure Data Factory (ADF).</p>

<p>For this in a Visual Studio solution I have two projects:</p>

<ul>
<li>one for ADF json files(linked services, datasets etc.).</li>
<li>one for PowerShell script for deploying this ADF into a Azure subscription. </li>
</ul>

<h1>Steps followed</h1>

<p>Took MSBUILD of ADF codes and used copy files task to copy into <code>$(Build.ArtifactStagingDirectory)</code>.</p>

<p>Used Publish artifacts task to publish in VSTS.</p>

<p>Publish artifacts for PowerShell script as a separate build. </p>

<h1>Release</h1>

<p>In my release I have a Azure PowerShell script which will invoke these ADF files and deploy it in Azure subscription. I'm using ""Build.ArtifactStagingDirectory"" for referring my ADF files. But I'm getting the below error -  </p>

<blockquote>
  <p>The term 'Build.ArtifactStagingDirectory' is not recognized as the name of a cmdlet, function, script file, or operable program</p>
</blockquote>

<pre><code>foreach($file in Get-ChildItem ""$(Build.ArtifactStagingDirectory)"" -filter ""*LinkedService*"")
{
    New-AzureRmDataFactoryLinkedService -ResourceGroupName ""ADFAutomationResource"" -DataFactoryName ""ADFCICD190218"" -Name $file.BaseName -File $file.FullName -Force | Format-List
}
</code></pre>

<p>Let me know how to proceed in this case..as there are no sufficient links explaining this.</p>
","<powershell><continuous-deployment><azure-data-factory>","2018-02-23 11:47:06","505","0","2","48947341","<p>Try:</p>

<pre><code>foreach($file in Get-ChildItem $Build.ArtifactStagingDirectory -filter ""*LinkedService*"")
{
    New-AzureRmDataFactoryLinkedService -ResourceGroupName ""ADFAutomationResource"" -DataFactoryName ""ADFCICD190218"" -Name $file.BaseName -File $file.FullName -Force | Format-List
}
</code></pre>
"
"48947169","Azure Data factory continous deployment using powershell","<p>I need to do Continous Integration and Deployment for my Azure Data Factory (ADF).</p>

<p>For this in a Visual Studio solution I have two projects:</p>

<ul>
<li>one for ADF json files(linked services, datasets etc.).</li>
<li>one for PowerShell script for deploying this ADF into a Azure subscription. </li>
</ul>

<h1>Steps followed</h1>

<p>Took MSBUILD of ADF codes and used copy files task to copy into <code>$(Build.ArtifactStagingDirectory)</code>.</p>

<p>Used Publish artifacts task to publish in VSTS.</p>

<p>Publish artifacts for PowerShell script as a separate build. </p>

<h1>Release</h1>

<p>In my release I have a Azure PowerShell script which will invoke these ADF files and deploy it in Azure subscription. I'm using ""Build.ArtifactStagingDirectory"" for referring my ADF files. But I'm getting the below error -  </p>

<blockquote>
  <p>The term 'Build.ArtifactStagingDirectory' is not recognized as the name of a cmdlet, function, script file, or operable program</p>
</blockquote>

<pre><code>foreach($file in Get-ChildItem ""$(Build.ArtifactStagingDirectory)"" -filter ""*LinkedService*"")
{
    New-AzureRmDataFactoryLinkedService -ResourceGroupName ""ADFAutomationResource"" -DataFactoryName ""ADFCICD190218"" -Name $file.BaseName -File $file.FullName -Force | Format-List
}
</code></pre>

<p>Let me know how to proceed in this case..as there are no sufficient links explaining this.</p>
","<powershell><continuous-deployment><azure-data-factory>","2018-02-23 11:47:06","505","0","2","48947359","<p>You're referencing a <em>Build</em> variable in a <em>Release</em>!</p>

<p>I assume you've added your artifacts to the release?</p>

<p><a href=""https://i.stack.imgur.com/MKH9z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MKH9z.png"" alt=""Release artifacts""></a></p>

<p>If so; you should be able to refer to them like so:</p>

<pre><code>$(System.DefaultWorkingDirectory)/&lt;Artifact Name&gt;
</code></pre>
"
"48933090","Azure Data Factory V2: Custom Activity inside a If Condition activity","<p>I'm working on an Azure Data Factory V2 Pipeline but I having a problem when I try to execute a ""<em>Custom activity</em>"" inside an ""<em>If Condition Activity</em>"".</p>

<p>If I try to test my pipeline with ""Test Run"" button on the ADF's Web interface, this error appeare:</p>

<blockquote>
  <p>{""code"":""BadRequest"",""message"":""Activity PPL_ANYFBRF01 failed: Invalid linked service reference. Name: LNK_BATCH_AZURE"",""target""...}</p>
</blockquote>

<p>I'm sure that there is no error in the linked service reference's name. If I create a ""Custom Activity"" directly in my pipeline, it's working.</p>

<p>I think it can be a syntax error on my activity but I can't find it.</p>

<p>Here is my ""If Condition Activity""'s Json template (the expression ""@equal(0,0)"" is just for testing purpose):</p>

<pre><code>{
    ""name"": ""IfPointComptageNotExist"",
    ""type"": ""IfCondition"",
    ""dependsOn"": [
        {
            ""activity"": ""PointComptage"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        },
        {
            ""activity"": ""SousPointComptage"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        }
    ],
    ""typeProperties"": {
        ""expression"": {
            ""value"": ""@equal(0,0)"",
            ""type"": ""Expression""
        },
        ""ifTrueActivities"": [
            {
                ""type"": ""Custom"",
                ""name"": ""CustomActivityTest"",
                ""linkedServiceName"": {
                    ""referenceName"": ""LNK_BATCH_AZURE"",
                    ""type"": ""LinkedServiceReference""
                },
                ""typeProperties"": {
                    ""command"": ""Batch.exe"",
                    ""resourceLinkedService"": {
                        ""referenceName"": ""LNK_BLOB_STORAGE"",
                        ""type"": ""LinkedServiceReference""
                    },
                    ""folderPath"": ""/test/app/""
                }
            }
        ]
    }
},
</code></pre>

<p>Thank you in advance for your help.</p>
","<azure-data-factory>","2018-02-22 16:55:47","1428","0","1","49251734","<p>The problem is now solved. I have recreate the pipeline and it's working now.</p>

<p>Regards,</p>

<p>Julien.</p>
"
"48931495","Upload ADF json files to my Data Factory","<p>I have a number of pipeline/linkedservice/dataset json files and I need to upload them to my Data Factory, opposed to creating new versions and copying the text over. Whats the simplest way to do this? </p>
","<.net><json><azure><azure-data-factory>","2018-02-22 15:43:30","2444","1","2","48931757","<p>If you are using version 1, you can use Visual Studio to do so as shown here <a href=""https://azure.microsoft.com/en-us/blog/azure-data-factory-visual-studio-extension-for-authoring-pipelines/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/azure-data-factory-visual-studio-extension-for-authoring-pipelines/</a></p>

<p>If you are using version 2, you can do this using powershell. First download and install the azure sdk for powershell from here: <a href=""https://azure.microsoft.com/en-us/downloads/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/downloads/</a>
Then from powershell, login and select subscription:</p>

<pre><code>Login-AzureRmAccount
Select-AzureRmSubscription -SubscriptionName ""your subs name here""
</code></pre>

<p>Then with the following command you can upload the json files:</p>

<pre><code>Set-AzureRmDataFactoryV2Pipeline -DataFactoryName ""your df name"" -ResourceGroupName ""your RG name"" -Name ""pipelineName"" -DefinitionFile ""path to json file""
</code></pre>

<p>Replace with your Data factory and resource group name.</p>

<p>The same arguments are used to upload linked services and datasets with the commands:</p>

<pre><code>Set-AzureRmDataFactoryV2LinkedService 
Set-AzureRmDataFactoryV2Dataset 
</code></pre>

<p>Hope this helped!</p>
"
"48931495","Upload ADF json files to my Data Factory","<p>I have a number of pipeline/linkedservice/dataset json files and I need to upload them to my Data Factory, opposed to creating new versions and copying the text over. Whats the simplest way to do this? </p>
","<.net><json><azure><azure-data-factory>","2018-02-22 15:43:30","2444","1","2","59886137","<p>AzRM is not recommended by microsoft anymore.</p>

<p>You can use the updtaed powershell AZ module to achieve this.</p>

<p>I won't repeat something that is already quite self-explanatory in the official documentation here: <a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/set-azdatafactoryv2pipeline?view=azps-3.3.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/az.datafactory/set-azdatafactoryv2pipeline?view=azps-3.3.0</a></p>
"
"48916801","How can an ADF Custom Activity access the Azure Key Vault?","<p>I have a custom activity that runs through and ADF pipeline, with an Azure Batch for the execution. The custom activity needs to access the Azure Key Vault, but I don't want to leave any secrets/keys coded in the custom activity. </p>

<p>How can I get the custom activity to use the service principal to access the Azure Key Vault? I have no problem coding access to the Azure Key Vault using the service principal credentials (appId and key), but I obviously don't want to leave a key in the code. </p>

<p>So the question is: How can an ADF Custom Activity, running in an Azure Batch, access the Azure Key Vault through a service principal?</p>

<p>Thanks!</p>
","<azure><azure-data-factory><azure-keyvault><azure-batch>","2018-02-21 22:45:12","1638","3","1","49023615","<p>This is similar to how to access Key Vault from a Azure Batch task. There are some ways to avoid hard-code the secrets in code. One is to use certificate which can be deployed to the Batch VM (<a href=""https://samcogan.com/secure-credential-access-with-azure-batch-and-keyvault/"" rel=""nofollow noreferrer"">https://samcogan.com/secure-credential-access-with-azure-batch-and-keyvault/</a>), or you may use resource file (<a href=""https://learn.microsoft.com/en-us/azure/batch/batch-dotnet-get-started#resourcefiles"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/batch/batch-dotnet-get-started#resourcefiles</a>) to provide the secrets.</p>
"
"48910668","How to copy files and folder from one ADLS to another one on different subscription?","<p>I need to be able to copy files and folder from one DataLake to another DataLake on a different subscription, I'm in possession of both Auth Token and secret key.</p>
<p>I've tried different solution including:</p>
<p><a href=""https://medium.com/azure-data-lake/connecting-your-own-hadoop-or-spark-to-azure-data-lake-store-93d426d6a5f4"" rel=""nofollow noreferrer"">https://medium.com/azure-data-lake/connecting-your-own-hadoop-or-spark-to-azure-data-lake-store-93d426d6a5f4</a></p>
<p>which is involving hadoop but didn't worked on two different subscriptions, due to the site-core.xml which only accept one subscription.</p>
<p>ADLcopy didn't worked as well, neither DataFactory.</p>
<p>Any ideas? Can anyone point me in the right direction?</p>
","<azure-data-factory><azure-data-lake>","2018-02-21 16:20:00","2420","0","1","49004961","<p>Azure Data Factory supports such scenario, you need to create 2 AzureDataLakeStoreLinkedService, each with correct corresponding subscription and resource group (they don't necessarily be the same with the subscription of the data factory), and the credential of the service principal to access ADLS. 
If this doesn't answer your question, could you tell more of your scenario, as I don't understand this: "" I'm trying to add both secret key and tokens of the two dirrent subscriptions in the core-site.xml"", do you mean</p>
"
"48908764","Azure Data Factory copy from one DataLake to another DataLake","<p>I need to build a pipeline that is able to copy from one DataLake to another one, but so far by googleing around I couldn't find anything related to it, can anyone please point me in the right direction? Thanks a lot.</p>
","<azure-data-lake><azure-data-factory>","2018-02-21 14:45:42","856","0","2","49119923","<p>Using the copy wizard in data factory v1 you are able to copy from one data lake store to another nice and easily.</p>

<p>I'm not sure if this is available yet for v2 in wizard form, but should be fine to do configured by hand.</p>
"
"48908764","Azure Data Factory copy from one DataLake to another DataLake","<p>I need to build a pipeline that is able to copy from one DataLake to another one, but so far by googleing around I couldn't find anything related to it, can anyone please point me in the right direction? Thanks a lot.</p>
","<azure-data-lake><azure-data-factory>","2018-02-21 14:45:42","856","0","2","49386956","<p>Use Data Lake v2 copy activity with Azure Data Lake Store DataSet.</p>

<p>This way you could copy files from one data lake to another.</p>
"
"48905401","Copying Data from SAP BW to Azure Data Lake Store using Azure Data Factory","<p>I've been trying to copy some data from an on-premises SAP BW to a cloud Azure Data Lake Store. I've already configured the sink as the Data Lake Store but i'm having trouble to configure the source. Already downloaded the netweaver library and put the dlls in my system 32 folder and created the integration runtime which is running on my local machine. Has anyone tried this before?</p>

<p>Thanks</p>
","<azure><integration><azure-data-factory><azure-data-lake><sap-bw>","2018-02-21 12:00:23","1460","1","2","48919203","<p>I recommend using SAP Open Hub function SAP BW to generate flat file if you do not have any other SAP Data Services or HANA tools in place. Then the files can be loaded into HDFS, which is Azure data lake storage.</p>

<p>The reason for this recommendation:
1. SAP BW Open hub is easy for developer and even for non-SAP-BW person. 
2. I do not recommend use netweaver RFC (dll library) approach to integration with SAP BW as this mostly uses MDX to read BW data with significant coding and understanding of BW metadata.
3. This can ensure no violation of SAP data distribution licensing.</p>

<p>Hope this can help.</p>

<p>Lei</p>
"
"48905401","Copying Data from SAP BW to Azure Data Lake Store using Azure Data Factory","<p>I've been trying to copy some data from an on-premises SAP BW to a cloud Azure Data Lake Store. I've already configured the sink as the Data Lake Store but i'm having trouble to configure the source. Already downloaded the netweaver library and put the dlls in my system 32 folder and created the integration runtime which is running on my local machine. Has anyone tried this before?</p>

<p>Thanks</p>
","<azure><integration><azure-data-factory><azure-data-lake><sap-bw>","2018-02-21 12:00:23","1460","1","2","48995107","<p>@lei the nw library Fabricio was mentioning has to be the nwlibrfc32.dll which has to be manually injected by customers such that ADF could work with BW data source. This is the shared framework that PowerBI service accesses BW data source. Technically speaking, it's doable, and Fabricio has to ensure the right DLL is injected (32bit vs. 64bit). Without error log we have to speculate.</p>

<p>However, from solution standpoint, please understand that this is not going to be a performing way due to the bottleneck at the underlying MDX engine and result processor of the BW connector. If the volume is small, then there is nos issue. Otherwise, we need to review other options. Open Hub could be an option, if the user is ok to deal with a set of batch job management in BW and separate set of ADF job management in Azure. From IT agility standpoint, coupling two sets of operational processes is not a best approach.</p>

<p>Another option to consider is to hold off on ADF but opt to SSIS instead. Use SSIS to load SAP data with Azure Feature pack, like we do. But this may not be the best approach either, as there is no ADF any more if Fabricio's team already invested in ADF. Or maybe they favor SSIS. All in all, there has to be some level of tradeoff towards sustainable solution. </p>

<p>Back to the original question, please post the error detail and we can help investigate.</p>
"
"48903732","Trying to add expressions in azure data factory and getting 'The template function 'dataset' is not defined or not valid.' error","<p>I am trying to add some expressions to my data factory dataset and getting this error:  <em>The template function 'dataset' is not defined or not valid.'</em></p>

<p>I have default value of pipeline parameter:</p>

<pre><code>""parameters"": {
       ""adlFolder"": {
                    ""type"": ""String"",
                    ""defaultValue"": ""/somePath/""
                },
</code></pre>

<p>also i have dataset parameters: </p>

<pre><code> ""parameters"": {
                    ""month"": {
                        ""type"": ""String"",
                        ""defaultValue"": """"
                    },
                    ""date"": {
                        ""type"": ""String"",
                        ""defaultValue"": """"
                    },
                    ""dest"": {
                        ""type"": ""String"",
                        ""defaultValue"": """"
                    }
                },
</code></pre>

<p>and in the activity output I set expression to this dataset parameters: </p>

<pre><code>""parameters"": {
                                    ""month"": ""@substring(string(dataset().date),5,2)"",
                                    ""date"": ""@{string(utcnow('yyyy/MM/dd'))}"",
                 ""dest"":""@concat(string(pipeline().parameters.adlFolder),'01')""}
</code></pre>

<p>and in the dataset i have this:</p>

<pre><code> ""folderPath"": {
                        ""value"": ""@{dataset().dest}"",
                        ""type"": ""Expression""
                    }
</code></pre>

<p>Does anybody see what is wrong here? Can i set expressions like this? Thanks a lot for any feedback.</p>
","<azure><azure-data-factory>","2018-02-21 10:35:31","10986","0","1","48944759","<p>Looks like you are referencing a dataset's parameter in activity, which is not possible according to my understand.</p>

<p>Activity can only use pipeline's parameters, and dataset can only use dataset's own parameters as well. If you need to use the same value in activity and dataset. I think you will need to specify the parameter both in pipeline and dataset, use pipeline's parameter in pipeline and activity, use datasert's parameter in dataset, then map pipeline's parameter to dataset's paramter in pipeline. </p>

<p>Refer to the examples here (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#examples"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#examples</a>).</p>

<p>For example, use</p>

<pre><code>            ""inputs"": [
                {
                    ""referenceName"": ""BlobDataset"",
                    ""parameters"": {
                        ""path"": ""@pipeline().parameters.inputPath""
                    },
                    ""type"": ""DatasetReference""
                }
            ],
</code></pre>

<p>to map pipeline's parameter inputPath to dataset's parameter path. Then use @pipeline().inputPath in activity and @dataset().path in dataset (need to specify them individually in pipeline and dataset).</p>
"
"48901563","Azure Data factory CI/CD","<p>I have my <code>ADF</code> files built using Visual studio and need to build continuous deployment for this using <code>VSTS</code>..I have my <code>ADF</code> files and PowerShell in a single solution. But in my PowerShell script how can I refer the <code>ADF</code> files. What should be the path instead of <code>C:/users/</code>. Where I can find this path in my build. Hope this is clear                                                                                                                               </p>

<pre><code>foreach($file in Get-ChildItem ""C:/users/"" -filter ""*LinkedService*"")
{
  New-AzureRmDataFactoryLinkedService -ResourceGroupName ""ADFAutomationResource"" -DataFactoryName ""ADFCICD190218"" -Name $file.BaseName -File $file.FullName -Force | Format-List
}
</code></pre>
","<powershell><azure><azure-data-factory>","2018-02-21 08:51:41","174","0","1","49234616","<p>In your build, in the ""Copy and Publish Build Artifacts"" step, add the data factory folder like this: 
<code>**\DataFactoryFolder\*.json</code></p>

<p>In your release you can pass the folder location to your PowerShell script by using the following variable:</p>

<p><code>$(System.DefaultWorkingDirectory)/DataFactory</code></p>
"
"48891259","How to cancel pipeline -- No cancel button is available","<p>I have a pipeline running (8735cc10-80db-4401-8f9e-516d733b450e). From the activity runs page, I see the pipeline is running, but from the Pipleline Runs page, it shows a status of failure.</p>

<p>Is there a way to cancel this pipeline run from the UI ?</p>
","<azure><azure-data-factory>","2018-02-20 17:44:19","1455","0","1","48893325","<p>The easiest way to do this is from powershell.</p>

<p>Download the Azure sdk for powershell from here: <a href=""https://azure.microsoft.com/en-us/downloads/"" rel=""noreferrer"">https://azure.microsoft.com/en-us/downloads/</a> then run this:</p>

<pre><code>Login-AzureRmAccount

Select-AzureRmSubscription -SubscriptionName ""SubscName""

Stop-AzureRmDataFactoryV2PipelineRun -ResourceGroupName ""Name"" -DataFactoryName ""DataFactoryName"" -PipelineRunId 8735cc10-80db-4401-8f9e-516d733b450e
</code></pre>

<p>Replace with your own names.</p>

<p>Hope this helped!</p>
"
"48884020","Refer files from storage account in PowerShell","<p>I have this following code where i need to refer my <code>ADF</code> codes stored in azure blob storage</p>

<pre><code>foreach($file in Get-ChildItem ""https://adfcicdazurestorage82.blob.core.windows.net/adfcicdazurestorage82"" -filter ""*LinkedService*"")
{
  New-AzureRmDataFactoryLinkedService -ResourceGroupName ""ADFAutomationResource"" -DataFactoryName ""ADFCICD190218"" -Name $file.BaseName -File $file.FullName -Force | Format-List
}
</code></pre>

<p>These lines of code throw an error. I need to get these files from azure blob storage and create linked service.</p>
","<powershell><azure><azure-data-factory>","2018-02-20 11:14:47","161","1","1","48884743","<p>What you need is a <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-copy-activity-tutorial-using-powershell.md#create-a-linked-service-for-an-azure-storage-account"" rel=""nofollow noreferrer"">Linked Service JSON File</a> that links to your storage account.</p>

<p>Your code could look something like this:</p>

<pre><code>#PowerShell:

New-AzureRmDataFactoryLinkedService -ResourceGroupName ""ADFAutomationResource"" -DataFactoryName ""ADFCICD190218""  -File .\AzureStorageLinkedService.json | Format-List
</code></pre>

<p>...and your linked service JSON file like this <em>(filename: AzureStorageLinkedService.json)</em>:</p>

<pre><code>     {
        ""name"": ""AzureStorageLinkedService"",
        ""properties"": {
            ""type"": ""AzureStorage"",
            ""typeProperties"": {
                ""connectionString"": ""DefaultEndpointsProtocol=https;AccountName=adfcicdazurestorage82;AccountKey=&lt;accountkey&gt;""
            }
        }
     }
</code></pre>
"
"48862124","Azure Data Factory V2: Create linked service for Azure Sql server using Azure Active Directory","<p>Creating linked service(Azure Data Factory V2) for Azure Sql supports SQL Authentication. I want to know if it also supports Azure Active Directory Integrated Authentication.</p>
","<azure><azure-data-factory><azure-sql-server>","2018-02-19 08:33:48","1331","2","1","48866476","<p>As of today (Feb '18), </p>

<p>ADFV2 can connect to some sources using Managed Service Identity (MSI). In short, the ADFV2 instance is given an identity in the Active Directory as an Active Directory Application. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">See the docs here</a>. Then the ADFV2 can connect to data sources as that identity. </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/active-directory/msi-overview"" rel=""nofollow noreferrer"">Although Azure SQL supports Managed Service Identity</a>, accessing Azure SQL Server through MSI is not available for ADFV2 yet. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database"" rel=""nofollow noreferrer"">From the docs</a>: </p>

<blockquote>
  <p>ConnectionString: Specify information needed to connect to the Azure
  SQL Database instance for the connectionString property. <em>Only basic
  authentication is supported</em>. Mark this field as a SecureString to
  store it securely in Data Factory, or reference a secret stored in
  Azure Key Vault.</p>
</blockquote>

<p>As the docs state, you can use the ADFV2 Managed Service Identity to connect to KeyVault and use keys and secrets stored there, which is probably your best best for limiting security information in configuration. </p>

<p>Keep in mind that the UI for ADFV2 is still quite far behind the API, so you may need to use PowerShell or Azure command line to set it up properly. </p>
"
"48859228","Powershell login with username and password parameterized","<p>in the powershell script for building continous deployment using VSTS do i need to include login-azurermaccount and if yes how do i parameterize it for user name and password.I have googled it and not able to find a correct solution..</p>
","<powershell><azure><azure-data-factory>","2018-02-19 03:36:04","617","0","2","48859732","<p>You cannot use MSCloud services, without being logged into them.</p>

<p>You'd parametrize the creds in the same way you'd do it for on-prem resources that require credentials.</p>

<p>See the built-in help files on credential handling.</p>

<pre><code># Get parameters, examples, full and Online help for a cmdlet or function

(Get-Command -Name Get-Credential).Parameters
Get-help -Name Get-Credential -Examples
Get-help -Name Get-Credential -Full
Get-help -Name Get-Credential -Online

Get-Help about_*

# All Help topics locations
explorer ""$pshome\$($Host.CurrentCulture.Name)""
</code></pre>

<p>Or using the PSCredentialObject.</p>

<p>PowerShell – How to create a PSCredential object</p>

<p>Several PowerShell commandlets take a PSCredential object to run using a particular user account. You can create the PSCredential object by using Get-Credential commandlet which opens a dialog to enter the username and password. This way of entering credentials can be used in an interactive mode.</p>

<p><a href=""https://blogs.msdn.microsoft.com/koteshb/2010/02/12/powershell-how-to-create-a-pscredential-object"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/koteshb/2010/02/12/powershell-how-to-create-a-pscredential-object</a></p>
"
"48859228","Powershell login with username and password parameterized","<p>in the powershell script for building continous deployment using VSTS do i need to include login-azurermaccount and if yes how do i parameterize it for user name and password.I have googled it and not able to find a correct solution..</p>
","<powershell><azure><azure-data-factory>","2018-02-19 03:36:04","617","0","2","48863209","<p>Microsoft recommends using an Azure service Principal for power shell automation. You can check <a href=""https://learn.microsoft.com/en-us/powershell/azure/create-azure-service-principal-azureps?view=azurermps-5.3.0"" rel=""nofollow noreferrer"">here</a> on how to create an Azure service Principal.</p>
"
"48826641","Not able to use SSIS package to Sync Data from MySql to AzureSql using ADF-V2","<p>As we can run SSIS package on ADF-v2.</p>

<p>So I've prepared an SSIS package to Sync records between Mysql and Azure SQL
and getting below error in reports </p>

<p>I'm tried using ODBC and ADO.Net connection but getting same result when trying executing  from SSIS catalog.</p>

<p>I'm able to sync records when i tried executing package from SSDT</p>

<p>Question : Can't we use SSIS in ADF-v2, other than Azure cloud connecters?</p>

<p>Also please suggest if there any steps i'm missing.</p>

<p>Reference links used to implement same
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-create-azure-ssis-runtime-portal"" rel=""nofollow noreferrer"">Link1</a>
 and <a href=""https://learn.microsoft.com/en-us/sql/integration-services/lift-shift/ssis-azure-deploy-run-monitor-tutorial"" rel=""nofollow noreferrer"">Link2</a></p>

<p><a href=""https://i.stack.imgur.com/a42nx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a42nx.png"" alt=""enter image description here""></a></p>
","<ssis><azure-data-factory>","2018-02-16 12:19:37","282","0","1","49185758","<p>You might need to install MySQL ADO.NET/ODBC driver on your Azure-SSIS IR via our custom setup interface that's still in Private Preview.  Please fill out our online survey to join: <a href=""https://www.surveybuilder.com/s/Dg6Bq?source_type=email"" rel=""nofollow noreferrer"">https://www.surveybuilder.com/s/Dg6Bq?source_type=email</a>.</p>
"
"48818925","Performance tuning Azure SQL when loading with Data Factory","<p>I've been able to successfully import data from blobs into Azure SQL and the performance I'm seeing is approximately 55k rows / minute. For my dataset this takes roughly 8 hours to complete.</p>

<p>I've tried changing the DTU of the database, write batch size to the database, parallel copy (from auto, to 1, 2, 3...). All made no noticeable difference.</p>

<p>I'm looking for suggestions on what I can do to improve the load rate into Azure SQL using Data Factory (v2).</p>
","<azure><azure-sql-database><azure-data-factory>","2018-02-16 01:30:19","484","0","1","50217512","<p>The issue turned out to be the amount of I/O required. Moving to 'Premium' level before doing the bulk load reduced the load time significantly. </p>
"
"48791371","Execute SSIS package using storted procedure in ADF v2","<p>I would like to execute a SSIS package in ADFv2. </p>

<p>I created a pipeline and used a stored procedure for it as described here </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-stored-procedure-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-stored-procedure-activity</a>.</p>

<pre><code>DECLARE @return_value INT, @exe_id BIGINT, @err_msg NVARCHAR(150)    
EXEC @return_value=[SSISDB].[catalog].[create_execution] 
@folder_name=N'xxx', 
@project_name=N'xxx', 
@package_name=N'xxx.dtsx', 
@use32bitruntime=0, @runinscaleout=1, @useanyworker=1, @execution_id=@exe_id OUTPUT    

EXEC [SSISDB].[catalog].[set_execution_parameter_value] @exe_id, @object_type=50, @parameter_name=N'SYNCHRONIZED', @parameter_value=1    
EXEC [SSISDB].[catalog].[start_execution] @execution_id=@exe_id, @retry_count=0    

IF(SELECT [status] FROM [SSISDB].[catalog].[executions] WHERE execution_id=@exe_id)&lt;&gt;7 
BEGIN 
SET @err_msg=N'Your package execution did not succeed for execution ID: ' 
+ CAST(@exe_id AS NVARCHAR(20)) RAISERROR(@err_msg,15,1) 
END
</code></pre>

<p>But this only works well when database credentials is saved in package. Is there a way to trigger the package with database credentials I configured in Environments properties from Integration Service Catalog?? Thanks in advance for any help!! </p>
","<ssis><azure-data-factory>","2018-02-14 16:01:15","292","1","1","48792528","<p>In a non-ADF scenario, the <code>create_execution</code> call accepts a parameter of <code>@reference_id</code></p>

<p>You can divine that value by querying the SSISDB</p>

<pre><code>SELECT ER.reference_id
FROM SSISDB.catalog.folders AS F
INNER JOIN SSISDB.catalog.environments AS E
ON E.folder_id = F.folder_id
INNER JOIN SSISDB.catalog.projects AS P
ON P.folder_id = F.folder_id
INNER JOIN SSISDB.catalog.environment_references AS ER
ON ER.project_id = P.project_id
WHERE F.name = 'MyFolder' AND E.name = 'EnvDemo' AND P.name = 'MyIspac'
</code></pre>

<p>Putting it all together, your script could look like </p>

<pre><code>DECLARE @return_value INT, @exe_id BIGINT, @err_msg NVARCHAR(150), @refid bigint;

SELECT @refid = ER.reference_id
FROM SSISDB.catalog.folders AS F
INNER JOIN SSISDB.catalog.environments AS E
ON E.folder_id = F.folder_id
INNER JOIN SSISDB.catalog.projects AS P
ON P.folder_id = F.folder_id
INNER JOIN SSISDB.catalog.environment_references AS ER
ON ER.project_id = P.project_id
WHERE F.name = 'MyFolder' AND E.name = 'EnvDemo' AND P.name = 'MyIspac';

EXEC @return_value=[SSISDB].[catalog].[create_execution] 
@folder_name=N'xxx', 
@project_name=N'xxx', 
@package_name=N'xxx.dtsx', 
@reference_id = @refid
@use32bitruntime=0, @runinscaleout=1, @useanyworker=1, @execution_id=@exe_id OUTPUT
</code></pre>

<p>Untested for ADF but it <em>ought</em> to work. Let me know otherwise and I'll see if I can scare up some documentation about the scale out scenarios</p>
"
"48757031","Azure Data Factory V2 how to process azure analysis service cube","<p>In Azure Data Factory V2, how to process azure analysis service cube?</p>

<p>I am able to use Azure Automation Account to run runbook powershell script to process cube automatically. However, I want to add cube processing into my ETL process developed on ADF V2. </p>

<p>Can you please advise?</p>
","<azure-sql-database><azure-analysis-services><azure-data-factory>","2018-02-12 23:06:48","4684","3","2","48759196","<p>You can set-up a Webhook from the Azure Automation runbook and call that URL endpoint from an ADF pipeline Web Activity using POST method.</p>

<p>Or, here is an example of processing AAS cubes using Web Activity, but calling a Logic Apps endpoint instead of an Automation Webhook (Thanks to Jorg Klein for this tip):</p>

<p><a href=""https://jorgklein.com/2018/01/30/process-azure-analysis-services-objects-from-azure-data-factory-v2-using-a-logic-app/"" rel=""nofollow noreferrer"">https://jorgklein.com/2018/01/30/process-azure-analysis-services-objects-from-azure-data-factory-v2-using-a-logic-app/</a></p>
"
"48757031","Azure Data Factory V2 how to process azure analysis service cube","<p>In Azure Data Factory V2, how to process azure analysis service cube?</p>

<p>I am able to use Azure Automation Account to run runbook powershell script to process cube automatically. However, I want to add cube processing into my ETL process developed on ADF V2. </p>

<p>Can you please advise?</p>
","<azure-sql-database><azure-analysis-services><azure-data-factory>","2018-02-12 23:06:48","4684","3","2","58788695","<p>My preferred approach for processing Azure Analysis Services from ADFv2 is to use built-in Web Activities along with the built-in MSI authentication. A sample can be found <a href=""https://github.com/furmangg/automating-azure-analysis-services/blob/master/README.md#processazureas"" rel=""nofollow noreferrer"">here</a>. </p>
"
"48739474","Data Lake Analytics FileSet Max","<p>Within my <strong>Azure Data Lake</strong> directory <code>ingest</code>, I have numbered files:</p>

<pre><code>ingest/file1.tsv
ingest/file2.tsv
ingest/file3.tsv
</code></pre>

<p>Within an <strong>Azure Data Factory v2</strong> <code>U-SQL</code> activity, how can I run <code>Extractors.Tsv()</code> on only the highest numbered file in the <code>FileSet</code>?</p>
","<azure-data-factory><azure-data-lake><u-sql>","2018-02-12 04:18:17","77","1","1","48741606","<p>One approach would require two scripts:</p>

<ol>
<li><p>Script 1 would get the max value using the file set feature, and then create the second script as a file (that you store in a WASB account, unless ADF allows you to refer to scripts in ADLS now as well).</p></li>
<li><p>Then execute Script 2.</p></li>
</ol>

<p>Another approach would be to get the highest number using the ADLS SDK and then pass that value to a script that uses a file set on the number as in:</p>

<pre><code>DECLARE EXTERNAL @fileno int;
@d = EXTRACT ..., fileno int 
     FROM ""ingest/file{fileno}.tsv"" 
     USING Extractors.Tsv();
// predicate will be pushed into EXTRACT statement.
@d = SELECT * FROM @d WHERE fileno == @fileno;
</code></pre>

<p>The syntactically simplest solution in a single script would be the following:</p>

<pre><code>@d = EXTRACT ..., fileno int 
     FROM ""ingest/file{fileno}.tsv"" 
     USING Extractors.Tsv();

@maxfno = SELECT DISTINCT(MAX(fileno)) AS maxno FROM @d;

@d = SELECT * FROM @d JOIN @maxfno ON fileno == maxno;
</code></pre>

<p>But that statement is not constant-foldable and thus you would read all files anyway.</p>
"
"48714290","Is there a way set SliceStart back a few days?","<p>I am working on a Data Factory where I want to query data from few days back.</p>

<p>I am executing a stored procedure that takes input based on the slice start:</p>

<p><strong>For Example:</strong> </p>

<pre><code>""value"": ""$$Text.Format('{0:dd}', SliceStart)""
</code></pre>

<p>So a run that starts on Friday queries data from Monday.</p>

<p>I can do some date manipulation in the stored procedure, but ideally I would like the window start and end date reflect the data the was copied.</p>

<p><a href=""https://i.stack.imgur.com/Isisn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Isisn.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2018-02-09 21:23:35","160","1","2","48735634","<p>Yes, you can use <code>Date.AddDays</code> function in order to acomplish this. From your screenshot and the fact you are talking about slices, I assume you are using DataFactory version 1. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-functions-variables#data-factory-functions"" rel=""nofollow noreferrer"">Here</a> is overview of ADF v1 functions.</p>

<p>For your example, to get date that is 4 days before <code>SliceStart</code>, you would write something like</p>

<p><code>""value"": ""$$Text.Format('{0:dd}', Date.AddDays(SliceStart, -4))""</code></p>
"
"48714290","Is there a way set SliceStart back a few days?","<p>I am working on a Data Factory where I want to query data from few days back.</p>

<p>I am executing a stored procedure that takes input based on the slice start:</p>

<p><strong>For Example:</strong> </p>

<pre><code>""value"": ""$$Text.Format('{0:dd}', SliceStart)""
</code></pre>

<p>So a run that starts on Friday queries data from Monday.</p>

<p>I can do some date manipulation in the stored procedure, but ideally I would like the window start and end date reflect the data the was copied.</p>

<p><a href=""https://i.stack.imgur.com/Isisn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Isisn.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2018-02-09 21:23:35","160","1","2","48748200","<p>In V1 you can specify ""delay"" in your <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-create-pipelines#policies"" rel=""nofollow noreferrer"">policy</a> of your activities. This allows you to postpone the execution of your activity. The example shows a couple of minutes, but I think you can use that to make it execute a few days later, while show the date/time of your slice.</p>
"
"48710575","Data Factory deployment through Visaul Studio - doesn't list existing Data Factories","<p>When attempting to deploy changes to an existing Data Factory on Azure from Visual Studio (2015), I am presented with the below screen where the drop down on the 'Use exisiting Data Factory' option is blank. I should be able to see a number of DF's.</p>

<p>I have access to the relevant subscription and right to deploy, however not entirely sure why it doesn't display the existing data factory.</p>

<p>Any idea why?</p>

<p><a href=""https://i.stack.imgur.com/JwsSV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JwsSV.png"" alt=""enter image description here""></a></p>
","<azure><azure-deployment><azure-data-factory>","2018-02-09 16:51:42","57","0","1","48748323","<p>I'm having the same problem. You need some permissions on the Azure subscription, but I've never figured out which ones exactly. I think you need subscription admin or something. Since I wanted to have automated deployments I ended up with a PowerShell script to do the deployment. For a powershell deployment, <a href=""https://blog.abatishchev.ru/2017/03/28/how-to-deploy-azure-data-factory-pipeline-and-its-dependencies-using-powershell/"" rel=""nofollow noreferrer"">this page</a> should get you started.</p>
"
"48698404","Azure Data Factory V2 SFTP Error","<p>I am trying to perform a copy activity using Azure Data Factory V2 and configured source and sink properties, everything looks fine. When i executed the process, it fails with the below error. I tried with a different SFTP server but still getting same issue.</p>

<p><em>Please make sure Sftp server is not throttling ADF Service or Integration Runtime (Self-hosted)., Source=Microsoft.DataTransfer.ClientLibrary.SftpConnector,''Type=Renci.SshNet.Common.SftpPathNotFoundException,Message=No such file,Source=Renci.SshNet,'</em></p>
","<azure><azure-data-factory>","2018-02-09 03:36:37","2018","1","2","48706405","<p>The message is clear, ""Path not found exception"" and ""No such file"" means that when it builds the path with folderPath+fileName and tries to navigate to it, the path doesnt work. Try fixing that, at least its not an authentication issue.</p>

<p>Hope this helped!</p>
"
"48698404","Azure Data Factory V2 SFTP Error","<p>I am trying to perform a copy activity using Azure Data Factory V2 and configured source and sink properties, everything looks fine. When i executed the process, it fails with the below error. I tried with a different SFTP server but still getting same issue.</p>

<p><em>Please make sure Sftp server is not throttling ADF Service or Integration Runtime (Self-hosted)., Source=Microsoft.DataTransfer.ClientLibrary.SftpConnector,''Type=Renci.SshNet.Common.SftpPathNotFoundException,Message=No such file,Source=Renci.SshNet,'</em></p>
","<azure><azure-data-factory>","2018-02-09 03:36:37","2018","1","2","48742441","<p>This is not due to the path or file not found but its due to the MaxSessions setting at SFTP server. I set this property in SFTP server and it works !!</p>
"
"48695260","Azure Data Factory Pipeline + ML","<p>I am trying to do a pipeline in Azure Data factory V1 which will do an Azure Batch Execution on a file. I implemented it using a blob storage as input and output and it worked. However, I am not trying to change the input and output to a folder in my data lake store. When I try to deploy it, it gives me the following error: </p>

<pre><code>Entity provisioning failed: AzureML Activity 'MLActivity' specifies 'DatalakeInput' in a property that requires an Azure Blob Dataset reference.  
</code></pre>

<p>How can I have the input and output as a datalakestore instead of a blob?</p>

<p>Pipeline:</p>

<pre><code>{
        ""name"": ""MLPipeline"",
        ""properties"": {
            ""description"": ""use AzureML model"",
            ""activities"": [
                {
                    ""type"": ""AzureMLBatchExecution"",
                    ""typeProperties"": {
                        ""webServiceInput"": ""DatalakeInput"",
                        ""webServiceOutputs"": {
                            ""output1"": ""DatalakeOutput""
                        },
                        ""webServiceInputs"": {},
                        ""globalParameters"": {}
                    },
                    ""inputs"": [
                        {
                            ""name"": ""DatalakeInput""
                        }
                    ],
                    ""outputs"": [
                        {
                            ""name"": ""DatalakeOutput""
                        }
                    ],
                    ""policy"": {
                        ""timeout"": ""02:00:00"",
                        ""concurrency"": 3,
                        ""executionPriorityOrder"": ""NewestFirst"",
                        ""retry"": 1
                    },
                    ""scheduler"": {
                        ""frequency"": ""Hour"",
                        ""interval"": 1
                    },
                    ""name"": ""MLActivity"",
                    ""description"": ""description"",
                    ""linkedServiceName"": ""MyAzureMLLinkedService""
                }
            ],
            ""start"": ""2016-02-08T00:00:00Z"",
            ""end"": ""2016-02-08T00:00:00Z"",
            ""isPaused"": false,
            ""hubName"": ""hubname"",
            ""pipelineMode"": ""Scheduled""
        }
    }
</code></pre>

<p>Output dataset:</p>

<pre><code>  {
        ""name"": ""DatalakeOutput"",
        ""properties"": {
            ""published"": false,
            ""type"": ""AzureDataLakeStore"",
            ""linkedServiceName"": ""AzureDataLakeStoreLinkedService"",
            ""typeProperties"": {
                ""folderPath"": ""/DATA_MANAGEMENT/""
            },
            ""availability"": {
                ""frequency"": ""Hour"",
                ""interval"": 1
            }
        }
    }
</code></pre>

<p>Input dataset:</p>

<pre><code> {
        ""name"": ""DatalakeInput"",
        ""properties"": {
            ""published"": false,
            ""type"": ""AzureDataLakeStore"",
            ""linkedServiceName"": ""AzureDataLakeStoreLinkedService"",
            ""typeProperties"": {
                ""fileName"": ""data.csv"",
                ""folderPath"": ""/RAW/"",
                ""format"": {
                    ""type"": ""TextFormat"",
                    ""columnDelimiter"": "",""
                }
            },
            ""availability"": {
                ""frequency"": ""Hour"",
                ""interval"": 1
            }
        }
    }
</code></pre>

<p>AzureDatalakeStoreLinkedService:</p>

<pre><code>{
    ""name"": ""AzureDataLakeStoreLinkedService"",
    ""properties"": {
        ""description"": """",
        ""hubName"": ""xyzdatafactoryv1_hub"",
        ""type"": ""AzureDataLakeStore"",
        ""typeProperties"": {
            ""dataLakeStoreUri"": ""https://xyzdatastore.azuredatalakestore.net/webhdfs/v1"",
            ""authorization"": ""**********"",
            ""sessionId"": ""**********"",
            ""subscriptionId"": ""*****"",
            ""resourceGroupName"": ""xyzresourcegroup""
        }
    }
}
</code></pre>

<p>The linked service was done following this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-datalake-connector"" rel=""nofollow noreferrer"">tutorial</a> based on data factory V1. </p>
","<azure><pipeline><azure-data-factory><azure-data-lake>","2018-02-08 21:43:05","302","3","1","48697816","<p>I assume there is some issue with AzureDataLakeStoreLinkedService. Please verify.</p>

<p>Depending on the authentication used for access data store, your AzureDataLakeStoreLinkedService json must look like below -</p>

<p><strong>Using service principal authentication</strong></p>



<pre class=""lang-html prettyprint-override""><code>{
    ""name"": ""AzureDataLakeStoreLinkedService"",
    ""properties"": {
        ""type"": ""AzureDataLakeStore"",
        ""typeProperties"": {
            ""dataLakeStoreUri"": ""https://&lt;accountname&gt;.azuredatalakestore.net/webhdfs/v1"",
            ""servicePrincipalId"": ""&lt;service principal id&gt;"",
            ""servicePrincipalKey"": {
                ""type"": ""SecureString"",
                ""value"": ""&lt;service principal key&gt;""
            },
            ""tenant"": ""&lt;tenant info, e.g. microsoft.onmicrosoft.com&gt;"",
            ""subscriptionId"": ""&lt;subscription of ADLS&gt;"",
            ""resourceGroupName"": ""&lt;resource group of ADLS&gt;""
        },
        ""connectVia"": {
            ""referenceName"": ""&lt;name of Integration Runtime&gt;"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p><strong>Using managed service identity authentication</strong></p>

<pre><code>{
    ""name"": ""AzureDataLakeStoreLinkedService"",
    ""properties"": {
        ""type"": ""AzureDataLakeStore"",
        ""typeProperties"": {
            ""dataLakeStoreUri"": ""https://&lt;accountname&gt;.azuredatalakestore.net/webhdfs/v1"",
            ""tenant"": ""&lt;tenant info, e.g. microsoft.onmicrosoft.com&gt;"",
            ""subscriptionId"": ""&lt;subscription of ADLS&gt;"",
            ""resourceGroupName"": ""&lt;resource group of ADLS&gt;""
        },
        ""connectVia"": {
            ""referenceName"": ""&lt;name of Integration Runtime&gt;"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p>This is Microsoft Document for reference - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store"" rel=""nofollow noreferrer"">Copy data to or from Azure Data Lake Store by using Azure Data Factory</a></p>
"
"48676085","Azure Data Factory V2 : How to create ForEach Activity using C#?","<pre><code>       int[] mitems = new int[] { 99, 98, 92, 97, 95 };

        //pass the variable as ""@item()""
        var forEachActivity = new ForEachActivity()
        {
            Name = ""ForEachActivity"",
            IsSequential = false,
            Items = mitems,
            Activities = activities
        };
</code></pre>

<p>This does not compile , report error at Items=mitems, expecting Expression and i am not sure how to provide items in Expression.</p>
","<azure-data-factory>","2018-02-08 01:32:56","3674","0","3","48688386","<p>I never created an activity of this type, but I can help a bit with your troubles. If you read the constructor method for the ForEachActivity object, you can see that the items parameter must be an object of the Expression class. When creating an Expression, it takes a string in the value parameter.</p>

<p>It is kind of weird that the documentation (Here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#type-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#type-properties</a>) says that Items is ""An expression that returns a JSON Array to be iterated over."" So my guess is that you must pass a String with a json format to create this Expression object.</p>

<p>So what you can do is replace the creation of the mitems object with something like this:</p>

<pre><code>Expression mitems = new Expression(""{99, 98, 92, 97, 95 }"");
</code></pre>

<p>Hope this helped!!</p>
"
"48676085","Azure Data Factory V2 : How to create ForEach Activity using C#?","<pre><code>       int[] mitems = new int[] { 99, 98, 92, 97, 95 };

        //pass the variable as ""@item()""
        var forEachActivity = new ForEachActivity()
        {
            Name = ""ForEachActivity"",
            IsSequential = false,
            Items = mitems,
            Activities = activities
        };
</code></pre>

<p>This does not compile , report error at Items=mitems, expecting Expression and i am not sure how to provide items in Expression.</p>
","<azure-data-factory>","2018-02-08 01:32:56","3674","0","3","48777698","<p>This worked for me</p>

<p>Items = new Expression { Value = ""@Json('[10, 20, 30, 40, 50]')"" }</p>
"
"48676085","Azure Data Factory V2 : How to create ForEach Activity using C#?","<pre><code>       int[] mitems = new int[] { 99, 98, 92, 97, 95 };

        //pass the variable as ""@item()""
        var forEachActivity = new ForEachActivity()
        {
            Name = ""ForEachActivity"",
            IsSequential = false,
            Items = mitems,
            Activities = activities
        };
</code></pre>

<p>This does not compile , report error at Items=mitems, expecting Expression and i am not sure how to provide items in Expression.</p>
","<azure-data-factory>","2018-02-08 01:32:56","3674","0","3","48933842","<p>I was struggling with this too but I think I got it working now.
The Items property is an Expression, which would look something like this:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>new ForEachActivity
{
  Name = ""myForEachLoop"",
  IsSequential = false,
  Items = new Expression(""@pipeline().parameters.foreachFileList""),
  Activities = new List&lt;Activity&gt;
  {
    ...
  }
}</code></pre>
</div>
</div>
</p>

<p>The pipeline parameters would look something like this</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Parameters = new Dictionary&lt;string, ParameterSpecification&gt;
{
  { ""foreachFileList"", new ParameterSpecification { Type = ParameterType.Array } }
}</code></pre>
</div>
</div>
</p>

<p>And finally, passing the parameters to your pipeline would look something like this:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Dictionary&lt;string, object&gt; arguments = new Dictionary&lt;string, object&gt;
{
  { ""foreachFileList"", new string[] { ""file1.txt"", ""file2.txt"" } }
};

client.Pipelines.CreateRunWithHttpMessagesAsync(resourceGroup, dataFactoryName, pipelineName, arguments)</code></pre>
</div>
</div>
</p>
"
"48671327","How To Check Entry of New Rows In ADW?","<p>I Have scheduled various pipeline that copys incremental data from On Premises SQL DB to Azure Datawarehouse, sometimes there are some issues with the number of rows, like the row count not matching or same rows getting copied, How can I check what rows are getting copied over to the Datawarehouse so that If there is a redundancy or discrepancy I should be aware about.</p>

<p>I know that ADF V1 Pipeline Monitoring feature just shows the number of rows that got copied over, but not what rows got copied over, is there a way so that I can find out what got copied and what not.</p>

<p>This things become much more hard when we are dealing with the tables which are having million rows.</p>
","<sql-server><azure><azure-data-factory><azure-synapse>","2018-02-07 18:56:12","138","0","2","48693923","<p>Since Azure SQL Datawarehouse doesnt have support for primary key constraints, you can create staging tables where you insert the new data, and then call a stored procedure where you compare this new data with what you already have, and only insert the new rows into the actual tables.</p>

<p>This isnt a very good practice, the best way is to control the insertion of new rows in an actual ETL process, if you dont have this you might be better off with an Azure SQL server that supports almost everything that the on premise sql server supports, and not a warehouse.</p>

<p>Cheers!</p>
"
"48671327","How To Check Entry of New Rows In ADW?","<p>I Have scheduled various pipeline that copys incremental data from On Premises SQL DB to Azure Datawarehouse, sometimes there are some issues with the number of rows, like the row count not matching or same rows getting copied, How can I check what rows are getting copied over to the Datawarehouse so that If there is a redundancy or discrepancy I should be aware about.</p>

<p>I know that ADF V1 Pipeline Monitoring feature just shows the number of rows that got copied over, but not what rows got copied over, is there a way so that I can find out what got copied and what not.</p>

<p>This things become much more hard when we are dealing with the tables which are having million rows.</p>
","<sql-server><azure><azure-data-factory><azure-synapse>","2018-02-07 18:56:12","138","0","2","49055646","<p>The pattern for DW is usually to land your data into a stage table and then continue processing your data into your prod tables with T-SQL. Using this pattern, you can simply run a SELECT COUNT_BIG(*) FROM  to get the row count in the staging table before you continue processing. </p>
"
"48662793","Python Custom Activity for Azure-Data Factory","<p>I am trying to create a Data Factory that once a week copies and process large blob files (The Source) to a SQL database (The Sink) in python - by reading the input data set line by line, extracting an ID - using that ID to do a lookup on CosmosDB to get an additional piece of data recomposing the output dataset and writing to the sink. 
I have a python script that does this once off (ie reads the entire blob every time) without ADF but am now wanting use the scheduling features on ADF to automate this.</p>

<p>Is there a way of creating a custom copy activity in Python that I can inject my current code logic into. Azure currently only documents .Net custom activities (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a>) which does not fit into my stack.</p>

<p>The python azure SDK doesn't currently have any documentation on creating custom activity.</p>
","<python><azure><azure-data-factory>","2018-02-07 11:33:27","1789","0","1","48690135","<p>If you look at the example, you see that you can run an executable on the node. </p>

<pre><code>     ""typeProperties"": {
          ""command"": ""helloworld.exe"",
          ""folderPath"": ""customactv2/helloworld"",
          ""resourceLinkedService"": {
            ""referenceName"": ""StorageLinkedService"",
            ""type"": ""LinkedServiceReference""
          }
        }
</code></pre>

<p>Further down, in the differences between v1 &amp; v2 they show just running ""cmd"".</p>

<pre><code>cmd /c echo hello world
</code></pre>

<p>So if you can create an executable to kick off your python code, it might just work. You can also use parameters. However, the code will be run on Azure Batch, which provisions a VM for you. This VM might not have all the dependecies that you need. You'll have to create a ""portable"" package for this to work. Maybe this <a href=""https://stackoverflow.com/questions/12059509/create-a-single-executable-from-a-python-project"">post</a> can help you with that.</p>

<p>A bit more classy would be to trigger Azure Functions with a web activity. But is seems to be quite bèta stuff: <a href=""https://ourwayoflyf.com/running-python-code-on-azure-functions-app/"" rel=""nofollow noreferrer"">https://ourwayoflyf.com/running-python-code-on-azure-functions-app/</a></p>
"
"48644455","ADF Onprem SQL Server to ADW Table copy issue","<p>I am getting an error while performing the copy of a given on prem table to ADW, I have checked the data types and all, all looks good here but I dont know what breaking or whats been going on here, so that I can figure it out, Please have a look at the below error, also I am not using the polybase here as In on prem sql server there are text datatypes so for those I am using the varchar(max) approach, as polybase does not work with max data types. </p>

<p>Error:</p>

<blockquote>
  <p>Copy activity encountered a user error at
  Sink:adatawarehouse.database.windows.net side:
  'Type=System.OverflowException,Message=Array dimensions exceeded
  supported range.,Source=Microsoft.DataTransfer.Common,'.</p>
</blockquote>
","<azure-data-factory><polybase>","2018-02-06 13:40:22","162","0","2","48645601","<p>Take a look at the warehouse's limitations here: <a href=""https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-service-capacity-limits"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-service-capacity-limits</a></p>

<p>Also read this: <a href=""https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types</a></p>

<p>It looks like Azure Data warehouse doesnt like varchar(max) very much, and its reasonable because its built to support a business intelligence architecture, where long text fields aren't useful to show the big picture of a company in summarized data. Try using varchar(8000) instead!</p>

<p>Hope this helped!</p>
"
"48644455","ADF Onprem SQL Server to ADW Table copy issue","<p>I am getting an error while performing the copy of a given on prem table to ADW, I have checked the data types and all, all looks good here but I dont know what breaking or whats been going on here, so that I can figure it out, Please have a look at the below error, also I am not using the polybase here as In on prem sql server there are text datatypes so for those I am using the varchar(max) approach, as polybase does not work with max data types. </p>

<p>Error:</p>

<blockquote>
  <p>Copy activity encountered a user error at
  Sink:adatawarehouse.database.windows.net side:
  'Type=System.OverflowException,Message=Array dimensions exceeded
  supported range.,Source=Microsoft.DataTransfer.Common,'.</p>
</blockquote>
","<azure-data-factory><polybase>","2018-02-06 13:40:22","162","0","2","48651930","<p>I was able to solve this by not selecting the Blob Staging, as I was not using the polybase technique that involves blobs for staging, There was something wrong with the blob in the storage account the values are not getting correctly inserted into the blob so they are not getting inserted into the ADW.</p>

<p>But I dont understand if we dont want to use polybase then we also have an option to use Staging Storage account why is that there??</p>

<p>Apparently the issue was resolved.</p>
"
"48633441","Azure Data Factory - Copy activity mappings","<p>I am trying to copy data from a json file to a database. There are two columns on the database that need to be filled with the same json field. However, if I do this mapping, I get an error when running the activity: <code>An item with the same key has already been added</code>. Is it not possible to do this one to many mapping?</p>
","<azure><azure-sql-database><azure-data-factory><azure-data-lake>","2018-02-05 23:57:19","1935","1","1","48637452","<p>Map one column in source dataset schema to multiple columns in destination is not supported yet in ADF. </p>

<p>But there are other ways to achive this, by altering the source dataset schema to let it have a duplicate column. Then you can use these duplicate columns to map to difference destination columns.</p>

<p>Like for SQL dataset, you can use sql query like ""select column1 as column1A, column1 as column1B from xxx"" to get duplicate columns in source dataset. Then you can map column1A to destCol1, column1B to destCol2.</p>

<p>As for JSON file in this case, you can use jsonPathDefinition to define duplicate columns. 
Reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format</a></p>

<p>For example you have a json with format like</p>

<pre><code>{""a"":1,""b"":2}
</code></pre>

<p>Then you can set the dataset format property like</p>

<pre><code>""format"": {
    ""type"": ""JsonFormat"",
    ""filePattern"": ""setOfObjects"",
    ""jsonPathDefinition"": {
        ""a1"": ""$.['a']"",
        ""b"": ""$.['b']"",
        ""a2"": ""$.['a']""
    }
},
</code></pre>

<p>It will create 3 columns a1, b, a2 in the source dataset schema with a1 and a2 refering to the same json field a. Then you will be able to map these columns to different columns in destinations.</p>

<p>Thanks</p>
"
"48629525","Substring of column name in Copy Activity in ADF v2","<p>Is there a way in the V2 Copy Activity to operate upon one of the input columns (of type string) with an expression? Before I load rows to the destination, I need to limit the number of characters in the column.</p>

<p>My hope was to simply switch from something like this:</p>

<pre><code>""ColumnMappings"": ""inColumn: outColumn""
</code></pre>

<p>to something like this:</p>

<pre><code>""ColumnMappings"": ""@substring(inColumn, 1, 300): outColumn""
</code></pre>

<p>If anyone can point me to where I can read-up on where &amp; when string expressions can be used, I could use the guidance.</p>
","<azure-data-factory>","2018-02-05 18:51:49","6168","2","2","48645436","<p>This is the official documentation on expressions and functions: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a></p>

<p>And this is the documentation on mappings: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping</a></p>

<p>Also remember that if you are using a defined query in the copy activity, you can use sql functions like CAST([fieldName] as varchar(300)) to limit the amount of characters on a particular field.</p>

<p>Hope this helped!</p>
"
"48629525","Substring of column name in Copy Activity in ADF v2","<p>Is there a way in the V2 Copy Activity to operate upon one of the input columns (of type string) with an expression? Before I load rows to the destination, I need to limit the number of characters in the column.</p>

<p>My hope was to simply switch from something like this:</p>

<pre><code>""ColumnMappings"": ""inColumn: outColumn""
</code></pre>

<p>to something like this:</p>

<pre><code>""ColumnMappings"": ""@substring(inColumn, 1, 300): outColumn""
</code></pre>

<p>If anyone can point me to where I can read-up on where &amp; when string expressions can be used, I could use the guidance.</p>
","<azure-data-factory>","2018-02-05 18:51:49","6168","2","2","48689576","<p>When you don't have a SQL Source, but your destination is a SQL sink, you can use a Stored Procedure to insert your data into the final table. That way, you can define these kinds of transformations in the stored procedure. I don't think the Data Factory can handle these kinds of activities, it is more intended as an orchestrator.</p>

<p>Have a look here:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-stored-procedure-from-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-stored-procedure-from-sql-sink</a></p>
"
"48613099","Testing & Deploy Azure Data Projects","<p>As Data Engineer and Azure developer I like to automate testing and deployments on the cloud. Currently I'm working with several azure resources such as Azure Data Factory, Azure Data Lake Storage, Azure Data Lake Analytics, SQL Server and PowerBI. </p>

<p>Doing some research I've found a bunch of articles in the Microsoft site related to DevOps in Data Science Team: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/team-data-science-process-for-devops"" rel=""nofollow noreferrer"">Team Data Science Process for Developer Operations</a></p>

<p>Apart from that I didn't find anything interesting. I would like to find a way to accelerate the process to deploy code and assuring quality at the same time. In the end, we want to move our processes into a DevOps ecosystem.</p>

<p>Someone could recommend a lecture o something where I can start? </p>
","<azure><azure-data-factory><azure-data-lake>","2018-02-04 21:17:43","193","-3","1","48650765","<p>The Azure Data Lake Tools team is working on CI/CD features. There may be a blog post on our <a href=""https://blogs.msdn.microsoft.com/azuredatalake/"" rel=""nofollow noreferrer"">Azure Data Lake Blog</a> that is of interest. Otherwise, I suggest to wait or contact a PM on that team to provide requirements and feedback.</p>
"
"48598228","Can one format a table name property in a Dataset?","<p>I have a data source that has sever tables formatted with a date stamp.</p>

<p><strong>Example:</strong> </p>

<ul>
<li><code>dbo.TableName20180203</code></li>
<li><code>dbo.TableName20180204</code></li>
<li><code>dbo.TableName20180205</code></li>
</ul>

<p>When defining a dataset for Blob I can format the folder path based on the slice.</p>

<p><strong>Example:</strong> <code>""folderPath"": ""mypath/{Year}/{Month}/{Day}""</code></p>

<p>I would like to define the table name property like so: <code>""tableName"": ""dbo.TableName{Year}{Month}{Day}""</code></p>

<p>In the blob dataset I define the <code>partitionedBy</code> property, but this does not appear to be allowed if the dataset type is a SQL table.</p>

<p>Another path I have considered but not tried is a stored procedure that accepts parameters and some dynamic SQL.</p>

<p>Is there a way to format the table name like I illustrated above?</p>
","<azure-data-factory>","2018-02-03 14:05:39","70","0","1","48608897","<p>I solved my issue by using the copy activity and using a stored procedure as the source.</p>

<p>The stored procedure takes three parameters: <code>@Year</code>, <code>@Month</code> and <code>@Day</code>. I then use those parameters to build a select statement and run <code>sp_executesql</code>.</p>

<p>In the end I am able to output the data in slices with a frequency of <code>Day</code>.</p>
"
"48593054","Programmatically delete files in Data Lake Storage","<p>I am importing data into an Azure Data Lake and using Data Lake Analytics to convert it from JSON format to CSV using U-SQL. After converting I want to delete the JSON formatted data. It seems that you cannot delete files using U-SQL. </p>

<p>I was wondering what the best way is to schedule a task that will run every 24 hours and will delete all data that has already been converted.</p>

<p>Thanks in advance.</p>
","<azure><azure-data-factory><azure-data-lake>","2018-02-03 01:25:58","1639","0","1","48593282","<p>I would use powershell and azure automation:
<a href=""https://learn.microsoft.com/en-us/azure/automation/automation-first-runbook-textual-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/automation/automation-first-runbook-textual-powershell</a></p>

<p>I think the this should get you started:</p>

<pre><code>Remove-AzureRmDataLakeStoreItem -AccountName $dataLakeStoreName -Paths $myrootdir\mynewdirectory\vehicle1_09142014.csv, $myrootdir\mynewdirectoryvehicle1_09142014_Copy.csv
</code></pre>

<p>Here is some more info on powershell with ADL:
<a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-powershell</a></p>

<p>You could also use a batch job from azure data factory or perhaps even a simple webhook from a logic app lots of options.</p>
"
"48586753","Azure Data Factory AWS PostgreSQL RDS as source connection","<p>I am attempting to use an AWS PostgreSQL RDS instance as my source for a data factory pipeline.  I am unable to get this connection to work in ADF v1 or v2.  I have tried everything from using a PostgreSQL connection to an Azure database for PostgreSQL.  Essentially I am going cloud to cloud, and this connection doesn't seem to be supported yet.  Has anyone else had any luck doing this?</p>
","<postgresql><amazon-web-services><amazon-rds><azure-data-factory>","2018-02-02 16:17:31","1856","1","1","48593315","<p>yes, this is horribly broken as you have found out. Two main problems:</p>

<p>1) You must install the NpgSQL 2.0.14.3. driver (chose core installation option to ensure both x86 and x64 versions are installed)  this version won't validate the server certificate</p>

<p>2) PostgreSQL connector can only enter connection information using by uploading via PowerShell the current GUI does not support the full configuration of the data source:</p>

<p>Here is the example json:</p>

<pre><code>{
    ""name"": ""PostgreSqlLinkedService"",
    ""properties"": {
        ""type"": ""PostgreSql"",
        ""typeProperties"": {
            ""server"": ""&lt;server&gt;"",
            ""database"": ""&lt;database&gt;"",
            ""username"": ""&lt;username&gt;"",
            ""password"": {
                ""type"": ""SecureString"",
                ""value"": ""&lt;password&gt;""
            }
        },
        ""connectVia"": {
            ""referenceName"": ""&lt;name of Integration Runtime&gt;"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p>alternatively, the ODBC driver can work around this issue as you need to specify additional properties on the connection string that are not exposed by the pg connector. you need to add the following value to the DSN:
**<code>sslmode=Require;Trust Server Certificate=true</code>*</p>

<p>and that should resolve the error </p>

<p><em>note:</em> the ODBC nor the Postgresql Connector's currently work with the ADF v2 Lookup activity.</p>
"
"48574489","Parse JSON into U-SQL then convert to csv","<p>I'm trying to convert some telemetry data that is in JSON format into CSV format, then write it out to a file, using U-SQL. </p>

<p>The problem is that some of the JSON key values have periods in them, and so when I'm doing the SELECT operation, U-SQL is not recognizing them. When I check the output file, all that I am seeing is the values for ""p1"". How can I represent the names of the JSON key names in the script so that they are recognized. Thanks in advance for any help!   </p>

<p>Code:</p>

<pre><code>REFERENCE ASSEMBLY MATSDevDB.[Newtonsoft.Json];
REFERENCE ASSEMBLY MATSDevDB.[Microsoft.Analytics.Samples.Formats]; 

USING Microsoft.Analytics.Samples.Formats.Json;


@jsonDocuments = 
    EXTRACT jsonString string 
    FROM @""adl://xxxx.azuredatalakestore.net/xxxx/{*}/{*}/{*}/telemetry_{*}.json"" 
    USING Extractors.Tsv(quoting:false);

@jsonify = 
    SELECT Microsoft.Analytics.Samples.Formats.Json.JsonFunctions.JsonTuple(jsonString) AS json 
    FROM @jsonDocuments;

@columnized = SELECT 
            json[""EventInfo.Source""] AS EventInfoSource,
            json[""EventInfo.InitId""] AS EventInfoInitId,
            json[""EventInfo.Sequence""] AS EventInfoSequence,
            json[""EventInfo.Name""] AS EventInfoName,
            json[""EventInfo.Time""] AS EventInfoTime,
            json[""EventInfo.SdkVersion""] AS EventInfoSdkVersion,
            json[""AppInfo.Language""] AS AppInfoLanguage,
            json[""UserInfo.Language""] AS UserInfoLanguage,
            json[""DeviceInfo.BrowserName""] AS DeviceInfoBrowswerName,
            json[""DeviceInfo.BrowserVersion""] AS BrowswerVersion,
            json[""DeviceInfo.OsName""] AS DeviceInfoOsName,
            json[""DeviceInfo.OsVersion""] AS DeviceInfoOsVersion,
            json[""DeviceInfo.Id""] AS DeviceInfoId,
            json[""p1""] AS p1,
            json[""PipelineInfo.AccountId""] AS PipelineInfoAccountId, 
            json[""PipelineInfo.IngestionTime""] AS PipelineInfoIngestionTime, 
            json[""PipelineInfo.ClientIp""] AS PipelineInfoClientIp,
            json[""PipelineInfo.ClientCountry""] AS PipelineInfoClientCountry,
            json[""PipelineInfo.IngestionPath""] AS PipelineInfoIngestionPath,
            json[""AppInfo.Id""] AS AppInfoId,
            json[""EventInfo.Id""] AS EventInfoId,
            json[""EventInfo.BaseType""] AS EventInfoBaseType,
            json[""EventINfo.IngestionTime""] AS EventINfoIngestionTime
    FROM @jsonify;

OUTPUT @columnized
TO ""adl://xxxx.azuredatalakestore.net/poc/TestResult.csv""
USING Outputters.Csv(quoting : false);
</code></pre>

<p>JSON: </p>

<p>{""EventInfo.Source"":""JS_default_source"",""EventInfo.Sequence"":""1"",""EventInfo.Name"":""daysofweek"",""EventInfo.Time"":""2018-01-25T21:09:36.779Z"",""EventInfo.SdkVersion"":""ACT-Web-JS-2.6.0"",""AppInfo.Language"":""en"",""UserInfo.Language"":""en-US"",""UserInfo.TimeZone"":""-08:00"",""DeviceInfo.BrowserName"":""Chrome"",""DeviceInfo.BrowserVersion"":""63.0.3239.132"",""DeviceInfo.OsName"":""Mac OS X"",""DeviceInfo.OsVersion"":""10"",""p1"":""V1"",""PipelineInfo.IngestionTime"":""2018-01-25T21:09:33.9930000Z"",""PipelineInfo.ClientCountry"":""CA"",""PipelineInfo.IngestionPath"":""FastPath"",""EventInfo.BaseType"":""custom"",""EventInfo.IngestionTime"":""2018-01-25T21:09:33.9930000Z""}</p>
","<json><azure-data-factory><azure-data-lake><u-sql>","2018-02-02 01:39:54","821","1","1","48612244","<p>I got this to work with single quotes and single square brackets, eg</p>

<pre><code>@columnized = SELECT 
            json[""['EventInfo.Source']""] AS EventInfoSource,
...
</code></pre>

<p>Full code:</p>

<pre><code>@columnized = SELECT 
            json[""['EventInfo.Source']""] AS EventInfoSource,
            json[""['EventInfo.InitId']""] AS EventInfoInitId,
            json[""['EventInfo.Sequence']""] AS EventInfoSequence,
            json[""['EventInfo.Name']""] AS EventInfoName,
            json[""['EventInfo.Time']""] AS EventInfoTime,
            json[""['EventInfo.SdkVersion']""] AS EventInfoSdkVersion,
            json[""['AppInfo.Language']""] AS AppInfoLanguage,
            json[""['UserInfo.Language']""] AS UserInfoLanguage,
            json[""['DeviceInfo.BrowserName']""] AS DeviceInfoBrowswerName,
            json[""['DeviceInfo.BrowserVersion']""] AS BrowswerVersion,
            json[""['DeviceInfo.OsName']""] AS DeviceInfoOsName,
            json[""['DeviceInfo.OsVersion']""] AS DeviceInfoOsVersion,
            json[""['DeviceInfo.Id']""] AS DeviceInfoId,
            json[""p1""] AS p1,
            json[""['PipelineInfo.AccountId']""] AS PipelineInfoAccountId, 
            json[""['PipelineInfo.IngestionTime']""] AS PipelineInfoIngestionTime, 
            json[""['PipelineInfo.ClientIp']""] AS PipelineInfoClientIp,
            json[""['PipelineInfo.ClientCountry']""] AS PipelineInfoClientCountry,
            json[""['PipelineInfo.IngestionPath']""] AS PipelineInfoIngestionPath,
            json[""['AppInfo.Id']""] AS AppInfoId,
            json[""['EventInfo.Id']""] AS EventInfoId,
            json[""['EventInfo.BaseType']""] AS EventInfoBaseType,
            json[""['EventINfo.IngestionTime']""] AS EventINfoIngestionTime
    FROM @jsonify;
</code></pre>

<p>My results:</p>

<p><a href=""https://i.stack.imgur.com/7RmxU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7RmxU.png"" alt=""Results""></a></p>
"
"48569437","Programmatically delete datasets by name from Data Factory using .NET SDK","<p>3 questions on Data Factory and the .NET SDK: </p>

<p>1) Is it possible to programmatically delete datasets from a Data Factory via the .NET SDK? I would ideally like to be able to pass the name of a dataset to delete and have it handle that. I've looked through the docs and can't seem to find any clear method.</p>

<p>2) When creating a dataset or pipeline, how can I test the response to see if it was successful.</p>

<p>3) When creating a dataset, is it possible to check the provisioning state and wait until it's been provisioned before creating a pipeline, since datasets must be created before the associated pipeline?</p>
","<.net><azure><sdk><azure-data-factory>","2018-02-01 18:25:16","265","0","1","48622934","<p>1) This is clearly possible with the methods Delete and DeleteAsync. They take the resource group name, data factory name and dataset name as parameter.</p>

<p>Follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net#create-a-data-factory-client"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net#create-a-data-factory-client</a> and use the client object to call the method. It will look like client.datasets.Delete(""ResourceGroupName"",""DataFactoryName"",""DatasetName"")</p>

<p>2) If it doesnt return an error, then it was created successfully.</p>

<p>3) In my experience, Datasets are created almost instantly when using the .net sdk. The same goes for pipelines, have you had any troubles with it? I wouldnt even bother checking.</p>
"
"48536720","Monitor ADF pipeline status using Power BI","<p>Is there any way we can fetch ADF pipeline metadata into Power BI. I want details like dataset names, slice start and end time and status.</p>
","<powerbi><azure-data-factory>","2018-01-31 07:27:43","1682","0","2","48549274","<p>I'm not sure if there is a built-in dataset or functionality in the portal (ADF V2) but the way I do this is by using the Python SDK, I basically get the metadata of every activity run, not sure if you're using ADF V2 and if your are using Python  (there are similar functions for .NET) </p>

<p>More details in <a href=""https://learn.microsoft.com/en-us/python/api/azure.mgmt.datafactory.models.activityrun?view=azure-python"" rel=""nofollow noreferrer"">here</a></p>

<pre><code> def _get_activity_metadata(self, pipeline_run_id, start_time, end_time, activity_name):

     act_runs = self.adf_client.activity_runs.list_by_pipeline_run(self.resource_group_name, self.data_factory_name,
                                                                  run_id=pipeline_run_id,
                                                                  start_time=start_time,
                                                                  end_time=end_time,
                                                                  activity_name=activity_name)
    for act in act_runs:
        act_run_id = act.activity_run_id
        act_status = act.status
        act_error = act.error
        act_run_end = act.activity_run_end
        act_run_end_est = act_run_end.astimezone(pytz.timezone('America/New_York'))
        result = {'last_run_time': act_run_end_est,
                  'status': act_status,
                  'error': act.error,
                  'duration_in_ms': act.duration_in_ms,
                  'input': act.input,
                  'output': act.output
                  }

    return result
</code></pre>

<p>Then, I get the result and insert into a SQL Table/View and visualize in PowerBI, DOMO, SSRS, Excel, etc.</p>
"
"48536720","Monitor ADF pipeline status using Power BI","<p>Is there any way we can fetch ADF pipeline metadata into Power BI. I want details like dataset names, slice start and end time and status.</p>
","<powerbi><azure-data-factory>","2018-01-31 07:27:43","1682","0","2","48557775","<p>In the Data Factory v1, you should have a look at this <a href=""https://blog.gbrueckl.at/2017/01/monitoring-azure-data-factory-powerbi/"" rel=""nofollow noreferrer"">blog post</a>. It explains how to use the REST API to get Data Factory run-data and import it to PowerBI. It even has a PowerBI template file to get you started. An alternative is using PowerShell as described <a href=""https://stackoverflow.com/questions/44259820/how-we-get-data-factory-logging-information"">here</a></p>

<p>For v2, at this moment I'm looking at the built-in logging. You can configure it  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">with PowerShell</a>. I'm writing the logs to a Storage Account and import it with PowerBI, but it's also possible to monitor it with OMS for example.</p>
"
"48525099","Python on anaconda cannot find azure.mgmt.datafactory","<p>I am trying to run this tutorial</p>

<p><a href=""https://learn.microsoft.com/en-US/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-US/azure/data-factory/quickstart-create-data-factory-python</a></p>

<p>but I fail to install the packages. I tried several installations but I keep getting the error <code>No module named 'azure.mgmt.datafactory'</code> when trying to run <code>from azure.mgmt.datafactory import DataFactoryManagementClient</code>.</p>

<p>I am using anaconda and windows 10. </p>

<p>I tried running the recommended anaconda packages <a href=""https://anaconda.org/anaconda/azure"" rel=""nofollow noreferrer"">https://anaconda.org/anaconda/azure</a> and <a href=""https://anaconda.org/clinicalgraphics/azure-mgmt-resource"" rel=""nofollow noreferrer"">https://anaconda.org/clinicalgraphics/azure-mgmt-resource</a> under a python 3.5 environment and I also tried to manually install everything from github (<a href=""https://github.com/Azure/azure-sdk-for-python"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-sdk-for-python</a>) using </p>

<pre><code>git clone git://github.com/Azure/azure-sdk-for-python.git 
cd azure-sdk-for-python 
python setup.py install
</code></pre>

<p>In both the normal (Python 3.6) and the new (Python 3.5, using  <a href=""https://stackoverflow.com/questions/42978349/anaconda-version-with-python-3-5?noredirect=1&amp;lq=1"">Anaconda version with Python 3.5</a>) environment. None of this worked. </p>

<p>What am I missing?</p>

<p>(Note that <code>from azure.mgmt.resource import ResourceManagementClient</code> worked fine with the anaconda installation)</p>

<p><strong>EDIT</strong></p>

<p>After the first response, I ran the following commands from the powershell</p>

<pre><code> pip install azure-mgmt-resource
 pip install azure-mgmt-datafactory
 pip install azure-mgmt
</code></pre>

<p>which resulted in <code>ModuleNotFoundError: No module named 'azure.mgmt'</code></p>

<p>Uninstalling the three packages and installing azure-mgmt as a first one did not solve the issue either. However, I don't know how to uninstall the manually installed package from <code>python setup.py install</code>, which still might be an issue.</p>
","<python><azure><anaconda><azure-data-factory>","2018-01-30 15:34:03","1978","1","2","48525198","<p>Have you tried pip install in powershell/cmd?</p>

<pre><code>pip install azure-mgmt-datafactory
</code></pre>

<p>Update (Jan's answer):</p>

<pre><code>pip freeze &gt; requirements.txt
pip uninstall -r requirements.txt
python -m pip install azure-common
python -m pip install azure-mgmt
python -m pip install azure-mgmt-datafactory (this might not be needed as it comes with azure-mgmt)
</code></pre>
"
"48525099","Python on anaconda cannot find azure.mgmt.datafactory","<p>I am trying to run this tutorial</p>

<p><a href=""https://learn.microsoft.com/en-US/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-US/azure/data-factory/quickstart-create-data-factory-python</a></p>

<p>but I fail to install the packages. I tried several installations but I keep getting the error <code>No module named 'azure.mgmt.datafactory'</code> when trying to run <code>from azure.mgmt.datafactory import DataFactoryManagementClient</code>.</p>

<p>I am using anaconda and windows 10. </p>

<p>I tried running the recommended anaconda packages <a href=""https://anaconda.org/anaconda/azure"" rel=""nofollow noreferrer"">https://anaconda.org/anaconda/azure</a> and <a href=""https://anaconda.org/clinicalgraphics/azure-mgmt-resource"" rel=""nofollow noreferrer"">https://anaconda.org/clinicalgraphics/azure-mgmt-resource</a> under a python 3.5 environment and I also tried to manually install everything from github (<a href=""https://github.com/Azure/azure-sdk-for-python"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-sdk-for-python</a>) using </p>

<pre><code>git clone git://github.com/Azure/azure-sdk-for-python.git 
cd azure-sdk-for-python 
python setup.py install
</code></pre>

<p>In both the normal (Python 3.6) and the new (Python 3.5, using  <a href=""https://stackoverflow.com/questions/42978349/anaconda-version-with-python-3-5?noredirect=1&amp;lq=1"">Anaconda version with Python 3.5</a>) environment. None of this worked. </p>

<p>What am I missing?</p>

<p>(Note that <code>from azure.mgmt.resource import ResourceManagementClient</code> worked fine with the anaconda installation)</p>

<p><strong>EDIT</strong></p>

<p>After the first response, I ran the following commands from the powershell</p>

<pre><code> pip install azure-mgmt-resource
 pip install azure-mgmt-datafactory
 pip install azure-mgmt
</code></pre>

<p>which resulted in <code>ModuleNotFoundError: No module named 'azure.mgmt'</code></p>

<p>Uninstalling the three packages and installing azure-mgmt as a first one did not solve the issue either. However, I don't know how to uninstall the manually installed package from <code>python setup.py install</code>, which still might be an issue.</p>
","<python><azure><anaconda><azure-data-factory>","2018-01-30 15:34:03","1978","1","2","48546136","<p>Ok, this is how I got the required azure libraries to work (thx to Saul Cruy, who gave me the idea)</p>

<p>Using this post <a href=""https://stackoverflow.com/questions/11248073/what-is-the-easiest-way-to-remove-all-packages-installed-by-pip?noredirect=1&amp;lq="">What is the easiest way to remove all packages installed by pip?</a>, I created a requirements file in PowerShell</p>

<pre><code>pip freeze &gt; requirements.txt
</code></pre>

<p>In this file, I manually kept only the entries with azure.
Then, I deleted all packages in the file</p>

<pre><code>pip uninstall -r requirements.txt
</code></pre>

<p>The steps above were repeated twice, as upon first delete, some azure packages survived.</p>

<p>Then, I ran (all in PowerShell, in that order)</p>

<pre><code>python -m pip install azure-common
python -m pip install azure-mgmt
python -m pip install azure-mgmt-datafactory
</code></pre>

<p>The reason might(!) be that installing packages in the anaconda console using the <code>conda</code> commands causes confusion in the dependencies (I tried a similar approach in a conda environment as it seemed like a good idea to seperate the azure packages from the other ones, but without success).</p>
"
"48523093","Create an Azure Data Factory pipeline to copy new records from DocumentDB to Azure SQL","<p>I am trying to find the best way to copy yesterday's data from DocumentDB to Azure SQL.</p>

<p>I have a working DocumentDB database that is recording data gathered via a web service.  I would like to routinely (daily) copy all <strong>new</strong> records from the DocumentDB to an Azure SQL DB table.  In order to do so I have created and successfully executed an Azure Data Factory Pipeline that copies records with a datetime > '2018-01-01', but I've only ever been able to get it to work with an arbitrary date - never getting the date from a variable.</p>

<p>My research on DocumentDB SQL querying shows that it has Mathematical, Type checking, String, Array, and Geospatial functions but no date-time functions equivalent to SQL Server's getdate() function.</p>

<p>I understand that Data Factory Pipelines have some system variables that are accessible, including utcnow().  I cannot figure out, though, how to actually use those by editing the JSON successfully.  If I try just including utcnow() within the query I get an error from DocumentDB that ""'utcnow' is not a recognized built-in function name"".</p>

<pre><code>""query"": ""SELECT * FROM c where c.StartTimestamp &gt; utcnow()"",
</code></pre>

<p>If I try instead to build the string within the JSON using utcnow() I can't even save it because of a syntax error:</p>

<pre><code>""query"": ""SELECT * FROM c where c.StartTimestamp &gt; "" + utcnow(),
</code></pre>

<p>I am willing to try a different technology than a Data Factory Pipeline, but I have a lot of data in our DocumentDB so I'm not interested in abandoning that, and I have much greater familiarity with SQL programming and need to move the data there for joining and other analysis.</p>

<p>What is the easiest and best way to copy those new entries over every day into the staging table in Azure SQL?</p>
","<azure-sql-database><azure-cosmosdb><azure-data-factory>","2018-01-30 13:50:55","1028","1","1","48525026","<p>Are you using ADF V2 or V1? </p>

<p>For ADF V2.
I think that you can follow the incremental approach that they recommend, for example you could have a watermark table (it could be in your target Azure SQL database) and two lookups activities, one of the lookups will obtain the previous run watermark value (it could be date, integer, whatever your audit value is) and another lookup activity to obtain the MAX (watermark_value, i.e. date) of your source document and have a CopyActivity that gets all the values where the c.StartTimeStamp&lt;=MaxWatermarkValueFromSource AND c.StartTimeStamp>LastWaterMarkValue.</p>

<p>I followed this example using the Python SDK and worked for me.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-powershell</a></p>
"
"48519589","Iterating Through azure SQL table in Azure Data Factory","<p>I'm using ADF 2 and trying to grapple with the web activity.</p>

<p>The tasks are.</p>

<p>Copy file from blob storage and put the data in an azure SQL database
Iterate through the data and use a PUT call to a REST API to update the data</p>

<p>Okay so I can get the data in the table no problem. I can also make the call using a Web activity to the the API and put some hard coded data there.</p>

<p>But I've been trying to use a For Each to iterate through the table I have and call the web activity to pass that data to the API</p>

<p>This is where I'm stuck. I'm New to data factory and been through all their standard help information but not getting any where.</p>

<p>Any help is appreciated</p>
","<azure><azure-data-factory>","2018-01-30 10:50:09","6395","1","1","48593353","<p>I think you need to drive the foreach via a SQL lookup task that populates a data set and then call the activity for each row:</p>

<p>here are some posts to get you started:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity</a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a></p>

<p>replace the copy activity with the web call in the tutorial below:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal</a></p>
"
"48517987","Data Factory v2: Data Lake incremental copy activity","<p>Within my <strong>Azure Data Lake</strong> directory <code>ingest</code>, I have timestamp-named subdirectories:</p>

<pre><code>ingest/1510395023/file.tsv
ingest/1510425023/file.tsv
ingest/1510546210/file.tsv
...
</code></pre>

<p>In an <strong>Azure Data Factory v2</strong> copy activity (<em>Data Lake</em> to <em>SQL Server</em>), how can I filter the <em>Data Lake</em> directories to those greater than a specified timestamp?</p>

<p>Assume I will record the already-copied timestamps into a <em>SQL Server</em>, and on the next pipeline run, I only want to copy incremental new directories based on a <em>Lookup Activity</em>.</p>

<p><em>Data Factory</em> documentation references <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#logical-functions"" rel=""nofollow noreferrer"">logical functions</a>, but I don't understand how to implement them in this scenario.</p>

<p><strong>Alternatively</strong>: If a logical comparison filter is available on the file name, as opposed to the directories, that would be helpful as well.</p>

<p><strong>Note</strong>: I want to load in historical data as well, so only filtering a timestamp greater or equal to <code>now</code> is insufficient.</p>
","<sql-server><azure-data-factory><azure-data-lake>","2018-01-30 09:28:15","911","1","1","48533500","<p>I assume you have a Data Lake Analytics account.</p>

<p>Using a U-SQL activity, you can copy the targeted files to a single file, which you can then copy using a copy activity.</p>

<p>You can accomplish this by extracting a <a href=""https://msdn.microsoft.com/en-us/azure/data-lake-analytics/u-sql/input-files-u-sql#inputfilesetpath"" rel=""nofollow noreferrer"">file set</a> and filtering it by its virtual column.</p>

<p>Let <code>@timestamp</code> string be the input parameter, which was obtained via a lookup activity and which is the latest timestamp that is already processed:</p>

<pre><code>@delta = 
    SELECT Col1, Col2 
    FROM(
        EXTRACT 
            [Col1] string,
            [Col2] string,
            [Timestamp] string
        FROM ""ingest/{Timestamp}/file.tsv""
        USING Extractors.Tsv(skipFirstNRows : 1/* change if the header is not part of the file*/)
    ) AS T
    WHERE long.Parse([Timestamp]) &gt; long.Parse(@timestamp);


OUTPUT @delta
TO ""delta/file.tsv""
USING Outputters.Tsv(outputHeader : true);
</code></pre>

<p>From there you can copy ""delta/file.tsv"" to your database.</p>
"
"48516256","Azure Data factory copy activity having time lag while triggering","<p>I am having several Copy activities in Azure Data Factory Pipeline from Azure SQL Source to Azure Data Lake Store for  different tables independent of each other.
I have scheduled it for every 15 mins. I am seeing a time lag of around 1 minute while triggering such as 12:00 AM jobs are triggering at 12:01 AM.</p>

<p>Also only 2 copy activities getting kick started at a time out of 20+ activities remaining getting triggered one by one .</p>

<p>Is this expected behavior? Any ways to eradicate this time lag? </p>
","<azure><azure-data-factory>","2018-01-30 07:39:22","150","1","1","48517545","<p>According to <a href=""https://azure.microsoft.com/en-us/support/legal/sla/data-factory/v1_0/"" rel=""nofollow noreferrer"">SLA for Data Factory</a>, Activity Runs SLA is within 4 minutes. And also a common practice is to avoid o'clock spike, especially for 12AM (UTC).</p>
"
"48496775","ADF V2 - MongoDB to ADLS","<p>I’m loading datas from MongoDB into Azure Data Lake Storage in JSon format, and I’m facing two issues : </p>

<ul>
<li>String fields are truncated, if I have a 4000 characters  value in Mongo it become a 250 characters value in my json output file.
Is this a known behavior ? Any way to avoid this data loss ?</li>
<li>Generated Json file are flattened version of my MongoDb document.
In mongo : </li>
</ul>

<p><a href=""https://i.stack.imgur.com/kx9sZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kx9sZ.jpg"" alt=""Mongo structure""></a></p>

<p>Json in ADLS : </p>

<pre><code>{
             ""_id"": ""NonConformityLine_492"",
             ""Id"":492,
             ""Code"": """",
             ""Label"": """",
             ""Type"": """",
             ""Comments"": """",
             ""DateRecorded"": """",
             ""Details_DeliveryAccepted"": true,
             ""Details_TemperatureIssueOnAllDelivery"": false,
             ""Details_TemperatureType"": """",
             ""Details_TemperatureValue"": """",
             ""Details_TemperatureTolerance"": """",
             ""IsSetOnEasyVista"": false,
             ""Level"": 2,
             ""ManufacturerCode"": ""n/a"",
             ""NonConformityReportCode"": ""
             ""NonConformityTypeCode"": """",
             ""NonConformityTypeTitle"": """",
             ""OrderCode"": """",
             ""RawMaterialCode"": """",
             ""RecorderName"": """",
             ""Response_ResponseFullText"": ""
             ""Response_ResponseDate"": ""”
             ""Response_ResponseAuthor"": """",
             ""SiteOrPosCode"": """",
             ""Status"": """",
             ""SupplierCode"": """",
             ""WarehouseCode"": """"
}
</code></pre>

<p>Any way to keep the data structure ? </p>

<p>Regards,</p>
","<mongodb><azure><azure-data-factory>","2018-01-29 08:00:36","612","0","1","48678870","<p>These are two limitations of ADF MongoDB connector implementation.</p>

<ul>
<li>String fields are truncated if it's longer than 255 currently. The default string column length is 255 as a compromise with the string data in MongoDB that will be relatively short (names, addresses, titles, etc.), and a larger value will impact performance and memory size. Later this month, ADF will expose a configuration for this issue.</li>
<li>The Mongo documents getting flattened is kind of by-design currently because we were using a tabular intermediate data structure for orthogonal data transfer across M*N data stores. We did realize that this approach doesn't work well for NoSql data stores and we are improving this area. MongoDB is in the backlog list. </li>
</ul>
"
"48478668","Azure Data Factory Flows","<p>my Data Factory Gateway (Azure IR) is having troubles to connect to cloud, this happens only when I try to use service principals to authenticate to data lake, in my case I have local (on promise)  proxy, I think that i need to add exception on the proxy to some endpoints (like *.login.windows.net), but I don't have the list of all endpoints that i should allows. can you help me with a list please.
Regards</p>
","<azure><azure-data-factory>","2018-01-27 17:45:07","149","0","1","48493396","<p>Please refer to the instructions described here for configuring your corporate firewall and windows firewall on the IR machine:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#ports-and-firewall"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#ports-and-firewall</a></p>
"
"48475720","Error when using ADF to copy from SQL Server to ADW","<p>I am getting the below error while using ADF, my source is an on-premise SQL Server and destination is ADW. Please shed some light on this:</p>

<blockquote>
  <p>Database operation failed. Error message from database execution : </p>
  
  <p>ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error happened when loading data into SQL Data Warehouse,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=Query aborted-- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.
  (/22799e45-a078-4f43-9050-87294328db61/Data.28013af9-5443-4bc1-bfd4-b0299552069d.txt)Column ordinal: 4, Expected data type: VARCHAR(800) collate SQL_Latin1_General_CP1_CI_AS NOT NULL, Offending value: </p>
</blockquote>
","<sql-server><azure><azure-data-factory><azure-synapse>","2018-01-27 12:18:33","602","0","1","48518726","<p>This was solved by using the varchar(MAX) with HASH Distribution as my source conatained a text datatype and it contained some values larger then 8k character and polybase do not support max values, </p>

<p>So had to copy the data without polybase and use varchar(MAX) in place of text.</p>

<p>When I ran the pipeline all the data was copied into the ADW.</p>
"
"48459667","Cannot execute dataset with runtime parameters","<p>We are creating some pipelines in Azure Datafactory V2 and want to re-use as much as we can the elements. Currently we are facing a problem with pipelines that use a dataset with runtime parameters. The error message we are having is:</p>
<ul>
<li>The template validation failed: 'the string character 'P' at position '41' is not expected..'</li>
</ul>
<p>We followed the directives from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#examples"" rel=""nofollow noreferrer"">MS documentation</a> but is still throws the error.
Below you will find the pipeline and dataset jsons:</p>
<pre><code>Dataset:

    {
    &quot;name&quot;: &quot;DS-PARAM&quot;,
    &quot;properties&quot;: {
        &quot;linkedServiceName&quot;: {
            &quot;referenceName&quot;: &quot;LS-ADLS&quot;,
            &quot;type&quot;: &quot;LinkedServiceReference&quot;
        },
        &quot;type&quot;: &quot;AzureDataLakeStoreFile&quot;,
        &quot;typeProperties&quot;: {
            &quot;fileName&quot;: {
                &quot;value&quot;: &quot;@dataset().file&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            },
            &quot;folderPath&quot;: {
                &quot;value&quot;: &quot;@dataset().directory&quot;,
                &quot;type&quot;: &quot;Expression&quot;
            }
        }
    },
    &quot;parameters&quot;: {
        &quot;directory&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;file&quot;: {
            &quot;type&quot;: &quot;String&quot;
        }
    }}
    

Pipeline:
    {
        &quot;name&quot;: &quot;pipeline2&quot;,
        &quot;properties&quot;: {
            &quot;activities&quot;: [
                {
                    &quot;name&quot;: &quot;Copy1&quot;,
                    &quot;type&quot;: &quot;Copy&quot;,
                    &quot;dependsOn&quot;: [],
                    &quot;policy&quot;: {
                        &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                        &quot;retry&quot;: 0,
                        &quot;retryIntervalInSeconds&quot;: 20
                    },
                    &quot;typeProperties&quot;: {
                        &quot;source&quot;: {
                            &quot;type&quot;: &quot;AzureDataLakeStoreSource&quot;,
                            &quot;recursive&quot;: true
                        },
                        &quot;sink&quot;: {
                            &quot;type&quot;: &quot;AzureDataLakeStoreSink&quot;
                        },
                        &quot;enableStaging&quot;: false,
                        &quot;cloudDataMovementUnits&quot;: 0
                    },
                    &quot;inputs&quot;: [
                        {
                            &quot;referenceName&quot;: &quot;DS-PARAM&quot;,
                            &quot;type&quot;: &quot;DatasetReference&quot;,
                            &quot;parameters&quot;: {
                                &quot;directory&quot;: &quot;/&quot;,
                                &quot;file&quot;: &quot;readme.txt&quot;
                            }
                        }
                    ],
                    &quot;outputs&quot;: [
                        {
                            &quot;referenceName&quot;: &quot;DS-PARAM&quot;,
                            &quot;type&quot;: &quot;DatasetReference&quot;,
                            &quot;parameters&quot;: {
                                &quot;directory&quot;: &quot;/&quot;,
                                &quot;file&quot;: &quot;readme2.txt&quot;
                            }
                        }
                    ]
                }
            ]
        }
    }
</code></pre>
<p>The linked service is for a Datalake Store. We have tried the same code with an ARM template and also by building the Dataset and Pipeline from the new interface MS released on 16.01.2018.</p>
<p>Any help would be appreciated.</p>
<p>Thank you</p>
","<azure-data-factory>","2018-01-26 10:35:06","550","0","1","48473664","<p><strong>Solution: Remove all dashes from the dataset name.</strong></p>

<p>I spent all day trying to solve the exact same problem and it drove me absolutely crazy. The good news is I figured out what was causing the problem. The bad news is we're going to have to adopt a different naming convention until the Azure team fixes the problem. I sent them my observations through the feedback option.</p>

<p>The unexpected character about which it complains is always the character immediately after the first dash. So call your dataset DSPARAM or dsParam or anything that contains only letters. I haven't tried with underscores and digits.</p>

<p>Regards</p>
"
"48445172","Is DotNetActivity still available on ADF V2?","<p>Recently project changed ADF version from v1 to v2.
The pipeline, from legacy workpiece, contains dotnetactivity that runs on v1.(w/IDoNetActivity Interface inhered to class)</p>

<p>I was searching if dotnetactivity still available on v2.
But official adf v2 documents does not seem to have dotnetactivity on activity list instead customactivity with console app. </p>

<p>If no, i may need to modify all dotnetactivity and complete corresponding test again...</p>
","<azure><pipeline><azure-data-factory><custom-activity>","2018-01-25 14:24:04","119","0","1","48456369","<p>DonNetActivity has been replaced by the Custom Activity in V2, as it has flexibility to run any command which is not limited to DotNet code. If you have a lot of projects which depends on the V1 interface, you might want to implement your own wrapper executable which converts V2 input (JSON files) to V1 interface. A migration tool is also considered in backlog of V2 GA, but no commitment yet.</p>
"
"48421040","ADF Not Copying Some tables in the pipeline","<p>I have 1000+ tables. </p>

<p>I have been using multiple pipelines to from source to destination.</p>

<p>I am copying each and every bit of data in one go.</p>

<p>My source is on-premise SQL server and the destination is ADW.</p>

<p>Some of my tables have been copying with no errors, but some of them are not getting copied to ADW; tables are being created but with no data in them. </p>

<p>I can see the produced slicer it stats the Data Read but no Data written inside the ADW.</p>

<p>Did something like this happen to anyone?</p>
","<azure><azure-data-factory>","2018-01-24 11:10:18","52","0","2","48433334","<p>You need to share your pipeline and dataset details here. Also, turn on errorlogs and see of there's an issue in the underlying data itself.</p>
"
"48421040","ADF Not Copying Some tables in the pipeline","<p>I have 1000+ tables. </p>

<p>I have been using multiple pipelines to from source to destination.</p>

<p>I am copying each and every bit of data in one go.</p>

<p>My source is on-premise SQL server and the destination is ADW.</p>

<p>Some of my tables have been copying with no errors, but some of them are not getting copied to ADW; tables are being created but with no data in them. </p>

<p>I can see the produced slicer it stats the Data Read but no Data written inside the ADW.</p>

<p>Did something like this happen to anyone?</p>
","<azure><azure-data-factory>","2018-01-24 11:10:18","52","0","2","48586863","<p>If you take the dataset(s) that is generating the blank table(s) out and run it on it's own in a separate pipeline does it work?  Is it failing only while running along side all the other ones?</p>
"
"48417129","How to fix Data Lake Analytics script","<p>I would like to use Azure Data Factory with Azure Data Lake Analytics as action, but without success.</p>

<p>This is my PIPELINE script</p>

<pre><code>{
""name"": ""UsageStatistivsPipeline"",
""properties"": {
    ""description"": ""Standardize JSON data into CSV, with friendly column names &amp; consistent output for all event types. Creates one output (standardized) file per day."",
    ""activities"": [{
            ""name"": ""UsageStatisticsActivity"",
            ""type"": ""DataLakeAnalyticsU-SQL"",
            ""linkedServiceName"": {
                ""referenceName"": ""DataLakeAnalytics"",
                ""type"": ""LinkedServiceReference""
            },
            ""typeProperties"": {
                ""scriptLinkedService"": {
                    ""referenceName"": ""BlobStorage"",
                    ""type"": ""LinkedServiceReference""
                },
                ""scriptPath"": ""adla-scripts/usage-statistics-adla-script.json"",
                ""degreeOfParallelism"": 30,
                ""priority"": 100,
                ""parameters"": {
                    ""sourcefile"": ""wasb://nameofblob.blob.core.windows.net/$$Text.Format('{0:yyyy}/{0:MM}/{0:dd}/0_647de4764587459ea9e0ce6a73e9ace7_2.json', SliceStart)"",
                    ""destinationfile"": ""$$Text.Format('wasb://nameofblob.blob.core.windows.net/{0:yyyy}/{0:MM}/{0:dd}/DailyResult.csv', SliceStart)""
                }
            },
            ""inputs"": [{
                    ""type"": ""DatasetReference"",
                    ""referenceName"": ""DirectionsData""
                }
            ],
            ""outputs"": [{
                    ""type"": ""DatasetReference"",
                    ""referenceName"": ""OutputData""
                }
            ],
            ""policy"": {
                ""timeout"": ""06:00:00"",
                ""concurrency"": 10,
                ""executionPriorityOrder"": ""NewestFirst""
            }
        }
    ],
    ""start"": ""2018-01-08T00:00:00Z"",
    ""end"": ""2017-01-09T00:00:00Z"",
    ""isPaused"": false,
    ""pipelineMode"": ""Scheduled""
}}
</code></pre>

<p>I have two parameters variables <code>sourcefile</code> and <code>destinationfile</code>, which are dynamic (path is from Date).</p>

<p>Then I have this ADLA script for execution.</p>

<pre><code>REFERENCE ASSEMBLY master.[Newtonsoft.Json];
REFERENCE ASSEMBLY master.[Microsoft.Analytics.Samples.Formats]; 

USING Microsoft.Analytics.Samples.Formats.Json;

@Data = 
    EXTRACT 
        jsonstring string
    FROM @sourcefile
    USING Extractors.Tsv(quoting:false);


@CreateJSONTuple = 
    SELECT 
        JsonFunctions.JsonTuple(jsonstring) AS EventData 
    FROM 
        @Data;

@records = 
    SELECT
        JsonFunctions.JsonTuple(EventData[""records""], ""[*].*"") AS record
    FROM 
        @CreateJSONTuple;

@properties =
    SELECT 
        JsonFunctions.JsonTuple(record[""[0].properties""]) AS prop,
        record[""[0].time""] AS time
    FROM 
        @records;

@result =
    SELECT 
        ...
    FROM @properties;


OUTPUT @result
TO @destinationfile
USING Outputters.Csv(outputHeader:false,quoting:true);
</code></pre>

<p>Job execution fails and the error is : 
<a href=""https://i.stack.imgur.com/DEC6W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DEC6W.png"" alt=""Error Detail""></a></p>

<p><strong>EDIT:</strong></p>

<p>It seems, that Text.Format is not executed and passed into script like string ... Then in Data Lake Analytics Job detail is this : </p>

<pre><code>DECLARE @sourcefile string = ""$$Text.Format('wasb://nameofblob.blob.core.windows.net/{0:yyyy}/{0:MM}/{0:dd}/0_647de4764587459ea9e0ce6a73e9ace7_2.json', SliceStart)"";
</code></pre>
","<azure-data-factory><azure-data-lake>","2018-01-24 07:37:39","100","0","1","48534235","<p>In your code sample, the sourcefile parameter is not defined the same way as destinationfile. The latter appears to be correct while the former does not.</p>

<p>The whole string should be wrapped inside $$Text.Format() for both:</p>

<pre><code>""paramName"" : ""$$Text.Format('...{0:pattern}...', param)""
</code></pre>

<p>Also consider passing only the formatted date like so:</p>

<pre><code>""sliceStart"": ""$$Text.Format('{0:yyyy-MM-dd}', SliceStart)""
</code></pre>

<p>and then doing the rest in U-SQL:</p>

<pre><code>DECLARE @sliceStartDate DateTime = DateTime.Parse(@sliceStart);

DECLARE @path string = String.Format(""wasb://path/to/file/{0:yyyy}/{0:MM}/{0:dd}/file.csv"", @sliceStartDate);
</code></pre>

<p>Hope this helps</p>
"
"48410366","Azure data factory V2 Azure ML batch processing","<p>I have tried to create ADF pipe line by invoking azure ML batch processing but i am not able to find to azure ML batch processing tasks. Can anyone help me on how can i use AZURE ML Batch process in AZURE DATA FACTORY V2 ?</p>
","<azure><machine-learning><azure-data-factory>","2018-01-23 20:36:34","491","0","2","48421674","<p>The BatchExecution activity is designed for invoking ML batch processing tasks. Hope below doc helps:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning</a></p>
"
"48410366","Azure data factory V2 Azure ML batch processing","<p>I have tried to create ADF pipe line by invoking azure ML batch processing but i am not able to find to azure ML batch processing tasks. Can anyone help me on how can i use AZURE ML Batch process in AZURE DATA FACTORY V2 ?</p>
","<azure><machine-learning><azure-data-factory>","2018-01-23 20:36:34","491","0","2","73498927","<p>You can now consider using Azure machine learning Execute Pipeline activity in Azure data factory to execute AML pipelines for batch ML tasks.
Also, if a pipeline endpoint ID is provided is provided as the 'Machine learning pipeline ID' type, then the Azure data factory pipeline will not need to be re-deployed even if a new machine learning pipeline is published in the AML workspace.</p>
<p>References:</p>
<ol>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service"" rel=""nofollow noreferrer"">Execute AML pipelines in ADF</a></p>
</li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-deploy-pipelines#create-a-versioned-pipeline-endpoint"" rel=""nofollow noreferrer"">Create a versioned pipeline endpoint</a></p>
</li>
</ol>
"
"48409518","HD Insight 3.6: Data Factory V2- Running Script Actions to Install Dependencies for On-Demand Cluster","<p>I am currently trying to use script actions to deploy an On-Demand cluster (spark), using Data Factory V2, to install dependencies that my Python scripts need to run. However, the concept of script actions seems lost with V2. </p>

<p>How do I install packages on the cluster nodes as of now ? Is there an alternative 
?</p>
","<azure><apache-spark><cluster-computing><azure-hdinsight><azure-data-factory>","2018-01-23 19:36:32","144","2","1","48632088","<p>As far as i know, it seems that script actions are still supported in HD Insight,for installing external dependencies:</p>
<blockquote>
<p>From the Azure Portal, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under Browse All &gt; HDInsight Clusters.</p>
<p>From the Spark cluster blade, click Script Actions from the left pane. Run the custom action that installs TensorFlow in the head nodes and the worker nodes. The bash script can be referenced from: <a href=""https://hdiconfigactions.blob.core.windows.net/linuxtensorflow/tensorflowinstall.sh"" rel=""nofollow noreferrer"">https://hdiconfigactions.blob.core.windows.net/linuxtensorflow/tensorflowinstall.sh</a> Visit the documentation on how to use custom script actions.</p>
</blockquote>
<p>Full documentation is available <a href=""https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-python-package-installation"" rel=""nofollow noreferrer"">here</a></p>
"
"48384813","How to create/access Hive tables with external Metastore on additional Azure Blob Storage?","<p>I want to perform some data transformation in Hive with Azure Data Factory (v1) running a Azure HDInsight On Demand cluster (3.6).</p>

<p>Since the HDInsight On Demand cluster gets destroyed after some idle time and I want/need to keep the metadata about the Hive tables (e.g. partitions), I also configured an external Hive metastore, using a Azure SQL Server database.</p>

<p>Now I want to store all production data on a separate storage account than the one ""default"" account, where Data Factory and HDInsight also create containers for logging and other runtime data.</p>

<p>So I have the following resources:</p>

<ul>
<li>Data Factory with HDInsight On Demand (as a linked service)</li>
<li>SQL Server and database for Hive metastore (configured in HDInsight On Demand)</li>
<li>Default storage account to be used by Data Factory and HDInsight On Demand cluster (blob storage, general purpose v1)</li>
<li>Additional storage account for data ingress and Hive tables (blob storage, general purpose v1)</li>
</ul>

<p>Except the Data Factory, which is in location <code>North Europe</code>, all resources are in the same location <code>West Europe</code>, which should be fine (the HDInsight cluster must be in the same location as any storage accounts you want to use). All Data Factory related deployment is done using the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactories.datafactorymanagementclient?view=azure-dotnet"" rel=""nofollow noreferrer"">DataFactoryManagementClient</a> API.</p>

<p>An example Hive script (deployed as a HiveActivity in Data Factory) looks like this:</p>

<pre><code>CREATE TABLE IF NOT EXISTS example_table (
  deviceId string,
  createdAt timestamp,
  batteryVoltage double,
  hardwareVersion string,
  softwareVersion string,
)
PARTITIONED BY (year string, month string) -- year and month from createdAt
CLUSTERED BY (deviceId) INTO 256 BUCKETS
STORED AS ORC
LOCATION 'wasb://container@additionalstorage.blob.core.windows.net/example_table'
TBLPROPERTIES ('transactional'='true');

INSERT INTO TABLE example_table PARTITIONS (year, month) VALUES (""device1"", timestamp ""2018-01-22 08:57:00"", 2.7, ""hw1.32.2"", ""sw0.12.3"");
</code></pre>

<p>Following the documentation <a href=""https://learn.microsoft.com/en-us/azure/data-factory/compute-linked-services"" rel=""nofollow noreferrer"">here</a> and <a href=""https://learn.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-create-linux-clusters-adf"" rel=""nofollow noreferrer"">here</a>, this should be rather straightforward: Simply add the new storage account as an additional linked service (using the <code>additionalLinkedServiceNames</code> property).</p>

<p>However, this resulted in the following exceptions when a Hive script tried to access a table stored on this account:</p>

<pre><code>IllegalStateException Error getting FileSystem for wasb : org.apache.hadoop.fs.azure.AzureException: org.apache.hadoop.fs.azure.KeyProviderException: ExitCodeException exitCode=2: Error reading S/MIME message
139827842123416:error:0D06B08E:asn1 encoding routines:ASN1_D2I_READ_BIO:not enough data:a_d2i_fp.c:247:
139827842123416:error:0D0D106E:asn1 encoding routines:B64_READ_ASN1:decode error:asn_mime.c:192:
139827842123416:error:0D0D40CB:asn1 encoding routines:SMIME_read_ASN1:asn1 parse error:asn_mime.c:517:
</code></pre>

<p><a href=""https://issues.apache.org/jira/browse/SPARK-19123?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel"" rel=""nofollow noreferrer"">Some googling told me</a> that this happens, when the key provider is not configured correctly (i.e. the exceptions is thrown because it tries to decrypt the key even though it is not encrypted). After manually setting <code>fs.azure.account.keyprovider.&lt;storage_name&gt;.blob.core.windows.net</code> to <code>org.apache.hadoop.fs.azure.SimpleKeyProvider</code> it seemed to work for reading and ""simple"" writing of data to tables, but failed again when the metastore got involved (creating a table, adding new partitions, ...):</p>

<pre><code>ERROR exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.azure.AzureException com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.)
  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:783)
  at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4434)
  at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:316)
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
[...]
  at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.azure.AzureException com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:38593)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:38561)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:38487)
  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1103)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1089)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2203)
  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:99)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:736)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:724)
  [...]
  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:178)
  at com.sun.proxy.$Proxy5.createTable(Unknown Source)
  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:777)
  ... 24 more
</code></pre>

<p>I tried googling that again, but had no luck finding something usable. I think it may have to do something with the fact, that the metastore service is running separately from Hive and for some reason does not have access to the configured storage account keys... but to be honest, I think this should all just work without manually tinkering with the Hadoop/Hive configuration.</p>

<p>So, my question is: What am I doing wrong and how is this supposed to work?</p>
","<azure><hive><azure-hdinsight><azure-data-factory><azure-blob-storage>","2018-01-22 15:17:55","1430","1","1","48395004","<p>You need to make sure you also add the hadoop-azure.jar and the azure-storage-5.4.0.jar to your Hadoop Classpath export in your hadoop-env.sh. </p>

<p>export HADOOP_CLASSPATH=/usr/lib/hadoop-client/hadoop-azure.jar:/usr/lib/hadoop-client/lib/azure-storage-5.4.0.jar:$HADOOP_CLASSPATH</p>

<p>And you will need to add the storage key via the following parameter in your core-site. 
fs.azure.account.key.{storageaccount}.blob.core.windows.net</p>

<p>When you create your DB and table you need to specify the location using your storage account and the user id
Create table {Tablename}
...
LOCATION 'wasbs://{container}@{storageaccount}.blob.core.windows.net/{filepath}'</p>

<p>If you still have problems after trying the above check to see whether the storage account is a V1 or V2. We had an issue where the V2 storage account did not work with our version of HDP. </p>
"
"48380441","Add datafactory project in visual studio for version2 of ADF","<p>As we can add Data Factory project for ADF version1. Can we add project for ADF version2 and if yes how can we do that? Whenever I am trying to add Data Factory Project it gives me option for version1 and not for version2. Is there anyway to add ADF version2 project to my solution?</p>
","<azure><azure-data-factory>","2018-01-22 11:22:39","567","0","1","48381358","<p>Things have changed when we talk &amp; work on ADF V1 and V2. I am presuming when you are saying <code>ADFV1</code> project in <code>VS</code> you used to create empty project for data integration solution where you had to create <code>linked services</code>, <code>datasets</code> &amp; <code>pipelines</code> based on the defined template - <code>json</code> format. </p>

<p>Now from <code>ADFV2</code> on wards if you want to code <code>ADFV2</code> in <code>C#</code> you can simply create a <code>console application</code> and code your way through <code>ADFV2</code>. Likewise if you choose other programming methods the way to do it changes. </p>

<p>For more detailed reference please have a look at the complete <a href=""https://learn.microsoft.com/en-us/azure/opbuildpdf/data-factory/TOC.pdf?branch=live"" rel=""nofollow noreferrer"">ADFV2 documentation</a>. </p>
"
"48373223","Azure Data Factory V2 Dataset Dynamic Folder","<p>In Azure Data Factory (V1) I was able to create a slide and store the output to a specific folder (i.e. {Year}/{Month}/{Day}.  See code below. </p>

<p>How do you create the same type of slice in Azure Data Factory V2?  I did find that you have to create a paramater.  Yes, I was unable to figure out how to pass the parameter.  </p>

<pre><code> ""folderPath"": ""@{dataset().path}"",
""parameters"": {
  ""path"": {
    ""type"": ""String""
</code></pre>

<p>Here is original ADF V1 code. </p>

<pre><code>{
    ""name"": ""EMS_EMSActivations_L1_Snapshot"",
    ""properties"": {
        ""published"": false,
        ""type"": ""AzureDataLakeStore"",
        ""linkedServiceName"": ""SalesIntelligence_ADLS_LS"",
        ""typeProperties"": {
            ""fileName"": ""EMS.FACT_EMSActivations_WTA.tsv"",
            ""folderPath"": ""/Snapshots/EMS/FACT_EMSActivations_WTA/{Year}/{Month}/{Day}"",
            ""format"": {
                ""type"": ""TextFormat"",
                ""rowDelimiter"": ""␀"",
                ""columnDelimiter"": ""\t"",
                ""nullValue"": ""#NULL#"",
                ""quoteChar"": ""\""""
            },
            ""partitionedBy"": [
                {
                    ""name"": ""Year"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""yyyy""
                    }
                },
                {
                    ""name"": ""Month"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""MM""
                    }
                },
                {
                    ""name"": ""Day"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""dd""
                    }
                },
                {
                    ""name"": ""Hour"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""HH""
                    }
                },
                {
                    ""name"": ""Minute"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""mm""
                    }
                }
            ]
        },
        ""availability"": {
            ""frequency"": ""Day"",
            ""interval"": 1
        }
    }
}
</code></pre>
","<azure-data-factory>","2018-01-22 00:35:30","7553","0","2","48375028","<p>Here is how you create a dynamic folder path when importing data from SQL into ADL. Look at folderPath line. </p>

<pre><code>{
    ""name"": ""EBC_BriefingActivitySummary_L1_Snapshot"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""SIAzureDataLakeStore"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureDataLakeStoreFile"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": "","",
                ""rowDelimiter"": """",
                ""nullValue"": ""\\N"",
                ""treatEmptyAsNull"": false,
                ""firstRowAsHeader"": false
            },
            ""fileName"": {
                ""value"": ""EBC.rpt_BriefingActivitySummary.tsv"",
                ""type"": ""Expression""
            },
            ""folderPath"": {
                ""value"": ""@concat('/Snapshots/EBC/rpt_BriefingActivitySummary/', formatDateTime(pipeline().parameters.scheduledRunTime, 'yyyy'), '/', formatDateTime(pipeline().parameters.scheduledRunTime, 'MM'), '/', formatDateTime(pipeline().parameters.scheduledRunTime, 'dd'), '/')"",
                ""type"": ""Expression""
            }
        }
    }
}
</code></pre>
"
"48373223","Azure Data Factory V2 Dataset Dynamic Folder","<p>In Azure Data Factory (V1) I was able to create a slide and store the output to a specific folder (i.e. {Year}/{Month}/{Day}.  See code below. </p>

<p>How do you create the same type of slice in Azure Data Factory V2?  I did find that you have to create a paramater.  Yes, I was unable to figure out how to pass the parameter.  </p>

<pre><code> ""folderPath"": ""@{dataset().path}"",
""parameters"": {
  ""path"": {
    ""type"": ""String""
</code></pre>

<p>Here is original ADF V1 code. </p>

<pre><code>{
    ""name"": ""EMS_EMSActivations_L1_Snapshot"",
    ""properties"": {
        ""published"": false,
        ""type"": ""AzureDataLakeStore"",
        ""linkedServiceName"": ""SalesIntelligence_ADLS_LS"",
        ""typeProperties"": {
            ""fileName"": ""EMS.FACT_EMSActivations_WTA.tsv"",
            ""folderPath"": ""/Snapshots/EMS/FACT_EMSActivations_WTA/{Year}/{Month}/{Day}"",
            ""format"": {
                ""type"": ""TextFormat"",
                ""rowDelimiter"": ""␀"",
                ""columnDelimiter"": ""\t"",
                ""nullValue"": ""#NULL#"",
                ""quoteChar"": ""\""""
            },
            ""partitionedBy"": [
                {
                    ""name"": ""Year"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""yyyy""
                    }
                },
                {
                    ""name"": ""Month"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""MM""
                    }
                },
                {
                    ""name"": ""Day"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""dd""
                    }
                },
                {
                    ""name"": ""Hour"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""HH""
                    }
                },
                {
                    ""name"": ""Minute"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""mm""
                    }
                }
            ]
        },
        ""availability"": {
            ""frequency"": ""Day"",
            ""interval"": 1
        }
    }
}
</code></pre>
","<azure-data-factory>","2018-01-22 00:35:30","7553","0","2","49784919","<p><strong>Step 1:</strong>
Use WindowStartTime / WindowEndTime in folderpath </p>

<pre><code>""folderPath"": {
""value"": ""&lt;&lt;path&gt;&gt;/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}-@{formatDateTime(pipeline().parameters.windowStart,'MM')}-@{formatDateTime(pipeline().parameters.windowStart,'dd')}/@{formatDateTime(pipeline().parameters.windowStart,'HH')}/"",
""type"": ""Expression""
}
</code></pre>

<p><strong>Step2 : Add in Pipeline JSON</strong></p>

<pre><code>""parameters"": {
    ""windowStart"": {
        ""type"": ""String""
    },
    ""windowEnd"": {
        ""type"": ""String""
    }
}
</code></pre>

<p><strong>Step3 : Add Run Parameter in TumblingWindow Trigger 
( This is referred in Step 2 )</strong></p>

<pre><code>   ""parameters"": {
        ""windowStart"": {
            ""type"": ""Expression"",
            ""value"": ""@trigger().outputs.windowStartTime""
        },
        ""windowEnd"": {
            ""type"": ""Expression"",
            ""value"": ""@trigger().outputs.windowEndTime""
        }
    }
</code></pre>

<p>For more details to understand , Refer </p>

<p>Refer this link. </p>

<p><a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/how-to-create-tumbling-window-trigger.md"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/how-to-create-tumbling-window-trigger.md</a></p>
"
"48360958","Azure Data Factory V2 + Key Vault","<p>I am trying to leverage Azure Key Vault to secure password for service account that moves data from on-prem SQL server to Azure Data Lake via Azure Data Factory. </p>

<p>I first created the Linked Service and hard coded the credentials. It works.  </p>

<p>However, I want to store the service account secret (i.e. password) within Key Value, and according to the following <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#reference-credential-stored-in-key-vault"" rel=""nofollow noreferrer"">post</a>, I added the Azure Key Vault Liked Service and referenced credentials stored in key value. </p>

<p>Under Advance within the Linked Service, see image #1, Iadded the following JSON:</p>

<pre><code>{
    ""name"": ""LinkedService"",
    ""properties"": {
        ""type"": ""SqlServer"",
        ""typeProperties"": {
            ""username"": ""&lt;domain&gt;\&lt;account name&gt;"",
            ""password"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""secretName"": ""&lt;service account name&gt;"",
                ""store"":{
                    ""referenceName"": ""https://&lt;name&gt;.vault.azure.net/"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        },
        ""connectVia"": {
            ""referenceName"": ""IRMYService"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
} 
</code></pre>

<p>However, when I look at the final JSON within Linked Service, it doesn't appear correct (refer to final image below). Why is username and credentials showing up? </p>

<p><a href=""https://i.stack.imgur.com/OZDhm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OZDhm.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/1vBv1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1vBv1.jpg"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory><azure-keyvault>","2018-01-20 20:48:24","4701","1","2","48381495","<p>You probably have missed the below part from that same link-</p>

<blockquote>
  <p>Currently, Dynamics connector, Salesforce connector and a few newly
  enable connectors support this feature. Expect more coming later. You
  can check each connector topic on details. For the secret fields which
  support this feature, you will see a note in the description saying
  ""You can choose to mark this field as a SecureString to store it
  securely in ADF, or store password in Azure Key Vault and let the copy
  acitivty pull from there when performing data copy - learn more from
  Store credentials in Key Vault.""</p>
</blockquote>

<p>As of now <code>Azure Data Lake Store</code> doesn't support <code>Key Vault</code> integration. You can always choose - <strong>managed service identity (MSI)</strong> authentication which would not expose your service principal information's. </p>

<p>The same applies to <code>Sql Server</code> as well - you have to choose <code>Secure String</code> for using the <code>connectionString</code> &amp; <code>password</code>.</p>
"
"48360958","Azure Data Factory V2 + Key Vault","<p>I am trying to leverage Azure Key Vault to secure password for service account that moves data from on-prem SQL server to Azure Data Lake via Azure Data Factory. </p>

<p>I first created the Linked Service and hard coded the credentials. It works.  </p>

<p>However, I want to store the service account secret (i.e. password) within Key Value, and according to the following <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#reference-credential-stored-in-key-vault"" rel=""nofollow noreferrer"">post</a>, I added the Azure Key Vault Liked Service and referenced credentials stored in key value. </p>

<p>Under Advance within the Linked Service, see image #1, Iadded the following JSON:</p>

<pre><code>{
    ""name"": ""LinkedService"",
    ""properties"": {
        ""type"": ""SqlServer"",
        ""typeProperties"": {
            ""username"": ""&lt;domain&gt;\&lt;account name&gt;"",
            ""password"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""secretName"": ""&lt;service account name&gt;"",
                ""store"":{
                    ""referenceName"": ""https://&lt;name&gt;.vault.azure.net/"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        },
        ""connectVia"": {
            ""referenceName"": ""IRMYService"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
} 
</code></pre>

<p>However, when I look at the final JSON within Linked Service, it doesn't appear correct (refer to final image below). Why is username and credentials showing up? </p>

<p><a href=""https://i.stack.imgur.com/OZDhm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OZDhm.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/1vBv1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1vBv1.jpg"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory><azure-keyvault>","2018-01-20 20:48:24","4701","1","2","52444425","<p>This is now supported out of the box. The link you provided has also been updated to reflect this: </p>

<blockquote>
  <p>Currently, all activity types except custom activity support this
  feature. For connector configuration specifically, check the ""linked
  service properties"" section in each connector topic for details.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/hqwDy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hqwDy.png"" alt=""enter image description here""></a></p>

<p>For more, check the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#linked-service-properties"" rel=""nofollow noreferrer"">docs for Data Lake Storage Gen 1</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#linked-service-properties"" rel=""nofollow noreferrer"">docs for Data Lake Storage Gen2</a>.</p>
"
"48321224","Azure data factory - Continuous deployment using VSTS","<p>I need to know how can i build continuous deployment for Azure Data factory using VSTS. I know there is an Azure data factory deployment available in VSTS release. But I'm looking for other options using Powershell for deployment.</p>

<p>If anyone has already done anything specific to this provide the links.</p>
","<deployment><azure-data-factory>","2018-01-18 12:20:41","875","0","1","48345708","<p><a href=""https://blog.abatishchev.ru/2017/03/28/352"" rel=""nofollow noreferrer"">This blog</a> should get you started. I'm using a comparable method for deployment. Before deploying the JSON files using a PowerShell command, I edit them to insert environment specific values into the Data Factory definitions. You can pass these values as parameters from the TFS deployment-pipeline. </p>
"
"48316458","Azure Data Factory - Python Custom Activity","<p>I am trying to create a data factory using Python Custom Activity (similar to .Net) to extract the data from source files and do some pre-processing on them. After the data is pre-processed, need to upload the file to a blob. I have a python code ready to do this but wanted to explore if i can use Data Factory Custom Activity to do this by injecting Python code into it. If yes, please tell me how to do it, links or any reference docs would help.</p>

<p>Appreciate your thoughts/ideas on this.</p>
","<azure><azure-data-factory>","2018-01-18 08:05:44","3686","0","2","48439416","<p>You can use ADF V2 custom activity for that, here's the document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a></p>
"
"48316458","Azure Data Factory - Python Custom Activity","<p>I am trying to create a data factory using Python Custom Activity (similar to .Net) to extract the data from source files and do some pre-processing on them. After the data is pre-processed, need to upload the file to a blob. I have a python code ready to do this but wanted to explore if i can use Data Factory Custom Activity to do this by injecting Python code into it. If yes, please tell me how to do it, links or any reference docs would help.</p>

<p>Appreciate your thoughts/ideas on this.</p>
","<azure><azure-data-factory>","2018-01-18 08:05:44","3686","0","2","61650411","<p>Dependencies should be installed at batch pool (referenced by custom activity) level. There are two ways to install dependencies on batch pool: use pool startup scripts learn.microsoft.com/en-us/azure/batch/… or custom image learn.microsoft.com/en-us/azure/batch/batch-custom-images</p>
"
"48297506","How Many tables can be selected when using ADF V2 copy data tool","<p>How many tables can be selected when using ADF V2 copy data tool? We can only select 1k tables when using ADF V1 
Can we edit the Copy Preview Pipline in V1?</p>
","<azure><azure-data-factory>","2018-01-17 09:20:05","158","0","1","48301052","<p>Up to a thousand tables can be selected in each ADFv2 Copy Data tool session. If copying more than a thousand tables is needed, multiple Copy Data tool sessions can be launched in multiple web browser tabs or windows. Check out the Copy Data tool documentation for other details (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool</a>).</p>

<p>The resultant ADFv2 JSON configurations created by Copy Data tool can be further edited with ADFv2 UI (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal#test-run-the-pipeline"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal#test-run-the-pipeline</a>).</p>
"
"48295830","Azure Data Factory - Custom Activity never complete","<p>I'm new to Azure and I am working on data factory and custom activity. I am creating a pipeline with only one custom activity (the activity actually do nothing and return immediately).</p>

<p>However, it seems that the custom activity is sent to batch account. I can see the Job and task created but task remains ""Active"" and never complete.</p>

<p>Is there anything I missed?</p>

<p>Job: Created and is belonged to desired application pool</p>

<p><a href=""https://i.stack.imgur.com/jR84O.png"" rel=""nofollow noreferrer"">Job</a></p>

<p>Task: Not sure why but  application pool is n/a and never complete</p>

<p><a href=""https://i.stack.imgur.com/HJ2GA.png"" rel=""nofollow noreferrer"">Job -> Task Status</a></p>

<p><a href=""https://i.stack.imgur.com/ENwDH.png"" rel=""nofollow noreferrer"">Task application pool n/a</a></p>

<p>Code of the dummy activity. I'm using ADF v2 and therefore it is just a simple console program.
<a href=""https://i.stack.imgur.com/URaJv.png"" rel=""nofollow noreferrer"">Dummy activity</a></p>
","<azure-data-factory>","2018-01-17 07:34:58","578","0","1","48313401","<p>I figured out.</p>

<p>The problem is from the batch account. The node of the pool failed at start task which block the node to take job. I have changed the start task of the pool not to wait for success so that even if the start task failed the node can still take job.</p>
"
"48284895","Unable to setup Azure alert on resource specific events","<p>In the past, it was possible to setup an Azure alert on a single event for a resource e.g. on data factory single RunFinished where the status is Failed*.</p>

<p>This appears to have been <a href=""https://learn.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-migrate-management-alerts"" rel=""nofollow noreferrer"">superseded by ""Activity Log Alerts""</a></p>

<p>However these alerts only seem to either work on a metric threshold (e.g. number of failures in 5 minutes) or on events which are related to the general admin of the resource (e.g. has it been deployed) not on the operations of the resource.</p>

<p>A threshold doesn't make sense for data factory, as a data factory may only run once a day, if a failure happens and then it doesn't happen X minutes later it doesn't mean it's been resolved.</p>

<p>The activity event alerts, don't seem to have things like failures.</p>

<p>Am I missing something?</p>

<p>It it because this is expected to be done in OMS Log Analytics now? Or perhaps even in Event Grid later?</p>

<p>*n.b. it is still possible to create these alert types via ARM templates, but you can't see them in the portal anymore.</p>
","<azure-data-factory><azure-monitoring><azure-eventgrid>","2018-01-16 15:39:36","260","0","1","48695181","<p>The events you're describing are part of a resource type's diagnostic logs, which are not alertable in the same way that the Activity Log is. I suggest routing the data to Log Analytics and setting the alert there: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor</a></p>
"
"48281633","Is there any way to run the datafactory slice using powershell cmdlets","<p>Is there any way to re run the failed Azure data factory slices by using the powershell cmdlets. </p>

<p>As of now I am re running the slices manually from diagram page. But this is not helping much as I have  more than 500 slices and all are scheduled to run on every week. </p>

<p>Just to brief you : 4 days back my database server  went down, and all the slices are failed to execute and now I want re run all the slice again. </p>

<p>Also I wanted know, is there any way to get failure notification, if slices failed to execute then I should able to get mail or something so that I can get notified.</p>

<p>Thanks in advance.</p>
","<powershell><azure><azure-data-factory>","2018-01-16 12:46:38","449","0","2","48284223","<p>Last time a similar issue happened to me I ended up using the ""Monitor &amp; Manage"" tool from the Azure Portal.
You can use the grid view to select your failed slices, and there is a very useful ""Rerun"" button <a href=""https://i.stack.imgur.com/0D89I.png"" rel=""nofollow noreferrer"">on the top left corner of the grid</a>.</p>

<p>To get email alerts when a slice fails you can add one using the ""Metrics and operations"" tool.
The settings is quite well hidden <a href=""https://i.stack.imgur.com/UzSSG.png"" rel=""nofollow noreferrer"">but it exists</a> :)</p>
"
"48281633","Is there any way to run the datafactory slice using powershell cmdlets","<p>Is there any way to re run the failed Azure data factory slices by using the powershell cmdlets. </p>

<p>As of now I am re running the slices manually from diagram page. But this is not helping much as I have  more than 500 slices and all are scheduled to run on every week. </p>

<p>Just to brief you : 4 days back my database server  went down, and all the slices are failed to execute and now I want re run all the slice again. </p>

<p>Also I wanted know, is there any way to get failure notification, if slices failed to execute then I should able to get mail or something so that I can get notified.</p>

<p>Thanks in advance.</p>
","<powershell><azure><azure-data-factory>","2018-01-16 12:46:38","449","0","2","48294130","<p>You may also try the script mentioned in the <a href=""https://blogs.msdn.microsoft.com/karang/2015/11/12/azure-data-factory-detecting-and-re-running-failed-adf-slices/"" rel=""nofollow noreferrer"">link</a> and let us know.
For more information, you may refer the <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-monitor-manage-pipelines.md"" rel=""nofollow noreferrer"">article</a> to monitor and manage Azure Data Factory pipelines.</p>
"
"48262440","Deploy azure data factory v2 app","<p>I'm trying to find any way to publish my console app (.net) written for azure data factory v2.
But could not find any solution.</p>
","<azure><azure-data-factory>","2018-01-15 11:50:58","240","0","2","48266855","<p>It would be useful to have a bit more info on your context. You're talking about running a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> from an Azure Batch account? What did you try already?</p>

<p>When running a custom activity, you'll have to upload your executable + depedencies to an Azure storage account. Create a blob container and copy the files there. Then you'll have to configure the activity to use this storage account and the point it to the right container.</p>

<p>If you're asking for a deployment like a right-click -> deploy option, it doesn't exist. I've automated my deployments using PowerShell, using Set-AzureStorageBlobContent to write the files to the Storage account.</p>
"
"48262440","Deploy azure data factory v2 app","<p>I'm trying to find any way to publish my console app (.net) written for azure data factory v2.
But could not find any solution.</p>
","<azure><azure-data-factory>","2018-01-15 11:50:58","240","0","2","48372113","<p>More details would be really appreciated but if you mean that you are using the .NET SDK to create ADF V2 objects, my understanding is that there is no such thing as publish compared to the new User Interface in the portal where you create/edit the objects first and then you click on publish.</p>

<p>if you use the library they get automatically uploaded to ADF V2 and you can easily test that now with the new UI.</p>
"
"48241780","Copy Activity with stored procedure rounds up decimals","<p>I am using an Azure Data Factory to get data from an on prem database to an Azure sql database.</p>

<p>I am doing it in 2 steps:</p>

<ol>
<li>Copy to blob</li>
<li>Insert into azure using a copy activity that runs a stored procedure.</li>
</ol>

<p>The problem i have is the decimal is rounded up and the destination values are different than source.</p>

<p>For the entire process i have defined 3 datasets:</p>

<ol>
<li>SQLDataset - the structure node is {... ""name"" : ""MyColumn"", ""type"":""Decimal""}.</li>
<li>BlobDataset - the structure node is the same as above</li>
<li>AzureSQLDataset - the structure node is the same</li>
</ol>

<p>On the SQL database, i have a user defined table type as follows:</p>

<pre><code>CREATE TYPE dbo.myType TABLE (...MyColumn decimal(28,10) null)
</code></pre>

<p>The stored procedure does an insert or an update and no data transformation. However, when i query for MyColumn value, the values end up being rounded up.</p>

<p>If source had a value of <em>266222451894.019990000</em> , target has a value of <em>266222451894.0200000000</em></p>

<p>Am i missing something ?</p>
","<azure><azure-data-factory>","2018-01-13 16:21:34","1289","3","1","50959396","<p>If you want to a precise numeric value, your best bet is to ship the numeric value out as a string.  </p>
"
"48232329","unexpected characters in JSON file","<p>Problem: Extra characters in JSON files from Azure DataFactory v1. </p>

<p>I have two files which are straight copies from two Cosmos db collections.<br>
I used Data Factory v1, selecting defaults, to copy the collections to a Blob storage container 
then used Azure Storage Explorer to copy the JSON files to a Windows 10 
desktop.</p>

<p>A. Looking at the files using an editor vi/Notrepad/Wordpad/Ultra Edit/Visual 
Studio Code they look OK.</p>

<p>B. When I attempt to read the files into a simple Nodejs(v9) application the 
I get JSON parse error:</p>

<pre><code>SyntaxError: C:\Users\ricko\Desktop\whippy\MorpheusDataProduction-01052018-
20.json: Unexpected token � in JSON at positi
on 0
at JSON.parse (&lt;anonymous&gt;)
at Object.Module._extensions..json (module.js:654:27)
at Module.load (module.js:554:32)
at tryModuleLoad (module.js:497:12)
at Function.Module._load (module.js:489:3)
at Module.require (module.js:579:17)
at require (internal/module.js:11:18)
at Object.&lt;anonymous&gt; (C:\Users\ricko\Desktop\whippy\appUpdateJSON.js:5:17)
at Module._compile (module.js:635:30)
at Object.Module._extensions..js (module.js:646:10)
</code></pre>

<p>C. A single Line validates using JSON <a href=""http://jsonlint.com"" rel=""nofollow noreferrer"">http://jsonlint.com</a>. Multiple lines do not
validate giving a Parse Error:</p>

<pre><code>    Error: Parse error on line 15:
    ...""_ts"": 1512601730} { ""path"": ""Dropbox\
    ----------------------^
    Expecting 'EOF', '}', ',', ']', got '{'
</code></pre>

<p>D. Also using node to read the file directly then writing a record
 to the console I get a wierd double spaced version. I also saw two �� 
characters at the beginning of the line in front of the open brace in one attempt. (example below)</p>

<pre><code>   { "" S T E P _ N A M E "" : "" O p e n s   T e s t "" , "" N A M E "" : "" A r t 
    h u r   J o b e r t "" , "" D A T E "" : "" 1 2
   1 9 / 2 0 1 6   3 : 5 7 : 4 7   P M "" , "" L O T "" : "" C G 1 5 "" , "" W A F 
   E R "" : "" "" , "" P R O C E S S _ S T E P "" : ""
   ...
</code></pre>
","<azure><azure-data-factory>","2018-01-12 18:48:10","1038","0","2","48233365","<p>I would assume it's an encoding issue. The two unprintable characters at the beginning are called BOM and denote the encoding. Smart editors can handle this. UltraEdit has a hex mode where you can see the <strong>real</strong> content byte for byte in hex form. Notepad++ is very powerful to convert the encoding to nearly everything you would use, with or without BOM.</p>

<p>C. I would guess there is a  comma missing between the closing and opening bracket.</p>

<p>D. Here the encoding seems to be unicode (fix two byte sized characters). Verify it with UltraEdit's hex mode.</p>

<p>I do not know Azure, but most programming languages are able to handle the byte-to-character encoding correctly if you indicate the encoding you need. You always have to be aware of this problem when you have to serialize text to a byte array (for sending over a line like socket) and vice versa.</p>
"
"48232329","unexpected characters in JSON file","<p>Problem: Extra characters in JSON files from Azure DataFactory v1. </p>

<p>I have two files which are straight copies from two Cosmos db collections.<br>
I used Data Factory v1, selecting defaults, to copy the collections to a Blob storage container 
then used Azure Storage Explorer to copy the JSON files to a Windows 10 
desktop.</p>

<p>A. Looking at the files using an editor vi/Notrepad/Wordpad/Ultra Edit/Visual 
Studio Code they look OK.</p>

<p>B. When I attempt to read the files into a simple Nodejs(v9) application the 
I get JSON parse error:</p>

<pre><code>SyntaxError: C:\Users\ricko\Desktop\whippy\MorpheusDataProduction-01052018-
20.json: Unexpected token � in JSON at positi
on 0
at JSON.parse (&lt;anonymous&gt;)
at Object.Module._extensions..json (module.js:654:27)
at Module.load (module.js:554:32)
at tryModuleLoad (module.js:497:12)
at Function.Module._load (module.js:489:3)
at Module.require (module.js:579:17)
at require (internal/module.js:11:18)
at Object.&lt;anonymous&gt; (C:\Users\ricko\Desktop\whippy\appUpdateJSON.js:5:17)
at Module._compile (module.js:635:30)
at Object.Module._extensions..js (module.js:646:10)
</code></pre>

<p>C. A single Line validates using JSON <a href=""http://jsonlint.com"" rel=""nofollow noreferrer"">http://jsonlint.com</a>. Multiple lines do not
validate giving a Parse Error:</p>

<pre><code>    Error: Parse error on line 15:
    ...""_ts"": 1512601730} { ""path"": ""Dropbox\
    ----------------------^
    Expecting 'EOF', '}', ',', ']', got '{'
</code></pre>

<p>D. Also using node to read the file directly then writing a record
 to the console I get a wierd double spaced version. I also saw two �� 
characters at the beginning of the line in front of the open brace in one attempt. (example below)</p>

<pre><code>   { "" S T E P _ N A M E "" : "" O p e n s   T e s t "" , "" N A M E "" : "" A r t 
    h u r   J o b e r t "" , "" D A T E "" : "" 1 2
   1 9 / 2 0 1 6   3 : 5 7 : 4 7   P M "" , "" L O T "" : "" C G 1 5 "" , "" W A F 
   E R "" : "" "" , "" P R O C E S S _ S T E P "" : ""
   ...
</code></pre>
","<azure><azure-data-factory>","2018-01-12 18:48:10","1038","0","2","48288794","<p>Error is due to the  Unicode BOM (Byte Order Mark) which are the hidden characters at the beginning of the file.</p>

<p>Answer can be found here:
<a href=""https://stackoverflow.com/questions/24356713/node-js-readfile-error-with-utf8-encoded-file-on-windows"" title=""node.js readfile error with utf8 encoded file on windows
"">node.js readfile error with utf8 encoded file on windows
</a></p>
"
"48219849","'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet","<p>I have installed Powershell 6.0.0 on Ubuntu 16.04. I am getting following error when trying to create Azure Data Factory using Powershell</p>

<pre><code>Set-AzureRmDataFactoryV2 : The term 'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet, function, script file, or operable program.
Check the spelling of the name, or if a path was included, verify that the path is correct and try again.
At line:1 char:16
+ $DataFactory = Set-AzureRmDataFactoryV2 -ResourceGroupName $ResGrp.Re ...
+                ~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo          : ObjectNotFound: (Set-AzureRmDataFactoryV2:String) [], CommandNotFoundException
+ FullyQualifiedErrorId : CommandNotFoundException
</code></pre>

<p>Then I tried to install the module and getting the following error</p>

<pre><code>Install-Module AzureRM -AllowClobber  
                                                                                                                                                                                                                                          Untrusted repository                                                                                                                           You are installing the modules from an untrusted repository. If you trust this repository, change its InstallationPolicy value by running the  Set-PSRepository cmdlet. Are you sure you want to install the modules from 'PSGallery'?                                                        
[Y] Yes  [A] Yes to All  [N] No  [L] No to All  [S] Suspend  [?] Help (default is ""N""): Y
PackageManagement\Install-Package : The member 'TypesToProcess' in the module manifest is not valid: Cannot find path '/tmp/787167149/Azure.Storage/.\Microsoft.WindowsAzure.Commands.Storage.Types.ps1xml' because it does not exist.. Verify that a valid value is specified for this field in the '/tmp/787167149/Azure.Storage/Azure.Storage.psd1' file.                                                                                 At /opt/microsoft/powershell/6.0.0/Modules/PowerShellGet/1.6.0/PSModule.psm1:2057 char:21                                                      + ...          $null = PackageManagement\Install-Package @PSBoundParameters                                                                    +                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                    + CategoryInfo          : ResourceUnavailable: (/tmp/787167149/...re.Storage.psd1:String) [Install-Package], Exception                         + FullyQualifiedErrorId : Modules_InvalidManifest,Microsoft.PowerShell.Commands.TestModuleManifestCommand,Microsoft.PowerShell.PackageManagement.Cmdlets.InstallPackage         
</code></pre>
","<powershell><azure><azure-data-factory>","2018-01-12 05:05:08","1107","2","2","48219928","<p>Currently, <code>Set-AzureRmDataFactoryV2</code>  cmdlet is not supported on Linux. <code>AzureRM.NetCore</code> supports following services.</p>

<ul>
<li>Virtual Machine</li>
<li>App Service (Websites)</li>
<li>SQL Database</li>
<li>Storage</li>
<li>Network</li>
</ul>

<p>More information please refer to this <a href=""https://learn.microsoft.com/en-us/powershell/azure/install-azurermps-maclinux?view=azurermps-5.1.1#available-cmdlets"" rel=""nofollow noreferrer"">official document</a>.</p>

<p>If possible, I suggest you could use Azure CLI.</p>
"
"48219849","'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet","<p>I have installed Powershell 6.0.0 on Ubuntu 16.04. I am getting following error when trying to create Azure Data Factory using Powershell</p>

<pre><code>Set-AzureRmDataFactoryV2 : The term 'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet, function, script file, or operable program.
Check the spelling of the name, or if a path was included, verify that the path is correct and try again.
At line:1 char:16
+ $DataFactory = Set-AzureRmDataFactoryV2 -ResourceGroupName $ResGrp.Re ...
+                ~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo          : ObjectNotFound: (Set-AzureRmDataFactoryV2:String) [], CommandNotFoundException
+ FullyQualifiedErrorId : CommandNotFoundException
</code></pre>

<p>Then I tried to install the module and getting the following error</p>

<pre><code>Install-Module AzureRM -AllowClobber  
                                                                                                                                                                                                                                          Untrusted repository                                                                                                                           You are installing the modules from an untrusted repository. If you trust this repository, change its InstallationPolicy value by running the  Set-PSRepository cmdlet. Are you sure you want to install the modules from 'PSGallery'?                                                        
[Y] Yes  [A] Yes to All  [N] No  [L] No to All  [S] Suspend  [?] Help (default is ""N""): Y
PackageManagement\Install-Package : The member 'TypesToProcess' in the module manifest is not valid: Cannot find path '/tmp/787167149/Azure.Storage/.\Microsoft.WindowsAzure.Commands.Storage.Types.ps1xml' because it does not exist.. Verify that a valid value is specified for this field in the '/tmp/787167149/Azure.Storage/Azure.Storage.psd1' file.                                                                                 At /opt/microsoft/powershell/6.0.0/Modules/PowerShellGet/1.6.0/PSModule.psm1:2057 char:21                                                      + ...          $null = PackageManagement\Install-Package @PSBoundParameters                                                                    +                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                    + CategoryInfo          : ResourceUnavailable: (/tmp/787167149/...re.Storage.psd1:String) [Install-Package], Exception                         + FullyQualifiedErrorId : Modules_InvalidManifest,Microsoft.PowerShell.Commands.TestModuleManifestCommand,Microsoft.PowerShell.PackageManagement.Cmdlets.InstallPackage         
</code></pre>
","<powershell><azure><azure-data-factory>","2018-01-12 05:05:08","1107","2","2","57989277","<p>I have solved this issue in my machine by updating Az.DataFactory module.</p>

<ul>
<li>Install/update latest AzureRM.DataFactoryV2 module.</li>
<li>Restart your powershell.</li>
<li>try the above command.</li>
</ul>

<p>it will work successfully.</p>
"
"48216584","Azure Data Factory get blob path with sas token in custom activity","<p>I'm trying to build a custom activity in Azure Data Factory that gets a blob as input dataset and would like to pass this blob's sas token path to an API that requires this type of path. </p>

<p>Is there any way to get the blob's path with the sas token in the custom activity?</p>
","<azure-storage><azure-data-factory><custom-activity>","2018-01-11 22:06:39","887","0","1","48797422","<p>I figured out a way to do it. Part of the custom activity in ADF v1 is the Execute method that has a context parameter. From that context you can get the connection string to the blob storage and the path of the blob and then you can extract the sas token like this:</p>

<pre><code>public override IDictionary&lt;string, string&gt; Execute(
AOMDotNetActivityContext context,
IActivityLogger logger)
{
    string blobConnectionString = context.ConnectionString;
    CloudStorageAccount inputStorageAccount = CloudStorageAccount.Parse(blobConnectionString);
    var blob = new CloudBlob(new Uri(inputStorageAccount.BlobEndpoint, Path.Combine(context.FolderPath, context.FileName)), inputStorageAccount.Credentials);
    SharedAccessBlobPolicy adHocSAS = new SharedAccessBlobPolicy()
    {
        SharedAccessExpiryTime = DateTime.UtcNow.AddHours(48),
        Permissions = SharedAccessBlobPermissions.Read | SharedAccessBlobPermissions.Delete
    };
    string sasBlobToken = blob.GetSharedAccessSignature(adHocSAS);
    string fullUri = new Uri(blob.Uri, sasBlobToken).ToString();
</code></pre>
"
"48198927","Azure data factories vs factory","<p>I'm building an Azure data lake using data factory at the moment, and am after some advice on having multiple data factories vs just one.</p>

<p>I have one data factory at the moment, that is sourcing data from one EBS instance, for one specific company under an enterprise. In the future though there might be other EBS instances, and other companies (with other applications as sources) to incorporate into the factory - and I'm thinking the diagram might get a bit messy. </p>

<p>I've searched around, and I found this site, that recommends to keep everything in a single data factory to reuse linked services. I guess that is a good thing, however as I have scripted the build for one data factory, it would be pretty easy to build the linked services again to point at the same data lake for instance.</p>

<p><a href=""https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/"" rel=""noreferrer"">https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/</a></p>

<p>Pros for having only one instance of data factory:</p>

<ul>
<li>have to only create the data sets, linked services once</li>
<li>Can see overall lineage in one diagram</li>
</ul>

<p>Cons</p>

<ul>
<li>Could get messy over time</li>
<li>Could get quite big to even find the pipeline you are after</li>
</ul>

<p>Has anyone got some large deployments of Azure Data Factories out there, that bring in potentially thousands of data sources, mix them together and transform? Would be interested in hearing your thoughts.</p>
","<azure><azure-data-factory>","2018-01-11 02:21:51","1366","8","5","48207949","<p>My suggestion is to have only one, as it makes it easier to configure multiple integration runtimes (gateways). If you decide to have more than one data factory, take into consideration that a pc can only have 1 integration runtime installed, and that the integration runtime can only be registered to only 1 data factory instance.</p>

<p>I think the cons you are listing are both fixed by having a naming rules. Its not messy to find a pipeline you want if you name them like: Pipeline_[Database name]<em>[db schema]</em>[table name] for example.</p>

<p>I have a project with thousands of datasets and pipelines, and its not harder to handle than smaller projects.</p>

<p>Hope this helped!</p>
"
"48198927","Azure data factories vs factory","<p>I'm building an Azure data lake using data factory at the moment, and am after some advice on having multiple data factories vs just one.</p>

<p>I have one data factory at the moment, that is sourcing data from one EBS instance, for one specific company under an enterprise. In the future though there might be other EBS instances, and other companies (with other applications as sources) to incorporate into the factory - and I'm thinking the diagram might get a bit messy. </p>

<p>I've searched around, and I found this site, that recommends to keep everything in a single data factory to reuse linked services. I guess that is a good thing, however as I have scripted the build for one data factory, it would be pretty easy to build the linked services again to point at the same data lake for instance.</p>

<p><a href=""https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/"" rel=""noreferrer"">https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/</a></p>

<p>Pros for having only one instance of data factory:</p>

<ul>
<li>have to only create the data sets, linked services once</li>
<li>Can see overall lineage in one diagram</li>
</ul>

<p>Cons</p>

<ul>
<li>Could get messy over time</li>
<li>Could get quite big to even find the pipeline you are after</li>
</ul>

<p>Has anyone got some large deployments of Azure Data Factories out there, that bring in potentially thousands of data sources, mix them together and transform? Would be interested in hearing your thoughts.</p>
","<azure><azure-data-factory>","2018-01-11 02:21:51","1366","8","5","48585486","<p>If you are using ADF v1 then it will get messy.  At a client of ours we have over 1000 pipelines in one Data Factory.  If you are starting fresh, I would recommend looking at v2 because it allows you to parameterize things and should make your scripts more reusable. </p>
"
"48198927","Azure data factories vs factory","<p>I'm building an Azure data lake using data factory at the moment, and am after some advice on having multiple data factories vs just one.</p>

<p>I have one data factory at the moment, that is sourcing data from one EBS instance, for one specific company under an enterprise. In the future though there might be other EBS instances, and other companies (with other applications as sources) to incorporate into the factory - and I'm thinking the diagram might get a bit messy. </p>

<p>I've searched around, and I found this site, that recommends to keep everything in a single data factory to reuse linked services. I guess that is a good thing, however as I have scripted the build for one data factory, it would be pretty easy to build the linked services again to point at the same data lake for instance.</p>

<p><a href=""https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/"" rel=""noreferrer"">https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/</a></p>

<p>Pros for having only one instance of data factory:</p>

<ul>
<li>have to only create the data sets, linked services once</li>
<li>Can see overall lineage in one diagram</li>
</ul>

<p>Cons</p>

<ul>
<li>Could get messy over time</li>
<li>Could get quite big to even find the pipeline you are after</li>
</ul>

<p>Has anyone got some large deployments of Azure Data Factories out there, that bring in potentially thousands of data sources, mix them together and transform? Would be interested in hearing your thoughts.</p>
","<azure><azure-data-factory>","2018-01-11 02:21:51","1366","8","5","50778847","<p>I'd initially agree with an integration runtime being tied to a single data factory being a restriction, however I suspect it is no longer or soon to be no longer a restriction.</p>

<p>In the <a href=""https://www.powershellgallery.com/packages/AzureRM.DataFactories/4.2.0"" rel=""nofollow noreferrer"">March 13th update to AzureRm.DataFactories</a>, there is a comment stating ""Enable integration runtime to be shared across data factory"".</p>

<p>I think it will depend on the complexity of the data factory and if there are inter-dependencies between the various sources and destinations.</p>

<p>The UI particularly (even more so in V2) makes managing a large data factory easy.</p>

<p><strong>However</strong> if you choose an ARM deployment technique the data factory JSON can soon become unwieldy in even a modestly complex data factory. And in that sense I'd recommend splitting them.</p>

<p>You can of course mitigate maintainability issues as people have mentioned, by breaking your ARM templates into nested deployments, ARM parameterisation or data factory V2 parameterisation, using the SDK direct with separate files. Or even just use the UI (now with git support :-) )</p>

<p>Perhaps more importantly particularly as you mention separate companies being sourced from; it perhaps sounds like the data isn't related and if it isn't - <em>should</em> it be isolated to avoid any coding errors? Or perhaps even to have segregated roles and responsibilities for the data factories.</p>

<p>On the other hand if the data is interrelated, having it in one data factory makes things far easier for allowing data factory to manage data dependencies and re-running failed slices in one go.</p>
"
"48198927","Azure data factories vs factory","<p>I'm building an Azure data lake using data factory at the moment, and am after some advice on having multiple data factories vs just one.</p>

<p>I have one data factory at the moment, that is sourcing data from one EBS instance, for one specific company under an enterprise. In the future though there might be other EBS instances, and other companies (with other applications as sources) to incorporate into the factory - and I'm thinking the diagram might get a bit messy. </p>

<p>I've searched around, and I found this site, that recommends to keep everything in a single data factory to reuse linked services. I guess that is a good thing, however as I have scripted the build for one data factory, it would be pretty easy to build the linked services again to point at the same data lake for instance.</p>

<p><a href=""https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/"" rel=""noreferrer"">https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/</a></p>

<p>Pros for having only one instance of data factory:</p>

<ul>
<li>have to only create the data sets, linked services once</li>
<li>Can see overall lineage in one diagram</li>
</ul>

<p>Cons</p>

<ul>
<li>Could get messy over time</li>
<li>Could get quite big to even find the pipeline you are after</li>
</ul>

<p>Has anyone got some large deployments of Azure Data Factories out there, that bring in potentially thousands of data sources, mix them together and transform? Would be interested in hearing your thoughts.</p>
","<azure><azure-data-factory>","2018-01-11 02:21:51","1366","8","5","55886186","<p>After the March release, you can link integration runtimes among different factories.</p>

<p>The other thing to do is to create different folders for the various pipelines and datasets</p>
"
"48198927","Azure data factories vs factory","<p>I'm building an Azure data lake using data factory at the moment, and am after some advice on having multiple data factories vs just one.</p>

<p>I have one data factory at the moment, that is sourcing data from one EBS instance, for one specific company under an enterprise. In the future though there might be other EBS instances, and other companies (with other applications as sources) to incorporate into the factory - and I'm thinking the diagram might get a bit messy. </p>

<p>I've searched around, and I found this site, that recommends to keep everything in a single data factory to reuse linked services. I guess that is a good thing, however as I have scripted the build for one data factory, it would be pretty easy to build the linked services again to point at the same data lake for instance.</p>

<p><a href=""https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/"" rel=""noreferrer"">https://www.purplefrogsystems.com/paul/2017/08/chaining-azure-data-factory-activities-and-datasets/</a></p>

<p>Pros for having only one instance of data factory:</p>

<ul>
<li>have to only create the data sets, linked services once</li>
<li>Can see overall lineage in one diagram</li>
</ul>

<p>Cons</p>

<ul>
<li>Could get messy over time</li>
<li>Could get quite big to even find the pipeline you are after</li>
</ul>

<p>Has anyone got some large deployments of Azure Data Factories out there, that bring in potentially thousands of data sources, mix them together and transform? Would be interested in hearing your thoughts.</p>
","<azure><azure-data-factory>","2018-01-11 02:21:51","1366","8","5","62797515","<p>My suggestion is to create one DataFactory service per each project. If you need to transfer data from two source to one destination and for each transformation you need several Pipelines and Linked Services and other stuffs, I suggest to create two separated ADF services for each Source. In this case I will see each source as an integration project separated.</p>
<p><a href=""https://i.stack.imgur.com/i8QvU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i8QvU.png"" alt=""enter image description here"" /></a></p>
<p>You will have two separated CI/CD for each project also.</p>
<p><a href=""https://i.stack.imgur.com/ycZjk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ycZjk.png"" alt=""enter image description here"" /></a></p>
<p>In your source controller also you need to have two separated repositories.</p>
<p><a href=""https://i.stack.imgur.com/XmidV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XmidV.png"" alt=""enter image description here"" /></a></p>
"
"48194540","How to Build Incremental data loading?","<p>I found this blog (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview</a>) and tried it, it was very helpful in moving the data b/w sql server and azure database,but how can I build something like this to copy data over in B/w On Prem SQL Server and Azure Datawarehouse??</p>

<p>Are there any links which I can get started with.</p>

<p>I have found various links but they are not very reasonable when I talk about Delta data loading?</p>

<p>Is there any way I can achive this using ADF V1?? or this is only possible using ADF V2??</p>
","<azure><azure-data-factory><azure-synapse>","2018-01-10 19:14:19","1667","1","1","48289692","<p>The change tracking part of the guidance is still relevant. You need some way to figure out what is the incremental data on the SQL Server on-premises instance that need to be pushed to your Azure SQL DW. What's different is how you can push the incremental data to SQL DW.</p>

<p>If you have something built using SSIS, it will work with SQL DW. Download the Azure pack for SSIS to get the SQL DW optimized workflow. ADF can do the same also if you already use it or are ok with spinning up another service. </p>

<p>In the simplest case you can export the incremental data into csv files, push to blob store using azcopy then insert into your SQL DW via external tables. For small data sets, this works just fine and can be automated with scripts to minimize risk and labor. Large data sets will be trickier as inserts are expensive fully logged operations. Check out <a href=""https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-develop-best-practices-transactions#minimal-logging-with-bulk-load"" rel=""nofollow noreferrer"">Minimal logging with bulk load</a> for hints on how INSERT.....SELECT can be minimally logged and the conditions for it to occur. While partitioning is a valid strategy, it's not a good idea for most environments for daily incremental loads mostly because there isn't enough data to build high quality row groups with clustered column store tables. That can results in poor query performance until the index is rebuilt. Optimizing for a single load operation daily instead of tens, hundreds or thousands of queries daily is usually not the right tradeoff.</p>
"
"48183112","Azure Data Factory U-SQL activity dynamic parameter","<p>Lets suppose I have a U-SQL script which get executed every one hour from ADF pipeline.
I have mssql database which contains config table. Is there any way to read a  config from database and pass it to U-SQL script? 
In <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-usql-activity"" rel=""nofollow noreferrer"">ADF docs</a> i couldn't find any way of doing it. Only SliceStart, SliceEnd, but what if my parameter is type of GUID ? </p>
","<azure><azure-data-factory><u-sql>","2018-01-10 08:25:28","809","0","1","48185016","<p>You can achieve this in ADF V2 (currently in Public Preview) using the Lookup activity.  The lookup activity can pass the lookup results to the subsequent activity (in your case U-SQL activity).</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a></p>
"
"48178057","azure Data factory migrate SQL Server database","<p>I know how to copy data from a database to another, but in one step I must create the exact table in the source db to the destination db.</p>

<p>Is there a method in data factory to migrate a SQL server database to SQL Azure without creating all the tables in the destination database or is Data Factory just for copying data between the tables?</p>
","<azure><azure-sql-database><azure-data-factory>","2018-01-09 23:17:45","268","1","1","48178612","<p>You can use Azure Data Factory to copy data or copy delta changes from a SQL Server database to an Azure SQL Database as explained on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-powershell"" rel=""nofollow noreferrer"">this</a> tutorial. You can also put data coming from on-premise SQL Server to Azure BLOB storage and from there upload data to Azure SQL database as explained <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf"" rel=""nofollow noreferrer"">here</a>.</p>
"
"48165947","Access datalake from Azure datafactory V2 using on demand HD Insight cluster","<p>I am trying to execute spark job from on demand HD Insight cluster using Azure datafactory.</p>

<p>Documentation indicates clearly that ADF(v2) does not support datalake linked service for on demand HD insight cluster and one have to copy data onto blob from copy activity and than execute the job. BUT this work around seems to be a hugely resource expensive in case of a billion files on a datalake. Is there any efficient way to access datalake files either from python script that execute spark jobs or any other way to directly access the files.</p>

<p>P.S Is there a possiblity of doing similar thing from v1, if yes then how? ""Create on-demand Hadoop clusters in HDInsight using Azure Data Factory"" describe on demand hadoop cluster that access blob storage but I want on demand spark cluster that access datalake.</p>

<p>P.P.s Thanks in advance</p>
","<python><pyspark><azure-hdinsight><azure-data-factory><azure-data-lake>","2018-01-09 10:18:28","352","1","2","48221294","<p>Currently, we don't have support for ADLS data store with HDI Spark cluster in ADF v2. We plan to add that in the coming months. Till then, you will have to contiue using the workaround as you mentioned in your post above. Sorry for the inconvenience.</p>
"
"48165947","Access datalake from Azure datafactory V2 using on demand HD Insight cluster","<p>I am trying to execute spark job from on demand HD Insight cluster using Azure datafactory.</p>

<p>Documentation indicates clearly that ADF(v2) does not support datalake linked service for on demand HD insight cluster and one have to copy data onto blob from copy activity and than execute the job. BUT this work around seems to be a hugely resource expensive in case of a billion files on a datalake. Is there any efficient way to access datalake files either from python script that execute spark jobs or any other way to directly access the files.</p>

<p>P.S Is there a possiblity of doing similar thing from v1, if yes then how? ""Create on-demand Hadoop clusters in HDInsight using Azure Data Factory"" describe on demand hadoop cluster that access blob storage but I want on demand spark cluster that access datalake.</p>

<p>P.P.s Thanks in advance</p>
","<python><pyspark><azure-hdinsight><azure-data-factory><azure-data-lake>","2018-01-09 10:18:28","352","1","2","49116105","<p>The Blob storage is used for the scripts and config files that the On Demand cluster will use. In the scripts you write and store in the attached Blob storage they can write from ADLS to SQLDB for example.</p>
"
"48164837","Azure ExpiredAuthenticationToken when fetching the Azure data Factory information by C# console application via Visual Studio","<p>I have written a console application which could fetch all information about Azure datafactory for a given azure subscription and resource group. I am using Microsoft provided .NET API.
The code runs for a while as it monitors all the data factories and pipelines and put it into azure SQL database.
I am getting following exception when code runs for more than a hour: 
<a href=""https://i.stack.imgur.com/tcwft.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tcwft.png"" alt=""token expire exception""></a></p>

<p>I am using following code to generate the to</p>

<pre><code> public static async Task&lt;string&gt; GetAuthorizationHeader ()
        {
        AuthenticationContext context = new AuthenticationContext(ConfigurationManager.AppSettings[""ActiveDirectoryEndpoint""] +
            ConfigurationManager.AppSettings[""ActiveDirectoryTenantId""]);
        ClientCredential credential = new ClientCredential(
            ConfigurationManager.AppSettings[""ApplicationId""],
            ConfigurationManager.AppSettings[""Password""]);
        AuthenticationResult result = await context.AcquireTokenAsync(
            resource: ConfigurationManager.AppSettings[""WindowsManagementUri""],
            clientCredential: credential);

        if ( result != null )
            return result.AccessToken;

        throw new InvalidOperationException(""Failed to acquire token"");
        }

public static TokenCloudCredentials GetTokenCloudCredentials ()
        {
        return new TokenCloudCredentials(ConfigurationManager.AppSettings[""SubscriptionId""],
            GetAuthorizationHeader().Result);
        }
</code></pre>

<p>This code is available at microsoft website <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-copy-activity-tutorial-using-dotnet-api"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Is there any way to increase the expiration time of token?
or
How do I refresh the authentication token?</p>

<p>Thanks,
Jai</p>
","<azure><azure-data-factory>","2018-01-09 09:18:42","2267","0","2","48167626","<p>I had a similar problem but with data factory version 2, I fixed it by capturing the exception with a try catch, and in the catch part call the same method to obtain the token.</p>

<p>I'm not sure if it will work on version 1, but it should!</p>

<p>Hope this helped</p>
"
"48164837","Azure ExpiredAuthenticationToken when fetching the Azure data Factory information by C# console application via Visual Studio","<p>I have written a console application which could fetch all information about Azure datafactory for a given azure subscription and resource group. I am using Microsoft provided .NET API.
The code runs for a while as it monitors all the data factories and pipelines and put it into azure SQL database.
I am getting following exception when code runs for more than a hour: 
<a href=""https://i.stack.imgur.com/tcwft.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tcwft.png"" alt=""token expire exception""></a></p>

<p>I am using following code to generate the to</p>

<pre><code> public static async Task&lt;string&gt; GetAuthorizationHeader ()
        {
        AuthenticationContext context = new AuthenticationContext(ConfigurationManager.AppSettings[""ActiveDirectoryEndpoint""] +
            ConfigurationManager.AppSettings[""ActiveDirectoryTenantId""]);
        ClientCredential credential = new ClientCredential(
            ConfigurationManager.AppSettings[""ApplicationId""],
            ConfigurationManager.AppSettings[""Password""]);
        AuthenticationResult result = await context.AcquireTokenAsync(
            resource: ConfigurationManager.AppSettings[""WindowsManagementUri""],
            clientCredential: credential);

        if ( result != null )
            return result.AccessToken;

        throw new InvalidOperationException(""Failed to acquire token"");
        }

public static TokenCloudCredentials GetTokenCloudCredentials ()
        {
        return new TokenCloudCredentials(ConfigurationManager.AppSettings[""SubscriptionId""],
            GetAuthorizationHeader().Result);
        }
</code></pre>

<p>This code is available at microsoft website <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-copy-activity-tutorial-using-dotnet-api"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Is there any way to increase the expiration time of token?
or
How do I refresh the authentication token?</p>

<p>Thanks,
Jai</p>
","<azure><azure-data-factory>","2018-01-09 09:18:42","2267","0","2","48168092","<p>As the error indicates, your token has expired. The token lifetimes are configured with your Token Service / Identity Provider (e.g. Azure AD). The token time can be changed but please consider the security aspects when issuing long lived tokens (and look for best practices around token lifetime if you decide to update this). </p>

<p>I believe, instead of further increasing the token lifetime, you could consider refreshing the token before it expires (or request for a new token after expiry) based on your use case. </p>

<p>Before you invoke the Api, Please check for the token expiry time with current time and refresh / request the token. This will help avoid a call to the service with an expired token.</p>
"
"48159589","How to Delete/modifiy ADF v2 Trigger","<p>I am following the artcile at
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</a></p>

<p>I have created a Scheduled Trigger to run the pipeline every day.
Where in portal will this show up ?
Also how can i delete or modify this Trigger ?</p>
","<azure-data-factory>","2018-01-08 23:39:25","4250","1","3","48167139","<p>The portal doesnt show much for data factory v2, you have the Monitor &amp; Manage interface that will show you pipeline runs, their activities and stuff, but thats about it, you dont see triggers, datasets, linked services or anything from the portal at the moment (remember its still in preview).</p>

<p>If you have powershell with azure libraries installed, you can modify or delete triggers from it. Just login with </p>

<pre><code>Login-AzureRmAccount
</code></pre>

<p>then select a subscription with </p>

<pre><code>Select-AzureRmSubscription -SubscriptionName ""your subscription name""
</code></pre>

<p>To delete the trigger you use the command (fill the """" with your data): </p>

<pre><code>Remove-AzureRmDataFactoryV2Trigger -ResourceGroupName """" -DataFactoryName """" -Name """"
</code></pre>

<p>To modify a trigger you use the same command you used to create a new one:</p>

<pre><code>Set-AzureRmDataFactoryV2Trigger -DataFactoryName """" -ResourceGroupName """" -Name """" -DefinitionFile C:/(path to json)
</code></pre>

<p>Hope this helped!</p>

<p>PS: also if you dont remember the name of the trigger, you can get a list of triggers in your data factory with this command:</p>

<pre><code>Get-AzureRmDataFactoryV2Trigger -ResourceGroupName """" -DataFactoryName """"
</code></pre>
"
"48159589","How to Delete/modifiy ADF v2 Trigger","<p>I am following the artcile at
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</a></p>

<p>I have created a Scheduled Trigger to run the pipeline every day.
Where in portal will this show up ?
Also how can i delete or modify this Trigger ?</p>
","<azure-data-factory>","2018-01-08 23:39:25","4250","1","3","65003754","<p>Super useful, I had renamed a trigger which had actually cloned it so I needed to delete the old one. I had just had a new laptop so I needed to do</p>
<pre><code>Install-Module Az

Set-ExecutionPolicy RemoteSigned --
</code></pre>
<p>That's a GPO that needs fixing by my organisation</p>
<p>Then I got a little stuck because our resource group name and factory names are very similar. Resource group is on the first page in the middle at the top, factory is the name of the factory</p>
"
"48159589","How to Delete/modifiy ADF v2 Trigger","<p>I am following the artcile at
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</a></p>

<p>I have created a Scheduled Trigger to run the pipeline every day.
Where in portal will this show up ?
Also how can i delete or modify this Trigger ?</p>
","<azure-data-factory>","2018-01-08 23:39:25","4250","1","3","68291607","<p>Click the &quot;Manage&quot; button on the left side of the screen, and then click &quot;Triggers&quot; under the &quot;Author&quot; heading.</p>
<p>When you hover over the trigger names you can delete them.</p>
<p><a href=""https://i.stack.imgur.com/avSxY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/avSxY.png"" alt=""manage ADF triggers"" /></a></p>
"
"48154035","How to debug Azure Data Factory V2 Invalid Trigger Type?","<p>I created a new ADF V2 pipleline with a tumbling window trigger via powershell.  When starting the pipeline I get an error:</p>

<pre><code>Start-AzureRmDataFactoryV2Trigger -ResourceGroupName $ResourceGroup -DataFactoryName $DataFactoryName -Name ""DailyTriggerCookForecastPipeline""

[ERROR] Start-AzureRmDataFactoryV2Trigger : HTTP Status Code: BadRequest
[ERROR] Error Code: BadRequest
[ERROR] Error Message: Invalid trigger type: Trigger
[ERROR] Request Id: 6591ae6b-902d-4b25-9f62-c6bb67796d57
[ERROR] Timestamp (Utc):01/08/2018 16:05:30
[ERROR] At line:1 char:1
[ERROR] + Start-AzureRmDataFactoryV2Trigger -ResourceGroupName $ResourceGroup - ...
[ERROR] + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[ERROR]     + CategoryInfo          : CloseError: (:) [Start-AzureRmDataFactoryV2Trigger], ErrorResponseException
[ERROR]     + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.StartAzureDataFactoryTriggerCommand
[ERROR]  
</code></pre>

<p>My trigger is defined as:</p>

<pre><code>{
  ""name"": ""DailyTriggerCookForecastPipeline"",
  ""properties"": {
    ""type"": ""TumblingWindowTrigger"",
    ""typeProperties"": {
      ""frequency"": ""Hour"",
      ""interval"": ""24"",
      ""startTime"": ""2018-01-01T04:00:00Z"",
      ""retryPolicy"": {
        ""count"": 2,
        ""intervalInSeconds"": 30
      },
      ""maxConcurrency"": 1
    },
    ""pipeline"": {
      ""pipelineReference"": {
        ""type"": ""PipelineReference"",
        ""referenceName"": ""CookForecastPipeline""
      },
      ""parameters"": {
        ""SliceStart"": ""@trigger().outputs.windowStartTime""
      }
    }
  }
}
</code></pre>

<p>I have no idea how to debug this further.  Is there a way to get a more detailed error message?  I couldn't find one in the portal.
Thanks!</p>
","<azure-data-factory>","2018-01-08 16:12:24","1350","0","1","48313929","<p>Like Martin suggested in the comments a good first step is to add -verbose flag.  Unfortunately it doesn't add much info.</p>

<p>The second step for debugging is to make sure your setup is correct.  In this case it turned out that TumblingWindowTrigger was just released but I had an older SDK.  This dependency wasn't documented anywhere.  So turns out the errors are pretty accurate.</p>
"
"48151753","How to use truncate in Copy Preview to Truncate various tables","<p>How can I use a truncate command to truncate all the values inside multiple tables? And how to pass this inside the Copy Preview feature?</p>

<p>How to copy the latest blob to Azure datawarehouse using copy preview?</p>

<p>I have various tables in various folders with multiple amounts of data. How can I write JSON to only copy the latest data to Azure datawarehouse?</p>
","<azure><azure-data-factory>","2018-01-08 13:54:59","2540","0","2","48152949","<p>I don't feel like you've tried very hard here and are expecting free dev work.</p>

<p>The copy wizard in ADFv1 is only for a very specific purpose and isn't very good. You won't be able to use it for more complex things like you describe above.</p>

<p>I recommend you open up Visual Studio 2015, load an ADFv1 project and start figuring out what JSON you need.</p>

<p>There are plenty of resources out there to use to develop complex data factory pipelines.</p>
"
"48151753","How to use truncate in Copy Preview to Truncate various tables","<p>How can I use a truncate command to truncate all the values inside multiple tables? And how to pass this inside the Copy Preview feature?</p>

<p>How to copy the latest blob to Azure datawarehouse using copy preview?</p>

<p>I have various tables in various folders with multiple amounts of data. How can I write JSON to only copy the latest data to Azure datawarehouse?</p>
","<azure><azure-data-factory>","2018-01-08 13:54:59","2540","0","2","48166806","<p>You can achieve what you need by modifying the sqlReaderQuery  property inside your pipeline. It was nicely explained here by g_brahimaj <a href=""https://stackoverflow.com/questions/47903441/execute-storedprocedure-from-azure-datafactory/47905719#47905719"">Execute storedProcedure from azure datafactory</a> do the same, but change the exec command for the truncate command that you need.</p>

<p>Cheers</p>
"
"48149558","Using Data Lake Analytics Linked Service Service Principal Authentication in Data Factory","<p>Trying to add a compute node to ADF (v1), using Azure Data Lake Analytics Linked Service to make some simple u-sql jobs. Data Lake Store Linked Service is already in place and working with service principle authentication, but I can't deploy ADLA linked service with servicePrincipalId and Key.</p>

<p>As per this article, ADLA Linked Service supports service principle authentication for some time now: 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-usql-activity#azure-data-lake-analytics-linked-service"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-usql-activity#azure-data-lake-analytics-linked-service</a></p>

<p>The intellisense tells me the parameters are not valid and insists on user credential. Here's a screenshot:</p>

<p><a href=""https://i.stack.imgur.com/RPQBQ.jpg"" rel=""nofollow noreferrer"">ADF Portal Editor ADLA-ls</a></p>

<p>(Same warnings in visual studio data factory project btw.)</p>

<p>When I deploy with SPN anyway, I get the error ""Entity provisioning failed: Internal Server Error"".</p>

<p>Anyone successfully use service principle auth with ADLA linked service?</p>
","<azure><azure-data-factory><azure-data-lake>","2018-01-08 11:40:16","940","1","2","48152940","<p>It works for me using the following settings:</p>

<pre><code>{
  ""name"": ""datalakeanalyticsinstanceLinkedService"",
  ""properties"": {
    ""type"": ""AzureDataLakeAnalytics"",
    ""typeProperties"": {
      ""accountName"": ""datalakeanalyticsinstance"",
      ""servicePrincipalId"": ""xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx"",
      ""servicePrincipalKey"": ""abcdefghijklmopqrstuvwxyz123456780"",
      ""tenant"": ""mytenant.onmicrosoft.com""
    }
  }
}
</code></pre>

<p>Also make sure you've run the ""Add User Wizard"" from Data Lake Analytics. Using this wizard I added the service principal as an Owner with R+W permissions.</p>

<p>Edit: <a href=""https://mrpaulandrew.com/2018/01/10/azure-data-factory-v1-v2-service-principal-authentication-for-azure-data-lake/"" rel=""nofollow noreferrer"">This blog post</a> was just published with many details on Serivce Principal authentication</p>
"
"48149558","Using Data Lake Analytics Linked Service Service Principal Authentication in Data Factory","<p>Trying to add a compute node to ADF (v1), using Azure Data Lake Analytics Linked Service to make some simple u-sql jobs. Data Lake Store Linked Service is already in place and working with service principle authentication, but I can't deploy ADLA linked service with servicePrincipalId and Key.</p>

<p>As per this article, ADLA Linked Service supports service principle authentication for some time now: 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-usql-activity#azure-data-lake-analytics-linked-service"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-usql-activity#azure-data-lake-analytics-linked-service</a></p>

<p>The intellisense tells me the parameters are not valid and insists on user credential. Here's a screenshot:</p>

<p><a href=""https://i.stack.imgur.com/RPQBQ.jpg"" rel=""nofollow noreferrer"">ADF Portal Editor ADLA-ls</a></p>

<p>(Same warnings in visual studio data factory project btw.)</p>

<p>When I deploy with SPN anyway, I get the error ""Entity provisioning failed: Internal Server Error"".</p>

<p>Anyone successfully use service principle auth with ADLA linked service?</p>
","<azure><azure-data-factory><azure-data-lake>","2018-01-08 11:40:16","940","1","2","48423610","<p>After a couple of weeks of frustration, suddenly this issue resolved itself.</p>

<p>Adding SP via the ""Add Users Wizard"" from ADLA is certainly a must for any data lake related pipelines, as @Simon suggested. 
Other than that <a href=""https://mrpaulandrew.com/2018/01/10/azure-data-factory-v1-v2-service-principal-authentication-for-azure-data-lake/"" rel=""nofollow noreferrer"">@Paul's blog</a> clearly explains the process.</p>

<p>The fact that my pipeline suddenly started working at exactly midnight and hasn't failed since, makes me think ADF has recently got some bugs fixed. So until ADFv2 is fully GA, I suggest deploying the linked services over and over again, or better yet (and more frustrating) deploying new pipelines and services from scratch.</p>
"
"48148076","How can I copy dynamic data from on prem sqlserver to azure datawarehouse","<p>I have created a linked service that takes the data from on prem and store into the azure blob, but my data is dynamic how can I build a pipeline that takes the updated table into the blob and takes that blob and transfer it into the azure datawarehouse, I need this in such a way so that all my tables are in realtime sync into the azure datawarehouse.</p>
","<azure><azure-data-factory><azure-synapse>","2018-01-08 10:08:52","157","1","1","48304604","<p>What you are probably looking for is incrementally loading data into your datawarehouse.</p>

<p>The procedure described below is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"" title=""documented here"">documented here</a>. It assumes you have periodic snapshots of your whole source table into blobstorage.</p>

<ol>
<li>You need to elect a column to track changes in your table.

<ul>
<li>If you are only appending and never changing existing rows, the primary key will do the job.</li>
<li>However, if you have to cope with changes in existing rows, you need a way to track those changes (for instance in with a column named ""timestamp-of-last-update"" - or any better, more succinct name).

<ul>
<li><strong>Note</strong>: if you don't have such a column, you will not be able to track changes and therefore will not be able to load data incrementally.</li>
</ul></li>
</ul></li>
<li>For a given snapshot, we are interested in the rows added or updated in the source table. This content is called the <strong>delta</strong> associated to the snapshot. Once delta is computed, it can be upserted into your table with a Copy Activity that invokes an stored procedure. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"" title=""here"">Here</a> you can find details on how this is done.</li>
<li>Assuming the values of the elected column will only grow as rows are added/updated in the source table, it is necessary to keep track of its maximum value through the snapshots. This tracked value is called <strong>watermark</strong>. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"" title=""documented here"">This page</a> describes a way to persist the watermark into SQL Server.</li>
<li>Finally, you need to be able to compute the delta for a given snapshot given the last watermark stored. The basic idea is to select the rows where the elected column is greater than the stored watermark. You can do so using SQL Server (as described in the referred documentation), or you can use Hive on HDInsight to do this filtering.</li>
<li>Do not forget to update the watermark with the maximum value of the elected column once delta is upserted into your datawarehouse.</li>
</ol>
"
"48135202","Azure Data Factory Stored Procedure Parameter","<p>I am in the process of writing a stored procedure for Azure Data Factory, and need to pass the date as a string, for example: 'November, 2017'.</p>

<p>I've tried the following but I keep getting the following error on this line of code: </p>

<pre><code>WHERE FiscalMonth = @FiscalMonth;
</code></pre>

<p>ErRror:</p>

<blockquote>
  <p>The expression for this clause must be of bool type</p>
</blockquote>

<p>Code:</p>

<pre><code>DECLARE @FiscalMonth string = DateTime.Today.AddMonths(-2).ToString(""MM"") + "","" + DateTime.Today.AddMonths(-2).ToString(""yyyy"");

    @Input =

        EXTRACT [TPID] int,
                FiscalMonth string,
                ProductGroup string,
                [Workload Customer Age] int,
                [Entitlements] int,
                [Field Activations Capped] int,
                [Field Activiations Uncapped] int,
                [True Activations Capped] int,
                [True Activations Uncapped] int,
                [CappedFlag] string,
                [AOM_Status_Current] string,
                [AOM_Status_Historical] string
        FROM @DeltaPublishOutputPath
        USING Extractors.Text(delimiter : '\u0001', rowDelimiter : ""\r"", nullEscape : ""#NULL#"", silent : true);

@Output =
    SELECT of. *
    FROM @Input AS of
    WHERE FiscalMonth = @FiscalMonth;
</code></pre>
","<azure-data-factory>","2018-01-07 07:31:15","695","0","1","48135591","<p>Ends up that I needed to add a doulbe = sign.  Here is updated line. </p>

<p>WHERE FiscalMonth == @FiscalMonth</p>
"
"48111212","Adding Extraction DateTime in Azure Data Factory","<p>I want to write a generic DataFactory in V2 with below scenario.</p>

<blockquote>
  <p>Source ---> Extracted (Salesforce or some other way), which don't have
  extraction timestamp. ---> I want to write it to Blob with extraction
  Time Stamp.</p>
</blockquote>

<p>I want it to be generic, so I don't want to give column mapping anywhere. </p>

<p>Is there any way to use expression or system variable in Custom activity to append a column in output dataset? I like to have a very simple solution to make implementation realistic. </p>
","<azure><azure-data-factory>","2018-01-05 10:02:41","2091","0","1","48114254","<p>To do that you should change the query to add the column you need, with the query property in the copy activity of the pipeline. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce#copy-activity-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce#copy-activity-properties</a></p>

<p>I dont know much about Salesforce, but in SQL Server you can do the following:</p>

<pre><code>SELECT *, CURRENT_TIMESTAMP as AddedTimeStamp from [schema].[table]
</code></pre>

<p>This will give you every field on your table and will add a column named AddedTimeStamp with the CURRENT_TIMESTAMP value in every row of the result.</p>

<p>Hope this helped!</p>
"
"48094949","Azure Data Factory json dataset missing property in typeProperties","<p>I am creating a dataset from json-formatted data in an Azure Data Factory (v1). When using the following code, I get a <code>Property expected</code> error with the infotext <code>Property specific to this data set type</code> on the <code>typeProperties</code> object. From what I can see, I am using the same properties as in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format"" rel=""nofollow noreferrer"">example docs</a>. What property am I missing?</p>

<p>Dataset definition:</p>

<pre><code>{
    ""name"": ""JsonDataSetData"",
    ""properties"": {
        ""type"": ""AzureDataLakeStore"",
        ""linkedServiceName"": ""TestAzureDataLakeStoreLinkedService"",
        ""structure"": [
            {
                ""name"": ""timestamp"",
                ""type"": ""String""
            },
            {
                ""name"": ""value"",
                ""type"": ""Double""
            }
        ],
        ""typeProperties"": {
            ""folderPath"": ""root_folder/sub_folder"",
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""setOfObjects"",
                ""jsonPathDefinition"": {
                    ""spid"": ""$.timestamp"",
                    ""value"": ""$.value""
                }
            },
        },
        ""availability"": {
            ""frequency"": ""Day"",
            ""interval"": 1
        }
    }
}
</code></pre>
","<json><azure><azure-data-factory><azure-data-lake>","2018-01-04 12:06:30","637","1","1","48098628","<p>Have you tried adding encodingName and nestingSeparator with the default values? The documentation has mistakes sometimes and a property that is documented as not required might be giving you this error.</p>
"
"48092088","Azure Data Factory copy activity with stored procedure","<p>Is there a workaround for the fact that you need to name the first parameter of the stored procedure (the one containing the table type) exactly as the property ""tableName"" in the input dataaset?</p>

<p><em>Im using Azure Data Factory V1.</em></p>

<p><strong>Input dataset (On-premise Oracle source)</strong></p>

<pre><code>{
""name"": ""DS-ORA-WMS-CDC-DLYTRN"",
""properties"": {
    ""published"": false,
    ""type"": ""OracleTable"",
    ""linkedServiceName"": ""LS-ORA-WMS-CDC"",
    ""typeProperties"": {
        ""tableName"": ""WMST.DLYTRN""
    },
    ""availability"": {
        ""frequency"": ""Hour"",
        ""interval"": 1
    },
    ""external"": true,
    ""policy"": {}
}}
</code></pre>

<p><strong>Output dataset (Azure SQL database)</strong></p>

<pre><code>{
""name"": ""DS-ASQL-ANALYTICS-DLYTRN"",
""properties"": {
    ""published"": false,
    ""type"": ""AzureSqlTable"",
    ""linkedServiceName"": ""LS-SQL-ANALYTICS-DB"",
    ""typeProperties"": {
        ""tableName"": ""wms.DLYTRN""
    },
    ""availability"": {
        ""frequency"": ""Hour"",
        ""interval"": 1
    }
}}
</code></pre>

<p><strong>Pipeline</strong></p>

<pre><code>{
""name"": ""test"",
""properties"": {
    ""description"": ""test pipeline"",
    ""activities"": [
        {
            ""type"": ""Copy"",
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""OracleSource"",
                    ""oracleReaderQuery"": ""select * from WMST.DLYTRN""
                },
                ""sink"": {
                    ""type"": ""SqlSink"",
                    ""sqlWriterStoredProcedureName"": ""wms.spPersistDlytrn"",
                    ""storedProcedureParameters"": {
                        ""srcdc"": {
                            ""value"": ""CDC""
                        }
                    },
                    ""sqlWriterTableType"": ""wms.DLYTRNType"",
                    ""writeBatchSize"": 0,
                    ""writeBatchTimeout"": ""00:00:00""
                }
            },
            ""inputs"": [
                {
                    ""name"": ""DS-ORA-WMS-CDC-DLYTRN""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""DS-ASQL-ANALYTICS-DLYTRN""
                }
            ],
            ""policy"": {
                ""timeout"": ""1.00:00:00"",
                ""concurrency"": 1,
                ""retry"": 3
            },
            ""scheduler"": {
                ""frequency"": ""Hour"",
                ""interval"": 1
            },
            ""name"": ""TestWMSCopyWithSproc""
        }
    ],
    ""start"": ""2018-01-04T07:00:00Z"",
    ""end"": ""2018-01-08T00:00:00Z"",
    ""isPaused"": false,
    ""hubName"": ""hub"",
    ""pipelineMode"": ""Scheduled""
}}
</code></pre>

<p><strong>Stored procedure</strong></p>

<pre><code>CREATE PROCEDURE [wms].[spPersistDlytrn]
   @DLYTRNTable [wms].[DLYTRNType] READONLY,
   @srcdc VARCHAR(4)
AS
...
RETURN 0
</code></pre>

<p>When running the activity it return the below error message:</p>

<blockquote>
  <p>Database operation failed on server 'Sink:tcp:someservername.database.windows.net,1433' with SQL Error Number '349'. Error message from database execution : The procedure ""spPersistDlytrn"" has no parameter named ""@wms.DLYTRN""..</p>
</blockquote>

<p>As it is not possible to name the stored procedure parameter ""wms.DLYTRN"" is there a way to exclude the schema prefix?</p>
","<azure><azure-data-factory>","2018-01-04 09:19:44","4066","0","2","48098054","<p>I cannot test this right now, but as it is said here <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-invoke-stored-procedure-from-copy-activity#stored-procedure-definition"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-invoke-stored-procedure-from-copy-activity#stored-procedure-definition</a> ""The name of the first parameter of stored procedure must match the tableName defined in the dataset JSON"".</p>

<p>In the example it declares two parameters in the sp: </p>

<ul>
<li>@Marketing [dbo].[MarketingType] READONLY</li>
<li>@stringData varchar(256)</li>
</ul>

<p>At the dataset it doesnt use the schema prefix, it just says: ""tableName"": ""Marketing"", without the schema (try this, as you have the schema in your output dataset definition).</p>

<p>Then at the pipeline, it just gives value for stringData. Also this: ""SqlWriterTableType"": ""MarketingType"", see that it doesnt have the schema prefix, and your pipeline definition does have it.</p>

<p>So to sum it up: MarketingType is the actual name of the table and is at the SqlWriterTableType property of the copy activity, while Marketing is the name of the parameter in the stored procedure, and the name of the tablename in the output dataset.</p>

<p>Hope this helps!</p>
"
"48092088","Azure Data Factory copy activity with stored procedure","<p>Is there a workaround for the fact that you need to name the first parameter of the stored procedure (the one containing the table type) exactly as the property ""tableName"" in the input dataaset?</p>

<p><em>Im using Azure Data Factory V1.</em></p>

<p><strong>Input dataset (On-premise Oracle source)</strong></p>

<pre><code>{
""name"": ""DS-ORA-WMS-CDC-DLYTRN"",
""properties"": {
    ""published"": false,
    ""type"": ""OracleTable"",
    ""linkedServiceName"": ""LS-ORA-WMS-CDC"",
    ""typeProperties"": {
        ""tableName"": ""WMST.DLYTRN""
    },
    ""availability"": {
        ""frequency"": ""Hour"",
        ""interval"": 1
    },
    ""external"": true,
    ""policy"": {}
}}
</code></pre>

<p><strong>Output dataset (Azure SQL database)</strong></p>

<pre><code>{
""name"": ""DS-ASQL-ANALYTICS-DLYTRN"",
""properties"": {
    ""published"": false,
    ""type"": ""AzureSqlTable"",
    ""linkedServiceName"": ""LS-SQL-ANALYTICS-DB"",
    ""typeProperties"": {
        ""tableName"": ""wms.DLYTRN""
    },
    ""availability"": {
        ""frequency"": ""Hour"",
        ""interval"": 1
    }
}}
</code></pre>

<p><strong>Pipeline</strong></p>

<pre><code>{
""name"": ""test"",
""properties"": {
    ""description"": ""test pipeline"",
    ""activities"": [
        {
            ""type"": ""Copy"",
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""OracleSource"",
                    ""oracleReaderQuery"": ""select * from WMST.DLYTRN""
                },
                ""sink"": {
                    ""type"": ""SqlSink"",
                    ""sqlWriterStoredProcedureName"": ""wms.spPersistDlytrn"",
                    ""storedProcedureParameters"": {
                        ""srcdc"": {
                            ""value"": ""CDC""
                        }
                    },
                    ""sqlWriterTableType"": ""wms.DLYTRNType"",
                    ""writeBatchSize"": 0,
                    ""writeBatchTimeout"": ""00:00:00""
                }
            },
            ""inputs"": [
                {
                    ""name"": ""DS-ORA-WMS-CDC-DLYTRN""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""DS-ASQL-ANALYTICS-DLYTRN""
                }
            ],
            ""policy"": {
                ""timeout"": ""1.00:00:00"",
                ""concurrency"": 1,
                ""retry"": 3
            },
            ""scheduler"": {
                ""frequency"": ""Hour"",
                ""interval"": 1
            },
            ""name"": ""TestWMSCopyWithSproc""
        }
    ],
    ""start"": ""2018-01-04T07:00:00Z"",
    ""end"": ""2018-01-08T00:00:00Z"",
    ""isPaused"": false,
    ""hubName"": ""hub"",
    ""pipelineMode"": ""Scheduled""
}}
</code></pre>

<p><strong>Stored procedure</strong></p>

<pre><code>CREATE PROCEDURE [wms].[spPersistDlytrn]
   @DLYTRNTable [wms].[DLYTRNType] READONLY,
   @srcdc VARCHAR(4)
AS
...
RETURN 0
</code></pre>

<p>When running the activity it return the below error message:</p>

<blockquote>
  <p>Database operation failed on server 'Sink:tcp:someservername.database.windows.net,1433' with SQL Error Number '349'. Error message from database execution : The procedure ""spPersistDlytrn"" has no parameter named ""@wms.DLYTRN""..</p>
</blockquote>

<p>As it is not possible to name the stored procedure parameter ""wms.DLYTRN"" is there a way to exclude the schema prefix?</p>
","<azure><azure-data-factory>","2018-01-04 09:19:44","4066","0","2","51021387","<p>I solved exactly the same issue recently.
Edit code of the dataset your copy activity references as ""Source"", so its <code>typeProperties</code> section contains table name without a schema prefix.
For example:</p>

<pre class=""lang-js prettyprint-override""><code>""typeProperties"": {
    ""tableName"": ""DLYTRN""
}
</code></pre>

<p>Also, the name of the first parameter of your procedure must match the name of the table so it should be <code>@DLYTRN</code> instead of <code>@DLYTRNTable</code>.</p>
"
"48088308","Fault tolerance and log the incompatible rows in Azure Blob storage","<p>Customer 's requirements: </p>

<ol>
<li><p>using use azure data factory to import csv file in blob storage to SQL data warehouse.</p></li>
<li><p>using the strategy ""Fault tolerance and log the incompatible rows in Azure Blob storage"" in ADF.</p></li>
<li><p>And using Azure Function to archive the processed file to other place in blob storage: one place for those files are imported successfully and one for fail files (the files have incompatible data - wrong format, wrong length)</p></li>
</ol>

<p>=> <strong>so I need get value of skippedRowCount of Activity Window to know this activity which has some incompatible rows? Is there any ways to get that ways or any solution to solve my problem?</strong> Many thanks.</p>
","<azure-functions><azure-data-factory><azure-sdk>","2018-01-04 03:35:49","2423","-1","2","48312960","<p>In ADF V2, the number of skipped rows is returned as ""rowsSkipped"" property of copy activity output.  See these two links: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#monitoring"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#monitoring</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#monitor-skipped-rows"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#monitor-skipped-rows</a></p>

<p>ADF V2 also allows you to use the output from a previous copy activity in the subsequent activity, using an expression like ""@activity('MyCopyActivity').output.rowsSkipped"")"".  Here is an example of how to use output form Lookup activity, and you can adapt to your particular situation.</p>

<p>For you use case, you can chain the copy activity with two Web activities, one to invoke the file archive for successful files, and another to record/reprocess failed rows logged in Storage blob or ADLS.</p>
"
"48088308","Fault tolerance and log the incompatible rows in Azure Blob storage","<p>Customer 's requirements: </p>

<ol>
<li><p>using use azure data factory to import csv file in blob storage to SQL data warehouse.</p></li>
<li><p>using the strategy ""Fault tolerance and log the incompatible rows in Azure Blob storage"" in ADF.</p></li>
<li><p>And using Azure Function to archive the processed file to other place in blob storage: one place for those files are imported successfully and one for fail files (the files have incompatible data - wrong format, wrong length)</p></li>
</ol>

<p>=> <strong>so I need get value of skippedRowCount of Activity Window to know this activity which has some incompatible rows? Is there any ways to get that ways or any solution to solve my problem?</strong> Many thanks.</p>
","<azure-functions><azure-data-factory><azure-sdk>","2018-01-04 03:35:49","2423","-1","2","70636483","<p>You can use the following screenshot as a reference</p>
<pre><code>@concat('logexceptions/people', '/', formatdatetime(utcnow(), 'yyyy'), '/', formatdatetime(utcnow(), 'MM'))
</code></pre>
<p><a href=""https://i.stack.imgur.com/YRCtm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YRCtm.png"" alt=""enter image description here"" /></a></p>
"
"48082633","Azure Data Factory On-Premises Copy Error","<p>I am trying to schedule an on-premises copy job that contains a SQL Server.
 However I am getting a different kind of error when trying to enter the sql server credentials.</p>

<blockquote>
  <p>Type=Microsoft.Data.Mashup.InternalMashupException.Message..sorry,en
  error occurred during
  evaluation.,Source=Microsoft.Data.Mashup""Type=Microsoft.Mas..data
  protection operation was unsuccessful. This may have been caused by
  not having the user profile loaded for the current thread's user
  context which may be the case when the thread is
  impersonanting.Sources, ""Type=Microsoft.Mashup.Evaluation.Inter...</p>
</blockquote>

<p>I have provisioned the gateway onto the server where the sql server is hosted but getting this error.</p>

<p>Also I am using the Copy Preview feature to get this working.</p>
","<azure><azure-data-factory>","2018-01-03 17:53:17","574","0","1","48083242","<p>I may be wrong here, but it looks like an authentication issue. Have you tried connecting to the server from the gateway? Open the Microsoft Integration Runtime Configuration Manager, go to Diagnostics and fill every field, then click test. If everything is correct, a green check should appear. You are trying to impersonate a user, so choose Windows instead of Basic.</p>

<p>Try with it until you get the green check, then make sure that the linked service you are using has the same info you used and it should work.</p>

<p>You can also try creating a database user and give it permissions to make the query you want, then changing the linked service to use the database user instead of the windows user.</p>
"
"48082294","Ingestion -> Transform -> Process -> Stage Tools in Azure","<p>I don't work with database so want to confirm best practices before I start. I have been given a project to where I am supposed to take few differently modeled data files, process them and show some standard reporting. </p>

<p>It will be batch ingestion, can be weekly or daily max. The cloud we will be using is Microsoft Azure. I am thinking of below tools for the job from my development experience. But will love some advise from you guys:</p>

<p>-> Ingestion [and stage] => write a rest api which takes the data and stage it in Elastic</p>

<p>-> Processing -> Azure HDInsight.. I haven't used it but from brief reading, that sounds like the recommended choice.</p>

<p>-> Staging and Reporting -> back to Elastic search, with Kibana/Graphana. I have used these tools so preferring it.</p>

<p>I also read some articles recommending Azure Data Factory. Look forward to do some suggestion.</p>

<p>Regards,
Gaurav Sharma</p>
","<azure><business-intelligence><azure-data-factory><azure-data-catalog>","2018-01-03 17:30:28","151","0","1","48083462","<p>For the ingestion you can use Data Factory, it supports calling a rest api with a WebActivity. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity</a></p>

<p>For processing, have you considered storing the data in a Data Lake Store and using U-SQL to make the transformations? I'm just mentioning it because you may have skipped considering it. <a href=""https://learn.microsoft.com/en-us/azure/data-lake-analytics/"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-analytics/</a></p>

<p>For reporting Microsoft offers Power Bi, it can connect to a lot of sources included rest APIs. It may or may not be useful for you, depending on the requirements of your project. <a href=""https://app.powerbi.com"" rel=""nofollow noreferrer"">https://app.powerbi.com</a></p>

<p>Hope this helps!</p>
"
"48077785","SSAS MDX query as Azure Data Factory source in Linked Service","<p>This question might not be well researched but I need to find out proper way to implement this solution before starting design.</p>

<p>Question is, Can we consume SSAS MDX query as datasource in Azure Data Factory Linked Service source? </p>
","<ssas><mdx><azure-data-factory>","2018-01-03 12:50:18","2712","6","2","48079013","<p>The supported list of connectors for the Copy activity available as at today is available here:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats</a></p>

<p>It looks like SSAS MDX queries are not included at this point.</p>

<p>ADF v2 supports the running of SSIS packages within ADF pipelines so it <em>may</em> be possible via that route (untested).</p>
"
"48077785","SSAS MDX query as Azure Data Factory source in Linked Service","<p>This question might not be well researched but I need to find out proper way to implement this solution before starting design.</p>

<p>Question is, Can we consume SSAS MDX query as datasource in Azure Data Factory Linked Service source? </p>
","<ssas><mdx><azure-data-factory>","2018-01-03 12:50:18","2712","6","2","48079664","<p>Data factory cannot query SSAS with MDX or DAX, but maybe you can query the source of the SSAS, in a traditional BI architecture it would be a Data Warehouse or a SQL server. This is because SSAS models are meant to be consumed by reporting tools (Power BI, reporting services, etc) and not data integration tools, which serve very different processes.</p>

<p>Cheers!</p>
"
"48074454","azure data factory start pipeline different from starting job","<p>I am getting crazy on this issue, I am running an Azure data factory V1, I need to schedule a copy job every week from 01/03/2009 through 01/31/2009, so I defined this schedule on the pipeline:</p>

<pre><code>    ""start"": ""2009-01-03T00:00:00Z"",
    ""end"": ""2009-01-31T00:00:00Z"",
    ""isPaused"": false,
</code></pre>

<p>monitoring the pipeline, the data factory schedule on these date:</p>

<pre><code>12/29/2008
01/05/2009
01/12/2009
01/19/2009
01/26/2009
</code></pre>

<p>instead of this wanted schedule:</p>

<pre><code>01/03/2009
01/10/2009
01/17/2009
01/24/2009
01/31/2009
</code></pre>

<p>why the starting date defined on the pipeline doesn't correspond to the schedule date on the monitor?</p>

<p>Many thanks!</p>

<p>Here is the JSON Pipeline:</p>

<pre><code>{
""name"": ""CopyPipeline-blob2datalake"",
""properties"": {
    ""description"": ""copy from blob storage to datalake directory structure"",
    ""activities"": [
        {
            ""type"": ""DataLakeAnalyticsU-SQL"",
            ""typeProperties"": {
                ""scriptPath"": ""script/dat230.usql"",
                ""scriptLinkedService"": ""AzureStorageLinkedService"",
                ""degreeOfParallelism"": 5,
                ""priority"": 100,
                ""parameters"": {
                    ""salesfile"": ""$$Text.Format('/DAT230/{0:yyyy}/{0:MM}/{0:dd}.txt', Date.StartOfDay (SliceStart))"",
                    ""lineitemsfile"": ""$$Text.Format('/dat230/dataloads/{0:yyyy}/{0:MM}/{0:dd}/factinventory/fact.csv', Date.StartOfDay (SliceStart))""
                }
            },
            ""inputs"": [
                {
                    ""name"": ""InputDataset-dat230""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""OutputDataset-dat230""
                }
            ],
            ""policy"": {
                ""timeout"": ""01:00:00"",
                ""concurrency"": 1,
                ""retry"": 1
            },
            ""scheduler"": {
                ""frequency"": ""Day"",
                ""interval"": 7
            },
            ""name"": ""DataLakeAnalyticsUSqlActivityTemplate"",
            ""linkedServiceName"": ""AzureDataLakeAnalyticsLinkedService""
        }
    ],
    ""start"": ""2009-01-03T00:00:00Z"",
    ""end"": ""2009-01-11T00:00:00Z"",
    ""isPaused"": false,
    ""hubName"": ""edxlearningdf_hub"",
    ""pipelineMode"": ""Scheduled""
}
}
</code></pre>

<p>and here the datasets:</p>

<pre><code>{
""name"": ""InputDataset-dat230"",
""properties"": {
    ""structure"": [
        {
            ""name"": ""Date"",
            ""type"": ""Datetime""
        },
        {
            ""name"": ""StoreID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""StoreName"",
            ""type"": ""String""
        },
        {
            ""name"": ""ProductID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""ProductName"",
            ""type"": ""String""
        },
        {
            ""name"": ""Color"",
            ""type"": ""String""
        },
        {
            ""name"": ""Size"",
            ""type"": ""String""
        },
        {
            ""name"": ""Manufacturer"",
            ""type"": ""String""
        },
        {
            ""name"": ""OnHandQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""OnOrderQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""SafetyStockQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""UnitCost"",
            ""type"": ""Double""
        },
        {
            ""name"": ""DaysInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MinDayInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MaxDayInStock"",
            ""type"": ""Int64""
        }
    ],
    ""published"": false,
    ""type"": ""AzureBlob"",
    ""linkedServiceName"": ""Source-BlobStorage-dat230"",
    ""typeProperties"": {
        ""fileName"": ""*.txt.gz"",
        ""folderPath"": ""dat230/{year}/{month}/{day}/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": ""\t"",
            ""firstRowAsHeader"": true
        },
        ""partitionedBy"": [
            {
                ""name"": ""year"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""yyyy""
                }
            },
            {
                ""name"": ""month"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""MM""
                }
            },
            {
                ""name"": ""day"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""dd""
                }
            }
        ],
        ""compression"": {
            ""type"": ""GZip""
        }
    },
    ""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 7
    },
    ""external"": true,
    ""policy"": {}
}
}

{
""name"": ""OutputDataset-dat230"",
""properties"": {
    ""structure"": [
        {
            ""name"": ""Date"",
            ""type"": ""Datetime""
        },
        {
            ""name"": ""StoreID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""StoreName"",
            ""type"": ""String""
        },
        {
            ""name"": ""ProductID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""ProductName"",
            ""type"": ""String""
        },
        {
            ""name"": ""Color"",
            ""type"": ""String""
        },
        {
            ""name"": ""Size"",
            ""type"": ""String""
        },
        {
            ""name"": ""Manufacturer"",
            ""type"": ""String""
        },
        {
            ""name"": ""OnHandQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""OnOrderQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""SafetyStockQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""UnitCost"",
            ""type"": ""Double""
        },
        {
            ""name"": ""DaysInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MinDayInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MaxDayInStock"",
            ""type"": ""Int64""
        }
    ],
    ""published"": false,
    ""type"": ""AzureDataLakeStore"",
    ""linkedServiceName"": ""Destination-DataLakeStore-dat230"",
    ""typeProperties"": {
        ""fileName"": ""txt.gz"",
        ""folderPath"": ""dat230/dataloads/{year}/{month}/{day}/factinventory/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": ""\t""
        },
        ""partitionedBy"": [
            {
                ""name"": ""year"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""yyyy""
                }
            },
            {
                ""name"": ""month"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""MM""
                }
            },
            {
                ""name"": ""day"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""dd""
                }
            }
        ]
    },
    ""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 7
    },
    ""external"": false,
    ""policy"": {}
}
}
</code></pre>
","<azure-data-factory>","2018-01-03 09:20:47","227","0","2","48076092","<p>You need to look at the time slices for the datasets and there activity.</p>

<p>The pipeline schedule (badly named) only defines the start and end period in which any activities can use to provision and run there time slices.</p>

<p>ADFv1 doesn't use a recursive schedule like the SQL Server Agent. Each execution has to be provisioned at an interval on the time line (the schedule) you create.</p>

<p>For example, if you pipeline start and end is for 1 year. But your dataset and activity has a frequency of monthly and interval of 1 month you will only get 12 executions of the whatever is happening.</p>

<p>Apologies, but the concept of time slices is a little difficult to explain if you aren't already familiar. Maybe read this post: <a href=""https://blogs.msdn.microsoft.com/ukdataplatform/2016/05/03/demystifying-activity-scheduling-with-azure-data-factory/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/ukdataplatform/2016/05/03/demystifying-activity-scheduling-with-azure-data-factory/</a></p>

<p>Hope this helps.</p>
"
"48074454","azure data factory start pipeline different from starting job","<p>I am getting crazy on this issue, I am running an Azure data factory V1, I need to schedule a copy job every week from 01/03/2009 through 01/31/2009, so I defined this schedule on the pipeline:</p>

<pre><code>    ""start"": ""2009-01-03T00:00:00Z"",
    ""end"": ""2009-01-31T00:00:00Z"",
    ""isPaused"": false,
</code></pre>

<p>monitoring the pipeline, the data factory schedule on these date:</p>

<pre><code>12/29/2008
01/05/2009
01/12/2009
01/19/2009
01/26/2009
</code></pre>

<p>instead of this wanted schedule:</p>

<pre><code>01/03/2009
01/10/2009
01/17/2009
01/24/2009
01/31/2009
</code></pre>

<p>why the starting date defined on the pipeline doesn't correspond to the schedule date on the monitor?</p>

<p>Many thanks!</p>

<p>Here is the JSON Pipeline:</p>

<pre><code>{
""name"": ""CopyPipeline-blob2datalake"",
""properties"": {
    ""description"": ""copy from blob storage to datalake directory structure"",
    ""activities"": [
        {
            ""type"": ""DataLakeAnalyticsU-SQL"",
            ""typeProperties"": {
                ""scriptPath"": ""script/dat230.usql"",
                ""scriptLinkedService"": ""AzureStorageLinkedService"",
                ""degreeOfParallelism"": 5,
                ""priority"": 100,
                ""parameters"": {
                    ""salesfile"": ""$$Text.Format('/DAT230/{0:yyyy}/{0:MM}/{0:dd}.txt', Date.StartOfDay (SliceStart))"",
                    ""lineitemsfile"": ""$$Text.Format('/dat230/dataloads/{0:yyyy}/{0:MM}/{0:dd}/factinventory/fact.csv', Date.StartOfDay (SliceStart))""
                }
            },
            ""inputs"": [
                {
                    ""name"": ""InputDataset-dat230""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""OutputDataset-dat230""
                }
            ],
            ""policy"": {
                ""timeout"": ""01:00:00"",
                ""concurrency"": 1,
                ""retry"": 1
            },
            ""scheduler"": {
                ""frequency"": ""Day"",
                ""interval"": 7
            },
            ""name"": ""DataLakeAnalyticsUSqlActivityTemplate"",
            ""linkedServiceName"": ""AzureDataLakeAnalyticsLinkedService""
        }
    ],
    ""start"": ""2009-01-03T00:00:00Z"",
    ""end"": ""2009-01-11T00:00:00Z"",
    ""isPaused"": false,
    ""hubName"": ""edxlearningdf_hub"",
    ""pipelineMode"": ""Scheduled""
}
}
</code></pre>

<p>and here the datasets:</p>

<pre><code>{
""name"": ""InputDataset-dat230"",
""properties"": {
    ""structure"": [
        {
            ""name"": ""Date"",
            ""type"": ""Datetime""
        },
        {
            ""name"": ""StoreID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""StoreName"",
            ""type"": ""String""
        },
        {
            ""name"": ""ProductID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""ProductName"",
            ""type"": ""String""
        },
        {
            ""name"": ""Color"",
            ""type"": ""String""
        },
        {
            ""name"": ""Size"",
            ""type"": ""String""
        },
        {
            ""name"": ""Manufacturer"",
            ""type"": ""String""
        },
        {
            ""name"": ""OnHandQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""OnOrderQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""SafetyStockQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""UnitCost"",
            ""type"": ""Double""
        },
        {
            ""name"": ""DaysInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MinDayInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MaxDayInStock"",
            ""type"": ""Int64""
        }
    ],
    ""published"": false,
    ""type"": ""AzureBlob"",
    ""linkedServiceName"": ""Source-BlobStorage-dat230"",
    ""typeProperties"": {
        ""fileName"": ""*.txt.gz"",
        ""folderPath"": ""dat230/{year}/{month}/{day}/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": ""\t"",
            ""firstRowAsHeader"": true
        },
        ""partitionedBy"": [
            {
                ""name"": ""year"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""yyyy""
                }
            },
            {
                ""name"": ""month"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""MM""
                }
            },
            {
                ""name"": ""day"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""dd""
                }
            }
        ],
        ""compression"": {
            ""type"": ""GZip""
        }
    },
    ""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 7
    },
    ""external"": true,
    ""policy"": {}
}
}

{
""name"": ""OutputDataset-dat230"",
""properties"": {
    ""structure"": [
        {
            ""name"": ""Date"",
            ""type"": ""Datetime""
        },
        {
            ""name"": ""StoreID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""StoreName"",
            ""type"": ""String""
        },
        {
            ""name"": ""ProductID"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""ProductName"",
            ""type"": ""String""
        },
        {
            ""name"": ""Color"",
            ""type"": ""String""
        },
        {
            ""name"": ""Size"",
            ""type"": ""String""
        },
        {
            ""name"": ""Manufacturer"",
            ""type"": ""String""
        },
        {
            ""name"": ""OnHandQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""OnOrderQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""SafetyStockQuantity"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""UnitCost"",
            ""type"": ""Double""
        },
        {
            ""name"": ""DaysInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MinDayInStock"",
            ""type"": ""Int64""
        },
        {
            ""name"": ""MaxDayInStock"",
            ""type"": ""Int64""
        }
    ],
    ""published"": false,
    ""type"": ""AzureDataLakeStore"",
    ""linkedServiceName"": ""Destination-DataLakeStore-dat230"",
    ""typeProperties"": {
        ""fileName"": ""txt.gz"",
        ""folderPath"": ""dat230/dataloads/{year}/{month}/{day}/factinventory/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": ""\t""
        },
        ""partitionedBy"": [
            {
                ""name"": ""year"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""yyyy""
                }
            },
            {
                ""name"": ""month"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""MM""
                }
            },
            {
                ""name"": ""day"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""WindowStart"",
                    ""format"": ""dd""
                }
            }
        ]
    },
    ""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 7
    },
    ""external"": false,
    ""policy"": {}
}
}
</code></pre>
","<azure-data-factory>","2018-01-03 09:20:47","227","0","2","48079579","<p>Would you share with us the json for the datasets and the pipeline? It would be easier to help you having that.</p>

<p>In the meanwhile, check if you are using ""style"": ""StartOfInterval"" at the scheduler property of the activity, and also check if you are using an offset.</p>

<p>Cheers!</p>
"
"48064215","Connect to SQL Data Warehouse from HDInsight OnDemand","<p>I'm trying to read/write data to an Azure SQL Data Warehouse from a spark on demand HDInsight cluster. </p>

<p>I can do this from a normal HDInsight spark cluster by <a href=""https://gist.github.com/CBurbidge/c05ae3829204bdcd545ab662d7cd7d39"" rel=""nofollow noreferrer"">using a script action to install the jdbc driver</a> but I don't think it's possible to run script actions on the on demand clusters.</p>

<p>I've tried</p>

<ul>
<li>Copying the files from %user%.m2\repository\com\microsoft\sqlserver\mssql-jdbc\6.2.2.jre8 up to blob storage in a folder called jars next to where the built spark code is. </li>
<li>including the driver dependency in the built jar file</li>
</ul>

<p>Both of these led to a java.lang.NoClassDefFoundError</p>

<p>I'm not too familiar with scala/maven/JVM/etc so not sure what else to try or include in this SO question.</p>

<p>Scala code i'm trying to run is</p>

<pre><code>val sqlContext = SparkSession.builder().appName(""GenerateEventsSql"").getOrCreate()

val jdbcSqlConnStr = ""jdbc:sqlserver://someserver.database.windows.net:1433;databaseName=myDW;user=admin;password=XXXX;""

val tableName = ""dbo.SomeTable""

val allTableData = sqlContext.read.format(""jdbc"")
                    .options(Map(
                      ""driver"" -&gt; ""com.microsoft.sqlserver.jdbc.SQLServerDriver"",
                      ""url"" -&gt; jdbcSqlConnStr, ""dbtable"" -&gt; tableName)
                    )
  .load()
</code></pre>
","<scala><azure-hdinsight><azure-data-factory><mssql-jdbc><azure-synapse>","2018-01-02 16:10:33","454","1","1","50364233","<p>Jars on Blob storage folder are not accessible to the Class path of HDinsight spark job. You need to copy the jar files to the local host for example /tmp/jars/xyz.jar and mention the same in Spark-submit command. </p>

<p>For e.g.</p>

<pre><code>nohup spark-submit --jars /tmp/jars/xyz.jar
</code></pre>
"
"48063221","Azure DF CopyFromBlob Failing","<p>I am trying to copy files from an Azure Blob to an Azure Data Lake using a data factory.  I keep running into this error and am not finding any information on what the parameter 'baseURI' maps to:</p>

<pre><code>""errorCode"": ""2200"",
""message"": ""ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property &amp;apos;baseUri&amp;apos; is invalid: &amp;apos;Value cannot be null.\r\nParameter name: baseUri&amp;apos;.,Source=,''Type=System.ArgumentNullException,Message=Value cannot be null.\r\nParameter name: baseUri,Source=Microsoft.DataTransfer.MsiStoreServiceClient,'"",
""failureType"": ""UserError"",
""target"": ""CopyFromBlob""
</code></pre>

<p>I am using Powershell with json files.  Anything obvious that I am missing here?</p>

<p>Azure Data Lake Linked Service</p>

<pre><code>{
    ""name"": ""&lt;redacted&gt;"",
    ""properties"": {
        ""type"": ""AzureDataLakeStore"",
        ""typeProperties"": {
            ""dataLakeStoreUri"": ""&lt;redacted&gt;"",
            ""tenant"": ""&lt;redacted&gt;"",
            ""subscriptionId"": ""&lt;redacted&gt;"",
            ""resourceGroupName"": ""&lt;redacted&gt;""
        },
        ""connectVia"": {
            ""referenceName"": ""&lt;redacted&gt;"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p>Azure Blob Linked Service:</p>

<pre><code>{
    ""name"": ""&lt;redacted&gt;"",
    ""properties"": {
        ""type"": ""AzureStorage"",
        ""typeProperties"": {
            ""connectionString"": {
                ""type"": ""SecureString"",
                ""value"": ""DefaultEndpointsProtocol=https;AccountName=&lt;redacted&gt;;AccountKey=&lt;redacted&gt;""
            }
        },
        ""connectVia"": {
           ""referenceName"": ""&lt;redacted&gt;"",
           ""type"": ""IntegrationRuntimeReference""
        }   
    }
}
</code></pre>

<p>Data Lake Dataset</p>

<pre><code>{
    ""name"": ""&lt;redacted&gt;"",
    ""properties"": {
        ""type"": ""AzureDataLakeStoreFile"",
        ""linkedServiceName"":{
            ""referenceName"": ""&lt;redacted&gt;"",
            ""type"": ""LinkedServiceReference""
        },
        ""typeProperties"": {
            ""folderPath"": ""&lt;redacted&gt;""
        }
    }
}
</code></pre>

<p>Blob DataSet</p>

<pre><code>{
    ""name"": ""&lt;redacted&gt;"",
    ""properties"": {
        ""type"": ""AzureBlob"",
        ""linkedServiceName"": {
            ""referenceName"": ""&lt;redacted&gt;"",
            ""type"": ""LinkedServiceReference""
        },
        ""typeProperties"": {
            ""folderPath"": ""&lt;redacted&gt;"",
        }
    }
}
</code></pre>

<p>Pipeline</p>

<pre><code>{
""name"": ""&lt;redacted&gt;"",
""properties"": {
""activities"":[

   {
        ""name"": ""CopyFromBlob"",
    ""type"": ""Copy"",
        ""inputs"": [
            {
                ""referenceName"": ""&lt;redacted&gt;"",
                ""type"": ""DatasetReference""
            }
        ],
        ""outputs"": [
            {
                ""referenceName"": ""&lt;redacted&gt;"",
                ""type"": ""DatasetReference""
            }
        ],

        ""typeProperties"": {
            ""source"": {
                ""type"": ""BlobSource""
            },
            ""sink"": {
                ""type"": ""AzureDataLakeStoreSink""
            }
        }
    }] 
}}
</code></pre>

<p>Powershell does the following:</p>

<pre><code>1. Create Data Factory
2. Create Azure Integration Runtime
3. Create Azure Data Lake Linked Service
4. Create Azure Blob Linked Service
5. Create Azure Blob Dataset
6. Create Azure Data Lake Dataset
7. Create pipeline
8. Invoke pipeline
</code></pre>
","<azure-blob-storage><azure-data-factory><azure-data-lake>","2018-01-02 15:02:39","1495","-1","1","48119708","<p>Check your Azure Integration Runtime powershell command - are you defining a location and type? I ran into a very similar error (my null parameter was 'dictionary') when I forgot to define a location for my integration runtime.  </p>

<p>Hope this helps! </p>
"
"48057324","Fault tolerance in Copy Activity by skipping incompatible rows","<p>I use Azure Fucntion with Azure SDK and Azure Data Factory, <strong>is there any way to get value of skippedRowCount</strong> of Activity Window when applying ""log the incompatible rows"" in Copy Activity (Source: Blob Storage, Sink: SQL Data Warehouse)?</p>
","<azure-functions><azure-data-factory><azure-sdk-.net>","2018-01-02 07:43:26","1220","1","1","48103103","<p>From the documentation here : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#monitor-skipped-rows"" rel=""nofollow noreferrer"">Monitor skipped rows</a>, looks like a log file is automatically generated when log incompatible rows is enabled at <a href=""https://[your-blob-account].blob.core.windows.net/[path-if-configured]/[copy-activity-run-id]/[auto-generated-GUID].csv"" rel=""nofollow noreferrer"">https://[your-blob-account].blob.core.windows.net/[path-if-configured]/[copy-activity-run-id]/[auto-generated-GUID].csv</a>. You can set up a <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob"" rel=""nofollow noreferrer"">blob trigger</a> on the generated log file. Here is a sample function.json</p>

<pre><code>""disabled"": false,
""bindings"": [
    {
        ""name"": ""myBlob"",
        ""type"": ""blobTrigger"",
        ""direction"": ""in"",
        ""path"": ""[path-if-configured]/[copy-activity-run-id]"",
        ""connection"":""MyStorageAccountAppSetting""
    }
]
</code></pre>

<p>Note: you could also use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob#trigger---blob-name-patterns"" rel=""nofollow noreferrer"">blob name patterns</a> to specify path in the binding.</p>

<p>In the function code, you can easily access the contents of the log file to get the skippedRowCount</p>
"
"48055769","Azure data Factory - write result of pipeline to REST API?","<p>While I've seen examples of getting data from HTTP, I haven't really seen once for outputting to a REST API.</p>

<p>What's the best way to do this?</p>

<p>Thanks,
-Greg</p>
","<azure><azure-data-factory>","2018-01-02 04:57:29","199","0","1","48059006","<p>I would suggest using the web activity in ADFv2. It offers GET, POST and PUT methods.</p>

<p>In ADFv1 you'd need to write a custom activity to interact with the API.</p>

<p>Hope this helps.</p>
"